id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2411664258,pull_request,closed,,Seen performance regression.,"Seen performance regression.

Reverts db06762633d89088a76d80b97b9e334aa6eaf95b
",copybara-service[bot],2024-07-16 17:14:40+00:00,[],2024-07-17 22:16:59+00:00,2024-07-17 22:16:58+00:00,https://github.com/tensorflow/tensorflow/pull/71945,[],[],
2411629734,pull_request,closed,,Reverts aba14f7f702d577686a51b7a2e648f784ba8f7bb,"Reverts aba14f7f702d577686a51b7a2e648f784ba8f7bb

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14947 from ROCm:ci_hotfix_240716 614fda0926c233d1ffc71b85b66e3bd772970392
",copybara-service[bot],2024-07-16 16:53:22+00:00,[],2024-07-16 17:48:39+00:00,2024-07-16 17:48:39+00:00,https://github.com/tensorflow/tensorflow/pull/71944,[],[],
2411601965,pull_request,closed,,[xla:cpu] Add SortThunk implementation,"[xla:cpu] Add SortThunk implementation

Add a generic SortThunk implementation that sorts input buffers using comparator function pointer.
",copybara-service[bot],2024-07-16 16:35:35+00:00,['ezhulenev'],2024-07-17 00:36:05+00:00,2024-07-17 00:36:05+00:00,https://github.com/tensorflow/tensorflow/pull/71943,[],[],
2411585372,pull_request,closed,,Exit early in SparseLocalLoadOpToLLVMPass if there are no sparse local loads.,"Exit early in SparseLocalLoadOpToLLVMPass if there are no sparse local loads.

This pass can add a barrier because it runs part of the TritonGPU->LLVM pass before it (necessary to get the barriers in the correct locations). This can be confusing when debugging things unrelated to sparsity.  This pass should do nothing if there are no sparse passes to change.
",copybara-service[bot],2024-07-16 16:25:46+00:00,[],2024-07-17 17:10:42+00:00,2024-07-17 17:10:40+00:00,https://github.com/tensorflow/tensorflow/pull/71942,[],[],
2411535394,pull_request,closed,,[XLA:GPU] Mark reductions with a non-const init value as supported for the Triton emitter.,"[XLA:GPU] Mark reductions with a non-const init value as supported for the Triton emitter.

Support was added with https://github.com/openxla/xla/commit/c12979e32dd6fc4c4e53015d82bb939fa599b314
but the test didn't fail because of the currently broken scalar block
pointers.
",copybara-service[bot],2024-07-16 15:57:30+00:00,[],2024-07-17 15:56:39+00:00,2024-07-17 15:56:38+00:00,https://github.com/tensorflow/tensorflow/pull/71941,[],[],
2411506653,pull_request,closed,,[xla:cpu] Rename HostKernels to FunctionRegistry,"[xla:cpu] Rename HostKernels to FunctionRegistry
",copybara-service[bot],2024-07-16 15:42:11+00:00,['ezhulenev'],2024-07-16 16:25:18+00:00,2024-07-16 16:25:17+00:00,https://github.com/tensorflow/tensorflow/pull/71940,[],[],
2411499916,pull_request,closed,,Pipe through the GPU core_count attribute to GPU Topology,"Pipe through the GPU core_count attribute to GPU Topology
",copybara-service[bot],2024-07-16 15:38:36+00:00,[],2024-07-20 00:23:45+00:00,2024-07-20 00:23:44+00:00,https://github.com/tensorflow/tensorflow/pull/71939,[],[],
2411396673,pull_request,closed,,Add potential conversion to `qp8` for `fully-connected` in the XNNPACK delegate.,"Add potential conversion to `qp8` for `fully-connected` in the XNNPACK delegate.
",copybara-service[bot],2024-07-16 15:03:44+00:00,[],2024-08-27 11:44:14+00:00,2024-08-27 11:44:14+00:00,https://github.com/tensorflow/tensorflow/pull/71938,[],[],
2411392729,pull_request,closed,,[XLA:GPU] Make the generic Triton emitter preserve tensor ranks during codegen.,"[XLA:GPU] Make the generic Triton emitter preserve tensor ranks during codegen.

Triton seems to have become robust enough to allow us to do this (at least for
non-GEMMs).

Notably, this requires the introduction of an explicit lowering path for
`reshape`s and `bitcast`s. Of course, this change required updating a lot of
change detectors `Filecheck` tests.

I took the opportunity to delete some of these that no longer brought useful 
coverage after the switch to the new generic Triton emitter. I made the
remaining tests more robust by focusing the matchers on key properties of the
generated code, and also by executing them (without running the optimization
pipeline) if it seemed warranted.
",copybara-service[bot],2024-07-16 15:02:06+00:00,[],2024-07-18 10:27:35+00:00,2024-07-18 10:27:34+00:00,https://github.com/tensorflow/tensorflow/pull/71936,[],[],
2411388252,pull_request,closed,,[XLA:GPU] Simplify sparse convert to llvm tests,"[XLA:GPU] Simplify sparse convert to llvm tests
",copybara-service[bot],2024-07-16 15:00:09+00:00,[],2024-07-17 13:30:21+00:00,2024-07-17 13:30:20+00:00,https://github.com/tensorflow/tensorflow/pull/71935,[],[],
2411335776,pull_request,closed,,"[XLA:GPU] Disable the constant folding for pad(broadcast(), constant)","[XLA:GPU] Disable the constant folding for pad(broadcast(), constant)

It does not give us significant benefits. At the same time for the big outputs like 1m parameters it needs significant compile time.
",copybara-service[bot],2024-07-16 14:36:25+00:00,[],2024-07-29 13:27:23+00:00,2024-07-29 13:27:22+00:00,https://github.com/tensorflow/tensorflow/pull/71934,[],[],
2411231791,pull_request,closed,,[XLA:GPU][IndexAnalysis] Fix symbol reordering in the indexing map composition.,"[XLA:GPU][IndexAnalysis] Fix symbol reordering in the indexing map composition.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14964 from ROCm:ci_oclc_abi_version e1691462396648278cc0d64615a574e9fb45836e
",copybara-service[bot],2024-07-16 14:01:46+00:00,['pifon2a'],2024-07-17 13:36:15+00:00,2024-07-17 13:36:14+00:00,https://github.com/tensorflow/tensorflow/pull/71933,[],[],
2411123411,pull_request,closed,,PR #14958: [ROCM][NFC] buffer_comparator refactoring,"PR #14958: [ROCM][NFC] buffer_comparator refactoring

Imported from GitHub PR https://github.com/openxla/xla/pull/14958

This is a refactoring extracted from this original PR: https://github.com/openxla/xla/pull/13425

Here I have added Parameters to later add xla_gpu_autotune_gemm_rtol parameter to buffer_comparator and avoid passing many parameters from one function to another which also improves code readabilioty.

Why did I make the template functions DeviceCompare, HostCompare and CompareEqualParameterized **non-members** again? 
They were introduced here: https://github.com/openxla/xla/commit/3b54582a16b989b005429c56b19cc81ebb25f92f
But later on, this issue popped up during the review of the original PR, see https://github.com/openxla/xla/pull/13425#discussion_r1634636228. But, personally, I am also fine with template member functions. 

@xla-rotation: could you have a look please ?



Copybara import of the project:

--
733496c001a4ca537e125970ab7e08df09e6f884 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:

small refactoring

Merging this change closes #14958

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14958 from ROCm:ci_buffer_comparator_refactor 733496c001a4ca537e125970ab7e08df09e6f884
",copybara-service[bot],2024-07-16 13:18:12+00:00,[],2024-07-16 14:42:29+00:00,2024-07-16 14:42:28+00:00,https://github.com/tensorflow/tensorflow/pull/71932,[],[],
2411108533,pull_request,closed,,[XLA:GPU] Move ConvertLayout method inside lowerSharedToSparseMeta,"[XLA:GPU] Move ConvertLayout method inside lowerSharedToSparseMeta
",copybara-service[bot],2024-07-16 13:11:54+00:00,[],2024-07-22 16:03:51+00:00,2024-07-22 16:03:50+00:00,https://github.com/tensorflow/tensorflow/pull/71931,[],[],
2411017652,pull_request,open,,Integrate LLVM at llvm/llvm-project@59e56eeb1d9c,"Integrate LLVM at llvm/llvm-project@59e56eeb1d9c

Updates LLVM usage to match
[59e56eeb1d9c](https://github.com/llvm/llvm-project/commit/59e56eeb1d9c)
",copybara-service[bot],2024-07-16 12:29:46+00:00,[],2024-07-16 13:29:10+00:00,,https://github.com/tensorflow/tensorflow/pull/71929,[],[],
2410997515,pull_request,closed,,[XLA:GPU] Remove old IsProfileApplicable method.,"[XLA:GPU] Remove old IsProfileApplicable method.
",copybara-service[bot],2024-07-16 12:19:43+00:00,[],2024-07-22 15:31:08+00:00,2024-07-22 15:31:07+00:00,https://github.com/tensorflow/tensorflow/pull/71928,[],[],
2410888892,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14692 from openxla:skip_high_shmem_test f6f8acff5ea2bdb5d8f53c8fb0457aecaf2b512c
",copybara-service[bot],2024-07-16 11:20:13+00:00,[],2024-07-16 11:20:13+00:00,,https://github.com/tensorflow/tensorflow/pull/71926,[],[],
2410838998,pull_request,closed,,[XLA:GPU] Fix a bug in symbolic tile rt_var printer.,"[XLA:GPU] Fix a bug in symbolic tile rt_var printer.
",copybara-service[bot],2024-07-16 10:52:23+00:00,[],2024-07-16 13:08:18+00:00,2024-07-16 13:08:17+00:00,https://github.com/tensorflow/tensorflow/pull/71925,[],[],
2410806396,pull_request,closed,,[XLA:GPU] Change the format of the Autotuning key.,"[XLA:GPU] Change the format of the Autotuning key.
",copybara-service[bot],2024-07-16 10:34:46+00:00,[],2024-07-19 10:51:57+00:00,2024-07-19 10:51:56+00:00,https://github.com/tensorflow/tensorflow/pull/71924,[],[],
2410757131,pull_request,closed,,[XLA:GPU] Add accuracy checker for PGLE on GPU.,"[XLA:GPU] Add accuracy checker for PGLE on GPU.
",copybara-service[bot],2024-07-16 10:09:24+00:00,[],2024-07-22 15:23:47+00:00,2024-07-22 15:23:46+00:00,https://github.com/tensorflow/tensorflow/pull/71923,[],[],
2410729001,pull_request,closed,,PR #14692: [GPU][NFC] Skip tests requiring large shared memory on low-spec GPUs.,"PR #14692: [GPU][NFC] Skip tests requiring large shared memory on low-spec GPUs.

Imported from GitHub PR https://github.com/openxla/xla/pull/14692


Copybara import of the project:

--
255b647915c09e0ad198697385da71c92d9aeec9 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Skip tests requiring large shared memory on low-spec GPUs.

--
f6f8acff5ea2bdb5d8f53c8fb0457aecaf2b512c by Ilia Sergachev <isergachev@nvidia.com>:

Address feedback

Merging this change closes #14692

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14692 from openxla:skip_high_shmem_test f6f8acff5ea2bdb5d8f53c8fb0457aecaf2b512c
",copybara-service[bot],2024-07-16 09:58:00+00:00,[],2024-07-16 11:34:53+00:00,2024-07-16 11:34:52+00:00,https://github.com/tensorflow/tensorflow/pull/71921,[],[],
2410630067,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 09:11:09+00:00,[],2024-07-19 07:29:18+00:00,2024-07-19 07:29:18+00:00,https://github.com/tensorflow/tensorflow/pull/71920,[],[],
2410607400,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 09:00:11+00:00,[],2024-07-17 06:18:45+00:00,,https://github.com/tensorflow/tensorflow/pull/71919,[],[],
2410604306,pull_request,closed,,PR #14947: [ROCm] Fix build break,"PR #14947: [ROCm] Fix build break

Imported from GitHub PR https://github.com/openxla/xla/pull/14947

Issue was introduced in [2cc8aba](https://github.com/openxla/xla/commit/2cc8abada7e2c15f51477a4b36b3e00fcfff1481)
Copybara import of the project:

--
1e3af2a11ca3210e3bb12bf7d8f679262b227a54 by mmakevic <Milica.Makevic@amd.com>:

Fix build break due to 2cc8aba

Merging this change closes #14947

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14947 from ROCm:ci_hotfix_240716 614fda0926c233d1ffc71b85b66e3bd772970392
",copybara-service[bot],2024-07-16 08:58:42+00:00,[],2024-07-16 16:46:40+00:00,2024-07-16 16:46:39+00:00,https://github.com/tensorflow/tensorflow/pull/71918,[],[],
2410596281,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:55:02+00:00,[],2024-07-17 06:54:29+00:00,,https://github.com/tensorflow/tensorflow/pull/71917,[],[],
2410594633,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:54:22+00:00,[],2024-07-16 11:54:44+00:00,,https://github.com/tensorflow/tensorflow/pull/71916,[],[],
2410587757,pull_request,open,,PR #14881: [GPU] Remove DRAM size from model_str in device description.,"PR #14881: [GPU] Remove DRAM size from model_str in device description.

Imported from GitHub PR https://github.com/openxla/xla/pull/14881

Exact DRAM size can slightly vary for the same GPU model so it shouldn't be used in model identification. The removed part of gpu_compiler_test_autotune_db.textproto illustrates that.
Copybara import of the project:

--
8ecbaf3fafade5ae7fd51ef90138fbc0e0666de3 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Remove DRAM size from model_str in device description.

Exact DRAM size can slightly vary for the same GPU model so it shouldn't
be used in model identification. The removed part of
gpu_compiler_test_autotune_db.textproto illustrates that.

Merging this change closes #14881

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14881 from openxla:remove_dram_size 8ecbaf3fafade5ae7fd51ef90138fbc0e0666de3
",copybara-service[bot],2024-07-16 08:51:09+00:00,[],2024-07-16 08:51:09+00:00,,https://github.com/tensorflow/tensorflow/pull/71915,[],[],
2410581741,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:48:13+00:00,[],2024-07-16 11:59:32+00:00,,https://github.com/tensorflow/tensorflow/pull/71914,[],[],
2410568280,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:41:44+00:00,[],2024-07-17 07:05:55+00:00,,https://github.com/tensorflow/tensorflow/pull/71913,[],[],
2410554064,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:34:39+00:00,[],2024-07-19 07:49:43+00:00,2024-07-19 07:49:42+00:00,https://github.com/tensorflow/tensorflow/pull/71911,[],[],
2410542591,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:28:52+00:00,[],2024-07-17 06:23:56+00:00,,https://github.com/tensorflow/tensorflow/pull/71910,[],[],
2410521613,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 08:18:11+00:00,[],2024-07-16 08:18:11+00:00,,https://github.com/tensorflow/tensorflow/pull/71909,[],[],
2410510038,pull_request,closed,,PR #14822: Add replicated testing utility functions to hlo_test_base,"PR #14822: Add replicated testing utility functions to hlo_test_base

Imported from GitHub PR https://github.com/openxla/xla/pull/14822

This patch adds the functions `RunAndCompareTwoModulesReplicated` and `CompareInputs` to `HloTestBase`. These functions can now be used in testing replicated HLO modules.
Copybara import of the project:

--
551aeec4c09495629d0b932f671050eadea6fbc4 by Shraiysh Vaishay <svaishay@nvidia.com>:

Add replicated testing utility functions to hlo_test_base

This patch adds the functions `RunAnd CompareTwoModulesReplicated`
and `CompareInputs` to `HloTestBase`. These functions can now be
used in testing replicated HLO modules.

Merging this change closes #14822

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14822 from shraiysh:add_compare_inputs_to_test 551aeec4c09495629d0b932f671050eadea6fbc4
",copybara-service[bot],2024-07-16 08:12:10+00:00,[],2024-07-16 09:05:22+00:00,2024-07-16 09:05:21+00:00,https://github.com/tensorflow/tensorflow/pull/71908,[],[],
2410421297,pull_request,closed,,Remove TestNoAdditionalSimplificationFromMlir fuzz test.,"Remove TestNoAdditionalSimplificationFromMlir fuzz test.

This triggers false positives too often, even with the cost
metric.
",copybara-service[bot],2024-07-16 07:28:22+00:00,[],2024-07-16 08:51:40+00:00,2024-07-16 08:51:39+00:00,https://github.com/tensorflow/tensorflow/pull/71907,[],[],
2410411159,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-16 07:22:42+00:00,[],2024-07-16 07:22:42+00:00,,https://github.com/tensorflow/tensorflow/pull/71906,[],[],
2410260406,pull_request,closed,,Also remove linker option --no-undefined for MacOS.,"Also remove linker option --no-undefined for MacOS.

Currently compilation fails with ld: unknown option: --no-undefined
",copybara-service[bot],2024-07-16 05:41:57+00:00,['akuegel'],2024-07-16 06:55:05+00:00,2024-07-16 06:55:05+00:00,https://github.com/tensorflow/tensorflow/pull/71905,[],[],
2410226731,pull_request,closed,,[XLA] Support forward-sink-pipelining collectives when they are used by multiple DUS ops.,"[XLA] Support forward-sink-pipelining collectives when they are used by multiple DUS ops.
",copybara-service[bot],2024-07-16 05:10:04+00:00,['seherellis'],2024-07-23 02:36:51+00:00,2024-07-23 02:36:51+00:00,https://github.com/tensorflow/tensorflow/pull/71904,[],[],
2410098990,pull_request,closed,,PR #14891: [XLA:GPU] Setting enabling memory pool access across devices,"PR #14891: [XLA:GPU] Setting enabling memory pool access across devices

Imported from GitHub PR https://github.com/openxla/xla/pull/14891

This PR enabling cross memory pool access for cuda_mallocasync allocator, otherwise it will lead to memory fault for intra node nccl operators 
Copybara import of the project:

--
6e78412751586fd5d0b0c1faa0dff1c1bf4579b6 by Shawn Wang <shawnw@nvidia.com>:

Setting enabling memory pool access across devices


Merging this change closes #14891

Reverts ca60e393c97b6c4af847dd32534beb1ac146d258

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14891 from shawnwang18:shawnw/add_access_permission_across_cuda_async_memory_pool 6e78412751586fd5d0b0c1faa0dff1c1bf4579b6
",copybara-service[bot],2024-07-16 03:10:49+00:00,[],2024-07-16 10:51:39+00:00,2024-07-16 10:51:38+00:00,https://github.com/tensorflow/tensorflow/pull/71903,[],[],
2409946931,pull_request,closed,,Re-enable `unary_ops_test` on Arm64 machines,"Re-enable `unary_ops_test` on Arm64 machines

Reverts ca60e393c97b6c4af847dd32534beb1ac146d258
",copybara-service[bot],2024-07-16 01:10:49+00:00,[],2024-07-16 09:34:56+00:00,2024-07-16 09:34:55+00:00,https://github.com/tensorflow/tensorflow/pull/71902,[],[],
2409942056,pull_request,closed,,[XLA] Reintroduce convert(constant) to constant rewrite with stronger conditions: do the rewrite only when the user count of the constant is 1 and use_convert_constant_folding is enabled,"[XLA] Reintroduce convert(constant) to constant rewrite with stronger conditions: do the rewrite only when the user count of the constant is 1 and use_convert_constant_folding is enabled
",copybara-service[bot],2024-07-16 01:05:40+00:00,[],2024-07-23 00:52:50+00:00,2024-07-23 00:52:50+00:00,https://github.com/tensorflow/tensorflow/pull/71901,[],[],
2409847739,pull_request,closed,,"Refactor HostOffloadLegalize by extracting copy movement and layout update into their own functions. Also, improve comments and debugging logging.","Refactor HostOffloadLegalize by extracting copy movement and layout update into their own functions. Also, improve comments and debugging logging.
",copybara-service[bot],2024-07-16 00:01:58+00:00,['SandSnip3r'],2024-07-23 22:47:01+00:00,2024-07-23 22:46:59+00:00,https://github.com/tensorflow/tensorflow/pull/71900,[],[],
2409847162,pull_request,closed,,Adds test to ensure that the `Delete()` methods of all `ifrt::Array` implementations satisfy the idempotent property.,"Adds test to ensure that the `Delete()` methods of all `ifrt::Array` implementations satisfy the idempotent property.
",copybara-service[bot],2024-07-16 00:01:38+00:00,[],2024-07-16 21:27:50+00:00,2024-07-16 21:27:50+00:00,https://github.com/tensorflow/tensorflow/pull/71899,[],[],
2409815707,pull_request,closed,,[XLA:GPU] Add a test case for collective permute forward cycle decomposer with while loop.,"[XLA:GPU] Add a test case for collective permute forward cycle decomposer with while loop.
",copybara-service[bot],2024-07-15 23:35:30+00:00,[],2024-07-16 22:00:35+00:00,2024-07-16 22:00:33+00:00,https://github.com/tensorflow/tensorflow/pull/71898,[],[],
2409776507,pull_request,open,,Remove ConvertTFExecutorToStablehloFlatbuffer. This is no longer used in production nor is an active project in development. Need to remove this to reduce TFLite dependency on TF SavedModelBundle and/or Session.,"Remove ConvertTFExecutorToStablehloFlatbuffer. This is no longer used in production nor is an active project in development. Need to remove this to reduce TFLite dependency on TF SavedModelBundle and/or Session.
",copybara-service[bot],2024-07-15 22:51:54+00:00,['vamsimanchala'],2024-07-15 22:51:55+00:00,,https://github.com/tensorflow/tensorflow/pull/71897,[],[],
2409772645,pull_request,closed,,Integrate StableHLO at openxla/stablehlo@7e749c84,"Integrate StableHLO at openxla/stablehlo@7e749c84
",copybara-service[bot],2024-07-15 22:47:48+00:00,['sdasgup3'],2024-07-16 19:23:20+00:00,2024-07-16 19:23:19+00:00,https://github.com/tensorflow/tensorflow/pull/71896,[],[],
2409768390,pull_request,closed,,Makes the `xla::ifrt::Value::Delete()` method idempotent.,"Makes the `xla::ifrt::Value::Delete()` method idempotent.
",copybara-service[bot],2024-07-15 22:43:21+00:00,[],2024-07-16 21:48:34+00:00,2024-07-16 21:48:33+00:00,https://github.com/tensorflow/tensorflow/pull/71895,[],[],
2409744526,pull_request,closed,,Remove RunHloToTfConversion function.,"Remove RunHloToTfConversion function.
",copybara-service[bot],2024-07-15 22:24:26+00:00,['vamsimanchala'],2024-07-17 20:20:23+00:00,2024-07-17 20:20:22+00:00,https://github.com/tensorflow/tensorflow/pull/71894,[],[],
2409726873,pull_request,closed,,Use MemoryType instead of MemorySpace in StreamExecutor::GetPointerMemorySpace,"Use MemoryType instead of MemorySpace in StreamExecutor::GetPointerMemorySpace

MemorySpace is a subset of MemoryType.

Reverts da6b16843ebbceb5ee654673045a671924f85783
",copybara-service[bot],2024-07-15 22:08:25+00:00,['superbobry'],2024-07-16 20:17:07+00:00,2024-07-16 20:17:06+00:00,https://github.com/tensorflow/tensorflow/pull/71893,[],[],
2409722351,pull_request,closed,,Use consistent paths for Eigen headers in XLA,"Use consistent paths for Eigen headers in XLA
",copybara-service[bot],2024-07-15 22:04:11+00:00,['ddunl'],2024-07-15 23:44:09+00:00,2024-07-15 23:44:09+00:00,https://github.com/tensorflow/tensorflow/pull/71892,[],[],
2409666930,pull_request,open,,Remove flex delegate from pywrap_tensorflow_internal,"Remove flex delegate from pywrap_tensorflow_internal
",copybara-service[bot],2024-07-15 21:22:25+00:00,['ecalubaquib'],2024-07-24 20:51:50+00:00,,https://github.com/tensorflow/tensorflow/pull/71891,[],"[{'comment_id': 2229464652, 'issue_id': 2409666930, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71891/checks?check_run_id=27478261735) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 15, 21, 22, 30, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-15 21:22:30 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71891/checks?check_run_id=27478261735) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2409645539,pull_request,closed,,[XLA:SPMD] Simplify the chain of sharding instructions.,"[XLA:SPMD] Simplify the chain of sharding instructions.

Rewrite the pattern
```
B = sharding-constraint(A, sharding=S1)
C = sharding-constraint(B, sharding=S2), which is the only user of B
```
as
```
C = sharding-constraint(A, sharding=S2)
```

The intermediate sharding S1 and tensor B are redundant, which are removed as a pre-processing in sharding propagation.
",copybara-service[bot],2024-07-15 21:10:25+00:00,[],2024-07-19 05:09:55+00:00,2024-07-19 05:09:54+00:00,https://github.com/tensorflow/tensorflow/pull/71890,[],[],
2409577132,pull_request,closed,,Reverts 22854f5590189838269532da6574d533217125d8,"Reverts 22854f5590189838269532da6574d533217125d8
",copybara-service[bot],2024-07-15 20:35:33+00:00,['JW1992'],2024-07-15 21:55:41+00:00,2024-07-15 21:55:40+00:00,https://github.com/tensorflow/tensorflow/pull/71889,[],[],
2409571227,pull_request,closed,,Added a method for looking up the memory space of a pointer to stream_executor::StreamExecutor,"Added a method for looking up the memory space of a pointer to stream_executor::StreamExecutor
",copybara-service[bot],2024-07-15 20:32:23+00:00,['superbobry'],2024-07-15 22:15:22+00:00,2024-07-15 22:15:22+00:00,https://github.com/tensorflow/tensorflow/pull/71888,[],[],
2409560029,pull_request,closed,,Add macros header import but behind !TFLITE_BUILD_WITH_XNNPACK_DELEGATE where it is actually used,"Add macros header import but behind !TFLITE_BUILD_WITH_XNNPACK_DELEGATE where it is actually used
",copybara-service[bot],2024-07-15 20:25:16+00:00,[],2024-07-16 10:42:35+00:00,2024-07-16 10:42:34+00:00,https://github.com/tensorflow/tensorflow/pull/71887,[],[],
2409537748,pull_request,closed,,[XLA] Introduce selective resources in latency hiding scheduler.,"[XLA] Introduce selective resources in latency hiding scheduler.

Introduce a new resource hazard type in latency hiding scheduler that informs the scheduler of async instructions that can only make progress during the execution of a limited set of instructions.
",copybara-service[bot],2024-07-15 20:11:44+00:00,[],2024-07-20 05:20:11+00:00,2024-07-20 05:20:10+00:00,https://github.com/tensorflow/tensorflow/pull/71886,[],[],
2409526831,pull_request,closed,,[xla:ffi] Add instantiation handler to XLA_FFI_Handler_Bundle,"[xla:ffi] Add instantiation handler to XLA_FFI_Handler_Bundle
",copybara-service[bot],2024-07-15 20:04:53+00:00,['ezhulenev'],2024-07-15 21:05:11+00:00,2024-07-15 21:05:11+00:00,https://github.com/tensorflow/tensorflow/pull/71885,[],[],
2409456698,pull_request,closed,,Export lists of keys from batch_stats.h,"Export lists of keys from batch_stats.h

Adds functionality to output the list of (model_name, op_name) tuples tracked by a BatchStats object and the list of batch sizes tracked by a ModelBatchStats object. This allows for iteration over these objects.
",copybara-service[bot],2024-07-15 19:23:24+00:00,[],2024-07-15 21:49:54+00:00,2024-07-15 21:49:54+00:00,https://github.com/tensorflow/tensorflow/pull/71883,[],[],
2409432525,pull_request,closed,,Add xla::PyArray::borrow(PyObject*) so that it can work with other binding framework.,"Add xla::PyArray::borrow(PyObject*) so that it can work with other binding framework.
",copybara-service[bot],2024-07-15 19:10:15+00:00,[],2024-07-17 18:46:56+00:00,2024-07-17 18:46:55+00:00,https://github.com/tensorflow/tensorflow/pull/71882,[],[],
2409429950,pull_request,closed,,"Remove Mac x86 jobs, configs and scripts","Remove Mac x86 jobs, configs and scripts

TensorFlow's Mac x86 packages have been deprecated and will no longer be released. The last release for these packages were done in TF `2.16.2`.
",copybara-service[bot],2024-07-15 19:08:42+00:00,['nitins17'],2024-07-17 21:38:38+00:00,2024-07-17 21:38:37+00:00,https://github.com/tensorflow/tensorflow/pull/71881,[],[],
2409416705,pull_request,closed,,[xla:cpu] Add support for XLA:CPU+Thunks compilation cache,"[xla:cpu] Add support for XLA:CPU+Thunks compilation cache

+ fix custom call thunk for tuple result of size 1

Reverts db06762633d89088a76d80b97b9e334aa6eaf95b
",copybara-service[bot],2024-07-15 19:01:12+00:00,['ezhulenev'],2024-07-18 00:21:45+00:00,2024-07-18 00:21:44+00:00,https://github.com/tensorflow/tensorflow/pull/71880,[],[],
2409391350,pull_request,closed,,Bump setuptools from 68.2.2 to 70.0.0,"Bumps [setuptools](https://github.com/pypa/setuptools) from 68.2.2 to 70.0.0.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pypa/setuptools/blob/main/NEWS.rst"">setuptools's changelog</a>.</em></p>
<blockquote>
<h1>v70.0.0</h1>
<h2>Features</h2>
<ul>
<li>Emit a warning when <code>[tools.setuptools]</code> is present in <code>pyproject.toml</code> and will be ignored. -- by :user:<code>SnoopJ</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4150"">#4150</a>)</li>
<li>Improved <code>AttributeError</code> error message if <code>pkg_resources.EntryPoint.require</code> is called without extras or distribution
Gracefully &quot;do nothing&quot; when trying to activate a <code>pkg_resources.Distribution</code> with a <code>None</code> location, rather than raising a <code>TypeError</code>
-- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4262"">#4262</a>)</li>
<li>Typed the dynamically defined variables from <code>pkg_resources</code> -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4267"">#4267</a>)</li>
<li>Modernized and refactored VCS handling in package_index. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4332"">#4332</a>)</li>
</ul>
<h2>Bugfixes</h2>
<ul>
<li>In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4136"">#4136</a>)</li>
<li>Fix finder template for lenient editable installs of implicit nested namespaces
constructed by using <code>package_dir</code> to reorganise directory structure. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4278"">#4278</a>)</li>
<li>Fix an error with <code>UnicodeDecodeError</code> handling in <code>pkg_resources</code> when trying to read files in UTF-8 with a fallback -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4348"">#4348</a>)</li>
</ul>
<h2>Improved Documentation</h2>
<ul>
<li>Uses RST substitution to put badges in 1 line. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4312"">#4312</a>)</li>
</ul>
<h2>Deprecations and Removals</h2>
<ul>
<li>
<p>Further adoption of UTF-8 in <code>setuptools</code>.
This change regards mostly files produced and consumed during the build process
(e.g. metadata files, script wrappers, automatically updated config files, etc..)
Although precautions were taken to minimize disruptions, some edge cases might
be subject to backwards incompatibility.</p>
<p>Support for <code>&quot;locale&quot;</code> encoding is now <strong>deprecated</strong>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4309"">#4309</a>)</p>
</li>
<li>
<p>Remove <code>setuptools.convert_path</code> after long deprecation period.
This function was never defined by <code>setuptools</code> itself, but rather a
side-effect of an import for internal usage. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4322"">#4322</a>)</p>
</li>
<li>
<p>Remove fallback for customisations of <code>distutils</code>' <code>build.sub_command</code> after long
deprecated period.
Users are advised to import <code>build</code> directly from <code>setuptools.command.build</code>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4322"">#4322</a>)</p>
</li>
<li>
<p>Removed <code>typing_extensions</code> from vendored dependencies -- by :user:<code>Avasam</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4324"">#4324</a>)</p>
</li>
<li>
<p>Remove deprecated <code>setuptools.dep_util</code>.
The provided alternative is <code>setuptools.modified</code>. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4360"">#4360</a>)</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pypa/setuptools/commit/5cbf12a9b63fd37985a4525617b46576b8ac3a7b""><code>5cbf12a</code></a> Workaround for release error in v70</li>
<li><a href=""https://github.com/pypa/setuptools/commit/9c1bcc3417bd12668123f7e731e241d9e57bfc57""><code>9c1bcc3</code></a> Bump version: 69.5.1 → 70.0.0</li>
<li><a href=""https://github.com/pypa/setuptools/commit/4dc0c31644b458ac43ce6148f6a9dc729a7e78b5""><code>4dc0c31</code></a> Remove deprecated <code>setuptools.dep_util</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4360"">#4360</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/6c1ef5748dbd70c8c5423e12680345766ee101d9""><code>6c1ef57</code></a> Remove xfail now that test passes. Ref <a href=""https://redirect.github.com/pypa/setuptools/issues/4371"">#4371</a>.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/d14fa0162c95450898c11534caf26a0f03553176""><code>d14fa01</code></a> Add all site-packages dirs when creating simulated environment for test_edita...</li>
<li><a href=""https://github.com/pypa/setuptools/commit/6b7f7a18afc90007544092c446dc0cd856d86b17""><code>6b7f7a1</code></a> Prevent <code>bin</code> folders to be taken as extern packages when vendoring (<a href=""https://redirect.github.com/pypa/setuptools/issues/4370"">#4370</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/69141f69f8bf38da34cbea552d6fdaa9c8619c53""><code>69141f6</code></a> Add doctest for vendorised bin folder</li>
<li><a href=""https://github.com/pypa/setuptools/commit/2a53cc1200ec4b14e08e84be3c042f8983dfb7d7""><code>2a53cc1</code></a> Prevent 'bin' folders to be taken as extern packages</li>
<li><a href=""https://github.com/pypa/setuptools/commit/720862807dea012f3a0e7061880691025f736f11""><code>7208628</code></a> Replace call to deprecated <code>validate_pyproject</code> command (<a href=""https://redirect.github.com/pypa/setuptools/issues/4363"">#4363</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/96d681aa405460f724c62c00ca125ae722ad810a""><code>96d681a</code></a> Remove call to deprecated validate_pyproject command</li>
<li>Additional commits viewable in <a href=""https://github.com/pypa/setuptools/compare/v68.2.2...v70.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=setuptools&package-manager=pip&previous-version=68.2.2&new-version=70.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).

</details>",dependabot[bot],2024-07-15 18:46:35+00:00,['gbaned'],2024-07-17 03:28:46+00:00,2024-07-17 03:28:45+00:00,https://github.com/tensorflow/tensorflow/pull/71879,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:S', 'CL Change Size: Small'), ('dependencies', 'Pull requests that update a dependency file'), ('python', 'Pull requests that update Python code')]",[],
2409358964,pull_request,closed,,[XLA:GPU] Remove GpuComplexType and GpuDoubleComplexType aliases from SYCL code.,"[XLA:GPU] Remove GpuComplexType and GpuDoubleComplexType aliases from SYCL code.

We removed these aliases from the code, race condition with the commit introducing SYCL code re-introduced them.
",copybara-service[bot],2024-07-15 18:27:42+00:00,[],2024-07-16 11:26:42+00:00,2024-07-16 11:26:41+00:00,https://github.com/tensorflow/tensorflow/pull/71878,[],[],
2409358765,pull_request,open,,Internal only copybara config change,"Internal only copybara config change
",copybara-service[bot],2024-07-15 18:27:34+00:00,['ddunl'],2024-07-15 18:27:35+00:00,,https://github.com/tensorflow/tensorflow/pull/71877,[],[],
2409346249,pull_request,closed,,Copy part of TFL portable_tensor_utils to MLIR.,"Copy part of TFL portable_tensor_utils to MLIR.
",copybara-service[bot],2024-07-15 18:20:13+00:00,['pak-laura'],2024-07-22 18:20:43+00:00,2024-07-22 18:20:42+00:00,https://github.com/tensorflow/tensorflow/pull/71876,[],[],
2409330699,pull_request,closed,,Move parts of quantization utils to a separate file.,"Move parts of quantization utils to a separate file.
",copybara-service[bot],2024-07-15 18:11:48+00:00,[],2024-07-30 23:16:10+00:00,2024-07-30 23:16:10+00:00,https://github.com/tensorflow/tensorflow/pull/71875,[],[],
2409326788,pull_request,open,,Internal only copybara config change,"Internal only copybara config change
",copybara-service[bot],2024-07-15 18:09:31+00:00,['ddunl'],2024-07-15 18:09:32+00:00,,https://github.com/tensorflow/tensorflow/pull/71874,[],[],
2409316087,pull_request,closed,,Give XLA it's own `.bazelversion` instead of copying TensorFlow's,"Give XLA it's own `.bazelversion` instead of copying TensorFlow's
",copybara-service[bot],2024-07-15 18:03:04+00:00,['ddunl'],2024-07-16 19:04:44+00:00,2024-07-16 19:04:43+00:00,https://github.com/tensorflow/tensorflow/pull/71873,[],[],
2409296597,pull_request,closed,,Disable msan and tsan for nvjitlink_test.,"Disable msan and tsan for nvjitlink_test.
The test has been failing since its introduction.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-15 17:51:50+00:00,[],2024-07-15 20:31:39+00:00,2024-07-15 20:31:38+00:00,https://github.com/tensorflow/tensorflow/pull/71872,[],[],
2409291316,pull_request,closed,,[xla:gpu] NFC: Prepare for adding stateful custom calls support to GPU thunks,"[xla:gpu] NFC: Prepare for adding stateful custom calls support to GPU thunks

XLA FFI instantiation call might fail and we might fail to create a custom call thunk. Replace constructor with absl::StatusOr factory.
",copybara-service[bot],2024-07-15 17:48:35+00:00,['ezhulenev'],2024-07-15 20:54:50+00:00,2024-07-15 20:54:49+00:00,https://github.com/tensorflow/tensorflow/pull/71871,[],[],
2409283594,pull_request,open,,Internal only copybara config change,"Internal only copybara config change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-15 17:43:50+00:00,['ddunl'],2024-07-15 17:43:51+00:00,,https://github.com/tensorflow/tensorflow/pull/71870,[],[],
2409279296,pull_request,closed,,[xla:gpu] Synchronize device activity before initializing NCCL clique,"[xla:gpu] Synchronize device activity before initializing NCCL clique
",copybara-service[bot],2024-07-15 17:41:08+00:00,['ezhulenev'],2024-07-15 19:52:16+00:00,2024-07-15 19:52:15+00:00,https://github.com/tensorflow/tensorflow/pull/71869,[],[],
2409259302,pull_request,closed,,PR #14862: Add SPMD config option to specify zero cost method for gather/scatter.,"PR #14862: Add SPMD config option to specify zero cost method for gather/scatter.

Imported from GitHub PR https://github.com/openxla/xla/pull/14862

Issue #13304 

In SPMD handling of gather/scatter the partition strategy is hardcoded to IndexParallel strategy. This is not optimal for all topology. This PR makes this option an SPMD config, but defaults to IndexParallel to maintain existing behavior. 

Clang-format also fixed some formatting. Tests were added and all tests pass.
Copybara import of the project:

--
7f83c21573f24cd4e314b13ce2e349dd6194b451 by ptoulme-aws <ptoulme@amazon.com>:

Add SPMD config option to specify zero cost method for gather/scatter.

Merging this change closes #14862

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14862 from ptoulme-aws:gather_scatter_config 7f83c21573f24cd4e314b13ce2e349dd6194b451
",copybara-service[bot],2024-07-15 17:29:09+00:00,[],2024-11-29 08:35:45+00:00,2024-07-16 08:58:46+00:00,https://github.com/tensorflow/tensorflow/pull/71868,[],[],
2409220020,pull_request,closed,,[XLA:GPU] Make HandleElementwiseOp virtual in HloCostAnalysis.,"[XLA:GPU] Make HandleElementwiseOp virtual in HloCostAnalysis.

Currently there are two distinct functions: `HloCostAnalysis::HandleElementwiseOp` and `GpuHloCostAnalysis::GpuHloCostAnalysis`.

From the perspective of `GpuHloCostAnalysis`:
* `GpuHloCostAnalysis::HandleElementwiseOp` is called in
`HandleElementwiseUnary` and
`HandleElementwiseBinary`.
* `HloCostAnalysis::HandleElementwiseOp` is called in `HandleCompare`, `HandleClamp`, `HandleReducePrecision` and `HandleConvert`.

The logic in `HloCostAnalysis` doesn't really apply to GPU, so it's better to have one function in `GpuHloCostAnalysis`.

After this change, Cost Model FLOPs estimate for `compare`, `clamp`, `reduce-precision`, `select` and `convert` will change from 1 to 3 flops per element. This is unlikely to make any fusion differences, but now all elementwise ops will have consistent estimates.
",copybara-service[bot],2024-07-15 17:05:29+00:00,[],2024-07-16 10:00:59+00:00,2024-07-16 10:00:58+00:00,https://github.com/tensorflow/tensorflow/pull/71867,[],[],
2409217941,pull_request,closed,,[xla:gpu] Add support for stateful XLA FFI handlers,"[xla:gpu] Add support for stateful XLA FFI handlers
",copybara-service[bot],2024-07-15 17:04:37+00:00,['ezhulenev'],2024-07-15 21:33:24+00:00,2024-07-15 21:33:23+00:00,https://github.com/tensorflow/tensorflow/pull/71866,[],[],
2408985530,pull_request,closed,,PR #14859: [GPU] Re-add GPU device info test and fix the spec files.,"PR #14859: [GPU] Re-add GPU device info test and fix the spec files.

Imported from GitHub PR https://github.com/openxla/xla/pull/14859


Copybara import of the project:

--
7428e40409f867d63a0c5815abb934bdb2144afb by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Re-add GPU device info test and fix the spec files.

Merging this change closes #14859

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-15 15:11:30+00:00,[],2024-07-15 17:57:37+00:00,2024-07-15 17:57:37+00:00,https://github.com/tensorflow/tensorflow/pull/71865,[],[],
2408981297,pull_request,open,,Simplify x // c + x mod c.,"Simplify x // c + x mod c.

While not a very useful simplification, it is technically one.
This fixes our fuzz test that compares our simplifier to the
upstream one.
",copybara-service[bot],2024-07-15 15:09:41+00:00,[],2024-07-15 15:09:41+00:00,,https://github.com/tensorflow/tensorflow/pull/71864,[],[],
2408975397,pull_request,closed,,[XLA:GPU] Fix null dereference in the `TritonFusion` emitter.,"[XLA:GPU] Fix null dereference in the `TritonFusion` emitter.

Previously, the emitter would crash when the launch config was `std::nullopt`.

Testing the fix was very hard given the organization of the code, so we also
rework the design of `TritonFusion::Emit` and introduce a method
`TritonFusion::GenerateTritonKernelAndWrapper` that is more readily testable.
",copybara-service[bot],2024-07-15 15:07:04+00:00,[],2024-07-15 15:46:19+00:00,2024-07-15 15:46:18+00:00,https://github.com/tensorflow/tensorflow/pull/71863,[],[],
2408921507,pull_request,open,,PR #14862: Add SPMD config option to specify zero cost method for gather/scatter.,"PR #14862: Add SPMD config option to specify zero cost method for gather/scatter.

Imported from GitHub PR https://github.com/openxla/xla/pull/14862

Issue #13304 

In SPMD handling of gather/scatter the partition strategy is hardcoded to IndexParallel strategy. This is not optimal for all topology. This PR makes this option an SPMD config, but defaults to IndexParallel to maintain existing behavior. 

Clang-format also fixed some formatting. Tests were added and all tests pass.
Copybara import of the project:

--
7f83c21573f24cd4e314b13ce2e349dd6194b451 by ptoulme-aws <ptoulme@amazon.com>:

Add SPMD config option to specify zero cost method for gather/scatter.

Merging this change closes #14862

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14862 from ptoulme-aws:gather_scatter_config 7f83c21573f24cd4e314b13ce2e349dd6194b451
",copybara-service[bot],2024-07-15 14:43:14+00:00,[],2024-07-16 08:24:09+00:00,,https://github.com/tensorflow/tensorflow/pull/71862,[],[],
2408833494,pull_request,closed,,Move -sparse-local-load-to-llvm code together,"Move -sparse-local-load-to-llvm code together
",copybara-service[bot],2024-07-15 14:05:10+00:00,[],2024-07-16 12:14:38+00:00,2024-07-16 12:14:37+00:00,https://github.com/tensorflow/tensorflow/pull/71861,[],[],
2408816592,pull_request,open,,PR #14865: [NVIDIA GPU] Add debug flag for syntactic sugar,"PR #14865: [NVIDIA GPU] Add debug flag for syntactic sugar

Imported from GitHub PR https://github.com/openxla/xla/pull/14865

This is a followup PR of https://github.com/openxla/xla/pull/14344. Originally the issue was HLO dumping and NVTX marker naming are inconsistent, with https://github.com/openxla/xla/pull/14344 now both of them are wrapped by syntactic sugar. There are some cases, especially when debugging, the original naming without syntactic sugar is helpful. This PR adds a debug flag to control the syntactic sugar of both HLO dumping and NVTX marker.
Copybara import of the project:

--
28d0b4595e9db52d205cb219c2e25a9cc7a5c18c by Terry Sun <tesun@nvidia.com>:

debug flag for syntax sugar

--
0db7f704f41b8984fab65a420ac58fb6d55c2ce3 by Terry Sun <tesun@nvidia.com>:

proper default

--
c10a7cbf05956e3d0facb06554d457bfff82fe4b by Terry Sun <tesun@nvidia.com>:

same flag for nvtx marker

--
8dc14d17631bbacc3bc063158e5bebb605860947 by Terry Sun <tesun@nvidia.com>:

add comment for flag

--
4b1ba992c4e90d3b99044b0d15f0d61cdc08fa59 by Terry Sun <tesun@nvidia.com>:

formatting

Merging this change closes #14865

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14865 from terryysun:terryysun/syntax_sugar_debug_flag 4b1ba992c4e90d3b99044b0d15f0d61cdc08fa59
",copybara-service[bot],2024-07-15 13:57:24+00:00,[],2024-07-18 09:15:53+00:00,,https://github.com/tensorflow/tensorflow/pull/71860,[],[],
2408809667,pull_request,closed,,Fix size of unused variables.,"Fix size of unused variables.

They should be 1, not 0. Setting the size to 0 results in an empty domain.

Using 0 still works, but leads to the fuzzer generating unnecessarily
complex expressions, with expressions that are constant zeroes.
",copybara-service[bot],2024-07-15 13:54:33+00:00,[],2024-07-15 15:38:29+00:00,2024-07-15 15:38:28+00:00,https://github.com/tensorflow/tensorflow/pull/71859,[],[],
2408794303,pull_request,open,,PR #14859: [GPU] Re-add GPU device info test and fix the spec files.,"PR #14859: [GPU] Re-add GPU device info test and fix the spec files.

Imported from GitHub PR https://github.com/openxla/xla/pull/14859


Copybara import of the project:

--
7428e40409f867d63a0c5815abb934bdb2144afb by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Re-add GPU device info test and fix the spec files.

Merging this change closes #14859

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-15 13:47:43+00:00,[],2024-07-15 13:47:43+00:00,,https://github.com/tensorflow/tensorflow/pull/71858,[],[],
2408786791,pull_request,open,,PR #14881: [GPU] Remove DRAM size from model_str in device description.,"PR #14881: [GPU] Remove DRAM size from model_str in device description.

Imported from GitHub PR https://github.com/openxla/xla/pull/14881

Exact DRAM size can slightly vary for the same GPU model so it shouldn't be used in model identification. The removed part of gpu_compiler_test_autotune_db.textproto illustrates that.
Copybara import of the project:

--
8ecbaf3fafade5ae7fd51ef90138fbc0e0666de3 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Remove DRAM size from model_str in device description.

Exact DRAM size can slightly vary for the same GPU model so it shouldn't
be used in model identification. The removed part of
gpu_compiler_test_autotune_db.textproto illustrates that.

Merging this change closes #14881

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14881 from openxla:remove_dram_size 8ecbaf3fafade5ae7fd51ef90138fbc0e0666de3
",copybara-service[bot],2024-07-15 13:44:16+00:00,[],2024-07-15 13:44:16+00:00,,https://github.com/tensorflow/tensorflow/pull/71857,[],[],
2408786727,pull_request,closed,,[XLA:GPU] Test passes from SparseConvertToLLVM separately,"[XLA:GPU] Test passes from SparseConvertToLLVM separately
",copybara-service[bot],2024-07-15 13:44:14+00:00,[],2024-07-16 14:35:10+00:00,2024-07-16 14:35:09+00:00,https://github.com/tensorflow/tensorflow/pull/71856,[],[],
2408758230,pull_request,closed,,[XLA:GPU][NFC] Simplify `TritonFusion` emitter tests.,"[XLA:GPU][NFC] Simplify `TritonFusion` emitter tests.

It seems like the HLO could be made more concise by holding only a single
Triton computation.

Also added explicit type annotations where the type was not obvious to me
without hovering over the `auto` keyword.
",copybara-service[bot],2024-07-15 13:32:04+00:00,[],2024-07-15 15:03:29+00:00,2024-07-15 15:03:29+00:00,https://github.com/tensorflow/tensorflow/pull/71855,[],[],
2408599425,pull_request,closed,,Enable more mlir emitters by default.,"Enable more mlir emitters by default.

With xla_gpu_mlir_emitter_level=2, we enable all emitters except transpose and
reduce.
Change some tests that expect certain IR to be emitted to still use the old
emitters. We have separate IR tests for the new emitters, and keeping the old
tests running with the old emitters ensures we still have coverage for the old
emitters, in case we need to rollback.
",copybara-service[bot],2024-07-15 12:19:21+00:00,['akuegel'],2024-07-17 09:54:08+00:00,2024-07-17 09:54:07+00:00,https://github.com/tensorflow/tensorflow/pull/71854,[],[],
2408588239,pull_request,closed,,Don't match branch labels in test expectation.,"Don't match branch labels in test expectation.

The branch labels are not meaningful. Instead, we match that there are 2 stores
in the emitted IR.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14899 from jaro-sevcik:add-tf-prefix-assert-ok-tests a0876532334cfa2a0f2a5d88b426178bd3b0fafb
",copybara-service[bot],2024-07-15 12:13:53+00:00,['akuegel'],2024-07-15 13:05:14+00:00,2024-07-15 13:05:12+00:00,https://github.com/tensorflow/tensorflow/pull/71853,[],[],
2408560269,pull_request,closed,,Move handling of TF_CUDNN_DETERMINISTIC from XLA to TF.,"The environment variable is TF-specific; controlling the behavior of determinism in XLA through the numeric options argument is more explicit.

Related XLA PR after this is committed will be https://github.com/openxla/xla/pull/14310.
",sergachev,2024-07-15 12:00:25+00:00,['gbaned'],2024-07-23 09:51:28+00:00,2024-07-23 02:29:38+00:00,https://github.com/tensorflow/tensorflow/pull/71852,"[('ready to pull', 'PR ready for merge process'), ('size:M', 'CL Change Size: Medium'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2244152653, 'issue_id': 2408560269, 'author': 'reedwm', 'body': ""When merging, I removed `numeric_options_utils.cc` and instead moved its function definition into `numeric_options_utils.h`, marking the function as inline. The reason I did this is: some very-complicated internal build rule was dynamically creating its `srcs` and `hdrs` based on what files were present, and somehow `numeric_options_utils.cc` was included and not `numeric_options_utils.h`, causing errors. I couldn't figure out why this was happening, so instead I just got rid of the .cc file."", 'created_at': datetime.datetime(2024, 7, 23, 2, 53, 55, tzinfo=datetime.timezone.utc)}]","reedwm on (2024-07-23 02:53:55 UTC): When merging, I removed `numeric_options_utils.cc` and instead moved its function definition into `numeric_options_utils.h`, marking the function as inline. The reason I did this is: some very-complicated internal build rule was dynamically creating its `srcs` and `hdrs` based on what files were present, and somehow `numeric_options_utils.cc` was included and not `numeric_options_utils.h`, causing errors. I couldn't figure out why this was happening, so instead I just got rid of the .cc file.

"
2408534231,pull_request,closed,,Integrate Triton up to go/triton-commits/95623038c75463286aa5d4a44782ba7492cc1afa,"Integrate Triton up to go/triton-commits/95623038c75463286aa5d4a44782ba7492cc1afa
",copybara-service[bot],2024-07-15 11:47:18+00:00,['gflegar'],2024-07-17 09:46:34+00:00,2024-07-17 09:46:33+00:00,https://github.com/tensorflow/tensorflow/pull/71851,[],[],
2408451359,pull_request,closed,,Remove conditional dependency from ptx_compiler_impl,"Remove conditional dependency from ptx_compiler_impl
",copybara-service[bot],2024-07-15 11:00:52+00:00,[],2024-07-16 16:06:08+00:00,2024-07-16 16:06:07+00:00,https://github.com/tensorflow/tensorflow/pull/71850,[],[],
2408448915,pull_request,closed,,PR #14899: Add TF_ prefix to ASSERT_OK in tests,"PR #14899: Add TF_ prefix to ASSERT_OK in tests

Imported from GitHub PR https://github.com/openxla/xla/pull/14899

Fixes OSS build breakage of `se_gpu_pjrt_client_test.cc`.
Copybara import of the project:

--
a0876532334cfa2a0f2a5d88b426178bd3b0fafb by Jaroslav Sevcik <jsevcik@nvidia.com>:

Add TF_ prefix to ASSERT_OK in tests

Merging this change closes #14899

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14899 from jaro-sevcik:add-tf-prefix-assert-ok-tests a0876532334cfa2a0f2a5d88b426178bd3b0fafb
",copybara-service[bot],2024-07-15 10:59:33+00:00,[],2024-07-15 12:36:22+00:00,2024-07-15 12:36:22+00:00,https://github.com/tensorflow/tensorflow/pull/71849,[],[],
2408418966,pull_request,closed,,[XLA:GPU] Make `FileCheck` test util log its input before its `stderr` output.,"[XLA:GPU] Make `FileCheck` test util log its input before its `stderr` output.

This makes it more convenient to track down errors when updating `FileCheck`
tests since it avoids having to scroll too far up.

Also drive-by clean includes.
",copybara-service[bot],2024-07-15 10:43:10+00:00,[],2024-07-15 11:16:25+00:00,2024-07-15 11:16:24+00:00,https://github.com/tensorflow/tensorflow/pull/71848,[],[],
2408386720,pull_request,closed,,PR #14605: [ROCm] Switch on Triton feature for ROCm.,"PR #14605: [ROCm] Switch on Triton feature for ROCm.

Imported from GitHub PR https://github.com/openxla/xla/pull/14605

Last in series of commits to switch on Triton in XLA for ROCm.

This is new version of:
https://github.com/openxla/xla/pull/13003

Changes in third_party/triton/temporary/amd_pr7.patch are already merged on:
https://github.com/triton-lang/triton/pull/4238
Copybara import of the project:

--
c2ce7e08180225a8e1433b013ded16ebabf25615 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Switch on Triton feature for ROCm.

--
563b3036893fccbd72f11ae6a80e110c516bd0ac by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed an issue with test cases from ir_emitter_triton_test.cc

--
a4d2ad85f251b37f75d8b4f80aeba075932e6777 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed an issue with gpu_compiler_test.cc

--
a1b9260a0e54e74ab5dd363e8b755f3534634064 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Applied comments from code review.

--
c694a95420b2bb73871389b5d3dd53c97e3e50da by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed failed tests because of https://github.com/openxla/xla/commit/19c11baa83f31e25a3f841cf41fa47a53e8ca161

--
7359619ba75d388817c07f30c0ee1094a1c11184 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed compilation issue with latest rebase.

--
82f58ce245c44065df2983c3f6585c3b8c536914 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Skip SplitLHSInputOutputIsFused test in ir_emitter_triton_test.cc untill issue is fixed.

--
57e776be2336e5a99f587390e35705dbf59277c8 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Triton related changes merged thus removed amd_pr7.patch

--
0d09d0e283d0210cac7f368d550a5e0e7e2d3a8b by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Applied comments from code review.

--
7b111478768dc267ec1eed8c3555ed1d1070791f by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Applied comments from code review.

--
9e7e0c7c05185fc476383b654744e4b2e5079da1 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Modified TestNoAutotuner test case.

Merging this change closes #14605

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14605 from ROCm:rocm_triton_backend_8 9e7e0c7c05185fc476383b654744e4b2e5079da1
",copybara-service[bot],2024-07-15 10:25:08+00:00,[],2024-07-15 13:12:48+00:00,2024-07-15 13:12:47+00:00,https://github.com/tensorflow/tensorflow/pull/71847,[],[],
2408331473,pull_request,closed,,[xla:ffi] Make ResultEncoding execution stage dependent,"[xla:ffi] Make ResultEncoding execution stage dependent
",copybara-service[bot],2024-07-15 09:56:30+00:00,['ezhulenev'],2024-07-15 17:49:45+00:00,2024-07-15 17:49:44+00:00,https://github.com/tensorflow/tensorflow/pull/71846,[],[],
2408326504,pull_request,closed,,Fix removal of constraints using dimensions in RemoveUnusedSymbols.,"Fix removal of constraints using dimensions in RemoveUnusedSymbols.

Removing a constraint on a dimension is invalid, even if the
dimension is unused in the affine map. The constraint still
restricts the domain of the map.

In some cases we could use the constraint to tighten the bounds
of the unused dimension instead, e.g. if both d0 and s0 are unused
in:

```
d0 in [0, 100]
s0 in [2, 3]
d0 + s0 in [2, 3]
```

We could change the bound of d0 to [0, 1]. But this is not always possible and
requires some extra work.
",copybara-service[bot],2024-07-15 09:53:54+00:00,[],2024-07-15 11:41:31+00:00,2024-07-15 11:41:31+00:00,https://github.com/tensorflow/tensorflow/pull/71845,[],[],
2408169544,pull_request,closed,,Use constraints to tighten expression ranges.,"Use constraints to tighten expression ranges.

Also remove the constraint cache, since it gets in the way of this and
is not helpful.

This is very simplistic for now. If we have a sum (a + b + c), and a
constraint on `a + c`, we may not see that it can be used.
",copybara-service[bot],2024-07-15 08:34:15+00:00,[],2024-07-15 13:40:38+00:00,2024-07-15 13:40:36+00:00,https://github.com/tensorflow/tensorflow/pull/71844,[],[],
2408074162,pull_request,closed,,Add comments to include paths.,"Add comments to include paths.

This is needed for path transformations.
",copybara-service[bot],2024-07-15 07:40:12+00:00,['akuegel'],2024-07-15 08:17:00+00:00,2024-07-15 08:16:59+00:00,https://github.com/tensorflow/tensorflow/pull/71843,[],[],
2408051990,pull_request,closed,,Do not use --version-script on MacOS.,"Do not use --version-script on MacOS.

It is not supported.
",copybara-service[bot],2024-07-15 07:27:08+00:00,['akuegel'],2024-07-15 10:56:28+00:00,2024-07-15 10:56:27+00:00,https://github.com/tensorflow/tensorflow/pull/71842,[],[],
2408046033,pull_request,closed,,bump Shardy commit hash,"bump Shardy commit hash
",copybara-service[bot],2024-07-15 07:23:36+00:00,[],2024-07-15 12:57:36+00:00,2024-07-15 12:57:34+00:00,https://github.com/tensorflow/tensorflow/pull/71841,[],[],
2408006736,pull_request,closed,,Fix call to std::max with conflicting types.,"Fix call to std::max with conflicting types.

Currently this results in a compile error on MacOS:

error: no matching function for call to 'max'
        intervals.push_back({std::max(0l, a - 1), std::max(0l, b - 1)})
",copybara-service[bot],2024-07-15 07:00:23+00:00,['akuegel'],2024-07-15 11:22:47+00:00,2024-07-15 11:22:46+00:00,https://github.com/tensorflow/tensorflow/pull/71840,[],[],
2407990302,pull_request,closed,,Fix call to std::max with conflicting types.,"Fix call to std::max with conflicting types.

Currently this results in a compile error on MacOS:

error: no matching function for call to 'max'
        intervals.push_back({std::max(0l, a - 1), std::max(0l, b - 1)})
",copybara-service[bot],2024-07-15 06:50:47+00:00,['akuegel'],2024-07-15 11:48:07+00:00,2024-07-15 11:48:06+00:00,https://github.com/tensorflow/tensorflow/pull/71839,[],[],
2407862266,pull_request,closed,,Replace auto assignment list in bot_config.yml,Replace auto assignment list,tilakrayal,2024-07-15 05:01:03+00:00,['gbaned'],2024-07-16 13:15:37+00:00,2024-07-16 13:15:36+00:00,https://github.com/tensorflow/tensorflow/pull/71837,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]",[],
2407641923,pull_request,closed,,Fix the deps error,"Fix the deps error
",copybara-service[bot],2024-07-14 23:36:53+00:00,['yijie-yang'],2024-07-16 19:44:15+00:00,2024-07-16 19:44:14+00:00,https://github.com/tensorflow/tensorflow/pull/71834,[],[],
2407555919,pull_request,closed,,Circleci project setup,Pull Request ,SsomsakTH,2024-07-14 18:52:59+00:00,['gbaned'],2024-07-14 22:08:59+00:00,2024-07-14 22:08:58+00:00,https://github.com/tensorflow/tensorflow/pull/71831,"[('size:S', 'CL Change Size: Small')]",[],
2407533361,pull_request,closed,,[xla:ffi] Add support for stateful FFI handlers,"[xla:ffi] Add support for stateful FFI handlers
",copybara-service[bot],2024-07-14 17:52:30+00:00,['ezhulenev'],2024-07-15 20:49:55+00:00,2024-07-15 20:49:55+00:00,https://github.com/tensorflow/tensorflow/pull/71830,[],[],
2407267070,pull_request,open,,[XLA:MSA] Use async-slice for slice produces in memory space assignment.. This is to avoid redundant slice operation.,"[XLA:MSA] Use async-slice for slice produces in memory space assignment.. This is to avoid redundant slice operation.
",copybara-service[bot],2024-07-14 04:57:25+00:00,[],2024-07-14 04:57:25+00:00,,https://github.com/tensorflow/tensorflow/pull/71827,[],[],
2407252584,pull_request,closed,,[XLA] Generalize CalculateFloatDistance for all float types,"[XLA] Generalize CalculateFloatDistance for all float types

No functional change is intended.
",copybara-service[bot],2024-07-14 03:48:34+00:00,['majnemer'],2024-07-15 22:47:00+00:00,2024-07-15 22:46:59+00:00,https://github.com/tensorflow/tensorflow/pull/71826,[],[],
2407234673,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:21:00+00:00,[],2024-07-14 02:21:00+00:00,,https://github.com/tensorflow/tensorflow/pull/71825,[],[],
2407234487,pull_request,closed,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14891 from shawnwang18:shawnw/add_access_permission_across_cuda_async_memory_pool 6e78412751586fd5d0b0c1faa0dff1c1bf4579b6
",copybara-service[bot],2024-07-14 02:20:06+00:00,[],2024-07-16 12:06:59+00:00,2024-07-16 12:06:58+00:00,https://github.com/tensorflow/tensorflow/pull/71824,[],[],
2407234333,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:19:16+00:00,[],2024-07-14 02:19:16+00:00,,https://github.com/tensorflow/tensorflow/pull/71823,[],[],
2407234271,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:18:58+00:00,[],2024-07-14 02:18:58+00:00,,https://github.com/tensorflow/tensorflow/pull/71822,[],[],
2407233967,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:17:34+00:00,[],2024-07-14 02:17:34+00:00,,https://github.com/tensorflow/tensorflow/pull/71821,[],[],
2407233787,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:16:45+00:00,[],2024-07-14 02:16:45+00:00,,https://github.com/tensorflow/tensorflow/pull/71820,[],[],
2407233664,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:16:08+00:00,[],2024-07-14 02:16:08+00:00,,https://github.com/tensorflow/tensorflow/pull/71819,[],[],
2407233447,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:15:04+00:00,[],2024-07-18 03:46:34+00:00,2024-07-18 03:46:34+00:00,https://github.com/tensorflow/tensorflow/pull/71818,[],[],
2407232850,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:12:14+00:00,[],2024-07-14 02:12:14+00:00,,https://github.com/tensorflow/tensorflow/pull/71817,[],[],
2407232578,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:11:01+00:00,[],2024-07-14 02:11:01+00:00,,https://github.com/tensorflow/tensorflow/pull/71816,[],[],
2407232540,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:10:50+00:00,[],2024-07-17 07:48:53+00:00,,https://github.com/tensorflow/tensorflow/pull/71815,[],[],
2407232106,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:09:11+00:00,[],2024-07-14 04:39:02+00:00,2024-07-14 04:39:01+00:00,https://github.com/tensorflow/tensorflow/pull/71814,[],[],
2407231679,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:07:27+00:00,[],2024-07-14 02:07:27+00:00,,https://github.com/tensorflow/tensorflow/pull/71813,[],[],
2407230697,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-14 02:03:06+00:00,[],2024-07-14 02:03:06+00:00,,https://github.com/tensorflow/tensorflow/pull/71812,[],[],
2407106293,pull_request,closed,,[xla:ffi] Add ErrorOr container for returning value or error,"[xla:ffi] Add ErrorOr container for returning value or error

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-13 19:22:14+00:00,['ezhulenev'],2024-07-15 20:23:54+00:00,2024-07-15 20:23:54+00:00,https://github.com/tensorflow/tensorflow/pull/71811,[],[],
2406841375,pull_request,open,,compat: Update forward compatibility horizon to 2024-07-13,"compat: Update forward compatibility horizon to 2024-07-13
",copybara-service[bot],2024-07-13 10:40:55+00:00,[],2024-07-13 10:40:55+00:00,,https://github.com/tensorflow/tensorflow/pull/71810,[],[],
2406812734,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 09:10:46+00:00,[],2024-07-17 04:38:34+00:00,,https://github.com/tensorflow/tensorflow/pull/71808,[],[],
2406808667,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:59:17+00:00,[],2024-07-13 09:28:34+00:00,,https://github.com/tensorflow/tensorflow/pull/71807,[],[],
2406808349,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:58:23+00:00,[],2024-07-13 08:58:23+00:00,,https://github.com/tensorflow/tensorflow/pull/71806,[],[],
2406804855,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:47:59+00:00,[],2024-07-13 20:52:55+00:00,2024-07-13 20:52:54+00:00,https://github.com/tensorflow/tensorflow/pull/71805,[],[],
2406802777,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:41:40+00:00,[],2024-07-18 04:43:35+00:00,2024-07-18 04:43:34+00:00,https://github.com/tensorflow/tensorflow/pull/71804,[],[],
2406802664,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:41:18+00:00,[],2024-07-13 08:41:18+00:00,,https://github.com/tensorflow/tensorflow/pull/71803,[],[],
2406802543,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:40:58+00:00,[],2024-07-13 08:40:58+00:00,,https://github.com/tensorflow/tensorflow/pull/71802,[],[],
2406802136,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:39:59+00:00,[],2024-07-17 09:13:19+00:00,2024-07-17 09:13:17+00:00,https://github.com/tensorflow/tensorflow/pull/71801,[],[],
2406801595,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:38:13+00:00,[],2024-07-13 08:38:13+00:00,,https://github.com/tensorflow/tensorflow/pull/71800,[],[],
2406801558,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:38:07+00:00,[],2024-07-13 12:05:02+00:00,,https://github.com/tensorflow/tensorflow/pull/71799,[],[],
2406801476,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:37:50+00:00,[],2024-07-17 05:08:30+00:00,,https://github.com/tensorflow/tensorflow/pull/71798,[],[],
2406801428,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:37:42+00:00,[],2024-07-13 08:37:42+00:00,,https://github.com/tensorflow/tensorflow/pull/71797,[],[],
2406799350,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:31:33+00:00,[],2024-07-16 07:11:23+00:00,2024-07-16 07:11:21+00:00,https://github.com/tensorflow/tensorflow/pull/71796,[],[],
2406798965,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:30:31+00:00,[],2024-07-16 07:04:27+00:00,2024-07-16 07:04:26+00:00,https://github.com/tensorflow/tensorflow/pull/71795,[],[],
2406798747,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-13 08:30:01+00:00,[],2024-07-13 08:30:01+00:00,,https://github.com/tensorflow/tensorflow/pull/71794,[],[],
2406692307,pull_request,closed,,Force semicolon to be placed after XLA_VLOG_LINES,"Force semicolon to be placed after XLA_VLOG_LINES

The macro definition previously had the `;`, which allowed users to exlcude it when using. I don't think this is the norm for C preprocessor macros. In addition, `XLA_LOG_LINES` requires it, so switching between the two would require it to be there anyway.
",copybara-service[bot],2024-07-13 04:18:13+00:00,[],2024-07-13 12:24:16+00:00,2024-07-13 12:24:15+00:00,https://github.com/tensorflow/tensorflow/pull/71793,[],[],
2406687147,pull_request,closed,,[xla:ffi] Extract TypeIdRegistry and add ExecutionState,"[xla:ffi] Extract TypeIdRegistry and add ExecutionState

Extract TypeId registration into a separate library as TypeId will be shared by execution context and execution state.

Similar to ExecutionContext implement a type-erased RAII wrapper for execution state.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14859 from openxla:device_info_test 7428e40409f867d63a0c5815abb934bdb2144afb
",copybara-service[bot],2024-07-13 04:01:11+00:00,['ezhulenev'],2024-07-15 19:20:16+00:00,2024-07-15 19:20:15+00:00,https://github.com/tensorflow/tensorflow/pull/71792,[],[],
2406562993,pull_request,closed,,Replace GetLevelForDuration with GetLevelBoundsForDuration and expose LayerResolutionPs.,"Replace GetLevelForDuration with GetLevelBoundsForDuration and expose LayerResolutionPs.
",copybara-service[bot],2024-07-13 01:03:17+00:00,[],2024-07-16 00:09:55+00:00,2024-07-16 00:09:54+00:00,https://github.com/tensorflow/tensorflow/pull/71790,[],[],
2406544338,pull_request,closed,,[XLA::SPMD] Propagating shardings between operands of element-wise operations.,"[XLA::SPMD] Propagating shardings between operands of element-wise operations.

Let us use `C = foo(A, B)` as an example. We have two key steps in sharding propagation.
1. Forward propagation. Infer C's sharding from A and B.
2. Backward propagation. Infer A's or B's sharding from C.

Ideally, we do not need to explicitly propagate shardings between operands since we have the paths `A -> C -> B` and `B -> C -> A`. However, this path may be unavailable if C has a pre-defined sharding, which impedes the forward propagation. To resolve this issue, we may add the propagation `A -> B` and `B -> A` directly.

It was already considered for several operations, such as dot and reduce. This cl support it for element-wise operations.
",copybara-service[bot],2024-07-13 00:50:54+00:00,[],2024-07-16 03:17:59+00:00,2024-07-16 03:17:59+00:00,https://github.com/tensorflow/tensorflow/pull/71789,[],[],
2406510608,pull_request,closed,,Add more patterns to unstacking pass.,"Add more patterns to unstacking pass.

Added the following patterns to the unstacking pass:
1. GetReduceFusionPattern:
fusion(stacked, loop_iteration_var)
computation {
   p0 = parameter(0)
   p1 = parameter(1)
   slice = dynamic_slice(p0, p1, zero, ...)
   ROOT reduce = reduce(slice, constant)
}
2. GetDUSFusionWithPadPattern:
fusion(stacked, update, loop_iteration_var)
computation {
   p0 = parameter(0)
   p1 = parameter(1)
   p2 = parameter(2)
   pad = pad(p1, ...)
   update = bitcast(pad)
   ROOT dus = dynamic_update_slice(p0, update, p2, zero, ...)
}
",copybara-service[bot],2024-07-13 00:12:28+00:00,[],2024-07-13 03:22:35+00:00,2024-07-13 03:22:35+00:00,https://github.com/tensorflow/tensorflow/pull/71788,[],[],
2406508497,pull_request,open,,[XLA:MSA] Implement an auxiliary function to simulate the bandwidth sharing between memory requests.,"[XLA:MSA] Implement an auxiliary function to simulate the bandwidth sharing between memory requests.

We implement an auxiliary function to simulate the process of draining memory request queues. There are two queues which track the read/write-default memory requests. When both queues are not empty, they need to share the bandwidth to process the front requests.
",copybara-service[bot],2024-07-13 00:08:07+00:00,[],2024-07-13 00:08:07+00:00,,https://github.com/tensorflow/tensorflow/pull/71787,[],[],
2406487211,pull_request,closed,,Use consistent include paths for mlir includes,"Use consistent include paths for mlir includes
",copybara-service[bot],2024-07-12 23:33:46+00:00,['ddunl'],2024-07-13 03:33:48+00:00,2024-07-13 03:33:47+00:00,https://github.com/tensorflow/tensorflow/pull/71786,[],[],
2406479022,pull_request,closed,,[XLA:SPMD] Add a `MergeShardingIfCompatible` overload that doesn't require minimum_tiles.,"[XLA:SPMD] Add a `MergeShardingIfCompatible` overload that doesn't require minimum_tiles.

This is a small cleanup that makes the code more concise. There is no behavior change.
",copybara-service[bot],2024-07-12 23:16:59+00:00,[],2024-07-13 03:17:04+00:00,2024-07-13 03:17:03+00:00,https://github.com/tensorflow/tensorflow/pull/71785,[],[],
2406474545,pull_request,closed,,[XLA:SPMD] Allow common sharding candidates from shard_as group to be empty as sharding-aware CSE in sharding propagation.,"[XLA:SPMD] Allow common sharding candidates from shard_as group to be empty as sharding-aware CSE in sharding propagation.
",copybara-service[bot],2024-07-12 23:09:30+00:00,['Tongfei-Guo'],2024-07-13 03:11:02+00:00,2024-07-13 03:11:02+00:00,https://github.com/tensorflow/tensorflow/pull/71784,[],[],
2406396890,pull_request,closed,,bump Shardy commit hash,"bump Shardy commit hash
",copybara-service[bot],2024-07-12 21:54:14+00:00,[],2024-07-13 00:41:33+00:00,2024-07-13 00:41:32+00:00,https://github.com/tensorflow/tensorflow/pull/71783,[],[],
2406371750,pull_request,open,,Fix recent breakage in fft_single_threaded_test due to include & dependency refactoring.,"Fix recent breakage in fft_single_threaded_test due to include & dependency refactoring.

Standardize all tests on using xla_internal_test_main so all relevant tsl::Flags are registered and not ignored by gunit_main.
",copybara-service[bot],2024-07-12 21:28:37+00:00,[],2024-07-15 16:31:58+00:00,,https://github.com/tensorflow/tensorflow/pull/71782,[],[],
2406371231,pull_request,closed,,"[XLA] Add support for dot(dot(a, b), c) to dot(a, dot(b, c)) reordering where dimensions of a and b are batched in the inner dot, and the corresponding dimensions are batched with dimensions of c in the outer dot.","[XLA] Add support for dot(dot(a, b), c) to dot(a, dot(b, c)) reordering where dimensions of a and b are batched in the inner dot, and the corresponding dimensions are batched with dimensions of c in the outer dot.
",copybara-service[bot],2024-07-12 21:28:03+00:00,[],2024-08-10 00:50:17+00:00,2024-08-10 00:50:17+00:00,https://github.com/tensorflow/tensorflow/pull/71781,[],[],
2406370779,pull_request,closed,,Clean up includes in stream_executor:all,"Clean up includes in stream_executor:all
",copybara-service[bot],2024-07-12 21:27:34+00:00,[],2024-07-16 20:09:16+00:00,2024-07-16 20:09:16+00:00,https://github.com/tensorflow/tensorflow/pull/71780,[],[],
2406359586,pull_request,closed,,Undo test breakage,"Undo test breakage
",copybara-service[bot],2024-07-12 21:17:44+00:00,[],2024-07-12 23:28:43+00:00,2024-07-12 23:28:42+00:00,https://github.com/tensorflow/tensorflow/pull/71779,[],[],
2406329607,pull_request,open,,[XLA:MSA] Implement an auxiliary function to simulate the async memory request overhead.,"[XLA:MSA] Implement an auxiliary function to simulate the async memory request overhead.

The off-the-shelf runtime predictor does not include the overhead of asynchronous copies. We need to simulate the memory system to get the overhead of these copies. We implement an auxiliary function is this patch to simulate the bandwidth sharing model. This function can be further used to support end2end simulator that involves async copies.
",copybara-service[bot],2024-07-12 20:50:16+00:00,[],2024-07-12 20:50:16+00:00,,https://github.com/tensorflow/tensorflow/pull/71778,[],[],
2406326738,pull_request,closed,,#tf-data-service Add tests for alt data transfer failure modes.,"#tf-data-service Add tests for alt data transfer failure modes.
",copybara-service[bot],2024-07-12 20:47:30+00:00,['mpcallanan'],2024-12-16 21:45:19+00:00,2024-12-16 21:45:18+00:00,https://github.com/tensorflow/tensorflow/pull/71777,[],[],
2406309197,pull_request,closed,,Internal visibility change,"Internal visibility change
",copybara-service[bot],2024-07-12 20:31:34+00:00,[],2024-07-16 21:21:10+00:00,2024-07-16 21:21:09+00:00,https://github.com/tensorflow/tensorflow/pull/71776,[],[],
2406305227,pull_request,closed,,Use `xla::ifrt::AttributeMap` for cost analysis results,"Use `xla::ifrt::AttributeMap` for cost analysis results

This is for consistency with other attribute maps in IFRT. Also, using `AttributeMap` makes it possible to provide stable serialization, e.g., for IFRT Proxy.

JAX Python binding still uses `xla::PjRtValueType` since pybind/nanobind doesn't recognize `xla::ifrt::AttributeMap::Value` as Python native types.
",copybara-service[bot],2024-07-12 20:27:59+00:00,[],2024-07-13 03:28:07+00:00,2024-07-13 03:28:06+00:00,https://github.com/tensorflow/tensorflow/pull/71775,[],[],
2406262944,pull_request,closed,,"Refactor HostOffloadLegalize by moving the searching for ""starting instructions"" into a separate function.","Refactor HostOffloadLegalize by moving the searching for ""starting instructions"" into a separate function.
",copybara-service[bot],2024-07-12 20:05:08+00:00,['SandSnip3r'],2024-07-12 23:23:13+00:00,2024-07-12 23:23:12+00:00,https://github.com/tensorflow/tensorflow/pull/71774,[],[],
2406261719,pull_request,closed,,"Rename `xla::ifrt::{From,To}PjRtDeviceAttributeMap` to `xla::ifrt::{From,To}PjRtAttributeMap`","Rename `xla::ifrt::{From,To}PjRtDeviceAttributeMap` to `xla::ifrt::{From,To}PjRtAttributeMap`

This is in preparation for using `xla::ifrt::AttributeMap` for `xla::ifrt::Executable`'s cost analysis map.
",copybara-service[bot],2024-07-12 20:04:32+00:00,[],2024-07-12 23:17:31+00:00,2024-07-12 23:17:31+00:00,https://github.com/tensorflow/tensorflow/pull/71773,[],[],
2406236949,pull_request,closed,,Refactor HostOffloadLegalize to minimize use of `auto`.,"Refactor HostOffloadLegalize to minimize use of `auto`.
",copybara-service[bot],2024-07-12 19:51:42+00:00,['SandSnip3r'],2024-07-12 22:37:39+00:00,2024-07-12 22:37:39+00:00,https://github.com/tensorflow/tensorflow/pull/71772,[],[],
2406235296,pull_request,closed,,"Add support for re-layouting mhlo.convolution to n{}c, o{}i input layouts in prepare phase.","Add support for re-layouting mhlo.convolution to n{}c, o{}i input layouts in prepare phase.
",copybara-service[bot],2024-07-12 19:50:14+00:00,['LukeBoyer'],2024-07-16 22:06:36+00:00,2024-07-16 22:06:34+00:00,https://github.com/tensorflow/tensorflow/pull/71771,[],[],
2406233982,pull_request,closed,,Add tests with CHECK-NOT for incoming conv relayout patterns,"Add tests with CHECK-NOT for incoming conv relayout patterns
",copybara-service[bot],2024-07-12 19:49:03+00:00,['LukeBoyer'],2024-07-16 19:37:17+00:00,2024-07-16 19:37:16+00:00,https://github.com/tensorflow/tensorflow/pull/71770,[],[],
2406228155,pull_request,closed,,Refactor HostOffloadLegalize to use a struct with named members rather than an std::pair.,"Refactor HostOffloadLegalize to use a struct with named members rather than an std::pair.
",copybara-service[bot],2024-07-12 19:44:07+00:00,['SandSnip3r'],2024-07-12 21:18:37+00:00,2024-07-12 21:18:36+00:00,https://github.com/tensorflow/tensorflow/pull/71769,[],[],
2406224226,pull_request,closed,,Fix a bug where two replicated `xla::ifrt::HloSharding`s with different numbers of devices were considered having the same partitioning,"Fix a bug where two replicated `xla::ifrt::HloSharding`s with different numbers of devices were considered having the same partitioning

`xla::HloSharding` does not encode the number of devices for replicated sharding, so we must explicitly compare the device count. This new semantics is consistent with other implementations, e.g., `ConcreteEvenSharding` also compares the number of devices in addition to the global/shard shapes.
",copybara-service[bot],2024-07-12 19:40:43+00:00,[],2024-07-12 21:11:26+00:00,2024-07-12 21:11:25+00:00,https://github.com/tensorflow/tensorflow/pull/71768,[],[],
2406208013,pull_request,open,,[XLA:SPMD] Optimize the partitioning for element-wise operations when all operands share the same sharding.,"[XLA:SPMD] Optimize the partitioning for element-wise operations when all operands share the same sharding.

Let us take `C with S3 = add(A with S1, B with S2)` as an example, where `A, B, C` are tensors, `S1, S2, S3` are their shardings.

Before this change, we always have
```
A with S3 = reshard(A with S1, new_sharding=S3)
B with S3 = reshard(B with S2, new_sharding=S3)
C with S3 = add(A with S3, B with S3)
```

With this cl, if S1 and S2 are the same, we will have
```
C with S1 = add(A with S1, B with S1)
C with S3 = reshard(C with S1)
```
The new partitioning method can reduce the number of reshards.
",copybara-service[bot],2024-07-12 19:26:57+00:00,['ddunl'],2024-07-12 20:47:24+00:00,,https://github.com/tensorflow/tensorflow/pull/71767,[],[],
2406182847,pull_request,open,,[NFC] debug_options: Make all singular scalar fields explicitly optional.,"[NFC] debug_options: Make all singular scalar fields explicitly optional.

By default in proto3, there is no way to tell the difference between when a
non-repeated (singular) non-message (scalar) field that is not set and when
that field has been explicitly set to its default value. Normally this is not a
problem for DebugOptions, as XLA_FLAGS simply translates default values as
expected. However, when replaying compilation of modules with saved
HloModuleConfigs, user XLA_FLAGS values should always take precedence, which
means we need to know the difference. Concrete use cases include setting
xla_dump_to to """", which is the supported way to disable dumping, as the saved
compilation may have set it to an arbitrary directory and thus replaying to it
would risk permissions errors or overwriting user data, and disabling GPU
autotuning by setting --xla_gpu_autotune_level to 0.

This property is enforced by a unit test. In an ideal world only fields that
directly correspond to XLA_FLAGS would have the enforcement, but tsl::Flag
doesn't expose accessors so we simply enforce it for all fields.
",copybara-service[bot],2024-07-12 19:06:35+00:00,[],2024-07-12 23:25:47+00:00,,https://github.com/tensorflow/tensorflow/pull/71766,[],[],
2406171830,pull_request,closed,,[xla:ffi] Add ExecutionStage to FFI handler and check it at run time,"[xla:ffi] Add ExecutionStage to FFI handler and check it at run time

In preparation for implementing ""stateful custom calls"" add execution stage to FFI handler.
",copybara-service[bot],2024-07-12 18:58:08+00:00,['ezhulenev'],2024-07-12 22:09:38+00:00,2024-07-12 22:09:37+00:00,https://github.com/tensorflow/tensorflow/pull/71765,[],[],
2406116293,pull_request,closed,,decouple quantization_util package from compiler tosa:legalize_common,"decouple quantization_util package from compiler tosa:legalize_common

Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b
",copybara-service[bot],2024-07-12 18:14:35+00:00,['ecalubaquib'],2024-07-26 01:06:00+00:00,2024-07-26 01:05:59+00:00,https://github.com/tensorflow/tensorflow/pull/71763,[],"[{'comment_id': 2226130828, 'issue_id': 2406116293, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71763/checks?check_run_id=27386851729) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 12, 18, 14, 41, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-12 18:14:41 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71763/checks?check_run_id=27386851729) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2406110604,pull_request,closed,,Allow TFL dialect in input-mlir to tf_tfl_translate,"Allow TFL dialect in input-mlir to tf_tfl_translate
",copybara-service[bot],2024-07-12 18:10:10+00:00,[],2024-07-12 19:32:21+00:00,2024-07-12 19:32:20+00:00,https://github.com/tensorflow/tensorflow/pull/71762,[],[],
2406083024,pull_request,open,,Remove unused includes from fp_util.h,"Remove unused includes from fp_util.h
",copybara-service[bot],2024-07-12 17:51:05+00:00,[],2024-07-12 17:51:05+00:00,,https://github.com/tensorflow/tensorflow/pull/71761,[],[],
2406048052,pull_request,closed,,[JAX] Do not wait for the array deletion result,"[JAX] Do not wait for the array deletion result

JAX `Array.delete()` had a mixed behavior depending on what the underlying
implementation of an IFRT `Array::Delete()` does. JAX currently waits for the
future result of IFRT `Array::Delete()` and surfaces it, but PjRt-IFRT always
returns an OK without blocking, which makes this behavior moot. On a different
IFRT runtime that can return an error from `Array::Delete()` results in a
different behavior for JAX, and it can be also very costly if the error is
available after a physical buffer deletion or after finishing an RPC
request-response roundtrip.

This change resolves it by making JAX `Array.delete()` not wait for the result
of `Array::Delete()`. This has two side effects:

1. JAX `Array.delete()` is idempotent for every runtime.
2. JAX `Array.delete()` will be always non-blocking.
3. No errors from deletion will be surfaced to the user.

This is technically a deletion API semantics change, but since the
implementation of deletion was exactly matching the API semantics and the users
do not use the semantics, either, so we expect that this does not introduce
regression in user workloads, while this change improves the consistency of the
deletion API and its performance in some runtimes.
",copybara-service[bot],2024-07-12 17:34:11+00:00,[],2024-07-12 20:24:25+00:00,2024-07-12 20:24:24+00:00,https://github.com/tensorflow/tensorflow/pull/71760,[],[],
2406010471,pull_request,closed,,Lower pred to i8.,"Lower pred to i8.

There are some rare cases where the distinction matters. In particular,
there's a test that converts from pred to i8, with the buffer using 
random bytes, that verifies that the random bytes are preserved.
",copybara-service[bot],2024-07-12 17:08:42+00:00,[],2024-07-12 19:09:30+00:00,2024-07-12 19:09:30+00:00,https://github.com/tensorflow/tensorflow/pull/71759,[],[],
2405983958,pull_request,open,,Integrate LLVM at llvm/llvm-project@c5ee3c05ca61,"Integrate LLVM at llvm/llvm-project@c5ee3c05ca61

Updates LLVM usage to match
[c5ee3c05ca61](https://github.com/llvm/llvm-project/commit/c5ee3c05ca61)
",copybara-service[bot],2024-07-12 16:49:50+00:00,[],2024-07-12 16:49:50+00:00,,https://github.com/tensorflow/tensorflow/pull/71758,[],[],
2405927931,pull_request,open,,Integrate LLVM at llvm/llvm-project@c5ee3c05ca61,"Integrate LLVM at llvm/llvm-project@c5ee3c05ca61

Updates LLVM usage to match
[c5ee3c05ca61](https://github.com/llvm/llvm-project/commit/c5ee3c05ca61)
",copybara-service[bot],2024-07-12 16:08:42+00:00,[],2024-07-12 16:08:42+00:00,,https://github.com/tensorflow/tensorflow/pull/71757,[],[],
2405854871,pull_request,closed,,Remove unnecessary dependencies from unary_ops_composition_test,"Remove unnecessary dependencies from unary_ops_composition_test
",copybara-service[bot],2024-07-12 15:23:30+00:00,[],2024-07-15 09:08:16+00:00,2024-07-15 09:08:15+00:00,https://github.com/tensorflow/tensorflow/pull/71756,[],[],
2405827057,pull_request,closed,,"Add support for absl::Hash to Index, Shape, and IndexDomain.","Add support for absl::Hash to Index, Shape, and IndexDomain.
",copybara-service[bot],2024-07-12 15:09:00+00:00,['mattmiec'],2024-07-12 19:17:59+00:00,2024-07-12 19:17:58+00:00,https://github.com/tensorflow/tensorflow/pull/71755,[],[],
2405812535,pull_request,closed,,Fix how we read the `xla_use_shardonnay` flag when delaying the use of tuples.,"Fix how we read the `xla_use_shardonnay` flag when delaying the use of tuples.

Before we were reading it based on flags. But the shardy debug option isn't always set based on flags. So pass it in from the `CompileOptions`.

Don't need to propagate this through everywhere, and it's best to use false in most places where API changes are needed, as this fix is temporary until Shardy is the first pass in the XLA pipeline. Delaying using tuple arguments in the Shardy code path is only because Shardy is in the middle of the XLA pipeline. This will not be the case in the future.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14956 from ROCm:ci_gemm_alg_picker_refactor 3ce7e30de1841333bf5a31a74812d7bb0845e558
",copybara-service[bot],2024-07-12 15:00:52+00:00,[],2024-07-17 12:11:37+00:00,2024-07-17 12:11:36+00:00,https://github.com/tensorflow/tensorflow/pull/71754,[],[],
2405811477,pull_request,closed,,[xla:sdy] Rename xla_use_shardonnay to xla_use_shardy.,"[xla:sdy] Rename xla_use_shardonnay to xla_use_shardy.

Reverts aba14f7f702d577686a51b7a2e648f784ba8f7bb
",copybara-service[bot],2024-07-12 15:00:17+00:00,['bixia1'],2024-07-16 18:45:45+00:00,2024-07-16 18:45:45+00:00,https://github.com/tensorflow/tensorflow/pull/71753,[],[],
2405798298,pull_request,closed,,[xla:sdy] Change Shardonnay to Shardy in xla passes.,"[xla:sdy] Change Shardonnay to Shardy in xla passes.

Also remove the local implementation of attributeToString and switch to use
mlir::sdy::attributeToString.
",copybara-service[bot],2024-07-12 14:53:15+00:00,['bixia1'],2024-07-16 01:45:13+00:00,2024-07-16 01:45:12+00:00,https://github.com/tensorflow/tensorflow/pull/71752,[],[],
2405712265,pull_request,closed,,PR #14703: Add option to not combine all-gathers with different dtypes. #13305,"PR #14703: Add option to not combine all-gathers with different dtypes. #13305

Imported from GitHub PR https://github.com/openxla/xla/pull/14703

#13305 
When user specifies `combine_different_dtypes=false` the pass will not combine all-gather ops with different dtypes. Default is true to maintain existing behavior.

Copybara import of the project:

--
0334e037b08191e2af1e96320bb26860cd85d7f5 by Patrick Toulme <ptoulme@amazon.com>:

Add option to not combine all-gathers with different dtypes.

Merging this change closes #14703

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14703 from ptoulme-aws:feature/all-gather-dtype2 0334e037b08191e2af1e96320bb26860cd85d7f5
",copybara-service[bot],2024-07-12 14:15:33+00:00,[],2024-07-12 15:59:19+00:00,2024-07-12 15:59:18+00:00,https://github.com/tensorflow/tensorflow/pull/71751,[],[],
2405665734,pull_request,closed,,[xla:cpu] Allow monkey patching llvm::IRBuilder in IrEmitter,"[xla:cpu] Allow monkey patching llvm::IRBuilder in IrEmitter
",copybara-service[bot],2024-07-12 13:51:58+00:00,[],2024-07-18 07:32:18+00:00,2024-07-18 07:32:18+00:00,https://github.com/tensorflow/tensorflow/pull/71750,[],"[{'comment_id': 2225635277, 'issue_id': 2405665734, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71750/checks?check_run_id=27376041449) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 12, 13, 52, 3, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-12 13:52:03 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71750/checks?check_run_id=27376041449) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2405628771,pull_request,closed,,[XLA:GPU] Fix ROCM GetHipVersion(),"[XLA:GPU] Fix ROCM GetHipVersion()
",copybara-service[bot],2024-07-12 13:32:18+00:00,[],2024-07-12 15:29:09+00:00,2024-07-12 15:29:07+00:00,https://github.com/tensorflow/tensorflow/pull/71749,[],[],
2405551914,pull_request,open,,Integrate LLVM at llvm/llvm-project@c5ee3c05ca61,"Integrate LLVM at llvm/llvm-project@c5ee3c05ca61

Updates LLVM usage to match
[c5ee3c05ca61](https://github.com/llvm/llvm-project/commit/c5ee3c05ca61)
",copybara-service[bot],2024-07-12 13:05:34+00:00,[],2024-07-12 13:56:31+00:00,,https://github.com/tensorflow/tensorflow/pull/71748,[],[],
2405509052,pull_request,closed,,Add kSI64 to IsSignedInteger.,"Add kSI64 to IsSignedInteger.

int64 is a signed type and is missing from this function.
",copybara-service[bot],2024-07-12 12:52:20+00:00,[],2024-07-12 16:44:32+00:00,2024-07-12 16:44:31+00:00,https://github.com/tensorflow/tensorflow/pull/71747,[],[],
2405480871,pull_request,closed,,Refactor sparse-blocked-to-mma pass,"Refactor sparse-blocked-to-mma pass
",copybara-service[bot],2024-07-12 12:40:03+00:00,[],2024-07-17 15:24:17+00:00,2024-07-17 15:24:16+00:00,https://github.com/tensorflow/tensorflow/pull/71746,[],[],
2405457330,pull_request,closed,,[XLA:GPU] Cache ptxas version to avoid repeated calls to GetAsmCompilerVersion.,"[XLA:GPU] Cache ptxas version to avoid repeated calls to GetAsmCompilerVersion.

The call to `GetAsmCompilerVersion` is very expensive and is done repeatedly in `MinThreadsXRowReduction`, but the result never changes during one execution.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14605 from ROCm:rocm_triton_backend_8 9e7e0c7c05185fc476383b654744e4b2e5079da1
",copybara-service[bot],2024-07-12 12:26:07+00:00,[],2024-07-15 13:48:19+00:00,2024-07-15 13:48:18+00:00,https://github.com/tensorflow/tensorflow/pull/71745,[],[],
2405448249,pull_request,open,,Work around LLVM bug with i1s.,"Work around LLVM bug with i1s.

The ptx backend chokes on loads/stores of i1s, see
https://github.com/llvm/llvm-project/issues/98033
",copybara-service[bot],2024-07-12 12:20:36+00:00,[],2024-07-12 12:20:36+00:00,,https://github.com/tensorflow/tensorflow/pull/71743,[],[],
2405395130,pull_request,open,,Reverts e61124b6b9933cc48a9bf0cbf35a9384599a5761,"Reverts e61124b6b9933cc48a9bf0cbf35a9384599a5761
",copybara-service[bot],2024-07-12 11:48:48+00:00,['akuegel'],2024-07-12 11:48:49+00:00,,https://github.com/tensorflow/tensorflow/pull/71742,[],[],
2405391673,pull_request,closed,,Add some missing mod simplifications.,"Add some missing mod simplifications.

- mod can be rewritten to a sum
- sub floordiv* can be rewritten to mod

Fuzzed for correctness.
",copybara-service[bot],2024-07-12 11:46:28+00:00,[],2024-07-12 13:42:02+00:00,2024-07-12 13:42:01+00:00,https://github.com/tensorflow/tensorflow/pull/71741,[],[],
2405307929,pull_request,closed,,[XLA:GPU][MLIR-based emitters] Generate the loop inside the inbounds check.,"[XLA:GPU][MLIR-based emitters] Generate the loop inside the inbounds check.

This CL is a preparation before enabling vectorization for scatters. 

For scatter with the following params:

  out = f32[4000,36] parameter(0)
  indices = s32[1400000,1] parameter(1)
  updates = f32[1400000,1,36] parameter(2)
  unique_indices = true

There is 1.12x improvement on Titan RTX over the vanilla emitters.
",copybara-service[bot],2024-07-12 10:58:25+00:00,['pifon2a'],2024-07-12 13:34:02+00:00,2024-07-12 13:34:01+00:00,https://github.com/tensorflow/tensorflow/pull/71739,[],[],
2405305146,pull_request,closed,,[XLA:GPU] Clean up constructors of GpuHloCostAnalysis.,"[XLA:GPU] Clean up constructors of GpuHloCostAnalysis.

`GpuHloCostAnalysis` get a pointer to `device_info` in the construction and `nullptr` is valid. In case of `nullptr`, `GpuHloCostAnalysis` will use the default `HloOpProfile` and function correctly. However, there are a few places outside of the class that expect that `device_info` is not null, but it's never checked.

This change makes the behaviour explicit and more predictable. There is now a separate constructor for `GpuHloCostAnalysis` without `device_info`. And the pointer to `device_info` is not stored inside.
",copybara-service[bot],2024-07-12 10:56:43+00:00,[],2024-07-15 11:54:52+00:00,2024-07-15 11:54:52+00:00,https://github.com/tensorflow/tensorflow/pull/71738,[],[],
2405267931,pull_request,closed,,[xla:cpu] Fix warning message & rename,"[xla:cpu] Fix warning message & rename
",copybara-service[bot],2024-07-12 10:34:11+00:00,[],2024-07-12 13:05:22+00:00,2024-07-12 13:05:21+00:00,https://github.com/tensorflow/tensorflow/pull/71737,[],"[{'comment_id': 2225295218, 'issue_id': 2405267931, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71737/checks?check_run_id=27368207296) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 12, 10, 34, 16, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-12 10:34:16 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71737/checks?check_run_id=27368207296) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2405188520,pull_request,closed,,Mean takes the legacy path if possible so that NCHW subgraph re-write is still possible,"Mean takes the legacy path if possible so that NCHW subgraph re-write is still possible

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14780 from lausannel:patch-btree_map 447f47430878c3cde517ce2430ae719cde02b198
",copybara-service[bot],2024-07-12 09:51:03+00:00,['alankelly'],2024-07-12 12:18:55+00:00,2024-07-12 12:18:54+00:00,https://github.com/tensorflow/tensorflow/pull/71736,[],[],
2405154824,pull_request,closed,,Add missing dependency to Tensorflow test,"Add missing dependency to Tensorflow test

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14780 from lausannel:patch-btree_map 447f47430878c3cde517ce2430ae719cde02b198
",copybara-service[bot],2024-07-12 09:32:55+00:00,[],2024-07-12 11:52:34+00:00,2024-07-12 11:52:34+00:00,https://github.com/tensorflow/tensorflow/pull/71735,[],[],
2405054100,pull_request,closed,,Bump Shardy commit hash and update patch file,"Bump Shardy commit hash and update patch file
",copybara-service[bot],2024-07-12 08:37:32+00:00,[],2024-07-12 19:39:24+00:00,2024-07-12 19:39:23+00:00,https://github.com/tensorflow/tensorflow/pull/71734,[],[],
2404989137,pull_request,closed,,Clean up `:nvptx_compiler_impl` build target,"Clean up `:nvptx_compiler_impl` build target

This
1. Adds missing includes
2. Removes unused includes
3. Removes CUDA guards from the build rule and tags it as gpu and manual
4. Removes unnecessary preprocessor definitions
5. Ran build_cleaner and has it remove a bunch of unneeded dependencies

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14780 from lausannel:patch-btree_map 447f47430878c3cde517ce2430ae719cde02b198
",copybara-service[bot],2024-07-12 07:58:42+00:00,[],2024-07-12 12:31:59+00:00,2024-07-12 12:31:58+00:00,https://github.com/tensorflow/tensorflow/pull/71733,[],[],
2404970237,pull_request,closed,,Handle dynamically quantized per tensor scale when output channels = 1,"Handle dynamically quantized per tensor scale when output channels = 1
",copybara-service[bot],2024-07-12 07:47:06+00:00,['alankelly'],2024-07-12 13:15:16+00:00,2024-07-12 13:15:15+00:00,https://github.com/tensorflow/tensorflow/pull/71732,[],[],
2404951497,pull_request,open,,Update shardy pin to latest revision,"Update shardy pin to latest revision
",copybara-service[bot],2024-07-12 07:35:14+00:00,['akuegel'],2024-07-12 08:17:01+00:00,,https://github.com/tensorflow/tensorflow/pull/71731,[],[],
2404935258,pull_request,closed,,PR #14780: Replace hashmap with btree map to determine the iteration order,"PR #14780: Replace hashmap with btree map to determine the iteration order

Imported from GitHub PR https://github.com/openxla/xla/pull/14780

This PR addresses one of the problems mentioned in #7248, regarding the non-deterministic iteration order of `StableHashMap`.

To resolve this, this PR proposes the following changes:

- Replaced `::absl::flat_hash_map` with `::absl::btree_map` within the `StableHashMap` implementation.
- Added new tests to validate the deterministic iteration order of the `StableHashMap`.

Copybara import of the project:

--
14fe0a718b3667e2d918640d9b67933a8ad12c3c by Zhan Lu <51200935+lausannel@users.noreply.github.com>:

replace hashmap with btree map to determine the iteration order
--
447f47430878c3cde517ce2430ae719cde02b198 by Zhan Lu <51200935+lausannel@users.noreply.github.com>:

test: add test for iteration order of StableHashMap

Merging this change closes #14780

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14780 from lausannel:patch-btree_map 447f47430878c3cde517ce2430ae719cde02b198
",copybara-service[bot],2024-07-12 07:25:18+00:00,[],2024-07-12 10:50:30+00:00,2024-07-12 10:50:29+00:00,https://github.com/tensorflow/tensorflow/pull/71730,[],[],
2404912206,pull_request,closed,,Integrate LLVM at llvm/llvm-project@9ddfe62f5c11,"Integrate LLVM at llvm/llvm-project@9ddfe62f5c11

Updates LLVM usage to match
[9ddfe62f5c11](https://github.com/llvm/llvm-project/commit/9ddfe62f5c11)
",copybara-service[bot],2024-07-12 07:10:19+00:00,[],2024-07-12 10:41:36+00:00,2024-07-12 10:41:35+00:00,https://github.com/tensorflow/tensorflow/pull/71729,[],[],
2404895303,pull_request,closed,,Add gpu_init dependency unconditionally,"Add gpu_init dependency unconditionally

The test is including the gpu_init.h header unconditionally. So we should also
add the dependency unconditionally.
",copybara-service[bot],2024-07-12 06:58:49+00:00,['akuegel'],2024-07-12 09:10:51+00:00,2024-07-12 09:10:51+00:00,https://github.com/tensorflow/tensorflow/pull/71728,[],[],
2404885670,pull_request,open,,[NCCL] Upgrade nvtx dependency,"[NCCL] Upgrade nvtx dependency
",copybara-service[bot],2024-07-12 06:52:28+00:00,[],2024-07-12 06:52:28+00:00,,https://github.com/tensorflow/tensorflow/pull/71727,[],[],
2404849028,pull_request,closed,,[XLA] Correctly pipeline data-dependent collectives when it is not the last run (of the pipeliner).,"[XLA] Correctly pipeline data-dependent collectives when it is not the last run (of the pipeliner).

If the pipeliner config has last_run=false, it inserts a custom-call in place of each sunk collective. When we have data-dependent collectives (one in the user subtree of another) the custom calls inserted in the previous iteration should be acceptable users for the current collective being sunk.
",copybara-service[bot],2024-07-12 06:24:59+00:00,['seherellis'],2024-07-12 20:31:46+00:00,2024-07-12 20:31:46+00:00,https://github.com/tensorflow/tensorflow/pull/71725,[],[],
2404779311,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 05:23:43+00:00,[],2024-07-12 05:23:43+00:00,,https://github.com/tensorflow/tensorflow/pull/71724,[],[],
2404769614,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 05:15:58+00:00,[],2024-07-16 08:07:41+00:00,2024-07-16 08:07:40+00:00,https://github.com/tensorflow/tensorflow/pull/71723,[],[],
2404763093,pull_request,closed,,[XLA] Introduce WindowPrefetchedAllocation,"[XLA] Introduce WindowPrefetchedAllocation

WindowPrefetchedAllocation enables memory space assignment to perform window prefetch. Previously, memory space assignment can only prefetch a whole tensor, window prefetch allows it to prefetch only a window worth of data. WindowPrefetchAllocation allocates buffers in the alternate memory that hold windows of data.

WindowPrefetchedAllocations kick in only when a tensor was decided to be placed in the default memory. When the tensor was placed in the default memory, we will now try to allocate a small buffer in the alternate memory using WindowPrefetchedAllocations. If the allocation succeeds, it will insert a pair of async WindowPrefetch operations, append a new operand to the use instruction and feed the output of the WindowPrefetch operation as the new operand. So the use instruction can use the buffer when being lowered.

If the allocation fails, window prefetch will not be used. Then this is the default behavior as today.

For simplicity, WindowPrefetch does not use sliced copy.

The insertion of WindowPrefetchedAllocation is gated under the `memory_space_assignment_options` field `enable_window_prefetch`, which is currently disabled, so this CL should be a no-op for now.
",copybara-service[bot],2024-07-12 05:09:35+00:00,[],2024-08-01 22:44:49+00:00,2024-08-01 22:44:48+00:00,https://github.com/tensorflow/tensorflow/pull/71722,[],[],
2404757214,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13799 from ROCm:ci_add_dot_algorithms_for_amd_gpus_20240614 b0274e9d02a2aa584c68cd0b69ae78607e8907a2
",copybara-service[bot],2024-07-12 05:03:55+00:00,[],2024-07-12 08:10:11+00:00,,https://github.com/tensorflow/tensorflow/pull/71721,[],[],
2404755527,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 05:02:21+00:00,[],2024-07-12 05:02:21+00:00,,https://github.com/tensorflow/tensorflow/pull/71720,[],[],
2404752606,pull_request,closed,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14692 from openxla:skip_high_shmem_test f6f8acff5ea2bdb5d8f53c8fb0457aecaf2b512c
",copybara-service[bot],2024-07-12 04:59:30+00:00,[],2024-07-16 12:22:51+00:00,2024-07-16 12:22:50+00:00,https://github.com/tensorflow/tensorflow/pull/71719,[],[],
2404749708,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14780 from lausannel:patch-btree_map 447f47430878c3cde517ce2430ae719cde02b198
",copybara-service[bot],2024-07-12 04:56:18+00:00,[],2024-07-12 09:26:23+00:00,,https://github.com/tensorflow/tensorflow/pull/71718,[],[],
2404748040,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:54:25+00:00,[],2024-07-13 09:42:41+00:00,2024-07-13 09:42:40+00:00,https://github.com/tensorflow/tensorflow/pull/71717,[],[],
2404746895,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:53:06+00:00,[],2024-07-12 04:53:06+00:00,,https://github.com/tensorflow/tensorflow/pull/71716,[],[],
2404744825,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:50:38+00:00,[],2024-07-16 06:45:36+00:00,2024-07-16 06:45:35+00:00,https://github.com/tensorflow/tensorflow/pull/71715,[],[],
2404743864,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:49:29+00:00,[],2024-07-12 04:49:29+00:00,,https://github.com/tensorflow/tensorflow/pull/71714,[],[],
2404742289,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:47:34+00:00,[],2024-07-13 06:01:22+00:00,2024-07-13 06:01:21+00:00,https://github.com/tensorflow/tensorflow/pull/71713,[],[],
2404741542,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:46:40+00:00,[],2024-07-12 04:46:40+00:00,,https://github.com/tensorflow/tensorflow/pull/71712,[],[],
2404740542,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:45:31+00:00,[],2024-07-12 04:45:31+00:00,,https://github.com/tensorflow/tensorflow/pull/71711,[],[],
2404728243,pull_request,closed,,[xla:cpu] Optimize ThunkExecutor::ExecuteState,"[xla:cpu] Optimize ThunkExecutor::ExecuteState

1. Use InlinedVector to keep ReadyNodes. For many nested computations thunk sequences are small and the cost of heap allocation is large
2. Remove atomic counter pointer indirection and keep counters + out_edges together
3. Add more relevant thunk sequence sizes to benchmark (not reported below as they are missing in a baseline)

name                                         old cpu/op   new cpu/op   delta
BM_SequentialThunkExecutor/1/process_time    29.7ns ± 2%  29.6ns ± 7%    ~     
BM_SequentialThunkExecutor/16/process_time    476ns ± 3%   479ns ± 4%    ~     
BM_SequentialThunkExecutor/64/process_time   2.01µs ± 3%  1.99µs ± 4%  -1.32%  
BM_SequentialThunkExecutor/128/process_time  4.13µs ± 4%  4.13µs ± 3%    ~     
BM_SequentialThunkExecutor/256/process_time  8.54µs ± 1%  8.58µs ± 3%    ~     
BM_SequentialThunkExecutor/512/process_time  18.1µs ± 1%  18.4µs ± 3%  +1.31%  
BM_SyncThunkExecutor/1/process_time          28.9ns ± 2%  29.7ns ± 7%  +2.67%  
BM_SyncThunkExecutor/16/process_time          782ns ± 1%   733ns ± 3%  -6.29%  
BM_SyncThunkExecutor/64/process_time         3.20µs ± 2%  2.99µs ± 2%  -6.56%  
BM_SyncThunkExecutor/128/process_time        6.86µs ± 2%  6.29µs ± 2%  -8.25%  
BM_SyncThunkExecutor/256/process_time        14.7µs ± 1%  13.4µs ± 1%  -8.92%  
BM_SyncThunkExecutor/512/process_time        33.3µs ± 3%  30.3µs ± 2%  -9.03%  
BM_AsyncThunkExecutor/1/process_time         9.22µs ± 8%  8.92µs ± 5%  -3.24%  
BM_AsyncThunkExecutor/16/process_time         146µs ± 8%   143µs ± 5%  -2.70%  
BM_AsyncThunkExecutor/64/process_time         194µs ± 7%   190µs ± 5%  -2.53%  
BM_AsyncThunkExecutor/128/process_time        278µs ± 6%   272µs ± 5%    ~     (p=0.056 n=20+20)
BM_AsyncThunkExecutor/256/process_time        385µs ± 6%   374µs ± 5%  -2.84%  
BM_AsyncThunkExecutor/512/process_time        659µs ± 7%   644µs ± 7%    ~     

name                                         old time/op      new time/op          delta
BM_SequentialThunkExecutor/1/process_time    29.7ns ± 2%      29.3ns ± 5%  -1.54%
BM_SequentialThunkExecutor/16/process_time    476ns ± 3%       478ns ± 3%    ~   
BM_SequentialThunkExecutor/64/process_time   2.00µs ± 4%      1.99µs ± 4%    ~   
BM_SequentialThunkExecutor/128/process_time  4.13µs ± 4%      4.12µs ± 3%    ~   
BM_SequentialThunkExecutor/256/process_time  8.55µs ± 3%      8.56µs ± 3%    ~   
BM_SequentialThunkExecutor/512/process_time  18.1µs ± 1%      18.4µs ± 4%  +1.33%
BM_SyncThunkExecutor/1/process_time          28.9ns ± 2%      29.7ns ± 7%  +2.70%
BM_SyncThunkExecutor/16/process_time          782ns ± 1%       733ns ± 3%  -6.28%
BM_SyncThunkExecutor/64/process_time         3.20µs ± 2%      2.99µs ± 2%  -6.55%
BM_SyncThunkExecutor/128/process_time        6.86µs ± 2%      6.30µs ± 1%  -8.12%
BM_SyncThunkExecutor/256/process_time        14.7µs ± 1%      13.4µs ± 1%  -8.92%
BM_SyncThunkExecutor/512/process_time        33.4µs ± 3%      30.3µs ± 2%  -9.02%
BM_AsyncThunkExecutor/1/process_time         2.58µs ± 8%      2.49µs ± 4%  -3.32%
BM_AsyncThunkExecutor/16/process_time        40.2µs ± 8%      39.2µs ± 6%  -2.55%
BM_AsyncThunkExecutor/64/process_time        36.3µs ± 8%      34.7µs ± 8%  -4.33%
BM_AsyncThunkExecutor/128/process_time       49.9µs ± 6%      48.0µs ± 5%  -3.85%
BM_AsyncThunkExecutor/256/process_time       60.6µs ± 7%      57.9µs ± 5%  -4.40%
BM_AsyncThunkExecutor/512/process_time       96.8µs ± 7%      92.6µs ± 6%  -4.36%
",copybara-service[bot],2024-07-12 04:37:14+00:00,['ezhulenev'],2024-07-12 12:25:25+00:00,2024-07-12 12:25:25+00:00,https://github.com/tensorflow/tensorflow/pull/71710,[],[],
2404681497,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-12 04:13:49+00:00,[],2024-07-12 05:04:40+00:00,,https://github.com/tensorflow/tensorflow/pull/71709,[],[],
2404646900,pull_request,closed,,Copy part of TFL model_utils to MLIR.,"Copy part of TFL model_utils to MLIR.
",copybara-service[bot],2024-07-12 04:02:56+00:00,['pak-laura'],2024-07-24 23:12:33+00:00,2024-07-24 23:12:32+00:00,https://github.com/tensorflow/tensorflow/pull/71708,[],[],
2404536922,pull_request,closed,,[XLA:SPMD] Remove `LookaheadUserSharding` in sharding propagation.,"[XLA:SPMD] Remove `LookaheadUserSharding` in sharding propagation.

When we infer the dot sharding from its operands, it is possible that both operands can improve the dot sharding. `LookaheadUserSharding` iterates the dot users and decides which dot operand sharding is preferred. This cl removes it for two reasons.
1. It is unnecessary. If we can predict the sharding from dot users, we can wait the sharding to be propagated from users. The propagted sharding from users can still help us make choice between dot operands.
2. The lookhead sharding may be wrong. `LookaheadUserSharding` is a heuristics. We cannot guarantee that the predicted sharding will hold in the dot users.
",copybara-service[bot],2024-07-12 02:49:32+00:00,[],2024-08-15 17:02:07+00:00,2024-08-15 17:02:06+00:00,https://github.com/tensorflow/tensorflow/pull/71707,[],[],
2404471512,pull_request,closed,,"r2.17 cherry-pick: a2bda886281 ""Fix breakage to TF Serving""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/a2bda8862811c8f15c09b43bffb67fa2dc460f71,tensorflow-jenkins,2024-07-12 01:23:24+00:00,[],2024-12-15 08:09:40+00:00,2024-12-15 08:09:40+00:00,https://github.com/tensorflow/tensorflow/pull/71706,[],"[{'comment_id': 2225614769, 'issue_id': 2404471512, 'author': 'mihaimaruseac', 'body': 'This needs to wait until the patch release', 'created_at': datetime.datetime(2024, 7, 12, 13, 40, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543548525, 'issue_id': 2404471512, 'author': 'mihaimaruseac', 'body': ""Unfortunately this was not merged in and there won't be new TF 2.17 releases. So, I'll have to close the PR"", 'created_at': datetime.datetime(2024, 12, 15, 8, 9, 40, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-12 13:40:11 UTC): This needs to wait until the patch release

mihaimaruseac on (2024-12-15 08:09:40 UTC): Unfortunately this was not merged in and there won't be new TF 2.17 releases. So, I'll have to close the PR

"
2404470867,pull_request,closed,,"r2.17 cherry-pick: 130d583ff62 ""Remove noexcept from optimized function graph info""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/130d583ff62c506156faf4077fd7c23798378d2f,tensorflow-jenkins,2024-07-12 01:22:28+00:00,[],2024-12-15 08:09:29+00:00,2024-12-15 08:09:28+00:00,https://github.com/tensorflow/tensorflow/pull/71705,[],"[{'comment_id': 2225614963, 'issue_id': 2404470867, 'author': 'mihaimaruseac', 'body': 'This needs to wait until the patch release', 'created_at': datetime.datetime(2024, 7, 12, 13, 40, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543548457, 'issue_id': 2404470867, 'author': 'mihaimaruseac', 'body': ""Unfortunately this was not merged in and there won't be new TF 2.17 releases. So, I'll have to close the PR"", 'created_at': datetime.datetime(2024, 12, 15, 8, 9, 29, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-12 13:40:18 UTC): This needs to wait until the patch release

mihaimaruseac on (2024-12-15 08:09:29 UTC): Unfortunately this was not merged in and there won't be new TF 2.17 releases. So, I'll have to close the PR

"
2404365255,pull_request,open,,Add stub for prepare hlo. Will be populated shortly with conv re-layout patterns.,"Add stub for prepare hlo. Will be populated shortly with conv re-layout patterns.

Also:
* Remove redundant ""Create a pass"" comment in passes.h
* Remove unused add patterns declaration in passes.h
",copybara-service[bot],2024-07-11 23:41:48+00:00,['LukeBoyer'],2024-07-11 23:41:49+00:00,,https://github.com/tensorflow/tensorflow/pull/71704,[],[],
2404347826,pull_request,closed,,Add back `tf2tensorrt` test_filter,"Add back `tf2tensorrt` test_filter

test_filter config doesn't apply this (not sure what order target patterns in configs are evaluated?)
",copybara-service[bot],2024-07-11 23:17:25+00:00,['ddunl'],2024-07-12 00:45:11+00:00,2024-07-12 00:45:11+00:00,https://github.com/tensorflow/tensorflow/pull/71703,[],[],
2404325570,pull_request,open,,Allow implementations of XLA:CPU collectives to be passed directly into `make_cpu_client`.,"Allow implementations of XLA:CPU collectives to be passed directly into `make_cpu_client`.
",copybara-service[bot],2024-07-11 23:03:32+00:00,[],2024-07-11 23:03:32+00:00,,https://github.com/tensorflow/tensorflow/pull/71702,[],[],
2404280658,pull_request,closed,,Update release notes at HEAD,"Update release notes at HEAD
",copybara-service[bot],2024-07-11 22:38:18+00:00,['rtg0795'],2024-07-12 21:25:35+00:00,2024-07-12 21:25:34+00:00,https://github.com/tensorflow/tensorflow/pull/71700,[],[],
2404216764,pull_request,closed,,Move the utility code in the tf_saved_model_freeze_variables to a reusable place.,"Move the utility code in the tf_saved_model_freeze_variables to a reusable place.

Get feature parity across freeze_global_tensors.cc and tf_saved_model_freeze_variables.cc, by reusing the underlying code that does the resource-propagation and freezing.

Both the passes/functions are trying to do similar things, one on an immutable GlobalTensorOp and the other on an immutable tf.VarHandleOp. So, it should make sense to reuse code to improve support.

freeze_global_tensors.cc appears to have issues, reported in the attached bug and currently only supports if the user of the resource is `TF::ReadVariableOp` or a `CallOpInterface`. But `tf_saved_model_freeze_variables.cc` has a broader support.

Reverts aba14f7f702d577686a51b7a2e648f784ba8f7bb
",copybara-service[bot],2024-07-11 22:07:42+00:00,['vamsimanchala'],2024-07-16 18:27:17+00:00,2024-07-16 18:27:16+00:00,https://github.com/tensorflow/tensorflow/pull/71699,[],[],
2404150345,pull_request,closed,,copybara config refactoring change,"copybara config refactoring change
",copybara-service[bot],2024-07-11 21:22:59+00:00,['ddunl'],2024-07-12 00:10:44+00:00,2024-07-12 00:10:44+00:00,https://github.com/tensorflow/tensorflow/pull/71698,[],[],
2404140934,pull_request,closed,,PR #14795: [GPU] Add methods for deterministic backend config fingerprinting.,"PR #14795: [GPU] Add methods for deterministic backend config fingerprinting.

Imported from GitHub PR https://github.com/openxla/xla/pull/14795

Backend config being a proto can contain maps, which are not printed in deterministic order by default. The new methods allow printing a deterministic fingerprint of a backend config, which is implemented as an encoded string of a deterministic binary serialization of the backend config proto.
Copybara import of the project:

--
2819a1c1c46c201bbf137f38232e7ba019f6d5f2 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Add methods for deterministic backend config fingerprinting.

Backend config being a proto can contain maps, which are not printed in
deterministic order by default. The new methods allow printing a
deterministic fingerprint of a backend config, which is implemented as
an encoded string of a deterministic binary serialization of the backend
config proto.

--
074f59f818bcb064e43c8707d5b756f796ef7c17 by Ilia Sergachev <isergachev@nvidia.com>:

Address feedback

Merging this change closes #14795

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14795 from openxla:backend_config_fingerprint 074f59f818bcb064e43c8707d5b756f796ef7c17
",copybara-service[bot],2024-07-11 21:17:00+00:00,[],2024-07-12 02:26:17+00:00,2024-07-12 02:26:17+00:00,https://github.com/tensorflow/tensorflow/pull/71697,[],[],
2404125816,pull_request,open,,Internal Copybara refactoring change,"Internal Copybara refactoring change
",copybara-service[bot],2024-07-11 21:05:34+00:00,['ddunl'],2024-07-11 21:05:35+00:00,,https://github.com/tensorflow/tensorflow/pull/71696,[],[],
2404030230,pull_request,closed,,PR #13799: Enable dot algorithms for AMD GPUs,"PR #13799: Enable dot algorithms for AMD GPUs

Imported from GitHub PR https://github.com/openxla/xla/pull/13799


Copybara import of the project:

--
e48bf30bbac80666fdb3b30ad8d18a86f0bf8c0b by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Enable dot algorithms for AMD GPUs

--
9f0db5b55ae08545ddd21592f58350b39d1c36fc by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Address review comments to remove ifdefs and use correct rocm version

--
4420b277e4103cf81c31b4e107d0d85e376307c4 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Use `runtime_version` from device descriptor to get ROCm version

--
b0274e9d02a2aa584c68cd0b69ae78607e8907a2 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Remove ISCuda() function

Merging this change closes #13799

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13799 from ROCm:ci_add_dot_algorithms_for_amd_gpus_20240614 b0274e9d02a2aa584c68cd0b69ae78607e8907a2
",copybara-service[bot],2024-07-11 20:17:12+00:00,[],2024-07-12 09:33:58+00:00,2024-07-12 09:33:56+00:00,https://github.com/tensorflow/tensorflow/pull/71695,[],[],
2404027540,pull_request,closed,,[xla] Restore input-output alias config that might accidentally dropped by FloatNormalization,"[xla] Restore input-output alias config that might accidentally dropped by FloatNormalization
",copybara-service[bot],2024-07-11 20:15:38+00:00,['ezhulenev'],2024-07-11 23:10:02+00:00,2024-07-11 23:10:02+00:00,https://github.com/tensorflow/tensorflow/pull/71694,[],[],
2403985826,pull_request,closed,,Copy runtime_shape to compiler/mlir/lite.,"Copy runtime_shape to compiler/mlir/lite.
",copybara-service[bot],2024-07-11 19:54:46+00:00,['pak-laura'],2024-07-25 23:08:13+00:00,2024-07-25 23:08:11+00:00,https://github.com/tensorflow/tensorflow/pull/71693,[],[],
2403932919,pull_request,closed,,[xla] shape_util: fix ForEachMutableLeafShape,"[xla] shape_util: fix ForEachMutableLeafShape
",copybara-service[bot],2024-07-11 19:28:46+00:00,['cota'],2024-07-11 23:17:11+00:00,2024-07-11 23:17:11+00:00,https://github.com/tensorflow/tensorflow/pull/71692,[],[],
2403914639,pull_request,closed,,Fix linking issue in reduce pattern library. Add some comments for the patterns.,"Fix linking issue in reduce pattern library. Add some comments for the patterns.
",copybara-service[bot],2024-07-11 19:15:48+00:00,['LukeBoyer'],2024-07-11 22:39:55+00:00,2024-07-11 22:39:55+00:00,https://github.com/tensorflow/tensorflow/pull/71691,[],[],
2403913161,pull_request,closed,,[XLA:GPU] Remove GpuComplexType & GpuDoubleComplexType.,"[XLA:GPU] Remove GpuComplexType & GpuDoubleComplexType.
XLA should not include any platform specific headers in XLA header files.
",copybara-service[bot],2024-07-11 19:14:49+00:00,[],2024-07-15 18:23:41+00:00,2024-07-15 18:23:40+00:00,https://github.com/tensorflow/tensorflow/pull/71690,[],[],
2403908680,pull_request,closed,,When we sink a copy in HostOffloadLegalize and fix up dynamic-update-slice's,"When we sink a copy in HostOffloadLegalize and fix up dynamic-update-slice's
shape along the way, we need to make the update operand's shape having the same
layout also. The fix does this by adding a copy on the update opernad.

A copy on DUS's operand side should not be treated as a copy to be moved. This
cl also fixes an issue due to inaccurately tracing to the DUS from its update
operand.

Reverts 8a7a7a3f0e6d6b6474f6a091b17590de425baec7
",copybara-service[bot],2024-07-11 19:11:52+00:00,[],2024-07-12 02:19:03+00:00,2024-07-12 02:19:02+00:00,https://github.com/tensorflow/tensorflow/pull/71689,[],[],
2403905005,pull_request,closed,,Delete HloCostAnalysisCosts constructor that will be implicitly deleted.,"Delete HloCostAnalysisCosts constructor that will be implicitly deleted.
",copybara-service[bot],2024-07-11 19:09:17+00:00,['sparc1998'],2024-07-11 22:46:44+00:00,2024-07-11 22:46:43+00:00,https://github.com/tensorflow/tensorflow/pull/71688,[],[],
2403903028,pull_request,open,,Integrate LLVM at llvm/llvm-project@9ddfe62f5c11,"Integrate LLVM at llvm/llvm-project@9ddfe62f5c11

Updates LLVM usage to match
[9ddfe62f5c11](https://github.com/llvm/llvm-project/commit/9ddfe62f5c11)
",copybara-service[bot],2024-07-11 19:08:03+00:00,[],2024-07-11 22:43:48+00:00,,https://github.com/tensorflow/tensorflow/pull/71687,[],[],
2403874514,pull_request,closed,,Delete unused `//tensorflow/python/integration_testing/...` and all users,"Delete unused `//tensorflow/python/integration_testing/...` and all users
",copybara-service[bot],2024-07-11 18:49:40+00:00,['ddunl'],2024-07-12 20:17:08+00:00,2024-07-12 20:17:08+00:00,https://github.com/tensorflow/tensorflow/pull/71686,[],[],
2403844891,pull_request,open,,Add support for absl::Hash to IndexDomain.,"Add support for absl::Hash to IndexDomain.

Reverts 8a7a7a3f0e6d6b6474f6a091b17590de425baec7
",copybara-service[bot],2024-07-11 18:34:04+00:00,['mattmiec'],2024-07-12 00:43:52+00:00,,https://github.com/tensorflow/tensorflow/pull/71685,[],[],
2403814772,pull_request,open,,Remove unused BlasSupport functions.,"Remove unused BlasSupport functions.
",copybara-service[bot],2024-07-11 18:20:11+00:00,[],2024-07-11 18:20:11+00:00,,https://github.com/tensorflow/tensorflow/pull/71683,[],[],
2403774446,pull_request,closed,,Add `third_party_mapping` for `shardy` in TF,"Add `third_party_mapping` for `shardy` in TF

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9
",copybara-service[bot],2024-07-11 18:06:16+00:00,['ddunl'],2024-07-11 22:11:08+00:00,2024-07-11 22:11:07+00:00,https://github.com/tensorflow/tensorflow/pull/71682,[],[],
2403742273,pull_request,closed,,Remove `xla::ifrt::LoadedExecutable::addressable_device_logical_ids`,"Remove `xla::ifrt::LoadedExecutable::addressable_device_logical_ids`

This API is not used by anyone and many implementations today even return an empty span (which is technically incorrect). This CL cleans up the API until we find it useful again.
",copybara-service[bot],2024-07-11 17:53:01+00:00,[],2024-07-12 06:14:30+00:00,2024-07-12 06:14:30+00:00,https://github.com/tensorflow/tensorflow/pull/71681,[],[],
2403731081,pull_request,closed,,Remove redundant target patterns already covered by the `*_test_filters` configs,"Remove redundant target patterns already covered by the `*_test_filters` configs

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9
",copybara-service[bot],2024-07-11 17:47:15+00:00,['ddunl'],2024-07-11 22:21:40+00:00,2024-07-11 22:21:39+00:00,https://github.com/tensorflow/tensorflow/pull/71680,[],[],
2403696743,pull_request,open,,PR #14807: Propagate arg and result info from MLIR to the compiler on GPUs,"PR #14807: Propagate arg and result info from MLIR to the compiler on GPUs

Imported from GitHub PR https://github.com/openxla/xla/pull/14807

Let us propagate memory space annotations on parameters and result from MLIR module to Hlo using existing utils functions (which are currently only used by TPUs). While we are there, we also hook up the layout mode annotations.

Reland of https://github.com/openxla/xla/pull/14090, together with a fix for result layout fix-up after propagating shardings to output.
Copybara import of the project:

--
bc739461496988612fea830419d05c715a9d93ec by Jaroslav Sevcik <jsevcik@nvidia.com>:

Revert ""Reverts 5b619ac97f0b15cdadf1eb67ac2d5234a17dbfea""

This reverts commit e21e3e0165c83ee659f4d681ac606b9fc6ad4172.

--
aeca587e90e25f4a7f20c2d44b331fea5fdf7534 by Jaroslav Sevcik <jsevcik@nvidia.com>:

Fix and test

Merging this change closes #14807

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14807 from jaro-sevcik:compile-argument-memory-kinds-and-layouts-from-mlir aeca587e90e25f4a7f20c2d44b331fea5fdf7534
",copybara-service[bot],2024-07-11 17:28:10+00:00,[],2024-07-18 19:22:19+00:00,,https://github.com/tensorflow/tensorflow/pull/71679,[],[],
2403668791,pull_request,closed,,Remove unused BlasSupport functions.,"Remove unused BlasSupport functions.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14803 from ROCm:ci_hotfix_20240711 411a4f9d5819ebb77f09fb53e8e1caeb43e0276a
",copybara-service[bot],2024-07-11 17:12:15+00:00,[],2024-07-11 22:05:49+00:00,2024-07-11 22:05:49+00:00,https://github.com/tensorflow/tensorflow/pull/71678,[],[],
2403624040,pull_request,open,,Remove upstream simplifier.,"Remove upstream simplifier.

The upstream simplifier is about 2/3 of the total runtime of simplification,
but it doesn't really do much for us.

Tested with a couple of fuzz tests that are not submitted yet:
- one that verifies that simplify(mlir-simplify(simplify) == simplify
- one that verifies correctness by enumerating the domain

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 16:43:39+00:00,[],2024-07-11 16:43:39+00:00,,https://github.com/tensorflow/tensorflow/pull/71677,[],[],
2403595430,pull_request,closed,,Created Dummy File,Plz Ignore this request,alifbasha22,2024-07-11 16:28:30+00:00,['gbaned'],2024-07-11 16:28:52+00:00,2024-07-11 16:28:52+00:00,https://github.com/tensorflow/tensorflow/pull/71676,"[('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2223376304, 'issue_id': 2403595430, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71676/checks?check_run_id=27334278403) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 11, 16, 28, 34, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-11 16:28:34 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/71676/checks?check_run_id=27334278403) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2403563267,pull_request,closed,,Add external KV cache op for GenAI.,"Add external KV cache op for GenAI.
",copybara-service[bot],2024-07-11 16:08:57+00:00,['hheydary'],2024-07-18 16:10:43+00:00,2024-07-18 16:10:42+00:00,https://github.com/tensorflow/tensorflow/pull/71675,[],[],
2403529487,pull_request,closed,,Add todo to remove duplicate dependency on schema_fbs.,"Add todo to remove duplicate dependency on schema_fbs.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 15:50:38+00:00,[],2024-07-11 18:46:18+00:00,2024-07-11 18:46:17+00:00,https://github.com/tensorflow/tensorflow/pull/71674,[],[],
2403527842,pull_request,closed,,Run ModuleMembarAnalysis run before LoadLocalOpConversion,"Run ModuleMembarAnalysis run before LoadLocalOpConversion
",copybara-service[bot],2024-07-11 15:49:42+00:00,[],2024-07-12 13:20:40+00:00,2024-07-12 13:20:39+00:00,https://github.com/tensorflow/tensorflow/pull/71673,[],[],
2403476958,pull_request,open,,Bump Shardy commit hash and update patch file,"Bump Shardy commit hash and update patch file

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 15:24:22+00:00,[],2024-07-11 15:24:22+00:00,,https://github.com/tensorflow/tensorflow/pull/71672,[],[],
2403400049,pull_request,open,,[xla:sdy] Testing a copybarachange. Open source xla passes for Shardy.,"[xla:sdy] Testing a copybarachange. Open source xla passes for Shardy.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9
",copybara-service[bot],2024-07-11 14:55:31+00:00,['bixia1'],2024-07-11 18:20:45+00:00,,https://github.com/tensorflow/tensorflow/pull/71671,[],[],
2403377611,pull_request,closed,,TfLite stablehlo add sort_op," - Adds stablehlo_sort implementation
 - Adds stablehlo sort unit tests
 - Adds bfloat16 typeToTfLiteType function for BF16
 - Adds Comparator Subgraph",RahulSundarMCW,2024-07-11 14:47:16+00:00,['gbaned'],2024-07-11 14:47:36+00:00,2024-07-11 14:47:36+00:00,https://github.com/tensorflow/tensorflow/pull/71670,"[('size:L', 'CL Change Size: Large')]",[],
2403354324,pull_request,closed,,"[XLA:GPU] Replace {Ceil,Floor}Div with LLVM's divide{Ceil,Floor}Signed","[XLA:GPU] Replace {Ceil,Floor}Div with LLVM's divide{Ceil,Floor}Signed

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 14:38:17+00:00,['d0k'],2024-07-11 19:14:18+00:00,2024-07-11 19:14:17+00:00,https://github.com/tensorflow/tensorflow/pull/71669,[],[],
2403268522,pull_request,closed,,PR #14803: [ROCm] Fixed gcc build errors,"PR #14803: [ROCm] Fixed gcc build errors

Imported from GitHub PR https://github.com/openxla/xla/pull/14803

Declared `ThunkExecutor::SplitReadyQueue` inline. 
Also changed the test introduced here https://github.com/openxla/xla/commit/114cab2b5f76df86ba325a8537ddd0f279f6e088 in order to avoid UB.
Copybara import of the project:

--
1dda22e417e82e7372096a751da89bbc5baeec14 by mmakevic <Milica.Makevic@amd.com>:

Declare SplitReadyQueue inline

--
411a4f9d5819ebb77f09fb53e8e1caeb43e0276a by mmakevic <Milica.Makevic@amd.com>:

Fix gcc build error due to UB

Merging this change closes #14803

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14803 from ROCm:ci_hotfix_20240711 411a4f9d5819ebb77f09fb53e8e1caeb43e0276a
",copybara-service[bot],2024-07-11 13:59:59+00:00,[],2024-07-11 17:39:09+00:00,2024-07-11 17:39:08+00:00,https://github.com/tensorflow/tensorflow/pull/71668,[],[],
2403231745,pull_request,closed,,[XLA:GPU] Reuse LLVM's AddOverflow/MulOverflow,"[XLA:GPU] Reuse LLVM's AddOverflow/MulOverflow

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13479 from ROCm:ci_rocm_62_features 215b92dddd440f5bacee8d7f678e1f5138761e00
",copybara-service[bot],2024-07-11 13:44:10+00:00,['d0k'],2024-07-11 17:28:45+00:00,2024-07-11 17:28:43+00:00,https://github.com/tensorflow/tensorflow/pull/71667,[],[],
2403099301,pull_request,closed,,[XLA:GPU] Return early in RemoveUnusedSymbols/Dimensions.,"[XLA:GPU] Return early in RemoveUnusedSymbols/Dimensions.

`DetectUnusedVariables` can be expensive, but often we don't have symbols in the indexing map at all, so there is nothing to remove.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14796 from openxla:skozub/gemm_fusion_autotuner_test 5005f288b67a2a34ec643cfcc3fbae815b5f0ef6
",copybara-service[bot],2024-07-11 12:47:49+00:00,[],2024-07-11 16:21:26+00:00,2024-07-11 16:21:26+00:00,https://github.com/tensorflow/tensorflow/pull/71665,[],[],
2403088867,pull_request,closed,,Add missing header include.,"Add missing header include.

This is needed for M_LN2l
Without the include the build is failing on MacOS.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14796 from openxla:skozub/gemm_fusion_autotuner_test 5005f288b67a2a34ec643cfcc3fbae815b5f0ef6
",copybara-service[bot],2024-07-11 12:42:58+00:00,['akuegel'],2024-07-11 16:28:26+00:00,2024-07-11 16:28:25+00:00,https://github.com/tensorflow/tensorflow/pull/71664,[],[],
2403078030,pull_request,closed,,PR #14364: [ROCm] Add script to run multi gpu tests,"PR #14364: [ROCm] Add script to run multi gpu tests

Imported from GitHub PR https://github.com/openxla/xla/pull/14364

This is a rocm specific script housed under `build_tools/rocm`  It runs following distributed tests which require more >= 4 gpus and these tests are skipped currently in the CI due to tag selection. These tests are tagged either as manual or with oss
```
//xla/tests:collective_ops_e2e_test_gpu_amd_any 
//xla/tests:collective_ops_test_gpu_amd_any 
//xla/tests:replicated_io_feed_test_gpu_amd_any 
//xla/tools/multihost_hlo_runner:functional_hlo_runner_test_gpu_amd_any 
//xla/pjrt/distributed:topology_util_test 
//xla/pjrt/distributed:client_server_test
 ```
 Also these tests do not use `--run_under=//tools/ci_build/gpu_build:parallel_gpu_execute` with bazel which locks down individual gpus thus making multi gpu tests impossible to run
 
 Eventually we would like to enable these tests in a separate pipeline to get better test coverage
Copybara import of the project:

--
300a3eb86fcf7c7bcdb8be6ae5e9a07356a0daa7 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Add script to run multi gpu tests

--
913b710d9332d5797d40c964bfaa65b52745fdfe by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Add pjrt distributed tests and check for number of gpus

--
326dc007a7b641be59e3265916fa5a528846f17b by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

Address review comment by adding description to shell script

Merging this change closes #14364

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 12:37:32+00:00,[],2024-07-11 17:58:03+00:00,2024-07-11 17:58:02+00:00,https://github.com/tensorflow/tensorflow/pull/71663,[],[],
2403065197,pull_request,open,,[XLA:GPU] Add cache for FlopsPerElement.,"[XLA:GPU] Add cache for FlopsPerElement.

Tiled Cost Model still relies on GpuHloCostAnalysis to get flops estimate. The estimate only depends on opcode, for some instruction on return type, so it's beneficial to cache. Once we stop using GpuHloCostAnalysis in other places, we could simply inline that estimate logic here.
",copybara-service[bot],2024-07-11 12:31:07+00:00,[],2024-07-11 12:31:07+00:00,,https://github.com/tensorflow/tensorflow/pull/71662,[],[],
2403058478,pull_request,closed,,[XLA:GPU] Remove sparse pass from ROCm Triton emitter,"[XLA:GPU] Remove sparse pass from ROCm Triton emitter

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14796 from openxla:skozub/gemm_fusion_autotuner_test 5005f288b67a2a34ec643cfcc3fbae815b5f0ef6
",copybara-service[bot],2024-07-11 12:27:46+00:00,[],2024-07-11 16:14:31+00:00,2024-07-11 16:14:30+00:00,https://github.com/tensorflow/tensorflow/pull/71661,[],[],
2403055002,pull_request,closed,,Remove upstream simplifier.,"Remove upstream simplifier.

The upstream simplifier is about 2/3 of the total runtime of simplification,
but it doesn't really do much for us.

Tested with a couple of fuzz tests that are not submitted yet:
- one that verifies that simplify(mlir-simplify(simplify) == simplify
- one that verifies correctness by enumerating the domain

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14364 from ROCm:ci_add_multi_gpu_tests_20240701 326dc007a7b641be59e3265916fa5a528846f17b
",copybara-service[bot],2024-07-11 12:26:03+00:00,[],2024-07-11 18:05:27+00:00,2024-07-11 18:05:26+00:00,https://github.com/tensorflow/tensorflow/pull/71660,[],[],
2403016598,pull_request,closed,,PR #14799: Skip TritonGemmTest.TestPreventMMAV3LoopUnrolling on non-Hopper devices,"PR #14799: Skip TritonGemmTest.TestPreventMMAV3LoopUnrolling on non-Hopper devices

Imported from GitHub PR https://github.com/openxla/xla/pull/14799

""wgmma"" PTX instruction is only available on ""sm_90a"" target, so will work only on Hopper.
https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-module-directives-target
Copybara import of the project:

--
fdf64b3c5ff0a5fd07ee32b575fec7a8de54d8c6 by Sergey Kozub <skozub@nvidia.com>:

Skip TritonGemmTest.TestPreventMMAV3LoopUnrolling on non-Hopper devices

Merging this change closes #14799

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14799 from openxla:skozub/ir_emitter_triton_test fdf64b3c5ff0a5fd07ee32b575fec7a8de54d8c6
",copybara-service[bot],2024-07-11 12:06:57+00:00,[],2024-07-11 17:05:47+00:00,2024-07-11 17:05:46+00:00,https://github.com/tensorflow/tensorflow/pull/71659,[],[],
2402969004,pull_request,closed,,Internal code change.,"Internal code change.
",copybara-service[bot],2024-07-11 11:42:01+00:00,[],2024-07-12 15:44:20+00:00,2024-07-12 15:44:20+00:00,https://github.com/tensorflow/tensorflow/pull/71658,[],[],
2402962947,pull_request,closed,,PR #13479: [ROCM] adding new ROCM-6.2 features: hipGetFuncBySymbol and error codes,"PR #13479: [ROCM] adding new ROCM-6.2 features: hipGetFuncBySymbol and error codes

Imported from GitHub PR https://github.com/openxla/xla/pull/13479

In this PR we enable some new rocm-6.2 features: mainly the missing **hipGetFuncBySymbol** in rocm_runtime, so that we had to the workaround. This affects only rocm-specific files.

@xla-rotation: could you have a look please ?
Copybara import of the project:

--
bcd2b2341887d305583161a592c23750f5ee584c by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:

adding new ROCM-6.2 features

--
3eb5aa9c69e8905d9f408ab2a141130084670d29 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:

solving conflicts after rebase

--
09938d6c6b9a358e6571fb13acdc1623fe205a4e by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:

added blas get_version test

--
215b92dddd440f5bacee8d7f678e1f5138761e00 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:

added runtime_version from DeviceDescription

Merging this change closes #13479

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13479 from ROCm:ci_rocm_62_features 215b92dddd440f5bacee8d7f678e1f5138761e00
",copybara-service[bot],2024-07-11 11:38:41+00:00,[],2024-07-11 15:47:08+00:00,2024-07-11 15:47:07+00:00,https://github.com/tensorflow/tensorflow/pull/71657,[],[],
2402874779,pull_request,closed,,PR #14796: Fix gemm_fusion_autotuner_test on Hopper,"PR #14796: Fix gemm_fusion_autotuner_test on Hopper

Imported from GitHub PR https://github.com/openxla/xla/pull/14796

Updated result type and error thresholds for the SelectsSplitK test.
Previously this failed on Hopper.
Copybara import of the project:

--
5005f288b67a2a34ec643cfcc3fbae815b5f0ef6 by Sergey Kozub <skozub@nvidia.com>:

Fix gemm_fusion_autotuner_test on Hopper

Merging this change closes #14796

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14796 from openxla:skozub/gemm_fusion_autotuner_test 5005f288b67a2a34ec643cfcc3fbae815b5f0ef6
",copybara-service[bot],2024-07-11 10:50:23+00:00,[],2024-07-11 14:30:54+00:00,2024-07-11 14:30:53+00:00,https://github.com/tensorflow/tensorflow/pull/71656,[],[],
2402860024,pull_request,closed,,Mark gloo_collectives_test with tag nomac.,"Mark gloo_collectives_test with tag nomac.

gloo is not supported on MacOS.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14792 from ROCm:ci_20240711 0f4236ca8a3767666ce03713fd7ae9e4d1254e5c
",copybara-service[bot],2024-07-11 10:42:10+00:00,['akuegel'],2024-07-11 13:40:53+00:00,2024-07-11 13:40:51+00:00,https://github.com/tensorflow/tensorflow/pull/71655,[],[],
2402793323,pull_request,open,,Debugging xla passes for Shardy,"Debugging xla passes for Shardy
",copybara-service[bot],2024-07-11 10:07:34+00:00,[],2024-07-11 10:07:34+00:00,,https://github.com/tensorflow/tensorflow/pull/71653,[],[],
2402688738,pull_request,closed,,PR #14792: [ROCM ] hotfix ROCm build  ,"PR #14792: [ROCM ] hotfix ROCm build  

Imported from GitHub PR https://github.com/openxla/xla/pull/14792

related rocm part change is missing and internal CL is merged without check due to this https://github.com/openxla/xla/commit/c40dbf2b3c93cce42370a971427b5dc65c6dcbfa

@xla-rotation @gflegar @beckerhe 

Thanks in advance!

Copybara import of the project:

--
0f4236ca8a3767666ce03713fd7ae9e4d1254e5c by Chao Chen <cchen104@amd.com>:

fixed build due to https://github.com/openxla/xla/commit/c40dbf2b3c93cce42370a971427b5dc65c6dcbfa

Merging this change closes #14792

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14792 from ROCm:ci_20240711 0f4236ca8a3767666ce03713fd7ae9e4d1254e5c
",copybara-service[bot],2024-07-11 09:19:58+00:00,[],2024-07-11 11:29:15+00:00,2024-07-11 11:29:14+00:00,https://github.com/tensorflow/tensorflow/pull/71652,[],[],
2402655251,pull_request,closed,,Simplifier optimizations.,"Simplifier optimizations.

- minimize storage uniquer invocations
- don't allocate std::functions
- don't put symbol and dims ranges in dense map in RangeEvaluator,
  also don't put them in a vector first.

After this, the biggest thing left to to is to remove the MLIR simplifier,
which is now responsible for 2/3 or so of the runtime of simplify.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14725 from shawnwang18:shawnw/mha_backward_cmd_buffer 9dd82651d0434beab21bed16ab2edea06611f8a0
",copybara-service[bot],2024-07-11 09:04:12+00:00,[],2024-07-11 11:21:51+00:00,2024-07-11 11:21:50+00:00,https://github.com/tensorflow/tensorflow/pull/71651,[],[],
2402584561,pull_request,closed,,Fix remaining build issues in the gpu directory for non-gpu builds.,"Fix remaining build issues in the gpu directory for non-gpu builds.
",copybara-service[bot],2024-07-11 08:29:23+00:00,['akuegel'],2024-07-11 10:40:13+00:00,2024-07-11 10:40:12+00:00,https://github.com/tensorflow/tensorflow/pull/71650,[],[],
2402523241,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 07:57:39+00:00,[],2024-07-12 09:03:17+00:00,2024-07-12 09:03:16+00:00,https://github.com/tensorflow/tensorflow/pull/71649,[],[],
2402506967,pull_request,closed,,Reduce number of simplify calls in multi-result affine map simplifier.,"Reduce number of simplify calls in multi-result affine map simplifier.

Currently, we rerun the simplifier for all results, even when only one changes.
Also, we rerun our simplifier in the last round (when the upstream simplifier
does not find any more changes), but it's not necessary, since Simplify is
idempotent.
",copybara-service[bot],2024-07-11 07:48:56+00:00,[],2024-07-11 10:01:37+00:00,2024-07-11 10:01:37+00:00,https://github.com/tensorflow/tensorflow/pull/71648,[],[],
2402503075,pull_request,open,,PR #14792: [ROCM ] hotfix ROCm build  ,"PR #14792: [ROCM ] hotfix ROCm build  

Imported from GitHub PR https://github.com/openxla/xla/pull/14792

related rocm part change is missing and internal CL is merged without check due to this https://github.com/openxla/xla/commit/c40dbf2b3c93cce42370a971427b5dc65c6dcbfa

@xla-rotation @gflegar @beckerhe 

Thanks in advance!

Copybara import of the project:

--
0f4236ca8a3767666ce03713fd7ae9e4d1254e5c by Chao Chen <cchen104@amd.com>:

fixed build due to https://github.com/openxla/xla/commit/c40dbf2b3c93cce42370a971427b5dc65c6dcbfa

Merging this change closes #14792

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14792 from ROCm:ci_20240711 0f4236ca8a3767666ce03713fd7ae9e4d1254e5c
",copybara-service[bot],2024-07-11 07:46:44+00:00,[],2024-07-11 09:51:11+00:00,,https://github.com/tensorflow/tensorflow/pull/71647,[],[],
2402479930,pull_request,closed,,[XLA:GPU][MLIR-based emitter] Set unroll factor to 1 for scatters.,"[XLA:GPU][MLIR-based emitter] Set unroll factor to 1 for scatters.
",copybara-service[bot],2024-07-11 07:34:04+00:00,['pifon2a'],2024-07-11 10:27:10+00:00,2024-07-11 10:27:09+00:00,https://github.com/tensorflow/tensorflow/pull/71646,[],[],
2402446732,pull_request,closed,,[XLA:GatherScatter] Support batch dimension in Gather/Scatter HLO syntax. (0/N),"[XLA:GatherScatter] Support batch dimension in Gather/Scatter HLO syntax. (0/N)
",copybara-service[bot],2024-07-11 07:17:09+00:00,['Tongfei-Guo'],2024-08-08 01:20:35+00:00,2024-08-08 01:20:34+00:00,https://github.com/tensorflow/tensorflow/pull/71645,[],[],
2402445690,pull_request,closed,,Avoid compile errors in builds without GPU configured.,"Avoid compile errors in builds without GPU configured.

Currently, triton_test_util depends on ir_emitter_triton unconditionally, but
ir_emitter_triton only gives access to the ir_emitter_triton.h header in builds
with a GPU configured.
We can make the ir_emitter_triton.h header available in all builds if we add a
stub implementation that returns errors.
",copybara-service[bot],2024-07-11 07:16:35+00:00,['akuegel'],2024-07-11 09:32:50+00:00,2024-07-11 09:32:49+00:00,https://github.com/tensorflow/tensorflow/pull/71644,[],[],
2402421477,pull_request,closed,,Reverts 8a7a7a3f0e6d6b6474f6a091b17590de425baec7,"Reverts 8a7a7a3f0e6d6b6474f6a091b17590de425baec7
",copybara-service[bot],2024-07-11 07:02:58+00:00,['Tongfei-Guo'],2024-07-12 00:40:24+00:00,2024-07-12 00:40:23+00:00,https://github.com/tensorflow/tensorflow/pull/71643,[],[],
2402403548,pull_request,closed,,Integrate LLVM at llvm/llvm-project@694b132177a9,"Integrate LLVM at llvm/llvm-project@694b132177a9

Updates LLVM usage to match
[694b132177a9](https://github.com/llvm/llvm-project/commit/694b132177a9)
",copybara-service[bot],2024-07-11 06:52:29+00:00,[],2024-07-11 12:40:17+00:00,2024-07-11 12:40:16+00:00,https://github.com/tensorflow/tensorflow/pull/71642,[],[],
2402394661,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14792 from ROCm:ci_20240711 0f4236ca8a3767666ce03713fd7ae9e4d1254e5c
",copybara-service[bot],2024-07-11 06:47:30+00:00,[],2024-07-11 11:33:29+00:00,,https://github.com/tensorflow/tensorflow/pull/71641,[],[],
2402359034,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 06:27:29+00:00,[],2024-07-11 06:27:29+00:00,,https://github.com/tensorflow/tensorflow/pull/71640,[],[],
2402358895,pull_request,closed,,Add support for libnvjitlink,"Add support for libnvjitlink

This is preparing the CUDA backend for linking and compiling with libnvjitlink. The plan is to replace ptxas and nvlink command line tools eventually.

This change is so far only adding a function `CompileAndLinkUsingLibNvJitLink`, but it's not yet being used (outside of the corresponding unit tests).
",copybara-service[bot],2024-07-11 06:27:23+00:00,[],2024-07-11 10:33:43+00:00,2024-07-11 10:33:42+00:00,https://github.com/tensorflow/tensorflow/pull/71639,[],[],
2402277269,pull_request,closed,,[NCCL] Upgrade nvtx dependency,"[NCCL] Upgrade nvtx dependency
",copybara-service[bot],2024-07-11 05:24:08+00:00,[],2024-07-12 07:18:15+00:00,2024-07-12 07:18:15+00:00,https://github.com/tensorflow/tensorflow/pull/71637,[],[],
2402254315,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 05:02:24+00:00,[],2024-07-12 04:24:29+00:00,,https://github.com/tensorflow/tensorflow/pull/71635,[],[],
2402242073,pull_request,closed,,support bad_indices_policy for TensorScatterOp and ScatterNdUpdateOp,"support bad_indices_policy for TensorScatterOp and ScatterNdUpdateOp

This CL introduces the bad_indices_policy as ScatterNd already had.
",copybara-service[bot],2024-07-11 04:50:28+00:00,[],2024-07-22 14:05:26+00:00,2024-07-22 14:05:25+00:00,https://github.com/tensorflow/tensorflow/pull/71634,[],[],
2402235368,pull_request,closed,,PR #14725: [XLA:GPU] Lowering FusedMHABackward thunk to command buffer,"PR #14725: [XLA:GPU] Lowering FusedMHABackward thunk to command buffer

Imported from GitHub PR https://github.com/openxla/xla/pull/14725

This PR lowers FusedMHABackwardThunk into command buffer, the command buffer lowering knob is DebugOptions::CUDNN.
Copybara import of the project:

--
ff9156f57569cb5e88a4671a110365e79c9f857f by Shawn Wang <shawnw@nvidia.com>:

support lowering fusedMHABackward to command buffer

--
83ddf0cbadf5f0f9513c67e7bbdd7ecea4f3404c by Shawn Wang <shawnw@nvidia.com>:

fix rebase conflicts

--
9dd82651d0434beab21bed16ab2edea06611f8a0 by Shawn Wang <shawnw@nvidia.com>:

remove duplicated inclusion

Merging this change closes #14725

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14725 from shawnwang18:shawnw/mha_backward_cmd_buffer 9dd82651d0434beab21bed16ab2edea06611f8a0
",copybara-service[bot],2024-07-11 04:43:32+00:00,[],2024-07-11 09:41:11+00:00,2024-07-11 09:41:11+00:00,https://github.com/tensorflow/tensorflow/pull/71633,[],[],
2402217437,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:24:43+00:00,[],2024-07-11 04:24:43+00:00,,https://github.com/tensorflow/tensorflow/pull/71631,[],[],
2402214188,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:21:10+00:00,[],2024-07-11 04:21:10+00:00,,https://github.com/tensorflow/tensorflow/pull/71630,[],[],
2402213007,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13799 from ROCm:ci_add_dot_algorithms_for_amd_gpus_20240614 b0274e9d02a2aa584c68cd0b69ae78607e8907a2
",copybara-service[bot],2024-07-11 04:19:45+00:00,[],2024-07-12 08:32:24+00:00,,https://github.com/tensorflow/tensorflow/pull/71629,[],[],
2402211472,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:18:04+00:00,[],2024-07-16 05:58:55+00:00,2024-07-16 05:58:53+00:00,https://github.com/tensorflow/tensorflow/pull/71628,[],[],
2402209762,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2024-07-11 04:16:35+00:00,[],2024-07-17 03:04:29+00:00,2024-07-17 03:04:28+00:00,https://github.com/tensorflow/tensorflow/pull/71627,[],[],
2402207197,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:13:48+00:00,[],2024-07-16 06:05:38+00:00,2024-07-16 06:05:36+00:00,https://github.com/tensorflow/tensorflow/pull/71626,[],[],
2402204658,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:10:52+00:00,[],2024-07-11 04:10:52+00:00,,https://github.com/tensorflow/tensorflow/pull/71625,[],[],
2402203503,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:09:32+00:00,[],2024-07-11 08:31:26+00:00,2024-07-11 08:31:25+00:00,https://github.com/tensorflow/tensorflow/pull/71624,[],[],
2402199130,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:04:52+00:00,[],2024-07-11 04:04:52+00:00,,https://github.com/tensorflow/tensorflow/pull/71623,[],[],
2402198412,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 04:04:05+00:00,[],2024-07-12 06:26:29+00:00,2024-07-12 06:26:29+00:00,https://github.com/tensorflow/tensorflow/pull/71622,[],[],
2402193257,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 03:58:33+00:00,[],2024-07-11 04:30:55+00:00,,https://github.com/tensorflow/tensorflow/pull/71621,[],[],
2402193188,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-11 03:58:28+00:00,[],2024-07-11 04:20:33+00:00,,https://github.com/tensorflow/tensorflow/pull/71620,[],[],
