id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2477273539,pull_request,closed,,[XLA:GPU] Remove root parameter of GetDescriptionForTiledTransposeEmitter().,"[XLA:GPU] Remove root parameter of GetDescriptionForTiledTransposeEmitter().

This parameter has become unused.
",copybara-service[bot],2024-08-21 07:22:40+00:00,['akuegel'],2024-08-21 12:38:29+00:00,2024-08-21 12:38:28+00:00,https://github.com/tensorflow/tensorflow/pull/74201,[],[],
2477252846,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16279 from knightXun:DEC-NFC 9befa8231c42219b8a1d1aaeb342346d48513b0d
",copybara-service[bot],2024-08-21 07:11:43+00:00,[],2024-08-21 10:29:19+00:00,,https://github.com/tensorflow/tensorflow/pull/74200,[],[],
2477247007,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 07:08:34+00:00,[],2024-08-21 09:55:28+00:00,,https://github.com/tensorflow/tensorflow/pull/74199,[],[],
2477243654,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 07:06:48+00:00,[],2024-08-21 08:54:31+00:00,2024-08-21 08:54:30+00:00,https://github.com/tensorflow/tensorflow/pull/74198,[],[],
2477217384,pull_request,closed,,Add direct legalizations for concat op,"Add direct legalizations for concat op
",copybara-service[bot],2024-08-21 06:52:25+00:00,['LukeBoyer'],2024-08-24 00:27:32+00:00,2024-08-24 00:27:31+00:00,https://github.com/tensorflow/tensorflow/pull/74197,[],[],
2477212328,pull_request,closed,,Add direct legalization for round nearest even,"Add direct legalization for round nearest even
",copybara-service[bot],2024-08-21 06:49:33+00:00,['LukeBoyer'],2024-08-24 00:35:20+00:00,2024-08-24 00:35:19+00:00,https://github.com/tensorflow/tensorflow/pull/74196,[],[],
2477184323,pull_request,closed,,PR #16095: Create overloads of cub::ThreadLoadVolatilePointer instead of template specializations,"PR #16095: Create overloads of cub::ThreadLoadVolatilePointer instead of template specializations

Imported from GitHub PR https://github.com/openxla/xla/pull/16095

This avoids a build failure with an upcoming CUDA change.
Commit https://github.com/NVIDIA/cccl/commit/6dfc8dddaef5f01adeb683790bf0c8ec05302460 changed the method signature and breaks our specializations.
Specializing cub:: templates is against the CCCL guidelines: https://github.com/NVIDIA/cccl?tab=readme-ov-file#compatibility-guidelines

The workaround here was suggested by someone on the CUDA team.
It is also against the guidelines, since it is adding to the cub:: namespace, but it fixes the compilation failure.
The more futureproof solution is to open a feature request with them (https://github.com/NVIDIA/cccl/issues) to support loading of `Eigen::half` and `tsl::bfloat16` in CUB directly.
Copybara import of the project:

--
81cc5a8bb3aa330f73419e9485fd0458cb1d6116 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Create overloads of cub::ThreadLoadVolatilePointer instead of template specializations, to avoid build failure with upcoming CUDA change.

Commit https://github.com/NVIDIA/cccl/commit/6dfc8dddaef5f01adeb683790bf0c8ec05302460 changed the method signature and breaks our specializations.
Specializing cub:: templates is against the CCCL guidelines: https://github.com/NVIDIA/cccl?tab=readme-ov-file#compatibility-guidelines

The workaround here was suggested by someone on the CUDA team. It is also against the guidelines, since it is adding to the cub:: namespace,
but it fixes the compilation failure. The more futureproof solution is to open a feature request with them to support loading of `Eigen::half` and `tsl::bfloat16` in CUB directly.
https://github.com/NVIDIA/cccl/issues

Merging this change closes #16095

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16095 from dimvar:cub-templates 81cc5a8bb3aa330f73419e9485fd0458cb1d6116
",copybara-service[bot],2024-08-21 06:32:05+00:00,[],2024-08-21 07:04:09+00:00,2024-08-21 07:04:08+00:00,https://github.com/tensorflow/tensorflow/pull/74195,[],[],
2477179507,pull_request,closed,,Add direct legalizations for dynamic broadcast in dims.,"Add direct legalizations for dynamic broadcast in dims.
",copybara-service[bot],2024-08-21 06:29:32+00:00,['LukeBoyer'],2024-08-24 00:12:26+00:00,2024-08-24 00:12:25+00:00,https://github.com/tensorflow/tensorflow/pull/74194,[],[],
2477178837,pull_request,closed,,Direct legalizations for broadcast in dims,"Direct legalizations for broadcast in dims
",copybara-service[bot],2024-08-21 06:29:16+00:00,['LukeBoyer'],2024-08-24 00:05:18+00:00,2024-08-24 00:05:18+00:00,https://github.com/tensorflow/tensorflow/pull/74193,[],[],
2477167631,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 06:23:44+00:00,[],2024-08-22 05:18:01+00:00,,https://github.com/tensorflow/tensorflow/pull/74192,[],[],
2477145416,pull_request,open,,Use SLM for depthwise op of OpenCL,"The depthwise conv kernel weights can be reused in a single workgroup. Then, use the local memory to cache these weights to accelerate the inference.",yzhou51,2024-08-21 06:09:55+00:00,['gbaned'],2025-01-16 06:31:28+00:00,,https://github.com/tensorflow/tensorflow/pull/74191,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2323990782, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 2, 7, 18, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354449589, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 17, 3, 50, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367671124, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 23, 9, 27, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384870075, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 1, 6, 1, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399583439, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 8, 11, 27, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431472160, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @vamsimanchala, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 23, 9, 29, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469835377, 'issue_id': 2477145416, 'author': 'keerthanakadiri', 'body': 'Hi @majiddadashi , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 12, 8, 1, 12, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-09-02 07:18:19 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-17 03:50:36 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-23 09:27:25 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 06:01:29 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-08 11:27:02 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-23 09:29:22 UTC): Hi @vamsimanchala, Can you please review this PR? Thank you !

keerthanakadiri on (2024-11-12 08:01:12 UTC): Hi @majiddadashi , Can you please review this PR? Thank you !

"
2477103808,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 05:37:14+00:00,[],2024-08-21 09:58:01+00:00,2024-08-21 09:58:00+00:00,https://github.com/tensorflow/tensorflow/pull/74190,[],[],
2477093822,pull_request,closed,,Parallelizing ScanAndTranslate Op to improve performance. Time/op goes down by ~80%,"Parallelizing ScanAndTranslate Op to improve performance. Time/op goes down by ~80%
",copybara-service[bot],2024-08-21 05:28:32+00:00,['rohan100jain'],2024-08-21 21:06:51+00:00,2024-08-21 21:06:50+00:00,https://github.com/tensorflow/tensorflow/pull/74189,[],[],
2477077192,pull_request,closed,,Cast i64->i32 when lowering GetDimensionSize.,"Cast i64->i32 when lowering GetDimensionSize.
",copybara-service[bot],2024-08-21 05:13:58+00:00,['LukeBoyer'],2024-08-21 20:27:11+00:00,2024-08-21 20:27:10+00:00,https://github.com/tensorflow/tensorflow/pull/74188,[],[],
2477055818,pull_request,closed,,Add direct legalizations for reverse,"Add direct legalizations for reverse
",copybara-service[bot],2024-08-21 04:55:29+00:00,['LukeBoyer'],2024-08-23 21:55:46+00:00,2024-08-23 21:55:46+00:00,https://github.com/tensorflow/tensorflow/pull/74187,[],[],
2476985860,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:51:01+00:00,[],2024-08-21 03:51:01+00:00,,https://github.com/tensorflow/tensorflow/pull/74186,[],[],
2476984303,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:49:23+00:00,[],2024-08-21 03:49:23+00:00,,https://github.com/tensorflow/tensorflow/pull/74185,[],[],
2476983411,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:48:30+00:00,[],2024-08-21 03:48:30+00:00,,https://github.com/tensorflow/tensorflow/pull/74184,[],[],
2476982341,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:47:24+00:00,[],2024-08-21 03:47:24+00:00,,https://github.com/tensorflow/tensorflow/pull/74183,[],[],
2476980420,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:45:25+00:00,[],2024-08-21 03:45:25+00:00,,https://github.com/tensorflow/tensorflow/pull/74182,[],[],
2476980125,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:45:06+00:00,[],2024-08-21 04:36:31+00:00,,https://github.com/tensorflow/tensorflow/pull/74181,[],[],
2476978932,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:43:52+00:00,[],2024-08-21 03:43:52+00:00,,https://github.com/tensorflow/tensorflow/pull/74180,[],[],
2476976619,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:42:16+00:00,[],2024-08-21 03:42:16+00:00,,https://github.com/tensorflow/tensorflow/pull/74179,[],[],
2476975187,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:41:11+00:00,[],2024-08-21 03:41:11+00:00,,https://github.com/tensorflow/tensorflow/pull/74178,[],[],
2476973736,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:39:42+00:00,[],2024-08-21 06:14:00+00:00,,https://github.com/tensorflow/tensorflow/pull/74177,[],[],
2476968890,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:34:56+00:00,[],2024-08-21 03:34:56+00:00,,https://github.com/tensorflow/tensorflow/pull/74176,[],[],
2476967835,pull_request,closed,,Automated Code Change,"Automated Code Change

Reverts changelist 613649570
",copybara-service[bot],2024-08-21 03:33:56+00:00,[],2024-08-27 09:09:47+00:00,2024-08-27 09:09:46+00:00,https://github.com/tensorflow/tensorflow/pull/74175,[],[],
2476967273,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:33:23+00:00,[],2024-08-21 03:33:23+00:00,,https://github.com/tensorflow/tensorflow/pull/74174,[],[],
2476953704,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-21 03:19:48+00:00,[],2024-08-27 07:11:34+00:00,2024-08-27 07:11:33+00:00,https://github.com/tensorflow/tensorflow/pull/74173,[],[],
2476818061,pull_request,closed,,Fix quantize_weights_portable.cc header,"Fix quantize_weights_portable.cc header

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16149 from ROCm:ci_workspace_fix_triangular_solve be3dddf99592d3c1474678a87adb1e5a4b8770bd
",copybara-service[bot],2024-08-21 01:45:26+00:00,['pak-laura'],2024-08-22 00:24:53+00:00,2024-08-22 00:24:52+00:00,https://github.com/tensorflow/tensorflow/pull/74172,[],[],
2476775540,pull_request,closed,,Creating a micro benchmark for the ScaleAndTranslateOp.,"Creating a micro benchmark for the ScaleAndTranslateOp.

Current numbers:
benchy --runs=10 third_party/tensorflow/core/kernels/image:scale_and_translate_op_benchmark_test
 10 / 10 [======================================================================================================================================================================] 100.00% 11s
(Generated by http://go/benchy. Settings: --runs 10 --compilation_mode """" --config ""benchmark"")

name                              cpu/op
BM_ScaleAndTranslateOp/real_time  45.4µs ± 6%

name                              time/op
BM_ScaleAndTranslateOp/real_time  15.5ms ± 1%

name                              INSTRUCTIONS/op
BM_ScaleAndTranslateOp/real_time    230M ± 0%

name                              CYCLES/op
BM_ScaleAndTranslateOp/real_time   57.1M ± 1%
",copybara-service[bot],2024-08-21 00:57:56+00:00,['rohan100jain'],2024-08-21 06:12:27+00:00,2024-08-21 06:12:26+00:00,https://github.com/tensorflow/tensorflow/pull/74171,[],[],
2476756828,pull_request,open,,Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
```

After:
```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit
```

Also, we can remove the hack (which I didn't like) in multihost_utils.py.
",copybara-service[bot],2024-08-21 00:32:46+00:00,['yashk2810'],2024-08-21 00:32:47+00:00,,https://github.com/tensorflow/tensorflow/pull/74169,[],[],
2476737530,pull_request,open,,Test CHLO decompositions for asin/acos,"Test CHLO decompositions for asin/acos
",copybara-service[bot],2024-08-21 00:08:29+00:00,['GleasonK'],2024-08-21 00:08:30+00:00,,https://github.com/tensorflow/tensorflow/pull/74168,[],[],
2476733105,pull_request,closed,,Skip meeting input and output type for zeros like in shape inference.,"Skip meeting input and output type for zeros like in shape inference.
",copybara-service[bot],2024-08-21 00:03:42+00:00,['LukeBoyer'],2024-08-21 21:50:10+00:00,2024-08-21 21:50:09+00:00,https://github.com/tensorflow/tensorflow/pull/74167,[],[],
2476717587,pull_request,open,,Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
```

After:
```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit
```

Also, we can remove the hack (which I didn't like) in multihost_utils.py.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15630 from Tixxx:tixxx/determine_local 31f79104d3bbc0413b8fea61dbeba8ec80419117
",copybara-service[bot],2024-08-20 23:49:10+00:00,['yashk2810'],2024-08-20 23:49:11+00:00,,https://github.com/tensorflow/tensorflow/pull/74166,[],[],
2476694904,pull_request,open,,PR #8589: Improve the accuracy of asin(x) and asinh(x) for complex x using modified Hull et al algorithm.,"PR #8589: Improve the accuracy of asin(x) and asinh(x) for complex x using modified Hull et al algorithm.

Imported from GitHub PR https://github.com/openxla/xla/pull/8589

As in the title.

~Fixes https://github.com/openxla/xla/issues/8553~ - PR https://github.com/openxla/xla/pull/9802 disabled the fix.

Update: the fix to https://github.com/openxla/xla/issues/8553 will be available via https://github.com/openxla/stablehlo/pull/2357
Copybara import of the project:

--
b3af7c00b48dadc8438086ed847ba8ae98a95f15 by Pearu Peterson <pearu.peterson@gmail.com>:

Improve the accuracy of asinh(z) for complex z with large absolute value.

--
4209a3e1d30e38656c944854c5e9de2bfd529b76 by Pearu Peterson <pearu.peterson@gmail.com>:

Implement the modified Hull et al algorithm for Asin and Asinh.

--
35c23555455021859b1f09a1f4dc9ac4b42a85dc by Pearu Peterson <pearu.peterson@gmail.com>:

Use functional_algorithms to generate Asin implementation

Merging this change closes #8589

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/8589 from pearu:pearu/asinh 35c23555455021859b1f09a1f4dc9ac4b42a85dc
",copybara-service[bot],2024-08-20 23:22:50+00:00,[],2024-08-21 22:51:51+00:00,,https://github.com/tensorflow/tensorflow/pull/74165,[],[],
2476666063,pull_request,closed,,correct the checks of the numbers in softsign_op_test,"correct the checks of the numbers in softsign_op_test
",copybara-service[bot],2024-08-20 22:51:09+00:00,[],2024-08-21 00:44:43+00:00,2024-08-21 00:44:42+00:00,https://github.com/tensorflow/tensorflow/pull/74164,[],[],
2476643811,pull_request,closed,,Delete `build_tools/docker` and `build_tools/github_actions`,"Delete `build_tools/docker` and `build_tools/github_actions`

These are files leftover from a microbenchmark action which has already been deleted
",copybara-service[bot],2024-08-20 22:33:25+00:00,['ddunl'],2024-08-21 19:00:05+00:00,2024-08-21 19:00:04+00:00,https://github.com/tensorflow/tensorflow/pull/74163,[],[],
2476632868,pull_request,closed,,Handle i64 for hlo.iota -> tfl.,"Handle i64 for hlo.iota -> tfl.
",copybara-service[bot],2024-08-20 22:24:32+00:00,['LukeBoyer'],2024-08-20 23:47:35+00:00,2024-08-20 23:47:34+00:00,https://github.com/tensorflow/tensorflow/pull/74162,[],[],
2476621731,pull_request,closed,,"Fix some numpy2.x incompatibilities in tf.numpy.sign, tf.numpy.linspace, and tf.numpy.logspace.","Fix some numpy2.x incompatibilities in tf.numpy.sign, tf.numpy.linspace, and tf.numpy.logspace.
",copybara-service[bot],2024-08-20 22:14:14+00:00,[],2024-08-21 01:10:05+00:00,2024-08-21 01:10:04+00:00,https://github.com/tensorflow/tensorflow/pull/74161,[],[],
2476609595,pull_request,closed,,Add if outline pass in TFLite converter.,"Add if outline pass in TFLite converter.
",copybara-service[bot],2024-08-20 22:03:57+00:00,[],2024-08-21 23:17:06+00:00,2024-08-21 23:17:04+00:00,https://github.com/tensorflow/tensorflow/pull/74160,[],[],
2476602135,pull_request,open,,Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
```

After:
```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit
```

Also, we can remove the hack (which I didn't like) in multihost_utils.py.
",copybara-service[bot],2024-08-20 21:57:45+00:00,[],2024-08-21 00:16:16+00:00,,https://github.com/tensorflow/tensorflow/pull/74159,[],[],
2476601322,pull_request,closed,,Includes instruction source files in the Auto Sharding request proto.,"Includes instruction source files in the Auto Sharding request proto.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16149 from ROCm:ci_workspace_fix_triangular_solve be3dddf99592d3c1474678a87adb1e5a4b8770bd
",copybara-service[bot],2024-08-20 21:56:59+00:00,[],2024-08-22 01:43:41+00:00,2024-08-22 01:43:41+00:00,https://github.com/tensorflow/tensorflow/pull/74158,[],[],
2476578818,pull_request,closed,,Migrate to using ErrorSpec::Builder in exhaustive tests to avoid designated initializers,"Migrate to using ErrorSpec::Builder in exhaustive tests to avoid designated initializers
",copybara-service[bot],2024-08-20 21:36:47+00:00,[],2024-08-22 09:56:53+00:00,2024-08-22 09:56:52+00:00,https://github.com/tensorflow/tensorflow/pull/74157,[],[],
2476569296,pull_request,closed,,Bump exhaustive test timeouts,"Bump exhaustive test timeouts
",copybara-service[bot],2024-08-20 21:29:15+00:00,[],2024-08-21 19:51:44+00:00,2024-08-21 19:51:44+00:00,https://github.com/tensorflow/tensorflow/pull/74156,[],[],
2476560665,pull_request,closed,,MHLO get_dimension_size legalization,"MHLO get_dimension_size legalization
",copybara-service[bot],2024-08-20 21:22:13+00:00,['turbotoribio'],2024-08-20 22:21:28+00:00,2024-08-20 22:21:27+00:00,https://github.com/tensorflow/tensorflow/pull/74155,[],[],
2476559537,pull_request,closed,,Improve exhaustive test debugging mode,"Improve exhaustive test debugging mode

Previously, we only updated one potential failure location to force a failed assert in debug mode. This updates all other places in `ExpectNear` to also throw the forced assert in debug mode.

Also updates floating point precision printing in the ErrorSpec debug logs to ensure we do not lose any possible digits that could distinguish `double` values.
",copybara-service[bot],2024-08-20 21:21:19+00:00,[],2024-08-20 22:26:41+00:00,2024-08-20 22:26:40+00:00,https://github.com/tensorflow/tensorflow/pull/74154,[],[],
2476503502,pull_request,closed,,Add support for sub-byte types in PjRt CPU's `TransferLiteralToBuffer`,"Add support for sub-byte types in PjRt CPU's `TransferLiteralToBuffer`

This is a follow-up of https://github.com/openxla/xla/pull/16240. `AbstractAsyncHostToHostMemoryTransferManager` is refactored slightly so that packed data can be written directly into the target buffer.

Also, `AbstractAsyncHostToHostMemoryTransferManager::TransferLiteralToBuffer` now fails explicitly if the literal does not have a major-to-minor layout. Since it uses raw memcpy, non-descending layouts have never been supported, but passing a literal with a wrong layout was causing the result to be silently wrong. Now the implementation explicitly returns an `UNIMPLEMENTED` error to avoid such confusion.
",copybara-service[bot],2024-08-20 20:39:05+00:00,[],2024-08-21 18:21:21+00:00,2024-08-21 18:21:17+00:00,https://github.com/tensorflow/tensorflow/pull/74153,[],[],
2476498728,pull_request,closed,,[xla:cpu] Split XLA:CPU LLVM module into parts in preparation for parallel compilation,"[xla:cpu] Split XLA:CPU LLVM module into parts in preparation for parallel compilation

For now simply split LLVM module into 2 parts to verify that XLA:CPU keep working and we never generate LLVM IR that LLVM can't correctly split.

Parallel compilation comes in followup CLs.
",copybara-service[bot],2024-08-20 20:35:42+00:00,['ezhulenev'],2024-08-21 20:35:26+00:00,2024-08-21 20:35:25+00:00,https://github.com/tensorflow/tensorflow/pull/74152,[],[],
2476479288,pull_request,closed,,Make node order identical to DFS when it sorts out-edges by name.,"Make node order identical to DFS when it sorts out-edges by name.
",copybara-service[bot],2024-08-20 20:22:16+00:00,[],2024-08-20 22:05:00+00:00,2024-08-20 22:05:00+00:00,https://github.com/tensorflow/tensorflow/pull/74151,[],[],
2476473107,pull_request,closed,,Fix stateless_random_ops_test.py for numpy2 compatiblity by using explicit casts to unsigned 64 bit numpy integers,"Fix stateless_random_ops_test.py for numpy2 compatiblity by using explicit casts to unsigned 64 bit numpy integers
",copybara-service[bot],2024-08-20 20:18:09+00:00,[],2024-08-21 03:06:19+00:00,2024-08-21 03:06:18+00:00,https://github.com/tensorflow/tensorflow/pull/74150,[],[],
2476388073,pull_request,closed,,Add direct legalization for mhlo.select.,"Add direct legalization for mhlo.select.
",copybara-service[bot],2024-08-20 19:28:23+00:00,['arfaian'],2024-08-21 23:26:43+00:00,2024-08-21 23:26:41+00:00,https://github.com/tensorflow/tensorflow/pull/74149,[],[],
2476354172,pull_request,open,,Reverts 72c615bc265431949f9a92fd5e2b4c57c5c524cc,"Reverts 72c615bc265431949f9a92fd5e2b4c57c5c524cc
",copybara-service[bot],2024-08-20 19:10:31+00:00,['sagunb'],2025-02-05 14:44:30+00:00,,https://github.com/tensorflow/tensorflow/pull/74148,"[('ready to pull', 'PR ready for merge process')]",[],
2476330661,pull_request,closed,,Set costs to mark strategies as invalid rather than removing them. Removing them can break the correspondence of strategies across follower-followeee pairs of nodes,"Set costs to mark strategies as invalid rather than removing them. Removing them can break the correspondence of strategies across follower-followeee pairs of nodes
",copybara-service[bot],2024-08-20 18:58:03+00:00,[],2024-08-20 20:10:51+00:00,2024-08-20 20:10:49+00:00,https://github.com/tensorflow/tensorflow/pull/74147,[],[],
2476310761,pull_request,closed,,Attempt to reduce flakiness on ARM64 builds by not installing parallel,"Attempt to reduce flakiness on ARM64 builds by not installing parallel
",copybara-service[bot],2024-08-20 18:48:14+00:00,['ddunl'],2024-08-20 20:56:26+00:00,2024-08-20 20:56:24+00:00,https://github.com/tensorflow/tensorflow/pull/74146,[],[],
2476267954,pull_request,open,,Integrate LLVM at llvm/llvm-project@f9031f00f2c9,"Integrate LLVM at llvm/llvm-project@f9031f00f2c9

Updates LLVM usage to match
[f9031f00f2c9](https://github.com/llvm/llvm-project/commit/f9031f00f2c9)
",copybara-service[bot],2024-08-20 18:25:48+00:00,[],2024-08-21 00:09:49+00:00,,https://github.com/tensorflow/tensorflow/pull/74145,[],[],
2476249528,pull_request,open,,Improve exhaustive test debugging mode,"Improve exhaustive test debugging mode

Previously, we only updated one potential failure location to force a failed assert in debug mode. This updates all other places in `ExpectNear` to also throw the forced assert in debug mode.

Also updates floating point precision printing in the ErrorSpec debug logs to ensure we do not lose any possible digits that could distinguish `double` values.
",copybara-service[bot],2024-08-20 18:14:58+00:00,[],2024-08-20 22:27:34+00:00,,https://github.com/tensorflow/tensorflow/pull/74144,[],[],
2476233680,pull_request,closed,,Add sanity check `bazel --version` to MacOS build script,"Add sanity check `bazel --version` to MacOS build script
",copybara-service[bot],2024-08-20 18:04:59+00:00,['ddunl'],2024-08-20 21:03:57+00:00,2024-08-20 21:03:56+00:00,https://github.com/tensorflow/tensorflow/pull/74143,[],[],
2476231299,pull_request,closed,,[RollForward][XLA][HostOffloader] Remove redundant copies to and from host for host offloaded computation outputs,"[RollForward][XLA][HostOffloader] Remove redundant copies to and from host for host offloaded computation outputs

The simple algorithm tracks usages of all outputs of each host offloaded computation. For each:
- If they are ONLY used on the host and they are outputs of the entry computation, it sets the memory space to Host.
- If they are ONLY used on the host, but are temporaries, no changes are made.
- For cases replaced, if a MoveToHost is found (NOTE: that the algorithm does not explicitly check that any exist nor that all paths
lead to a MoveToHost) for an output that is only used on the host, we simply replace the usage.

Reverts dba95d6a678db59f7cadbc976de02016a75e5b0f

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16291 from openxla:cudnn_fe_161 fc63a514cd3885cad5e727670f67195cc68cab54
",copybara-service[bot],2024-08-20 18:03:26+00:00,[],2024-08-22 22:38:49+00:00,2024-08-22 22:38:48+00:00,https://github.com/tensorflow/tensorflow/pull/74142,[],[],
2476181353,pull_request,closed,,PR #15460: Add the host memory deallocation in GpuExecutor::Deallocate,"PR #15460: Add the host memory deallocation in GpuExecutor::Deallocate

Imported from GitHub PR https://github.com/openxla/xla/pull/15460

This CL adds the missing host memory deallocation according to the pointer's host memory space allocated by GpuExecutor::Allocate() .
Copybara import of the project:

--
ceb25a951e758b92ef317b680945693dc935f5cd by Jane Liu <janeliu@nvidia.com>:

Add the host memory deallocation in GpuDriver

--
dc7d76d1244f12494ea9385ec63f39f274fe1b20 by Jane Liu <janeliu@nvidia.com>:

Add the unit test

--
e0443385eb7bacc852ab800aad9707c48315f480 by Jane Liu <janeliu@nvidia.com>:

function Deallocate does not return a value

Merging this change closes #15460

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15460 from zhenying-liu:memfix e0443385eb7bacc852ab800aad9707c48315f480
",copybara-service[bot],2024-08-20 17:33:29+00:00,[],2024-08-21 18:37:26+00:00,2024-08-21 18:37:25+00:00,https://github.com/tensorflow/tensorflow/pull/74141,[],[],
2476156103,pull_request,closed,,PR #15630: [NVIDIA GPU] Use cuda runtime api to determine if 2 ranks are on the same host,"PR #15630: [NVIDIA GPU] Use cuda runtime api to determine if 2 ranks are on the same host

Imported from GitHub PR https://github.com/openxla/xla/pull/15630

The current logic in nccl clique sets is_local to true by looking at the number of local participants and total devices in the clique. It's been used to determine if replica group is a local group, but this doesn't always translates to a local communicator.
i.e on a 8-gpu machine, if we have a group for collective permute
replica_group={{0,1},{1,0},{9,10},{10,9}}

In XLA's perspective, this is not a local replica group, but the communicators we create are only for rank (0,1) and (10,9), which are both local communicators in nccl perspective.
This pr uses the cuda runtime api to get the number of devices on a host and use the current rank id to determine if source and targets are located in the same host in collective permute thunk.
Copybara import of the project:

--
f7a7d5d7d658a7c19a66a61a5b6fcc73de5bd995 by TJ Xu <tjx@nvidia.com>:

Use cuda runtime api to determine if 2 ranks are on the same host or
not.

--
c14e0f5d4ec13af975ad33267ad6bc2a5f1e6c66 by TJ Xu <tjx@nvidia.com>:

changed a typo

--
707bfbf811b0cf80638f190ef9128dd525f3baf8 by TJ Xu <tjx@nvidia.com>:

Changed the GetDeviceCount function in executor interface to return
unimplemented

--
490b2f5d097b50ec5e6da6738d40a071bf6bdb5a by TJ Xu <tjx@nvidia.com>:

removed new device count api to SE

--
f2e15de29d433cc0d5db258acb6f2ec3f15bfa22 by TJ Xu <tjx@nvidia.com>:

pass local device count through executable options

--
2dc0d5cee445a4a882d3fc5078d591c3b72234e5 by TJ Xu <tjx@nvidia.com>:

Updated comment for IsLocalPeerTransfer

--
31f79104d3bbc0413b8fea61dbeba8ec80419117 by TJ Xu <tjx@nvidia.com>:

change check to check_gt

Merging this change closes #15630

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15630 from Tixxx:tixxx/determine_local 31f79104d3bbc0413b8fea61dbeba8ec80419117
",copybara-service[bot],2024-08-20 17:17:41+00:00,[],2024-08-20 23:41:46+00:00,2024-08-20 23:41:46+00:00,https://github.com/tensorflow/tensorflow/pull/74140,[],[],
2476154699,pull_request,closed,,Integrate LLVM at llvm/llvm-project@f9031f00f2c9,"Integrate LLVM at llvm/llvm-project@f9031f00f2c9

Updates LLVM usage to match
[f9031f00f2c9](https://github.com/llvm/llvm-project/commit/f9031f00f2c9)
",copybara-service[bot],2024-08-20 17:16:53+00:00,[],2024-08-21 12:59:28+00:00,2024-08-21 12:59:27+00:00,https://github.com/tensorflow/tensorflow/pull/74139,[],[],
2476145742,pull_request,closed,,[xla:cpu] Forward thunk execution session lock to async continuation,"[xla:cpu] Forward thunk execution session lock to async continuation

Do not waste CPU cycles on expensive execution session lock creation (atomic operation) if we have a lock that can be forwarded to async continuation.

name                                               old cpu/op   new cpu/op   delta
BM_FifoReadyQueuePushPop/1/process_time            7.09ns ± 6%  6.95ns ± 7%   -1.90%
BM_FifoReadyQueuePushPop/2/process_time            14.1ns ± 5%  13.9ns ± 6%   -1.70%
BM_FifoReadyQueuePushPop/4/process_time            25.8ns ± 5%  25.2ns ± 6%   -2.50%
BM_FifoReadyQueuePushPop/8/process_time            58.9ns ± 4%  58.1ns ± 5%   -1.27%
BM_FifoReadyQueuePushPop/16/process_time            104ns ± 4%   103ns ± 4%   -0.93%
BM_FifoReadyQueuePushPopHalf/1/process_time        11.4ns ± 1%  11.5ns ± 1%   +1.20%
BM_FifoReadyQueuePushPopHalf/2/process_time        13.8ns ± 1%  13.7ns ± 1%     ~   
BM_FifoReadyQueuePushPopHalf/4/process_time        18.9ns ± 1%  18.8ns ± 1%     ~   
BM_FifoReadyQueuePushPopHalf/8/process_time        31.7ns ± 6%  31.6ns ± 2%     ~   
BM_FifoReadyQueuePushPopHalf/16/process_time       60.9ns ± 2%  61.2ns ± 2%   +0.49%
BM_PriorityReadyQueuePushPop/1/process_time        6.52ns ± 1%  5.92ns ± 1%   -9.29%
BM_PriorityReadyQueuePushPop/2/process_time        14.8ns ±13%  12.6ns ± 2%  -14.95%
BM_PriorityReadyQueuePushPop/4/process_time        35.9ns ±11%  31.2ns ± 2%  -13.31%
BM_PriorityReadyQueuePushPop/8/process_time        89.2ns ± 4%  78.9ns ± 3%  -11.56%
BM_PriorityReadyQueuePushPop/16/process_time        190ns ± 2%   192ns ± 2%   +0.62%
BM_PriorityReadyQueuePushPopHalf/1/process_time    19.5ns ± 2%  19.5ns ± 1%     ~   
BM_PriorityReadyQueuePushPopHalf/2/process_time    28.0ns ± 1%  27.1ns ± 7%   -3.53%
BM_PriorityReadyQueuePushPopHalf/4/process_time    53.0ns ± 1%  51.3ns ± 2%   -3.09%
BM_PriorityReadyQueuePushPopHalf/8/process_time     137ns ± 1%   139ns ± 2%   +1.26%
BM_PriorityReadyQueuePushPopHalf/16/process_time    290ns ± 2%   298ns ± 1%   +2.68%
BM_SequentialThunkExecutor/1/process_time          28.9ns ± 1%  28.9ns ± 2%     ~   
BM_SequentialThunkExecutor/2/process_time           129ns ± 1%   122ns ± 5%   -5.18%
BM_SequentialThunkExecutor/4/process_time           202ns ± 1%   194ns ± 2%   -3.98%
BM_SequentialThunkExecutor/8/process_time           354ns ± 2%   340ns ± 1%   -3.82%
BM_SequentialThunkExecutor/16/process_time          686ns ± 2%   667ns ± 3%   -2.83%
BM_SequentialThunkExecutor/32/process_time         1.36µs ± 3%  1.33µs ± 5%   -1.76%
BM_SequentialThunkExecutor/64/process_time         2.78µs ± 6%  2.76µs ± 4%     ~   
BM_SequentialThunkExecutor/128/process_time        5.61µs ± 5%  5.61µs ± 5%     ~   
BM_SequentialThunkExecutor/256/process_time        11.7µs ± 3%  11.7µs ± 5%     ~   
BM_SequentialThunkExecutor/512/process_time        25.9µs ± 4%  26.0µs ± 7%     ~   
BM_SyncThunkExecutor/1/process_time                28.8ns ± 1%  28.7ns ± 1%   -0.32%
BM_SyncThunkExecutor/2/process_time                 120ns ± 1%   105ns ± 3%  -12.33%
BM_SyncThunkExecutor/4/process_time                 195ns ± 2%   181ns ± 4%   -7.02%
BM_SyncThunkExecutor/8/process_time                 356ns ± 1%   336ns ± 2%   -5.80%
BM_SyncThunkExecutor/16/process_time                681ns ± 2%   660ns ± 2%   -3.07%
BM_SyncThunkExecutor/32/process_time               1.34µs ± 2%  1.32µs ± 1%   -1.62%
BM_SyncThunkExecutor/64/process_time               2.79µs ± 2%  2.77µs ± 2%   -0.90%
BM_SyncThunkExecutor/128/process_time              5.77µs ± 1%  5.75µs ± 2%   -0.34%
BM_SyncThunkExecutor/256/process_time              11.8µs ± 1%  11.8µs ± 2%     ~   
BM_SyncThunkExecutor/512/process_time              25.1µs ± 3%  25.2µs ± 4%     ~   
BM_AsyncThunkExecutor/1/process_time               14.1µs ±11%  14.2µs ± 9%     ~   
BM_AsyncThunkExecutor/2/process_time               13.3µs ±27%  12.9µs ±25%     ~   
BM_AsyncThunkExecutor/4/process_time               22.2µs ±21%  22.3µs ±24%     ~   
BM_AsyncThunkExecutor/8/process_time               53.8µs ±15%  54.3µs ±17%     ~   
BM_AsyncThunkExecutor/16/process_time              88.4µs ±22%  90.6µs ±19%     ~   
BM_AsyncThunkExecutor/32/process_time               133µs ±13%   129µs ±17%   -3.07%
BM_AsyncThunkExecutor/64/process_time               201µs ± 9%   196µs ± 8%   -2.84%
BM_AsyncThunkExecutor/128/process_time              273µs ±11%   261µs ±13%   -4.29%
BM_AsyncThunkExecutor/256/process_time              393µs ±17%   378µs ±13%   -3.81%
BM_AsyncThunkExecutor/512/process_time              708µs ±14%   669µs ±10%   -5.54%
",copybara-service[bot],2024-08-20 17:12:06+00:00,['ezhulenev'],2024-08-20 17:52:43+00:00,2024-08-20 17:52:42+00:00,https://github.com/tensorflow/tensorflow/pull/74138,[],[],
2476142570,pull_request,closed,,[xla:cpu] Optimize invariant checks,"[xla:cpu] Optimize invariant checks
",copybara-service[bot],2024-08-20 17:10:15+00:00,['ezhulenev'],2024-08-21 15:10:10+00:00,2024-08-21 15:10:09+00:00,https://github.com/tensorflow/tensorflow/pull/74137,[],[],
2476087249,pull_request,closed,,Remove unused #include.,"Remove unused #include.

Note that this won't break any clients since the same header file
is also included from the other header included by this file.
",copybara-service[bot],2024-08-20 16:38:30+00:00,[],2024-08-22 12:33:03+00:00,2024-08-22 12:33:01+00:00,https://github.com/tensorflow/tensorflow/pull/74136,[],[],
2476081819,pull_request,open,,Build a 2022 image.,"Build a 2022 image.
",copybara-service[bot],2024-08-20 16:35:23+00:00,['belitskiy'],2024-08-20 16:35:29+00:00,,https://github.com/tensorflow/tensorflow/pull/74135,[],"[{'comment_id': 2299285217, 'issue_id': 2476081819, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74135/checks?check_run_id=29012569535) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 20, 16, 35, 28, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-20 16:35:28 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74135/checks?check_run_id=29012569535) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2475992136,pull_request,open,,[XLA:CPU] Vectorize reductions in thunk runtime,"[XLA:CPU] Vectorize reductions in thunk runtime

This brings the values of float operations closer to what's expected.
In particular, fixes some failing tests.
",copybara-service[bot],2024-08-20 15:52:38+00:00,[],2024-09-09 14:50:24+00:00,,https://github.com/tensorflow/tensorflow/pull/74134,[],"[{'comment_id': 2299189669, 'issue_id': 2475992136, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74134/checks?check_run_id=29010426788) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 20, 15, 52, 43, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-20 15:52:43 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74134/checks?check_run_id=29010426788) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2475937748,pull_request,closed,,PR #15969: [PJRT:GPU] Handle sub-byte allocations in pinned host memory,"PR #15969: [PJRT:GPU] Handle sub-byte allocations in pinned host memory

Imported from GitHub PR https://github.com/openxla/xla/pull/15969

Let us not skip calling transfer manager's HostShapeToDeviceShape method for pinned memory allocations. With this patch, the [test_deserialization_with_int4 JAX test](https://github.com/google/jax/blob/23effba503081b176673bd4d5284f1b70584f2df/jax/experimental/array_serialization/serialization_test.py#L581) passes on GPU.

Without this patch, we do not pack sub-byte types in the layout (implemented in GenericTransferManager::HostShapeToDeviceShape).
Copybara import of the project:

--
74a0a938fe36dac7673b45ac1770e0f6011bd308 by Jaroslav Sevcik <jsevcik@nvidia.com>:

Pack bits in layout for pinned memory allocs

Merging this change closes #15969

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15969 from jaro-sevcik:copy-to-memory-space-int4 74a0a938fe36dac7673b45ac1770e0f6011bd308
",copybara-service[bot],2024-08-20 15:26:42+00:00,[],2024-08-20 17:20:14+00:00,2024-08-20 17:20:14+00:00,https://github.com/tensorflow/tensorflow/pull/74133,[],[],
2475844128,pull_request,closed,,Adds missing datatype support for various tflite operations,"- Adds f16,bf16 support for tfl.exp
- Adds bf16, f16 support for tfl.atan2
- Adds bf16,f16,int8,int16 for tfl.neg
- Adds bf16, f16 support for tfl.min, tfl.max
- Adds bf16, f16 support for tfl.slice
- Adds bf16, f16 support for tfl.round
- Adds bf16, f16 support for tfl.reverse
- Adds bf16, f16 support for tfl.pad
- Adds bf16, f16 support for tfl.tanh and tfl.logistic
- Adds bf16, f16 support for tfl.floor
",amrinfathima-mcw,2024-08-20 14:44:58+00:00,['gbaned'],2024-08-23 12:56:19+00:00,2024-08-23 12:56:19+00:00,https://github.com/tensorflow/tensorflow/pull/74132,"[('size:XL', 'CL Change Size:Extra Large')]","[{'comment_id': 2299040673, 'issue_id': 2475844128, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74132/checks?check_run_id=29006681649) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 20, 14, 45, 6, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-20 14:45:06 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74132/checks?check_run_id=29006681649) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2475830506,pull_request,closed,,Make :derived_timeline and :timespan libraries visible to :friends,"Make :derived_timeline and :timespan libraries visible to :friends
",copybara-service[bot],2024-08-20 14:38:47+00:00,[],2024-08-30 23:53:02+00:00,2024-08-30 23:53:02+00:00,https://github.com/tensorflow/tensorflow/pull/74130,[],[],
2475683571,pull_request,closed,,Handle atomic dumping of per fusion autotuning cache to cns.,"Handle atomic dumping of per fusion autotuning cache to cns.

To avoid multiple processes are writing to the same file, the per-fusion autotuning cache dumps the result to temporary file and then renames it, moving it to the location specified with the flag. 

The current implementation uses `tsl::GetTempFilename`, which does not work for cns out of the box. This change proposes alternative solution: create temporary files in 'tmp' directory under user specified cache directory.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15798 from elfiegg:broadcast 3b56979f88430bbbe43f223ad20d5e79db35c0d4
",copybara-service[bot],2024-08-20 13:39:44+00:00,[],2024-08-27 12:48:27+00:00,2024-08-27 12:48:26+00:00,https://github.com/tensorflow/tensorflow/pull/74129,[],[],
2475615621,pull_request,closed,,[XLA:GPU] Do not fuse in SoftmaxRewriterTriton if resulting fusion can not be tiled.,"[XLA:GPU] Do not fuse in SoftmaxRewriterTriton if resulting fusion can not be tiled.

Right now that can happen if any of the tiles have more than 1048576 elements.
",copybara-service[bot],2024-08-20 13:11:40+00:00,[],2024-08-20 15:33:34+00:00,2024-08-20 15:33:33+00:00,https://github.com/tensorflow/tensorflow/pull/74128,[],[],
2475591593,pull_request,open,,Delete the old (non-opensourced) version of the Windows Dockerfile.,"Delete the old (non-opensourced) version of the Windows Dockerfile.

Use the open-sourced Dockerfile in the Docker image-building jobs.
",copybara-service[bot],2024-08-20 13:00:36+00:00,['belitskiy'],2024-08-20 13:00:44+00:00,,https://github.com/tensorflow/tensorflow/pull/74127,[],"[{'comment_id': 2298804620, 'issue_id': 2475591593, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74127/checks?check_run_id=29000699752) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 20, 13, 0, 42, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-20 13:00:42 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74127/checks?check_run_id=29000699752) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2475579519,pull_request,closed,,Integrate LLVM at llvm/llvm-project@60bffe221a1d,"Integrate LLVM at llvm/llvm-project@60bffe221a1d

Updates LLVM usage to match
[60bffe221a1d](https://github.com/llvm/llvm-project/commit/60bffe221a1d)
",copybara-service[bot],2024-08-20 12:54:58+00:00,[],2024-08-20 15:13:50+00:00,2024-08-20 15:13:48+00:00,https://github.com/tensorflow/tensorflow/pull/74126,[],[],
2475564797,pull_request,closed,,Default enable custom kernel fusions for V100 GPUs and test that we run triton gemms for using A100 GPUS and custom kernel fusions using V100.,"Default enable custom kernel fusions for V100 GPUs and test that we run triton gemms for using A100 GPUS and custom kernel fusions using V100.
",copybara-service[bot],2024-08-20 12:48:03+00:00,[],2024-09-04 16:51:05+00:00,2024-09-04 16:51:04+00:00,https://github.com/tensorflow/tensorflow/pull/74125,[],[],
2475464372,pull_request,closed,,PR #15636: [GPU] Expose caching DebugOptions to python,"PR #15636: [GPU] Expose caching DebugOptions to python

Imported from GitHub PR https://github.com/openxla/xla/pull/15636

Adds python bindings for `xla_gpu_kernel_cache_file`, `xla_gpu_enable_llvm_module_compilation_parallelism` and `xla_gpu_per_fusion_autotune_cache_dir`.

We would like to add some convenience  features to JAX which will enable all caches with one flag/option (will open PR for that soon). This change is necessary for that.
Copybara import of the project:

--
87dbb3d3abe5969d1509d965071c5c7dbdcb46d0 by Trevor Morris <tmorris@nvidia.com>:

Expose caching DebugOptions to python

--
3bb41fca800bc12d00870e7b8cbde3641de9beab by Trevor Morris <tmorris@nvidia.com>:

Add test

--
f0e04233522d8740c83b9b19138b76304b242604 by Trevor Morris <tmorris@nvidia.com>:

Fix tests

Merging this change closes #15636

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15636 from trevor-m:cache-python f0e04233522d8740c83b9b19138b76304b242604
",copybara-service[bot],2024-08-20 11:59:19+00:00,[],2024-08-20 19:38:28+00:00,2024-08-20 19:38:27+00:00,https://github.com/tensorflow/tensorflow/pull/74124,[],[],
2475458419,pull_request,closed,,[xla:cpu] Add test for verifying xla_cpu_use_thunk_runtime is disabled in xla_jit_compiled_cpu_function,"[xla:cpu] Add test for verifying xla_cpu_use_thunk_runtime is disabled in xla_jit_compiled_cpu_function
",copybara-service[bot],2024-08-20 11:56:13+00:00,[],2024-08-20 18:45:07+00:00,2024-08-20 18:45:07+00:00,https://github.com/tensorflow/tensorflow/pull/74123,[],[],
2475455821,pull_request,open,,[XLA:GPU] Print xla flags and DebugOptions after their initialization.,"[XLA:GPU] Print xla flags and DebugOptions after their initialization.

There is no place where we print the flags and the debug options we use at the run time. At the same time it is the thing that we always need to know because we have so many of them. Lets print them once at the start time.
",copybara-service[bot],2024-08-20 11:54:52+00:00,[],2024-08-20 11:54:52+00:00,,https://github.com/tensorflow/tensorflow/pull/74122,[],[],
2475436642,pull_request,open,,Internal change only,"Internal change only
",copybara-service[bot],2024-08-20 11:44:47+00:00,[],2024-08-20 11:44:47+00:00,,https://github.com/tensorflow/tensorflow/pull/74121,[],[],
2475418534,pull_request,closed,,[XLA:GPU] Remove workaround for certain transpose fusions.,"[XLA:GPU] Remove workaround for certain transpose fusions.

With the new MLIR transpose emitter, we can vectorize, and that makes this case
faster than using the loop emitter.
Reproducer fusion:

fused_computation.269 {
  bitcast.11045 = f32[4,32,192,384]{3,2,1,0} parameter(0)
  transpose.1177 = f32[4,192,384,32]{3,2,1,0} transpose(bitcast.11045), dimensions={0,2,3,1}
  ROOT convert.5365 = s8[4,192,384,32]{3,2,1,0} convert(transpose.1177)
}

Measured on A100
Before:
bandwidth: 930 GB/s
  bandwidth roofline: 59%
  time wasted: 20us
Total device time: 50us

Now:
  bandwidth: 1150 GB/s
  bandwidth roofline: 73%
  time wasted: 11us
Total device time: 41us
",copybara-service[bot],2024-08-20 11:34:59+00:00,['akuegel'],2024-08-20 13:49:09+00:00,2024-08-20 13:49:07+00:00,https://github.com/tensorflow/tensorflow/pull/74120,[],[],
2475379958,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 11:14:36+00:00,[],2024-08-20 11:14:36+00:00,,https://github.com/tensorflow/tensorflow/pull/74119,[],[],
2475315058,pull_request,open,,No change to OSS.,"No change to OSS.
",copybara-service[bot],2024-08-20 10:41:09+00:00,[],2024-08-21 09:04:41+00:00,,https://github.com/tensorflow/tensorflow/pull/74118,[],[],
2475301706,pull_request,closed,,Add IWYU pragma export for `model_builder_base.h` to `model_builder.h`.,"Add IWYU pragma export for `model_builder_base.h` to `model_builder.h`.

This include-what-you-use pragma allow clients of TF Lite to
not have to depend directly on headers from `tensorflow/compiler/mlir/...`
In particular this is needed to avoid lint warnings for clients using the
`tflite::GetAllocationFromFile` function and including `model_builder.h` but not
`tensorflow/compiler/mlir/core/model_builder_base.h`.
",copybara-service[bot],2024-08-20 10:34:36+00:00,[],2024-08-21 09:14:38+00:00,2024-08-21 09:14:36+00:00,https://github.com/tensorflow/tensorflow/pull/74117,[],[],
2475284171,pull_request,closed,,Fix Doxygen formatting bug: move class doc to after namespace declaration.,"Fix Doxygen formatting bug: move class doc to after namespace declaration.

This fixes a bug in the documentation produced by running Doxygen, where
the documentation for the `FlatBufferBuilderBase` class was being wrongly
associated with the `impl` namespace rather than the class.
",copybara-service[bot],2024-08-20 10:26:11+00:00,[],2024-08-20 18:38:14+00:00,2024-08-20 18:38:13+00:00,https://github.com/tensorflow/tensorflow/pull/74116,[],[],
2475183883,pull_request,closed,,[XLA:GPU] Soft-deprecate `--xla_gpu_enable_triton_gemm`.,"[XLA:GPU] Soft-deprecate `--xla_gpu_enable_triton_gemm`.

The enablement of sharded autotuning by default resolved the previous issue of
incoherent autotuning results when profiling the same configuration on several
hosts during the same autotuning run---making the bypass unnecessary.

The flag still exists but is now a no-op. In order to toggle support for
Triton GEMMs in tests and for internal use, we introduce a new debug option
called `xla_gpu_unsupported_enable_triton_gemm`.
",copybara-service[bot],2024-08-20 09:37:34+00:00,[],2024-08-21 12:11:16+00:00,2024-08-21 12:11:15+00:00,https://github.com/tensorflow/tensorflow/pull/74115,[],[],
2475123370,pull_request,closed,,Automated Code Change,"Automated Code Change

Reverts 6363b8458710ca98369efd774b3ccab01942da41
",copybara-service[bot],2024-08-20 09:10:46+00:00,[],2024-08-21 12:17:14+00:00,2024-08-21 12:17:13+00:00,https://github.com/tensorflow/tensorflow/pull/74114,[],[],
2475100238,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 09:01:07+00:00,[],2024-08-21 03:22:43+00:00,,https://github.com/tensorflow/tensorflow/pull/74113,[],[],
2475086145,pull_request,closed,,PR #16024: [GPU] cuDNN GEMM: add support of slicing of inputs.,"PR #16024: [GPU] cuDNN GEMM: add support of slicing of inputs.

Imported from GitHub PR https://github.com/openxla/xla/pull/16024


Copybara import of the project:

--
b740f2d1a1bc41ea9ccd94d3744552005c77fcd4 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] cuDNN GEMM: add support of slicing of inputs.

Merging this change closes #16024

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16024 from openxla:cudnn_gemm_slice b740f2d1a1bc41ea9ccd94d3744552005c77fcd4
",copybara-service[bot],2024-08-20 08:54:45+00:00,[],2024-08-28 16:07:25+00:00,2024-08-28 16:07:24+00:00,https://github.com/tensorflow/tensorflow/pull/74112,[],[],
2475072107,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 08:47:55+00:00,[],2024-08-20 12:16:49+00:00,,https://github.com/tensorflow/tensorflow/pull/74111,[],[],
2475065501,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 08:44:32+00:00,[],2024-08-21 06:02:44+00:00,,https://github.com/tensorflow/tensorflow/pull/74110,[],[],
2475061556,pull_request,open,,PR #16241: Align the scheduling name with the instruction name for constants,"PR #16241: Align the scheduling name with the instruction name for constants

Imported from GitHub PR https://github.com/openxla/xla/pull/16241

This commit corrects the scheduling name after it is altered by SanitizeConstantNames, preventing assertion failures in the HLO verifier.
Copybara import of the project:

--
8c784ea28d8a1a0fc7e3a4d1df78e1f4cfe8aac9 by Jane Liu <janeliu@nvidia.com>:

Align the scheduling name with the instruction name for constants

Merging this change closes #16241

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16241 from zhenying-liu:constant_name 8c784ea28d8a1a0fc7e3a4d1df78e1f4cfe8aac9
",copybara-service[bot],2024-08-20 08:42:36+00:00,[],2024-08-20 12:16:53+00:00,,https://github.com/tensorflow/tensorflow/pull/74109,[],[],
2475056340,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16095 from dimvar:cub-templates 81cc5a8bb3aa330f73419e9485fd0458cb1d6116
",copybara-service[bot],2024-08-20 08:39:58+00:00,[],2024-08-21 07:11:19+00:00,,https://github.com/tensorflow/tensorflow/pull/74108,[],[],
2475052086,pull_request,closed,,"[XLA:GPU[NFC] Lower size requirements for `TritonGemmTestAny.LowerDotWith{L,R}hsWithoutNonContractingDimThroughTriton`.","[XLA:GPU[NFC] Lower size requirements for `TritonGemmTestAny.LowerDotWith{L,R}hsWithoutNonContractingDimThroughTriton`.
",copybara-service[bot],2024-08-20 08:37:58+00:00,[],2024-08-20 09:39:58+00:00,2024-08-20 09:39:58+00:00,https://github.com/tensorflow/tensorflow/pull/74107,[],[],
2475043749,pull_request,closed,,Integrate Triton up to [6a5638e2](https://github.com/openai/triton/commits/6a5638e23586c6ae47ea1c652458ef1cefb22e4c),"Integrate Triton up to [6a5638e2](https://github.com/openai/triton/commits/6a5638e23586c6ae47ea1c652458ef1cefb22e4c)
",copybara-service[bot],2024-08-20 08:33:41+00:00,[],2024-08-20 10:40:14+00:00,2024-08-20 10:40:13+00:00,https://github.com/tensorflow/tensorflow/pull/74106,[],[],
2475042538,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 08:33:02+00:00,[],2024-08-20 08:33:02+00:00,,https://github.com/tensorflow/tensorflow/pull/74105,[],[],
2475013386,pull_request,closed,,[XLA:GPU][NFC] Add `TODO` to figure out the infinite timeout issue for sharded autotuning.,"[XLA:GPU][NFC] Add `TODO` to figure out the infinite timeout issue for sharded autotuning.
",copybara-service[bot],2024-08-20 08:18:25+00:00,[],2024-08-20 09:48:36+00:00,2024-08-20 09:48:35+00:00,https://github.com/tensorflow/tensorflow/pull/74104,[],[],
2474865496,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 06:57:49+00:00,[],2024-08-20 12:07:17+00:00,,https://github.com/tensorflow/tensorflow/pull/74103,[],[],
2474775619,pull_request,closed,,Fix a bug where PjRt CPU's `BufferFromHostLiteral` does not handle sub-byte types correctly,"Fix a bug where PjRt CPU's `BufferFromHostLiteral` does not handle sub-byte types correctly

Since the rest of the implementation assumes that sub-byte types are packed, H2D must also perform packing for sub-byte types.

`AbstractAsyncHostToHostMemoryTransferManager::TransferLiteralToBuffer` is more broken because it does not even check layout or perform any packing, but this CL at least makes it return an error if `TransferLiteralToBuffer` is given with a sub-byte element type.
",copybara-service[bot],2024-08-20 05:53:06+00:00,[],2024-08-20 19:31:56+00:00,2024-08-20 19:31:54+00:00,https://github.com/tensorflow/tensorflow/pull/74102,[],[],
2474752145,pull_request,closed,,Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
```

After:
```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit
```

Also, we can remove the hack (which I didn't like) in multihost_utils.py.
",copybara-service[bot],2024-08-20 05:31:30+00:00,['yashk2810'],2024-08-21 00:36:41+00:00,2024-08-21 00:36:39+00:00,https://github.com/tensorflow/tensorflow/pull/74101,[],[],
2474740436,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-20 05:19:47+00:00,[],2024-08-20 05:19:47+00:00,,https://github.com/tensorflow/tensorflow/pull/74100,[],[],
2474702649,pull_request,closed,,"[Numpy] Replace every np.asarray(..., dtype=...) call in TF with a call to numpy_compat.np_asarray(..., dtype=...)","[Numpy] Replace every np.asarray(..., dtype=...) call in TF with a call to numpy_compat.np_asarray(..., dtype=...)
",copybara-service[bot],2024-08-20 04:40:23+00:00,['kanglant'],2024-08-20 20:30:53+00:00,2024-08-20 20:30:52+00:00,https://github.com/tensorflow/tensorflow/pull/74099,[],[],
2474513436,pull_request,open,,Cosmetic refactoring in CollectivePermuteCycleDecomposer,"Cosmetic refactoring in CollectivePermuteCycleDecomposer
",copybara-service[bot],2024-08-20 00:57:40+00:00,[],2024-08-20 00:57:40+00:00,,https://github.com/tensorflow/tensorflow/pull/74098,[],[],
2474508603,pull_request,closed,,Refactor hermetic CUDA flags and update `--config=cuda` to add CUDA dependencies both for `bazel build` and `bazel test` phases.,"Refactor hermetic CUDA flags and update `--config=cuda` to add CUDA dependencies both for `bazel build` and `bazel test` phases.

Add `--@local_config_cuda//cuda:override_include_cuda_libs` to override settings for TF wheel.

Forbid building TF wheel with `--@local_config_cuda//cuda:include_cuda_libs=true`
",copybara-service[bot],2024-08-20 00:50:55+00:00,[],2024-08-23 18:43:41+00:00,2024-08-23 18:43:41+00:00,https://github.com/tensorflow/tensorflow/pull/74097,[],[],
2474475751,pull_request,closed,,[XLA] Propagate original_value during fusions,"[XLA] Propagate original_value during fusions

This preserves original_value attribute when an instruction is fused during instruction or multi-output fusions.
",copybara-service[bot],2024-08-20 00:11:38+00:00,['jcai19'],2024-08-24 00:20:51+00:00,2024-08-24 00:20:51+00:00,https://github.com/tensorflow/tensorflow/pull/74096,[],[],
2474426549,pull_request,open,,[XLA] Propagate original_value during instruction fusion,"[XLA] Propagate original_value during instruction fusion

This copies over original_value attribute when an instruction is fused during instruction fusion.
",copybara-service[bot],2024-08-19 23:27:45+00:00,['jcai19'],2024-08-19 23:27:46+00:00,,https://github.com/tensorflow/tensorflow/pull/74095,[],[],
2474415501,pull_request,closed,,[XLA] Propagate original_value when instructions are replaced,"[XLA] Propagate original_value when instructions are replaced

This copies over original_value attribute when an value is replaced during HLO transformations.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14689 from Tixxx:tixxx/unroll_cm b64cb889d8ef1c5afd01676766095de77c5102e3
",copybara-service[bot],2024-08-19 23:15:01+00:00,['jcai19'],2024-08-23 21:25:26+00:00,2024-08-23 21:25:24+00:00,https://github.com/tensorflow/tensorflow/pull/74094,[],[],
2474334833,pull_request,open,,Fix quantize_weights_portable header.,"Fix quantize_weights_portable header.
",copybara-service[bot],2024-08-19 22:23:44+00:00,['pak-laura'],2024-08-21 19:05:45+00:00,,https://github.com/tensorflow/tensorflow/pull/74093,[],[],
2474334558,pull_request,closed,,[tsl:concurrency] Mark AsyncValueRef::SetError with string_view argument deprecated,"[tsl:concurrency] Mark AsyncValueRef::SetError with string_view argument deprecated
",copybara-service[bot],2024-08-19 22:23:35+00:00,['ezhulenev'],2024-08-20 02:01:41+00:00,2024-08-20 02:01:39+00:00,https://github.com/tensorflow/tensorflow/pull/74092,[],[],
2474286485,pull_request,closed,,Move `tsl/lib/core/*` to `xla/tsl/lib/core` and update users,"Move `tsl/lib/core/*` to `xla/tsl/lib/core` and update users
",copybara-service[bot],2024-08-19 21:59:51+00:00,['ddunl'],2024-08-21 21:29:17+00:00,2024-08-21 21:29:16+00:00,https://github.com/tensorflow/tensorflow/pull/74091,[],[],
2474250008,pull_request,closed,,Removing distutils leftover,"This aims to finish #58073. The patch is untested but follow https://setuptools.pypa.io/en/latest/deprecated/distutils-legacy.html.
I did not touch the install scripts.

Thanks,",apraga,2024-08-19 21:37:37+00:00,['gbaned'],2024-10-02 22:28:00+00:00,2024-10-02 22:16:14+00:00,https://github.com/tensorflow/tensorflow/pull/74090,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2297501803, 'issue_id': 2474250008, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74090/checks?check_run_id=28969467927) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 19, 21, 37, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298027585, 'issue_id': 2474250008, 'author': 'keerthanakadiri', 'body': 'Hi @apraga, Can you please sign CLA , thank you!', 'created_at': datetime.datetime(2024, 8, 20, 5, 58, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2299328085, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'This looks good on a quick glance, but without signing the CLA we cannot take it.', 'created_at': datetime.datetime(2024, 8, 20, 16, 58, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2299720227, 'issue_id': 2474250008, 'author': 'apraga', 'body': 'With the proper email, it should be good now !', 'created_at': datetime.datetime(2024, 8, 20, 20, 36, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302893970, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'Oh, now we have a merge conflict to resolve :(', 'created_at': datetime.datetime(2024, 8, 21, 19, 48, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303017611, 'issue_id': 2474250008, 'author': 'apraga', 'body': 'It should be good to go now !\r\n\r\n\r\nOn Wednesday, August 21st, 2024 at 9:49 PM, Mihai Maruseac ***@***.***> wrote:\r\n\r\n> Oh, now we have a merge conflict to resolve :(\r\n> \r\n\r\n> —\r\n> Reply to this email directly, view it on GitHub, or unsubscribe.\r\n> You are receiving this because you were mentioned.', 'created_at': datetime.datetime(2024, 8, 21, 21, 9, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303054612, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'Umm, it seems this has unrelated changes', 'created_at': datetime.datetime(2024, 8, 21, 21, 37, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312611040, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'You still have changes that are unrelated to the PR, you have included commits that already existed on the branch. Unfortunately, this breaks importing the PR into the internal system.', 'created_at': datetime.datetime(2024, 8, 27, 13, 46, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312632860, 'issue_id': 2474250008, 'author': 'apraga', 'body': ""Ha, I tried to solve the merge conflict above. So what would be the best way to proceed ? Re-ordering the commts ? Undo the merge (but it looked like the CI wasn't run). Thanks"", 'created_at': datetime.datetime(2024, 8, 27, 13, 54, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312805172, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'Reordering the commits and removing all the others is the best. But unsure if that would be possible now.\r\n\r\nAnother alternative would be to close this and recreate a new one', 'created_at': datetime.datetime(2024, 8, 27, 14, 59, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2319391127, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': ""If you open a new PR, please tag me and I'll try to speed it up"", 'created_at': datetime.datetime(2024, 8, 29, 23, 20, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322977391, 'issue_id': 2474250008, 'author': 'apraga', 'body': ""Okay, I've reset the branch unto master et cherry-picked the commit before force-pushing it. Hope it solve the issue :)"", 'created_at': datetime.datetime(2024, 8, 31, 17, 19, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351778737, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': ""This seems to fail with\r\n\r\n```\r\n    for scheme in sysconfig.get_scheme_names():\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'tensorflow._api.v2.sysconfig' has no attribute 'get_scheme_names'\r\n```\r\n\r\nI think it was passing before the mishap with the bad merge, so maybe conflict resolution removed something that used to work?"", 'created_at': datetime.datetime(2024, 9, 15, 20, 18, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351778883, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'This seems to be from a test that does `import tensorflow as tf`. Can you try building a wheel with these changes and see if the import proceeds, please?', 'created_at': datetime.datetime(2024, 9, 15, 20, 18, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354783275, 'issue_id': 2474250008, 'author': 'apraga', 'body': ""@mihaimaruseac thanks for the follow-up. I tried to build tensorflow from source with different ways (clang18, gcc, clang17 with the binary, clang17 inside the docker image). Each resulted in compilation error. Clang17 on gentoo lead to the farthest but exhaust all the memory \r\n\r\n```[24,203 / 24,842] 16 actions running\r\n    Compiling .../compiler/mlir/tensorflow/transforms/xla_call_module_deserialization.cc; 160s local\r\n    Compiling .../mlir/quantization/stablehlo/passes/quantize_composite_functions.cc; 156s local\r\n    Compiling tensorflow/compiler/mlir/quantization/stablehlo/passes/quantize.cc; 154s local\r\n    Compiling tensorflow/compiler/mlir/tensorflow/transforms/tpu_validate_inputs.cc; 153s local\r\n    Compiling tensorflow/compiler/mlir/quantization/stablehlo/passes/prepare_quantize.cc; 152s local\r\n    Compiling .../compiler/mlir/tensorflow/transforms/drop_while_shape_invariant.cc; 150s local\r\n    Compiling tensorflow/compiler/tf2xla/kernels/fft_ops.cc; 149s local\r\n    Compiling tensorflow/dtensor/mlir/dtensor_send_recv.cc; 148s local ...\r\n\r\nServer terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/home/alex/.cache/bazel/_bazel_alex/b43089709cdb88f3f459426ef0ba5527/server/jvm.out')\r\n````"", 'created_at': datetime.datetime(2024, 9, 17, 7, 44, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357682888, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': ""Oh😥 there are two options here: you can try to compile but passing flags to limit the total amount of memory being used by Bazel (`-j 1` might also help), or we can try a hacky test: in an environment where you have TF installed, go to `...dist-packages/tensorflow` and edit all Python files there to not have `distutils` then let's try `import tensorflow as tf` in a Python interpreter started in the same environment, but outside of the `...dist-packages` directory. \r\n\r\nIf that passes, I would then check that all edited files are reflected in the PR too"", 'created_at': datetime.datetime(2024, 9, 18, 7, 13, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367461077, 'issue_id': 2474250008, 'author': 'apraga', 'body': 'Okay, I managed to compile tensorflow with `- j 1` and can reproduce the issue. After installation `lib/python3.12/site-packages/tensorflow/__init__.py`, has 2 sysconfig\r\n\r\n```python\r\nfrom tensorflow._api.v2 import sysconfig\r\nimport sysconfig\r\n```\r\nCommit below offers a fix (untested, currently compiling).\r\n \r\n Edit : compile and import work in my machine :)', 'created_at': datetime.datetime(2024, 9, 23, 7, 49, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384805458, 'issue_id': 2474250008, 'author': 'keerthanakadiri', 'body': 'Hi @apraga, Can you please resolve the conflicts? Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 5, 13, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389677742, 'issue_id': 2474250008, 'author': 'apraga', 'body': ""@keerthanakadiri Hi, I've rebasend onto latest master and squashed all commits into one. Hope it's good now !"", 'created_at': datetime.datetime(2024, 10, 2, 20, 53, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389680950, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'I took this internally and made a bunch of changes to it (needed some fixed to `BUILD` files and copybara changes). Running the last set of tests now and hopefully it can land by EoD', 'created_at': datetime.datetime(2024, 10, 2, 20, 55, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389739594, 'issue_id': 2474250008, 'author': 'mihaimaruseac', 'body': 'This landed internally, so it should also land externally in the next hour or so.\r\n\r\nFingers crossed it does not get rolled back', 'created_at': datetime.datetime(2024, 10, 2, 21, 38, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389795062, 'issue_id': 2474250008, 'author': 'apraga', 'body': 'Awesome ! Thanks a lot', 'created_at': datetime.datetime(2024, 10, 2, 22, 27, 59, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-19 21:37:41 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74090/checks?check_run_id=28969467927) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

keerthanakadiri on (2024-08-20 05:58:25 UTC): Hi @apraga, Can you please sign CLA , thank you!

mihaimaruseac on (2024-08-20 16:58:58 UTC): This looks good on a quick glance, but without signing the CLA we cannot take it.

apraga (Issue Creator) on (2024-08-20 20:36:29 UTC): With the proper email, it should be good now !

mihaimaruseac on (2024-08-21 19:48:52 UTC): Oh, now we have a merge conflict to resolve :(

apraga (Issue Creator) on (2024-08-21 21:09:06 UTC): It should be good to go now !


On Wednesday, August 21st, 2024 at 9:49 PM, Mihai Maruseac ***@***.***> wrote:

mihaimaruseac on (2024-08-21 21:37:16 UTC): Umm, it seems this has unrelated changes

mihaimaruseac on (2024-08-27 13:46:11 UTC): You still have changes that are unrelated to the PR, you have included commits that already existed on the branch. Unfortunately, this breaks importing the PR into the internal system.

apraga (Issue Creator) on (2024-08-27 13:54:23 UTC): Ha, I tried to solve the merge conflict above. So what would be the best way to proceed ? Re-ordering the commts ? Undo the merge (but it looked like the CI wasn't run). Thanks

mihaimaruseac on (2024-08-27 14:59:04 UTC): Reordering the commits and removing all the others is the best. But unsure if that would be possible now.

Another alternative would be to close this and recreate a new one

mihaimaruseac on (2024-08-29 23:20:02 UTC): If you open a new PR, please tag me and I'll try to speed it up

apraga (Issue Creator) on (2024-08-31 17:19:59 UTC): Okay, I've reset the branch unto master et cherry-picked the commit before force-pushing it. Hope it solve the issue :)

mihaimaruseac on (2024-09-15 20:18:12 UTC): This seems to fail with

```
    for scheme in sysconfig.get_scheme_names():
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'tensorflow._api.v2.sysconfig' has no attribute 'get_scheme_names'
```

I think it was passing before the mishap with the bad merge, so maybe conflict resolution removed something that used to work?

mihaimaruseac on (2024-09-15 20:18:53 UTC): This seems to be from a test that does `import tensorflow as tf`. Can you try building a wheel with these changes and see if the import proceeds, please?

apraga (Issue Creator) on (2024-09-17 07:44:26 UTC): @mihaimaruseac thanks for the follow-up. I tried to build tensorflow from source with different ways (clang18, gcc, clang17 with the binary, clang17 inside the docker image). Each resulted in compilation error. Clang17 on gentoo lead to the farthest but exhaust all the memory 

```[24,203 / 24,842] 16 actions running
    Compiling .../compiler/mlir/tensorflow/transforms/xla_call_module_deserialization.cc; 160s local
    Compiling .../mlir/quantization/stablehlo/passes/quantize_composite_functions.cc; 156s local
    Compiling tensorflow/compiler/mlir/quantization/stablehlo/passes/quantize.cc; 154s local
    Compiling tensorflow/compiler/mlir/tensorflow/transforms/tpu_validate_inputs.cc; 153s local
    Compiling tensorflow/compiler/mlir/quantization/stablehlo/passes/prepare_quantize.cc; 152s local
    Compiling .../compiler/mlir/tensorflow/transforms/drop_while_shape_invariant.cc; 150s local
    Compiling tensorflow/compiler/tf2xla/kernels/fft_ops.cc; 149s local
    Compiling tensorflow/dtensor/mlir/dtensor_send_recv.cc; 148s local ...

Server terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/home/alex/.cache/bazel/_bazel_alex/b43089709cdb88f3f459426ef0ba5527/server/jvm.out')
````

mihaimaruseac on (2024-09-18 07:13:23 UTC): Oh😥 there are two options here: you can try to compile but passing flags to limit the total amount of memory being used by Bazel (`-j 1` might also help), or we can try a hacky test: in an environment where you have TF installed, go to `...dist-packages/tensorflow` and edit all Python files there to not have `distutils` then let's try `import tensorflow as tf` in a Python interpreter started in the same environment, but outside of the `...dist-packages` directory. 

If that passes, I would then check that all edited files are reflected in the PR too

apraga (Issue Creator) on (2024-09-23 07:49:09 UTC): Okay, I managed to compile tensorflow with `- j 1` and can reproduce the issue. After installation `lib/python3.12/site-packages/tensorflow/__init__.py`, has 2 sysconfig

```python
from tensorflow._api.v2 import sysconfig
import sysconfig
```
Commit below offers a fix (untested, currently compiling).
 
 Edit : compile and import work in my machine :)

keerthanakadiri on (2024-10-01 05:13:27 UTC): Hi @apraga, Can you please resolve the conflicts? Thank you!

apraga (Issue Creator) on (2024-10-02 20:53:16 UTC): @keerthanakadiri Hi, I've rebasend onto latest master and squashed all commits into one. Hope it's good now !

mihaimaruseac on (2024-10-02 20:55:25 UTC): I took this internally and made a bunch of changes to it (needed some fixed to `BUILD` files and copybara changes). Running the last set of tests now and hopefully it can land by EoD

mihaimaruseac on (2024-10-02 21:38:38 UTC): This landed internally, so it should also land externally in the next hour or so.

Fingers crossed it does not get rolled back

apraga (Issue Creator) on (2024-10-02 22:27:59 UTC): Awesome ! Thanks a lot

"
2474221354,pull_request,closed,,"Test that `bazel query ""deps(//xla/...)""` works","Test that `bazel query ""deps(//xla/...)""` works

Load `benchmark_deps` from `@com_google_benchmark` to fix complaint about missing dependency
",copybara-service[bot],2024-08-19 21:16:31+00:00,['ddunl'],2024-08-20 00:04:31+00:00,2024-08-20 00:04:30+00:00,https://github.com/tensorflow/tensorflow/pull/74089,[],[],
2474173737,pull_request,closed,,Add hlo_query::TextChannelId Test and extend hlo_query::FindFirstInstruction test,"Add hlo_query::TextChannelId Test and extend hlo_query::FindFirstInstruction test
",copybara-service[bot],2024-08-19 20:44:01+00:00,[],2024-08-23 22:50:16+00:00,2024-08-23 22:50:15+00:00,https://github.com/tensorflow/tensorflow/pull/74088,[],[],
2474157482,pull_request,open,,Placeholder. Will removed after base cl is submitted.,"Placeholder. Will removed after base cl is submitted.
",copybara-service[bot],2024-08-19 20:33:56+00:00,[],2024-08-19 20:33:56+00:00,,https://github.com/tensorflow/tensorflow/pull/74087,[],[],
2474138526,pull_request,closed,,Update README.md,Fixed a minor typo with capitalization.,tactipus,2024-08-19 20:21:46+00:00,['gbaned'],2024-08-20 13:20:48+00:00,2024-08-20 13:20:45+00:00,https://github.com/tensorflow/tensorflow/pull/74086,"[('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2297382047, 'issue_id': 2474138526, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74086/checks?check_run_id=28966446029) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 19, 20, 21, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298024523, 'issue_id': 2474138526, 'author': 'keerthanakadiri', 'body': 'Hi @tactipus, Can you please sign CLA , thank you!', 'created_at': datetime.datetime(2024, 8, 20, 5, 55, 37, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-19 20:21:50 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74086/checks?check_run_id=28966446029) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

keerthanakadiri on (2024-08-20 05:55:37 UTC): Hi @tactipus, Can you please sign CLA , thank you!

"
2474080395,pull_request,open,,[Numpy] Fix a bug in session.py that causes int overflow on NumPy 2.0.,"[Numpy] Fix a bug in session.py that causes int overflow on NumPy 2.0.

Creates a common numpy_compat library to centralize NumPy version compatibility functions, such as np_asarray.
",copybara-service[bot],2024-08-19 19:47:08+00:00,[],2024-08-19 20:01:20+00:00,,https://github.com/tensorflow/tensorflow/pull/74085,[],[],
2474078231,pull_request,closed,,[XLA:GPU] Fix CHECK-fail for tuples of int4.,"[XLA:GPU] Fix CHECK-fail for tuples of int4.
",copybara-service[bot],2024-08-19 19:46:01+00:00,[],2024-08-20 07:41:40+00:00,2024-08-20 07:41:36+00:00,https://github.com/tensorflow/tensorflow/pull/74084,[],[],
2474068037,pull_request,open,,egalizes TFL::IfOp to dwc.functional_ifop.,"egalizes TFL::IfOp to dwc.functional_ifop.
",copybara-service[bot],2024-08-19 19:40:27+00:00,[],2024-08-19 19:40:27+00:00,,https://github.com/tensorflow/tensorflow/pull/74083,[],[],
2474036370,pull_request,open,,[pjrt:cpu] Set default layout element size in bits,"[pjrt:cpu] Set default layout element size in bits
",copybara-service[bot],2024-08-19 19:21:54+00:00,['ezhulenev'],2024-08-19 19:21:55+00:00,,https://github.com/tensorflow/tensorflow/pull/74082,[],[],
2474033572,pull_request,closed,,Add a Dockerfile for Windows.,"Add a Dockerfile for Windows.
",copybara-service[bot],2024-08-19 19:20:29+00:00,['belitskiy'],2024-08-20 13:57:16+00:00,2024-08-20 13:57:15+00:00,https://github.com/tensorflow/tensorflow/pull/74081,[],"[{'comment_id': 2297275584, 'issue_id': 2474033572, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74081/checks?check_run_id=28963912551) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 19, 19, 20, 35, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-19 19:20:35 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74081/checks?check_run_id=28963912551) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2474013315,pull_request,closed,,Copy CUDA/CUDNN headers to third_party/gpus/cuda and third_party/gpus/cudnn in build_pip_package.py.,"Copy CUDA/CUDNN headers to third_party/gpus/cuda and third_party/gpus/cudnn in build_pip_package.py.

This setup replicates the TF wheel setup available before hermetic CUDA landed.
",copybara-service[bot],2024-08-19 19:08:21+00:00,[],2024-08-20 16:17:40+00:00,2024-08-20 16:17:39+00:00,https://github.com/tensorflow/tensorflow/pull/74080,[],[],
2474012720,pull_request,open,,Fixes L2Pool implementation to not average pooling region squares,"Fixes L2Pool implementation to not average pooling region squares
See discussion here: https://github.com/webmachinelearning/webnn/issues/278
",copybara-service[bot],2024-08-19 19:07:58+00:00,[],2025-01-24 21:13:47+00:00,,https://github.com/tensorflow/tensorflow/pull/74079,[],[],
2473976006,pull_request,closed,,Add alias rule for nvjitlink in hermetic CUDA build,"Add alias rule for nvjitlink in hermetic CUDA build

The nvjitlink package already exists, but the alias rule was still missing and therefore failing some bazel queries.
",copybara-service[bot],2024-08-19 18:46:08+00:00,[],2024-08-19 19:56:39+00:00,2024-08-19 19:56:38+00:00,https://github.com/tensorflow/tensorflow/pull/74078,[],[],
2473955581,pull_request,closed,,Remove DummyAutoSharding as it is not used anywhere.,"Remove DummyAutoSharding as it is not used anywhere.
",copybara-service[bot],2024-08-19 18:34:09+00:00,[],2024-08-19 20:38:55+00:00,2024-08-19 20:38:54+00:00,https://github.com/tensorflow/tensorflow/pull/74077,[],[],
2473945298,pull_request,closed,,Introduce SetOptions interface and a visitor concept to `visit` passes and pipelines to `SetOptions`,"Introduce SetOptions interface and a visitor concept to `visit` passes and pipelines to `SetOptions`
",copybara-service[bot],2024-08-19 18:28:23+00:00,['vamsimanchala'],2024-09-04 16:24:48+00:00,2024-09-04 16:24:47+00:00,https://github.com/tensorflow/tensorflow/pull/74076,[],[],
2473937707,pull_request,open,,Add support for TFLite If op in MLIR flatbuffer exporter. Serialize function call subgraph in then/else region and append function arguments to If op operand.,"Add support for TFLite If op in MLIR flatbuffer exporter. Serialize function call subgraph in then/else region and append function arguments to If op operand.

This is 1st of 4 changes to enable direct legalization of Stable HLO -> TFL If op.
",copybara-service[bot],2024-08-19 18:23:49+00:00,['vamsimanchala'],2024-08-20 22:21:45+00:00,,https://github.com/tensorflow/tensorflow/pull/74075,[],[],
2473900010,pull_request,closed,,Internal exhaustive test refactoring,"Internal exhaustive test refactoring
",copybara-service[bot],2024-08-19 18:00:51+00:00,[],2024-08-19 19:14:55+00:00,2024-08-19 19:14:54+00:00,https://github.com/tensorflow/tensorflow/pull/74074,[],[],
2473896954,pull_request,closed,,Update pybind11 to v2.13.4,"Update pybind11 to v2.13.4
",copybara-service[bot],2024-08-19 17:59:03+00:00,[],2024-08-19 19:22:28+00:00,2024-08-19 19:22:26+00:00,https://github.com/tensorflow/tensorflow/pull/74073,[],[],
2473890451,pull_request,closed,,Only wrap custom-calls that are for GEMMs.,"Only wrap custom-calls that are for GEMMs.

Also, move the `AsyncWrapper` towards the end of the optimization pipeline.
",copybara-service[bot],2024-08-19 17:54:53+00:00,[],2024-08-20 14:53:04+00:00,2024-08-20 14:53:03+00:00,https://github.com/tensorflow/tensorflow/pull/74072,[],[],
2473888330,pull_request,closed,,[XLA:GPU] add a test case containing a forward cycle to gpu_p2p_pipeliner_test,"[XLA:GPU] add a test case containing a forward cycle to gpu_p2p_pipeliner_test
",copybara-service[bot],2024-08-19 17:53:41+00:00,[],2024-09-03 10:35:24+00:00,2024-09-03 10:35:24+00:00,https://github.com/tensorflow/tensorflow/pull/74071,[],[],
2473871764,pull_request,closed,,[xla:cpu] Optimize f32 -> bf16 conversion,"[xla:cpu] Optimize f32 -> bf16 conversion

name                                    old cpu/op   new cpu/op   delta
BM_AddF32/128/process_time              20.4µs ±27%  20.3µs ±24%     ~   
BM_AddF32/256/process_time              79.1µs ± 6%  78.0µs ± 8%     ~   
BM_AddF32/512/process_time               119µs ± 8%   118µs ±12%     ~   
BM_AddF32/1024/process_time              197µs ±13%   194µs ± 9%     ~   
BM_AddF32/8192/process_time             3.60ms ±22%  4.16ms ±52%     ~   
BM_AddF32/16384/process_time            8.66ms ±44%  8.03ms ±29%     ~   
BM_AddF32/32768/process_time            17.1ms ±49%  17.7ms ±47%     ~   
BM_AddBF16/128/process_time             20.9µs ± 2%  17.0µs ± 3%  -18.60%
BM_AddBF16/256/process_time             57.5µs ± 3%  48.2µs ± 5%  -16.12%
BM_AddBF16/512/process_time              133µs ±17%   117µs ±19%  -12.60%
BM_AddBF16/1024/process_time             231µs ±28%   194µs ±31%  -16.36%
BM_AddBF16/8192/process_time            1.59ms ±22%  1.38ms ±25%  -12.98%
BM_AddBF16/16384/process_time           4.08ms ±36%  4.17ms ±46%     ~   
BM_AddBF16/32768/process_time           9.31ms ±39%  7.80ms ±11%  -16.21%
BM_ConvertF32ToBF16/128/process_time    17.1µs ± 2%  14.3µs ± 3%  -16.49%
BM_ConvertF32ToBF16/256/process_time    50.5µs ± 3%  44.6µs ± 4%  -11.57%
BM_ConvertF32ToBF16/512/process_time     114µs ±17%   102µs ±12%   -9.92%
BM_ConvertF32ToBF16/1024/process_time    181µs ±17%   154µs ± 6%  -15.06%
BM_ConvertF32ToBF16/8192/process_time   1.42ms ±16%  1.27ms ±31%  -10.42%
BM_ConvertF32ToBF16/16384/process_time  4.10ms ±43%  3.87ms ±49%     ~   
BM_ConvertF32ToBF16/32768/process_time  8.46ms ±31%  7.77ms ±19%   -8.21%
",copybara-service[bot],2024-08-19 17:43:30+00:00,['ezhulenev'],2024-08-19 18:38:50+00:00,2024-08-19 18:38:50+00:00,https://github.com/tensorflow/tensorflow/pull/74070,[],[],
2473840413,pull_request,closed,,merge tests with same options into one target,"merge tests with same options into one target
",copybara-service[bot],2024-08-19 17:26:11+00:00,[],2024-08-20 18:30:53+00:00,2024-08-20 18:30:52+00:00,https://github.com/tensorflow/tensorflow/pull/74069,[],[],
2473833938,pull_request,closed,,[tsl:concurrency] Add a FlatMap overload that accepts AsyncValuePtr<T> to allow extending the lifetime of mapped async value,"[tsl:concurrency] Add a FlatMap overload that accepts AsyncValuePtr<T> to allow extending the lifetime of mapped async value
",copybara-service[bot],2024-08-19 17:22:09+00:00,['ezhulenev'],2024-08-19 18:52:59+00:00,2024-08-19 18:52:57+00:00,https://github.com/tensorflow/tensorflow/pull/74068,[],[],
2473833235,pull_request,closed,,[tsl:concurrency] NFC: Simplify Map/TryMap/FlatMap functor templates,"[tsl:concurrency] NFC: Simplify Map/TryMap/FlatMap functor templates
",copybara-service[bot],2024-08-19 17:21:44+00:00,['ezhulenev'],2024-08-19 20:54:23+00:00,2024-08-19 20:54:22+00:00,https://github.com/tensorflow/tensorflow/pull/74067,[],[],
2473790158,pull_request,closed,,[xla:cpu] Use fast f32<->bf16 conversion in nested IrEmitter,"[xla:cpu] Use fast f32<->bf16 conversion in nested IrEmitter

-----------------------------------------------------------------------------
Benchmark                                   Time             CPU   Iterations
-----------------------------------------------------------------------------
BEFORE
BM_ReduceAddBF16/1024/process_time    1016164 ns      6518412 ns          106

AFTER
BM_ReduceAddBF16/1024/process_time     550877 ns      3080795 ns          217
",copybara-service[bot],2024-08-19 16:54:42+00:00,['ezhulenev'],2024-08-19 17:37:38+00:00,2024-08-19 17:37:38+00:00,https://github.com/tensorflow/tensorflow/pull/74066,[],[],
2473784196,pull_request,closed,,Introduce pywrap bazel rules and migrate Tensorflow to it,"Introduce pywrap bazel rules and migrate Tensorflow to it

The gist of this change (the new rules implementation) is contained within `rules_pywrap` folder. The rules are generic and not tensorflow-specific

1) (internal-specific)

2) (internal-specific)

3) It provides same linking strategy of final artifacts on all 3 supported platforms (no major differences between Linux, Mac and Windows).

4) It makes it possible to abandon usage of header-only targets to prevent ODR violations. Simply speaking you can now depend on generated protobuf message classes normally, without need to worry how that is linked afterwards.

5) The current version is backward-compatible and unless explicitly enabled is a no-op. To enable the new rules pass `--repo_env=USE_PYWRAP_RULES=True` flag to build/test command.

6) The `if_pywrap` construct is temporary and will be removed once full migration is completed. Currently if_pywrap is mainly used to pass normal dependencies (instead of header-only). The header-only stuff is kept for backward compatibility and smoother migration but will be eventually removed.

7) This CL migrates TF and the most problematic among all google ML repositories. Once TF is sabilized the other repositories, such as JAX and XLA will be migrated too (which should be way easier than migrating TF anyways)
",copybara-service[bot],2024-08-19 16:51:08+00:00,['vam-google'],2024-10-10 07:12:13+00:00,2024-10-10 07:12:12+00:00,https://github.com/tensorflow/tensorflow/pull/74065,[],[],
2473778975,pull_request,open,,Add `ReportInfoToService` to coordination service for runtime info/error reporting and aggregation. This allows the coordinator to become a central place to gather runtime information and report it.,"Add `ReportInfoToService` to coordination service for runtime info/error reporting and aggregation. This allows the coordinator to become a central place to gather runtime information and report it.
",copybara-service[bot],2024-08-19 16:48:04+00:00,[],2024-08-19 19:37:25+00:00,,https://github.com/tensorflow/tensorflow/pull/74064,[],[],
2473771201,pull_request,closed,,Make the program execution asynchronous with arguments being available.,"Make the program execution asynchronous with arguments being available.
",copybara-service[bot],2024-08-19 16:43:08+00:00,['changhuilin'],2024-08-26 23:34:20+00:00,2024-08-26 23:34:19+00:00,https://github.com/tensorflow/tensorflow/pull/74063,[],[],
2473769856,pull_request,closed,,[NFC] Make GPU run input own the data.,"[NFC] Make GPU run input own the data.
",copybara-service[bot],2024-08-19 16:42:17+00:00,['changhuilin'],2024-08-21 19:32:36+00:00,2024-08-21 19:32:35+00:00,https://github.com/tensorflow/tensorflow/pull/74062,[],[],
2473691693,pull_request,open,,Add a Dockerfile for Windows.,"Add a Dockerfile for Windows.
",copybara-service[bot],2024-08-19 15:55:57+00:00,['belitskiy'],2024-08-19 15:56:03+00:00,,https://github.com/tensorflow/tensorflow/pull/74061,[],"[{'comment_id': 2296911170, 'issue_id': 2473691693, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74061/checks?check_run_id=28954715185) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 19, 15, 56, 2, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-19 15:56:02 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74061/checks?check_run_id=28954715185) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2473631626,pull_request,closed,,Add an option `poll_for_error_from_service_at_startup` to use long polling as the error propagation mechanism in distributed service.,"Add an option `poll_for_error_from_service_at_startup` to use long polling as the error propagation mechanism in distributed service.
",copybara-service[bot],2024-08-19 15:25:14+00:00,[],2024-08-19 16:18:29+00:00,2024-08-19 16:18:27+00:00,https://github.com/tensorflow/tensorflow/pull/74060,[],[],
2473599280,pull_request,closed,,Update pybind11 to v2.13.4.,"Update pybind11 to v2.13.4.

v2.13.0 or later is needed for free-threading/nogil support under Python 3.13.
",copybara-service[bot],2024-08-19 15:09:52+00:00,[],2024-08-19 18:04:14+00:00,2024-08-19 18:04:13+00:00,https://github.com/tensorflow/tensorflow/pull/74059,[],[],
2473539645,pull_request,closed,,Fix : Argument missing in TPUClusterResolver.connect,FIX: `TPUClusterResolver.connect` missing argument,edwardyehuang,2024-08-19 14:43:12+00:00,['gbaned'],2024-09-20 18:24:50+00:00,2024-09-20 18:24:50+00:00,https://github.com/tensorflow/tensorflow/pull/74058,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2311670986, 'issue_id': 2473539645, 'author': 'keerthanakadiri', 'body': 'Hi @fionalang, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 27, 6, 24, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323994241, 'issue_id': 2473539645, 'author': 'keerthanakadiri', 'body': 'Hi @fionalang, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 2, 7, 20, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332625125, 'issue_id': 2473539645, 'author': 'edwardyehuang', 'body': '> Hi @fionalang, Can you please review this PR? Thank you !\r\n\r\nHas Google completely abandoned TensorFlow? If so, I will shift my development focus to Jax. @keerthanakadiri @gbaned', 'created_at': datetime.datetime(2024, 9, 5, 21, 2, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355062029, 'issue_id': 2473539645, 'author': 'keerthanakadiri', 'body': '> > Hi @fionalang, Can you please review this PR? Thank you !\r\n> \r\n> Has Google completely abandoned TensorFlow? If so, I will shift my development focus to Jax. @keerthanakadiri @gbaned\r\n\r\nHi @edwardyehuang , Sorry for the delay in response.  This PR is in awaiting review status. Thank you for your patience and understanding.', 'created_at': datetime.datetime(2024, 9, 17, 9, 39, 3, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-08-27 06:24:34 UTC): Hi @fionalang, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-02 07:20:18 UTC): Hi @fionalang, Can you please review this PR? Thank you !

edwardyehuang (Issue Creator) on (2024-09-05 21:02:58 UTC): Has Google completely abandoned TensorFlow? If so, I will shift my development focus to Jax. @keerthanakadiri @gbaned

keerthanakadiri on (2024-09-17 09:39:03 UTC): Hi @edwardyehuang , Sorry for the delay in response.  This PR is in awaiting review status. Thank you for your patience and understanding.

"
2473492206,pull_request,closed,,"#tf-data-service Optionally, allocate to pinned memory in data service client.","#tf-data-service Optionally, allocate to pinned memory in data service client.
",copybara-service[bot],2024-08-19 14:22:26+00:00,['mpcallanan'],2024-08-26 19:32:35+00:00,2024-08-26 19:32:35+00:00,https://github.com/tensorflow/tensorflow/pull/74056,[],[],
2473467324,pull_request,closed,,PR #16210: [ROCm] Disable triton gemm while running determinsim test,"PR #16210: [ROCm] Disable triton gemm while running determinsim test

Imported from GitHub PR https://github.com/openxla/xla/pull/16210


Copybara import of the project:

--
13a2234e0256690523cbbac6270e3a5e22abff02 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Disable triton gemm while running determinsim test

Merging this change closes #16210

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16210 from ROCm:ci_disable_triton_20240819 13a2234e0256690523cbbac6270e3a5e22abff02
",copybara-service[bot],2024-08-19 14:11:14+00:00,[],2024-08-19 21:25:27+00:00,2024-08-19 21:25:26+00:00,https://github.com/tensorflow/tensorflow/pull/74055,[],[],
2473465346,pull_request,closed,,#tf-data-service Don't force local protocol in unit tests.,"#tf-data-service Don't force local protocol in unit tests.
",copybara-service[bot],2024-08-19 14:10:18+00:00,['mpcallanan'],2024-08-19 21:17:01+00:00,2024-08-19 21:17:00+00:00,https://github.com/tensorflow/tensorflow/pull/74054,[],[],
2473276438,pull_request,closed,,[XLA:GPU] Fix asan bug in Triton fusion emitter in int4 support code.,"[XLA:GPU] Fix asan bug in Triton fusion emitter in int4 support code.
Asan fails when the params.size() is zero on the line *params.cbegin().
",copybara-service[bot],2024-08-19 12:46:30+00:00,[],2024-08-19 13:41:15+00:00,2024-08-19 13:41:14+00:00,https://github.com/tensorflow/tensorflow/pull/74053,[],[],
2473261486,pull_request,open,,Remove more GPU/CUDA/ROCm attribute guards from xla/service/gpu,"Remove more GPU/CUDA/ROCm attribute guards from xla/service/gpu

- This removes `if_gpu_is_configured` guards from targets that are only supposed to be built for GPU. (Also tags them as `gpu` so that they get automatically excluded from the CPU build)
- This removes a bunch of `local_defines = if_cuda_is_configured([GOOGLE_CUDA])` parameters from targets that don't use the `GOOGLE_CUDA` preprocessor definition (anymore).
- Same for `TENSORFLOW_USE_ROCM`
- It tags many gpu-only targets with `gpu` so that they won't be built in a CPU-only build.
",copybara-service[bot],2024-08-19 12:39:18+00:00,[],2024-10-03 17:16:50+00:00,,https://github.com/tensorflow/tensorflow/pull/74052,[],[],
2473248381,pull_request,open,,[XLA:GPU] enable int4 support for triton_gemm by default,"[XLA:GPU] enable int4 support for triton_gemm by default
",copybara-service[bot],2024-08-19 12:32:43+00:00,[],2024-08-19 12:32:43+00:00,,https://github.com/tensorflow/tensorflow/pull/74051,[],[],
2473213872,pull_request,closed,,[XLA:GPU] move legacy_triton namespace from triton_support.{h|cc} to triton_support_legacy....,"[XLA:GPU] move legacy_triton namespace from triton_support.{h|cc} to triton_support_legacy....

We have a set of new triton functions and a set of legacy triton functions. In some cases they even named the same way. Let's move the legacy ones to the legacy files and reduce the confusion. CanTritonHandleReduce was not migrated because it is in use only by the new implementation but not yet migrated to the new infrastructure.

It is a pure mechanical move.
",copybara-service[bot],2024-08-19 12:14:53+00:00,[],2024-08-20 10:13:42+00:00,2024-08-20 10:13:41+00:00,https://github.com/tensorflow/tensorflow/pull/74050,[],[],
2473151489,pull_request,closed,,[xla:cpu] Fix f32->bf16 NaN conversion. (Retain sign bit.),"[xla:cpu] Fix f32->bf16 NaN conversion. (Retain sign bit.)

+ Update the convert_test to catch the difference between NaN and -NaN.
",copybara-service[bot],2024-08-19 11:40:54+00:00,['penpornk'],2024-08-19 13:20:33+00:00,2024-08-19 13:20:32+00:00,https://github.com/tensorflow/tensorflow/pull/74047,[],[],
2473035223,pull_request,closed,,[XLA:GPU] Add tests to ensure that all HLO opcodes are either tested or known to be untested in Triton support tests.,"[XLA:GPU] Add tests to ensure that all HLO opcodes are either tested or known to be untested in Triton support tests.

This ensures that we know exactly what's covered and that coverage will be tracked correctly in case HLO Ops are added.
",copybara-service[bot],2024-08-19 10:38:37+00:00,[],2024-08-20 10:21:28+00:00,2024-08-20 10:21:27+00:00,https://github.com/tensorflow/tensorflow/pull/74046,[],[],
2472987473,pull_request,closed,,Bump shardy hash,"Bump shardy hash
",copybara-service[bot],2024-08-19 10:13:46+00:00,[],2024-08-19 12:55:57+00:00,2024-08-19 12:55:56+00:00,https://github.com/tensorflow/tensorflow/pull/74045,[],[],
2472976316,pull_request,closed,,Support transposes with more than 3 dimensions.,"Support transposes with more than 3 dimensions.

We tile the most minor dimension of the transpose operand and the dimension
that becomes the most minor dimension in the output. This was already the
case for the transposes we supported, but some code assumed that the transpose
will be a direct swap. This change removes this assumption.
",copybara-service[bot],2024-08-19 10:08:25+00:00,['akuegel'],2024-08-20 08:33:50+00:00,2024-08-20 08:33:49+00:00,https://github.com/tensorflow/tensorflow/pull/74044,[],[],
2472782101,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2024-08-19 08:33:11+00:00,[],2024-08-20 20:49:16+00:00,2024-08-20 20:49:16+00:00,https://github.com/tensorflow/tensorflow/pull/74042,[],[],
2472763847,pull_request,closed,,Remove optimized dynamically quantized BMM due to bug with scaling factors,"Remove optimized dynamically quantized BMM due to bug with scaling factors

XNNPack can be used for all models now
",copybara-service[bot],2024-08-19 08:25:16+00:00,['alankelly'],2024-08-19 10:12:23+00:00,2024-08-19 10:12:22+00:00,https://github.com/tensorflow/tensorflow/pull/74041,[],[],
2472634307,pull_request,closed,,"[XLA:GPU] Enable --xla_gpu_enable_pipelined_{all_gather,all_reduce,reduce_scatter} by default.","[XLA:GPU] Enable --xla_gpu_enable_pipelined_{all_gather,all_reduce,reduce_scatter} by default.
",copybara-service[bot],2024-08-19 07:16:59+00:00,[],2024-08-20 08:12:27+00:00,2024-08-20 08:12:27+00:00,https://github.com/tensorflow/tensorflow/pull/74040,[],[],
2472609548,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-19 07:01:49+00:00,[],2024-08-21 08:04:35+00:00,,https://github.com/tensorflow/tensorflow/pull/74039,[],[],
2472608145,pull_request,closed,,Replace StaticDeviceMgr with DynamicDeviceMgr for files under //third_party/tensorflow/core/tfrt/.,"Replace StaticDeviceMgr with DynamicDeviceMgr for files under //third_party/tensorflow/core/tfrt/.

StaticDeviceMgr is an alias of DynamicDeviceMgr.
",copybara-service[bot],2024-08-19 07:01:00+00:00,[],2024-08-30 18:43:32+00:00,2024-08-30 18:43:30+00:00,https://github.com/tensorflow/tensorflow/pull/74038,[],[],
2472563419,pull_request,closed,,[XLA:GPU] NFC: Avoid redundant BitcastMap.,"[XLA:GPU] NFC: Avoid redundant BitcastMap.

There is no need to first bitcast to the hero shape and then to the root shape.
We can directly bitcast to the root shape.
",copybara-service[bot],2024-08-19 06:30:35+00:00,['akuegel'],2024-08-19 08:19:06+00:00,2024-08-19 08:19:06+00:00,https://github.com/tensorflow/tensorflow/pull/74037,[],[],
2472348335,pull_request,closed,,Add support for conv to resize.,"Add support for conv to resize.

This CL adds support for conv to resize in StableHLO. Some convolutions that can be represented as resize_bilinear ops are better when run as resize ops.
",copybara-service[bot],2024-08-19 02:57:45+00:00,['vamsimanchala'],2024-08-20 01:08:23+00:00,2024-08-20 01:08:23+00:00,https://github.com/tensorflow/tensorflow/pull/74036,[],[],
2472157306,pull_request,closed,,[xla:cpu] Add benchmark for elementwise f32->bf16 conversion,"[xla:cpu] Add benchmark for elementwise f32->bf16 conversion
",copybara-service[bot],2024-08-18 21:39:52+00:00,['ezhulenev'],2024-08-19 07:51:10+00:00,2024-08-19 07:51:10+00:00,https://github.com/tensorflow/tensorflow/pull/74034,[],[],
2472156873,pull_request,closed,,[xla:cpu] Use bitcasts for bf16<->f32 conversions in ElementalIrEmitter,"[xla:cpu] Use bitcasts for bf16<->f32 conversions in ElementalIrEmitter

name                           old cpu/op   new cpu/op   delta
BM_AddF32/128/process_time     22.3µs ±18%  21.5µs ±23%     ~     
BM_AddF32/256/process_time     80.0µs ± 8%  79.7µs ±11%     ~     
BM_AddF32/512/process_time      121µs ±11%   122µs ± 8%     ~     
BM_AddF32/1024/process_time     196µs ±10%   196µs ± 8%     ~     
BM_AddF32/8192/process_time    3.13ms ±21%  3.12ms ±22%     ~     
BM_AddF32/16384/process_time   7.28ms ± 5%  7.23ms ± 5%     ~     
BM_AddBF16/128/process_time     133µs ± 1%    14µs ± 3%  -89.81%  
BM_AddBF16/256/process_time     265µs ± 2%    44µs ± 3%  -83.45%  
BM_AddBF16/512/process_time     641µs ±21%   101µs ±10%  -84.16%  
BM_AddBF16/1024/process_time   1.21ms ±24%  0.16ms ±18%  -86.40%  
BM_AddBF16/8192/process_time   9.25ms ±25%  1.08ms ±18%  -88.35%  
BM_AddBF16/16384/process_time  18.8ms ±24%   3.1ms ±19%  -83.33%  

name                           old time/op          new time/op   delta
BM_AddF32/128/process_time     12.2µs ±22%          11.7µs ±28%   -4.09%  
BM_AddF32/256/process_time     33.0µs ± 8%          32.9µs ± 7%     ~     
BM_AddF32/512/process_time     45.5µs ± 9%          45.6µs ± 8%     ~     
BM_AddF32/1024/process_time    69.2µs ±10%          69.0µs ± 8%     ~     
BM_AddF32/8192/process_time    1.02ms ±16%          1.01ms ±14%     ~     
BM_AddF32/16384/process_time   2.16ms ± 8%          2.12ms ± 9%     ~     
BM_AddBF16/128/process_time     133µs ± 1%            14µs ± 2%  -89.84%  
BM_AddBF16/256/process_time     136µs ± 2%            24µs ± 3%  -82.14%  
BM_AddBF16/512/process_time     186µs ±30%            38µs ±10%  -79.37%  
BM_AddBF16/1024/process_time    341µs ±41%            55µs ±19%  -83.79%  
BM_AddBF16/8192/process_time   2.52ms ±46%          0.33ms ±25%  -86.79%  
BM_AddBF16/16384/process_time  5.18ms ±43%          1.00ms ±17%  -80.69%
",copybara-service[bot],2024-08-18 21:38:55+00:00,['ezhulenev'],2024-08-19 07:31:58+00:00,2024-08-19 07:31:56+00:00,https://github.com/tensorflow/tensorflow/pull/74033,[],[],
2472143315,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 21:06:58+00:00,[],2024-08-22 09:50:20+00:00,2024-08-22 09:50:19+00:00,https://github.com/tensorflow/tensorflow/pull/74032,[],[],
2472132032,pull_request,closed,,"Add an implementation of Tile() that uses the HloShardingV2 representation. The implementation currently only supports ""iota"" device meshes i.e. device meshes where the device mesh ids are contiguous. This will be generalized in future changes.","Add an implementation of Tile() that uses the HloShardingV2 representation. The implementation currently only supports ""iota"" device meshes i.e. device meshes where the device mesh ids are contiguous. This will be generalized in future changes.
",copybara-service[bot],2024-08-18 20:29:47+00:00,[],2024-08-19 20:25:13+00:00,2024-08-19 20:25:13+00:00,https://github.com/tensorflow/tensorflow/pull/74031,[],[],
2472123036,pull_request,closed,,"Create an explicit DeviceMesh struct to track. This is currently used to track if the device mesh is a contiguous list of values. In future CLs, this will be used to determine whether or not to use the HloShardingV2 (iota shardings) sharding representation.","Create an explicit DeviceMesh struct to track. This is currently used to track if the device mesh is a contiguous list of values. In future CLs, this will be used to determine whether or not to use the HloShardingV2 (iota shardings) sharding representation.
",copybara-service[bot],2024-08-18 20:02:18+00:00,[],2024-08-19 16:44:59+00:00,2024-08-19 16:44:57+00:00,https://github.com/tensorflow/tensorflow/pull/74030,[],[],
2472113968,pull_request,closed,,Add a test for F32->BF16 conversion.,"Add a test for F32->BF16 conversion.
",copybara-service[bot],2024-08-18 19:34:18+00:00,['penpornk'],2024-08-18 20:10:09+00:00,2024-08-18 20:10:08+00:00,https://github.com/tensorflow/tensorflow/pull/74029,[],[],
2472102145,pull_request,closed,,[xla:cpu] Add benchmark for elementwise bf16 add,"[xla:cpu] Add benchmark for elementwise bf16 add
",copybara-service[bot],2024-08-18 19:00:05+00:00,['ezhulenev'],2024-08-18 19:48:15+00:00,2024-08-18 19:48:14+00:00,https://github.com/tensorflow/tensorflow/pull/74028,[],[],
2472019528,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 15:17:15+00:00,[],2024-08-18 15:17:15+00:00,,https://github.com/tensorflow/tensorflow/pull/74027,[],[],
2471777928,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 04:09:37+00:00,[],2024-08-18 04:09:37+00:00,,https://github.com/tensorflow/tensorflow/pull/74026,[],[],
2471777004,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 04:05:56+00:00,[],2024-08-18 04:05:56+00:00,,https://github.com/tensorflow/tensorflow/pull/74025,[],[],
2471776960,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 04:05:46+00:00,[],2024-08-18 04:05:46+00:00,,https://github.com/tensorflow/tensorflow/pull/74024,[],[],
2471751498,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-18 02:31:28+00:00,[],2024-08-18 02:31:28+00:00,,https://github.com/tensorflow/tensorflow/pull/74023,[],[],
2471735365,pull_request,closed,,Rollback of [XLA:SPMD] Remove LookaheadUserSharding in sharding propagation.,"Rollback of [XLA:SPMD] Remove LookaheadUserSharding in sharding propagation.

Reverts 339d798df2aff84af89bf269cab2ff3743dea4be
",copybara-service[bot],2024-08-18 01:23:01+00:00,[],2024-08-18 02:40:25+00:00,2024-08-18 02:40:25+00:00,https://github.com/tensorflow/tensorflow/pull/74022,[],[],
2471687734,pull_request,closed,,"If input layouts are specified via `in_shardings` to `jit` and the array that the jitted function is called with is uncommitted, reshard the input array to the layout specified by the user.","If input layouts are specified via `in_shardings` to `jit` and the array that the jitted function is called with is uncommitted, reshard the input array to the layout specified by the user.

Not doing the resharding, leads to incorrect outputs on GPU and a crash on TPU which is not good.

Fixes: https://github.com/google/jax/issues/23100
",copybara-service[bot],2024-08-17 22:09:46+00:00,['yashk2810'],2024-08-19 23:51:14+00:00,2024-08-19 23:51:13+00:00,https://github.com/tensorflow/tensorflow/pull/74021,[],[],
2471671328,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 21:06:40+00:00,[],2024-08-19 07:58:08+00:00,2024-08-19 07:58:07+00:00,https://github.com/tensorflow/tensorflow/pull/74020,[],[],
2471668222,pull_request,open,,Add direct legalization for min and max.,"Add direct legalization for min and max.
",copybara-service[bot],2024-08-17 20:54:59+00:00,['LukeBoyer'],2024-08-17 20:55:00+00:00,,https://github.com/tensorflow/tensorflow/pull/74019,[],[],
2471579494,pull_request,open,,Make GPU benchmark run 10 times instead of once to reduce noise,"Make GPU benchmark run 10 times instead of once to reduce noise
",copybara-service[bot],2024-08-17 16:08:30+00:00,['khanhlvg'],2024-08-17 16:25:25+00:00,,https://github.com/tensorflow/tensorflow/pull/74018,[],[],
2471464123,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 10:15:34+00:00,[],2024-08-20 11:57:41+00:00,2024-08-20 11:57:40+00:00,https://github.com/tensorflow/tensorflow/pull/74017,[],[],
2471462511,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 10:10:45+00:00,[],2024-08-20 12:45:47+00:00,2024-08-20 12:45:47+00:00,https://github.com/tensorflow/tensorflow/pull/74016,[],[],
2471435637,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 08:47:36+00:00,[],2024-08-17 11:55:07+00:00,2024-08-17 11:55:07+00:00,https://github.com/tensorflow/tensorflow/pull/74014,[],[],
2471399097,pull_request,closed,,"Correct ""seperated"" to ""separated""",fix typo,shengyu7697,2024-08-17 07:16:46+00:00,['gbaned'],2024-09-03 17:03:59+00:00,2024-09-03 17:03:59+00:00,https://github.com/tensorflow/tensorflow/pull/74013,"[('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2294731263, 'issue_id': 2471399097, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74013/checks?check_run_id=28889101796) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 8, 17, 7, 16, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295809974, 'issue_id': 2471399097, 'author': 'keerthanakadiri', 'body': 'Hi @shengyu7697, Can you please sign CLA , thank you !', 'created_at': datetime.datetime(2024, 8, 19, 7, 1, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311676735, 'issue_id': 2471399097, 'author': 'keerthanakadiri', 'body': 'Hi @shengyu7697, Can you please sign CLA , thank you !', 'created_at': datetime.datetime(2024, 8, 27, 6, 28, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322854180, 'issue_id': 2471399097, 'author': 'shengyu7697', 'body': 'I have already signed the CLA.', 'created_at': datetime.datetime(2024, 8, 31, 10, 25, 18, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-08-17 07:16:51 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/74013/checks?check_run_id=28889101796) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

keerthanakadiri on (2024-08-19 07:01:23 UTC): Hi @shengyu7697, Can you please sign CLA , thank you !

keerthanakadiri on (2024-08-27 06:28:46 UTC): Hi @shengyu7697, Can you please sign CLA , thank you !

shengyu7697 (Issue Creator) on (2024-08-31 10:25:18 UTC): I have already signed the CLA.

"
2471395461,pull_request,closed,,[tsl:concurrency] Add AsyncValueConstructors from a functor + Executor,"[tsl:concurrency] Add AsyncValueConstructors from a functor + Executor
",copybara-service[bot],2024-08-17 07:08:59+00:00,['ezhulenev'],2024-08-19 18:23:55+00:00,2024-08-19 18:23:54+00:00,https://github.com/tensorflow/tensorflow/pull/74012,[],[],
2471392578,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 07:02:35+00:00,[],2024-08-17 07:02:35+00:00,,https://github.com/tensorflow/tensorflow/pull/74011,[],[],
2471355091,pull_request,closed,,Fixing typos in comments,"Fixing typos in comments
",copybara-service[bot],2024-08-17 05:39:49+00:00,[],2024-08-17 05:48:17+00:00,2024-08-17 05:48:17+00:00,https://github.com/tensorflow/tensorflow/pull/74010,[],[],
2471345260,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 05:14:26+00:00,[],2024-08-17 05:14:26+00:00,,https://github.com/tensorflow/tensorflow/pull/74009,[],[],
2471336351,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:50:56+00:00,[],2024-08-17 04:50:56+00:00,,https://github.com/tensorflow/tensorflow/pull/74008,[],[],
2471336341,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:50:53+00:00,[],2024-08-17 04:50:53+00:00,,https://github.com/tensorflow/tensorflow/pull/74007,[],[],
2471335857,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:49:53+00:00,[],2024-08-17 06:00:55+00:00,,https://github.com/tensorflow/tensorflow/pull/74006,[],[],
2471333854,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:44:34+00:00,[],2024-08-17 04:44:34+00:00,,https://github.com/tensorflow/tensorflow/pull/74005,[],[],
2471332505,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:40:10+00:00,[],2024-08-17 04:40:10+00:00,,https://github.com/tensorflow/tensorflow/pull/74004,[],[],
2471331576,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:37:11+00:00,[],2024-08-22 05:35:48+00:00,2024-08-22 05:35:48+00:00,https://github.com/tensorflow/tensorflow/pull/74003,[],[],
2471331410,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:36:33+00:00,[],2024-08-17 04:36:33+00:00,,https://github.com/tensorflow/tensorflow/pull/74002,[],[],
2471330559,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:34:15+00:00,[],2024-08-17 04:34:15+00:00,,https://github.com/tensorflow/tensorflow/pull/74001,[],[],
2471328077,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:28:53+00:00,[],2024-08-17 04:28:53+00:00,,https://github.com/tensorflow/tensorflow/pull/74000,[],[],
2471327653,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:27:37+00:00,[],2024-08-22 06:20:37+00:00,,https://github.com/tensorflow/tensorflow/pull/73999,[],[],
2471327217,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:25:54+00:00,[],2024-08-17 04:25:54+00:00,,https://github.com/tensorflow/tensorflow/pull/73998,[],[],
2471323569,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:15:21+00:00,[],2024-08-17 04:15:21+00:00,,https://github.com/tensorflow/tensorflow/pull/73997,[],[],
2471323106,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:14:17+00:00,[],2024-08-17 04:14:17+00:00,,https://github.com/tensorflow/tensorflow/pull/73996,[],[],
2471322517,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:12:39+00:00,[],2024-08-20 11:46:05+00:00,2024-08-20 11:46:04+00:00,https://github.com/tensorflow/tensorflow/pull/73995,[],[],
2471322428,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 04:12:28+00:00,[],2024-08-17 04:12:28+00:00,,https://github.com/tensorflow/tensorflow/pull/73994,[],[],
2471310240,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-17 03:29:29+00:00,[],2024-08-20 12:03:32+00:00,2024-08-20 12:03:30+00:00,https://github.com/tensorflow/tensorflow/pull/73993,[],[],
2471280762,pull_request,closed,,Avoid read-write race on state,"Avoid read-write race on state
",copybara-service[bot],2024-08-17 02:21:34+00:00,[],2024-08-17 03:43:26+00:00,2024-08-17 03:43:26+00:00,https://github.com/tensorflow/tensorflow/pull/73992,[],[],
2471259268,pull_request,closed,,Add direct legalizations for sub op.,"Add direct legalizations for sub op.
",copybara-service[bot],2024-08-17 01:36:03+00:00,['LukeBoyer'],2024-08-20 21:39:45+00:00,2024-08-20 21:39:44+00:00,https://github.com/tensorflow/tensorflow/pull/73991,[],[],
2471251474,pull_request,closed,,Proto bfc_memory_map change to compiler/xla/tsl,"Proto bfc_memory_map change to compiler/xla/tsl
",copybara-service[bot],2024-08-17 01:13:48+00:00,[],2024-08-20 01:16:09+00:00,2024-08-20 01:16:08+00:00,https://github.com/tensorflow/tensorflow/pull/73990,[],[],
2471250866,pull_request,closed,,Add direct legalization for atan2.,"Add direct legalization for atan2.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14689 from Tixxx:tixxx/unroll_cm b64cb889d8ef1c5afd01676766095de77c5102e3
",copybara-service[bot],2024-08-17 01:11:47+00:00,['LukeBoyer'],2024-08-23 21:12:19+00:00,2024-08-23 21:12:19+00:00,https://github.com/tensorflow/tensorflow/pull/73989,[],[],
2471249942,pull_request,closed,,Add direct legalizations for pow.,"Add direct legalizations for pow.
",copybara-service[bot],2024-08-17 01:09:00+00:00,['LukeBoyer'],2024-08-20 13:29:07+00:00,2024-08-20 13:29:05+00:00,https://github.com/tensorflow/tensorflow/pull/73988,[],[],
2471246170,pull_request,closed,,Fold sub ops when rhs is zero.,"Fold sub ops when rhs is zero.
",copybara-service[bot],2024-08-17 00:57:25+00:00,['LukeBoyer'],2024-08-20 21:32:22+00:00,2024-08-20 21:32:21+00:00,https://github.com/tensorflow/tensorflow/pull/73987,[],[],
2471243797,pull_request,closed,,Add direct legalizations for div op.,"Add direct legalizations for div op.
",copybara-service[bot],2024-08-17 00:49:41+00:00,['LukeBoyer'],2024-08-20 19:13:17+00:00,2024-08-20 19:13:16+00:00,https://github.com/tensorflow/tensorflow/pull/73986,[],[],
2471242455,pull_request,closed,,Rename and make `xla::ifrt::MemoryKind::DebugString` private.,"Rename and make `xla::ifrt::MemoryKind::DebugString` private.
",copybara-service[bot],2024-08-17 00:45:37+00:00,[],2024-08-19 21:56:06+00:00,2024-08-19 21:56:04+00:00,https://github.com/tensorflow/tensorflow/pull/73985,[],[],
2471241808,pull_request,closed,,Add folding for div when rhs is 1.,"Add folding for div when rhs is 1.
",copybara-service[bot],2024-08-17 00:43:44+00:00,['LukeBoyer'],2024-08-20 12:39:25+00:00,2024-08-20 12:39:24+00:00,https://github.com/tensorflow/tensorflow/pull/73984,[],[],
2471240205,pull_request,closed,,Add direct legalizations for add op.,"Add direct legalizations for add op.
",copybara-service[bot],2024-08-17 00:39:04+00:00,['LukeBoyer'],2024-08-27 23:19:38+00:00,2024-08-27 23:19:36+00:00,https://github.com/tensorflow/tensorflow/pull/73983,[],[],
2471237978,pull_request,closed,,Fold adds when one of the inputs are zero.,"Fold adds when one of the inputs are zero.
",copybara-service[bot],2024-08-17 00:33:28+00:00,['LukeBoyer'],2024-08-27 22:06:36+00:00,2024-08-27 22:06:36+00:00,https://github.com/tensorflow/tensorflow/pull/73982,[],[],
2471229801,pull_request,closed,,Add direct legalizations for mul.,"Add direct legalizations for mul.
",copybara-service[bot],2024-08-17 00:22:46+00:00,['LukeBoyer'],2024-08-20 12:13:14+00:00,2024-08-20 12:13:13+00:00,https://github.com/tensorflow/tensorflow/pull/73981,[],[],
2471220305,pull_request,closed,,Rely on `AbslStringify` instead of `DebugString` when printing an `xla::ifrt::MemoryKind` object.,"Rely on `AbslStringify` instead of `DebugString` when printing an `xla::ifrt::MemoryKind` object.
",copybara-service[bot],2024-08-17 00:17:09+00:00,[],2024-08-17 01:13:22+00:00,2024-08-17 01:13:21+00:00,https://github.com/tensorflow/tensorflow/pull/73980,[],[],
2471193046,pull_request,closed,,Fold muls when one side is a constant 1 or zero.,"Fold muls when one side is a constant 1 or zero.
",copybara-service[bot],2024-08-16 23:53:43+00:00,['LukeBoyer'],2024-08-20 11:51:40+00:00,2024-08-20 11:51:38+00:00,https://github.com/tensorflow/tensorflow/pull/73979,[],[],
2471177151,pull_request,closed,,[XLA:HloParser] Add a flag `set_to_default_entry_computation_layout` with default value true.,"[XLA:HloParser] Add a flag `set_to_default_entry_computation_layout` with default value true.

If it is false, do not overwrite the raw input with the default layout in entry_compuation_layout. If entry_compuation_layout is defined explicitly, we should exactly follow the raw definition. For instance, if the raw input has only shape and does not have layout, we should not set the default layout.

If entry_compuation_layout is not defined explicitly, we still infer it from the parameter and root instructions of entry computation. The layout of these instructions can be either explicitly defined or the default one.
",copybara-service[bot],2024-08-16 23:32:50+00:00,[],2024-09-11 00:18:57+00:00,2024-09-11 00:18:56+00:00,https://github.com/tensorflow/tensorflow/pull/73977,[],[],
2471176423,pull_request,closed,,Add cast folding from bool to f32,"Add cast folding from bool to f32
",copybara-service[bot],2024-08-16 23:31:30+00:00,['LukeBoyer'],2024-08-20 11:40:24+00:00,2024-08-20 11:40:23+00:00,https://github.com/tensorflow/tensorflow/pull/73976,[],[],
2471173760,pull_request,closed,,[IFRT] Add pass that outline an IFRT IR atom program to a module.,"[IFRT] Add pass that outline an IFRT IR atom program to a module.
",copybara-service[bot],2024-08-16 23:27:08+00:00,[],2024-08-17 03:49:53+00:00,2024-08-17 03:49:52+00:00,https://github.com/tensorflow/tensorflow/pull/73975,[],[],
2471172828,pull_request,closed,,Add folding for relu op.,"Add folding for relu op.
",copybara-service[bot],2024-08-16 23:25:55+00:00,['LukeBoyer'],2024-08-21 00:07:56+00:00,2024-08-21 00:07:55+00:00,https://github.com/tensorflow/tensorflow/pull/73974,[],[],
2471169907,pull_request,closed,,Add direct legalization for min and max.,"Add direct legalization for min and max.
",copybara-service[bot],2024-08-16 23:22:26+00:00,['LukeBoyer'],2024-08-17 20:47:26+00:00,2024-08-17 20:47:25+00:00,https://github.com/tensorflow/tensorflow/pull/73973,[],[],
2471169416,pull_request,closed,,Check for infinity when folding max and min ops.,"Check for infinity when folding max and min ops.
",copybara-service[bot],2024-08-16 23:21:45+00:00,['LukeBoyer'],2024-08-17 01:35:03+00:00,2024-08-17 01:35:02+00:00,https://github.com/tensorflow/tensorflow/pull/73972,[],[],
2471143929,pull_request,open,,Include CUDA headers in target dependencies for all GPU builds and tests.,"Include CUDA headers in target dependencies for all GPU builds and tests.
",copybara-service[bot],2024-08-16 22:50:27+00:00,[],2024-08-16 23:52:03+00:00,,https://github.com/tensorflow/tensorflow/pull/73971,[],[],
2471143201,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 22:49:16+00:00,[],2024-08-17 00:19:30+00:00,2024-08-17 00:19:29+00:00,https://github.com/tensorflow/tensorflow/pull/73970,[],[],
2471142545,pull_request,open,,Load input tensors from tf example proto file,"Load input tensors from tf example proto file
",copybara-service[bot],2024-08-16 22:48:09+00:00,[],2024-08-16 23:04:55+00:00,,https://github.com/tensorflow/tensorflow/pull/73969,[],[],
2471117661,pull_request,closed,,Add matchers for TfLiteTensor.,"Add matchers for TfLiteTensor.
",copybara-service[bot],2024-08-16 22:21:20+00:00,['impjdi'],2024-08-19 17:05:46+00:00,2024-08-19 17:05:45+00:00,https://github.com/tensorflow/tensorflow/pull/73968,[],[],
2471098765,pull_request,closed,,Delete `tsl/lib/core/status_test_util.h` now that all users have been moved,"Delete `tsl/lib/core/status_test_util.h` now that all users have been moved
",copybara-service[bot],2024-08-16 22:01:20+00:00,['ddunl'],2024-08-19 22:38:41+00:00,2024-08-19 22:38:40+00:00,https://github.com/tensorflow/tensorflow/pull/73967,[],[],
2471094831,pull_request,open,,Remove --tf_mlir_enable_mlir_bridge=true from TF_XLA_FLAGS in tests.,"Remove --tf_mlir_enable_mlir_bridge=true from TF_XLA_FLAGS in tests.

Reverts f8a3e0d08f3a35ecc04448c896185fc8825af2e6

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15998 from openxla:fix_kernel_cache b51fbcac5c7f172d06eb9de79770564f2e2c1250
",copybara-service[bot],2024-08-16 21:56:41+00:00,[],2024-08-16 21:56:41+00:00,,https://github.com/tensorflow/tensorflow/pull/73966,[],[],
2471079705,pull_request,open,,Remove cc_api_version stage 4: deletion where cc_api_version = 2,"Remove cc_api_version stage 4: deletion where cc_api_version = 2

Reverts f8a3e0d08f3a35ecc04448c896185fc8825af2e6

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15998 from openxla:fix_kernel_cache b51fbcac5c7f172d06eb9de79770564f2e2c1250
",copybara-service[bot],2024-08-16 21:39:59+00:00,[],2024-08-16 21:39:59+00:00,,https://github.com/tensorflow/tensorflow/pull/73965,[],[],
2471076020,pull_request,closed,,Expose the utility function to check if a given node can trigger XLA compilation,"Expose the utility function to check if a given node can trigger XLA compilation
",copybara-service[bot],2024-08-16 21:36:22+00:00,[],2024-08-17 00:26:58+00:00,2024-08-17 00:26:57+00:00,https://github.com/tensorflow/tensorflow/pull/73964,[],[],
2471068261,pull_request,open,,Simplify transpose(reshape(x)) with reshape(transpose(x)) when reshape only adds trivial dims.,"Simplify transpose(reshape(x)) with reshape(transpose(x)) when reshape only adds trivial dims.

Reverts f8a3e0d08f3a35ecc04448c896185fc8825af2e6

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15998 from openxla:fix_kernel_cache b51fbcac5c7f172d06eb9de79770564f2e2c1250
",copybara-service[bot],2024-08-16 21:28:24+00:00,[],2024-08-16 21:28:24+00:00,,https://github.com/tensorflow/tensorflow/pull/73963,[],[],
2471018237,pull_request,open,,Add profiling to the ifrt-proxy client that shows a trace of request-responses.,"Add profiling to the ifrt-proxy client that shows a trace of request-responses.
",copybara-service[bot],2024-08-16 21:00:05+00:00,[],2024-08-16 21:00:05+00:00,,https://github.com/tensorflow/tensorflow/pull/73962,[],[],
2471017589,pull_request,closed,,Update TFRT dependency to use revision,"Update TFRT dependency to use revision
http://github.com/tensorflow/runtime/commit/07992d7c1ead60f610c17b7c1f9e50b6898adc87.
",copybara-service[bot],2024-08-16 20:59:59+00:00,[],2024-08-16 23:23:24+00:00,2024-08-16 23:23:24+00:00,https://github.com/tensorflow/tensorflow/pull/73961,[],[],
2471000960,pull_request,closed,,[Numpy] Fix a bug in session.py that causes int overflow on NumPy 2.0.,"[Numpy] Fix a bug in session.py that causes int overflow on NumPy 2.0.

Creates a common numpy_compat library to centralize NumPy version compatibility functions, such as np_asarray.
",copybara-service[bot],2024-08-16 20:55:10+00:00,['kanglant'],2024-08-19 20:45:49+00:00,2024-08-19 20:45:48+00:00,https://github.com/tensorflow/tensorflow/pull/73960,[],[],
2470957335,pull_request,closed,,Remove hlo module name empty check from to fix internal issue,"Remove hlo module name empty check from to fix internal issue
",copybara-service[bot],2024-08-16 20:35:37+00:00,[],2024-08-16 22:47:31+00:00,2024-08-16 22:47:30+00:00,https://github.com/tensorflow/tensorflow/pull/73959,[],[],
2470947626,pull_request,closed,,#sdy Put MeshOp axes inside a pair of square brackets.,"#sdy Put MeshOp axes inside a pair of square brackets.

This is to prepare for supporting a permutation of an iota list of device IDs
for a mesh.
",copybara-service[bot],2024-08-16 20:30:04+00:00,['bixia1'],2024-08-20 23:11:56+00:00,2024-08-20 23:11:55+00:00,https://github.com/tensorflow/tensorflow/pull/73958,[],[],
2470944159,pull_request,open,,Add CompatibilityRequirement for JAX Export,"Add CompatibilityRequirement for JAX Export

```
# CompatibilityRequirement ::= NONE, WEEK_4, WEEK_12, MAX
export.export(f, compatibility_requirement=export.CompatibilityRequirement.MAX)
```
",copybara-service[bot],2024-08-16 20:28:15+00:00,['GleasonK'],2024-08-17 15:49:28+00:00,,https://github.com/tensorflow/tensorflow/pull/73957,[],[],
2470938314,pull_request,closed,,Update `xla::hlo_sharding_util::ReshapeSharding` for suffix shape size one.,"Update `xla::hlo_sharding_util::ReshapeSharding` for suffix shape size one.

We swap two patterns in the ReshapeSharding.
* Pattern 1, `s_partitions > 1 && s_size % s_partitions == 0 && t_size % s_partitions == 0`.
* Pattern 2, `s_size == t_size`.

Previously, we first check if we match the Pattern 1, then check Pattern 2. We swap the order in this cl since Pattern 2 is more general. When matching Pattern 1, we set `inplace_add_sharding_dim`, which is error-prone.

Here is an example with different behavior.
```
input shape: [4, 2, 1]
output shape: [4, 2]
input sharding: [4, 2, 4]

output sharding before this cl: [4, 8]
output sharding with this cl: [4, 2, 4] last_tile_dim_replicate
```
",copybara-service[bot],2024-08-16 20:25:02+00:00,[],2024-08-16 23:59:36+00:00,2024-08-16 23:59:36+00:00,https://github.com/tensorflow/tensorflow/pull/73956,[],[],
2470894861,pull_request,closed,,Add a new ModelRuntimeInfo proto and a function to generate it from an interpreter.,"Add a new ModelRuntimeInfo proto and a function to generate it from an interpreter.
",copybara-service[bot],2024-08-16 19:49:58+00:00,[],2024-08-30 17:21:14+00:00,2024-08-30 17:21:12+00:00,https://github.com/tensorflow/tensorflow/pull/73955,[],[],
2470880477,pull_request,closed,,Reverts b13762c17e6a05dbd099d8245748e00adcfd4579,"Reverts b13762c17e6a05dbd099d8245748e00adcfd4579
",copybara-service[bot],2024-08-16 19:37:51+00:00,['sagunb'],2024-08-16 20:30:49+00:00,2024-08-16 20:30:48+00:00,https://github.com/tensorflow/tensorflow/pull/73954,"[('ready to pull', 'PR ready for merge process')]",[],
2470852366,pull_request,closed,,Remove MLIR bridge flag usage.,"Remove MLIR bridge flag usage.
",copybara-service[bot],2024-08-16 19:28:28+00:00,[],2024-08-19 14:01:20+00:00,2024-08-19 14:01:18+00:00,https://github.com/tensorflow/tensorflow/pull/73953,[],[],
2470763357,pull_request,closed,,Add portability to toco_legacy quantize weights,"Add portability to toco_legacy quantize weights
",copybara-service[bot],2024-08-16 18:32:01+00:00,['pak-laura'],2024-08-19 22:02:10+00:00,2024-08-19 22:02:09+00:00,https://github.com/tensorflow/tensorflow/pull/73952,[],[],
2470712816,pull_request,closed,,Reverts f8a3e0d08f3a35ecc04448c896185fc8825af2e6,"Reverts f8a3e0d08f3a35ecc04448c896185fc8825af2e6

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16153 from openxla:fix_autotuner_timeout ea59210f7ec7bad918304af63684beb8dc8100e7
",copybara-service[bot],2024-08-16 17:54:47+00:00,['sagunb'],2024-08-16 21:57:47+00:00,2024-08-16 21:57:47+00:00,https://github.com/tensorflow/tensorflow/pull/73951,"[('ready to pull', 'PR ready for merge process')]",[],
2470703226,pull_request,closed,,An internal change,"An internal change
",copybara-service[bot],2024-08-16 17:46:50+00:00,[],2024-08-16 18:33:39+00:00,2024-08-16 18:33:39+00:00,https://github.com/tensorflow/tensorflow/pull/73950,[],[],
2470668642,pull_request,closed,,Minor fixes and documentation update for custom hermetic Python interpreter support.,"Minor fixes and documentation update for custom hermetic Python interpreter support.
",copybara-service[bot],2024-08-16 17:25:02+00:00,['vam-google'],2024-08-16 18:40:33+00:00,2024-08-16 18:40:32+00:00,https://github.com/tensorflow/tensorflow/pull/73949,[],[],
2470661576,pull_request,closed,,TF: Fix a bug where the batch_padding_policy wouldn't be propagated all the way from the flag to the model.,"TF: Fix a bug where the batch_padding_policy wouldn't be propagated all the way from the flag to the model.
",copybara-service[bot],2024-08-16 17:20:49+00:00,[],2024-08-19 12:36:36+00:00,2024-08-19 12:36:35+00:00,https://github.com/tensorflow/tensorflow/pull/73948,[],[],
2470656483,pull_request,closed,,Internal change only.,"Internal change only.
",copybara-service[bot],2024-08-16 17:17:57+00:00,['pak-laura'],2024-08-16 20:23:05+00:00,2024-08-16 20:23:04+00:00,https://github.com/tensorflow/tensorflow/pull/73947,[],[],
2470632576,pull_request,closed,,[XLA][HostOffloader] s/GetSuccessors/<namespace>::GetSuccessors,"[XLA][HostOffloader] s/GetSuccessors/<namespace>::GetSuccessors

Small update to also include namespace.
",copybara-service[bot],2024-08-16 17:04:14+00:00,[],2024-08-16 18:26:17+00:00,2024-08-16 18:26:15+00:00,https://github.com/tensorflow/tensorflow/pull/73945,[],[],
2470588571,pull_request,closed,,PR #15092: Support cuDNN frontend scaled dot product attention for FP8. Part- 1(forward),"PR #15092: Support cuDNN frontend scaled dot product attention for FP8. Part- 1(forward)

Imported from GitHub PR https://github.com/openxla/xla/pull/15092

Add cudnn frontend support of scaled dot product attention for FP8 forward. doc [here](https://github.com/NVIDIA/cudnn-frontend/blob/98ca4e1941fe3263f128f74f10063a3ea35c7019/docs/operations/Attention.md).

NOTE: this feature relies on cudnn-frontend v1.6.1 which is not in XLA yet.
Copybara import of the project:

--
0913bfb4daf5985e223725b0a38ab08100766483 by shuw <shuw@nvidia.com>:

Fwd compile

fwd_work

cleanup

fwd cleean

clang-format

Remove comments

add unittest

Improve after review 1

Improve after review 2

Rebase

--
a9f1bf9dfb908a66a6282b038570162579e6ed72 by shuw <shuw@nvidia.com>:

cleanup FMHA graph creation code

Merging this change closes #15092

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15092 from wenscarl:sdpa_fp8 a9f1bf9dfb908a66a6282b038570162579e6ed72
",copybara-service[bot],2024-08-16 16:32:33+00:00,[],2024-08-21 14:14:01+00:00,2024-08-21 14:14:00+00:00,https://github.com/tensorflow/tensorflow/pull/73944,[],[],
2470577848,pull_request,closed,,#sdy Change device_id=n to device_ids=[n] in the MeshOp assembly format.,"#sdy Change device_id=n to device_ids=[n] in the MeshOp assembly format.

This is to prepare for supporting a permutation of an iota list of device IDs
for a mesh.
",copybara-service[bot],2024-08-16 16:24:53+00:00,['bixia1'],2024-08-16 19:08:24+00:00,2024-08-16 19:08:23+00:00,https://github.com/tensorflow/tensorflow/pull/73943,[],[],
2470568813,pull_request,closed,,PR #16153: [GPU] Change sharded GEMM autotuning timeout from infinity to 24 hours.,"PR #16153: [GPU] Change sharded GEMM autotuning timeout from infinity to 24 hours.

Imported from GitHub PR https://github.com/openxla/xla/pull/16153

Fixes a problem with MPI: https://github.com/google/jax/issues/22995
Copybara import of the project:

--
df0dfdd323385fbd07fdb2f909240b9f6264c712 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Change sharded GEMM autotuning timeout from infinity to 24 hours.

--
ea59210f7ec7bad918304af63684beb8dc8100e7 by Ilia Sergachev <isergachev@nvidia.com>:

Infinite duration causes issues with MPI.

Merging this change closes #16153

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16153 from openxla:fix_autotuner_timeout ea59210f7ec7bad918304af63684beb8dc8100e7
",copybara-service[bot],2024-08-16 16:18:41+00:00,[],2024-08-16 17:53:01+00:00,2024-08-16 17:53:00+00:00,https://github.com/tensorflow/tensorflow/pull/73942,[],[],
2470560056,pull_request,closed,,PR #16127: Fix the driver version parsing in the diagnostics.,"PR #16127: Fix the driver version parsing in the diagnostics.

Imported from GitHub PR https://github.com/openxla/xla/pull/16127

The version number doesn't always follow ""Kernel Module"" immediately. Two spaces somewhere after ""Kernel Module"" are the signifier that the driver version follows.
Copybara import of the project:

--
a83946f8788b0918b384fe962c16c286eb16feda by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Fix the driver version parsing in the diagnostics.

The version number doesn't always follow ""Kernel Module"" immediately.
Two spaces somewhere after ""Kernel Module"" are the signifier that the driver version follows.

Merging this change closes #16127

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16127 from dimvar:fix-driver-version-parsing a83946f8788b0918b384fe962c16c286eb16feda
",copybara-service[bot],2024-08-16 16:12:32+00:00,[],2024-08-16 21:38:23+00:00,2024-08-16 21:38:22+00:00,https://github.com/tensorflow/tensorflow/pull/73941,[],[],
2470554962,pull_request,closed,,Move python bindings to portable apis that use C-API datatypes.,"Move python bindings to portable apis that use C-API datatypes.

Add `stablehlo.get_version_from_compatibility_requirement` API.

Suffix string APIs with `_str`.
",copybara-service[bot],2024-08-16 16:09:14+00:00,['GleasonK'],2024-08-21 00:28:29+00:00,2024-08-21 00:28:28+00:00,https://github.com/tensorflow/tensorflow/pull/73940,[],[],
2470515056,pull_request,closed,,[XLA] Remove logging statement which segfaults,"[XLA] Remove logging statement which segfaults
",copybara-service[bot],2024-08-16 15:43:32+00:00,['cheshire'],2024-08-16 16:48:16+00:00,2024-08-16 16:48:15+00:00,https://github.com/tensorflow/tensorflow/pull/73939,[],[],
2470513764,pull_request,closed,,PR #15998: [GPU] Fix kernel cache for loaded executables.,"PR #15998: [GPU] Fix kernel cache for loaded executables.

Imported from GitHub PR https://github.com/openxla/xla/pull/15998

LoadCache() has to be called also in GpuThunkAotCompilationResult::LoadExecutable() to properly initialize the state of the name uniquer in the IR emitter context on this path to execution.

This makes the XLA kernel cache compatible with the JAX module cache (example test: `bazel test --test_env=XLA_FLAGS=""--xla_gpu_enable_llvm_module_compilation_parallelism --xla_gpu_kernel_cache_file=/dev/shm/xla.kernel.cache"" tests/compilation_cache_test_gpu`).
Copybara import of the project:

--
b51fbcac5c7f172d06eb9de79770564f2e2c1250 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Fix kernel cache for loaded executables.

This makes the XLA kernel cache compatible with JAX module cache.

Merging this change closes #15998

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15998 from openxla:fix_kernel_cache b51fbcac5c7f172d06eb9de79770564f2e2c1250
",copybara-service[bot],2024-08-16 15:42:41+00:00,[],2024-08-16 21:46:24+00:00,2024-08-16 21:46:24+00:00,https://github.com/tensorflow/tensorflow/pull/73938,[],[],
2470510037,pull_request,closed,,Reverts 6b3afd5aab2f5106d828000be08e94e9215c0af2,"Reverts 6b3afd5aab2f5106d828000be08e94e9215c0af2
",copybara-service[bot],2024-08-16 15:40:00+00:00,[],2024-08-16 17:07:04+00:00,2024-08-16 17:07:03+00:00,https://github.com/tensorflow/tensorflow/pull/73937,[],[],
2470499316,pull_request,open,,[XLA:GPU] Do not remove PTX for those inputs which crash ptxas for easier repros,"[XLA:GPU] Do not remove PTX for those inputs which crash ptxas for easier repros
",copybara-service[bot],2024-08-16 15:32:54+00:00,['cheshire'],2024-08-16 15:59:46+00:00,,https://github.com/tensorflow/tensorflow/pull/73936,[],[],
2470491830,pull_request,open,,PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status,"PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status

Imported from GitHub PR https://github.com/openxla/xla/pull/14897

The current behavior crashes the program whenever a nccl async error has occured, timeout errors are also not detected for async events. This pr adds a mechanism to do:
1. poll statuses of async events and return timeout if status is pending for too long
2. return nccl async event status as xla status so a proper python exception can be thrown.
Copybara import of the project:

--
4a6e3b1cee3af3bc0c83e31ee1ce5ecfffd524ac by TJ Xu <tjx@nvidia.com>:

Add mechanism to detect nccl timeout and return error status

--
c7bdda819ade875f03d6fd8ed4a6d07c00aba5e7 by TJ Xu <tjx@nvidia.com>:

move async status and queue management to gpu executable

--
d4b44f1afde05a78a4e233b05503015218c29ec7 by TJ <tjx@nvidia.com>:

Added e2e test for testing nccl timeout and error propagation

--
c64997afee2d83a857b61cb0d91d8ca2a641097c by TJ <tjx@nvidia.com>:

address pr comments

--
d3318e7f8703519b060f7efb1c77d3f409fc5ed8 by TJ <tjx@nvidia.com>:

changed back the formatting for xla/python/pjit.cc

--
d396cc6972275fd718e4961a0d5040f23679bd24 by TJ Xu <tjx@nvidia.com>:

Added back the IsIdle api in gpu stream interface

--
77d86076ffa3d5688f03ba29bfe49db441b06cc5 by TJ Xu <tjx@nvidia.com>:

Fix ci failure

--
dd3f1520e7f0225225ff7fa196d26d5e4a079cad by TJ Xu <tjx@nvidia.com>:

Add a grace period when async error happens so nccl has time to shut
down

Merging this change closes #14897

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14897 from Tixxx:tixxx/nccl_error_prop dd3f1520e7f0225225ff7fa196d26d5e4a079cad
",copybara-service[bot],2024-08-16 15:28:21+00:00,[],2024-08-28 15:43:54+00:00,,https://github.com/tensorflow/tensorflow/pull/73935,[],[],
2470471577,pull_request,closed,,PR #16152: [ROCm] CI hotfix ,"PR #16152: [ROCm] CI hotfix 

Imported from GitHub PR https://github.com/openxla/xla/pull/16152

Issue was caused by this commit: https://github.com/openxla/xla/commit/05cb60b588d0f0ca13151fc0f5d630d7a26d6b05#diff-09c72ba3125587b3998e45c7a179e18a7df5da72e8c56ec89fcedac1df49f23cR70

Log:
```
[2024-08-16T05:07:42.977Z] xla/stream_executor/rocm/rocm_platform.cc:65:46: error: expected ‘)’ before ‘{’ token
[2024-08-16T05:07:42.977Z]    65 | ROCmPlatform::GetUncachedExecutor(int ordinal {
[2024-08-16T05:07:42.978Z]       |                                  ~           ^~
[2024-08-16T05:07:42.978Z]       |                                              )
```
Copybara import of the project:

--
954e77a306881fe35b04e31afc9f9143ed6c0626 by mmakevic <Milica.Makevic@amd.com>:

Add missing parentheses

Merging this change closes #16152

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16152 from ROCm:ci_hotfix_240816 954e77a306881fe35b04e31afc9f9143ed6c0626
",copybara-service[bot],2024-08-16 15:16:18+00:00,[],2024-08-16 15:55:34+00:00,2024-08-16 15:55:34+00:00,https://github.com/tensorflow/tensorflow/pull/73934,[],[],
2470468579,pull_request,closed,,PR #16104: [GPU][NFC] Further cleanup FMHA graph creation code.,"PR #16104: [GPU][NFC] Further cleanup FMHA graph creation code.

Imported from GitHub PR https://github.com/openxla/xla/pull/16104


Copybara import of the project:

--
e24e2a16793004b0fa96f0a80d9a788ba7e66727 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Further cleanup FMHA graph creation code.

Merging this change closes #16104

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16104 from openxla:cleanup_fmha e24e2a16793004b0fa96f0a80d9a788ba7e66727
",copybara-service[bot],2024-08-16 15:14:30+00:00,[],2024-08-16 15:49:25+00:00,2024-08-16 15:49:24+00:00,https://github.com/tensorflow/tensorflow/pull/73933,[],[],
2470449559,pull_request,closed,,Add clang to the path in the ARM64 Dockerfile.,"Add clang to the path in the ARM64 Dockerfile.
",copybara-service[bot],2024-08-16 15:02:42+00:00,['MichaelHudgins'],2024-08-16 17:42:01+00:00,2024-08-16 17:42:01+00:00,https://github.com/tensorflow/tensorflow/pull/73932,[],[],
2470217403,pull_request,closed,,Properly support 2D transposes.,"Properly support 2D transposes.

Before, the transpose emitter only supported shapes that were padded to 3D. Now,
it also supports 2D shapes. This change also prepares for supporting any rank
(but a bit more work is needed, e.g. adjusting the computation of thread
offsets).

Reverts b6736102cc009d34f55e5e2a029b23a5263d302b

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16153 from openxla:fix_autotuner_timeout ea59210f7ec7bad918304af63684beb8dc8100e7
",copybara-service[bot],2024-08-16 12:51:37+00:00,['akuegel'],2024-08-16 18:19:29+00:00,2024-08-16 18:19:28+00:00,https://github.com/tensorflow/tensorflow/pull/73931,[],[],
2470151061,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 12:11:18+00:00,[],2024-08-22 05:17:04+00:00,,https://github.com/tensorflow/tensorflow/pull/73929,[],[],
2470102505,pull_request,open,,Add support for dynamic update slice in mhlo -> tfl,"Add support for dynamic update slice in mhlo -> tfl
",copybara-service[bot],2024-08-16 11:41:03+00:00,['LukeBoyer'],2024-08-16 11:59:33+00:00,,https://github.com/tensorflow/tensorflow/pull/73928,[],[],
2470100519,pull_request,open,,Add support for dynamic update slice in mhlo -> tfl,"Add support for dynamic update slice in mhlo -> tfl
",copybara-service[bot],2024-08-16 11:39:37+00:00,['LukeBoyer'],2024-08-16 11:58:19+00:00,,https://github.com/tensorflow/tensorflow/pull/73927,[],[],
2470087271,pull_request,closed,,[NFC][ROCM] Replaced DoMatmul with ExecuteOnStream call for gpu_blas_lt,"The only reason gpu_blas_lt exposes [DoMatmul](https://github.com/ROCm/tensorflow-upstream/blob/e20ec8d81ee4084cd562e4df35c0cfccfaf8417e/third_party/xla/xla/stream_executor/gpu/gpu_blas_lt.h#L157) interfaces is this single place. This is probably happens due to historical reasons.

Here I refactor it to use well-established **ExecuteOnStream** interface function which handles valid data type combinations [automatically](https://github.com/ROCm/tensorflow-upstream/blob/e20ec8d81ee4084cd562e4df35c0cfccfaf8417e/third_party/xla/xla/stream_executor/cuda/cuda_blas_lt.cc#L601).

Once this place is fixed, we can greatly simplify gpu_blas_lt inteface in XLA by removing those DoMatmul low-level functions.
Besides, I also did some improvements for ROCM platform (e.g. for hipblas-lt support).
",pemeliya,2024-08-16 11:31:50+00:00,['gbaned'],2025-01-06 19:10:57+00:00,2025-01-06 19:10:57+00:00,https://github.com/tensorflow/tensorflow/pull/73926,"[('ready to pull', 'PR ready for merge process'), ('size:M', 'CL Change Size: Medium'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2311683505, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 27, 6, 33, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324001584, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 2, 7, 24, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354453332, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 17, 3, 55, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367679391, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 23, 9, 30, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384871213, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 1, 6, 2, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399585595, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 8, 11, 28, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431482974, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 23, 9, 31, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469837677, 'issue_id': 2470087271, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 12, 8, 2, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511241411, 'issue_id': 2470087271, 'author': 'pemeliya', 'body': '@klucke, thank you for reviewing this. I wonder what else needs to be done to get this PR merged ?', 'created_at': datetime.datetime(2024, 12, 2, 11, 10, 34, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-08-27 06:33:48 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-02 07:24:17 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-17 03:55:37 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-23 09:30:45 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 06:02:18 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-08 11:28:05 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-23 09:31:30 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

keerthanakadiri on (2024-11-12 08:02:32 UTC): Hi @rdzhabarov, Can you please review this PR? Thank you !

pemeliya (Issue Creator) on (2024-12-02 11:10:34 UTC): @klucke, thank you for reviewing this. I wonder what else needs to be done to get this PR merged ?

"
2470071090,pull_request,closed,,Restore default disabled backends for test.,"Restore default disabled backends for test.

This test overrides disabled_backends, dropping the default
value in the process.
",copybara-service[bot],2024-08-16 11:22:04+00:00,[],2024-08-16 13:43:52+00:00,2024-08-16 13:43:51+00:00,https://github.com/tensorflow/tensorflow/pull/73925,[],[],
2470069565,pull_request,closed,,Replace GpuInstructionFusion with priority fusion.,"Replace GpuInstructionFusion with priority fusion.

This should be a drop-in replacement. GpuInstructionFusion is
deprecated and slated for removal.
",copybara-service[bot],2024-08-16 11:21:00+00:00,[],2024-08-16 13:24:35+00:00,2024-08-16 13:24:35+00:00,https://github.com/tensorflow/tensorflow/pull/73924,[],[],
2470030020,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 10:58:13+00:00,[],2024-08-16 12:40:40+00:00,2024-08-16 12:40:39+00:00,https://github.com/tensorflow/tensorflow/pull/73923,[],[],
2469918792,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 09:51:17+00:00,[],2024-08-16 09:51:17+00:00,,https://github.com/tensorflow/tensorflow/pull/73919,[],[],
2469843699,pull_request,closed,,[numpy] Fix test failures under NumPy 2.0.,"[numpy] Fix test failures under NumPy 2.0.
",copybara-service[bot],2024-08-16 09:06:11+00:00,[],2024-08-16 22:17:37+00:00,2024-08-16 22:17:36+00:00,https://github.com/tensorflow/tensorflow/pull/73918,[],[],
2469834176,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 09:00:40+00:00,[],2024-08-16 09:00:40+00:00,,https://github.com/tensorflow/tensorflow/pull/73917,[],[],
2469827446,pull_request,closed,,Remove MayPreventVectorization from unrolling heuristic.,"Remove MayPreventVectorization from unrolling heuristic.

MLIR emitters don't rely on unrolling for vectorization, so this
heuristic doesn't make any sense anymore. There are a few opcodes
that may prevent vectorization:

- certain pads (depending on the layout and padding sizes)
- certain concats (depending on the layout and operand sizes)
- certain reduces (not entirely sure which ones)
- reverse (depending on the reversed dimension)

I haven't yet seen this happen in a real model. Also, the current
heuristic doesn't handle these opcodes meaningfully, so we can just
remove it for now.
",copybara-service[bot],2024-08-16 08:56:34+00:00,[],2024-08-26 09:56:32+00:00,2024-08-26 09:56:31+00:00,https://github.com/tensorflow/tensorflow/pull/73916,[],[],
2469797249,pull_request,closed,,PR #16074: Fix GemmFusionAutotunerTest.DoNotRunAutotuningKernelSpillingRegisters test on Hopper,"PR #16074: Fix GemmFusionAutotunerTest.DoNotRunAutotuningKernelSpillingRegisters test on Hopper

Imported from GitHub PR https://github.com/openxla/xla/pull/16074

Previously the target `//xla/service/gpu/autotuning:gemm_fusion_autotuner_test_gpu_h100` failed on Hopper (but was somehow missed by the CI)
Copybara import of the project:

--
86987724a0ba1fefb6e8299ac5d1ae63f50a1fb1 by Sergey Kozub <skozub@nvidia.com>:

Fix GemmFusionAutotunerTest.DoNotRunAutotuningKernelSpillingRegisters test on Hopper

Merging this change closes #16074

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16074 from openxla:skozub/gemm_fusion_autotuner_test 86987724a0ba1fefb6e8299ac5d1ae63f50a1fb1
",copybara-service[bot],2024-08-16 08:38:03+00:00,[],2024-08-16 16:01:30+00:00,2024-08-16 16:01:29+00:00,https://github.com/tensorflow/tensorflow/pull/73915,[],[],
2469786183,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 08:30:58+00:00,[],2024-08-17 04:49:51+00:00,2024-08-17 04:49:51+00:00,https://github.com/tensorflow/tensorflow/pull/73914,[],[],
2469775742,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 08:25:13+00:00,[],2024-08-16 08:25:13+00:00,,https://github.com/tensorflow/tensorflow/pull/73913,[],[],
2469751238,pull_request,closed,,Adding script for triton integration,"Adding script for triton integration
",copybara-service[bot],2024-08-16 08:12:20+00:00,[],2024-08-23 15:03:02+00:00,2024-08-23 15:03:01+00:00,https://github.com/tensorflow/tensorflow/pull/73912,[],[],
2469638804,pull_request,closed,,Improve HloCSE for MultiOutputFusion computations.,"Improve HloCSE for MultiOutputFusion computations.

Due to CSE we can have multi-output fusions with duplicate fusion roots. We can
choose one representative and make the duplicates unused, thus allowing HloDCE
to remove the unused duplicates.
",copybara-service[bot],2024-08-16 07:01:56+00:00,['akuegel'],2024-08-16 10:56:33+00:00,2024-08-16 10:56:33+00:00,https://github.com/tensorflow/tensorflow/pull/73911,[],[],
2469628490,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 06:55:24+00:00,[],2024-08-16 08:50:42+00:00,2024-08-16 08:50:41+00:00,https://github.com/tensorflow/tensorflow/pull/73910,[],[],
2469594094,pull_request,closed,,PR #15500: Add option to disable dynamic-slice to slice conversion,"PR #15500: Add option to disable dynamic-slice to slice conversion

Imported from GitHub PR https://github.com/openxla/xla/pull/15500

Add option to disable conversion of dynamic-slice to slice. Defaulting to false to maintain existing behavior. 
Copybara import of the project:

--
8ff3497a8231ff2c83a61b1ed3503ad84c6904a9 by ptoulme-aws <ptoulme@amazon.com>:

Add option to disable dynamic-slice to slice conversion

Merging this change closes #15500

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15500 from ptoulme-aws:alg_simp_ds 8ff3497a8231ff2c83a61b1ed3503ad84c6904a9
",copybara-service[bot],2024-08-16 06:31:39+00:00,[],2024-08-16 08:15:51+00:00,2024-08-16 08:15:49+00:00,https://github.com/tensorflow/tensorflow/pull/73909,[],[],
2469544490,pull_request,open,,PR #15092: Support cuDNN frontend scaled dot product attention for FP8. Part- 1(forward),"PR #15092: Support cuDNN frontend scaled dot product attention for FP8. Part- 1(forward)

Imported from GitHub PR https://github.com/openxla/xla/pull/15092

Add cudnn frontend support of scaled dot product attention for FP8 forward. doc [here](https://github.com/NVIDIA/cudnn-frontend/blob/98ca4e1941fe3263f128f74f10063a3ea35c7019/docs/operations/Attention.md).

NOTE: this feature relies on cudnn-frontend v1.6.1 which is not in XLA yet.
Copybara import of the project:

--
0913bfb4daf5985e223725b0a38ab08100766483 by shuw <shuw@nvidia.com>:

Fwd compile

fwd_work

cleanup

fwd cleean

clang-format

Remove comments

add unittest

Improve after review 1

Improve after review 2

Rebase

--
a9f1bf9dfb908a66a6282b038570162579e6ed72 by shuw <shuw@nvidia.com>:

cleanup FMHA graph creation code

--
547d9caec60a57f0508c5a9f072137a8f57e8190 by shuw <shuw@nvidia.com>:

clang-format

--
43efc70f355df9af6a551863e28a118fe9039e4a by shuw <shuw@nvidia.com>:

Remove unused code

--
95e556e3a0625232c7b6a5cc838095c88c2c0fa1 by shuw <shuw@nvidia.com>:

Correct custom-call name

--
310a790d446af33e8869b022f5d219194c681069 by Shu Wang <shuw@nvidia.com>:

update version check.

Merging this change closes #15092

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15092 from wenscarl:sdpa_fp8 310a790d446af33e8869b022f5d219194c681069
",copybara-service[bot],2024-08-16 05:55:55+00:00,[],2024-08-21 09:02:05+00:00,,https://github.com/tensorflow/tensorflow/pull/73908,[],[],
2469476343,pull_request,closed,,Add logging when unwinding error,"Add logging when unwinding error
",copybara-service[bot],2024-08-16 04:59:36+00:00,[],2024-08-16 23:34:54+00:00,2024-08-16 23:34:53+00:00,https://github.com/tensorflow/tensorflow/pull/73907,[],[],
2469450808,pull_request,closed,,Add support for TFLite If op in MLIR flatbuffer exporter. Serialize function call subgraph in then/else region and append function arguments to If op operand.,"Add support for TFLite If op in MLIR flatbuffer exporter. Serialize function call subgraph in then/else region and append function arguments to If op operand.

This is 1st of 4 changes to enable direct legalization of Stable HLO -> TFL If op.
",copybara-service[bot],2024-08-16 04:35:04+00:00,[],2024-08-19 19:48:55+00:00,2024-08-19 19:48:54+00:00,https://github.com/tensorflow/tensorflow/pull/73906,[],[],
2469425296,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 04:08:43+00:00,[],2024-08-16 04:08:43+00:00,,https://github.com/tensorflow/tensorflow/pull/73905,[],[],
2469423363,pull_request,open,,"Add folding for floor, exp and logical not in tfl.","Add folding for floor, exp and logical not in tfl.
",copybara-service[bot],2024-08-16 04:06:27+00:00,['arfaian'],2024-08-16 04:25:04+00:00,,https://github.com/tensorflow/tensorflow/pull/73904,[],[],
2469420666,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-08-16 04:03:21+00:00,[],2024-08-16 04:03:21+00:00,,https://github.com/tensorflow/tensorflow/pull/73903,[],[],
2469358522,pull_request,closed,,[XLA:SPMD] Remove LookaheadUserSharding in sharding propagation.,"[XLA:SPMD] Remove LookaheadUserSharding in sharding propagation.

When we infer the dot sharding from its operands, it is possible that both operands can improve the dot sharding. LookaheadUserSharding iterates the dot users and decides which dot operand sharding is preferred. This cl removes it for two reasons.
1. It is unnecessary. If we can predict the sharding from dot users, we can wait the sharding to be propagated from users. The propagted sharding from users can still help us make choice between dot operands.
2. The lookhead sharding may be wrong. LookaheadUserSharding is a heuristics. We cannot guarantee that the predicted sharding will hold in the dot users.

This cl is different from cl/663339516. If the shardings from two operands are compatible (there is no conflict), we should proceed with the improved sharding.

Reverts b6736102cc009d34f55e5e2a029b23a5263d302b
",copybara-service[bot],2024-08-16 03:21:54+00:00,[],2024-08-16 17:13:46+00:00,2024-08-16 17:13:46+00:00,https://github.com/tensorflow/tensorflow/pull/73902,[],[],
2469234664,pull_request,closed,,[tflite-gpu] Add atan2 to gpu_compatibility.cc.,"[tflite-gpu] Add atan2 to gpu_compatibility.cc.
",copybara-service[bot],2024-08-16 01:09:01+00:00,['grantjensen'],2024-08-19 00:34:29+00:00,2024-08-19 00:34:28+00:00,https://github.com/tensorflow/tensorflow/pull/73900,[],[],
2469203338,pull_request,open,,PR #16127: Fix the driver version parsing in the diagnostics.,"PR #16127: Fix the driver version parsing in the diagnostics.

Imported from GitHub PR https://github.com/openxla/xla/pull/16127

The version number doesn't always follow ""Kernel Module"" immediately. Two spaces somewhere after ""Kernel Module"" are the signifier that the driver version follows.
Copybara import of the project:

--
a83946f8788b0918b384fe962c16c286eb16feda by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Fix the driver version parsing in the diagnostics.

The version number doesn't always follow ""Kernel Module"" immediately.
Two spaces somewhere after ""Kernel Module"" are the signifier that the driver version follows.

Merging this change closes #16127

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16127 from dimvar:fix-driver-version-parsing a83946f8788b0918b384fe962c16c286eb16feda
",copybara-service[bot],2024-08-16 00:32:52+00:00,[],2024-08-16 05:17:07+00:00,,https://github.com/tensorflow/tensorflow/pull/73899,[],[],
2469200104,pull_request,closed,,Add missing header files to TFLite installation,"Add missing header files to TFLite installation

This patch adds some header files that are missing to the TFLite installation. Without this patch, when building LLVM against TFLite, we get missing include errors for the two files added here.

Tested by building TFLite locally and then building LLVM against that TFLite to ensure the build and tests succeeded.
",copybara-service[bot],2024-08-16 00:28:52+00:00,[],2024-08-16 02:09:00+00:00,2024-08-16 02:08:59+00:00,https://github.com/tensorflow/tensorflow/pull/73898,[],[],
2469188615,pull_request,closed,,"Add `Build.commands()`, make all methods on `Build` pure","Add `Build.commands()`, make all methods on `Build` pure

We can use this to see what commands will be executed without running the script.
",copybara-service[bot],2024-08-16 00:15:27+00:00,['ddunl'],2024-08-20 18:11:18+00:00,2024-08-20 18:11:16+00:00,https://github.com/tensorflow/tensorflow/pull/73897,[],[],
2469171975,pull_request,closed,,[Numpy] Fix int overflow errors in make_tensor_proto introduced by NumPy 2.0 update.,"[Numpy] Fix int overflow errors in make_tensor_proto introduced by NumPy 2.0 update.
",copybara-service[bot],2024-08-15 23:56:42+00:00,['kanglant'],2024-08-16 17:21:00+00:00,2024-08-16 17:20:59+00:00,https://github.com/tensorflow/tensorflow/pull/73896,[],[],
2469148122,pull_request,closed,,[XLA] Make adding mandatory constraints in layout assignment more flexible,"[XLA] Make adding mandatory constraints in layout assignment more flexible

We want to be able to add mandatory constraints to while instructions, however not to the entire instruction, but only to specific operands. Today that's not possible via `LayoutAssignment::SetInstructionLayout()`.

Be more flexible and only set constraints on specified subshape (if any).
",copybara-service[bot],2024-08-15 23:32:15+00:00,[],2024-08-16 04:26:05+00:00,2024-08-16 04:26:03+00:00,https://github.com/tensorflow/tensorflow/pull/73895,[],[],
2469105466,pull_request,open,,"Introduce TristateProto, for use with boolean flags","Introduce TristateProto, for use with boolean flags
",copybara-service[bot],2024-08-15 22:39:41+00:00,[],2024-08-15 22:39:41+00:00,,https://github.com/tensorflow/tensorflow/pull/73894,[],[],
2469049436,pull_request,closed,,Add new function TopologicalOrdering.,"Add new function TopologicalOrdering.
",copybara-service[bot],2024-08-15 21:57:02+00:00,[],2024-08-19 23:57:43+00:00,2024-08-19 23:57:43+00:00,https://github.com/tensorflow/tensorflow/pull/73893,[],[],
2468982081,pull_request,closed,,Use C23's sized free to avoid a costly pointer-to-size lookup.,"Use C23's sized free to avoid a costly pointer-to-size lookup.

In deallocations via TypedAllocator, we know the true size.  Adding an overload
to DeallocateRaw allows us to pass this back to the malloc implementation to be
more efficient.
",copybara-service[bot],2024-08-15 21:16:19+00:00,[],2024-08-16 16:07:28+00:00,2024-08-16 16:07:26+00:00,https://github.com/tensorflow/tensorflow/pull/73892,[],[],
2468947902,pull_request,open,,[Tosa] EqualizeRanks for ops with SameOperandsAndResultRank,"This patch refactors tf/tfl to TOSA lowering to prepare for adding trait SameOperandsAndResultRank to TOSA element wise operators (which will then require these operators be built with operands with same ranks)

This blocks llvm PR: https://github.com/llvm/llvm-project/pull/104501

1. Refactored to use:
        getTosaConstTensorSingleF32
        getTosaConstTensorSingleI32
        getTosagetTosaConstTensorScalarInt
   to construct tosa constant tensors with single value and specified rank

2. Changed lowering of following tf operatos:
        BitwiseOr
        BitwiseXor
        BitwiseAnd
        LogicalAnd
        LogicalOr
        Pow
    to go through CreateReplaceOpAndInfer and CreateOpAndInfer builder functions

3. Refactor to use CreateMulOpAndInfer to construct tosa Mul operations
       - this calls EqualizeRanks on multiply inputs to insert reshape as needed to ensure mul operands have same ranks
       - and default shift value to 0 if unspecified

4. Refactor callers of CreateOpAndInfer/CreateReplaceOpAndInfer to pass Value arguments for inputs of tosa element wise operators

5. In CreateOpAndInfer, call tosa::CreateOpAndInferShape which in turn checks for operator trait SameOperandsAndResultRank and calls EqualizeRanks to insert reshape as needed to ensure operands have same ranks.

6. Changed lowering of following tfl operations:
         LogicalAnd
         LogicalOr
         Pow
    to go through CreateReplaceOpAndInfer and CreateOpAndInfer builder functions


",Tai78641,2024-08-15 21:01:16+00:00,['gbaned'],2025-02-07 02:54:34+00:00,,https://github.com/tensorflow/tensorflow/pull/73891,"[('awaiting review', 'Pull request awaiting review'), ('size:XL', 'CL Change Size:Extra Large')]","[{'comment_id': 2295867570, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @jpienaar, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 19, 7, 36, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311684234, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @jpienaar, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 27, 6, 34, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324002456, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @jpienaar, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 2, 7, 24, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384873465, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @Tai78641, Can you please resolve the conflicts? Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 3, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397738459, 'issue_id': 2468947902, 'author': 'Tai78641', 'body': '> Hi @Tai78641, Can you please resolve the conflicts? Thank you!\r\n\r\ndone', 'created_at': datetime.datetime(2024, 10, 7, 19, 39, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413436167, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @jpienaar, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 15, 9, 56, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431486502, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @jpienaar, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 23, 9, 32, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469841137, 'issue_id': 2468947902, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 12, 8, 4, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637272977, 'issue_id': 2468947902, 'author': 'leandron', 'body': '@Tai78641 can you rebase this to solve the conflicts in preparation for review, please?', 'created_at': datetime.datetime(2025, 2, 5, 15, 38, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641814290, 'issue_id': 2468947902, 'author': 'Tai78641', 'body': '> @Tai78641 can you rebase this to solve the conflicts in preparation for review, please?\r\n\r\ndone', 'created_at': datetime.datetime(2025, 2, 7, 2, 54, 32, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-08-19 07:36:30 UTC): Hi @jpienaar, Can you please review this PR? Thank you !

keerthanakadiri on (2024-08-27 06:34:24 UTC): Hi @jpienaar, Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-02 07:24:45 UTC): Hi @jpienaar, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 06:03:53 UTC): Hi @Tai78641, Can you please resolve the conflicts? Thank you!

Tai78641 (Issue Creator) on (2024-10-07 19:39:07 UTC): done

keerthanakadiri on (2024-10-15 09:56:44 UTC): Hi @jpienaar, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-23 09:32:34 UTC): Hi @jpienaar, Can you please review this PR? Thank you !

keerthanakadiri on (2024-11-12 08:04:28 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

leandron on (2025-02-05 15:38:46 UTC): @Tai78641 can you rebase this to solve the conflicts in preparation for review, please?

Tai78641 (Issue Creator) on (2025-02-07 02:54:32 UTC): done

"
2468920099,pull_request,closed,,Limit the number of additional `ExecutionStreamIds` to 4.,"Limit the number of additional `ExecutionStreamIds` to 4.
",copybara-service[bot],2024-08-15 20:44:57+00:00,[],2024-08-16 02:21:33+00:00,2024-08-16 02:21:32+00:00,https://github.com/tensorflow/tensorflow/pull/73890,[],[],
2468916869,pull_request,closed,,[XLA:SPMD] Internal change.,"[XLA:SPMD] Internal change.

Reverts 4c0632186a4c0289cd1c4a804c4b60cdbac2f6b8
",copybara-service[bot],2024-08-15 20:42:17+00:00,['aaroey'],2024-08-15 22:10:26+00:00,2024-08-15 22:10:25+00:00,https://github.com/tensorflow/tensorflow/pull/73889,[],[],
2468889093,pull_request,closed,,Add logging of status messages from platform manager NOT_FOUND failures.,"Add logging of status messages from platform manager NOT_FOUND failures.
",copybara-service[bot],2024-08-15 20:22:39+00:00,[],2024-08-15 21:07:49+00:00,2024-08-15 21:07:48+00:00,https://github.com/tensorflow/tensorflow/pull/73888,[],[],
2468851299,pull_request,open,,Add GCC support in hermetic CUDA rules,"Add GCC support in hermetic CUDA rules
",copybara-service[bot],2024-08-15 20:06:41+00:00,[],2024-08-15 21:18:27+00:00,,https://github.com/tensorflow/tensorflow/pull/73887,[],[],
