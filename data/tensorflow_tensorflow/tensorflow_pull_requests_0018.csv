id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2677219840,pull_request,closed,,Reverts 2dee6183d0f2ff7a90f4883fa18c86a184517b5d,"Reverts 2dee6183d0f2ff7a90f4883fa18c86a184517b5d
",copybara-service[bot],2024-11-20 21:25:23+00:00,[],2024-11-21 19:59:25+00:00,2024-11-21 19:59:24+00:00,https://github.com/tensorflow/tensorflow/pull/80385,[],[],
2677205169,pull_request,closed,,[NFC] hlo_op_profiler_test: Internal testing change.,"[NFC] hlo_op_profiler_test: Internal testing change.
",copybara-service[bot],2024-11-20 21:16:29+00:00,[],2024-11-20 23:06:29+00:00,2024-11-20 23:06:27+00:00,https://github.com/tensorflow/tensorflow/pull/80384,[],[],
2677186299,pull_request,open,,Reverts 555a25976d0d9318b8f53c0ca56ecadeeb389139,"Reverts 555a25976d0d9318b8f53c0ca56ecadeeb389139
",copybara-service[bot],2024-11-20 21:03:54+00:00,[],2024-11-20 21:03:54+00:00,,https://github.com/tensorflow/tensorflow/pull/80383,[],[],
2677168182,pull_request,closed,,Expose SignatureRunner via interpreter.h,"Expose SignatureRunner via interpreter.h
",copybara-service[bot],2024-11-20 20:53:32+00:00,[],2024-11-21 01:06:27+00:00,2024-11-21 01:06:26+00:00,https://github.com/tensorflow/tensorflow/pull/80382,[],[],
2677119517,pull_request,closed,,Add utility function to compute min num of bytes for tensor with strides,"Add utility function to compute min num of bytes for tensor with strides
",copybara-service[bot],2024-11-20 20:35:35+00:00,[],2024-11-21 02:38:15+00:00,2024-11-21 02:38:14+00:00,https://github.com/tensorflow/tensorflow/pull/80381,[],[],
2677022896,pull_request,open,,Noop in oss,"Noop in oss

Reverts f62e0d0982c03b5f7798bfe93c52da819fc0a83b
",copybara-service[bot],2024-11-20 19:56:59+00:00,['ddunl'],2024-11-20 19:57:00+00:00,,https://github.com/tensorflow/tensorflow/pull/80380,[],[],
2677016228,pull_request,closed,,[Control flow] Add a lighter implementation of `cond_v2()` that is optimized for latency.,"[Control flow] Add a lighter implementation of `cond_v2()` that is optimized for latency.

This change introduces `cond_v2.fast_cond_v2()`, which is a tool for writing latency-optimized conditionals using the functional `IfOp` implementation.
",copybara-service[bot],2024-11-20 19:53:23+00:00,[],2024-11-21 19:02:07+00:00,2024-11-21 19:02:06+00:00,https://github.com/tensorflow/tensorflow/pull/80378,[],[],
2677000435,pull_request,closed,,Add backend kwargs to xla tests.,"Add backend kwargs to xla tests.
",copybara-service[bot],2024-11-20 19:45:54+00:00,[],2024-11-21 19:19:35+00:00,2024-11-21 19:19:34+00:00,https://github.com/tensorflow/tensorflow/pull/80377,[],[],
2676978435,pull_request,closed,,Add backend_kwargs to XLA tests config.,"Add backend_kwargs to XLA tests config.
",copybara-service[bot],2024-11-20 19:34:20+00:00,[],2024-11-20 21:23:27+00:00,2024-11-20 21:23:26+00:00,https://github.com/tensorflow/tensorflow/pull/80376,[],[],
2676897577,pull_request,closed,,Adding step to constant_value and add support for multiplication while recursively calculating the range of an expression.,"Adding step to constant_value and add support for multiplication while recursively calculating the range of an expression.
",copybara-service[bot],2024-11-20 19:12:34+00:00,[],2024-11-21 00:21:10+00:00,2024-11-21 00:21:10+00:00,https://github.com/tensorflow/tensorflow/pull/80375,[],[],
2676881712,pull_request,closed,,Move ScalarizeSplatConstantForBroadcastableOps to optimize_broadcast_like_pass.cc,"Move ScalarizeSplatConstantForBroadcastableOps to optimize_broadcast_like_pass.cc

Adds support for more ops to fuse slat constants into TFL ops supporting impicit broadcast.
",copybara-service[bot],2024-11-20 19:02:54+00:00,['vamsimanchala'],2024-12-03 02:15:38+00:00,2024-12-03 02:15:37+00:00,https://github.com/tensorflow/tensorflow/pull/80374,[],[],
2676867594,pull_request,open,,Integrate LLVM at llvm/llvm-project@68b7ab127f58,"Integrate LLVM at llvm/llvm-project@68b7ab127f58

Updates LLVM usage to match
[68b7ab127f58](https://github.com/llvm/llvm-project/commit/68b7ab127f58)
",copybara-service[bot],2024-11-20 18:55:55+00:00,[],2024-11-20 18:55:55+00:00,,https://github.com/tensorflow/tensorflow/pull/80373,[],[],
2676779188,pull_request,open,,Noop in OSS,"Noop in OSS
",copybara-service[bot],2024-11-20 18:32:23+00:00,['ddunl'],2024-11-20 19:16:05+00:00,,https://github.com/tensorflow/tensorflow/pull/80372,[],[],
2676732564,pull_request,closed,,Internal CI/CD change,"Internal CI/CD change
",copybara-service[bot],2024-11-20 18:03:38+00:00,['changm'],2024-11-20 19:38:07+00:00,2024-11-20 19:38:06+00:00,https://github.com/tensorflow/tensorflow/pull/80371,[],[],
2676608766,pull_request,open,,[tsl:concurrency] NFC: Align CountDownAsyncValueRef atomics to cache line boundary,"[tsl:concurrency] NFC: Align CountDownAsyncValueRef atomics to cache line boundary
",copybara-service[bot],2024-11-20 17:20:28+00:00,['ezhulenev'],2024-11-20 17:20:29+00:00,,https://github.com/tensorflow/tensorflow/pull/80370,[],[],
2676600066,pull_request,closed,,Refactor PjRt environment initialization to have clearer data flow,"Refactor PjRt environment initialization to have clearer data flow

Split the initialization into several methods to have a better distinction between their responisbilities.
",copybara-service[bot],2024-11-20 17:15:31+00:00,[],2024-11-21 14:38:15+00:00,2024-11-21 14:38:14+00:00,https://github.com/tensorflow/tensorflow/pull/80369,[],[],
2676588704,pull_request,closed,,[tsl] CountDownAsyncValueRef: enforce memory ordering around fetch_sub,"[tsl] CountDownAsyncValueRef: enforce memory ordering around fetch_sub

name                     old cpu/op   new cpu/op   delta
BM_CountDownSuccess/4    97.6ns ± 2%  97.9ns ± 1%    ~     (p=0.841 n=5+5)
BM_CountDownSuccess/8     123ns ± 2%   122ns ± 1%    ~     (p=0.548 n=5+5)
BM_CountDownSuccess/16    171ns ± 1%   172ns ± 2%    ~     (p=0.548 n=5+5)
BM_CountDownSuccess/32    270ns ± 1%   271ns ± 1%    ~     (p=0.310 n=5+5)
BM_CountDownError/4       215ns ± 1%   212ns ± 3%    ~     (p=0.310 n=5+5)
BM_CountDownError/8       309ns ± 2%   307ns ± 1%    ~     (p=0.421 n=5+5)
BM_CountDownError/16      500ns ± 1%   496ns ± 2%    ~     (p=0.421 n=5+5)
BM_CountDownError/32      888ns ± 1%   885ns ± 2%    ~     (p=0.548 n=5+5)
",copybara-service[bot],2024-11-20 17:09:20+00:00,['cota'],2024-11-20 18:31:13+00:00,2024-11-20 18:31:13+00:00,https://github.com/tensorflow/tensorflow/pull/80368,[],[],
2676572421,pull_request,closed,,Fix TSAN for new mixed priority unit tests.,"Fix TSAN for new mixed priority unit tests.
",copybara-service[bot],2024-11-20 17:02:10+00:00,[],2024-11-20 21:44:11+00:00,2024-11-20 21:44:10+00:00,https://github.com/tensorflow/tensorflow/pull/80367,[],[],
2676550104,pull_request,closed,,Forgot to reset the map of skipped `f16`->`f32` dequantizations between calls to `Delegate::PrepareOpsToDelegate`.,"Forgot to reset the map of skipped `f16`->`f32` dequantizations between calls to `Delegate::PrepareOpsToDelegate`.
",copybara-service[bot],2024-11-20 16:53:47+00:00,[],2024-11-20 20:10:37+00:00,2024-11-20 20:10:36+00:00,https://github.com/tensorflow/tensorflow/pull/80366,[],[],
2676368113,pull_request,open,,[JAX] Ignore process index when calculating module hash.,"[JAX] Ignore process index when calculating module hash.
",copybara-service[bot],2024-11-20 15:57:04+00:00,[],2024-11-20 17:35:02+00:00,,https://github.com/tensorflow/tensorflow/pull/80364,[],[],
2676323937,pull_request,closed,,Move upstreamable part of sparse_dot to be a public patch,"Move upstreamable part of sparse_dot to be a public patch
",copybara-service[bot],2024-11-20 15:41:03+00:00,[],2024-11-21 18:47:15+00:00,2024-11-21 18:47:15+00:00,https://github.com/tensorflow/tensorflow/pull/80363,[],[],
2676254459,pull_request,closed,,[XLA:GPU] Change `ConstraintExpression` to use operator||/&& which return a new instance.,"[XLA:GPU] Change `ConstraintExpression` to use operator||/&& which return a new instance.

This CL changes the `ConstraintExpression` class by making it a value type and using C++ operators for logical operations. This hopefully makes the code more concise and easier to read.
",copybara-service[bot],2024-11-20 15:32:05+00:00,['chsigg'],2024-11-21 16:46:08+00:00,2024-11-21 16:46:07+00:00,https://github.com/tensorflow/tensorflow/pull/80362,[],[],
2676249840,pull_request,closed,,Remove unused gpu_types.h include from nccl_collective_thunk.cc,"Remove unused gpu_types.h include from nccl_collective_thunk.cc
",copybara-service[bot],2024-11-20 15:30:30+00:00,[],2024-11-20 20:53:37+00:00,2024-11-20 20:53:36+00:00,https://github.com/tensorflow/tensorflow/pull/80361,[],[],
2676249528,pull_request,closed,,Remove unused and add used headers in hlo_runner_main and create_client,"Remove unused and add used headers in hlo_runner_main and create_client
",copybara-service[bot],2024-11-20 15:30:22+00:00,[],2024-11-20 16:08:24+00:00,2024-11-20 16:08:22+00:00,https://github.com/tensorflow/tensorflow/pull/80360,[],[],
2676236895,pull_request,closed,,Merge sparsity_layout.patch into sparse_dot.patch,"Merge sparsity_layout.patch into sparse_dot.patch
",copybara-service[bot],2024-11-20 15:25:31+00:00,[],2024-11-20 16:18:05+00:00,2024-11-20 16:18:05+00:00,https://github.com/tensorflow/tensorflow/pull/80359,[],[],
2676216836,pull_request,closed,,Migrate SplitMergedOperandsPass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate SplitMergedOperandsPass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-20 15:17:43+00:00,['vamsimanchala'],2024-11-20 22:17:17+00:00,2024-11-20 22:17:16+00:00,https://github.com/tensorflow/tensorflow/pull/80358,[],[],
2676211461,pull_request,closed,,Remove :cuda_driver_version,"Remove :cuda_driver_version

Since `CudaDriverVersion()` is now only used in one place, let's inline the function and remove the target.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19552 from openxla:horizontal_fusion_cleanup e273aea41dd15efbc5d79c363810cf634e73203e
",copybara-service[bot],2024-11-20 15:16:04+00:00,[],2024-11-21 14:00:15+00:00,2024-11-21 14:00:14+00:00,https://github.com/tensorflow/tensorflow/pull/80357,[],[],
2676203915,pull_request,closed,,Remove remnants of GpuDriver,"Remove remnants of GpuDriver

The build target doesn't exist anymore but there is still a header file which gets deleted in this change.
",copybara-service[bot],2024-11-20 15:13:27+00:00,[],2024-11-21 15:58:06+00:00,2024-11-21 15:58:05+00:00,https://github.com/tensorflow/tensorflow/pull/80356,[],[],
2676139848,pull_request,closed,,Integrate LLVM at llvm/llvm-project@33fcd6acc755,"Integrate LLVM at llvm/llvm-project@33fcd6acc755

Updates LLVM usage to match
[33fcd6acc755](https://github.com/llvm/llvm-project/commit/33fcd6acc755)
",copybara-service[bot],2024-11-20 14:49:24+00:00,['metaflow'],2024-11-21 13:49:09+00:00,2024-11-21 13:49:08+00:00,https://github.com/tensorflow/tensorflow/pull/80355,[],[],
2676126999,pull_request,closed,,Remove :cuda_runtime and :rocm_runtime targets,"Remove :cuda_runtime and :rocm_runtime targets

- The remaining `GetRuntimeVersion` and `GetFuncBySymbol` functions get moved into the executors - the only place where they are needed.
- For CUDA is also create an overload of `cuda::ToStatus` which can convert a CUDA runtime error (`cudaError_t`) into an `absl::Status`.
- I also had to adjust the `RocmKernel` and `CudaKernel` tests which were using `GetFuncBySymbol` directly. Now they rely on `LoadKernel` from the executors.
",copybara-service[bot],2024-11-20 14:44:49+00:00,[],2024-11-21 12:08:50+00:00,2024-11-21 12:08:48+00:00,https://github.com/tensorflow/tensorflow/pull/80354,[],[],
2675951851,pull_request,closed,,TEST Ci implement hermetic rocm dependency baseline,,alekstheod,2024-11-20 13:53:43+00:00,['gbaned'],2024-11-20 13:53:59+00:00,2024-11-20 13:53:51+00:00,https://github.com/tensorflow/tensorflow/pull/80353,"[('awaiting review', 'Pull request awaiting review'), ('size:XL', 'CL Change Size:Extra Large')]",[],
2675938688,pull_request,closed,,[XLA:GPU] Adjust GetNumWarps heuristic in Tiled Cost Model.,"[XLA:GPU] Adjust GetNumWarps heuristic in Tiled Cost Model.

We need to adjust the heuristic because before our emitter had an issue that prevented Triton from doing proper layout optimizations. It was fixed in https://github.com/openxla/xla/commit/7280b9ad5fe1433baadf34ce9b59ffbaf607603f.

We needed to use higher number of warps (up to 32) before to cover the lack of layout optimization, but now it can cause performance regressions, because Triton likes to insert shmem usage and barrier syncs.
",copybara-service[bot],2024-11-20 13:48:19+00:00,[],2024-11-20 17:29:16+00:00,2024-11-20 17:29:15+00:00,https://github.com/tensorflow/tensorflow/pull/80352,[],[],
2675894885,pull_request,closed,,[XLA:GPU] Use DeviceDescription instead of GetDriverVersion in NVPTXCompiler,"[XLA:GPU] Use DeviceDescription instead of GetDriverVersion in NVPTXCompiler

NVPTXCompiler was calling `cuda::GetDriverVersion` to determine whether the CUDA driver is new enough to consider it for PTX JIT compilation.

This change makes it use the driver version available in the `DeviceDescription` type.
",copybara-service[bot],2024-11-20 13:31:22+00:00,[],2024-11-21 08:31:10+00:00,2024-11-21 08:31:09+00:00,https://github.com/tensorflow/tensorflow/pull/80351,[],[],
2675863837,pull_request,closed,,PR #19484: [ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/te…,"PR #19484: [ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/te…

Imported from GitHub PR https://github.com/openxla/xla/pull/19484

…sts:gpu_input_fusible_slice_test


Copybara import of the project:

--
0d307384bff386d5182f89ae5a5422f8ca1a1290 by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:

[ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/tests:gpu_input_fusible_slice_test

Merging this change closes #19484

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19484 from ROCm:mlir_tests_new 0d307384bff386d5182f89ae5a5422f8ca1a1290
",copybara-service[bot],2024-11-20 13:20:18+00:00,[],2024-11-20 14:57:09+00:00,2024-11-20 14:57:09+00:00,https://github.com/tensorflow/tensorflow/pull/80350,[],[],
2675796636,pull_request,open,,PR #19451: Remove xla_gpu_multi_streamed_windowed_einsum,"PR #19451: Remove xla_gpu_multi_streamed_windowed_einsum

Imported from GitHub PR https://github.com/openxla/xla/pull/19451

Deprecate xla_gpu_multi_streamed_windowed_einsum flag in gpu collective matmul since using multi streamed dots will always give better performance.
Copybara import of the project:

--
b6024b3f438a36337b6184dbbca9a9d02dac9bf6 by TJ Xu <tjx@nvidia.com>:

Remove xla_gpu_multi_streamed_windowed_einsum

Merging this change closes #19451

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag b6024b3f438a36337b6184dbbca9a9d02dac9bf6
",copybara-service[bot],2024-11-20 13:06:52+00:00,[],2024-11-20 13:06:52+00:00,,https://github.com/tensorflow/tensorflow/pull/80349,[],[],
2675773972,pull_request,closed,,Remove CUDA 12.1 workaround from reduction logic,"Remove CUDA 12.1 workaround from reduction logic

There was a check in place that works around a performance bug in ptxas from CUDA 12.1. This check has various problems:

1. It's untested and the way it's implemented it can't be easily test.
2. The version check doesn't work library compilation which we transition towards as it's checking the version of a local ptxas binary
3. It's unclear whether the workaround is still needed with the new MLIR emitters.

So I'm removing it here since it blocks me from making more refactoring around PTX compilation.
",copybara-service[bot],2024-11-20 12:56:54+00:00,[],2024-11-21 12:20:38+00:00,2024-11-21 12:20:37+00:00,https://github.com/tensorflow/tensorflow/pull/80348,[],[],
2675734856,pull_request,closed,,Use fast version of log if type is F16 or BF16.,"Use fast version of log if type is F16 or BF16.

There seems to be no dedicated libdevice call for Log with F16 or BF16 type.
Currently we upcast to F32 and use __nv_logf. However it seems likely that
__nv_fast_logf is good enough for F16 and BF16 type, so use it as it is
considerably faster.
",copybara-service[bot],2024-11-20 12:39:05+00:00,['akuegel'],2024-11-21 08:42:04+00:00,2024-11-21 08:42:02+00:00,https://github.com/tensorflow/tensorflow/pull/80347,[],[],
2675719032,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19363 from philipphack:u_pipeliner_increment_xla 6da45bcb26643d8994bf608f05230fa748286b02
",copybara-service[bot],2024-11-20 12:31:33+00:00,[],2024-11-20 12:31:33+00:00,,https://github.com/tensorflow/tensorflow/pull/80346,[],[],
2675563148,pull_request,closed,,Multiple subgraphs may share the same delegate.,"Multiple subgraphs may share the same delegate.

Dequantized static data is cached. However, when there are multiple subgraphs, the data is overwritten by each subgraph.
",copybara-service[bot],2024-11-20 11:28:03+00:00,['alankelly'],2024-11-20 12:41:14+00:00,2024-11-20 12:41:13+00:00,https://github.com/tensorflow/tensorflow/pull/80345,[],[],
2675401086,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-20 10:43:37+00:00,[],2024-11-20 11:22:53+00:00,,https://github.com/tensorflow/tensorflow/pull/80344,[],[],
2675363645,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-20 10:39:30+00:00,[],2024-11-20 10:39:30+00:00,,https://github.com/tensorflow/tensorflow/pull/80343,[],[],
2675339063,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-20 10:29:06+00:00,[],2024-11-20 10:29:06+00:00,,https://github.com/tensorflow/tensorflow/pull/80342,[],[],
2675329926,pull_request,closed,,delete hlo-legalize-to-memref-unranked.mlir,"delete hlo-legalize-to-memref-unranked.mlir

func-bufferize pass is removed by https://github.com/llvm/llvm-project/commit/e394fecd5b351e9108d0600c872759d8dea88a42
",copybara-service[bot],2024-11-20 10:25:17+00:00,['metaflow'],2024-11-20 11:02:57+00:00,2024-11-20 11:02:56+00:00,https://github.com/tensorflow/tensorflow/pull/80341,[],[],
2675115690,pull_request,closed,,Remove unused GpuAsmOpts parameter from RedzoneAllocator,"Remove unused GpuAsmOpts parameter from RedzoneAllocator
",copybara-service[bot],2024-11-20 09:21:51+00:00,[],2024-11-21 18:26:38+00:00,2024-11-21 18:26:37+00:00,https://github.com/tensorflow/tensorflow/pull/80340,[],[],
2675108323,pull_request,closed,,Remove unused GpuAsmOpts parameter from Cholesky and TriangularSolveThunks,"Remove unused GpuAsmOpts parameter from Cholesky and TriangularSolveThunks

Also the usual drive-by cleanups:
- Remove unused includes
- Add explicit includes for things we depended on transitively
- Clean up dependencies of the build targets
",copybara-service[bot],2024-11-20 09:19:27+00:00,[],2024-11-20 10:44:17+00:00,2024-11-20 10:44:15+00:00,https://github.com/tensorflow/tensorflow/pull/80339,[],[],
2675075505,pull_request,closed,,PR #19363: Loop Counter Increment in Collective Pipeliner,"PR #19363: Loop Counter Increment in Collective Pipeliner

Imported from GitHub PR https://github.com/openxla/xla/pull/19363

Sets the loop iteration counter increment in the backward transformation of the collective pipeliner pass to account for cases with non-zero initial value of the loop iteration counter. See #16953 and #18568.
Copybara import of the project:

--
06137aa0618d372e2d4badbf16920bead9922cfb by Philipp Hack <phack@nvidia.com>:

Modifies the loop counter increment set in the backward transformation of the collective pipeliner.

--
6da45bcb26643d8994bf608f05230fa748286b02 by Philipp Hack <phack@nvidia.com>:

Modifies the loop counter increment set in the backward transformation of the collective pipeliner.

Merging this change closes #19363

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19363 from philipphack:u_pipeliner_increment_xla 6da45bcb26643d8994bf608f05230fa748286b02
",copybara-service[bot],2024-11-20 09:09:09+00:00,[],2024-11-20 12:30:30+00:00,2024-11-20 12:30:30+00:00,https://github.com/tensorflow/tensorflow/pull/80338,[],[],
2675047793,pull_request,closed,,Remove custom compilation call from DynamicSharedMemoryTest,"Remove custom compilation call from DynamicSharedMemoryTest

This is not needed since the runtime can compile PTX for us. Actually I'm surprised
that this even worked because this original code compiled PTX into CUBIN and then
forced the CUBIN into the PTX argument in the kernel creation helper. But this is
now all fixed.
",copybara-service[bot],2024-11-20 08:58:28+00:00,[],2024-11-20 09:57:41+00:00,2024-11-20 09:57:41+00:00,https://github.com/tensorflow/tensorflow/pull/80337,[],[],
2675030218,pull_request,closed,,Fix comments in `convolution_test_1d.cc`,"Fix comments in `convolution_test_1d.cc`

The correct output dimension when dumped to HLO text is `bf0`, where `f` means the output feature dimension. There is no dimension called `o`.
",copybara-service[bot],2024-11-20 08:51:37+00:00,[],2024-11-20 19:56:21+00:00,2024-11-20 19:56:21+00:00,https://github.com/tensorflow/tensorflow/pull/80336,[],"[{'comment_id': 2487925893, 'issue_id': 2675030218, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80336/checks?check_run_id=33249524691) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 11, 20, 8, 51, 42, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-11-20 08:51:42 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80336/checks?check_run_id=33249524691) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2674712594,pull_request,closed,,PR #19393: [GPU] Horizontal loop fusion: pass bitcasts when looking for fusion candidates.,"PR #19393: [GPU] Horizontal loop fusion: pass bitcasts when looking for fusion candidates.

Imported from GitHub PR https://github.com/openxla/xla/pull/19393


Copybara import of the project:

--
ec107a12fbee6826f1f668218b7c7a40f5886420 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Horizontal loop fusion: pass bitcasts when looking for fusion candidates.

--
71241097ce67412246ec18efca5165619601eace by Ilia Sergachev <isergachev@nvidia.com>:

simplify cuDNN norm test

Merging this change closes #19393

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19393 from openxla:horizontal_fusion_bitcast 71241097ce67412246ec18efca5165619601eace
",copybara-service[bot],2024-11-20 07:12:38+00:00,[],2024-11-20 13:45:19+00:00,2024-11-20 13:45:18+00:00,https://github.com/tensorflow/tensorflow/pull/80333,[],[],
2674650947,pull_request,closed,,[XLA:CollectivePipeliner-Sinking] Stop pipelining iterations if a large sunk collective is encountered.,"[XLA:CollectivePipeliner-Sinking] Stop pipelining iterations if a large sunk collective is encountered.
",copybara-service[bot],2024-11-20 06:49:07+00:00,['seherellis'],2024-11-21 23:03:09+00:00,2024-11-21 23:03:08+00:00,https://github.com/tensorflow/tensorflow/pull/80330,[],[],
2674556355,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-20 06:06:44+00:00,[],2024-11-20 06:06:44+00:00,,https://github.com/tensorflow/tensorflow/pull/80329,[],[],
2674544122,pull_request,closed,,Add helper function to add new tensors to internal subgraph.,"Add helper function to add new tensors to internal subgraph.
",copybara-service[bot],2024-11-20 05:59:47+00:00,['LukeBoyer'],2024-11-21 06:08:54+00:00,2024-11-21 06:08:53+00:00,https://github.com/tensorflow/tensorflow/pull/80328,[],[],
2674527556,pull_request,closed,,Rename a Google Tensor field,"Rename a Google Tensor field
",copybara-service[bot],2024-11-20 05:52:47+00:00,[],2024-11-20 17:11:22+00:00,2024-11-20 17:11:21+00:00,https://github.com/tensorflow/tensorflow/pull/80327,[],[],
2674493058,pull_request,closed,,[XLA:GPU] Use HloPredicateIsOp in collective_select_folder,"[XLA:GPU] Use HloPredicateIsOp in collective_select_folder
",copybara-service[bot],2024-11-20 05:39:54+00:00,['frgossen'],2024-11-20 20:44:06+00:00,2024-11-20 20:44:05+00:00,https://github.com/tensorflow/tensorflow/pull/80326,[],[],
2674448558,pull_request,closed,,Add helper methods to add inputs/outputs to internal tensor def.,"Add helper methods to add inputs/outputs to internal tensor def.
",copybara-service[bot],2024-11-20 05:34:37+00:00,['LukeBoyer'],2024-11-21 01:27:22+00:00,2024-11-21 01:27:21+00:00,https://github.com/tensorflow/tensorflow/pull/80325,[],[],
2674435490,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-20 05:24:25+00:00,[],2024-11-20 05:24:25+00:00,,https://github.com/tensorflow/tensorflow/pull/80324,[],[],
2674413569,pull_request,closed,,[xla:cpu] Replace Thunk::ExecuteEvent with tsl::CountDownAsyncValueRef,"[xla:cpu] Replace Thunk::ExecuteEvent with tsl::CountDownAsyncValueRef
",copybara-service[bot],2024-11-20 05:07:36+00:00,['ezhulenev'],2024-11-20 20:32:08+00:00,2024-11-20 20:32:07+00:00,https://github.com/tensorflow/tensorflow/pull/80323,[],[],
2674395363,pull_request,closed,,Implement getting per-tensor quantization in the c and cc api,"Implement getting per-tensor quantization in the c and cc api
",copybara-service[bot],2024-11-20 04:54:50+00:00,['LukeBoyer'],2024-11-21 01:00:33+00:00,2024-11-21 01:00:33+00:00,https://github.com/tensorflow/tensorflow/pull/80322,[],[],
2674367325,pull_request,closed,,Add AssertEq wrapper and switch assert funcs to use generalized function poitners. check for correct union types in tensor cc api,"Add AssertEq wrapper and switch assert funcs to use generalized function poitners. check for correct union types in tensor cc api
",copybara-service[bot],2024-11-20 04:43:07+00:00,['LukeBoyer'],2024-11-20 22:58:46+00:00,2024-11-20 22:58:45+00:00,https://github.com/tensorflow/tensorflow/pull/80321,[],[],
2674324363,pull_request,closed,,[IFRT] Add pass to legalize VIFRT into IFRT.,"[IFRT] Add pass to legalize VIFRT into IFRT.
",copybara-service[bot],2024-11-20 04:39:21+00:00,[],2024-11-20 23:37:29+00:00,2024-11-20 23:37:29+00:00,https://github.com/tensorflow/tensorflow/pull/80320,[],[],
2674256748,pull_request,closed,,Add types to c api for quantization.,"Add types to c api for quantization.

Also add a bit more comments and re-organize some things.
",copybara-service[bot],2024-11-20 03:53:11+00:00,['LukeBoyer'],2024-11-20 22:09:18+00:00,2024-11-20 22:09:18+00:00,https://github.com/tensorflow/tensorflow/pull/80318,[],[],
2674213007,pull_request,closed,,[tsl:concurrency] Keep AsyncValueRef a part of CountDownAsyncValueRef State,"[tsl:concurrency] Keep AsyncValueRef a part of CountDownAsyncValueRef State

By keeping AsyncValueRef as a part of the State we avoid one extra reference counting operation when copying CountDownAsyncValue (and we expect to copy it `cnt` times).

name                     old cpu/op   new cpu/op   delta
BM_CountDownSuccess/8    95.8ns ± 4%  81.7ns ± 1%  -14.64%  (p=0.000 n=40+35)
BM_CountDownSuccess/16    142ns ± 1%   127ns ± 1%  -10.05%  (p=0.000 n=37+38)
BM_CountDownSuccess/32    229ns ± 2%   216ns ± 1%   -5.56%  (p=0.000 n=40+38)
BM_CountDownError/4       165ns ± 1%   152ns ± 2%   -7.65%  (p=0.000 n=39+40)
BM_CountDownError/8       238ns ± 2%   225ns ± 1%   -5.65%  (p=0.000 n=40+38)
BM_CountDownError/16      388ns ± 2%   369ns ± 2%   -4.77%  (p=0.000 n=40+36)
BM_CountDownError/32      684ns ± 1%   666ns ± 1%   -2.50%  (p=0.000 n=38+38)
",copybara-service[bot],2024-11-20 03:22:55+00:00,['ezhulenev'],2024-11-20 20:03:36+00:00,2024-11-20 20:03:35+00:00,https://github.com/tensorflow/tensorflow/pull/80317,[],[],
2674083780,pull_request,open,,Assert same channel ID if present,"Assert same channel ID if present

If present sned/recv and send-done/recv-done should hold the same channel ID.
",copybara-service[bot],2024-11-20 02:08:38+00:00,['frgossen'],2024-11-20 02:08:39+00:00,,https://github.com/tensorflow/tensorflow/pull/80313,[],[],
2674038518,pull_request,closed,,Cleanup. Refactor GetGatherScatterBatchParallelDims. No behavior change.,"Cleanup. Refactor GetGatherScatterBatchParallelDims. No behavior change.
",copybara-service[bot],2024-11-20 01:27:41+00:00,[],2024-11-20 04:14:03+00:00,2024-11-20 04:14:03+00:00,https://github.com/tensorflow/tensorflow/pull/80311,[],[],
2674037574,pull_request,closed,,[XLA:GPU][Emitters] Remove the complex.expm1 approximation.,"[XLA:GPU][Emitters] Remove the complex.expm1 approximation.

It was upstreamed in https://github.com/llvm/llvm-project/pull/115082#pullrequestreview-2417362971
Now we can use complex-to-standard pass.

Reverts e0ccb4b1069e70f5104cf9d1d0028cb6240db668
",copybara-service[bot],2024-11-20 01:26:47+00:00,['pifon2a'],2024-12-03 12:53:07+00:00,2024-12-03 12:53:05+00:00,https://github.com/tensorflow/tensorflow/pull/80310,[],[],
2674001736,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2024-11-20 00:56:38+00:00,[],2024-11-20 02:56:42+00:00,2024-11-20 02:56:41+00:00,https://github.com/tensorflow/tensorflow/pull/80309,[],[],
2673975879,pull_request,closed,,[Upkeep][XLA-Code-Health] Resolve 4 instances of the following issue: Todo (resolved),"[Upkeep][XLA-Code-Health] Resolve 4 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-20 00:30:54+00:00,[],2024-11-24 07:59:06+00:00,2024-11-24 07:59:05+00:00,https://github.com/tensorflow/tensorflow/pull/80308,[],[],
2673974050,pull_request,open,,Remove `py_import.bzl` and `verify_manylinux_compliance` from export after https://github.com/openxla/xla/commit/c6f26d4efa1ac071a1b39c9a81dae6214da22be3#diff-58867c8fba0bb0782bc1a4ef7e3f3c2727706be99debc019c256640ff720b3d7,"Remove `py_import.bzl` and `verify_manylinux_compliance` from export after https://github.com/openxla/xla/commit/c6f26d4efa1ac071a1b39c9a81dae6214da22be3#diff-58867c8fba0bb0782bc1a4ef7e3f3c2727706be99debc019c256640ff720b3d7
",copybara-service[bot],2024-11-20 00:29:13+00:00,['ddunl'],2024-11-20 00:29:14+00:00,,https://github.com/tensorflow/tensorflow/pull/80307,[],[],
2673972036,pull_request,closed,,[Upkeep][XLA-Code-Health] Resolve the following technical debt issue: Todo(resolved),"[Upkeep][XLA-Code-Health] Resolve the following technical debt issue: Todo(resolved)
",copybara-service[bot],2024-11-20 00:27:14+00:00,[],2024-11-20 04:04:23+00:00,2024-11-20 04:04:23+00:00,https://github.com/tensorflow/tensorflow/pull/80306,[],[],
2673967152,pull_request,closed,,Reverts feeb338707a7d2f3e69422f848bb4700d21e44a9,"Reverts feeb338707a7d2f3e69422f848bb4700d21e44a9
",copybara-service[bot],2024-11-20 00:22:29+00:00,[],2024-11-20 00:59:27+00:00,2024-11-20 00:59:27+00:00,https://github.com/tensorflow/tensorflow/pull/80305,[],[],
2673963133,pull_request,closed,,Migrate UnfoldLargeSplatConstantPass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate UnfoldLargeSplatConstantPass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-20 00:18:39+00:00,['vamsimanchala'],2024-11-20 04:50:52+00:00,2024-11-20 04:50:51+00:00,https://github.com/tensorflow/tensorflow/pull/80304,[],[],
2673942918,pull_request,closed,,Remove unused functions from ir_emission_utils.cc,"Remove unused functions from ir_emission_utils.cc
",copybara-service[bot],2024-11-19 23:59:29+00:00,['majnemer'],2024-11-20 23:42:58+00:00,2024-11-20 23:42:58+00:00,https://github.com/tensorflow/tensorflow/pull/80303,[],[],
2673939840,pull_request,open,,[Upkeep][XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved),"[Upkeep][XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-19 23:57:08+00:00,[],2024-11-19 23:57:08+00:00,,https://github.com/tensorflow/tensorflow/pull/80302,[],[],
2673934987,pull_request,open,,[IFRT] Legalize IFRT dialect into VIFRT dialect.,"[IFRT] Legalize IFRT dialect into VIFRT dialect.

This change adds the legalization pass from IFRT to VIFRT. Legalization uses a templated OpConversion class, which is refined via the `IFRT` <-> `VIFRT` and `mlir::Func::*` <-> `VIFRT` op mappings defined in `map_ifrt_to_vifrt.h` The change versions also `mlir::func::FuncOp`, `mlir::func::ReturnOp` and `mlir::func::CallOp` because this provides the following advantages: 1) we can use the templated OpConversion class rather than implementing a separate converter for each op, and 2) we can restrict the surface of possible breaking changes to just builtin types and attributes. Moreover, the change versions `mlir::FunctionType` and `mlir::TypeAttr` in order to be able to use the generic Op converter, and to restrict types allowed in functions (just builtin and IFRT types).
",copybara-service[bot],2024-11-19 23:53:44+00:00,[],2024-11-19 23:53:44+00:00,,https://github.com/tensorflow/tensorflow/pull/80301,[],[],
2673929031,pull_request,closed,,Stop using gpu_types.h where it's not needed.,"Stop using gpu_types.h where it's not needed.
",copybara-service[bot],2024-11-19 23:48:13+00:00,[],2024-11-20 03:45:14+00:00,2024-11-20 03:45:13+00:00,https://github.com/tensorflow/tensorflow/pull/80300,[],[],
2673921693,pull_request,closed,,[Code-Health] Resolve the following technical debt issue: ,"[Code-Health] Resolve the following technical debt issue: 

	Todo(resolved)
",copybara-service[bot],2024-11-19 23:42:46+00:00,[],2024-11-20 19:13:01+00:00,2024-11-20 19:13:00+00:00,https://github.com/tensorflow/tensorflow/pull/80299,[],[],
2673913065,pull_request,closed,,[XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved),"[XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-19 23:35:15+00:00,[],2024-11-20 19:07:18+00:00,2024-11-20 19:07:17+00:00,https://github.com/tensorflow/tensorflow/pull/80298,[],[],
2673852227,pull_request,closed,,[xla:cpu] Resolve constant buffers,"[xla:cpu] Resolve constant buffers
",copybara-service[bot],2024-11-19 23:25:08+00:00,['ezhulenev'],2024-11-21 04:40:07+00:00,2024-11-21 04:40:06+00:00,https://github.com/tensorflow/tensorflow/pull/80297,[],[],
2673811008,pull_request,closed,,Make g_trace_filter_bitmap atomic to avoid race across threads.,"Make g_trace_filter_bitmap atomic to avoid race across threads.
",copybara-service[bot],2024-11-19 22:59:41+00:00,[],2024-11-20 10:06:19+00:00,2024-11-20 10:06:18+00:00,https://github.com/tensorflow/tensorflow/pull/80296,[],[],
2673806965,pull_request,closed,,Add quantized OP in test data.,"Add quantized OP in test data.
",copybara-service[bot],2024-11-19 22:56:44+00:00,[],2024-11-19 23:44:00+00:00,2024-11-19 23:43:59+00:00,https://github.com/tensorflow/tensorflow/pull/80295,[],[],
2673800160,pull_request,open,,Enable a test,"Enable a test
",copybara-service[bot],2024-11-19 22:52:08+00:00,[],2024-11-20 23:43:55+00:00,,https://github.com/tensorflow/tensorflow/pull/80294,[],[],
2673794418,pull_request,closed,,[IFRT] Legalize IFRT dialect into VIFRT dialect.,"[IFRT] Legalize IFRT dialect into VIFRT dialect.

This change adds the legalization pass from IFRT to VIFRT. Legalization uses a templated OpConversion class, which is refined via the `IFRT` <-> `VIFRT` and `mlir::Func::*` <-> `VIFRT` op mappings defined in `map_ifrt_to_vifrt.h` The change versions also `mlir::func::FuncOp`, `mlir::func::ReturnOp` and `mlir::func::CallOp` because this provides the following advantages: 1) we can use the templated OpConversion class rather than implementing a separate converter for each op, and 2) we can restrict the surface of possible breaking changes to just builtin types and attributes. Moreover, the change versions `mlir::FunctionType` and `mlir::TypeAttr` in order to be able to use the generic Op converter, and to restrict types allowed in functions (just builtin and IFRT types).
",copybara-service[bot],2024-11-19 22:48:28+00:00,[],2024-11-19 23:59:53+00:00,2024-11-19 23:59:52+00:00,https://github.com/tensorflow/tensorflow/pull/80292,[],[],
2673792100,pull_request,closed,,legalization_op_config: Delete unused IsOpLegalizedWithMlir.,"legalization_op_config: Delete unused IsOpLegalizedWithMlir.

Reverts feeb338707a7d2f3e69422f848bb4700d21e44a9
",copybara-service[bot],2024-11-19 22:46:58+00:00,[],2024-11-20 01:42:44+00:00,2024-11-20 01:42:43+00:00,https://github.com/tensorflow/tensorflow/pull/80291,[],[],
2673767637,pull_request,closed,,Updating file visibility for path.,"Updating file visibility for path.
",copybara-service[bot],2024-11-19 22:39:40+00:00,[],2024-12-03 21:38:37+00:00,2024-12-03 21:38:35+00:00,https://github.com/tensorflow/tensorflow/pull/80290,[],[],
2673723578,pull_request,open,,Remove unused lowering,"Remove unused lowering

Softsign (and softsigngrad) are not used as MlirXlaOpKernel, removing unused lowering.
",copybara-service[bot],2024-11-19 22:32:09+00:00,['jparkerh'],2024-11-25 18:08:39+00:00,,https://github.com/tensorflow/tensorflow/pull/80289,[],[],
2673705249,pull_request,open,,Integrate LLVM at llvm/llvm-project@68b7ab127f58,"Integrate LLVM at llvm/llvm-project@68b7ab127f58

Updates LLVM usage to match
[68b7ab127f58](https://github.com/llvm/llvm-project/commit/68b7ab127f58)
",copybara-service[bot],2024-11-19 22:18:44+00:00,[],2024-11-20 03:05:28+00:00,,https://github.com/tensorflow/tensorflow/pull/80288,[],[],
2673665567,pull_request,closed,,Reverts f62e0d0982c03b5f7798bfe93c52da819fc0a83b,"Reverts f62e0d0982c03b5f7798bfe93c52da819fc0a83b
",copybara-service[bot],2024-11-19 21:53:00+00:00,['BlaziusMaximus'],2024-11-20 20:17:34+00:00,2024-11-20 20:17:32+00:00,https://github.com/tensorflow/tensorflow/pull/80287,[],[],
2673662279,pull_request,closed,,Move compiler plugin unique ptr alias to cc api. Also use string view for bytes return from plugin in tests to avoid copy,"Move compiler plugin unique ptr alias to cc api. Also use string view for bytes return from plugin in tests to avoid copy
",copybara-service[bot],2024-11-19 21:50:59+00:00,['LukeBoyer'],2024-11-20 05:48:24+00:00,2024-11-20 05:48:23+00:00,https://github.com/tensorflow/tensorflow/pull/80286,[],[],
2673660537,pull_request,closed,,Add a few more mlir based test model,"Add a few more mlir based test model
",copybara-service[bot],2024-11-19 21:49:51+00:00,['LukeBoyer'],2024-11-19 22:56:48+00:00,2024-11-19 22:56:47+00:00,https://github.com/tensorflow/tensorflow/pull/80285,[],[],
2673659711,pull_request,closed,,Optimize explicit broadcasting-like patterns for TFL_Select*Ops in TFLite.,"Optimize explicit broadcasting-like patterns for TFL_Select*Ops in TFLite.

This CL optimizes explicit broadcasting-like patterns in TFLite, because TFLite Ops support implicit broadcasting.

Also, this CL is moving the existing fusions on broadcast-to+select to the dedicated pass.

The patterns are:
- Fuse splat const into select op.
- Fuse fill-op into select op.
",copybara-service[bot],2024-11-19 21:49:15+00:00,['vamsimanchala'],2024-11-20 23:17:16+00:00,2024-11-20 23:17:15+00:00,https://github.com/tensorflow/tensorflow/pull/80284,[],[],
2673659671,pull_request,closed,,[xla:cpu] Resolve arguments/results/temp mapping from buffer assignment,"[xla:cpu] Resolve arguments/results/temp mapping from buffer assignment
",copybara-service[bot],2024-11-19 21:49:14+00:00,['ezhulenev'],2024-11-21 03:41:31+00:00,2024-11-21 03:41:30+00:00,https://github.com/tensorflow/tensorflow/pull/80283,[],[],
2673563558,pull_request,closed,,Move some tests to public XLA:CPU API,"Move some tests to public XLA:CPU API
",copybara-service[bot],2024-11-19 21:17:37+00:00,['changm'],2024-11-19 23:53:51+00:00,2024-11-19 23:53:51+00:00,https://github.com/tensorflow/tensorflow/pull/80281,[],[],
2673562184,pull_request,closed,,[XLA:ALGEBRAIC_SIMPLIFIER] Turn constant all-gather into broadcast,"[XLA:ALGEBRAIC_SIMPLIFIER] Turn constant all-gather into broadcast
",copybara-service[bot],2024-11-19 21:16:43+00:00,['blakehechtman'],2024-11-20 16:00:39+00:00,2024-11-20 16:00:38+00:00,https://github.com/tensorflow/tensorflow/pull/80280,[],[],
2673550781,pull_request,closed,,PR #19346: Bumped rules_python version to 0.39.0,"PR #19346: Bumped rules_python version to 0.39.0

Imported from GitHub PR https://github.com/openxla/xla/pull/19346

cc @hawkinsp 
Copybara import of the project:

--
292e7ebb7ee57e5af5977c08f0aaf28fc1f852e2 by vfdev-5 <vfdev.5@gmail.com>:

Bumped rules_python version to 0.39.0

Merging this change closes #19346

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19346 from vfdev-5:bump-rules-python-version-to-0.39 292e7ebb7ee57e5af5977c08f0aaf28fc1f852e2
",copybara-service[bot],2024-11-19 21:09:20+00:00,[],2024-11-22 10:42:34+00:00,2024-11-22 10:42:33+00:00,https://github.com/tensorflow/tensorflow/pull/80279,[],[],
2673530403,pull_request,closed,,Move next pluggable device to public XLA:CPU API,"Move next pluggable device to public XLA:CPU API
",copybara-service[bot],2024-11-19 20:57:02+00:00,['changm'],2024-11-20 21:57:49+00:00,2024-11-20 21:57:48+00:00,https://github.com/tensorflow/tensorflow/pull/80278,[],[],
2673516503,pull_request,closed,,Split out MemorySpaceAssignmentTest class for re-use.,"Split out MemorySpaceAssignmentTest class for re-use.
",copybara-service[bot],2024-11-19 20:49:30+00:00,[],2024-12-17 22:51:41+00:00,2024-12-17 22:51:40+00:00,https://github.com/tensorflow/tensorflow/pull/80277,[],[],
2673515581,pull_request,closed,,Move stable hlo compile test to XLA:CPU public API,"Move stable hlo compile test to XLA:CPU public API
",copybara-service[bot],2024-11-19 20:48:57+00:00,['changm'],2024-11-19 21:22:51+00:00,2024-11-19 21:22:50+00:00,https://github.com/tensorflow/tensorflow/pull/80276,[],[],
2673414130,pull_request,closed,,Fold FillOp into TFL Ops that support implicit broadcasting.,"Fold FillOp into TFL Ops that support implicit broadcasting.
",copybara-service[bot],2024-11-19 20:19:57+00:00,['vamsimanchala'],2024-11-19 22:35:22+00:00,2024-11-19 22:35:21+00:00,https://github.com/tensorflow/tensorflow/pull/80275,[],[],
2673403877,pull_request,closed,,Remove unneeded xla:statusor dependency.,"Remove unneeded xla:statusor dependency.
",copybara-service[bot],2024-11-19 20:13:41+00:00,[],2024-11-19 22:09:02+00:00,2024-11-19 22:09:01+00:00,https://github.com/tensorflow/tensorflow/pull/80274,[],[],
2673397981,pull_request,closed,,Refactor exhaustive_test_main into a separate library target,"Refactor exhaustive_test_main into a separate library target
",copybara-service[bot],2024-11-19 20:10:00+00:00,[],2024-11-20 19:24:59+00:00,2024-11-20 19:24:58+00:00,https://github.com/tensorflow/tensorflow/pull/80273,[],[],
2673388197,pull_request,closed,,[Upkeep][XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved),"[Upkeep][XLA-Code-Health] Resolve 2 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-19 20:04:14+00:00,[],2024-11-19 22:16:52+00:00,2024-11-19 22:16:51+00:00,https://github.com/tensorflow/tensorflow/pull/80272,[],[],
2673377209,pull_request,open,,[Upkeep][XLA-Code-Health] Resolve 3 instances of the following issue: Todo (resolved),"[Upkeep][XLA-Code-Health] Resolve 3 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-19 19:58:42+00:00,[],2024-11-19 19:58:42+00:00,,https://github.com/tensorflow/tensorflow/pull/80271,[],[],
2673361154,pull_request,closed,,Remove unneeded use of gpu_types.h in topk_kernel_test.cc.,"Remove unneeded use of gpu_types.h in topk_kernel_test.cc.
",copybara-service[bot],2024-11-19 19:51:03+00:00,[],2024-11-20 00:26:18+00:00,2024-11-20 00:26:17+00:00,https://github.com/tensorflow/tensorflow/pull/80270,[],[],
2673349328,pull_request,closed,,Add subgraph name to model runtime info proto.,"Add subgraph name to model runtime info proto.
",copybara-service[bot],2024-11-19 19:45:15+00:00,[],2024-11-20 23:27:17+00:00,2024-11-20 23:27:16+00:00,https://github.com/tensorflow/tensorflow/pull/80269,[],[],
2673347275,pull_request,open,,Refactor TF build wheel rule.,"Refactor TF build wheel rule.
",copybara-service[bot],2024-11-19 19:44:06+00:00,[],2025-01-06 20:44:13+00:00,,https://github.com/tensorflow/tensorflow/pull/80268,[],[],
2673342642,pull_request,open,,"Switch JAX to PJRT C API, for GPU.","Switch JAX to PJRT C API, for GPU.
",copybara-service[bot],2024-11-19 19:41:27+00:00,[],2024-11-19 19:41:27+00:00,,https://github.com/tensorflow/tensorflow/pull/80267,[],[],
2673328240,pull_request,closed,,Migrate WhileOutlinePass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate WhileOutlinePass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-19 19:34:40+00:00,['vamsimanchala'],2024-11-20 00:18:43+00:00,2024-11-20 00:18:43+00:00,https://github.com/tensorflow/tensorflow/pull/80266,[],[],
2673326701,pull_request,closed,,[XLA:TPU:MSA],"[XLA:TPU:MSA]
* Add support for overriding cross program prefetch behavior.
* Add support for filtering buffer intervals based on the uses of the buffer.
* Add tests for overriding cross program prefetch behavior
* Add tests for expanding filtering criteria.
",copybara-service[bot],2024-11-19 19:33:50+00:00,['subhankarshah'],2024-11-21 01:50:00+00:00,2024-11-21 01:49:58+00:00,https://github.com/tensorflow/tensorflow/pull/80265,[],[],
2673324117,pull_request,closed,,[XLA:MSA] Allow more flexible filtering when picking instruction to schedule after/before for prefetch time override.,"[XLA:MSA] Allow more flexible filtering when picking instruction to schedule after/before for prefetch time override.
",copybara-service[bot],2024-11-19 19:32:22+00:00,['subhankarshah'],2024-11-21 07:57:15+00:00,2024-11-21 07:57:14+00:00,https://github.com/tensorflow/tensorflow/pull/80264,[],[],
2673263606,pull_request,closed,,Add a test to check C header compiler compatibility,"Add a test to check C header compiler compatibility

Also fixed invalid C++ header usage.
",copybara-service[bot],2024-11-19 19:22:04+00:00,['terryheo'],2024-11-20 00:11:00+00:00,2024-11-20 00:10:59+00:00,https://github.com/tensorflow/tensorflow/pull/80263,[],[],
2673222368,pull_request,closed,,[Code-Health] Resolve the following technical debt issue: Todo(resolved) in CUDA BUILD file.,"[Code-Health] Resolve the following technical debt issue: Todo(resolved) in CUDA BUILD file.
",copybara-service[bot],2024-11-19 19:03:33+00:00,[],2024-11-20 18:57:39+00:00,2024-11-20 18:57:38+00:00,https://github.com/tensorflow/tensorflow/pull/80262,[],[],
2673203730,pull_request,closed,,Fix the heuristic for extending events in derived timeline to have a 2x(previous event's duration) threshold for the gap for TPU sessions.,"Fix the heuristic for extending events in derived timeline to have a 2x(previous event's duration) threshold for the gap for TPU sessions.
",copybara-service[bot],2024-11-19 18:54:17+00:00,[],2024-12-07 03:27:03+00:00,2024-12-07 03:27:02+00:00,https://github.com/tensorflow/tensorflow/pull/80261,[],[],
2673200634,pull_request,closed,,[Code-Health] Resolve 2 instances of the following issue: Todo (resolved),"[Code-Health] Resolve 2 instances of the following issue: Todo (resolved)
",copybara-service[bot],2024-11-19 18:52:51+00:00,[],2024-11-21 05:57:23+00:00,2024-11-21 05:57:23+00:00,https://github.com/tensorflow/tensorflow/pull/80260,[],[],
2673121741,pull_request,closed,,[xla:cpu] Optimize buffer allocations construction from se::DeviceMemoryBase,"[xla:cpu] Optimize buffer allocations construction from se::DeviceMemoryBase

name                 old cpu/op   new cpu/op   delta
BM_NanoRtAddScalars  82.2ns ± 2%  63.1ns ± 2%  -23.17%  (p=0.000 n=37+40)
BM_NanoRtFibonacci   86.7ns ± 2%  68.4ns ± 2%  -21.09%  (p=0.000 n=37+35)
BM_PjRtAddScalars    1.78µs ± 2%  1.79µs ± 2%     ~     (p=0.280 n=39+38)
BM_PjRtFibonacci     1.79µs ± 3%  1.79µs ± 3%     ~     (p=0.355 n=38+38)
",copybara-service[bot],2024-11-19 18:36:10+00:00,['ezhulenev'],2024-11-21 16:13:01+00:00,2024-11-21 16:13:00+00:00,https://github.com/tensorflow/tensorflow/pull/80259,[],[],
2673084165,pull_request,closed,,[XLA:GPU] Use ShuffleOp to reverse the order of elements in a vector.,"[XLA:GPU] Use ShuffleOp to reverse the order of elements in a vector.

No functional change is intended but it generates less IR.
",copybara-service[bot],2024-11-19 18:14:23+00:00,['majnemer'],2024-11-20 20:59:52+00:00,2024-11-20 20:59:51+00:00,https://github.com/tensorflow/tensorflow/pull/80258,[],[],
2673072542,pull_request,closed,,[Code-Health] Resolve the following technical debt issue: ,"[Code-Health] Resolve the following technical debt issue: 

	Todo(resolved)
",copybara-service[bot],2024-11-19 18:08:37+00:00,[],2024-11-20 19:19:00+00:00,2024-11-20 19:18:59+00:00,https://github.com/tensorflow/tensorflow/pull/80257,[],[],
2672984636,pull_request,closed,,[Code-Health] Resolve the following technical debt issue: Todo(resolved),"[Code-Health] Resolve the following technical debt issue: Todo(resolved)
",copybara-service[bot],2024-11-19 17:36:02+00:00,[],2024-11-22 23:49:53+00:00,2024-11-22 23:49:52+00:00,https://github.com/tensorflow/tensorflow/pull/80256,[],[],
2672980305,pull_request,closed,,Add a canonicalization pattern for TFL_DivOp with constant divisor.,"Add a canonicalization pattern for TFL_DivOp with constant divisor.

Floating point division can be ~10x more expensive than a multiplication. This pattern replaces division by a constant with a multiplication by a reciprocal of that constant.
",copybara-service[bot],2024-11-19 17:35:44+00:00,['vamsimanchala'],2024-12-13 19:47:05+00:00,2024-12-13 19:47:03+00:00,https://github.com/tensorflow/tensorflow/pull/80255,[],[],
2672950507,pull_request,closed,,Remove unneeded xla:status and xla::statusor dependencies.,"Remove unneeded xla:status and xla::statusor dependencies.
",copybara-service[bot],2024-11-19 17:31:47+00:00,[],2024-11-20 18:17:58+00:00,2024-11-20 18:17:57+00:00,https://github.com/tensorflow/tensorflow/pull/80254,[],[],
2672581030,pull_request,closed,,Integrate LLVM at llvm/llvm-project@b03a747fc0fc,"Integrate LLVM at llvm/llvm-project@b03a747fc0fc

Updates LLVM usage to match
[b03a747fc0fc](https://github.com/llvm/llvm-project/commit/b03a747fc0fc)
",copybara-service[bot],2024-11-19 15:35:03+00:00,['metaflow'],2024-11-19 17:44:08+00:00,2024-11-19 17:44:07+00:00,https://github.com/tensorflow/tensorflow/pull/80252,[],[],
2672051524,pull_request,closed,,Reverts 67267fbb0750d97e1be3b918fe00ba494b93b656,"Reverts 67267fbb0750d97e1be3b918fe00ba494b93b656
",copybara-service[bot],2024-11-19 12:45:06+00:00,[],2024-11-19 13:41:25+00:00,2024-11-19 13:41:24+00:00,https://github.com/tensorflow/tensorflow/pull/80251,[],[],
2672045265,pull_request,open,,[XLA:GPU] Support bf16 for sort,"[XLA:GPU] Support bf16 for sort
",copybara-service[bot],2024-11-19 12:42:24+00:00,['thomasjoerg'],2024-11-19 13:55:56+00:00,,https://github.com/tensorflow/tensorflow/pull/80250,[],[],
2671971800,pull_request,open,,Enable some tests for float8 types in elemental_ir_emitter_test.cc,"Enable some tests for float8 types in elemental_ir_emitter_test.cc
",copybara-service[bot],2024-11-19 12:08:59+00:00,[],2024-11-19 12:09:06+00:00,,https://github.com/tensorflow/tensorflow/pull/80249,[],"[{'comment_id': 2485541164, 'issue_id': 2671971800, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80249/checks?check_run_id=33196913416) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 11, 19, 12, 9, 5, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-11-19 12:09:05 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80249/checks?check_run_id=33196913416) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2671943687,pull_request,closed,,[XLA:GPU] Another attempt to enable HLO auto layout.,"[XLA:GPU] Another attempt to enable HLO auto layout.

Instead of not resetting the layout, select the (missing) layout in the layout reset callback -- that way we are able to keep it local to stream executor.
",copybara-service[bot],2024-11-19 11:57:45+00:00,[],2024-11-19 12:53:22+00:00,2024-11-19 12:53:20+00:00,https://github.com/tensorflow/tensorflow/pull/80248,[],[],
2671937831,pull_request,open,,PR #19432: [GPU] Sharded autotuning: exchange only results of the latest compilation.,"PR #19432: [GPU] Sharded autotuning: exchange only results of the latest compilation.

Imported from GitHub PR https://github.com/openxla/xla/pull/19432

Exchanging the cached results of previous compilation of other modules isn't necessary and leads to conflicts.
Copybara import of the project:

--
f85f07505b0b44c7ec59fb13f2fae172489e82e1 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Sharded autotuning: exchange only results of the latest compilation.

Exchanging the cached results of previous compilation of other modules
isn't necessary and leads to conflicts.

Merging this change closes #19432

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19432 from openxla:fix_sharded_autotuning_result_exchange f85f07505b0b44c7ec59fb13f2fae172489e82e1
",copybara-service[bot],2024-11-19 11:55:17+00:00,[],2024-11-19 11:55:17+00:00,,https://github.com/tensorflow/tensorflow/pull/80247,[],[],
2671814884,pull_request,open,,Reverts 2dee6183d0f2ff7a90f4883fa18c86a184517b5d,"Reverts 2dee6183d0f2ff7a90f4883fa18c86a184517b5d
",copybara-service[bot],2024-11-19 11:22:10+00:00,[],2024-11-21 17:58:25+00:00,,https://github.com/tensorflow/tensorflow/pull/80246,[],[],
2671602462,pull_request,open,,PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR #18838: [NVIDIA GPU] Support multi-operand collective-permute

Imported from GitHub PR https://github.com/openxla/xla/pull/18838

For collective-permutes with small message sizes, it is beneficial to combine them into a single collective because
1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion;
2. fewer collectives make it easier for LHS to make better decision.

In order to support combining collective-permutes, we need to support multi-operand collective-permute first, a.k.a. the combined collective-permute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands.
Copybara import of the project:

--
5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun <tesun@nvidia.com>:

support multi-operand cp

--
170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun <tesun@nvidia.com>:

minor refactoring

--
0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun <tesun@nvidia.com>:

update python interface

--
9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun <tesun@nvidia.com>:

polish python interface

--
3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun <tesun@nvidia.com>:

formatting

--
d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun <tesun@nvidia.com>:

formatting

--
9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun <tesun@nvidia.com>:

refactor overloading

--
d6ed6c5836302f308dff7c4bdd89b18655ceff7a by Terry Sun <tesun@nvidia.com>:

minor refactor

Merging this change closes #18838

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp d6ed6c5836302f308dff7c4bdd89b18655ceff7a
",copybara-service[bot],2024-11-19 10:17:51+00:00,[],2024-11-22 15:32:56+00:00,,https://github.com/tensorflow/tensorflow/pull/80245,[],[],
2671547933,pull_request,closed,,[XLA:GPU] Introduce EmitterLocOpBuilder that could annotate the mlir with the file:line annotations that are visible in the triton dump,"[XLA:GPU] Introduce EmitterLocOpBuilder that could annotate the mlir with the file:line annotations that are visible in the triton dump

During the troubleshooting sessions it sometimes hard to find the emitter code that emitted the particular instruction. It make sense to instrument the emitter code and annotate the generated code with file:line info. The annotations emitting and dumping code is guarded with the --xla_dump_emitter_loc flag.
",copybara-service[bot],2024-11-19 09:55:35+00:00,[],2024-12-11 11:07:10+00:00,2024-12-11 11:07:09+00:00,https://github.com/tensorflow/tensorflow/pull/80244,[],[],
2671447838,pull_request,closed,,PR #19426: [ROCm] Disable gpu_too_many_blocks_test for rocm,"PR #19426: [ROCm] Disable gpu_too_many_blocks_test for rocm

Imported from GitHub PR https://github.com/openxla/xla/pull/19426

After this change to the test inputs https://github.com/openxla/xla/commit/b10653ff606506dc510d71ddecf6483033b6a6e2 ""too many blocks"" exception is not getting triggered anymore (shape is not big enough). 

Due to the low importance of the test, it was decided to disable it.
Copybara import of the project:

--
ee36ca0f0c0dddb8db724327af78f0b59de3903b by Milica Makevic <Milica.Makevic@amd.com>:

Disable gpu_too_many_blocks_test for rocm

Merging this change closes #19426

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19426 from ROCm:disable_too_many_blocks_test ee36ca0f0c0dddb8db724327af78f0b59de3903b
",copybara-service[bot],2024-11-19 09:32:48+00:00,[],2024-11-19 13:29:57+00:00,2024-11-19 13:29:56+00:00,https://github.com/tensorflow/tensorflow/pull/80243,[],[],
2671440005,pull_request,closed,,Integrate LLVM at llvm/llvm-project@6e1acdcdc1b3,"Integrate LLVM at llvm/llvm-project@6e1acdcdc1b3

Updates LLVM usage to match
[6e1acdcdc1b3](https://github.com/llvm/llvm-project/commit/6e1acdcdc1b3)
",copybara-service[bot],2024-11-19 09:29:32+00:00,['metaflow'],2024-11-19 13:19:33+00:00,2024-11-19 13:19:33+00:00,https://github.com/tensorflow/tensorflow/pull/80242,[],[],
2671389857,pull_request,open,,Interface and generic graph tools for compiler plugins. Stubs for qnn,"Interface and generic graph tools for compiler plugins. Stubs for qnn
",copybara-service[bot],2024-11-19 09:10:33+00:00,['LukeBoyer'],2024-11-19 09:10:34+00:00,,https://github.com/tensorflow/tensorflow/pull/80240,[],[],
2671372668,pull_request,closed,,[XLA:GPU] Don't capture a string_view for SlowOperationAlarm.,"[XLA:GPU] Don't capture a string_view for SlowOperationAlarm.

This was not caught because the string_view is only used for logging if
autotuning takes too long.
",copybara-service[bot],2024-11-19 09:03:55+00:00,['akuegel'],2024-11-19 12:21:54+00:00,2024-11-19 12:21:53+00:00,https://github.com/tensorflow/tensorflow/pull/80239,[],[],
2671331866,pull_request,closed,,PR #19429: Fix deterministic scatter expander pass,"PR #19429: Fix deterministic scatter expander pass

Imported from GitHub PR https://github.com/openxla/xla/pull/19429

The pass implementation branches on `non_scalar_update` and (previously) only updated `index_vector_dim` in one branch, whereas the indices tensor is canonicalized unconditionally.

This PR fixes the bug and adds the regression test.
Copybara import of the project:

--
e0192acb21a47374bdca700fa87b0f2b85b66e0e by Sergey Kozub <skozub@nvidia.com>:

Fix deterministic scatter expander pass and re-enable it by default

Merging this change closes #19429

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19429 from openxla:skozub/scatter_fix e0192acb21a47374bdca700fa87b0f2b85b66e0e
",copybara-service[bot],2024-11-19 08:48:45+00:00,[],2024-11-19 11:21:34+00:00,2024-11-19 11:21:33+00:00,https://github.com/tensorflow/tensorflow/pull/80238,[],[],
2671036279,pull_request,closed,,Add lowerings for PT2E QDQ ops,"Add lowerings for PT2E QDQ ops
",copybara-service[bot],2024-11-19 07:10:52+00:00,['chunnienc'],2024-12-04 01:35:38+00:00,2024-12-04 01:35:37+00:00,https://github.com/tensorflow/tensorflow/pull/80236,[],[],
2670957842,pull_request,open,,PR #19429: Fix deterministic scatter expander pass and re-enable it by default,"PR #19429: Fix deterministic scatter expander pass and re-enable it by default

Imported from GitHub PR https://github.com/openxla/xla/pull/19429

The pass implementation branches on `non_scalar_update` and (previously) only updated `index_vector_dim` in one branch, whereas the indices tensor is canonicalized unconditionally.

This PR fixes the bug, adds the regression test and re-enables the pass by default.
Copybara import of the project:

--
e0192acb21a47374bdca700fa87b0f2b85b66e0e by Sergey Kozub <skozub@nvidia.com>:

Fix deterministic scatter expander pass and re-enable it by default

Merging this change closes #19429

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19429 from openxla:skozub/scatter_fix e0192acb21a47374bdca700fa87b0f2b85b66e0e
",copybara-service[bot],2024-11-19 06:47:04+00:00,[],2024-11-19 06:47:04+00:00,,https://github.com/tensorflow/tensorflow/pull/80235,[],[],
2670877560,pull_request,closed,,Clarify the dimensions in gather/scatter dimensions. The following dimensions do NOT overlap. These dims are processed separately in spmd partitioner.,"Clarify the dimensions in gather/scatter dimensions. The following dimensions do NOT overlap. These dims are processed separately in spmd partitioner.
1. Explicit batching dims exist in all tensors (operand, indices, output).
2. Index pass-through dims exist in indices and output.
3. Operand pass-through dims exist in operand and output.

We replace `GatherOutputShardingFromIndexIndexPassthroughDimensions` with `GatherOutputShardingFromIndex(bool consider_explict_batch_dims=true)`.

The added test failed before this change since it process explicit batch dims as index pass-through dims. This change fix this issue.
",copybara-service[bot],2024-11-19 05:56:16+00:00,[],2024-11-21 19:09:43+00:00,2024-11-21 19:09:42+00:00,https://github.com/tensorflow/tensorflow/pull/80234,[],[],
2670769523,pull_request,closed,,Do not allow overlap between explicit and implicit batching dims in gather/scatter instructions. Implicit batching dims are also known as index parallel dims.,"Do not allow overlap between explicit and implicit batching dims in gather/scatter instructions. Implicit batching dims are also known as index parallel dims.

Update `GetGatherScatterBatchParallelDims` accordingly. The sharding propagation and spmd partitioner will process explicit and implicit batching dims separately.
",copybara-service[bot],2024-11-19 05:05:05+00:00,[],2024-11-20 17:50:47+00:00,2024-11-20 17:50:47+00:00,https://github.com/tensorflow/tensorflow/pull/80233,[],[],
2670669745,pull_request,closed,,Internal only change.,"Internal only change.
",copybara-service[bot],2024-11-19 04:17:48+00:00,[],2024-11-25 08:01:50+00:00,2024-11-25 08:01:48+00:00,https://github.com/tensorflow/tensorflow/pull/80232,[],[],
2670461341,pull_request,closed,,Cleanup. Simplify the gather/scatter related functions in hlo_sharding_util by using `PropagateShardingAlongDimsAndReplicateOthers`. This is a no-op change.,"Cleanup. Simplify the gather/scatter related functions in hlo_sharding_util by using `PropagateShardingAlongDimsAndReplicateOthers`. This is a no-op change.
",copybara-service[bot],2024-11-19 01:38:58+00:00,[],2024-11-20 16:47:05+00:00,2024-11-20 16:47:03+00:00,https://github.com/tensorflow/tensorflow/pull/80230,[],[],
2670436760,pull_request,closed,,Add a test to verify that the PJRT API struct is laid out as expected.,"Add a test to verify that the PJRT API struct is laid out as expected.

This test will help to ensure that the API is stable and that changes to the API are made in a backwards-compatible way.
",copybara-service[bot],2024-11-19 01:17:49+00:00,['majnemer'],2024-12-03 03:41:22+00:00,2024-12-03 03:41:20+00:00,https://github.com/tensorflow/tensorflow/pull/80229,[],[],
2670430166,pull_request,closed,,Update clone with new operands to handle ragged all-to-all.,"Update clone with new operands to handle ragged all-to-all.
",copybara-service[bot],2024-11-19 01:12:03+00:00,[],2024-12-04 19:30:00+00:00,2024-12-04 19:29:59+00:00,https://github.com/tensorflow/tensorflow/pull/80228,[],[],
2670406636,pull_request,closed,,[xla:codegen] First version of KernelEmitter and KernelSpec APIs for shared codegen,"[xla:codegen] First version of KernelEmitter and KernelSpec APIs for shared codegen
",copybara-service[bot],2024-11-19 00:50:23+00:00,['ezhulenev'],2024-11-19 01:30:03+00:00,2024-11-19 01:30:02+00:00,https://github.com/tensorflow/tensorflow/pull/80227,[],[],
2670399177,pull_request,open,,Remove unused `tf_additional_core_deps`,"Remove unused `tf_additional_core_deps`
",copybara-service[bot],2024-11-19 00:43:20+00:00,['ddunl'],2024-11-19 00:43:22+00:00,,https://github.com/tensorflow/tensorflow/pull/80226,[],[],
2670387976,pull_request,open,,In progress. Not ready for review. Approach 2.,"In progress. Not ready for review. Approach 2.
",copybara-service[bot],2024-11-19 00:31:45+00:00,[],2024-11-19 05:14:25+00:00,,https://github.com/tensorflow/tensorflow/pull/80225,[],[],
2670331590,pull_request,open,,PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,"PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default

Imported from GitHub PR https://github.com/openxla/xla/pull/19451

We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability.
Copybara import of the project:

--
7facf3c8a1401a1dcd578f3d192cc4ff631052ce by Tj Xu <tjx@nvidia.com>:

Turn xla_gpu_multi_streamed_windowed_einsum on by default


Merging this change closes #19451

Reverts c3fd63e779156e3d9eb8fbeac1aebea59c16c564

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag 7facf3c8a1401a1dcd578f3d192cc4ff631052ce
",copybara-service[bot],2024-11-18 23:45:14+00:00,[],2024-11-25 12:02:59+00:00,,https://github.com/tensorflow/tensorflow/pull/80224,[],[],
2670265860,pull_request,closed,,Cleanup. Refactor gather_scatter_handler. Remove unused code. Replace sort with stable_sort.,"Cleanup. Refactor gather_scatter_handler. Remove unused code. Replace sort with stable_sort.
",copybara-service[bot],2024-11-18 23:27:01+00:00,[],2024-11-19 05:59:47+00:00,2024-11-19 05:59:46+00:00,https://github.com/tensorflow/tensorflow/pull/80222,[],[],
2670264290,pull_request,closed,,Update XLA's `warnings.bazelrc`,"Update XLA's `warnings.bazelrc`
",copybara-service[bot],2024-11-18 23:25:30+00:00,['ddunl'],2024-11-19 00:04:15+00:00,2024-11-19 00:04:14+00:00,https://github.com/tensorflow/tensorflow/pull/80221,[],[],
2670225113,pull_request,closed,,Update the default Python version to 3.11,"Update the default Python version to 3.11

This is to fix issue with gsutil which expects Python 3.5-3.11:
```
Error: gsutil requires Python version 2.7 or 3.5-3.11, but a different version is installed.
```
",copybara-service[bot],2024-11-18 23:05:53+00:00,['nitins17'],2024-11-19 22:27:55+00:00,2024-11-19 22:27:55+00:00,https://github.com/tensorflow/tensorflow/pull/80220,[],[],
2670208919,pull_request,closed,,[XLA:LatencyHidingScheduler] Code Refactor (NFC).,"[XLA:LatencyHidingScheduler] Code Refactor (NFC).
1. Use a more general way to define target specific resource enums instead of hard-coded offset.
2. Define template function to do Resource type conversion.
3. some other clean-ups.
",copybara-service[bot],2024-11-18 22:58:33+00:00,['Tongfei-Guo'],2024-12-03 01:15:33+00:00,2024-12-03 01:15:32+00:00,https://github.com/tensorflow/tensorflow/pull/80219,[],[],
2670180014,pull_request,closed,,Migrate LegalizeTensorListPass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate LegalizeTensorListPass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-18 22:44:23+00:00,['vamsimanchala'],2024-11-19 19:32:54+00:00,2024-11-19 19:32:53+00:00,https://github.com/tensorflow/tensorflow/pull/80218,[],[],
2670104654,pull_request,closed,,Refactor and repurpose the existing `fold_broadcast_to_pass` to handle ALL broadcast-like inputs on TFLite ops that support implicit broadcasting,"Refactor and repurpose the existing `fold_broadcast_to_pass` to handle ALL broadcast-like inputs on TFLite ops that support implicit broadcasting
",copybara-service[bot],2024-11-18 22:30:43+00:00,['vamsimanchala'],2024-11-19 18:15:23+00:00,2024-11-19 18:15:22+00:00,https://github.com/tensorflow/tensorflow/pull/80217,[],[],
2670061056,pull_request,open,,Integrate LLVM at llvm/llvm-project@6e1acdcdc1b3,"Integrate LLVM at llvm/llvm-project@6e1acdcdc1b3

Updates LLVM usage to match
[6e1acdcdc1b3](https://github.com/llvm/llvm-project/commit/6e1acdcdc1b3)
",copybara-service[bot],2024-11-18 22:04:17+00:00,[],2024-11-19 04:14:29+00:00,,https://github.com/tensorflow/tensorflow/pull/80216,[],[],
2670024393,pull_request,closed,,Separate authoritative vs Q-DQ DRR patterns.,"Separate authoritative vs Q-DQ DRR patterns.

Some patterns added the the quantize_patterns.td were making decisions about quantizing some weights that are not annotated by Q-DQ nodes. This PR separates these two categories for cases we want strict adherence to Q-DQ annotations (e.g. QAT).
",copybara-service[bot],2024-11-18 21:45:08+00:00,['majiddadashi'],2024-11-22 01:13:01+00:00,2024-11-22 01:13:00+00:00,https://github.com/tensorflow/tensorflow/pull/80215,[],[],
2670024106,pull_request,closed,,Replace MockGpuExecutor with MockStreamExecutor in the only use.,"Replace MockGpuExecutor with MockStreamExecutor in the only use.
",copybara-service[bot],2024-11-18 21:44:59+00:00,[],2024-11-19 18:02:45+00:00,2024-11-19 18:02:44+00:00,https://github.com/tensorflow/tensorflow/pull/80214,[],[],
2670014619,pull_request,closed,,Move StableHLO test to public XLA:CPU PJRT plugin,"Move StableHLO test to public XLA:CPU PJRT plugin
",copybara-service[bot],2024-11-18 21:40:13+00:00,['changm'],2024-11-20 02:25:05+00:00,2024-11-20 02:25:05+00:00,https://github.com/tensorflow/tensorflow/pull/80213,[],[],
2669907704,pull_request,closed,,Add ROUND to gpu_compatiblity,"Add ROUND to gpu_compatiblity
",copybara-service[bot],2024-11-18 21:03:17+00:00,['grantjensen'],2024-11-19 00:19:16+00:00,2024-11-19 00:19:15+00:00,https://github.com/tensorflow/tensorflow/pull/80212,[],[],
2669881660,pull_request,open,,In progress. Not ready for review.,"In progress. Not ready for review.
",copybara-service[bot],2024-11-18 20:50:02+00:00,[],2024-11-18 20:50:02+00:00,,https://github.com/tensorflow/tensorflow/pull/80211,[],[],
2669866218,pull_request,closed,,Prevent dequantizing/requantizing `f16` to `f32` and back.,"Prevent dequantizing/requantizing `f16` to `f32` and back.

What this change does is it:

 1. Identifies all `kTfLiteBuiltinDequantize` nodes converting `kTfLiteFloat16` to `kTfLiteFloat32` and plugging into a `kTfLiteBuiltinFullyConnected`, `kTfLiteBuiltinConv2d`, or `kTfLiteBuiltinDepthwiseConv2d` node.
 2. Re-maps XNNPACK tensors pointing to the `kTfLiteFloat32` output to point to the original `kTfLiteFloat16` input.

The `kTfLiteFloat16` weights/filters and biases are handled by XNNPACK directly.
",copybara-service[bot],2024-11-18 20:42:40+00:00,[],2024-11-20 16:32:11+00:00,2024-11-20 16:32:11+00:00,https://github.com/tensorflow/tensorflow/pull/80210,[],[],
2669816295,pull_request,closed,,Reverts d23ce258547e48d01c3f2e159fdb5bbb3354c89c,"Reverts d23ce258547e48d01c3f2e159fdb5bbb3354c89c
",copybara-service[bot],2024-11-18 20:32:35+00:00,[],2024-11-18 21:41:49+00:00,2024-11-18 21:41:48+00:00,https://github.com/tensorflow/tensorflow/pull/80209,[],[],
2669788890,pull_request,closed,,Import hashlib to avoid a linkage issue.,"Import hashlib to avoid a linkage issue.
",copybara-service[bot],2024-11-18 20:24:50+00:00,['chandrasekhard2'],2024-11-18 22:19:15+00:00,2024-11-18 22:19:14+00:00,https://github.com/tensorflow/tensorflow/pull/80208,[],[],
2669767062,pull_request,closed,,Migrate AnalyzeVariablesPass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate AnalyzeVariablesPass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-18 20:12:29+00:00,['vamsimanchala'],2024-11-18 22:48:18+00:00,2024-11-18 22:48:18+00:00,https://github.com/tensorflow/tensorflow/pull/80207,[],[],
2669749161,pull_request,open,,Fix APInt assertion failure,"Fix APInt assertion failure

We're using a signed one bit int to represent true/false, and the true value should be -1 instead of 1.

The checks got more strict with https://github.com/llvm/llvm-project/commit/3494ee95902cef62f767489802e469c58a13ea04
",copybara-service[bot],2024-11-18 20:05:21+00:00,[],2024-11-18 20:05:21+00:00,,https://github.com/tensorflow/tensorflow/pull/80206,[],[],
2669740690,pull_request,open,,Mess-up the API.,"Mess-up the API.
Add PJRT_Buffer_CopyRawToHost to PJRT C API.
",copybara-service[bot],2024-11-18 20:02:17+00:00,[],2024-11-18 20:02:17+00:00,,https://github.com/tensorflow/tensorflow/pull/80205,[],[],
2669739035,pull_request,closed,,Add tensor name member to LiteRtTensors and expose in public apis,"Add tensor name member to LiteRtTensors and expose in public apis
",copybara-service[bot],2024-11-18 20:01:40+00:00,['LukeBoyer'],2024-11-18 21:18:30+00:00,2024-11-18 21:18:29+00:00,https://github.com/tensorflow/tensorflow/pull/80204,[],[],
2669707960,pull_request,closed,,Change launch_id in HLO runner to be 1-based.,"Change launch_id in HLO runner to be 1-based.
",copybara-service[bot],2024-11-18 19:50:45+00:00,[],2024-11-18 21:06:18+00:00,2024-11-18 21:06:17+00:00,https://github.com/tensorflow/tensorflow/pull/80203,[],[],
2669682337,pull_request,closed,,Add backend kwargs to xla tests.,"Add backend kwargs to xla tests.
",copybara-service[bot],2024-11-18 19:39:59+00:00,[],2024-11-19 23:28:09+00:00,2024-11-19 23:28:08+00:00,https://github.com/tensorflow/tensorflow/pull/80202,[],[],
2669666298,pull_request,closed,,"r2.18 cherry-pick: 27ad610b6c7 ""Add deletion warning to tf.lite.interpreter with a redirection notice to ai-edge-litert.interpreter""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/27ad610b6c7242a69cee90e88f26d89fdbc5da0a,tensorflow-jenkins,2024-11-18 19:31:27+00:00,[],2024-11-18 21:28:17+00:00,2024-11-18 21:28:15+00:00,https://github.com/tensorflow/tensorflow/pull/80201,[],[],
2669583191,pull_request,closed,,Push xla test 'main' deps to leaf test targets,"Push xla test 'main' deps to leaf test targets
",copybara-service[bot],2024-11-18 19:07:45+00:00,[],2024-11-18 23:12:57+00:00,2024-11-18 23:12:56+00:00,https://github.com/tensorflow/tensorflow/pull/80200,[],[],
2669569371,pull_request,closed,,"Add batch tests to RemapArrays, and with different shapes.","Add batch tests to RemapArrays, and with different shapes.
",copybara-service[bot],2024-11-18 19:00:11+00:00,[],2024-11-22 01:44:56+00:00,2024-11-22 01:44:55+00:00,https://github.com/tensorflow/tensorflow/pull/80199,[],[],
2669558070,pull_request,closed,,[StableHLO] Add VhloDialect to the dependent dialects of vhlo-to-version,"[StableHLO] Add VhloDialect to the dependent dialects of vhlo-to-version
",copybara-service[bot],2024-11-18 18:55:47+00:00,['GleasonK'],2024-11-18 20:00:46+00:00,2024-11-18 20:00:45+00:00,https://github.com/tensorflow/tensorflow/pull/80198,[],[],
2669554575,pull_request,closed,,IFRT proxy optimization: Make more IFRT operations asynchronous.,"IFRT proxy optimization: Make more IFRT operations asynchronous.

As of this CL, all array operations (except `IsDeleted()`) are asynchronous.

This CL also makes the following drive-by changes:

1. Version management is getting refactored to use an enum and a header file
within /common.

2. All error responses from the server (except connection
terminations, which follow the previous behavior) are now printed out as a
WARNING.
",copybara-service[bot],2024-11-18 18:54:38+00:00,[],2024-11-20 21:36:47+00:00,2024-11-20 21:36:46+00:00,https://github.com/tensorflow/tensorflow/pull/80197,[],[],
2669454654,pull_request,closed,,Update gpu_test_kernels_fatbin to use runfiles,"Update gpu_test_kernels_fatbin to use runfiles
",copybara-service[bot],2024-11-18 18:25:10+00:00,[],2024-11-18 21:54:00+00:00,2024-11-18 21:53:59+00:00,https://github.com/tensorflow/tensorflow/pull/80196,[],[],
2669290300,pull_request,open,,Internal change only,"Internal change only
",copybara-service[bot],2024-11-18 17:23:37+00:00,[],2024-11-18 17:23:37+00:00,,https://github.com/tensorflow/tensorflow/pull/80195,[],[],
2669226059,pull_request,closed,,[XLA:GPU] Symbolic tiling: support bitcasts that reduce rank by more than one.,"[XLA:GPU] Symbolic tiling: support bitcasts that reduce rank by more than one.

The algorithm now supports indexing maps of the form `(d0) -> (d0 floordiv d mod m)`.
",copybara-service[bot],2024-11-18 16:55:58+00:00,['chsigg'],2024-12-04 20:07:47+00:00,2024-12-04 20:07:46+00:00,https://github.com/tensorflow/tensorflow/pull/80194,[],[],
2669080452,pull_request,closed,,[Triton] Restrict block_m to be > 16 in the GEMM autotuner to resolve CUDA_ERROR_ILLEGAL_ADDRESS in (micro)benchmarks with FP8 Triton kernels during exhaustive autotuning.,"[Triton] Restrict block_m to be > 16 in the GEMM autotuner to resolve CUDA_ERROR_ILLEGAL_ADDRESS in (micro)benchmarks with FP8 Triton kernels during exhaustive autotuning.
",copybara-service[bot],2024-11-18 16:10:29+00:00,[],2024-11-20 15:50:02+00:00,2024-11-20 15:50:01+00:00,https://github.com/tensorflow/tensorflow/pull/80193,[],[],
2669017487,pull_request,closed,,[XLA:CPU] Add benchmarks for 2D strided convolutions,"[XLA:CPU] Add benchmarks for 2D strided convolutions


Currently the transposed convolution is orders of magnitude slower than the regular one. Ideally performance should be similar. Detailed results:

----------------------------------------------------------------------------------
Benchmark                                        Time             CPU   Iterations
----------------------------------------------------------------------------------
BM_Conv2DStrided/process_time              3737222 ns     41608631 ns           16
BM_Conv2DTransposedStrided/process_time  590079914 ns   1.0847e+10 ns            1
",copybara-service[bot],2024-11-18 15:45:57+00:00,[],2024-11-20 19:48:28+00:00,2024-11-20 19:48:26+00:00,https://github.com/tensorflow/tensorflow/pull/80192,[],"[{'comment_id': 2483426021, 'issue_id': 2669017487, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80192/checks?check_run_id=33146802462) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 11, 18, 15, 46, 6, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-11-18 15:46:06 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80192/checks?check_run_id=33146802462) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2668557568,pull_request,closed,,PR #19432: [GPU] Sharded autotuning: exchange only results of the latest compilation.,"PR #19432: [GPU] Sharded autotuning: exchange only results of the latest compilation.

Imported from GitHub PR https://github.com/openxla/xla/pull/19432

Exchanging the cached results of previous compilation of other modules isn't necessary and leads to conflicts.
Copybara import of the project:

--
f85f07505b0b44c7ec59fb13f2fae172489e82e1 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Sharded autotuning: exchange only results of the latest compilation.

Exchanging the cached results of previous compilation of other modules
isn't necessary and leads to conflicts.

Merging this change closes #19432

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19432 from openxla:fix_sharded_autotuning_result_exchange f85f07505b0b44c7ec59fb13f2fae172489e82e1
",copybara-service[bot],2024-11-18 13:26:42+00:00,[],2024-11-19 11:54:31+00:00,2024-11-19 11:54:30+00:00,https://github.com/tensorflow/tensorflow/pull/80190,[],[],
2668045366,pull_request,closed,,PR #19210: [XLA:CPU] Modify matmul test cases with constant weights,"PR #19210: [XLA:CPU] Modify matmul test cases with constant weights

Imported from GitHub PR https://github.com/openxla/xla/pull/19210

On certain platforms, oneDNN may avoid blocking constant weights for performance reasons. Consequently, these weights remain as tensors of constant values that after further simplification, might get reduced to broadcasted scalar values, messing up the intended `MatchOptimizedHlo` pattern in tests. This PR updates the constant weights in the test file to ensure that even if oneDNN does not block the weights, subsequent simplification passes will not alter them.
Copybara import of the project:

--
1434c7a18dfa20209e4ca7aea2af882dfad7a31a by Akhil Goel <akhil.goel@intel.com>:

Change constant tensors

Merging this change closes #19210

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19210 from Intel-tensorflow:akhil/mmul_test 1434c7a18dfa20209e4ca7aea2af882dfad7a31a
",copybara-service[bot],2024-11-18 10:40:34+00:00,[],2024-11-19 11:09:13+00:00,2024-11-19 11:09:12+00:00,https://github.com/tensorflow/tensorflow/pull/80188,[],"[{'comment_id': 2482647790, 'issue_id': 2668045366, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80188/checks?check_run_id=33129921939) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 11, 18, 10, 40, 40, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-11-18 10:40:40 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80188/checks?check_run_id=33129921939) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2668024159,pull_request,closed,,[XLA:CPU] Support asynchronous execution for custom transposed convolutions,"[XLA:CPU] Support asynchronous execution for custom transposed convolutions


Performance is comparable to the synchronous version. Detailed results (where 'old' is the synchronous execution, 'new' is async execution; both use the same, custom algorithm for transposed conv):

name                                                     old cpu/op   new cpu/op   delta
BM_Conv1DStrided/process_time                            29.4ms ± 6%  29.7ms ± 5%    ~     (p=0.841 n=5+5)
BM_Conv1DTransposedStrided/process_time                  29.6ms ± 2%  30.7ms ± 2%  +3.52%  (p=0.008 n=5+5)
BM_Conv1DTransposedStridedNonDefaultLayout/process_time  28.5ms ± 3%  28.3ms ± 1%    ~     (p=0.222 n=5+5)

name                                                     old time/op  new time/op  delta
BM_Conv1DStrided/process_time                            2.68ms ± 7%  2.72ms ± 5%    ~     (p=0.548 n=5+5)
BM_Conv1DTransposedStrided/process_time                  7.91ms ± 3%  7.98ms ± 5%    ~     (p=0.548 n=5+5)
BM_Conv1DTransposedStridedNonDefaultLayout/process_time  7.00ms ± 2%  7.32ms ± 4%  +4.58%  (p=0.016 n=5+5)
",copybara-service[bot],2024-11-18 10:31:24+00:00,[],2024-11-22 21:02:53+00:00,2024-11-22 21:02:52+00:00,https://github.com/tensorflow/tensorflow/pull/80187,[],"[{'comment_id': 2482626289, 'issue_id': 2668024159, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80187/checks?check_run_id=33129443264) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 11, 18, 10, 31, 29, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-11-18 10:31:29 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/80187/checks?check_run_id=33129443264) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2667944100,pull_request,open,,PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR #18838: [NVIDIA GPU] Support multi-operand collective-permute

Imported from GitHub PR https://github.com/openxla/xla/pull/18838

For collective-permutes with small message sizes, it is beneficial to combine them into a single collective because
1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion;
2. fewer collectives make it easier for LHS to make better decision.

In order to support combining collective-permutes, we need to support multi-operand collective-permute first, a.k.a. the combined collective-permute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands.
Copybara import of the project:

--
5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun <tesun@nvidia.com>:

support multi-operand cp

--
170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun <tesun@nvidia.com>:

minor refactoring

--
0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun <tesun@nvidia.com>:

update python interface

--
9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun <tesun@nvidia.com>:

polish python interface

--
3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun <tesun@nvidia.com>:

formatting

--
d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun <tesun@nvidia.com>:

formatting

--
9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun <tesun@nvidia.com>:

refactor overloading

--
0aff5e0a372af9e4a859b54681acf0501adca096 by Terry Sun <tesun@nvidia.com>:

minor refactor

--
20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a by Terry Sun <tesun@nvidia.com>:

add parser test

Merging this change closes #18838

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp 20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a
",copybara-service[bot],2024-11-18 10:04:18+00:00,[],2024-11-27 10:04:17+00:00,,https://github.com/tensorflow/tensorflow/pull/80186,[],[],
2667582710,pull_request,closed,,PR #19372: [GPU] Consider small kInput fusions with concatenations in the horizontal loop fusion pass.,"PR #19372: [GPU] Consider small kInput fusions with concatenations in the horizontal loop fusion pass.

Imported from GitHub PR https://github.com/openxla/xla/pull/19372


Copybara import of the project:

--
9990047d0c36763d0e840a228aec546ce892d3f6 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Consider small kInput fusions with concatenations in the horizontal loop fusion pass.

Merging this change closes #19372

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19372 from openxla:horizontal_fusion_concat 9990047d0c36763d0e840a228aec546ce892d3f6
",copybara-service[bot],2024-11-18 08:08:38+00:00,[],2024-11-19 08:37:34+00:00,2024-11-19 08:37:34+00:00,https://github.com/tensorflow/tensorflow/pull/80185,[],[],
2667525934,pull_request,closed,,Add examples for tf.queue.FIFOQueue functions.,"Add code examples with outputs to the docstrings of all the functions in tf.queue.FIFOQueue. There seems to be some confusion on the usage based on the following [issue](https://github.com/tensorflow/tensorflow/issues/78829).

Functions covered:
 - __init__
 - enqueue
 - enqueue_many
 - dequeue
 - dequeue_many
 - dequeue_up_to
 - close
 - is_closed
 - size",SanjaySG,2024-11-18 07:42:25+00:00,['gbaned'],2024-12-27 03:11:31+00:00,2024-12-27 00:00:29+00:00,https://github.com/tensorflow/tensorflow/pull/80184,"[('awaiting review', 'Pull request awaiting review'), ('comp:ops', 'OPs related issues'), ('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2503360321, 'issue_id': 2667525934, 'author': 'keerthanakadiri', 'body': 'Hi @wangpengmit , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 27, 9, 27, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552953002, 'issue_id': 2667525934, 'author': 'SanjaySG', 'body': ""@wangpengmit, @cantonios Can you please review this PR?\r\nIf it's easier to review it through the internal Google fork in piper, I send a PR internally instead."", 'created_at': datetime.datetime(2024, 12, 19, 7, 19, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563268707, 'issue_id': 2667525934, 'author': 'SanjaySG', 'body': 'The code changes in this PR have been submitted through the internal Google repository and copied over by copybara-service.', 'created_at': datetime.datetime(2024, 12, 27, 3, 11, 30, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-11-27 09:27:14 UTC): Hi @wangpengmit , Can you please review this PR? Thank you !

SanjaySG (Issue Creator) on (2024-12-19 07:19:12 UTC): @wangpengmit, @cantonios Can you please review this PR?
If it's easier to review it through the internal Google fork in piper, I send a PR internally instead.

SanjaySG (Issue Creator) on (2024-12-27 03:11:30 UTC): The code changes in this PR have been submitted through the internal Google repository and copied over by copybara-service.

"
2667457597,pull_request,closed,,Integrate LLVM at llvm/llvm-project@64c455077abe,"Integrate LLVM at llvm/llvm-project@64c455077abe

Updates LLVM usage to match
[64c455077abe](https://github.com/llvm/llvm-project/commit/64c455077abe)
",copybara-service[bot],2024-11-18 07:27:39+00:00,['metaflow'],2024-11-18 18:16:18+00:00,2024-11-18 18:16:12+00:00,https://github.com/tensorflow/tensorflow/pull/80183,[],[],
2667319537,pull_request,open,,to be removed,"to be removed
",copybara-service[bot],2024-11-18 06:33:08+00:00,['changhuilin'],2024-11-18 06:33:09+00:00,,https://github.com/tensorflow/tensorflow/pull/80181,[],[],
2666407698,pull_request,closed,,IFRT proxy: hacky flag for faster `is_deleted()`.,"IFRT proxy: hacky flag for faster `is_deleted()`.

Setting the environment variable `IFRT_PROXY_ARRAY_IS_DELETED_HACK=enabled` in the OSS-compiled version of the IFRT proxy will make all `Array::IsDeleted()` calls to immediately return false.

This is not a generally safe optimization and trades off API correctness.

This acts as an interim solution for users while proper solutions are being debated or implemented.
",copybara-service[bot],2024-11-17 19:55:17+00:00,[],2024-11-18 01:30:55+00:00,2024-11-18 01:30:54+00:00,https://github.com/tensorflow/tensorflow/pull/80178,[],[],
2665278733,pull_request,closed,,[XLA:GPU][IndexAnalysis] Update documentation for indexing maps.,"[XLA:GPU][IndexAnalysis] Update documentation for indexing maps.
",copybara-service[bot],2024-11-17 04:58:18+00:00,['pifon2a'],2024-11-18 22:31:41+00:00,2024-11-18 22:31:39+00:00,https://github.com/tensorflow/tensorflow/pull/80176,[],[],
2665219792,pull_request,closed,,Style improvement: use a common method to create ArraySpec,"Style improvement: use a common method to create ArraySpec
",copybara-service[bot],2024-11-17 03:53:38+00:00,[],2024-11-18 19:18:01+00:00,2024-11-18 19:18:00+00:00,https://github.com/tensorflow/tensorflow/pull/80175,[],[],
2665032890,pull_request,closed,,Adds a sharding config to XLA's HloModuleConfig (as part of AutoFDO integration).,"Adds a sharding config to XLA's HloModuleConfig (as part of AutoFDO integration).
",copybara-service[bot],2024-11-16 22:36:07+00:00,[],2024-11-26 01:52:56+00:00,2024-11-26 01:52:55+00:00,https://github.com/tensorflow/tensorflow/pull/80174,[],[],
2664351691,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-16 13:33:55+00:00,[],2024-11-16 15:13:05+00:00,2024-11-16 15:13:05+00:00,https://github.com/tensorflow/tensorflow/pull/80166,[],[],
2663876543,pull_request,closed,,Move BatchedGatherScatterNormalizer from pre-SPMD for pose-SPMD.,"Move BatchedGatherScatterNormalizer from pre-SPMD for pose-SPMD.
",copybara-service[bot],2024-11-16 06:27:58+00:00,[],2024-11-23 06:44:35+00:00,2024-11-23 06:44:34+00:00,https://github.com/tensorflow/tensorflow/pull/80165,[],[],
2663873820,pull_request,open,,Fix issues on explicit batch dimensions in xla sharding propagation and spmd partitioner.,"Fix issues on explicit batch dimensions in xla sharding propagation and spmd partitioner.

Explicit batch dimensions and index parallel dimensions cannot co-exist. Namely, if a gather/scatter has explicit batch dimensions, we do not consider index parallel dimensions in it.

The default and prioritized method to partition gather/scatter is now processing explicit batch dims, while it was index parallel dims before this cl.
",copybara-service[bot],2024-11-16 06:20:00+00:00,[],2024-11-18 22:09:24+00:00,,https://github.com/tensorflow/tensorflow/pull/80164,[],[],
2663751767,pull_request,closed,,Add LiteRT CompiledModel API,"Add LiteRT CompiledModel API

The CompiledModel is a higher level inference API.  It is created by
provided model with compilation options. Internally, it instantiates runtime
and applies Delegates mapped to the complication options.
It also supports getting BufferRequirements to create input/output
TensorBuffers, and it allows to invoke the model with the input/output
TensorBuffers.
",copybara-service[bot],2024-11-16 04:41:11+00:00,['terryheo'],2024-12-05 20:04:19+00:00,2024-12-05 20:04:19+00:00,https://github.com/tensorflow/tensorflow/pull/80163,[],[],
2663631767,pull_request,closed,,Clean up disabling reduce_hlo_test on TPU,"Clean up disabling reduce_hlo_test on TPU
",copybara-service[bot],2024-11-16 02:16:05+00:00,[],2024-11-22 12:33:00+00:00,2024-11-22 12:32:58+00:00,https://github.com/tensorflow/tensorflow/pull/80162,[],[],
2663628697,pull_request,closed,,Disable scatter_determinism_expander as it causes failure on internal test.,"Disable scatter_determinism_expander as it causes failure on internal test.
",copybara-service[bot],2024-11-16 02:10:14+00:00,['reedwm'],2024-11-16 03:07:45+00:00,2024-11-16 03:07:45+00:00,https://github.com/tensorflow/tensorflow/pull/80161,[],[],
2663580366,pull_request,closed,,Fixed a bug in model type instrumentation.,"Fixed a bug in model type instrumentation.
",copybara-service[bot],2024-11-16 01:23:09+00:00,[],2024-11-18 23:25:44+00:00,2024-11-18 23:25:43+00:00,https://github.com/tensorflow/tensorflow/pull/80160,[],[],
2663569462,pull_request,open,,Integrate LLVM at llvm/llvm-project@64c455077abe,"Integrate LLVM at llvm/llvm-project@64c455077abe

Updates LLVM usage to match
[64c455077abe](https://github.com/llvm/llvm-project/commit/64c455077abe)
",copybara-service[bot],2024-11-16 01:04:19+00:00,[],2024-11-16 01:04:19+00:00,,https://github.com/tensorflow/tensorflow/pull/80159,[],[],
2663567884,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2024-11-16 01:01:13+00:00,[],2024-11-19 20:51:50+00:00,2024-11-19 20:51:48+00:00,https://github.com/tensorflow/tensorflow/pull/80158,[],[],
2663549552,pull_request,closed,,"Refactor TF wheel build rule, common python rules and flag names.","Refactor TF wheel build rule, common python rules and flag names.

For Linux platforms the build rule generates the `auditwheel show` log in the output regardless of compliance check flag.
",copybara-service[bot],2024-11-16 00:29:47+00:00,[],2024-11-19 17:55:20+00:00,2024-11-19 17:55:20+00:00,https://github.com/tensorflow/tensorflow/pull/80157,[],[],
2663544808,pull_request,open,,Add GPU topology proto python target.,"Add GPU topology proto python target.
",copybara-service[bot],2024-11-16 00:24:31+00:00,['changhuilin'],2024-11-16 00:24:32+00:00,,https://github.com/tensorflow/tensorflow/pull/80156,[],[],
2663529289,pull_request,closed,,[XLA:GPU] remove channel ID checks in hlo_instructions.cc,"[XLA:GPU] remove channel ID checks in hlo_instructions.cc
",copybara-service[bot],2024-11-16 00:06:48+00:00,[],2024-11-22 20:34:11+00:00,2024-11-22 20:34:11+00:00,https://github.com/tensorflow/tensorflow/pull/80155,[],[],
2663504783,pull_request,closed,,IFRT proxy: asynchronous and faster `MakeArrayFromHostBuffer`,"IFRT proxy: asynchronous and faster `MakeArrayFromHostBuffer`

Note: I use the term 'control path' to refer to everything except `HostBufferStore.Store()` and `HostBufferStore.Lookup()` operations.

This CL improves performance with the following changes:
- The client manufactures array handles for `MakeArrayFromHostBufferRequest` (instead of the server generating them) and returns to the caller immediately after sending the request. Since ordering is maintained for control path requests across the proxy, future operations on the array do not require any special handling.
- The data-path `HostBufferStoreRequest` that corresponds to a `MakeArrayFromHostBufferRequest` is not ordered by the client (before or after) the `MakeArrayFromHostBufferRequest`. On the server-side, the loop that handles control path requests, when it sees a `MakeArrayFromHostBufferRequest`, blocks until the corresponding `HostBufferStoreRequest` is processed.
- The data-path (`HostBufferStore` implementation) and control path now use different gRPC channels.
- Resulting performance: BM_HostToDeviceAsync/1M/2k results in more than 3 GB/s, making it bottlenecked by gRPC `stream.Write()` latency. BM_HostToDeviceAsync/1K/98k (~4MB/s) was already bottlenecked by gRPC `stream.Write()` latency.

This CL also:
- Adds more XProf tracemes
- Introduces `global_flags.h` and `global_flags_google.cc` so we can conveniently use command-line flags in the proxy-client. This may not be ideal from a clean-code perspective, but makes it much easier to develop and debug the client.
",copybara-service[bot],2024-11-15 23:51:35+00:00,[],2024-11-17 19:30:43+00:00,2024-11-17 19:30:42+00:00,https://github.com/tensorflow/tensorflow/pull/80154,[],[],
2663441116,pull_request,closed,,Refactor GetModelBufWithByteCode() to use it from core logic,"Refactor GetModelBufWithByteCode() to use it from core logic
",copybara-service[bot],2024-11-15 23:26:52+00:00,['terryheo'],2024-11-18 19:26:05+00:00,2024-11-18 19:26:03+00:00,https://github.com/tensorflow/tensorflow/pull/80153,[],[],
2663428364,pull_request,open,,Reverts ee3517fc51ca93510b840914839c48cc6b7e26b6,"Reverts ee3517fc51ca93510b840914839c48cc6b7e26b6
",copybara-service[bot],2024-11-15 23:09:03+00:00,[],2024-11-15 23:09:03+00:00,,https://github.com/tensorflow/tensorflow/pull/80152,[],[],
2663427049,pull_request,closed,,Fix APInt assertion failure,"Fix APInt assertion failure

We're using a signed one bit int to represent true/false, and the true value should be -1 instead of 1.

The checks got more strict with https://github.com/llvm/llvm-project/commit/3494ee95902cef62f767489802e469c58a13ea04
",copybara-service[bot],2024-11-15 23:07:14+00:00,[],2024-11-18 19:36:44+00:00,2024-11-18 19:36:43+00:00,https://github.com/tensorflow/tensorflow/pull/80151,[],[],
2663421355,pull_request,closed,,Fix APInt assertion failure,"Fix APInt assertion failure

We're using a signed one bit int to represent true/false, and the true value should be -1 instead of 1.

The checks got more strict with https://github.com/llvm/llvm-project/commit/3494ee95902cef62f767489802e469c58a13ea04
",copybara-service[bot],2024-11-15 23:00:44+00:00,[],2024-11-18 20:18:08+00:00,2024-11-18 20:18:08+00:00,https://github.com/tensorflow/tensorflow/pull/80150,[],[],
2663418687,pull_request,closed,,[xla:cpu] Add initial implementation of NanoRt backends for XLA:CPU,"[xla:cpu] Add initial implementation of NanoRt backends for XLA:CPU

Minimal XLA:CPU runtime implementation optimized for low latency inference.

--------------------------------------------------------------
Benchmark                    Time             CPU   Iterations
--------------------------------------------------------------
BM_NanoRtAddScalars       84.8 ns         84.8 ns      8277118
BM_NanoRtFibonacci        81.1 ns         81.1 ns      8468298
BM_PjRtAddScalars         1517 ns         1517 ns       460076
BM_PjRtFibonacci          1523 ns         1523 ns       460415
",copybara-service[bot],2024-11-15 22:58:40+00:00,['ezhulenev'],2024-11-20 18:08:23+00:00,2024-11-20 18:08:21+00:00,https://github.com/tensorflow/tensorflow/pull/80149,[],[],
2663418080,pull_request,open,,Use default QNN graph when a function name is not specified,"Use default QNN graph when a function name is not specified

If the function_name is not specified and there is only one graph, then we take that graph for execution
",copybara-service[bot],2024-11-15 22:58:09+00:00,[],2024-11-15 22:58:09+00:00,,https://github.com/tensorflow/tensorflow/pull/80148,[],[],
2663411285,pull_request,closed,,Bring down_cast function to ::tsl namespace,"Bring down_cast function to ::tsl namespace

We don't want to use tensorflow namespace inside xla/tsl
",copybara-service[bot],2024-11-15 22:52:27+00:00,['ezhulenev'],2024-11-16 02:48:52+00:00,2024-11-16 02:48:49+00:00,https://github.com/tensorflow/tensorflow/pull/80147,[],[],
2663393760,pull_request,closed,,Replace uses of absl::StatusOr with litert::Expected,"Replace uses of absl::StatusOr with litert::Expected
",copybara-service[bot],2024-11-15 22:44:40+00:00,[],2024-11-16 00:42:14+00:00,2024-11-16 00:42:13+00:00,https://github.com/tensorflow/tensorflow/pull/80146,[],[],
2663306132,pull_request,open,,Fuse splat constants into binary ops without fused activation.,"Fuse splat constants into binary ops without fused activation.

This CL adds patterns to fuse splat constants into binary ops without fused activation. This is useful for reducing the number of ops and the size of the model.
",copybara-service[bot],2024-11-15 22:13:04+00:00,['vamsimanchala'],2024-11-15 22:13:05+00:00,,https://github.com/tensorflow/tensorflow/pull/80145,[],[],
2663262094,pull_request,closed,,[tsl:concurrency] Add CountDownAsyncValueRef to concurrency library,"[tsl:concurrency] Add CountDownAsyncValueRef to concurrency library

This idiom is used in a few places in XLA:CPU executor, and TPU runtime.

Conceptually it's similar to https://en.cppreference.com/w/cpp/thread/latch, but for async values.

BM_CountDownSuccess/4        31.4 ns         31.4 ns     19308520
BM_CountDownSuccess/8        37.1 ns         37.0 ns     18911191
BM_CountDownSuccess/16       57.3 ns         57.3 ns     12371255
BM_CountDownSuccess/32       90.1 ns         90.0 ns      7746279
BM_CountDownError/4          75.8 ns         75.8 ns      8899579
BM_CountDownError/8           110 ns          110 ns      6349328
BM_CountDownError/16          171 ns          171 ns      4068219
BM_CountDownError/32          290 ns          290 ns      2402659
",copybara-service[bot],2024-11-15 21:49:36+00:00,['ezhulenev'],2024-11-18 23:00:49+00:00,2024-11-18 23:00:49+00:00,https://github.com/tensorflow/tensorflow/pull/80144,[],[],
2663260376,pull_request,closed,,PR #19313: [nfc] Cleaning up dynamic slice fusion,"PR #19313: [nfc] Cleaning up dynamic slice fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/19313

Offset arrays and loop iteration offset are no longer handled in `xla/service/gpu/fusions/custom.cc`. Removing that to simplify the code. We are working on better handling with runtime computation of offset on the host.
Copybara import of the project:

--
3dc7d8a9d274693e434040331cfe25d27566973d by Shraiysh Vaishay <svaishay@nvidia.com>:

[nfc] Cleaning up dynamic slice fusion

Offset arrays and loop iteration offset are no longer
handled in `xla/service/gpu/fusions/custom.cc`. Removing that to
simplify the code. We are working on better handling with runtime
computation of offset on the host.

Merging this change closes #19313

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19313 from shraiysh:cleanup-dynamic-slice-fusion 3dc7d8a9d274693e434040331cfe25d27566973d
",copybara-service[bot],2024-11-15 21:48:47+00:00,[],2024-11-18 19:45:14+00:00,2024-11-18 19:45:13+00:00,https://github.com/tensorflow/tensorflow/pull/80143,[],[],
2663109465,pull_request,closed,,Move new field to end of struct PJRT_Api and update struct size accordingly.,"Move new field to end of struct PJRT_Api and update struct size accordingly.
",copybara-service[bot],2024-11-15 20:47:55+00:00,[],2024-11-16 06:01:44+00:00,2024-11-16 06:01:44+00:00,https://github.com/tensorflow/tensorflow/pull/80142,[],[],
2663033055,pull_request,open,,Fuses splat constants into select op.,"Fuses splat constants into select op.
",copybara-service[bot],2024-11-15 20:25:20+00:00,['vamsimanchala'],2024-11-15 20:25:21+00:00,,https://github.com/tensorflow/tensorflow/pull/80141,[],[],
2663014895,pull_request,closed,,Support explicit batch dimensions for gather/scatter in HLO evaluator.,"Support explicit batch dimensions for gather/scatter in HLO evaluator.

Explicit batch dims reserves in all the involved tensors for gather/scatter operations.
",copybara-service[bot],2024-11-15 20:12:06+00:00,[],2024-11-18 20:11:34+00:00,2024-11-18 20:11:33+00:00,https://github.com/tensorflow/tensorflow/pull/80140,[],[],
2663000954,pull_request,closed,,Add deletion warning to tf.lite.interpreter with a redirection notice to ai-edge-litert.interpreter,"Add deletion warning to tf.lite.interpreter with a redirection notice to ai-edge-litert.interpreter
",copybara-service[bot],2024-11-15 20:02:25+00:00,['pak-laura'],2024-11-15 21:17:36+00:00,2024-11-15 21:17:35+00:00,https://github.com/tensorflow/tensorflow/pull/80139,[],[],
2662990892,pull_request,closed,,Move `GetStartIndicesDimsToOutputDims` to `gather_scatter_utils.h`,"Move `GetStartIndicesDimsToOutputDims` to `gather_scatter_utils.h`

We will use this function in both algebraic_simplifier and hlo_evaluator. This change only moves util function without behavior change.
",copybara-service[bot],2024-11-15 19:56:40+00:00,[],2024-11-16 01:19:26+00:00,2024-11-16 01:19:25+00:00,https://github.com/tensorflow/tensorflow/pull/80138,[],[],
2662942892,pull_request,closed,,Migrate OptimizeBatchMatmulPass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate OptimizeBatchMatmulPass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-15 19:28:25+00:00,['vamsimanchala'],2024-11-18 20:43:47+00:00,2024-11-18 20:43:45+00:00,https://github.com/tensorflow/tensorflow/pull/80137,[],[],
2662941100,pull_request,closed,,Add new workaround target due to https://github.com/bazelbuild/bazel/issues/21519,"Add new workaround target due to https://github.com/bazelbuild/bazel/issues/21519

Previously, the only failure case was XLA depending on TSL header accessed via `filegroup` or `exports_files`. The new target stops the reverse where TSL depends on XLA. We can delete this post TSL move.
",copybara-service[bot],2024-11-15 19:27:11+00:00,['ddunl'],2024-11-15 22:00:27+00:00,2024-11-15 22:00:26+00:00,https://github.com/tensorflow/tensorflow/pull/80136,[],[],
2662931937,pull_request,closed,,Temporarily exclude `xla/tsl` from buildifier checks,"Temporarily exclude `xla/tsl` from buildifier checks
",copybara-service[bot],2024-11-15 19:25:04+00:00,['ddunl'],2024-11-15 21:31:51+00:00,2024-11-15 21:31:51+00:00,https://github.com/tensorflow/tensorflow/pull/80135,[],[],
2662901500,pull_request,closed,,[IFRT] Ensure that VIFRT td file structure matches that of IFRT dialect.,"[IFRT] Ensure that VIFRT td file structure matches that of IFRT dialect.
",copybara-service[bot],2024-11-15 19:22:03+00:00,[],2024-11-15 20:25:53+00:00,2024-11-15 20:25:52+00:00,https://github.com/tensorflow/tensorflow/pull/80134,[],[],
2662884474,pull_request,closed,,Fix integer overflow in range,"Fixes #64081

I modified the fixes from the rolled back PR's #77018 and #76252 to resolve an overflow that was identified by one of the new test cases. The issue was that in `ragged_range_op.cc`, `value` is incremented at the end of the loop logic, which results in a needless addition on the final iteration and creates the possibility for an overflow. This had no impact on the output of the operation and did not cause any of the tests to fail, but it was picked up by ASan.

https://github.com/tensorflow/tensorflow/blob/ec9e1531f0114662baef0f3985de5620b43d0d29/tensorflow/core/kernels/ragged_range_op.cc#L123-L126

I added a condition that only increments the value if there are more iterations to be completed.",mattbahr,2024-11-15 19:10:24+00:00,['gbaned'],2024-12-10 18:25:43+00:00,2024-12-10 17:20:53+00:00,https://github.com/tensorflow/tensorflow/pull/80133,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:M', 'CL Change Size: Medium'), ('prtype:bugfix', 'PR to fix a bug'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2514787404, 'issue_id': 2662884474, 'author': 'mattbahr', 'body': '@cantonios When you get the chance, would you mind taking a look at this review? Thanks!', 'created_at': datetime.datetime(2024, 12, 3, 14, 50, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529113734, 'issue_id': 2662884474, 'author': 'mattbahr', 'body': '@cantonios Leaving the size unsigned can cause a signed integer overflow when it is passed to the AddDimWithStatus function.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/kernels/sequence_ops.cc#L109\r\n\r\nThis function expects type int64_t, and for a very large size will interpret it as negative.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/framework/tensor_shape.cc#L427-L433', 'created_at': datetime.datetime(2024, 12, 9, 18, 59, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531946582, 'issue_id': 2662884474, 'author': 'mattbahr', 'body': '@cantonios @mihaimaruseac Looks like the tests at //tensorflow/python/kernel_tests:collective_ops_multi_worker_test are timing out.', 'created_at': datetime.datetime(2024, 12, 10, 15, 6, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532379679, 'issue_id': 2662884474, 'author': 'cantonios', 'body': '> @cantonios @mihaimaruseac Looks like the tests at //tensorflow/python/kernel_tests:collective_ops_multi_worker_test are timing out.\r\n\r\nMerged.  Thanks!', 'created_at': datetime.datetime(2024, 12, 10, 17, 38, 42, tzinfo=datetime.timezone.utc)}]","mattbahr (Issue Creator) on (2024-12-03 14:50:40 UTC): @cantonios When you get the chance, would you mind taking a look at this review? Thanks!

mattbahr (Issue Creator) on (2024-12-09 18:59:24 UTC): @cantonios Leaving the size unsigned can cause a signed integer overflow when it is passed to the AddDimWithStatus function.

https://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/kernels/sequence_ops.cc#L109

This function expects type int64_t, and for a very large size will interpret it as negative.

https://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/framework/tensor_shape.cc#L427-L433

mattbahr (Issue Creator) on (2024-12-10 15:06:47 UTC): @cantonios @mihaimaruseac Looks like the tests at //tensorflow/python/kernel_tests:collective_ops_multi_worker_test are timing out.

cantonios on (2024-12-10 17:38:42 UTC): Merged.  Thanks!

"
2662856114,pull_request,closed,,Add a moduleop to the MlirToHloArgs and enable compilation without serializing any modules.,"Add a moduleop to the MlirToHloArgs and enable compilation without serializing any modules.

Also pulled the deserialization a little further up the stack and only do it if the input doesn't already have a full module op.
",copybara-service[bot],2024-11-15 18:53:07+00:00,[],2024-11-19 21:10:02+00:00,2024-11-19 21:10:00+00:00,https://github.com/tensorflow/tensorflow/pull/80132,[],[],
2662845631,pull_request,open,,remove an unnecessary local variable in GpuDelegateNative.java,"remove an unnecessary local variable in GpuDelegateNative.java

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19313 from shraiysh:cleanup-dynamic-slice-fusion 3dc7d8a9d274693e434040331cfe25d27566973d
",copybara-service[bot],2024-11-15 18:47:58+00:00,[],2024-11-18 19:49:23+00:00,,https://github.com/tensorflow/tensorflow/pull/80131,[],[],
2662822079,pull_request,open,,[PJRT C API] Move PJRT_Buffer_CopyRawToHost to end of PjRT API struct. Update struct size accordingly.,"[PJRT C API] Move PJRT_Buffer_CopyRawToHost to end of PjRT API struct. Update struct size accordingly.
",copybara-service[bot],2024-11-15 18:45:08+00:00,[],2024-11-15 18:45:08+00:00,,https://github.com/tensorflow/tensorflow/pull/80130,[],[],
2662796419,pull_request,open,,Move IFRT lib to XLA:CPU Public API,"Move IFRT lib to XLA:CPU Public API
",copybara-service[bot],2024-11-15 18:37:08+00:00,['changm'],2024-11-15 18:37:09+00:00,,https://github.com/tensorflow/tensorflow/pull/80129,[],[],
2662447106,pull_request,open,,Add a pass to optimize explicit broadcasting-like patterns.,"Add a pass to optimize explicit broadcasting-like patterns.

This pass is to optimize explicit broadcasting-like patterns, because TFLite Ops support implicit broadcasting.
",copybara-service[bot],2024-11-15 16:08:06+00:00,['vamsimanchala'],2024-11-15 20:14:52+00:00,,https://github.com/tensorflow/tensorflow/pull/80128,[],[],
2662420996,pull_request,closed,,Load the builtin Bazel java rules from @rules_java,"Load the builtin Bazel java rules from @rules_java
",copybara-service[bot],2024-11-15 15:55:40+00:00,[],2024-11-16 05:06:48+00:00,2024-11-16 05:06:48+00:00,https://github.com/tensorflow/tensorflow/pull/80127,[],[],
2662396934,pull_request,closed,,#sdy unskip test and change to correct result now that we resolve real conflicts.,"#sdy unskip test and change to correct result now that we resolve real conflicts.
",copybara-service[bot],2024-11-15 15:45:10+00:00,[],2024-11-15 19:25:08+00:00,2024-11-15 19:25:07+00:00,https://github.com/tensorflow/tensorflow/pull/80126,[],[],
2662257039,pull_request,closed,,Move IFRT lib to XLA:CPU Public API,"Move IFRT lib to XLA:CPU Public API
",copybara-service[bot],2024-11-15 15:07:48+00:00,['changm'],2024-11-15 18:35:47+00:00,2024-11-15 18:35:46+00:00,https://github.com/tensorflow/tensorflow/pull/80125,[],[],
2662247671,pull_request,closed,,Move IFRT integration tests to public XLA:CPU API,"Move IFRT integration tests to public XLA:CPU API
",copybara-service[bot],2024-11-15 15:02:57+00:00,['changm'],2024-11-15 17:14:42+00:00,2024-11-15 17:14:42+00:00,https://github.com/tensorflow/tensorflow/pull/80124,[],[],
2662205987,pull_request,closed,,Reenable nvjitlink by default with implib inplace,"Reenable nvjitlink by default with implib inplace
",copybara-service[bot],2024-11-15 14:44:45+00:00,[],2024-11-18 18:22:38+00:00,2024-11-18 18:22:36+00:00,https://github.com/tensorflow/tensorflow/pull/80123,[],[],
2661995545,pull_request,closed,,[XLA:GPU] Use namespace aliases for mlir::mhlo and mlir::arith in triton_fusion_emitter_legacy_matmul.cc,"[XLA:GPU] Use namespace aliases for mlir::mhlo and mlir::arith in triton_fusion_emitter_legacy_matmul.cc
",copybara-service[bot],2024-11-15 13:34:46+00:00,[],2024-11-15 16:17:14+00:00,2024-11-15 16:17:12+00:00,https://github.com/tensorflow/tensorflow/pull/80122,[],[],
2661782861,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-15 11:57:15+00:00,[],2024-11-15 11:57:15+00:00,,https://github.com/tensorflow/tensorflow/pull/80121,[],[],
2661732004,pull_request,open,,Modernize API for cuda_asm_compiler functions,"Modernize API for cuda_asm_compiler functions

- Replace `int cc_major, int cc_minor` arguments by `CudaComputeCapability`
- Replace `const char* ptx` arguments by `std::string ptx`.
- Remove overload of `CompileGpuAsm` that takes a `StreamExecutor` in favor of the version taking a `CudaComputeCapability`
- Remove the `ActivateContext` call in `LinkGpuAsm` since cuLink* CUDA driver functions don't need an active context
",copybara-service[bot],2024-11-15 11:31:42+00:00,[],2024-11-15 11:31:42+00:00,,https://github.com/tensorflow/tensorflow/pull/80120,[],[],
2661722079,pull_request,closed,,[XLA:GPU] Fix the dot algorithm rewriter for infinity and NaN cases.,"[XLA:GPU] Fix the dot algorithm rewriter for infinity and NaN cases.

The dot algorithm rewriter was not handling infinity and NaN correctly. This CL fixes the issue by masking out the NaN values. The error was affecting the dots with BF16_BF16_F32_X3 and BF16_BF16_F32_X6 that were rewritten for cublas.

Now we test the algorithms against the plain regular dot without the specific algorithm being set.

Example:

lhs = +infinity

rhs = 1.0
rhs_high = 1.0
rhs_low = 0.0

low_dot = +infinity * 0.0 = NaN
high_dot = +infinity * 1.0 = +infinity

sum = low_dot + high_dot = +infinity + NaN = NaN

but the expected result is +infinity
",copybara-service[bot],2024-11-15 11:28:39+00:00,[],2024-11-18 18:30:15+00:00,2024-11-18 18:30:12+00:00,https://github.com/tensorflow/tensorflow/pull/80119,[],[],
2661608935,pull_request,closed,,upgrade rules_android_ndk,"This patch upgrades rules_android_ndk to v0.1.2 to support NDK 27

NDK 27 is required to enable xnn_enable_avxvnniint8 in android_x86_64 build.

Test done: benchmark build and run.",soniravi31,2024-11-15 10:46:38+00:00,['gbaned'],2024-11-26 06:34:39+00:00,2024-11-26 06:34:39+00:00,https://github.com/tensorflow/tensorflow/pull/80118,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]",[],
2661548114,pull_request,closed,,PR #18825: [GPU] GEMM fusions: let fusing effective parameters and their broadcasts in the epilogues.,"PR #18825: [GPU] GEMM fusions: let fusing effective parameters and their broadcasts in the epilogues.

Imported from GitHub PR https://github.com/openxla/xla/pull/18825


Copybara import of the project:

--
37dc0d2f706bf681b1eff2088eb8d1000abf79b8 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] GEMM fusions: let fusing effective parameters and their broadcasts in the epilogues.

Merging this change closes #18825

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18825 from openxla:gemm_fusion_effective_parameters 37dc0d2f706bf681b1eff2088eb8d1000abf79b8
",copybara-service[bot],2024-11-15 10:34:04+00:00,[],2024-11-15 20:32:23+00:00,2024-11-15 20:32:22+00:00,https://github.com/tensorflow/tensorflow/pull/80117,[],[],
2661511820,pull_request,closed,,[XLA] Fix a race in `SlowOperationAlarm` and add a test,"[XLA] Fix a race in `SlowOperationAlarm` and add a test

Before this change, the alarm would execute the provided msg_fn in two cases:
- when the alarm fires, protected by a mutex.
- when the alarm is destroyed, not protected by a mutex.

This posed two problems:
- The call to `msg_fn` within the destructor is not synchronized and may causes a race condition.
- It's a bit surprising that `msg_fn` could be called twice, especially in the destructor. It's even more surprising that `msg_fn` could be called even after an alarm has been explicitly cancelled. This is because the condition for calling for the function in the destructor was only based on time.

This change tweakes the behavior:
- `msg_fn` is now called at most once. This happens within the mutex-protected region in the alarm loop.
- The result of `msg_fn` is cached and used again in the destructor if needed.
- The destructor now only prints a message if the alarm has fired. This ensures that if an alarm was cancelled before it ever fired, its destructor will not print anything.

Also added tests that fail before this change.
",copybara-service[bot],2024-11-15 10:14:43+00:00,[],2024-11-15 15:46:44+00:00,2024-11-15 15:46:40+00:00,https://github.com/tensorflow/tensorflow/pull/80113,[],[],
2661326388,pull_request,closed,,Integrate LLVM at llvm/llvm-project@b3134fa23383,"Integrate LLVM at llvm/llvm-project@b3134fa23383

Updates LLVM usage to match
[b3134fa23383](https://github.com/llvm/llvm-project/commit/b3134fa23383)
",copybara-service[bot],2024-11-15 09:10:26+00:00,[],2024-11-15 21:06:04+00:00,2024-11-15 21:06:03+00:00,https://github.com/tensorflow/tensorflow/pull/80095,[],[],
2661309920,pull_request,closed,,PR #19275: [NVIDIA] Add fixes for supporting determinism expander for high-dimensional scatter operation and a flag to disable it,"PR #19275: [NVIDIA] Add fixes for supporting determinism expander for high-dimensional scatter operation and a flag to disable it

Imported from GitHub PR https://github.com/openxla/xla/pull/19275

This PR is the 2nd step (out of 2) to improve the performance of deterministic scatter. Originally, the scatter op will be expanded to be deterministic in xla/service/ScatterExpander.cc. However, since it took a while-loop-based approach, the performance is extremely poor. We designed and implemented a prefix-scan-based approach to rewrite the scatter operation to be an efficient deterministic scatter. This PR completes the optimization of deterministic scatter operations with non-scalar indices and updates.

The change of this PR is on top of https://github.com/openxla/xla/pull/17886, and has fixed issues reported in the reverted PR https://github.com/openxla/xla/pull/18326. The issue was that the changes in https://github.com/openxla/xla/pull/18326 were not able to handle different kinds of complicated but realistic scatter dimension numbers. Specifically, this PR unifies the implementation of 1D and multi-dimensional scatter operation to make the code easier to maintain, adds multiple tests for various scatter dimension numbers, and thoroughly handles all cases of different kinds of dimension numbers. 

Moreover, this PR also adds an option `xla_gpu_enable_scatter_determinism_expander`, the default value of which is set to be true. This option could make sure that although unlikely, if anything happens with changes in this PR, the user can easily disable  the `scatter_determinism_expander` pass without getting blocked.


Design doc: https://docs.google.com/document/d/1K204VZR3OP0SUDOPsGUYgIIDf2ucTKEC4yQj8XRG2SA/edit

Bugs resolved: https://github.com/jax-ml/jax/issues/17844
Copybara import of the project:

--
3b7b56a2b95e52654daf83a359d17a809dc3b784 by Chenhao Jiang <chenhaoj@nvidia.com>:

PR #18326: [NVIDIA] Complete the optimization of deterministic scatter operations

Imported from GitHub PR https://github.com/openxla/xla/pull/18326

This PR is the 2nd step (out of 2) to improve the performance of deterministic scatter. Originally, the scatter op will be expanded to be deterministic in xla/service/ScatterExpander.cc. However, since it took a while-loop-based approach, the performance is extremely poor. We designed and implemented a prefix-scan-based approach to rewrite the scatter operation to be an efficient deterministic scatter. This PR completes the optimization of deterministic scatter operations with non-scalar indices and updates.

The change of this PR is on top of https://github.com/openxla/xla/pull/17886

Design doc: https://docs.google.com/document/d/1K204VZR3OP0SUDOPsGUYgIIDf2ucTKEC4yQj8XRG2SA/edit

Bugs resolved: https://github.com/jax-ml/jax/issues/17844
Copybara import of the project:

--
de647d44eb28af71e1580b6e8ed9adc751e50f52 by Chenhao Jiang <chenhaoj@nvidia.com>:

Support scatter with non-scalar indices and updates

Merging this change closes #18326

PiperOrigin-RevId: 691023328

--
126c952d6ccd3a4c00e1885923cb0f8ba6db9cf2 by Chenhao Jiang <chenhaoj@nvidia.com>:

Add the scatter indices to operand space mapping
and change the offset column-wise permutation
based on scatter_dims_to_operand_dims, so that
they can add together correctly.

--
1ecb608e3687cda358965d9fb60144362fdba477 by Chenhao Jiang <chenhaoj@nvidia.com>:

Fix the scatter determinism expander for various dimension numbers

--
985079f4257e632e85162b5525cfd4655ddf555d by Chenhao Jiang <chenhaoj@nvidia.com>:

Add a flag for enabling the scatter_determinism_expander on GPU.

Merging this change closes #19275

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18326 from serach24:chenhao/opt_det_scatter_full de647d44eb28af71e1580b6e8ed9adc751e50f52
",copybara-service[bot],2024-11-15 09:02:31+00:00,[],2024-11-15 20:04:21+00:00,2024-11-15 20:04:20+00:00,https://github.com/tensorflow/tensorflow/pull/80093,[],[],
2661192999,pull_request,open,,Add python symlink to python 3.12.,"Add python symlink to python 3.12.
Modify gcloud installation path so gcloud will be recognized.
",copybara-service[bot],2024-11-15 08:20:18+00:00,['quoctruong'],2024-11-15 08:20:20+00:00,,https://github.com/tensorflow/tensorflow/pull/80092,[],[],
2661046512,pull_request,closed,,[XLA] Remove redundant transposes early on,"[XLA] Remove redundant transposes early on
",copybara-service[bot],2024-11-15 07:16:04+00:00,[],2024-11-15 17:04:52+00:00,2024-11-15 17:04:51+00:00,https://github.com/tensorflow/tensorflow/pull/80091,[],[],
2660847713,pull_request,closed,,[XLA:SPMD] Add HLO annotation to disable collective matmul in SPMD.,"[XLA:SPMD] Add HLO annotation to disable collective matmul in SPMD.
",copybara-service[bot],2024-11-15 05:49:22+00:00,['seherellis'],2024-11-20 07:24:11+00:00,2024-11-20 07:24:09+00:00,https://github.com/tensorflow/tensorflow/pull/80090,[],[],
2660632309,pull_request,closed,,PR #16775: Add test for EmitReducePrecisionIR,"PR #16775: Add test for EmitReducePrecisionIR

Imported from GitHub PR https://github.com/openxla/xla/pull/16775

I noticed that the `EmitReducePrecisionIR` function from `xla/service/elemental_ir_emitter.h` is not covered by unit tests.

Given its non-trivial logic, I believe it should be thoroughly tested, particularly for corner cases.

Changes in this PR:
- Declare `EmitReducePrecisionIR` function in `xla/service/elemental_ir_emitter.h`
- Add `EmitReducePrecisionIR_F16ToF8e5m2` test
- Add `EmitReducePrecisionIR_F16ToF8e4m3fn` test


Related PR:
- [PR-16585](https://github.com/openxla/xla/pull/16585) Add support for float8_e4m3

Copybara import of the project:

--
59722056e36e5a0bab7736b4ad3897446861de0f by Alexander Pivovarov <pivovaa@amazon.com>:

Add test for EmitReducePrecisionIR

Merging this change closes #16775

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16775 from apivovarov:elemental_ir_emitter_test 59722056e36e5a0bab7736b4ad3897446861de0f
",copybara-service[bot],2024-11-15 03:44:29+00:00,[],2024-11-15 08:55:23+00:00,2024-11-15 08:55:22+00:00,https://github.com/tensorflow/tensorflow/pull/80088,[],[],
2660621208,pull_request,closed,,PR #19336: [ROCm] Fix rocm_executor_test,"PR #19336: [ROCm] Fix rocm_executor_test

Imported from GitHub PR https://github.com/openxla/xla/pull/19336

The issue was introduced here https://github.com/openxla/xla/commit/7c468b089a5fb467f7ea9104d3bf6218ffb37d5d
Error log:
```
[ RUN      ] RocmExecutorTest.GetRocmKernel
xla/stream_executor/rocm/rocm_executor_test.cc:70: Failure
Value of: rocm_executor->GetRocmKernel(kernel.get())
Expected: is OK and has a value that is equal to 0x55c94b2c7730
Actual: 16-byte object <01-28 2F-4B C9-55 00-00 00-00 00-00 00-00 00-00>, which has status NOT_FOUND: Kernel not loaded in this executor.
[  FAILED  ] RocmExecutorTest.GetRocmKernel (5 ms)
```
Copybara import of the project:

--
5f5af854b394312707673a3ffc918d4edde31483 by Milica Makevic <Milica.Makevic@amd.com>:

Keep track of loaded kernels in rocm executor

Merging this change closes #19336

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19336 from ROCm:ci_fix_rocm_executor_test 5f5af854b394312707673a3ffc918d4edde31483
",copybara-service[bot],2024-11-15 03:32:16+00:00,[],2024-11-15 04:25:35+00:00,2024-11-15 04:25:34+00:00,https://github.com/tensorflow/tensorflow/pull/80087,[],[],
2660611268,pull_request,closed,,Relocates ShardingConfig's nested proto definition from platforms/ to third_party/.,"Relocates ShardingConfig's nested proto definition from platforms/ to third_party/.
",copybara-service[bot],2024-11-15 03:21:08+00:00,[],2024-11-25 22:51:17+00:00,2024-11-25 22:51:16+00:00,https://github.com/tensorflow/tensorflow/pull/80086,[],[],
2660551072,pull_request,closed,,Fix some typos in comments.,"Fix some typos in comments.
",copybara-service[bot],2024-11-15 02:51:55+00:00,[],2024-11-15 04:37:14+00:00,2024-11-15 04:37:13+00:00,https://github.com/tensorflow/tensorflow/pull/80084,[],[],
2660541232,pull_request,closed,,"Create HloModuleInterface and HloInstructionInterface, and implement that with HloModuleWrapper and HloInstructionWrapper","Create HloModuleInterface and HloInstructionInterface, and implement that with HloModuleWrapper and HloInstructionWrapper
",copybara-service[bot],2024-11-15 02:40:28+00:00,['zzzaries'],2024-11-15 03:46:50+00:00,2024-11-15 03:46:48+00:00,https://github.com/tensorflow/tensorflow/pull/80083,[],[],
2660527517,pull_request,closed,,Update the error message for no TensorCore device trace collected.,"Update the error message for no TensorCore device trace collected.
",copybara-service[bot],2024-11-15 02:25:04+00:00,[],2024-11-15 18:21:22+00:00,2024-11-15 18:21:21+00:00,https://github.com/tensorflow/tensorflow/pull/80082,[],[],
2660521288,pull_request,closed,,[XLA] Add an option to disable verifier in HloExtractor,"[XLA] Add an option to disable verifier in HloExtractor

The verifier is not necessary for extraction and can be expensive. This CL adds an option to disable the verifier in HloExtractor.
",copybara-service[bot],2024-11-15 02:17:59+00:00,[],2024-11-15 08:43:46+00:00,2024-11-15 08:43:45+00:00,https://github.com/tensorflow/tensorflow/pull/80081,[],[],
2660504965,pull_request,closed,,Create a simplified cost interface that can be used in various components.,"Create a simplified cost interface that can be used in various components.

In addition to the simplified interface, we introduce the following auxiliary functionality:
* Delegation between different cost implementations

* Support for allowing cost implementations to be partial

* Caching of cost values

* Logging of cost values

The design of the simplified cost interface is as follows:
* CostMetricid: uniquely identifies a metric we want to compute, e.g., latency of a particular HLO instruction.

* CostValue: the value assigned to a metric.

* MetricCalculator: a function that takes a CostMetricId and returns a CostValue, for a given instruction.

* OpCostCalculator: a function that takes an HLO instruction and returns a MetricCalculator (computing metrics for that instruction).
   - We use a 2 layer approach (i.e., OpCostCalculator and MetricCalculator) to support cases where we compute intermediate computations for an HLO that can be refined into specific metric values.

* OpCostManager: a class that manages the computation of costs. it supports delegation, partial cost implementation, caching, and logging.
",copybara-service[bot],2024-11-15 02:00:24+00:00,['sparc1998'],2024-11-25 20:38:05+00:00,2024-11-25 20:38:04+00:00,https://github.com/tensorflow/tensorflow/pull/80080,[],[],
2660503956,pull_request,closed,,Handle ragged dot in precision config methods.,"Handle ragged dot in precision config methods.
",copybara-service[bot],2024-11-15 01:59:20+00:00,[],2024-11-15 19:52:30+00:00,2024-11-15 19:52:29+00:00,https://github.com/tensorflow/tensorflow/pull/80079,[],[],
2660502496,pull_request,open,,[XLA] Move ScopedLoggingTimerAndTraceMe into OSS level,"[XLA] Move ScopedLoggingTimerAndTraceMe into OSS level
",copybara-service[bot],2024-11-15 01:57:46+00:00,[],2024-11-15 01:57:46+00:00,,https://github.com/tensorflow/tensorflow/pull/80078,[],[],
2660494311,pull_request,closed,,Fix broken test of dispatch_delegate_google_tensor_test,"Fix broken test of dispatch_delegate_google_tensor_test
",copybara-service[bot],2024-11-15 01:48:29+00:00,['terryheo'],2024-11-15 02:30:05+00:00,2024-11-15 02:30:05+00:00,https://github.com/tensorflow/tensorflow/pull/80077,[],[],
2660470844,pull_request,closed,,Pass a nullptr function_name to SB for models with no signatures,"Pass a nullptr function_name to SB for models with no signatures
",copybara-service[bot],2024-11-15 01:23:03+00:00,[],2024-11-15 02:04:05+00:00,2024-11-15 02:04:03+00:00,https://github.com/tensorflow/tensorflow/pull/80076,[],[],
2660446646,pull_request,open,,Use system max group size for Reduce op of OpenCL,"Use the max group size to system supported max size on Intel platform, which can improve the reduce op inference performance. It has about 30% to 60% when the x,y size larger than z size for Reduce ops of segmentation_benchmark_512x512.f32.tflite from [Model zoo](https://chromium.googlesource.com/chromiumos/platform2/+/main/ml_benchmark/model_zoo/).

Bug: N/A",yzhou51,2024-11-15 00:55:49+00:00,['gbaned'],2025-01-16 04:51:12+00:00,,https://github.com/tensorflow/tensorflow/pull/80075,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:XS', 'CL Change Size: Extra Small')]",[],
2660442655,pull_request,open,,Use SLM for FullyConnected op of OpenCL,"Enable the shared local memory for fully connected kernel weights on Intel Platform. There are about 6% to 23% compute saving for FullyConnected ops for different sizes of segmentation_benchmark_512x512.f32.tflite from the [Model Zoo](https://chromium.googlesource.com/chromiumos/platform2/+/main/ml_benchmark/model_zoo/).

Bug: N/A",yzhou51,2024-11-15 00:51:31+00:00,['gbaned'],2025-01-16 04:51:55+00:00,,https://github.com/tensorflow/tensorflow/pull/80074,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2514292430, 'issue_id': 2660442655, 'author': 'keerthanakadiri', 'body': 'Hi @junjiang-lab, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 3, 11, 30, 32, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-12-03 11:30:32 UTC): Hi @junjiang-lab, Can you please review this PR? Thank you !

"
2660439700,pull_request,open,,Integrate LLVM at llvm/llvm-project@b3134fa23383,"Integrate LLVM at llvm/llvm-project@b3134fa23383

Updates LLVM usage to match
[b3134fa23383](https://github.com/llvm/llvm-project/commit/b3134fa23383)
",copybara-service[bot],2024-11-15 00:48:08+00:00,[],2024-11-15 00:48:08+00:00,,https://github.com/tensorflow/tensorflow/pull/80073,[],[],
2660437711,pull_request,open,,Use SLM for depthwise3x3 of OpenCL,"Enable the shared local memory for kernel weights for opencl on Intel Platform. There were about 40% compute time saving for the DepthwiseConv3x3 kernels of segmentation_benchmark_512x512.f32.tflite from the [model zoo](https://chromium.googlesource.com/chromiumos/platform2/+/main/ml_benchmark/model_zoo/)

BUG: N/A",yzhou51,2024-11-15 00:46:19+00:00,['gbaned'],2024-12-17 06:16:46+00:00,,https://github.com/tensorflow/tensorflow/pull/80072,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:S', 'CL Change Size: Small')]","[{'comment_id': 2486395311, 'issue_id': 2660437711, 'author': 'Linchenn', 'body': 'Hi, @roserg, saw you contributed a lot to these codes. Could you help review this?', 'created_at': datetime.datetime(2024, 11, 19, 18, 0, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516242408, 'issue_id': 2660437711, 'author': 'keerthanakadiri', 'body': 'Hi @junjiang-lab, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 4, 5, 42, 15, tzinfo=datetime.timezone.utc)}]","Linchenn on (2024-11-19 18:00:18 UTC): Hi, @roserg, saw you contributed a lot to these codes. Could you help review this?

keerthanakadiri on (2024-12-04 05:42:15 UTC): Hi @junjiang-lab, Can you please review this PR? Thank you !

"
2660432117,pull_request,open,,HostOffloader: use HloInstructionSet/HloInstructionMap instead of absl::flat_hash_set/absl::flat_hash_map.,"HostOffloader: use HloInstructionSet/HloInstructionMap instead of absl::flat_hash_set/absl::flat_hash_map.

If absl::flat_hash_set/map is used, the order in which the instructions are iterated is non-deterministic, potentially resulting in different compilation result fingerprint for the same program.
",copybara-service[bot],2024-11-15 00:41:13+00:00,[],2024-11-15 00:41:13+00:00,,https://github.com/tensorflow/tensorflow/pull/80071,[],[],
2660415978,pull_request,closed,,"Add node attributes to the pinned node of a fusion computation, because fused computation is merged into its caller fusion instruction.","Add node attributes to the pinned node of a fusion computation, because fused computation is merged into its caller fusion instruction.
",copybara-service[bot],2024-11-15 00:25:19+00:00,['zzzaries'],2024-11-15 00:35:34+00:00,2024-11-15 00:35:33+00:00,https://github.com/tensorflow/tensorflow/pull/80070,[],[],
2660314294,pull_request,closed,,Cherrypick: Merge pull request #79955 from gerwout:curl-upgrade-patch,PiperOrigin-RevId: 696661168,mihaimaruseac,2024-11-14 23:19:59+00:00,[],2024-11-16 13:35:35+00:00,2024-11-16 13:35:34+00:00,https://github.com/tensorflow/tensorflow/pull/80068,[],[],
2660307425,pull_request,open,,Fix seenGroupNodeIds mismatch with modelGraph nodes data.,"Fix seenGroupNodeIds mismatch with modelGraph nodes data.

looks like the `seenGroupNodeIds` data set may contain group node id that does not belong to the current `modelGraph` nodes data, causing the lookup for node from modelGraph returns undefined (NodeA/NodeB), thus the group layer comparison fails.
",copybara-service[bot],2024-11-14 23:13:21+00:00,['zzzaries'],2024-11-14 23:13:22+00:00,,https://github.com/tensorflow/tensorflow/pull/80067,[],[],
2660220291,pull_request,closed,,"[XLA:MSA] Refactoring: moves AllocationValue, AllocationRequest, Result, and AliasedOffset classes under allocation_value.h. Result is renamed to AllocationResult.","[XLA:MSA] Refactoring: moves AllocationValue, AllocationRequest, Result, and AliasedOffset classes under allocation_value.h. Result is renamed to AllocationResult.
",copybara-service[bot],2024-11-14 22:32:57+00:00,[],2024-11-15 22:35:41+00:00,2024-11-15 22:35:40+00:00,https://github.com/tensorflow/tensorflow/pull/80066,[],[],
2660213476,pull_request,open,,PR #16775: Add test for EmitReducePrecisionIR,"PR #16775: Add test for EmitReducePrecisionIR

Imported from GitHub PR https://github.com/openxla/xla/pull/16775

I noticed that the `EmitReducePrecisionIR` function from `xla/service/elemental_ir_emitter.h` is not covered by unit tests.

Given its non-trivial logic, I believe it should be thoroughly tested, particularly for corner cases.

Changes in this PR:
- Declare `EmitReducePrecisionIR` function in `xla/service/elemental_ir_emitter.h`
- Add `EmitReducePrecisionIR_F16ToF8e5m2` test
- Add `EmitReducePrecisionIR_F16ToF8e4m3fn` test


Related PR:
- [PR-16585](https://github.com/openxla/xla/pull/16585) Add support for float8_e4m3

Copybara import of the project:

--
59722056e36e5a0bab7736b4ad3897446861de0f by Alexander Pivovarov <pivovaa@amazon.com>:

Add test for EmitReducePrecisionIR

Merging this change closes #16775

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16775 from apivovarov:elemental_ir_emitter_test 59722056e36e5a0bab7736b4ad3897446861de0f
",copybara-service[bot],2024-11-14 22:27:29+00:00,[],2024-11-15 00:42:11+00:00,,https://github.com/tensorflow/tensorflow/pull/80065,[],[],
2660205099,pull_request,closed,,[XLA:GPU] Eagerly free temporary constants created during constant folding to reduce peak heap memory.,"[XLA:GPU] Eagerly free temporary constants created during constant folding to reduce peak heap memory.

In one production workload, peak heap memory went down from 12 GB to 4GB. In the newly added test, peak heap memory usage of the constant folding pass is only 4% of what it was:
- before - test fails with ~ 17X mem over limit:
  ```
  Expected: (mc.PeakHeapGrowth()) < (kExpectedPeakHeapGrowth),
  actual: 6760584170 vs 400000000
  ```
- after - test passes with actual ~ 250000000, so only about 63% of the limit.

After this change the max used memory allocated by the constant folding pass grows sublinearly, whereas it grew linearly with the size of the HLO before.

This change also fixes the comment of `RemoveInstruction` which incorrectly claimed that the instruction was deallocated by the call to `RemoveInstruction`.
",copybara-service[bot],2024-11-14 22:20:27+00:00,[],2024-11-19 10:47:57+00:00,2024-11-19 10:47:56+00:00,https://github.com/tensorflow/tensorflow/pull/80064,[],[],
2660190910,pull_request,closed,,Migrate CanonicalizeBoundaryValuePass to new TFL::Pass mechanism and. remove the .td definition.,"Migrate CanonicalizeBoundaryValuePass to new TFL::Pass mechanism and. remove the .td definition.
",copybara-service[bot],2024-11-14 22:08:54+00:00,['vamsimanchala'],2024-11-15 19:07:03+00:00,2024-11-15 19:07:02+00:00,https://github.com/tensorflow/tensorflow/pull/80063,[],[],
2660185941,pull_request,closed,,Add wrapper for PM Sampling metrics to be added from std::vector<PmSamples> to XLines,"Add wrapper for PM Sampling metrics to be added from std::vector<PmSamples> to XLines
",copybara-service[bot],2024-11-14 22:05:26+00:00,[],2024-12-06 00:43:06+00:00,2024-12-06 00:43:06+00:00,https://github.com/tensorflow/tensorflow/pull/80062,[],[],
2660180323,pull_request,closed,,This CL updates HostTracer to use Traceme Filter mask from profiling request. Also add utility functions to prepare filter mask.,"This CL updates HostTracer to use Traceme Filter mask from profiling request. Also add utility functions to prepare filter mask.
",copybara-service[bot],2024-11-14 22:01:09+00:00,[],2024-11-27 19:45:01+00:00,2024-11-27 19:45:00+00:00,https://github.com/tensorflow/tensorflow/pull/80061,[],[],
2660156447,pull_request,closed,,C++ tree with path API,"C++ tree with path API

* Make tree_util.tree_flatten_with_path and tree_map_with_path APIs to be C++-based, to speed up the pytree flattening.

* Moves all the key classes down to C++ level, while keeping the APIs unchanged. 
  * Known small caveats: they are no longer Python dataclasses, and pattern matching might make pytype unhappy.

* Registered defaultdict and ordereddict via the keypath API now.
",copybara-service[bot],2024-11-14 21:47:08+00:00,[],2024-12-01 05:34:31+00:00,2024-12-01 05:34:31+00:00,https://github.com/tensorflow/tensorflow/pull/80060,[],[],
2660143120,pull_request,closed,,[XLA:MSA] Fixes a bug in GetInefficientAllocationSites(allocation_values). The function was previously assuming allocation_values can never be empty.,"[XLA:MSA] Fixes a bug in GetInefficientAllocationSites(allocation_values). The function was previously assuming allocation_values can never be empty.
",copybara-service[bot],2024-11-14 21:39:38+00:00,[],2024-11-21 00:09:54+00:00,2024-11-21 00:09:53+00:00,https://github.com/tensorflow/tensorflow/pull/80059,[],[],
2660085341,pull_request,closed,,"[XLA:MSA] Adding two debugging functions for memory space assignment to facilitate reproduction of production bugs in small tests through steering decisions at two key points in the MSA pass flow, before and after AllocateSegment() call:","[XLA:MSA] Adding two debugging functions for memory space assignment to facilitate reproduction of production bugs in small tests through steering decisions at two key points in the MSA pass flow, before and after AllocateSegment() call:

1) debugging_allocation_request_modifier_fn(): allows modification of AllocationRequest before AllocateSegment(AllocationRequest) calls.

2) debugging_allocation_result_modifier_fn(): enables enforcing arbitrary failures on allocation requests, modifying the output of AllocateSegment(AllocationRequest) calls.
",copybara-service[bot],2024-11-14 21:21:57+00:00,[],2024-11-15 23:42:11+00:00,2024-11-15 23:42:10+00:00,https://github.com/tensorflow/tensorflow/pull/80058,[],[],
2660081458,pull_request,closed,,Add python symlink to python 3.12.,"Add python symlink to python 3.12.
Modify gcloud installation path so gcloud will be recognized.
",copybara-service[bot],2024-11-14 21:19:34+00:00,['quoctruong'],2024-11-15 08:09:44+00:00,2024-11-15 08:09:43+00:00,https://github.com/tensorflow/tensorflow/pull/80057,[],[],
2660047589,pull_request,closed,,"Moves our C++ ShardingConfig to third_party/ so that it can be used by hlo_module_config.h, etc.","Moves our C++ ShardingConfig to third_party/ so that it can be used by hlo_module_config.h, etc.
",copybara-service[bot],2024-11-14 20:57:28+00:00,[],2024-11-15 00:20:27+00:00,2024-11-15 00:20:27+00:00,https://github.com/tensorflow/tensorflow/pull/80056,[],[],
2659951108,pull_request,closed,,"In a previous CL to cache the results of GetMeshDimPermutationOrderInShardingSpec, the hash and equality functions used for the cache key were incompatible. This CL fixes that.","In a previous CL to cache the results of GetMeshDimPermutationOrderInShardingSpec, the hash and equality functions used for the cache key were incompatible. This CL fixes that.
",copybara-service[bot],2024-11-14 20:20:06+00:00,[],2024-11-14 21:49:21+00:00,2024-11-14 21:49:20+00:00,https://github.com/tensorflow/tensorflow/pull/80055,[],[],
2659878644,pull_request,closed,,Remove now unused type aliases from gpu_types.h.,"Remove now unused type aliases from gpu_types.h.
",copybara-service[bot],2024-11-14 19:40:45+00:00,[],2024-11-14 20:57:05+00:00,2024-11-14 20:57:03+00:00,https://github.com/tensorflow/tensorflow/pull/80054,[],[],
2659866063,pull_request,closed,,We need to set value of `hardware_type` in `OpStats.RunEnviornment` because,"We need to set value of `hardware_type` in `OpStats.RunEnviornment` because

- It is frequently used for code branching for the profiler.
- There's no good way to infer the hardware type enum from the device type string (aka. `OpStats.RunEnvironment.device_type`)
",copybara-service[bot],2024-11-14 19:34:57+00:00,['zzzaries'],2024-11-14 19:48:43+00:00,2024-11-14 19:48:42+00:00,https://github.com/tensorflow/tensorflow/pull/80053,[],[],
2659863310,pull_request,closed,,Add filter functionality to TraceMeRecorder to filter events based on filter parameter.,"Add filter functionality to TraceMeRecorder to filter events based on filter parameter.
",copybara-service[bot],2024-11-14 19:33:20+00:00,[],2024-11-14 20:45:34+00:00,2024-11-14 20:45:33+00:00,https://github.com/tensorflow/tensorflow/pull/80052,[],[],
2659836322,pull_request,closed,,kAsyncStart is missing from CalculatePostOrderScheduleHelper() which will,"kAsyncStart is missing from CalculatePostOrderScheduleHelper() which will
not initialize the ordinal/priority correctly for some instructions. Later
priority-queue-based worklist may not work in the correct order as a result.
",copybara-service[bot],2024-11-14 19:24:10+00:00,[],2024-11-15 20:57:14+00:00,2024-11-15 20:57:13+00:00,https://github.com/tensorflow/tensorflow/pull/80051,[],[],
2659786401,pull_request,closed,,Integrate LLVM at llvm/llvm-project@03730cdd3d10,"Integrate LLVM at llvm/llvm-project@03730cdd3d10

Updates LLVM usage to match
[03730cdd3d10](https://github.com/llvm/llvm-project/commit/03730cdd3d10)
",copybara-service[bot],2024-11-14 19:07:14+00:00,[],2024-11-14 23:43:52+00:00,2024-11-14 23:43:51+00:00,https://github.com/tensorflow/tensorflow/pull/80050,[],[],
2659786139,pull_request,closed,,Remove unneeded caching of parent Executor in RocmCommandBuffer class.,"Remove unneeded caching of parent Executor in RocmCommandBuffer class.
",copybara-service[bot],2024-11-14 19:07:04+00:00,[],2024-11-14 19:42:32+00:00,2024-11-14 19:42:30+00:00,https://github.com/tensorflow/tensorflow/pull/80049,[],[],
2659751269,pull_request,closed,,Disable libnvjitlink by default in OSS.,"Disable libnvjitlink by default in OSS.

This breaks the manylinux compliance of the JAX wheels, so I revert this flag flip until I can fix the nvjitlink build rule
",copybara-service[bot],2024-11-14 18:49:25+00:00,[],2024-11-14 20:02:49+00:00,2024-11-14 20:02:47+00:00,https://github.com/tensorflow/tensorflow/pull/80047,[],[],
2659749144,pull_request,open,,Reverts 8b2d98e648baa88ad32029695bb5611fb53bddae,"Reverts 8b2d98e648baa88ad32029695bb5611fb53bddae
",copybara-service[bot],2024-11-14 18:48:44+00:00,[],2024-11-14 18:48:44+00:00,,https://github.com/tensorflow/tensorflow/pull/80046,[],[],
2659731022,pull_request,closed,,Debugging code cleanup?,"Debugging code cleanup?
",copybara-service[bot],2024-11-14 18:45:56+00:00,[],2024-12-09 20:59:11+00:00,2024-12-09 20:59:10+00:00,https://github.com/tensorflow/tensorflow/pull/80045,[],[],
2659713949,pull_request,closed,,PR #79955: Update the curl dependency: 8.6.0 -> 8.11.0.,"PR #79955: Update the curl dependency: 8.6.0 -> 8.11.0.

Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/79955

Due to multiple security vulnerabilities CVE-2024-2004, CVE-2024-2379, CVE-2024-2398, CVE-2024-2466, CVE-2024-6197, CVE-2024-7264, CVE-2024-8096 and CVE-2024-9681
Copybara import of the project:

--
f05738b11ff2a6654ef5460d9dac8c6795c04ac6 by Gerwout van der Veen <gerwoutvdveen@gmail.com>:

Update the curl dependency: 8.6.0 -> 8.11.0.

Due to multiple security vulnerabilities CVE-2024-2004, CVE-2024-2379, CVE-2024-2398, CVE-2024-2466,
CVE-2024-6197, CVE-2024-7264, CVE-2024-8096 and CVE-2024-9681

--
b5ce0879ff980d61089c74257f450419d8d167cf by Gerwout van der Veen <gerwoutvdveen@gmail.com>:

buildifier formatting

Merging this change closes #79955

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/79955 from gerwout:curl-upgrade-patch b5ce0879ff980d61089c74257f450419d8d167cf
",copybara-service[bot],2024-11-14 18:44:10+00:00,[],2024-11-14 23:07:46+00:00,2024-11-14 23:07:45+00:00,https://github.com/tensorflow/tensorflow/pull/80044,[],[],
2659706623,pull_request,closed,,Create HloUnaryInstruction to support result_accuracy for certain unary functions.,"Create HloUnaryInstruction to support result_accuracy for certain unary functions.
",copybara-service[bot],2024-11-14 18:43:11+00:00,[],2024-11-16 06:36:19+00:00,2024-11-16 06:36:18+00:00,https://github.com/tensorflow/tensorflow/pull/80043,[],[],
2659694334,pull_request,closed,,"r2.18 cherry-pick: 21dce872767 ""Warn if tf.lite.interpreter is used instead of ai-edge-litert.interpreter""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/21dce8727675de018d630beb8288a2929f78972d,tensorflow-jenkins,2024-11-14 18:37:45+00:00,[],2024-11-14 19:23:21+00:00,2024-11-14 19:23:18+00:00,https://github.com/tensorflow/tensorflow/pull/80042,[],[],
2659659800,pull_request,open,,Add S4/U4 support for Reshape.,"Add S4/U4 support for Reshape.
",copybara-service[bot],2024-11-14 18:18:42+00:00,['changhuilin'],2024-11-15 22:36:05+00:00,,https://github.com/tensorflow/tensorflow/pull/80041,[],[],
2659648636,pull_request,closed,,Reverts 9127beadd75028a4e0b46f964b937cff3f3fd878,"Reverts 9127beadd75028a4e0b46f964b937cff3f3fd878
",copybara-service[bot],2024-11-14 18:12:22+00:00,[],2024-11-15 16:30:35+00:00,2024-11-15 16:30:34+00:00,https://github.com/tensorflow/tensorflow/pull/80040,[],[],
2659503188,pull_request,closed,,Fix formatting in developer_guide.md,"Fix formatting in developer_guide.md

<USER> was interpreted as an HTML tag which caused it to fail to render
",copybara-service[bot],2024-11-14 17:15:19+00:00,['majnemer'],2024-11-14 21:22:08+00:00,2024-11-14 21:22:06+00:00,https://github.com/tensorflow/tensorflow/pull/80038,[],[],
2659498600,pull_request,closed,,Add IndexCastUIOp to AxisInfoAnalysis.,"Add IndexCastUIOp to AxisInfoAnalysis.

`index_cast` is supported, so there is not reason why `index_castui` shouldn't be supported. `index_castui` happens to appear in our Triton fusion emitter. Without this change Triton fails to correctly propagate axis info and do layout optimization.
",copybara-service[bot],2024-11-14 17:13:32+00:00,[],2024-11-15 18:11:36+00:00,2024-11-15 18:11:35+00:00,https://github.com/tensorflow/tensorflow/pull/80037,[],[],
2659496182,pull_request,closed,,Remove TODOs associated with fixed bugs.,"Remove TODOs associated with fixed bugs.
",copybara-service[bot],2024-11-14 17:12:15+00:00,[],2024-11-14 18:56:02+00:00,2024-11-14 18:56:02+00:00,https://github.com/tensorflow/tensorflow/pull/80036,[],[],
2659473674,pull_request,open,,Enable subgraph reshaping by default in XNNPACK delegate,"Enable subgraph reshaping by default in XNNPACK delegate

This should improve performance (and could subtly change results) for some models that previously did not delegate to XNNPACK because this flag was disabled.
",copybara-service[bot],2024-11-14 17:00:28+00:00,[],2025-01-02 19:21:29+00:00,,https://github.com/tensorflow/tensorflow/pull/80035,[],[],
2659295321,pull_request,closed,,Fix build errors with GCC,"Fix build errors with GCC
",copybara-service[bot],2024-11-14 16:04:54+00:00,[],2024-11-14 22:01:50+00:00,2024-11-14 22:01:49+00:00,https://github.com/tensorflow/tensorflow/pull/80034,[],[],
2659166624,pull_request,closed,,[XLA:GPU] Add test with support matrix for the dot with batch and contracting dim only.,"[XLA:GPU] Add test with support matrix for the dot with batch and contracting dim only.
",copybara-service[bot],2024-11-14 15:28:22+00:00,[],2024-11-14 21:39:49+00:00,2024-11-14 21:39:48+00:00,https://github.com/tensorflow/tensorflow/pull/80033,[],[],
2659160525,pull_request,closed,,[XLA:GPU] Allow auto layout in multihost HLO runner.,"[XLA:GPU] Allow auto layout in multihost HLO runner.

Reverts 67267fbb0750d97e1be3b918fe00ba494b93b656
",copybara-service[bot],2024-11-14 15:25:50+00:00,[],2024-11-19 14:15:35+00:00,2024-11-19 14:15:34+00:00,https://github.com/tensorflow/tensorflow/pull/80032,[],[],
2658901750,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-14 13:57:27+00:00,[],2024-11-14 19:41:18+00:00,,https://github.com/tensorflow/tensorflow/pull/80031,[],[],
2658753113,pull_request,closed,,PR #19342: [ROCm] Skip unsupported tests in dot_algorithms_test,"PR #19342: [ROCm] Skip unsupported tests in dot_algorithms_test

Imported from GitHub PR https://github.com/openxla/xla/pull/19342

Triton is currently disabled on ROCm. Skipping the following subtests in `dot_algorithms_test`:
- TritonAlgorithmTest.Algorithm_BF16_BF16_F32_X3
- TritonAlgorithmTest.Algorithm_BF16_BF16_F32_X6
- TritonAlgorithmTest.Algorithm_TF32_TF32_F32
- TritonAlgorithmTest.Algorithm_TF32_TF32_F32_X3
- TritonAlgorithmTest.Algorithm_BF16_BF16_F32
Copybara import of the project:

--
32bd775f87e142bcb194dcc8cc9807c864c995da by Milica Makevic <Milica.Makevic@amd.com>:

Disable unsupported Triton subtests

Merging this change closes #19342

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19342 from ROCm:disable_triton_tests 32bd775f87e142bcb194dcc8cc9807c864c995da
",copybara-service[bot],2024-11-14 13:06:30+00:00,[],2024-11-15 04:16:19+00:00,2024-11-15 04:16:18+00:00,https://github.com/tensorflow/tensorflow/pull/80030,[],[],
2658580875,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-11-14 11:47:20+00:00,[],2024-11-20 05:13:55+00:00,,https://github.com/tensorflow/tensorflow/pull/80029,[],[],
