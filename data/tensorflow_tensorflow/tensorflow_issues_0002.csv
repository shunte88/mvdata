id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2615614653,issue,open,,"`data_flow_ops.Barrier` aborts with ""Check failed: i >= 0 (0 vs. -100)"" ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.eager import context
import tensorflow as tf

with context.graph_mode():
    sess = tf.compat.v1.Session()
    with sess.as_default():
        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')
        keys = [b'a', b'b', b'c', b'd']
        values_0 = [10.0, 20.0, 30.0, 40.0]
        values_1 = [100.0, 200.0, 300.0, 400.0]
        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) 
        insert_1_1_op.run()
```


### Relevant log output

```shell
F tensorflow/core/kernels/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)
Aborted (core dumped)
```
",cybersupersoap,2024-10-26 07:04:16+00:00,['tilakrayal'],2024-10-28 10:37:00+00:00,,https://github.com/tensorflow/tensorflow/issues/78828,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2441211326, 'issue_id': 2615614653, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow 2.17, 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d1e76396d5177d0eb87574be165e6571/untitled2207.ipynb).\r\n\r\n![Screenshot 2024-10-28 4 04 54 PM](https://github.com/user-attachments/assets/d1d8211c-9e52-45a1-86d9-4aa58cbe2a79)', 'created_at': datetime.datetime(2024, 10, 28, 10, 36, 43, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-28 10:36:43 UTC): I was able to reproduce the issue on tensorflow 2.17, 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d1e76396d5177d0eb87574be165e6571/untitled2207.ipynb).

![Screenshot 2024-10-28 4 04 54 PM](https://github.com/user-attachments/assets/d1d8211c-9e52-45a1-86d9-4aa58cbe2a79)

"
2614957479,issue,closed,not_planned,SPM support for TFLite Swift,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

No

### OS platform and distribution

iOS, iPadOS, MacOS

### Mobile device

iPhone

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This issue #44609 has stayed open for the last 4 years now, every Apple dev has given up on official SPM support for TFLite. Hence creating a new issue here to bring some much needed attention. 4 years is really a long time to address this minor but important issue.

SPM is now the default preferred way, cocoapods has gone into maintenance this year, soon to be forgotten. 
This makes adding SPM support all the more important! Please add support at the earliest. Regards.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_",r-a-o,2024-10-25 19:37:13+00:00,['pkgoogle'],2024-10-30 20:06:07+00:00,2024-10-30 20:05:00+00:00,https://github.com/tensorflow/tensorflow/issues/78787,"[('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2447321402, 'issue_id': 2614957479, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 30, 14, 19, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448261271, 'issue_id': 2614957479, 'author': 'pkgoogle', 'body': ""Hi, I'm closing this as duplicate... please follow progress on the original issue, +1 and comment more on that thread. Thanks."", 'created_at': datetime.datetime(2024, 10, 30, 20, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448261337, 'issue_id': 2614957479, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78787"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78787"">No</a>', 'created_at': datetime.datetime(2024, 10, 30, 20, 5, 2, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-10-30 14:19:46 UTC): Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-30 20:05:00 UTC): Hi, I'm closing this as duplicate... please follow progress on the original issue, +1 and comment more on that thread. Thanks.

google-ml-butler[bot] on (2024-10-30 20:05:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78787"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78787"">No</a>

"
2614920888,issue,open,,Regression: TF 2.18 crashes with cudaSetDevice failing due to GPU being busy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow[and-cuda] 2.18

### Custom code

No

### OS platform and distribution

Rocky 9.1

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

get_build_info gives cuda 12.5.1, cudnn version 9, Driver version 525.78.01, system CUDA version 12.0

### GPU model and memory

RTX 3090

### Current behavior?

Tensorflow[and-cuda] 2.17 installed cuda_version 12.3 and cudnn 8, and the code below works with the normal error spew about being unable to register cuda factories. With 2.18, I get a crash about the gpu is busy.

This is on a desktop workstation with a window manager, so TF cannot have the entire GPU, but I'm not sure if that's the cause of the problem.

These are pretty normal conda environments, with `pip install ""tensorflow[and-cuda]""` and then 

```
export XLA_FLAGS=""--xla_gpu_cuda_data_dir=/path/to/my/conda/environment""


NVIDIA_DIR=$(dirname $(dirname $(python -c ""import nvidia.cudnn; print(nvidia.cudnn.__file__)"")))

pathAccum=""""

for dir in $NVIDIA_DIR/*; do
    if [ -d ""$dir/lib"" ]; then
        pathAccum=""$dir/lib:$pathAccum""
    fi
done

export LD_LIBRARY_PATH=""${pathAccum}${LD_LIBRARY_PATH}""
```

inside the activate.d for the conda environment. 
I have tried with and without this activate script. It is necessary for 2.17, and I get the same error when I remove it for 2.18. I'm not sure if this is due to ""hermetic cuda"", since it's not at all clear what that means to an end user.

Interestingly, `tf.config.list_physical_devices(""GPU"")` correctly identifies the GPU, it returns `PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')`. 

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3
# TensorFlow and tf.keras
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)
])
```


### Relevant log output

```shell
TF 2.17 (no bug):

2024-10-25 14:07:40.179745: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-25 14:07:40.187066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-25 14:07:40.194965: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-25 14:07:40.197313: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-25 14:07:40.203777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-25 14:07:40.624911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/scratch/miniconda/install/envs/bpreveal-teak/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1729883261.043484 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.057001 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.057122 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.058905 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.059226 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.059269 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521319 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521431 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521503 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-10-25 14:07:41.521550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18871 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6



TF 2.18 (with bug):

2024-10-25 14:02:53.202498: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-25 14:02:53.209011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729882973.216967 2360012 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729882973.219326 2360012 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-25 14:02:53.227106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Traceback (most recent call last):
  File ""/home/cm2363/scratch/./tfTest.py"", line 5, in <module>
    model = keras.Sequential([
            ^^^^^^^^^^^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 76, in __init__
    self._maybe_rebuild()
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 141, in _maybe_rebuild
    self.build(input_shape)
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 226, in build_wrapper
    original_build_method(*args, **kwargs)
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 187, in build
    x = layer(x)
        ^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py"", line 125, in convert_to_tensor
    return tf.convert_to_tensor(x, dtype=dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable
```
",mmtrebuchet,2024-10-25 19:14:15+00:00,['tilakrayal'],2025-02-05 10:18:03+00:00,,https://github.com/tensorflow/tensorflow/issues/78784,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2443519445, 'issue_id': 2614920888, 'author': 'tilakrayal', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 10, 29, 8, 10, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445173305, 'issue_id': 2614920888, 'author': 'mmtrebuchet', 'body': ""Since it's been recommended elsewhere, I can verify that `nvidia-smi --query-gpu=compute_mode --format=csv` gives `Default` as the compute mode on my GPU. I have also tested on 2.17.1 and I do *not* get the bug there (but I do get the usual errors about being unable to register cuFFT, cuDNN, and cuBLAS that were present in 2.17). This is reproducible with a minimal conda environment that contains only Python 3.12 and tensorflow[and-cuda] installed with pip."", 'created_at': datetime.datetime(2024, 10, 29, 19, 37, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495131916, 'issue_id': 2614920888, 'author': 'mmtrebuchet', 'body': 'The problem seems to be when the devices are initialized, and not when the physical devices are detected.\r\n```\r\nimport tensorflow as tf\r\nprint(tf.config.list_physical_devices(""GPU""))\r\nprint(tf.config.list_logical_devices(""GPU""))\r\n```\r\n\r\nWith this program, it prints out a list of physical devices and then I get the crash when I try to list logical devices.\r\n\r\n\r\n```\r\n2024-11-22 17:26:46.469844: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-22 17:26:46.476671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1732318006.484988 1217743 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1732318006.487432 1217743 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-22 17:26:46.495845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\nTraceback (most recent call last):\r\n  File ""/home/cm2363/scratch/./tfTest.py"", line 11, in <module>\r\n    print(tf.config.list_logical_devices(\'GPU\'))\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/framework/config.py"", line 497, in list_logical_devices\r\n    return context.context().list_logical_devices(device_type=device_type)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/eager/context.py"", line 1903, in list_logical_devices\r\n    self.ensure_initialized()\r\n  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/eager/context.py"", line 726, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InternalError: cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable\r\n\r\n```\r\n\r\nThis has made TF 2.18 literally unusable for me. Restarting doesn\'t help, there is plenty of RAM available on the GPU, and it works just fine with TF 2.17. \r\n\r\nGood news, though: It does NOT recur with the nightly release of 2.19. Bad news: The nightly release has a serious performance regression (training now takes TWICE as long) and until I can get that addressed, I\'m still locked on 2.17.', 'created_at': datetime.datetime(2024, 11, 23, 0, 13, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519705980, 'issue_id': 2614920888, 'author': 'maifeeulasad', 'body': '@mmtrebuchet I can confirm, downgrading to `tf-2.17` will resolve the issue.', 'created_at': datetime.datetime(2024, 12, 5, 9, 13, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636309929, 'issue_id': 2614920888, 'author': 'RabJon', 'body': 'An update of my NVIDIA GPU driver through NVIDIA Geforce Experience and a subsequent system restart fixed the problem for me.', 'created_at': datetime.datetime(2025, 2, 5, 10, 18, 2, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-29 08:10:19 UTC): @learning-to-play

mmtrebuchet (Issue Creator) on (2024-10-29 19:37:22 UTC): Since it's been recommended elsewhere, I can verify that `nvidia-smi --query-gpu=compute_mode --format=csv` gives `Default` as the compute mode on my GPU. I have also tested on 2.17.1 and I do *not* get the bug there (but I do get the usual errors about being unable to register cuFFT, cuDNN, and cuBLAS that were present in 2.17). This is reproducible with a minimal conda environment that contains only Python 3.12 and tensorflow[and-cuda] installed with pip.

mmtrebuchet (Issue Creator) on (2024-11-23 00:13:12 UTC): The problem seems to be when the devices are initialized, and not when the physical devices are detected.
```
import tensorflow as tf
print(tf.config.list_physical_devices(""GPU""))
print(tf.config.list_logical_devices(""GPU""))
```

With this program, it prints out a list of physical devices and then I get the crash when I try to list logical devices.


```
2024-11-22 17:26:46.469844: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-22 17:26:46.476671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732318006.484988 1217743 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732318006.487432 1217743 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-22 17:26:46.495845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Traceback (most recent call last):
  File ""/home/cm2363/scratch/./tfTest.py"", line 11, in <module>
    print(tf.config.list_logical_devices('GPU'))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/framework/config.py"", line 497, in list_logical_devices
    return context.context().list_logical_devices(device_type=device_type)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/eager/context.py"", line 1903, in list_logical_devices
    self.ensure_initialized()
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/tensorflow/python/eager/context.py"", line 726, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable

```

This has made TF 2.18 literally unusable for me. Restarting doesn't help, there is plenty of RAM available on the GPU, and it works just fine with TF 2.17. 

Good news, though: It does NOT recur with the nightly release of 2.19. Bad news: The nightly release has a serious performance regression (training now takes TWICE as long) and until I can get that addressed, I'm still locked on 2.17.

maifeeulasad on (2024-12-05 09:13:33 UTC): @mmtrebuchet I can confirm, downgrading to `tf-2.17` will resolve the issue.

RabJon on (2025-02-05 10:18:02 UTC): An update of my NVIDIA GPU driver through NVIDIA Geforce Experience and a subsequent system restart fixed the problem for me.

"
2614694737,issue,open,,It doesn't support on python3.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

macos sequoia arm

### Mobile device

_No response_

### Python version

3.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It doesn't work when I want to install via terminal with the installation code. 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```


### Relevant log output

_No response_",toufa12,2024-10-25 17:18:07+00:00,['Venkat6871'],2025-02-03 17:12:05+00:00,,https://github.com/tensorflow/tensorflow/issues/78774,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2439568356, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'TensorFlow always supports the new Python version in the release that follows nearly a quarter after the Python release. This is because the TensorFlow release cycle starts before the Python release occurs and cannot be changed mid-flight. If you look at past issues, you see that this has been an issue since at least python 3.8.\r\n\r\nIt has been suggested to test release candidates, etc., but no improvements have been made on this front.', 'created_at': datetime.datetime(2024, 10, 26, 12, 34, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455826772, 'issue_id': 2614694737, 'author': 'BaseMax', 'body': 'Same issue\r\n\r\nI am going to uninstall Python 3.13 and install an older python version.', 'created_at': datetime.datetime(2024, 11, 4, 22, 24, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476357040, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': ""python 3.13 is important because it's the default and only version in the latest fedora 41 (released 2024-10-29)\r\nfedora is a major distro and many users are upgrading to the latest version.\r\n\r\nI don't expect tf to support every distro. but python 3.13 is the latest released stable version from python foundation and is used by a major distro. it would be a shame not to support it. I believe we need to adjust or sync the process.\r\n\r\nthose releases are scheduled and predictable"", 'created_at': datetime.datetime(2024, 11, 14, 13, 26, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497828478, 'issue_id': 2614694737, 'author': 'iseeface', 'body': 'still no fix?', 'created_at': datetime.datetime(2024, 11, 25, 12, 2, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498148533, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""> TensorFlow always supports the new Python version in the release that follows nearly a quarter after the Python release. This is because the TensorFlow release cycle starts before the Python release occurs and cannot be changed mid-flight. If you look at past issues, you see that this has been an issue since at least python 3.8.\r\n> \r\n> It has been suggested to test release candidates, etc., but no improvements have been made on this front.\r\n\r\nThis is still true. Until the next release next year, there won't be a change."", 'created_at': datetime.datetime(2024, 11, 25, 14, 20, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513218333, 'issue_id': 2614694737, 'author': 'yuchen001', 'body': 'In addition to Python 3.13 not being supported, could someone confirm if Python 3.12 is currently supported?', 'created_at': datetime.datetime(2024, 12, 2, 23, 53, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514056283, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'It should be (3.12)', 'created_at': datetime.datetime(2024, 12, 3, 9, 51, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515006890, 'issue_id': 2614694737, 'author': 'tbitai', 'body': ""According to the [install docs](https://www.tensorflow.org/install/), it's\r\n> Python 3.8–3.11"", 'created_at': datetime.datetime(2024, 12, 3, 16, 18, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515967472, 'issue_id': 2614694737, 'author': 'thebaptiste', 'body': 'According to the available wheels on PyPI, it seems rather to be 3.9-3.12 since release 2.16.1\r\nMay be the install doc has not been updated...', 'created_at': datetime.datetime(2024, 12, 4, 1, 31, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519324546, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': ""we just need to do the following\r\nwhen a new stable python is released by PSF just get last tag and rebuild it and push the wheel for that python version.\r\nthere is nothing preventing us from doing this simple automation and I'm not aware of any compatibility issue preventing us from doing this. the current state seems careless, lazy and incompetent."", 'created_at': datetime.datetime(2024, 12, 5, 6, 37, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519539464, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'Unfortunately the build system for TF is too complicated for this.\r\n\r\nThere needs to be a lot of work done to make that possible. Plus, TF has a large number of dependencies that also need to start supporting the new version of Python. And, C++ <-> Python interop has a lot of issues with any new version of Python.', 'created_at': datetime.datetime(2024, 12, 5, 8, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558462759, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': ""would you please point to the CI/CD failed build logs showing the claimed incompatibility? I'm not aware of any. This is not compatibility issue but rather process issue.\r\n\r\nAlso related:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/79349"", 'created_at': datetime.datetime(2024, 12, 22, 13, 47, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558576401, 'issue_id': 2614694737, 'author': 'svenstaro', 'body': 'Here are my build logs: [tensorflow-python313.txt](https://github.com/user-attachments/files/18223075/tensorflow-python313.txt)', 'created_at': datetime.datetime(2024, 12, 22, 19, 52, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559273416, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': '```\r\nError in fail: \r\nCould not find requirements_lock.txt file matching specified Python version.\r\nSpecified python version: 3.13\r\nPython versions with available requirement_lock.txt files: 3.9, 3.10, 3.11, 3.12\r\n```\r\n\r\nyou must be kidding, let\'s have mutual respect, I can read and this is not the actual build log this is just an early exist resulted from a pinned version that can patched by a single-line change. basically it says ""we don\'t want you to to try to build it on 3.13"". Pinning the versions happens after a successful build not before it. It\'s a political decision not a technical barrier.\r\n\r\nthe current status of tensorflow is unacceptable as it\'s uninstallable on windows, macos and latest fedora linux all with latest stable python from python foundation.', 'created_at': datetime.datetime(2024, 12, 23, 9, 17, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559421364, 'issue_id': 2614694737, 'author': 'svenstaro', 'body': ""So, two things:\n- This issue is about tensorflow not supporting 3.13. If you read the error message, it quite clearly doesn't support 3.13.\n- If you try to actually patch the files (as we tried in Arch), you'll see various failures in parts of tensorflow. I can get you some logs but you can also try it yourself."", 'created_at': datetime.datetime(2024, 12, 23, 10, 44, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559810047, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""> would you please point to the CI/CD failed build logs showing the claimed incompatibility\r\n\r\nI no longer work on TF (left in 2022, but still helped with triage and some issues). From my time leading the OSS build team, the issues were usually some dependencies that also needed to be updated. TF has quite a large number of them, and some still need C++/Python interop. It looks like some of those have been going through removal now, so maybe this is not as bad.\r\n\r\nThen, TF itself uses C++ Python interop and that is tied to the Python versions supported. It might be that for 3.13 there's nothing to change, but in general we had to. Similarly, for numpy compat.\r\n\r\nIf I find some time over these two weeks, I'll try to get a build started, but since I'm not in the team the only way I can contribute is the same as you: PRs welcome."", 'created_at': datetime.datetime(2024, 12, 23, 14, 29, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560204529, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""For the last release of TF (2.18), bramnch cut ocurred slightly before Sept 25 (given https://github.com/tensorflow/tensorflow/commit/b4b3e2da6b64a996f418810524a3274a6bef7fa5 to update version number to RC0).\r\n\r\nBut on Sept 25, `grpcio` was at 1.66.1, which has [no Python 3.13 support](https://pypi.org/project/grpcio/1.66.1/#files).\r\n\r\nThis is just the first dependency in the chain that needed to be updated before TF is able to support a new version of Python. I didn't check [any other of the direct dependencies from deps.dev](https://deps.dev/pypi/tensorflow/2.18.0/dependencies) (note that the dependency support versions listed there are not the ones available when the branch was cut, not the ones for RC0).\r\n\r\nBecause the release process for TF is expensive, once a branch is cut, it can only receive minor cherry-picks, not features from master branch.\r\n\r\nThat being said, `tf-nightly` should have support for a new version of Python much earlier than it does now, as soon as all dependencies support the new version of Python, TF should be able to do so too. But it is now a requirement on the planning for the OSS team, I am no longer TL there and won't make planning statements."", 'created_at': datetime.datetime(2024, 12, 23, 19, 31, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561966317, 'issue_id': 2614694737, 'author': 'jyo64', 'body': 'Tensorflow support for Python 3.13 is a deal breaker now. Even where project that have tensorflow as a dependency 😢.', 'created_at': datetime.datetime(2024, 12, 25, 18, 4, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561971309, 'issue_id': 2614694737, 'author': 'jyo64', 'body': ""> For the last release of TF (2.18), bramnch cut ocurred slightly before Sept 25 (given [b4b3e2d](https://github.com/tensorflow/tensorflow/commit/b4b3e2da6b64a996f418810524a3274a6bef7fa5) to update version number to RC0).\r\n> \r\n> But on Sept 25, `grpcio` was at 1.66.1, which has [no Python 3.13 support](https://pypi.org/project/grpcio/1.66.1/#files).\r\n> \r\n> This is just the first dependency in the chain that needed to be updated before TF is able to support a new version of Python. I didn't check [any other of the direct dependencies from deps.dev](https://deps.dev/pypi/tensorflow/2.18.0/dependencies) (note that the dependency support versions listed there are not the ones available when the branch was cut, not the ones for RC0).\r\n> \r\n> Because the release process for TF is expensive, once a branch is cut, it can only receive minor cherry-picks, not features from master branch.\r\n> \r\n> That being said, `tf-nightly` should have support for a new version of Python much earlier than it does now, as soon as all dependencies support the new version of Python, TF should be able to do so too. But it is now a requirement on the planning for the OSS team, I am no longer TL there and won't make planning statements.\r\n\r\n\r\n\r\nI just tried installing all the dependencies manually as mentioned here. The only thing that has failed is tensorflow-io-gcs-filesystem. I think grpcio (the latest version) now supports Python 3.13."", 'created_at': datetime.datetime(2024, 12, 25, 18, 28, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562776162, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': "">  tensorflow-io-gcs-filesystem. I think grpcio (the latest version) now supports Python 3.13.\r\n\r\nyou are correct\r\n\r\n![image](https://github.com/user-attachments/assets/9ae6face-7596-49e5-9f6e-a87c98e584b8)\r\n\r\nwhich further indicate it's a political decision within tensorflow team (tensorflow-io-gcs-filesystem) not a technical one.\r\nit's a matter of flipping a switch.\r\n\r\n>  I'm not aware of any compatibility issue preventing us from doing this. the current state seems careless, lazy and incompetent."", 'created_at': datetime.datetime(2024, 12, 26, 14, 0, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562847205, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'grpcio support for 3.13 was added after TF branch cut.\r\n\r\nI already explained several times that it is expensive to change the build process once the release train has started.', 'created_at': datetime.datetime(2024, 12, 26, 14, 44, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562866535, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': '> I already explained several times that it is expensive to change the build process once the release train has started.\r\n\r\nwhich is what I\'ve said, there is no breaking changes introduced by PSF\'s 3.13 \r\nit\'s just careless, lazy and incompetent process within TF team.\r\nyou have an established process and you refuse to fix it.\r\n\r\nI have no problem I missed the train, the problem that next train is scheduled to be after 6 months. this is not how infra should be built.\r\n\r\nyou know when PSF plan their python release, you schedule a ""train"" when it\'s in RC phase so that when it\'s release you release a new build.', 'created_at': datetime.datetime(2024, 12, 26, 14, 59, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562871935, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'Google has its own requirements too, it\'s not ""you can just schedule"".\r\n\r\nYou can also send PRs, the build process is 90% in the repo itself.', 'created_at': datetime.datetime(2024, 12, 26, 15, 6, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563012307, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'There are actually Python 3.13 features that require more work from TF:\r\n\r\n- the new free-threaded runtime: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython. Since TF cannot know if it is called from the new runtime or not, we need to ensure that both are supported. But this entails making sure that there are no incompatibilities with the way TF kernels have been written, there might be assumptions that are broken. Hence, this needs a longer baking in period than ""just"" releasing.\r\n- the new JIT compiler: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython. TF already does a lot of JIT via XLA.\r\n- there are `ctypes` changes that we need to look into: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython\r\n\r\nThis is on a quick glance on the release notes. I might have missed some other changes and maybe not all of the above are true, TF is too big and I barely scratched the entire surface of it.', 'created_at': datetime.datetime(2024, 12, 26, 18, 31, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563013286, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""> the problem that next train is scheduled to be after 6 months\r\n\r\nThat's not true. Releases are quarterly, not biannually."", 'created_at': datetime.datetime(2024, 12, 26, 18, 33, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563032421, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': ""All of those are runtime not build time. Supporting some runtimes is better\r\nthan supporting zero runtimes.\r\n\r\nOn Thu, Dec 26, 2024, 9:33\u202fPM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> the problem that next train is scheduled to be after 6 months\r\n>\r\n> That's not true. Releases are quarterly, not biannually.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2563013286>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAKAPKZH7E6IWXG74V6DVFL2HRDYFAVCNFSM6AAAAABQTWNUISVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRTGAYTGMRYGY>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 12, 26, 19, 5, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563034869, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': '> Supporting some runtimes is better\r\n\r\nYes, and TF supports Python 3.12, 3.11 and so on. More runtimes by February.', 'created_at': datetime.datetime(2024, 12, 26, 19, 8, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563039949, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': 'I was referring to ""...the new free-threaded runtime... the new JIT\r\ncompiler""\r\n\r\nWe did not push wheels for py 3.13 because it can be called in an\r\nunsupported rt. Well! If i call it in the new rt it would be my problem.\r\nI\'ll be sure to call it in one of the supported ways.\r\n\r\nThe current state in case you don\'t know there is no usable wheels for\r\nWindows, mac or fedora linux, arch linux ..etc.\r\n\r\nHow to reproduce? get the latest stable os from any of those vendors,\r\ninstall the latest stable python. Try to install TF.\r\n\r\nOn Thu, Dec 26, 2024, 10:08\u202fPM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Supporting some runtimes is better\r\n>\r\n> Yes, and TF supports Python 3.12, 3.11 and so on. More runtimes by\r\n> February.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2563034869>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAKAPK2LFQQKLK5AXQO4XLT2HRH2XAVCNFSM6AAAAABQTWNUISVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRTGAZTIOBWHE>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 12, 26, 19, 16, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563043900, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'NixOS 24.11 is on Python 3.12 (by default, but supports even 3.14a). Fedora 41 released with Python 3.13 by default, but it does not block you from installing a previous version. `hatch`, `uv`, `venv` are all tools you could use to manage python installations.', 'created_at': datetime.datetime(2024, 12, 26, 19, 23, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563046793, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""To be clear, I am still of the opinion that `tf-nightly` should already support 3.13, it's not really ok to delay adding support to it for so long."", 'created_at': datetime.datetime(2024, 12, 26, 19, 28, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568162170, 'issue_id': 2614694737, 'author': 'nbecker', 'body': ""> python 3.13 is important because it's the default and only version in the latest fedora 41 (released 2024-10-29) fedora is a major distro and many users are upgrading to the latest version.\r\n> \r\n> I don't expect tf to support every distro. but python 3.13 is the latest released stable version from python foundation and is used by a major distro. it would be a shame not to support it. I believe we need to adjust or sync the process.\r\n> \r\n> those releases are scheduled and predictable\r\n\r\nThere is python 3.12 on Fedora 41, \r\nsudo dnf install python3.12"", 'created_at': datetime.datetime(2025, 1, 2, 18, 2, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568172506, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': ""This reminds me of nvidia/ubuntu mentality. Canonical had one kernel\r\ndeveloper and his only job is to make sure they ship a kernel that works\r\nwith nvidia drivers, if not they ship the older kernel.\r\n\r\nAs long as there are no breaking changes done by PSF in their latest stable\r\npython, we should build and publish the wheels. We should not hold\r\ninnovation waiting for church approval.\r\n\r\nOn Thu, Jan 2, 2025, 9:03\u202fPM ndbecker ***@***.***> wrote:\r\n\r\n> python 3.13 is important because it's the default and only version in the\r\n> latest fedora 41 (released 2024-10-29) fedora is a major distro and many\r\n> users are upgrading to the latest version.\r\n>\r\n> I don't expect tf to support every distro. but python 3.13 is the latest\r\n> released stable version from python foundation and is used by a major\r\n> distro. it would be a shame not to support it. I believe we need to adjust\r\n> or sync the process.\r\n>\r\n> those releases are scheduled and predictable\r\n>\r\n> There is python 3.12 on Fedora 41,\r\n> sudo dnf install python3.12\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2568162170>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAKAPK53JUKE37TFVN5K3AL2IV5OTAVCNFSM6AAAAABQTWNUISVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRYGE3DEMJXGA>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 2, 18, 10, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568885455, 'issue_id': 2614694737, 'author': 'ilcesko', 'body': ""I read the whole discussion but I couldn't find an actual ETA for 3.13 support, do we have one ?\r\nthanks"", 'created_at': datetime.datetime(2025, 1, 3, 9, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569300829, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'Next release of TF. Branch cut should occur by mid of February at most and final release at most a month after, assuming no delays.', 'created_at': datetime.datetime(2025, 1, 3, 14, 28, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571623930, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': 'it seems that church priests keep down voting me\r\n\r\n![image](https://github.com/user-attachments/assets/74bfe9af-fa7f-40c5-b611-2ee97f8e64a3)\r\n\r\nand ignore the community\r\n\r\n![image](https://github.com/user-attachments/assets/e9af7f2d-29ed-4fa2-81a6-4f9b1e7ae4b8)\r\n\r\n> Next release of TF. Branch cut should occur by mid of February \r\n \r\nThis is not the first time this happens, we had such tickets before and we will continue to have them when PSF release python 3.14\r\n\r\nsupporting python 3.13 is not the root cause of the problem.\r\nthere should be a way within TF church to request a build of the current stable of version of TF (no new features) against the current stable version of python without version pinning.\r\n\r\n> That being said, tf-nightly should have support for a new version of Python much earlier than it does now,\r\n\r\nyes, should have, but did not.', 'created_at': datetime.datetime(2025, 1, 5, 13, 16, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573559656, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""Apologies for the wrong signal sent by :-1:. I wanted a quick feedback to signal that you are repeating the same information, in a way that could be construed as aggressive and against [the code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\r\n\r\nThis is not Reddit or similar, so a :-1: has no effect (I got a few myself too, even here, including from you). But, taking into account your feedback, I have removed it.\r\n\r\nRegarding the rest of your message, I can only reply by repeating what I have already said. In the spirit of making progress here, I will just summarize:\r\n\r\n- Stable release of new Python version is impossible to add after branch cut\r\n- Testing RCs of new Python version just before release train starts risks delaying releases for everyone and then can lead to issues down the line (e.g., Python 3.11 docker images for some TF versions are on an RC version even now because of this)\r\n- Release calendar has to take into account multiple priorities and coding freezes. It cannot be shifted around to match the Python release schedule.\r\n- Nightly support of new Python version can be added, but in general it gets delayed. It is the responsibility of the OSS team to do so and they have to juggle quite a large number of priorities. Please assume the best intentions, there is no one that wants the community to not be supported\r\n- I have left the TF OSS team in 2022 and have been only helping here and there to smooth the transition. I do not represent the TF team and I don't know why you are picking on me. I'm just responding to the issue rather than let it stale.\r\n\r\nPlease make sure to read the code of conduct."", 'created_at': datetime.datetime(2025, 1, 6, 17, 19, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576240682, 'issue_id': 2614694737, 'author': 'muayyad-alsadi', 'body': 'Code of conduct? Church here is not a religious reference!! It\'s a\r\nhierarchical management reference used by Eric S. Raymond in his book ""The\r\nCathedral and the Bazaar"" (abbreviated CatB) which is one of the\r\nfoundations of OpenSource community (he is the one behind OSI definition).\r\n\r\nHow come you did not recognize the reference to CatB and OSI? Never heard\r\nof it? Am I that old?\r\n\r\nhttps://en.m.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar\r\n\r\nOn Mon, Jan 6, 2025, 8:19\u202fPM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Apologies for the wrong signal sent by 👎. I wanted a quick feedback to\r\n> signal that you are repeating the same information, in a way that could be\r\n> construed as aggressive and against the code of conduct\r\n> <https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md>.\r\n>\r\n> This is not Reddit or similar, so a 👎 has no effect (I got a few myself\r\n> too, even here). But, taking into account your feedback, I have removed it.\r\n>\r\n> Regarding the rest of your message, I can only reply by repeating what I\r\n> have already said. In the spirit of making progress here, I will just\r\n> summarize:\r\n>\r\n>    - Stable release of new Python version is impossible to add after\r\n>    branch cut\r\n>    - Testing RCs of new Python version just before release train starts\r\n>    risks delaying releases for everyone and then can lead to issues down the\r\n>    line (e.g., Python 3.11 docker images for some TF versions are on an RC\r\n>    version even now because of this)\r\n>    - Release calendar has to take into account multiple priorities and\r\n>    coding freezes. It cannot be shifted around to match the Python release\r\n>    schedule.\r\n>    - Nightly support of new Python version can be added, but in general\r\n>    it gets delayed. It is the responsibility of the OSS team to do so and they\r\n>    have to juggle quite a large number of priorities. Please assume the best\r\n>    intentions, there is no one that wants the community to not be supported\r\n>    - I have left the TF OSS team in 2022 and have been only helping here\r\n>    and there to smooth the transition. I do not represent the TF team and I\r\n>    don\'t know why you are picking on me. I\'m just responding to the issue\r\n>    rather than let it stale.\r\n>\r\n> Please make sure to read the code of conduct.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2573559656>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAKAPK245PH7JUM3DW6AC7D2JK3L7AVCNFSM6AAAAABQTWNUISVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZTGU2TSNRVGY>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 7, 21, 19, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600881190, 'issue_id': 2614694737, 'author': 'medaminezghal', 'body': 'I’m using Arch Linux and I can’t update keras to the latest version due to its dependency of tensorflow that is still unusable because it isn’t supported in python 3.13.\nSo I need to wait until the team make it compatible with python 3.13 to use keras updated.\nThat’s every year problem. 😑', 'created_at': datetime.datetime(2025, 1, 19, 14, 19, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600943311, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'Or you can install python3.12 on Arch, you can have multiple versions in parallel.', 'created_at': datetime.datetime(2025, 1, 19, 17, 1, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611309338, 'issue_id': 2614694737, 'author': 'xatier', 'body': '@medaminezghal, you can simply `$ yay python312` to install Python 3.12 on Arch to the path `/usr/bin/python3.12`, just FYI.\n\nRef: https://wiki.archlinux.org/title/Python#Other_versions', 'created_at': datetime.datetime(2025, 1, 24, 0, 46, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611327230, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': 'Also there are tools (like `hatch`) that manage Python installation for you, so ""I cannot use it on $platform because it comes with Python 3.13 by default"" arguments don\'t stand.', 'created_at': datetime.datetime(2025, 1, 24, 1, 3, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620947623, 'issue_id': 2614694737, 'author': 'ieshaan12', 'body': ""Are there any release candidates for the same? Potentially, some release candidates, I think it breaks a lot of builds where the default python is 3.13. I'm fine downgrading, but its a bit of a hassle nonetheless."", 'created_at': datetime.datetime(2025, 1, 29, 8, 10, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622203557, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""It doesn't seem to be in nightly yet. And there is no new branch already cut to signal starting a new release (which would be a prerequisite for RCs)."", 'created_at': datetime.datetime(2025, 1, 29, 16, 59, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629421899, 'issue_id': 2614694737, 'author': 'VedeshP', 'body': 'so is there any other way to install tensorflow as i am using a virtual environment and i have other dependencies already installed - some code already written - if i have to revert and make a new env with older python version it will be a tedious task', 'created_at': datetime.datetime(2025, 2, 2, 14, 36, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629425721, 'issue_id': 2614694737, 'author': 'xatier', 'body': ""@VedeshP why don't you dump the venv with `pip freeze` to a `requirementx.txt` file? You may also use `pipreqs` if you want leaf packages only."", 'created_at': datetime.datetime(2025, 2, 2, 14, 46, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629463399, 'issue_id': 2614694737, 'author': 'mihaimaruseac', 'body': ""It's really not tedious at all, `pip freeze > requirements.txt` and then in the new environment `pip install -r requirements.txt`"", 'created_at': datetime.datetime(2025, 2, 2, 16, 32, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629732255, 'issue_id': 2614694737, 'author': 'VedeshP', 'body': ""@xatier @mihaimaruseac \nThat's a good idea, never thought about this \nThank you \nThere's one more question \nShall I use the jupyter kernel made from the global python installed in my computer?"", 'created_at': datetime.datetime(2025, 2, 3, 2, 4, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629811542, 'issue_id': 2614694737, 'author': 'xatier', 'body': '@VedeshP please do not go off-topic. You should do your homework on Stack Overflow or other forums instead.', 'created_at': datetime.datetime(2025, 2, 3, 3, 24, 45, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-10-26 12:34:49 UTC): TensorFlow always supports the new Python version in the release that follows nearly a quarter after the Python release. This is because the TensorFlow release cycle starts before the Python release occurs and cannot be changed mid-flight. If you look at past issues, you see that this has been an issue since at least python 3.8.

It has been suggested to test release candidates, etc., but no improvements have been made on this front.

BaseMax on (2024-11-04 22:24:44 UTC): Same issue

I am going to uninstall Python 3.13 and install an older python version.

muayyad-alsadi on (2024-11-14 13:26:59 UTC): python 3.13 is important because it's the default and only version in the latest fedora 41 (released 2024-10-29)
fedora is a major distro and many users are upgrading to the latest version.

I don't expect tf to support every distro. but python 3.13 is the latest released stable version from python foundation and is used by a major distro. it would be a shame not to support it. I believe we need to adjust or sync the process.

those releases are scheduled and predictable

iseeface on (2024-11-25 12:02:29 UTC): still no fix?

mihaimaruseac on (2024-11-25 14:20:03 UTC): This is still true. Until the next release next year, there won't be a change.

yuchen001 on (2024-12-02 23:53:43 UTC): In addition to Python 3.13 not being supported, could someone confirm if Python 3.12 is currently supported?

mihaimaruseac on (2024-12-03 09:51:57 UTC): It should be (3.12)

tbitai on (2024-12-03 16:18:06 UTC): According to the [install docs](https://www.tensorflow.org/install/), it's

thebaptiste on (2024-12-04 01:31:11 UTC): According to the available wheels on PyPI, it seems rather to be 3.9-3.12 since release 2.16.1
May be the install doc has not been updated...

muayyad-alsadi on (2024-12-05 06:37:07 UTC): we just need to do the following
when a new stable python is released by PSF just get last tag and rebuild it and push the wheel for that python version.
there is nothing preventing us from doing this simple automation and I'm not aware of any compatibility issue preventing us from doing this. the current state seems careless, lazy and incompetent.

mihaimaruseac on (2024-12-05 08:15:32 UTC): Unfortunately the build system for TF is too complicated for this.

There needs to be a lot of work done to make that possible. Plus, TF has a large number of dependencies that also need to start supporting the new version of Python. And, C++ <-> Python interop has a lot of issues with any new version of Python.

muayyad-alsadi on (2024-12-22 13:47:51 UTC): would you please point to the CI/CD failed build logs showing the claimed incompatibility? I'm not aware of any. This is not compatibility issue but rather process issue.

Also related:

https://github.com/tensorflow/tensorflow/issues/79349

svenstaro on (2024-12-22 19:52:02 UTC): Here are my build logs: [tensorflow-python313.txt](https://github.com/user-attachments/files/18223075/tensorflow-python313.txt)

muayyad-alsadi on (2024-12-23 09:17:23 UTC): ```
Error in fail: 
Could not find requirements_lock.txt file matching specified Python version.
Specified python version: 3.13
Python versions with available requirement_lock.txt files: 3.9, 3.10, 3.11, 3.12
```

you must be kidding, let's have mutual respect, I can read and this is not the actual build log this is just an early exist resulted from a pinned version that can patched by a single-line change. basically it says ""we don't want you to to try to build it on 3.13"". Pinning the versions happens after a successful build not before it. It's a political decision not a technical barrier.

the current status of tensorflow is unacceptable as it's uninstallable on windows, macos and latest fedora linux all with latest stable python from python foundation.

svenstaro on (2024-12-23 10:44:10 UTC): So, two things:
- This issue is about tensorflow not supporting 3.13. If you read the error message, it quite clearly doesn't support 3.13.
- If you try to actually patch the files (as we tried in Arch), you'll see various failures in parts of tensorflow. I can get you some logs but you can also try it yourself.

mihaimaruseac on (2024-12-23 14:29:38 UTC): I no longer work on TF (left in 2022, but still helped with triage and some issues). From my time leading the OSS build team, the issues were usually some dependencies that also needed to be updated. TF has quite a large number of them, and some still need C++/Python interop. It looks like some of those have been going through removal now, so maybe this is not as bad.

Then, TF itself uses C++ Python interop and that is tied to the Python versions supported. It might be that for 3.13 there's nothing to change, but in general we had to. Similarly, for numpy compat.

If I find some time over these two weeks, I'll try to get a build started, but since I'm not in the team the only way I can contribute is the same as you: PRs welcome.

mihaimaruseac on (2024-12-23 19:31:36 UTC): For the last release of TF (2.18), bramnch cut ocurred slightly before Sept 25 (given https://github.com/tensorflow/tensorflow/commit/b4b3e2da6b64a996f418810524a3274a6bef7fa5 to update version number to RC0).

But on Sept 25, `grpcio` was at 1.66.1, which has [no Python 3.13 support](https://pypi.org/project/grpcio/1.66.1/#files).

This is just the first dependency in the chain that needed to be updated before TF is able to support a new version of Python. I didn't check [any other of the direct dependencies from deps.dev](https://deps.dev/pypi/tensorflow/2.18.0/dependencies) (note that the dependency support versions listed there are not the ones available when the branch was cut, not the ones for RC0).

Because the release process for TF is expensive, once a branch is cut, it can only receive minor cherry-picks, not features from master branch.

That being said, `tf-nightly` should have support for a new version of Python much earlier than it does now, as soon as all dependencies support the new version of Python, TF should be able to do so too. But it is now a requirement on the planning for the OSS team, I am no longer TL there and won't make planning statements.

jyo64 on (2024-12-25 18:04:31 UTC): Tensorflow support for Python 3.13 is a deal breaker now. Even where project that have tensorflow as a dependency 😢.

jyo64 on (2024-12-25 18:28:28 UTC): I just tried installing all the dependencies manually as mentioned here. The only thing that has failed is tensorflow-io-gcs-filesystem. I think grpcio (the latest version) now supports Python 3.13.

muayyad-alsadi on (2024-12-26 14:00:09 UTC): you are correct

![image](https://github.com/user-attachments/assets/9ae6face-7596-49e5-9f6e-a87c98e584b8)

which further indicate it's a political decision within tensorflow team (tensorflow-io-gcs-filesystem) not a technical one.
it's a matter of flipping a switch.

mihaimaruseac on (2024-12-26 14:44:56 UTC): grpcio support for 3.13 was added after TF branch cut.

I already explained several times that it is expensive to change the build process once the release train has started.

muayyad-alsadi on (2024-12-26 14:59:36 UTC): which is what I've said, there is no breaking changes introduced by PSF's 3.13 
it's just careless, lazy and incompetent process within TF team.
you have an established process and you refuse to fix it.

I have no problem I missed the train, the problem that next train is scheduled to be after 6 months. this is not how infra should be built.

you know when PSF plan their python release, you schedule a ""train"" when it's in RC phase so that when it's release you release a new build.

mihaimaruseac on (2024-12-26 15:06:07 UTC): Google has its own requirements too, it's not ""you can just schedule"".

You can also send PRs, the build process is 90% in the repo itself.

mihaimaruseac on (2024-12-26 18:31:46 UTC): There are actually Python 3.13 features that require more work from TF:

- the new free-threaded runtime: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython. Since TF cannot know if it is called from the new runtime or not, we need to ensure that both are supported. But this entails making sure that there are no incompatibilities with the way TF kernels have been written, there might be assumptions that are broken. Hence, this needs a longer baking in period than ""just"" releasing.
- the new JIT compiler: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython. TF already does a lot of JIT via XLA.
- there are `ctypes` changes that we need to look into: https://docs.python.org/3/whatsnew/3.13.html#free-threaded-cpython

This is on a quick glance on the release notes. I might have missed some other changes and maybe not all of the above are true, TF is too big and I barely scratched the entire surface of it.

mihaimaruseac on (2024-12-26 18:33:13 UTC): That's not true. Releases are quarterly, not biannually.

muayyad-alsadi on (2024-12-26 19:05:07 UTC): All of those are runtime not build time. Supporting some runtimes is better
than supporting zero runtimes.

On Thu, Dec 26, 2024, 9:33 PM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2024-12-26 19:08:01 UTC): Yes, and TF supports Python 3.12, 3.11 and so on. More runtimes by February.

muayyad-alsadi on (2024-12-26 19:16:10 UTC): I was referring to ""...the new free-threaded runtime... the new JIT
compiler""

We did not push wheels for py 3.13 because it can be called in an
unsupported rt. Well! If i call it in the new rt it would be my problem.
I'll be sure to call it in one of the supported ways.

The current state in case you don't know there is no usable wheels for
Windows, mac or fedora linux, arch linux ..etc.

How to reproduce? get the latest stable os from any of those vendors,
install the latest stable python. Try to install TF.

On Thu, Dec 26, 2024, 10:08 PM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2024-12-26 19:23:21 UTC): NixOS 24.11 is on Python 3.12 (by default, but supports even 3.14a). Fedora 41 released with Python 3.13 by default, but it does not block you from installing a previous version. `hatch`, `uv`, `venv` are all tools you could use to manage python installations.

mihaimaruseac on (2024-12-26 19:28:55 UTC): To be clear, I am still of the opinion that `tf-nightly` should already support 3.13, it's not really ok to delay adding support to it for so long.

nbecker on (2025-01-02 18:02:54 UTC): There is python 3.12 on Fedora 41, 
sudo dnf install python3.12

muayyad-alsadi on (2025-01-02 18:10:17 UTC): This reminds me of nvidia/ubuntu mentality. Canonical had one kernel
developer and his only job is to make sure they ship a kernel that works
with nvidia drivers, if not they ship the older kernel.

As long as there are no breaking changes done by PSF in their latest stable
python, we should build and publish the wheels. We should not hold
innovation waiting for church approval.

On Thu, Jan 2, 2025, 9:03 PM ndbecker ***@***.***> wrote:

ilcesko on (2025-01-03 09:02:00 UTC): I read the whole discussion but I couldn't find an actual ETA for 3.13 support, do we have one ?
thanks

mihaimaruseac on (2025-01-03 14:28:40 UTC): Next release of TF. Branch cut should occur by mid of February at most and final release at most a month after, assuming no delays.

muayyad-alsadi on (2025-01-05 13:16:55 UTC): it seems that church priests keep down voting me

![image](https://github.com/user-attachments/assets/74bfe9af-fa7f-40c5-b611-2ee97f8e64a3)

and ignore the community

![image](https://github.com/user-attachments/assets/e9af7f2d-29ed-4fa2-81a6-4f9b1e7ae4b8)

 
This is not the first time this happens, we had such tickets before and we will continue to have them when PSF release python 3.14

supporting python 3.13 is not the root cause of the problem.
there should be a way within TF church to request a build of the current stable of version of TF (no new features) against the current stable version of python without version pinning.


yes, should have, but did not.

mihaimaruseac on (2025-01-06 17:19:33 UTC): Apologies for the wrong signal sent by :-1:. I wanted a quick feedback to signal that you are repeating the same information, in a way that could be construed as aggressive and against [the code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).

This is not Reddit or similar, so a :-1: has no effect (I got a few myself too, even here, including from you). But, taking into account your feedback, I have removed it.

Regarding the rest of your message, I can only reply by repeating what I have already said. In the spirit of making progress here, I will just summarize:

- Stable release of new Python version is impossible to add after branch cut
- Testing RCs of new Python version just before release train starts risks delaying releases for everyone and then can lead to issues down the line (e.g., Python 3.11 docker images for some TF versions are on an RC version even now because of this)
- Release calendar has to take into account multiple priorities and coding freezes. It cannot be shifted around to match the Python release schedule.
- Nightly support of new Python version can be added, but in general it gets delayed. It is the responsibility of the OSS team to do so and they have to juggle quite a large number of priorities. Please assume the best intentions, there is no one that wants the community to not be supported
- I have left the TF OSS team in 2022 and have been only helping here and there to smooth the transition. I do not represent the TF team and I don't know why you are picking on me. I'm just responding to the issue rather than let it stale.

Please make sure to read the code of conduct.

muayyad-alsadi on (2025-01-07 21:19:19 UTC): Code of conduct? Church here is not a religious reference!! It's a
hierarchical management reference used by Eric S. Raymond in his book ""The
Cathedral and the Bazaar"" (abbreviated CatB) which is one of the
foundations of OpenSource community (he is the one behind OSI definition).

How come you did not recognize the reference to CatB and OSI? Never heard
of it? Am I that old?

https://en.m.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar

On Mon, Jan 6, 2025, 8:19 PM Mihai Maruseac ***@***.***>
wrote:

medaminezghal on (2025-01-19 14:19:14 UTC): I’m using Arch Linux and I can’t update keras to the latest version due to its dependency of tensorflow that is still unusable because it isn’t supported in python 3.13.
So I need to wait until the team make it compatible with python 3.13 to use keras updated.
That’s every year problem. 😑

mihaimaruseac on (2025-01-19 17:01:09 UTC): Or you can install python3.12 on Arch, you can have multiple versions in parallel.

xatier on (2025-01-24 00:46:27 UTC): @medaminezghal, you can simply `$ yay python312` to install Python 3.12 on Arch to the path `/usr/bin/python3.12`, just FYI.

Ref: https://wiki.archlinux.org/title/Python#Other_versions

mihaimaruseac on (2025-01-24 01:03:33 UTC): Also there are tools (like `hatch`) that manage Python installation for you, so ""I cannot use it on $platform because it comes with Python 3.13 by default"" arguments don't stand.

ieshaan12 on (2025-01-29 08:10:34 UTC): Are there any release candidates for the same? Potentially, some release candidates, I think it breaks a lot of builds where the default python is 3.13. I'm fine downgrading, but its a bit of a hassle nonetheless.

mihaimaruseac on (2025-01-29 16:59:41 UTC): It doesn't seem to be in nightly yet. And there is no new branch already cut to signal starting a new release (which would be a prerequisite for RCs).

VedeshP on (2025-02-02 14:36:05 UTC): so is there any other way to install tensorflow as i am using a virtual environment and i have other dependencies already installed - some code already written - if i have to revert and make a new env with older python version it will be a tedious task

xatier on (2025-02-02 14:46:19 UTC): @VedeshP why don't you dump the venv with `pip freeze` to a `requirementx.txt` file? You may also use `pipreqs` if you want leaf packages only.

mihaimaruseac on (2025-02-02 16:32:38 UTC): It's really not tedious at all, `pip freeze > requirements.txt` and then in the new environment `pip install -r requirements.txt`

VedeshP on (2025-02-03 02:04:35 UTC): @xatier @mihaimaruseac 
That's a good idea, never thought about this 
Thank you 
There's one more question 
Shall I use the jupyter kernel made from the global python installed in my computer?

xatier on (2025-02-03 03:24:45 UTC): @VedeshP please do not go off-topic. You should do your homework on Stack Overflow or other forums instead.

"
2613999243,issue,closed,completed,"When the user sets a different precision in the model definition, TF should give a corresponding warning","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While TensorFlow supports mixed precision, some warning or hint should be introduced when user code arbitrarily specifies the dtype of different layers (e.g. float16, float32, float64) as in the example you provided. For example, a hint that TensorFlow may automatically convert between dtypes (e.g., from float16 to float32) could lead to performance loss and debugging difficulties.
I think these approaches can help to improve User Experience ：
1. API Warnings and Type Checking:
TensorFlow could introduce runtime warnings or validation checks when users specify inconsistent data types (e.g., float16 with float64 in the same model). This would prevent unintended type mixing and notify users about potential performance degradation or numerical instability.

2. Enhanced Documentation and Error Messages: TensorFlow’s documentation could highlight the potential pitfalls of combining multiple data types, such as the impact of frequent implicit conversions on both performance and numerical stability. Error messages could provide actionable suggestions when detecting potentially problematic type mixing, guiding users to adjust their code more effectively.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import os

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(input):
    input = tf.keras.Input(shape=input)
    _input = input
    _zeropadding_input = tf.keras.layers.ZeroPadding2D(padding=((0, 0), (0, 0)))(input)
    conv1_output = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", output_padding=(0, 0), data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True, dtype=tf.float16, name=""conv1_mutated"")(input)
    output_transpose = [(0), (0, 1), (0, 2, 1), (0, 3, 1, 2), (0, 4, 1, 2, 3)]
    conv1_output = tf.transpose(conv1_output, list(output_transpose[(len(conv1_output.shape) - 1)]))
    tail_flatten_output = tf.keras.layers.Flatten(name=""tail_flatten"", dtype=tf.float32)(conv1_output)
    tail_fc_output = tf.keras.layers.Dense(units=10, use_bias=True, dtype=tf.float64, name=""tail_fc"")(tail_flatten_output)

    tail_fc_output = tail_fc_output
    model = tf.keras.models.Model(inputs=_input, outputs=tail_fc_output)
    return model


inp = np.random.random([1, 1, 28, 28])
tf_input = tf.convert_to_tensor(inp.transpose(0, 2, 3, 1))
tf_model = Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(tf_input.shape[1:])
tf_output = tf_model(tf_input)
print(tf_output)

Output:
```
tf.Tensor(
[[-0.11903492  0.12290937  0.11915011  0.07089125 -0.33258601  0.46096263
   0.3098348   0.115148   -0.46920742  0.23619687]], shape=(1, 10), dtype=float64)
​
```
```


### Relevant log output

_No response_",PhyllisJi,2024-10-25 12:39:57+00:00,['tilakrayal'],2024-11-23 02:02:45+00:00,2024-11-23 02:02:45+00:00,https://github.com/tensorflow/tensorflow/issues/78761,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2439589588, 'issue_id': 2613999243, 'author': 'kamisama-coder', 'body': 'class CustomModel(tf.keras.Model):\r\n    def compile(self, enforce_dtype_consistency=False, *args, **kwargs):\r\n        dtypes = {layer.dtype for layer in self.layers if hasattr(layer, \'dtype\')}\r\n        \r\n        if len(dtypes) > 1:\r\n            inconsistent_layers = [layer.name for layer in self.layers if hasattr(layer, \'dtype\') and layer.dtype in dtypes]\r\n            warning_message = (\r\n                f""Inconsistent dtypes detected in model layers: {dtypes}. ""\r\n                ""Mixing dtypes (e.g., float16 with float64) can lead to performance issues and numerical instability. ""\r\n                ""Layers with inconsistent dtypes include: "" + "", "".join(inconsistent_layers)\r\n            )\r\n            if enforce_dtype_consistency:\r\n                unified_dtype = kwargs.get(""dtype"", ""float32"")\r\n                for layer in self.layers:\r\n                    if hasattr(layer, \'dtype\') and layer.dtype != unified_dtype:\r\n                        layer.dtype = unified_dtype\r\n                warnings.warn(""Dtypes enforced to be consistent across layers with dtype: "" + unified_dtype)\r\n            else:\r\n                warnings.warn(warning_message)\r\n        \r\n        super().compile(*args, **kwargs)                                                                                                                                                          created the CustomModel class', 'created_at': datetime.datetime(2024, 10, 26, 13, 47, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439862583, 'issue_id': 2613999243, 'author': 'PhyllisJi', 'body': '> class CustomModel(tf.keras.Model): def compile(self, enforce_dtype_consistency=False, *args, **kwargs): dtypes = {layer.dtype for layer in self.layers if hasattr(layer, \'dtype\')}\r\n> \r\n> ```\r\n>     if len(dtypes) > 1:\r\n>         inconsistent_layers = [layer.name for layer in self.layers if hasattr(layer, \'dtype\') and layer.dtype in dtypes]\r\n>         warning_message = (\r\n>             f""Inconsistent dtypes detected in model layers: {dtypes}. ""\r\n>             ""Mixing dtypes (e.g., float16 with float64) can lead to performance issues and numerical instability. ""\r\n>             ""Layers with inconsistent dtypes include: "" + "", "".join(inconsistent_layers)\r\n>         )\r\n>         if enforce_dtype_consistency:\r\n>             unified_dtype = kwargs.get(""dtype"", ""float32"")\r\n>             for layer in self.layers:\r\n>                 if hasattr(layer, \'dtype\') and layer.dtype != unified_dtype:\r\n>                     layer.dtype = unified_dtype\r\n>             warnings.warn(""Dtypes enforced to be consistent across layers with dtype: "" + unified_dtype)\r\n>         else:\r\n>             warnings.warn(warning_message)\r\n>     \r\n>     super().compile(*args, **kwargs)                                                                                                                                                          created the CustomModel class\r\n> ```\r\n\r\nSorry, I don\'t understand.', 'created_at': datetime.datetime(2024, 10, 27, 5, 36, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440799622, 'issue_id': 2613999243, 'author': 'tilakrayal', 'body': '@PhyllisJi,\r\nI tried to execute the tensorflow v2.17 which contains Keras3.0, and observed that the code was executed with the expected error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a015765f11b0ffd6e56042b2f5f142bf/untitled2206.ipynb). The tensorflow V2.15 is pretty old which consists of  Keras2.0 version. Thank you!', 'created_at': datetime.datetime(2024, 10, 28, 7, 59, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441023151, 'issue_id': 2613999243, 'author': 'PhyllisJi', 'body': '> @PhyllisJi, I tried to execute the tensorflow v2.17 which contains Keras3.0, and observed that the code was executed with the expected error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a015765f11b0ffd6e56042b2f5f142bf/untitled2206.ipynb). The tensorflow V2.15 is pretty old which consists of Keras2.0 version. Thank you!\r\n\r\nyou can try this code on tf 2.17, It works fine and gives float64 results. \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\ntf.random.set_seed(42)\r\ntf.config.experimental.enable_op_determinism()\r\n\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\r\n\r\ndef Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(input):\r\n    input = tf.keras.Input(shape=input)\r\n    _input = input\r\n    _zeropadding_input = tf.keras.layers.ZeroPadding2D(padding=((0, 0), (0, 0)))(input)\r\n    conv1_output = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True, dtype=tf.float16, name=""conv1_mutated"")(input)\r\n    output_transpose = [(0), (0, 1), (0, 2, 1), (0, 3, 1, 2), (0, 4, 1, 2, 3)]\r\n    tail_flatten_output = tf.keras.layers.Flatten(name=""tail_flatten"", dtype=tf.float32)(conv1_output)\r\n    tail_fc_output = tf.keras.layers.Dense(units=10, use_bias=True, dtype=tf.float64, name=""tail_fc"")(tail_flatten_output)\r\n\r\n    tail_fc_output = tail_fc_output\r\n    model = tf.keras.models.Model(inputs=_input, outputs=tail_fc_output)\r\n    return model\r\n\r\ninp = np.random.random([1, 1, 28, 28])\r\ntf_input = tf.convert_to_tensor(inp.transpose(0, 2, 3, 1))\r\ntf_model = Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(tf_input.shape[1:])\r\ntf_output = tf_model(tf_input)\r\nprint(tf_output)\r\n```\r\nWhat I find strange is that the new version removes output_padding, but checks for its validity and throws the following exception: \r\n```\r\nThe `output_padding` argument must be a tuple of 2 integers. Received output_padding=(0, 0), including values {0} that do not satisfy `value > 0`. including values {0} that do not satisfy `value > 0`. \r\n```\r\nThe older version we use does not remove output_padding, but does not check for its validity. And the constraints in the error are not mentioned in the documentation for tf 2.14.0', 'created_at': datetime.datetime(2024, 10, 28, 9, 22, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461391527, 'issue_id': 2613999243, 'author': 'tilakrayal', 'body': '@PhyllisJi,\r\nHave you tried with the latest tensorflow v2.18? If not, please try with the stable version and provide if you are facing the same issue/error. Thank you!', 'created_at': datetime.datetime(2024, 11, 7, 5, 57, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805504, 'issue_id': 2613999243, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212216, 'issue_id': 2613999243, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 44, tzinfo=datetime.timezone.utc)}]","kamisama-coder on (2024-10-26 13:47:05 UTC): class CustomModel(tf.keras.Model):
    def compile(self, enforce_dtype_consistency=False, *args, **kwargs):
        dtypes = {layer.dtype for layer in self.layers if hasattr(layer, 'dtype')}
        
        if len(dtypes) > 1:
            inconsistent_layers = [layer.name for layer in self.layers if hasattr(layer, 'dtype') and layer.dtype in dtypes]
            warning_message = (
                f""Inconsistent dtypes detected in model layers: {dtypes}. ""
                ""Mixing dtypes (e.g., float16 with float64) can lead to performance issues and numerical instability. ""
                ""Layers with inconsistent dtypes include: "" + "", "".join(inconsistent_layers)
            )
            if enforce_dtype_consistency:
                unified_dtype = kwargs.get(""dtype"", ""float32"")
                for layer in self.layers:
                    if hasattr(layer, 'dtype') and layer.dtype != unified_dtype:
                        layer.dtype = unified_dtype
                warnings.warn(""Dtypes enforced to be consistent across layers with dtype: "" + unified_dtype)
            else:
                warnings.warn(warning_message)
        
        super().compile(*args, **kwargs)                                                                                                                                                          created the CustomModel class

PhyllisJi (Issue Creator) on (2024-10-27 05:36:26 UTC): Sorry, I don't understand.

tilakrayal (Assginee) on (2024-10-28 07:59:18 UTC): @PhyllisJi,
I tried to execute the tensorflow v2.17 which contains Keras3.0, and observed that the code was executed with the expected error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a015765f11b0ffd6e56042b2f5f142bf/untitled2206.ipynb). The tensorflow V2.15 is pretty old which consists of  Keras2.0 version. Thank you!

PhyllisJi (Issue Creator) on (2024-10-28 09:22:45 UTC): you can try this code on tf 2.17, It works fine and gives float64 results. 
```python
import tensorflow as tf
import numpy as np
import os

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(input):
    input = tf.keras.Input(shape=input)
    _input = input
    _zeropadding_input = tf.keras.layers.ZeroPadding2D(padding=((0, 0), (0, 0)))(input)
    conv1_output = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True, dtype=tf.float16, name=""conv1_mutated"")(input)
    output_transpose = [(0), (0, 1), (0, 2, 1), (0, 3, 1, 2), (0, 4, 1, 2, 3)]
    tail_flatten_output = tf.keras.layers.Flatten(name=""tail_flatten"", dtype=tf.float32)(conv1_output)
    tail_fc_output = tf.keras.layers.Dense(units=10, use_bias=True, dtype=tf.float64, name=""tail_fc"")(tail_flatten_output)

    tail_fc_output = tail_fc_output
    model = tf.keras.models.Model(inputs=_input, outputs=tail_fc_output)
    return model

inp = np.random.random([1, 1, 28, 28])
tf_input = tf.convert_to_tensor(inp.transpose(0, 2, 3, 1))
tf_model = Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(tf_input.shape[1:])
tf_output = tf_model(tf_input)
print(tf_output)
```
What I find strange is that the new version removes output_padding, but checks for its validity and throws the following exception: 
```
The `output_padding` argument must be a tuple of 2 integers. Received output_padding=(0, 0), including values {0} that do not satisfy `value > 0`. including values {0} that do not satisfy `value > 0`. 
```
The older version we use does not remove output_padding, but does not check for its validity. And the constraints in the error are not mentioned in the documentation for tf 2.14.0

tilakrayal (Assginee) on (2024-11-07 05:57:58 UTC): @PhyllisJi,
Have you tried with the latest tensorflow v2.18? If not, please try with the stable version and provide if you are facing the same issue/error. Thank you!

github-actions[bot] on (2024-11-15 02:05:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2613636513,issue,closed,completed,tf.keras.Input layer performs implicit data conversion when dtype is not specified,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Since tf.keras.Input layer performs implicit data conversion when dtype is not specified, the model can still operate normally even if the input tensor is of bool type, whereas other frameworks will explicitly indicate that the user input data type does not match the data type of the weights. 
This feature is also not described in the documentation. https://www.tensorflow.org/api_docs/python/tf/keras/Input
As shown in the code, in TensorFlow, convolution-related operators (such as tf.nn.conv2d or tf.keras.layers.Conv2D) do not natively support bool type inputs. However, when a user provides a tensor of bool type, the framework performs an implicit type conversion, converting the boolean values to a supported numerical type (e.g., float32) without notifying the user. This implicit conversion can lead to several issues:

1. Unexpected Behavior: Users may mistakenly believe that convolution operators can process boolean data directly, unaware that the data has been converted, resulting in outcomes that do not align with their expectations.

2. Performance Overhead: Implicit type conversions introduce additional computational overhead, which can negatively impact performance, especially when handling large datasets.

3. Loss of Data Semantics: Converting bool values to numerical types (e.g., True to 1.0 and False to 0.0) may alter the original meaning of the data, potentially leading to incorrect inferences or training results.

4. Lack of Explicit Warnings: Since the framework does not warn users about the conversion, it can be difficult to identify and debug issues caused by unintended data type changes, affecting the reliability and maintainability of the model.

To mitigate these risks, it is recommended to introduce explicit type checks and error messages within the API. This would ensure that when an unsupported data type (such as bool) is provided, the framework immediately notifies the user, preventing unintended behavior and improving the clarity of TensorFlow’s operator behavior.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import os

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(input):
    input = tf.keras.Input(shape=input)
    print(input)
    _input = input
    conv1_output = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", output_padding=(0, 0), data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True, name=""conv1_mutated"")(input)
    model = tf.keras.models.Model(inputs=_input, outputs=conv1_output)
    return model


input_tensor = tf.random.uniform((1, 20, 3, 1), minval=0, maxval=2, dtype=tf.int32) > 0
tf_model = Model_WdZYze5JTnF3OjiUHVAKw_iFV2jkQLyL(input_tensor.shape[1:])
tf_output = tf_model(tf_input)
print(tf_output)

conv_transpose = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", output_padding=(0, 0), data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True)
cov_output = conv_transpose(input_tensor)
```


### Relevant log output

```shell
KerasTensor(type_spec=TensorSpec(shape=(None, 20, 3, 1), dtype=tf.float32, name='input_38'), name='input_38', description=""created by layer 'input_38'"")
tf.Tensor(
[[[[-7.40485862e-02  4.80350107e-02  7.99815208e-02 -4.41036373e-02
    -8.51454362e-02  1.62318498e-02]
   [-8.31062645e-02 -1.64438933e-01 -1.27500117e-01  1.00624129e-01
     3.34546417e-02  2.75535733e-02]
   [-1.96273416e-01  3.73793095e-02  1.25477001e-01  7.15976655e-02
    -1.04968920e-02 -1.54422924e-01]
   ...
   [-2.34454736e-01 -6.32669479e-02  7.60323107e-02  3.84105071e-02
     2.50659883e-01 -2.23536283e-01]
   [ 5.74071109e-02  1.42531544e-02  7.83358514e-03  1.72497943e-01
    -2.27022767e-02 -3.41754705e-02]
   [-1.12229913e-01 -5.26112467e-02  3.05368304e-02 -7.72907957e-02
     1.76011339e-01 -5.28815091e-02]]

  [[-7.56227598e-02  1.61828905e-01 -4.07438651e-02 -6.02847338e-03
    -1.76872984e-02 -1.39532030e-01]
   [-1.89279020e-01 -2.94572055e-01 -9.61521864e-02  7.07925856e-02
    -2.58026049e-02 -7.99539834e-02]
   [-4.37560976e-02 -6.63283467e-03 -8.16324800e-02  1.44223779e-01
     8.37134719e-02 -4.75612342e-01]
   ...
   [ 2.74093181e-01 -1.25522703e-01  1.30851418e-02 -2.42470428e-02
     1.46799758e-01 -9.69628543e-02]
   [-9.97005403e-03  1.31986856e-01 -1.31720647e-01  3.18675488e-03
    -8.20864737e-03  1.16216645e-01]
   [ 1.65766820e-01 -1.41832516e-01 -1.46382824e-01 -9.27735865e-03
     8.53385478e-02  1.66841403e-01]]

  [[ 1.02173470e-01  1.13967612e-01  2.87022665e-02 -3.41266394e-03
    -9.46889371e-02 -2.30735779e-01]
   [-3.66873085e-01 -3.19382042e-01 -2.55495101e-01  2.72331506e-01
    -1.18161269e-01 -1.09872714e-01]
   [-4.39989865e-02 -3.90910536e-01  1.62915736e-02 -9.38135237e-02
    -1.90863043e-01 -5.60778439e-01]
   ...
   [-7.09365085e-02 -8.18643421e-02 -4.83587027e-01  2.53073871e-02
    -7.42834732e-02 -1.70356929e-02]
   [ 1.98097110e-01 -3.59873474e-02  6.62651509e-02 -8.07644650e-02
     1.62230998e-01  1.17830709e-01]
   [-2.00704932e-01 -3.57379913e-02 -1.04680404e-01 -1.61345690e-01
     4.35111821e-02 -1.05926320e-01]]

  ...

  [[ 2.72406161e-01 -1.15699254e-01  5.33519685e-03  5.81376851e-02
    -2.22525075e-02 -6.04584739e-02]
   [ 1.86778411e-01 -1.63272589e-01  9.26669091e-02  6.83098435e-02
    -1.22894779e-01 -9.46379155e-02]
   [-2.92863995e-01 -3.90239120e-01  1.93792686e-01 -5.63800335e-02
    -3.73294979e-01 -1.53631270e-01]
   ...
   [ 6.70315847e-02 -3.92585546e-02 -3.23885053e-01 -2.71105677e-01
    -4.34245527e-01  1.10423237e-01]
   [ 9.87675264e-02  3.47033143e-04  8.33582431e-02 -1.04814813e-01
    -3.47234100e-01 -3.63905281e-02]
   [-8.84750113e-02  1.68732554e-02 -1.35217234e-01 -8.40548873e-02
    -1.32500157e-01 -5.30448109e-02]]

  [[ 2.72435993e-01 -9.19810012e-02 -6.43352866e-02  1.17616192e-01
     7.26721883e-02 -3.30447927e-02]
   [ 1.02649257e-02 -1.01452775e-01  1.17028326e-01 -7.20781535e-02
     1.56928256e-01 -4.90562171e-02]
   [ 9.31865275e-02 -1.70680910e-01  1.81988969e-01 -2.37688914e-01
     5.96717894e-02 -2.83467472e-01]
   ...
   [-4.82709706e-02 -3.13978195e-01  1.04269162e-01  2.43905872e-01
    -5.08092046e-01  9.15794671e-02]
   [-1.86816752e-02 -1.22914612e-01 -1.07511654e-01  1.79948762e-01
    -2.97979146e-01  2.44713187e-01]
   [ 1.09467044e-01 -5.24229556e-02 -7.40535706e-02  1.67156503e-01
    -1.21360570e-01  1.66974708e-01]]

  [[ 1.77826062e-01 -2.41430402e-02 -2.24351883e-04  6.20943159e-02
     1.79230571e-02 -6.37900755e-02]
   [ 8.63050297e-02 -1.05093494e-01  1.18852243e-01  9.43583250e-02
     2.41015851e-02 -7.81023428e-02]
   [ 2.07384348e-01 -2.24937975e-01  1.35406584e-01  4.29887921e-02
     4.81921434e-03 -1.95084155e-01]
   ...
   [ 2.53495753e-01 -3.35996687e-01 -1.04444146e-01  7.97981322e-02
    -1.90708429e-01 -1.37708083e-01]
   [ 1.32416457e-01 -2.16152206e-01 -1.20998487e-01  1.31167665e-01
    -1.71426058e-01 -2.07262784e-02]
   [ 1.00089446e-01 -1.60729483e-01  4.42979187e-02  1.45934328e-01
    -1.38317347e-01 -2.13920027e-02]]]], shape=(1, 24, 7, 6), dtype=float32)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[45], line 25
     23 # 定义一个 Conv2DTranspose 层
     24 conv_transpose = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=(5, 5), strides=(1, 1), padding=""valid"", output_padding=(0, 0), data_format=""channels_last"", dilation_rate=(1, 1), use_bias=True)
---> 25 cov_output = conv_transpose(input_tensor)

File ~/miniconda3/envs/myconda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniconda3/envs/myconda/lib/python3.10/site-packages/keras/src/backend.py:6270, in conv2d_transpose(x, kernel, output_shape, strides, padding, data_format, dilation_rate)
   6267     strides = (1, 1) + strides
   6269 if dilation_rate == (1, 1):
-> 6270     x = tf.compat.v1.nn.conv2d_transpose(
   6271         x,
   6272         kernel,
   6273         output_shape,
   6274         strides,
   6275         padding=padding,
   6276         data_format=tf_data_format,
   6277     )
   6278 else:
   6279     if dilation_rate[0] != dilation_rate[1]:

InvalidArgumentError: Exception encountered when calling layer 'conv2d_transpose_2' (type Conv2DTranspose).

cannot compute Conv2DBackpropInput as input #2(zero-based) was expected to be a float tensor but is a bool tensor [Op:Conv2DBackpropInput] name: 

Call arguments received by layer 'conv2d_transpose_2' (type Conv2DTranspose):
  • inputs=tf.Tensor(shape=(1, 20, 3, 1), dtype=bool)
```
We also tested inputs of type int and found that due to implicit conversions, the output often had ''nan''.",PhyllisJi,2024-10-25 09:53:29+00:00,['Venkat6871'],2024-11-28 02:06:45+00:00,2024-11-28 02:06:42+00:00,https://github.com/tensorflow/tensorflow/issues/78754,"[('type:docs-bug', 'Document issues'), ('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('type:performance', 'Performance Issue'), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2443280623, 'issue_id': 2613636513, 'author': 'Venkat6871', 'body': 'Hi **@PhyllisJi** ,\r\nApologies for the delay. I tried to running your code on colab using Tensorflow 2.17.0 and nightly version and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/8dfc7a92d949dbb338e62c91a45f0423/78754_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 29, 5, 55, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458567308, 'issue_id': 2613636513, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 6, 2, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461823434, 'issue_id': 2613636513, 'author': 'Vishesh-Mistry', 'body': 'can i work on fixing this issue?\r\nim new to the community and this could be a good starting point for me.', 'created_at': datetime.datetime(2024, 11, 7, 10, 8, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462086088, 'issue_id': 2613636513, 'author': 'PhyllisJi', 'body': '> can i work on fixing this issue? im new to the community and this could be a good starting point for me.\r\n\r\nIt would be really great if you could!', 'created_at': datetime.datetime(2024, 11, 7, 12, 14, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913776, 'issue_id': 2613636513, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124223, 'issue_id': 2613636513, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124287, 'issue_id': 2613636513, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78754"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78754"">No</a>', 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 45, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-29 05:55:22 UTC): Hi **@PhyllisJi** ,
Apologies for the delay. I tried to running your code on colab using Tensorflow 2.17.0 and nightly version and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/8dfc7a92d949dbb338e62c91a45f0423/78754_tf-2-17-0-nightly-v.ipynb) here for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.
Thank you!

github-actions[bot] on (2024-11-06 02:00:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

Vishesh-Mistry on (2024-11-07 10:08:54 UTC): can i work on fixing this issue?
im new to the community and this could be a good starting point for me.

PhyllisJi (Issue Creator) on (2024-11-07 12:14:15 UTC): It would be really great if you could!

github-actions[bot] on (2024-11-21 02:04:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-28 02:06:42 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-28 02:06:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78754"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78754"">No</a>

"
2613444429,issue,open,,Does TFLite dequantize opertor support constant buffer input,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
<img width=""724"" alt=""image"" src=""https://github.com/user-attachments/assets/849a0952-90ce-42c4-b2dc-9a9e7333fb0b"">
The [tflite model for user input](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_input.tflite) can work as expected

<img width=""713"" alt=""image"" src=""https://github.com/user-attachments/assets/43cafc84-0ea6-4a06-84b4-4772c6cea8e8"">

The [tflite model with constant buffer](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_constant.tflite) can't compute, so does [dequantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) support constant input?

**Any other info / logs**

You can also modify [dequantize_tester in the Repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/dequantize_tester.cc#L110) to reproduce this issue like below code:

1, Create a constant buffer
```
  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),
                                                                              constant_buffer.size());
  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());
  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));
```

2, Use the `buffer_index ` when creating tensor:
```
const std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,
          /*buffer=*/buffer_index , /*name=*/0,
          ::tflite::CreateQuantizationParameters(
              builder, /*min=*/0, /*max=*/0,
              builder.CreateVector<float>({InputScale()}),
              builder.CreateVector<int64_t>({InputZeroPoint()}))),
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          ::tflite::TensorType_FLOAT32,
          /*buffer=*/0, /*name=*/builder.CreateString(""dequantizeLinearOutput"")),
  }};

```",fujunwei,2024-10-25 08:28:42+00:00,"['paulinesho', 'pkgoogle']",2024-10-29 17:56:37+00:00,,https://github.com/tensorflow/tensorflow/issues/78748,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('ModelOptimizationToolkit', 'TF Model Optimization Toolkit'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437831171, 'issue_id': 2613444429, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 25, 13, 47, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438750138, 'issue_id': 2613444429, 'author': 'pkgoogle', 'body': ""I can confirm that it's not running properly:\r\n\r\n```sh\r\n$ ./benchmark_model --graph=dequantize_constant.tflite\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [dequantize_constant.tflite]\r\nINFO: Signature to run: []\r\nINFO: Loaded model dequantize_constant.tflite\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: The input model file size (MB): 0.000432\r\nINFO: Initialized session in 2.275ms.\r\nINFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nERROR: failed to setup XNNPACK runtime\r\nERROR: Node number 1 (TfLiteXNNPackDelegate) failed to invoke.\r\nINFO: count=2694653 first=25 curr=0 min=0 max=636 avg=0.0894939 std=0\r\n\r\nERROR: Benchmarking failed.\r\n```\r\n\r\n@paulinesho can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 10, 25, 20, 36, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441790671, 'issue_id': 2613444429, 'author': 'paulinesho', 'body': ""I'm not sure if the pasted snippet in the description is the proper way of feeding the dequantize node a constant. Could you try using `CreateTensor()` instead? Something like this https://github.com/tensorflow/tensorflow/blob/1fb34d9667642ee8046a97b3d5e4905fa402a9a0/tensorflow/lite/delegates/xnnpack/quantized_pad_tester.cc#L151.\r\n\r\nI believe the dequantize node expects its input to be a TFLite tensor (instead of a just a vector) hence could fail to parse a model like this."", 'created_at': datetime.datetime(2024, 10, 28, 14, 43, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443111793, 'issue_id': 2613444429, 'author': 'fujunwei', 'body': '[tf.pad](https://www.tensorflow.org/mlir/tfl_ops#operands_87) requires two input tensors,  so the `padding` is a TFLite tensor \r\n\r\n|Operand | Description|\r\n|-- | --|\r\n|input | tensor of 32-bit float or 32-bit signless integer or 64-bit signless integer or QI8 type or QUI8 type or TFLite quint8 type or QI16 type values|\r\n|padding | tensor of 32/64-bit signless integer values|\r\n\r\nbut [tf.quantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) only have one input tensor, so the constant data should be used with the `buffer_index` when creating tensor.\r\n\r\n|Operand | Description|\r\n|-- | --|\r\n|input | tensor of QI4 type or QI8 type or QUI8 type or QI16 type or 16-bit float values|', 'created_at': datetime.datetime(2024, 10, 29, 3, 13, 49, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-10-25 13:47:59 UTC): Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-25 20:36:48 UTC): I can confirm that it's not running properly:

```sh
$ ./benchmark_model --graph=dequantize_constant.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [dequantize_constant.tflite]
INFO: Signature to run: []
INFO: Loaded model dequantize_constant.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.000432
INFO: Initialized session in 2.275ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
ERROR: failed to setup XNNPACK runtime
ERROR: Node number 1 (TfLiteXNNPackDelegate) failed to invoke.
INFO: count=2694653 first=25 curr=0 min=0 max=636 avg=0.0894939 std=0

ERROR: Benchmarking failed.
```

@paulinesho can you please take a look? Thanks.

paulinesho (Assginee) on (2024-10-28 14:43:50 UTC): I'm not sure if the pasted snippet in the description is the proper way of feeding the dequantize node a constant. Could you try using `CreateTensor()` instead? Something like this https://github.com/tensorflow/tensorflow/blob/1fb34d9667642ee8046a97b3d5e4905fa402a9a0/tensorflow/lite/delegates/xnnpack/quantized_pad_tester.cc#L151.

I believe the dequantize node expects its input to be a TFLite tensor (instead of a just a vector) hence could fail to parse a model like this.

fujunwei (Issue Creator) on (2024-10-29 03:13:49 UTC): [tf.pad](https://www.tensorflow.org/mlir/tfl_ops#operands_87) requires two input tensors,  so the `padding` is a TFLite tensor 

|Operand | Description|
|-- | --|
|input | tensor of 32-bit float or 32-bit signless integer or 64-bit signless integer or QI8 type or QUI8 type or TFLite quint8 type or QI16 type values|
|padding | tensor of 32/64-bit signless integer values|

but [tf.quantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) only have one input tensor, so the constant data should be used with the `buffer_index` when creating tensor.

|Operand | Description|
|-- | --|
|input | tensor of QI4 type or QI8 type or QUI8 type or QI16 type or 16-bit float values|

"
2613143617,issue,closed,completed,`tf.io.encode_png` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20241023

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 22.19.0-dev20241023` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1A2ONnR2DN5-gx6_H0byFy8jk3ZmgwiUn?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import tensorflow
import numpy
    
image_tensor = tf.saturate_cast(tf.random.uniform([0, 28, 3], minval=0, maxval=256, dtype=tf.int64), dtype=tf.uint8)
image = tf.identity(image_tensor)
compression = -1
name = None
out = tf.io.encode_png(image=image,compression=compression,name=name,)
```


### Relevant log output

```shell
2024-10-25 05:46:33.066290: F tensorflow/core/lib/png/png_io.cc:350] 'image' Must be non NULL
Aborted (core dumped)
```
",cybersupersoap,2024-10-25 05:49:19+00:00,['Venkat6871'],2024-11-13 02:01:11+00:00,2024-11-13 02:01:09+00:00,https://github.com/tensorflow/tensorflow/issues/78735,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2441015190, 'issue_id': 2613143617, 'author': 'Venkat6871', 'body': 'Hi **@cybersupersoap** ,\r\nApologies for the delay. Thank you for reporting this issue. I suggest taking a look at issue [#76726](https://github.com/tensorflow/tensorflow/issues/76726), where a similar issue has been proposed and is still open. Following this related issue may also help you stay updated on potential solutions.\r\nI tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. I have provided an alternative solution that I hope will be helpful. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/125675589e3416610def4b0c67e37705/78735_tf-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 28, 9, 20, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456067097, 'issue_id': 2613143617, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 5, 2, 0, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472173383, 'issue_id': 2613143617, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472173536, 'issue_id': 2613143617, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78735"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78735"">No</a>', 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 11, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-28 09:20:43 UTC): Hi **@cybersupersoap** ,
Apologies for the delay. Thank you for reporting this issue. I suggest taking a look at issue [#76726](https://github.com/tensorflow/tensorflow/issues/76726), where a similar issue has been proposed and is still open. Following this related issue may also help you stay updated on potential solutions.
I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. I have provided an alternative solution that I hope will be helpful. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/125675589e3416610def4b0c67e37705/78735_tf-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-11-05 02:00:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-13 02:01:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-13 02:01:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78735"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78735"">No</a>

"
2613033557,issue,closed,completed,"aot_compile_cpu ValueError: Expected fully defined input shape for signature_def 'f_14821', tensor name: 'predict_f_14821'; but shape is: (None, 20)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to perform Ahead-of-Time (AOT) compilation when the source model has dynamic shapes? Should we first fix the shape? Do you have a demo for fixing the shape? How to fix the model's batch?

### Standalone code to reproduce the issue

```shell
saved_model_cli aot_compile_cpu --dir /xxxmodel --output_prefix /xxxmodel/aot_out/ --signature_def_key predict --cpp_class ""gg::GG"" --tag_set serve
```


### Relevant log output

```shell
File ""/usr/local/bin/saved_model_cli"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 1340, in main
    app.run(smcli_main)
  File ""/usr/local/lib/python3.11/dist-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.11/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
             ^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 1338, in smcli_main
    args.func()
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 1140, in aot_compile_cpu
    saved_model_aot_compile.aot_compile_cpu_meta_graph_def(
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_aot_compile.py"", line 374, in aot_compile_cpu_meta_graph_def
    frozen_graph_def_location, config_pbtxt_location = freeze_model(
                                                       ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_aot_compile.py"", line 251, in freeze_model
    _replace_input_placeholders_with_default_values(
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/tools/saved_model_aot_compile.py"", line 452, in _replace_input_placeholders_with_default_values
    raise ValueError(
ValueError: Expected fully defined input shape for signature_def 'f_2862', tensor name: 'predict_f_2862'; but shape is: (None, 4).
```
",LinGeLin,2024-10-25 04:20:42+00:00,['tilakrayal'],2024-11-07 07:25:17+00:00,2024-11-05 12:45:23+00:00,https://github.com/tensorflow/tensorflow/issues/78733,"[('type:support', 'Support issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2440648748, 'issue_id': 2613033557, 'author': 'tilakrayal', 'body': '@LinGeLin,\r\nCould you please provide the steps you are following and also the complete code to reproduce the issue which helps to debug the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 10, 28, 6, 19, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456067134, 'issue_id': 2613033557, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 5, 2, 0, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457074730, 'issue_id': 2613033557, 'author': 'LinGeLin', 'body': 'Thank you for your reply. Nothing is impossible in the world, as long as you are willing to give up. I have given up.', 'created_at': datetime.datetime(2024, 11, 5, 12, 45, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457075299, 'issue_id': 2613033557, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78733"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78733"">No</a>', 'created_at': datetime.datetime(2024, 11, 5, 12, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461500879, 'issue_id': 2613033557, 'author': 'xiangpdu', 'body': 'Meet the same issue, is there any solution to freeze a model with dynamic shape input?', 'created_at': datetime.datetime(2024, 11, 7, 7, 25, 16, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-28 06:19:23 UTC): @LinGeLin,
Could you please provide the steps you are following and also the complete code to reproduce the issue which helps to debug the issue in an effective way. Thank you!

github-actions[bot] on (2024-11-05 02:00:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

LinGeLin (Issue Creator) on (2024-11-05 12:45:09 UTC): Thank you for your reply. Nothing is impossible in the world, as long as you are willing to give up. I have given up.

google-ml-butler[bot] on (2024-11-05 12:45:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78733"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78733"">No</a>

xiangpdu on (2024-11-07 07:25:16 UTC): Meet the same issue, is there any solution to freeze a model with dynamic shape input?

"
2612231102,issue,closed,completed,AttributeError: 'ModelCheckpoint' object has no attribute '_implements_train_batch_hooks' in MoViNet Streaming Model Training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an AttributeError while trying to fit the model using the MoViNet Streaming Model Training and Inference notebook provided in the TensorFlow Model Garden repository. The specific error message is as follows:
![image](https://github.com/user-attachments/assets/91b77694-ff6e-4cc7-8f87-1a481dc45562)

in addition, there is a minor adjustment needed to be done in this code:
![image](https://github.com/user-attachments/assets/e6875949-0e4d-4cf0-8570-28e3816eea76)


### Standalone code to reproduce the issue

```shell
The issue can be reproduced using the notebook from the official TensorFlow Models GitHub repository:

https://github.com/tensorflow/models/blob/f9fdc4faef47af76351204b6d8df576f0e79baab/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
<ipython-input-18-49dc73e1e0f6> in <cell line: 1>()
----> 1 results = model.fit(train_ds,
      2                     validation_data=val_ds,
      3                     epochs=2,
      4                     validation_freq=1,
      5                     verbose=1,

1 frames
/usr/local/lib/python3.10/dist-packages/tf_keras/src/callbacks.py in <genexpr>(.0)
    243             getattr(cb, ""_supports_tf_logs"", False)
    244             for cb in self.callbacks
--> 245             if cb._implements_train_batch_hooks()
    246             or cb._implements_test_batch_hooks()
    247             or cb._implements_predict_batch_hooks()

AttributeError: 'ModelCheckpoint' object has no attribute '_implements_train_batch_hooks'
```
",sailesh2710,2024-10-24 17:51:15+00:00,['Venkat6871'],2024-11-10 02:03:27+00:00,2024-11-10 02:03:24+00:00,https://github.com/tensorflow/tensorflow/issues/78694,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:model', 'Model related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437335085, 'issue_id': 2612231102, 'author': 'Venkat6871', 'body': 'Hi **@sailesh2710** ,\r\nThanks for raising your concern. The issue you are facing may be due to TensorFlow 2.17.0 automatically using Keras 3. This can cause compatibility errors. To resolve this, you can import Keras separately, which will avoid this issue.\r\nPlease make these changes:\r\nReplace:\r\n```\r\nimport tensorflow as tf\r\n```\r\n\r\nwith:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tf_keras\r\n```\r\nUpdate all instances of `tf.keras` in your code to `tf_keras`.\r\n\r\nI have provided a [gist](https://colab.sandbox.google.com/gist/Venkat6871/5fe3e99f52771fb154608bda152d18c6/78694_tf-2-17-0-v.ipynb) here for reference. Let me know if this resolves the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 25, 9, 35, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452796970, 'issue_id': 2612231102, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547277, 'issue_id': 2612231102, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547304, 'issue_id': 2612231102, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78694"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78694"">No</a>', 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 26, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-25 09:35:22 UTC): Hi **@sailesh2710** ,
Thanks for raising your concern. The issue you are facing may be due to TensorFlow 2.17.0 automatically using Keras 3. This can cause compatibility errors. To resolve this, you can import Keras separately, which will avoid this issue.
Please make these changes:
Replace:
```
import tensorflow as tf
```

with:
```
import tensorflow as tf
import tensorflow.keras as tf_keras
```
Update all instances of `tf.keras` in your code to `tf_keras`.

I have provided a [gist](https://colab.sandbox.google.com/gist/Venkat6871/5fe3e99f52771fb154608bda152d18c6/78694_tf-2-17-0-v.ipynb) here for reference. Let me know if this resolves the issue.
Thank you!

github-actions[bot] on (2024-11-02 02:00:44 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-10 02:03:23 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-10 02:03:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78694"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78694"">No</a>

"
2610790633,issue,closed,completed,"java tensorflow and maven version is org.tensorflow:libtensorflow:1.15.0  after session.runner.run() for many times, the memory grow higher and higher then oom","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:libtensorflow:1.15.0

### Custom code

Yes

### OS platform and distribution

linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 

this is memory behavior
![20241024-155157](https://github.com/user-attachments/assets/8ff1d54e-0530-4213-9849-08372257a779)


### Standalone code to reproduce the issue

```shell
my test code with language scala ,this is one predict, we will predict 100 QPS for a docker   
 val config = ConfigProto.newBuilder
      .putDeviceCount(""CPU"",  Runtime.getRuntime.availableProcessors)
      .setInterOpParallelismThreads(8) 
      .setIntraOpParallelismThreads(8)
      .setOperationTimeoutInMs(3000)
      .build

    val options = RunOptions.newBuilder
      .setTimeoutInMs(5000)
      .build 

    val modelBundle = SavedModelBundle
      .loader(s""$path"")
      .withTags(""serve"")
      .withConfigProto(config.toByteArray)
      .withRunOptions(options.toByteArray)
      .load
    
   val kernel = modelBundle.session
   
    val data = Map(""tensor1"" -> Seq(0.1f,0.122f),……)
    val runner = kernel.runner()
    val inputTensorList: util.ArrayList[Tensor[java.lang.Float]] = new util.ArrayList[Tensor[java.lang.Float]]()
    data.map{
      case (tensorName, featureId) => {

        val dataInput:FloatBuffer = FloatBuffer.allocate(featureId.size)
        featureId.foreach(featureValue => {
          dataInput.put(featureValue)
        })
        dataInput.asInstanceOf[Buffer].flip()
        val tensorShape:Array[Long] = Array(1,featureId.size)
        val tensor = Tensor.create(tensorShape,dataInput)
        runner.feed(tensorName,tensor)
        inputTensorList.add(tensor)
      }
    }

    for(i <- 0 until 2 ){
      runner.fetch(""StatefulPartitionedCall"",i)
    }

    val output = runner.run.asScala
    val scores:Array[Float] = output.map(ten => {
      val tensorData: Array[Array[Float]] = ten.copyTo(Array.ofDim[Float](ten.shape()(0).toInt, ten.shape()(1).toInt))
      tensorData(0).head
    }).toArray
    inputTensorList.asScala.foreach(_.close())
    output.foreach(_.close())
```


### Relevant log output

_No response_",hanfengatonline,2024-10-24 08:00:42+00:00,['tilakrayal'],2024-11-11 02:01:44+00:00,2024-11-11 02:01:40+00:00,https://github.com/tensorflow/tensorflow/issues/78663,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 1.15', 'for issues seen on TF 1.15')]","[{'comment_id': 2439391986, 'issue_id': 2610790633, 'author': 'tilakrayal', 'body': '@hanfengatonline,\r\nThis issue is more related to tensorflow java. Could you please raise the request in the tensorflow/java for the quick resolution from [here](https://github.com/tensorflow/java). Thank you!', 'created_at': datetime.datetime(2024, 10, 26, 6, 35, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453261499, 'issue_id': 2610790633, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 3, 2, 5, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467087311, 'issue_id': 2610790633, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 11, 2, 1, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467087350, 'issue_id': 2610790633, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78663"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78663"">No</a>', 'created_at': datetime.datetime(2024, 11, 11, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-26 06:35:46 UTC): @hanfengatonline,
This issue is more related to tensorflow java. Could you please raise the request in the tensorflow/java for the quick resolution from [here](https://github.com/tensorflow/java). Thank you!

github-actions[bot] on (2024-11-03 02:05:38 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-11 02:01:40 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-11 02:01:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78663"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78663"">No</a>

"
2610725264,issue,open,,How to map ScatterElements with TFLite operator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
[WebNN scatterElements](https://source.chromium.org/chromium/chromium/src/+/main:services/webnn/public/mojom/webnn_graph.mojom;l=1028?q=webnn_graph.&ss=chromium%2Fchromium%2Fsrc)  operation first copies the values of `input` tensor to `output` tensor, and then overwrites the values of `output` tensor to values specified by `updates` tensor at specific index positions specified by `indices` tensor along `axis` dimension.

For example: Scatter elements along axis 0
```
 input = [[0.0, 0.0, 0.0],
          [0.0, 0.0, 0.0],
          [0.0, 0.0, 0.0]]
 indices = [[1, 0, 2],
            [0, 2, 1]]
 updates = [[1.0, 1.1, 1.2],
            [2.0, 2.1, 2.2]]
 output = [[2.0, 1.1, 0.0]
           [1.0, 0.0, 2.2]
           [0.0, 2.1, 1.2]]
```

there is no TFLite operator to map with scatterElements, can the [BuiltinOperator_STABLEHLO_SCATTER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1210;bpv=0;bpt=1)  implement the `scatterElements` operation?

**Any other info / logs**

WebNN scatterElements is similar with [ONNX ScatterElements](https://onnx.ai/onnx/operators/onnx__ScatterElements.html).
",fujunwei,2024-10-24 07:29:50+00:00,"['Ferev', 'gaikwadrahul8', 'pkgoogle']",2024-10-31 01:53:31+00:00,,https://github.com/tensorflow/tensorflow/issues/78661,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2441988967, 'issue_id': 2610725264, 'author': 'gaikwadrahul8', 'body': ""Hi, @fujunwei \r\n\r\nI apologize for the delayed response, As far I know TensorFlow Lite doesn't have a direct equivalent to the **ScatterElements** operation as found in **WebNN** or **ONNX** so most probably this will be considered as feature request. We do have [tf.scatter_nd](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/core/ScatterNd?hl=en) in TensorFlow core but it's missing implementation wise a copy of the input tensor rather than an empty or zero-filled tensor, axis specific operation and simplified indexing focusing on a single axis to make it equivalent to **WebNN scatterElements**\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 28, 15, 58, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447300607, 'issue_id': 2610725264, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 30, 14, 12, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448069016, 'issue_id': 2610725264, 'author': 'pkgoogle', 'body': ""I'm fairly sure there's a way, here's the documentation: https://openxla.org/stablehlo/spec#scatter. @Ferev, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 10, 30, 18, 42, 50, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-28 15:58:07 UTC): Hi, @fujunwei 

I apologize for the delayed response, As far I know TensorFlow Lite doesn't have a direct equivalent to the **ScatterElements** operation as found in **WebNN** or **ONNX** so most probably this will be considered as feature request. We do have [tf.scatter_nd](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/core/ScatterNd?hl=en) in TensorFlow core but it's missing implementation wise a copy of the input tensor rather than an empty or zero-filled tensor, axis specific operation and simplified indexing focusing on a single axis to make it equivalent to **WebNN scatterElements**

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-30 14:12:03 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-30 18:42:50 UTC): I'm fairly sure there's a way, here's the documentation: https://openxla.org/stablehlo/spec#scatter. @Ferev, can you please take a look? Thanks.

"
2610689836,issue,open,,How to map GatherElements with TFLite operator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
[WebNN GatherElements](https://source.chromium.org/chromium/chromium/src/+/main:services/webnn/public/mojom/webnn_graph.mojom;l=606?q=webnn_graph.&ss=chromium%2Fchromium%2Fsrc)  operation gather elements from the axis dimension of the input tensor indexed by the indices tensor following the equation below:

```
 output[dIndex0, ..., dIndexN] =  input[dIndex0, ..., indices[dIndex0, ..., dIndexN], ..., dIndexN]
                                                        ^ This is dAxis, indicated by `axis` parameter.
```

For example:
```
an input =  [[ 0,  1,  2],
             [10, 11, 12],
             [20, 21, 22]] with shape (3, 3),
an indices = [[1, 0],
             [2, 1],
             [0, 2]] with shape (3, 2),
and axis = 1,
the output should be [[ 1,  0],
                       [12, 11],
                       [20, 22]] with shape (3, 2).
```

WebNN has supported gather and gatherND like below table:

| WebNN operation | TFLite operator| Status|
| ------------ | ---------------| ---------------|
| gather | [tfl.gather](https://www.tensorflow.org/mlir/tfl_ops#tflgather_tflgatherop) | Done |
| gatherND| [tfl.gather_nd](https://www.tensorflow.org/mlir/tfl_ops#tflgather_nd_tflgatherndop) | Done |
| gatherElements| | |

but there is no TFLite operator to map with gatherElements, can the [BuiltinOperator_STABLEHLO_GATHER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1221?q=BuiltinOperator_STABLEHLO_GATHER&ss=chromium%2Fchromium%2Fsrc)  implement the `gatherElements` operation?

**Any other info / logs**

WebNN gatherElements is similar with [ONNX GatherElements](https://onnx.ai/onnx/operators/onnx__GatherElements.html).
",fujunwei,2024-10-24 07:11:14+00:00,"['Ferev', 'pkgoogle']",2024-10-31 01:53:39+00:00,,https://github.com/tensorflow/tensorflow/issues/78660,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437824295, 'issue_id': 2610689836, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 25, 13, 45, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438453004, 'issue_id': 2610689836, 'author': 'pkgoogle', 'body': ""Hi @fujunwei, I'm not sure how the conversion will work out, can you try it out and see what happens? This will tell us more, faster. Thanks."", 'created_at': datetime.datetime(2024, 10, 25, 17, 57, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440002615, 'issue_id': 2610689836, 'author': 'fujunwei', 'body': 'The [axis in the GatherElements](https://onnx.ai/onnx/operators/onnx__GatherElements.html#attributes) specify which axis to gather on, but the [STABLEHLO_GATHER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1221?q=BuiltinOperator_STABLEHLO_GATHER&ss=chromium%2Fchromium%2Fsrc) has no the option, do you know how to map with the `StablehloGatherOptions`?', 'created_at': datetime.datetime(2024, 10, 27, 12, 48, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442341618, 'issue_id': 2610689836, 'author': 'pkgoogle', 'body': ""Hi @fujunwei, I'm fairly sure there's a way ... here's the documentation: https://openxla.org/stablehlo/spec#gather. Hi @Ferev , can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 10, 28, 18, 35, 25, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-10-25 13:45:08 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-25 17:57:03 UTC): Hi @fujunwei, I'm not sure how the conversion will work out, can you try it out and see what happens? This will tell us more, faster. Thanks.

fujunwei (Issue Creator) on (2024-10-27 12:48:10 UTC): The [axis in the GatherElements](https://onnx.ai/onnx/operators/onnx__GatherElements.html#attributes) specify which axis to gather on, but the [STABLEHLO_GATHER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1221?q=BuiltinOperator_STABLEHLO_GATHER&ss=chromium%2Fchromium%2Fsrc) has no the option, do you know how to map with the `StablehloGatherOptions`?

pkgoogle (Assginee) on (2024-10-28 18:35:25 UTC): Hi @fujunwei, I'm fairly sure there's a way ... here's the documentation: https://openxla.org/stablehlo/spec#gather. Hi @Ferev , can you please take a look? Thanks.

"
2609737782,issue,closed,completed,Calling .batch() on a Dataset alters the input shape and leads to EfficientNet aborting training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

ArchLinux

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

GeForce RTX 4080

### Current behavior?

Calling .batch() on a dataset as presented in the Prepare Inputs section of https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#transfer-learning-from-pretrained-weights, leads to the input dataset changing shape from (None, 224, 224, 3) to (32, None, 224, 224, 3), where 32 is the chose batch size. This then gives a value error: ValueError: Input 0 of layer ""efficientnetb0"" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, None, 224, 224, 3).

If relevant, I am attempting to train a model on the IP102 dataset, which can be downloaded from https://www.kaggle.com/datasets/rtlmhjbn/ip02-dataset. 


### Standalone code to reproduce the issue

```shell
#Training and validation dataset locations
path_to_train_setbgr = pathlib.PosixPath(
    ""../Insect Pest Classification Dataset/classification/trainbgr"")
path_to_val_setbgr = pathlib.PosixPath(
    ""../Insect Pest Classification Dataset/classification/valbgr"")


IMG_SIZE = 224
size = (IMG_SIZE, IMG_SIZE)
BATCH_SIZE = 32
NUM_CLASSES = 102
epochs = 30

#Loading Dataset
train_ds = tf.keras.utils.image_dataset_from_directory(
    path_to_train_setbgr,
    label_mode=""int"",
    seed=1,
    batch_size=BATCH_SIZE,
    image_size=size)

val_ds = tf.keras.utils.image_dataset_from_directory(
    path_to_val_setbgr,
    label_mode=""int"",
    seed=1,
    batch_size=BATCH_SIZE,
    image_size=size)

#Data Augmentation Functions

img_augmentation_layers = [
    layers.Normalization(axis =-1, mean= [0.485, 0.456, 0.406], variance=[0.229, 0.224, 0.225]),                                                                            
    layers.RandomRotation(factor=0.15),
    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
    layers.RandomFlip(),
    layers.RandomContrast(factor=0.1),
    layers.RandomCrop(height=IMG_SIZE, width=IMG_SIZE),
    layers.RandomBrightness(factor=0.1),
    layers.RandomZoom(height_factor=0.1)
]

def img_augmentation(images):
    for layer in img_augmentation_layers:
        images = layer(images)
    return images


# One-hot / categorical encoding
def input_preprocess_train(image, label):
    image = img_augmentation(image)
    label = tf.one_hot(label, NUM_CLASSES)
    return image, label


def input_preprocess_val(image, label):
    label = tf.one_hot(label, NUM_CLASSES)
    return image, label

#Applying data augmentation

train_ds = train_ds.map(input_preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.batch(batch_size=BATCH_SIZE,
                          drop_remainder=True)
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)

val_ds = val_ds.map(input_preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True,)

#Defining the mode;=
model_B0 = EfficientNetB0(include_top=True,
                          weights=None,
                          classes = NUM_CLASSES,
                          input_shape=(IMG_SIZE, IMG_SIZE, 3))


model_B0.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

#Fitting the model
hist_1= model_B0.fit(train_ds, epochs=epochs, validation_data=val_ds)
```


### Relevant log output

```shell
ValueError: Input 0 of layer ""efficientnetb0"" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, None, 224, 224, 3).
```
",AlexandruIordan99,2024-10-23 20:13:20+00:00,['Venkat6871'],2024-10-24 06:46:12+00:00,2024-10-24 06:46:09+00:00,https://github.com/tensorflow/tensorflow/issues/78621,"[('type:bug', 'Bug')]","[{'comment_id': 2434432854, 'issue_id': 2609737782, 'author': 'AlexandruIordan99', 'body': 'Calling .batch() is unnecessary if you already declare batch size when first declaring the model', 'created_at': datetime.datetime(2024, 10, 24, 6, 46, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434432906, 'issue_id': 2609737782, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78621"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78621"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 6, 46, 11, tzinfo=datetime.timezone.utc)}]","AlexandruIordan99 (Issue Creator) on (2024-10-24 06:46:09 UTC): Calling .batch() is unnecessary if you already declare batch size when first declaring the model

google-ml-butler[bot] on (2024-10-24 06:46:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78621"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78621"">No</a>

"
2609336496,issue,closed,completed,Build Failure: Compiling upb/upb.c failed; defining a type within 'offsetof' is a Clang extension.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

gcc 11.4.0 / Clang 18.1.8

### CUDA/cuDNN version

CUDA 12.5.1 / cuDNN 9.3.0

### GPU model and memory

NVIDIA RTX 3090 24GB VRAM; 32GB SDRAM @3600MHz

### Current behavior?

Expected to compile tensorflow-gpu successfully and export a `.whl` for installation into `conda` environments.

### Standalone code to reproduce the issue

```shell
# Clone Tensorflow and change directory in to tensorflow.
git clone https://github.com/tensorflow/tensorflow.git && cd tensorflow

# Run a container to build tensorflow
docker run \
	-i \
	-t \
	--name tf-build \
	-h tf-build \
	-u root \
	--runtime=nvidia \
	--gpus=all \
	--rm \
	--shm-size=2g \
	--ulimit memlock=-1 \
	--ulimit stack=67108864 \
	-v $PWD:/mnt/$PWD \
	-w /mnt/$PWD \
	tensorflow/tensorflow:build-python3.12 \
	bash

# Configure the build.
./configure

Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.12/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version.


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version.


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-18/bin/clang]:


You have Clang 18.1.8 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

# Use bazel to build.
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
```


### Relevant log output

```shell
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/src/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=163
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1729383360620263506 --host_action_env=DOCKER_HOST_CACHEBUSTER=1729383360708659883
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-18/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /mnt/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /mnt/src/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:opt in file /mnt/src/tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native
INFO: Found applicable config definition build:linux in file /mnt/src/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/py/python_repo.bzl:105:14:
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.12 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/py/python_repo.bzl:116:10: Using hermetic Python 3.12
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/ed/1f/6482380ec8dcec4894e7503490fc536d846b0d59694acad9cf99f27d0e7d/nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (763 packages loaded, 56307 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing command (from target @upb//:upb) /usr/lib/llvm-18/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""upb""' -iquote ... (remaining 45 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-18/lib/clang/18/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1162.201s, Critical Path: 23.64s
INFO: 5591 processes: 4540 internal, 1051 local.
FAILED: Build did NOT complete successfully
```
",NeilPandya,2024-10-23 17:30:15+00:00,['tilakrayal'],2024-11-02 08:48:49+00:00,2024-11-02 08:48:45+00:00,https://github.com/tensorflow/tensorflow/issues/78613,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437532035, 'issue_id': 2609336496, 'author': 'tilakrayal', 'body': '@NeilPandya,\r\nTensorflow v2.17 is compatible with Clang 17.0.6, Bazel 6.5.0, CUDA - 8.9, cudNN-12.3. Could you please try with the mentioned versions and also please take a look at the official document for the tested build configurations. \r\nhttps://www.tensorflow.org/install/source\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 25, 11, 22, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437538516, 'issue_id': 2609336496, 'author': 'NeilPandya', 'body': '> @NeilPandya,\n> Tensorflow v2.17 is compatible with Clang 17.0.6, Bazel 6.5.0, CUDA - 8.9, cudNN-12.3. Could you please try with the mentioned versions and also please take a look at the official document for the tested build configurations. \n> https://www.tensorflow.org/install/source\n> \n> Thank you!\n\n@tilakrayal Just to confirm, I think you have the CUDA and cuDNN versions reversed?', 'created_at': datetime.datetime(2024, 10, 25, 11, 26, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437650555, 'issue_id': 2609336496, 'author': 'tilakrayal', 'body': '@NeilPandya,\r\nApologies for that. Yes it is .For tensorflow v2.17,  cudNN - 8.9, CUDA-12.3 is compatible. Thank you!', 'created_at': datetime.datetime(2024, 10, 25, 12, 27, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437667304, 'issue_id': 2609336496, 'author': 'NeilPandya', 'body': '> @NeilPandya,\n> Apologies for that. Yes it is .For tensorflow v2.17,  cudNN - 8.9, CUDA-12.3 is compatible. Thank you!\n\n@tilakrayal I will run a container pulling from `tensorflow/build:2.17-python3.12`; I think this would be the cleanest way to ensure compatibility across `clang`, `CUDA`, and `cuDNN`. Hopefully, this does the trick.', 'created_at': datetime.datetime(2024, 10, 25, 12, 36, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438074038, 'issue_id': 2609336496, 'author': 'NeilPandya', 'body': '@tilakrayal I successfully completed the compilation, but what\'s strange is that it\'s outputting the `.whl` tagged as `tf2.19`.\r\n\r\n# Docker Command\r\n```bash\r\ndocker run \\\r\n\t-i \\\r\n\t-t \\\r\n\t--name tf-build \\\r\n\t-h tf-build \\\r\n\t-u root \\\r\n\t--runtime=nvidia \\\r\n\t--gpus=all \\\r\n\t--rm \\\r\n\t--shm-size=2g \\\r\n\t--ulimit memlock=-1 \\\r\n\t--ulimit stack=67108864 \\\r\n\t-v $PWD:/mnt/$PWD \\\r\n\t-w /mnt/$PWD \\\r\n\ttensorflow/build:2.17-python3.12 \\\r\n\tbash\r\n```\r\n# Configure\r\n```bash\r\ntf-docker /mnt/src/tensorflow > ./configure\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.10/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the hermetic CUDA version you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify the hermetic cuDNN version you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6\r\n\r\n\r\nPlease specify the local CUDA path you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify the local CUDNN path you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify the local NCCL path you want to use or leave empty to use the default version.\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [Y/n]: y\r\nClang will be used as CUDA compiler.\r\n\r\nPlease specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]:\r\n\r\n\r\nYou have Clang 17.0.6 installed.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v1          \t# Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n# Build\r\n```bash\r\ntf-docker /mnt/src/tensorflow > export HERMETIC_PYTHON_VERSION=3.12 && bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Reading \'startup\' options from /mnt/src/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited \'common\' options: --isatty=1 --terminal_columns=199\r\nINFO: Reading rc options for \'build\' from /mnt/src/tensorflow/.bazelrc:\r\n  Inherited \'common\' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for \'build\' from /etc/bazel.bazelrc:\r\n  \'build\' options: --action_env=DOCKER_CACHEBUSTER=1717287178289175682 --host_action_env=DOCKER_HOST_CACHEBUSTER=1717287178360752164\r\nINFO: Reading rc options for \'build\' from /mnt/src/tensorflow/.bazelrc:\r\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for \'build\' from /mnt/src/tensorflow/.tf_configure.bazelrc:\r\n  \'build\' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /mnt/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /mnt/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda_wheel in file /mnt/src/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false\r\nINFO: Found applicable config definition build:opt in file /mnt/src/tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native\r\nINFO: Found applicable config definition build:linux in file /mnt/src/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /mnt/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/py/python_repo.bzl:119:10: Using hermetic Python 3.12\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/ed/1f/6482380ec8dcec4894e7503490fc536d846b0d59694acad9cf99f27d0e7d/nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found\r\nDEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz\r\nINFO: Analyzed target //tensorflow/tools/pip_package:wheel (763 packages loaded, 56339 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:wheel up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.19.0-cp312-cp312-linux_x86_64.whl\r\nINFO: Elapsed time: 4198.477s, Critical Path: 357.32s\r\nINFO: 33870 processes: 5919 internal, 27951 local.\r\nINFO: Build completed successfully, 33870 total actions\r\n```\r\n# Inspect the Output\r\n```bash\r\ntf-docker /mnt/src/tensorflow > cd bazel-bin/tensorflow/tools/pip_package/wheel_house/\r\ntf-docker /mnt/src/tensorflow/bazel-bin/tensorflow/tools/pip_package/wheel_house > ls -la\r\ntotal 380996\r\ndr-xr-xr-x 2 root root      4096 Oct 25 14:51 .\r\ndrwxr-xr-x 3 root root      4096 Oct 25 14:50 ..\r\n-r-xr-xr-x 1 root root 390124750 Oct 25 14:52 tensorflow-2.19.0-cp312-cp312-linux_x86_64.whl\r\n```\r\nDid I clone the wrong `tensorflow` branch?\r\n\r\nThanks for your help.', 'created_at': datetime.datetime(2024, 10, 25, 15, 6, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452924501, 'issue_id': 2609336496, 'author': 'NeilPandya', 'body': 'I checked out the wrong branch. Closing this issue.', 'created_at': datetime.datetime(2024, 11, 2, 8, 48, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452924509, 'issue_id': 2609336496, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78613"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78613"">No</a>', 'created_at': datetime.datetime(2024, 11, 2, 8, 48, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-25 11:22:13 UTC): @NeilPandya,
Tensorflow v2.17 is compatible with Clang 17.0.6, Bazel 6.5.0, CUDA - 8.9, cudNN-12.3. Could you please try with the mentioned versions and also please take a look at the official document for the tested build configurations. 
https://www.tensorflow.org/install/source

Thank you!

NeilPandya (Issue Creator) on (2024-10-25 11:26:03 UTC): @tilakrayal Just to confirm, I think you have the CUDA and cuDNN versions reversed?

tilakrayal (Assginee) on (2024-10-25 12:27:42 UTC): @NeilPandya,
Apologies for that. Yes it is .For tensorflow v2.17,  cudNN - 8.9, CUDA-12.3 is compatible. Thank you!

NeilPandya (Issue Creator) on (2024-10-25 12:36:44 UTC): @tilakrayal I will run a container pulling from `tensorflow/build:2.17-python3.12`; I think this would be the cleanest way to ensure compatibility across `clang`, `CUDA`, and `cuDNN`. Hopefully, this does the trick.

NeilPandya (Issue Creator) on (2024-10-25 15:06:38 UTC): @tilakrayal I successfully completed the compilation, but what's strange is that it's outputting the `.whl` tagged as `tf2.19`.

# Docker Command
```bash
docker run \
	-i \
	-t \
	--name tf-build \
	-h tf-build \
	-u root \
	--runtime=nvidia \
	--gpus=all \
	--rm \
	--shm-size=2g \
	--ulimit memlock=-1 \
	--ulimit stack=67108864 \
	-v $PWD:/mnt/$PWD \
	-w /mnt/$PWD \
	tensorflow/build:2.17-python3.12 \
	bash
```
# Configure
```bash
tf-docker /mnt/src/tensorflow > ./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.10/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version.


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version.


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]:


You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```
# Build
```bash
tf-docker /mnt/src/tensorflow > export HERMETIC_PYTHON_VERSION=3.12 && bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/src/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=199
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1717287178289175682 --host_action_env=DOCKER_HOST_CACHEBUSTER=1717287178360752164
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /mnt/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /mnt/src/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/src/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /mnt/src/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:opt in file /mnt/src/tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native
INFO: Found applicable config definition build:linux in file /mnt/src/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/py/python_repo.bzl:119:10: Using hermetic Python 3.12
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/ed/1f/6482380ec8dcec4894e7503490fc536d846b0d59694acad9cf99f27d0e7d/nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/2fbebbd9618522b350b0fc1eacb3a8c9/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:273:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (763 packages loaded, 56339 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:wheel up-to-date:
  bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.19.0-cp312-cp312-linux_x86_64.whl
INFO: Elapsed time: 4198.477s, Critical Path: 357.32s
INFO: 33870 processes: 5919 internal, 27951 local.
INFO: Build completed successfully, 33870 total actions
```
# Inspect the Output
```bash
tf-docker /mnt/src/tensorflow > cd bazel-bin/tensorflow/tools/pip_package/wheel_house/
tf-docker /mnt/src/tensorflow/bazel-bin/tensorflow/tools/pip_package/wheel_house > ls -la
total 380996
dr-xr-xr-x 2 root root      4096 Oct 25 14:51 .
drwxr-xr-x 3 root root      4096 Oct 25 14:50 ..
-r-xr-xr-x 1 root root 390124750 Oct 25 14:52 tensorflow-2.19.0-cp312-cp312-linux_x86_64.whl
```
Did I clone the wrong `tensorflow` branch?

Thanks for your help.

NeilPandya (Issue Creator) on (2024-11-02 08:48:45 UTC): I checked out the wrong branch. Closing this issue.

google-ml-butler[bot] on (2024-11-02 08:48:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78613"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78613"">No</a>

"
2609091053,issue,closed,completed,Accuracy is lost after save_weights/load_weights,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.9

### GPU model and memory

RTX 4060 TI

### Current behavior?

1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.5000 - loss: 1.1560
[1.1560312509536743, 0.5]
Epoch 1/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 596ms/step - accuracy: 0.5000 - loss: 1.1560
Epoch 2/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.5000 - loss: 14.5018
Epoch 3/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 0.5000 - loss: 9.9714
Epoch 4/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.7500 - loss: 1.3363
Epoch 5/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 1.0000 - loss: 8.9407e-08
Epoch 6/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 1.0000 - loss: 4.7684e-07
Epoch 7/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.7500 - loss: 0.2545
Epoch 8/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.7500 - loss: 0.8729
Epoch 9/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 1.0000 - loss: 9.1682e-04
Epoch 10/10
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 1.0000 - loss: 2.6822e-07
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 1.0000 - loss: 0.0000e+00
[0.0, 1.0]
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 335ms/step - accuracy: 0.2500 - loss: 0.8475  # this should be acc 1.0 loss 0
[0.847506046295166, 0.25]


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class CusModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense = tf.keras.layers.Dense(units=2, activation='softmax', name='output')

    def call(self, x):
        return self.dense(x)

dummy_data_x = tf.convert_to_tensor([[0, 0],
                [1, 0],
                [0, 1],
                [1, 1]])
dummy_data_y = tf.convert_to_tensor([0, 1, 0, 1])

model = CusModel()
model.compile(optimizer=tf.keras.optimizers.Adam(10.0),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))
model.fit(x=dummy_data_x, y=dummy_data_y, epochs=10)
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))
model.save_weights('test_model.weights.h5')

model = CusModel()
model.load_weights('test_model.weights.h5')
model.compile(optimizer=tf.keras.optimizers.Adam(10.0),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
print(model.evaluate(x=dummy_data_x, y=dummy_data_y))
```


### Relevant log output

_No response_",Pandaaaa906,2024-10-23 15:51:00+00:00,['Venkat6871'],2024-12-04 02:08:34+00:00,2024-12-04 02:08:31+00:00,https://github.com/tensorflow/tensorflow/issues/78610,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.16', '')]","[{'comment_id': 2437009697, 'issue_id': 2609091053, 'author': 'Venkat6871', 'body': 'Hi **@Pandaaaa906** ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 with GPU and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6c8a8d3330291f01f3f462f033afa5e0/78610_tf_2-17-gpu-v.ipynb) here for reference. We may need to look deeper into this issue, and I will update you once we gain more clarity.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 25, 6, 46, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442884116, 'issue_id': 2609091053, 'author': 'Pandaaaa906', 'body': ""@Venkat6871 if i call it using dummy random data before load_weights, it will be ok. not sure it's meant to be or wht, should mention in document."", 'created_at': datetime.datetime(2024, 10, 28, 23, 49, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484835146, 'issue_id': 2609091053, 'author': 'Venkat6871', 'body': 'Hi **@Pandaaaa906** ,\r\nApologies for the delay, and thank you for your patience. Please post this issue on the [keras-team/keras repo.](https://github.com/keras-team/keras/issues)repository, as it seems to be more related to Keras.\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 19, 6, 51, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503231, 'issue_id': 2609091053, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 27, 2, 7, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516005046, 'issue_id': 2609091053, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516005121, 'issue_id': 2609091053, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78610"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78610"">No</a>', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 33, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-25 06:46:18 UTC): Hi **@Pandaaaa906** ,
Apologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 with GPU and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6c8a8d3330291f01f3f462f033afa5e0/78610_tf_2-17-gpu-v.ipynb) here for reference. We may need to look deeper into this issue, and I will update you once we gain more clarity.
Thank you!

Pandaaaa906 (Issue Creator) on (2024-10-28 23:49:28 UTC): @Venkat6871 if i call it using dummy random data before load_weights, it will be ok. not sure it's meant to be or wht, should mention in document.

Venkat6871 (Assginee) on (2024-11-19 06:51:21 UTC): Hi **@Pandaaaa906** ,
Apologies for the delay, and thank you for your patience. Please post this issue on the [keras-team/keras repo.](https://github.com/keras-team/keras/issues)repository, as it seems to be more related to Keras.


Thank you!

github-actions[bot] on (2024-11-27 02:07:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-04 02:08:31 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-04 02:08:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78610"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78610"">No</a>

"
2607858565,issue,closed,completed,Error while Installing !pip install -q tflite-model-maker-nightly !pip install -q tflite-support-nightly,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nighty

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Train a custom object detection model using your data
When I try to install tf model maker I keep getting error related to version but no idea how to handle it.
!pip install -q tflite-model-maker-nightly
!pip install -q tflite-support-nightly
i even tried this but no luck: 
!pip install -q tflite-model-maker
!pip install -q tflite-support
Is there updated or working model of the Collab. 
Your help is truly appreciated.
Thank you in advance.. 

### Standalone code to reproduce the issue

```shell
!pip install -q tflite-model-maker-nightly
!pip install -q tflite-support-nightly
```


### Relevant log output

```shell
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.2/87.2 kB 2.6 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.1/41.1 kB 1.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.9/37.9 MB 14.4 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
WARNING: Ignoring version 0.3.1.dev202105110329 of tflite-model-maker-nightly since it has invalid metadata:
Requested tflite-model-maker-nightly from https://files.pythonhosted.org/packages/12/03/d0a828aebab37a35a9ff6fe3944ba61330311bc6b07756ff58c3df03dcab/tflite_model_maker_nightly-0.3.1.dev202105110329-py3-none-any.whl has invalid metadata: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier
    absl-py (<0.11>=0.10.0)
            ~~~~~~^
Please use pip<24.1 if you need to use this version.
WARNING: Ignoring version 0.3.1.dev202105102248 of tflite-model-maker-nightly since it has invalid metadata:
Requested tflite-model-maker-nightly from https://files.pythonhosted.org/packages/80/a7/4c2001ef8c1f96d272ab1f66204c5ccea86b35eb12824d6b4276d838e0f4/tflite_model_maker_nightly-0.3.1.dev202105102248-py3-none-any.whl has invalid metadata: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier
    absl-py (<0.11>=0.10.0)
            ~~~~~~^
```
",mbilalyaq,2024-10-23 09:11:27+00:00,['gaikwadrahul8'],2024-11-09 01:58:27+00:00,2024-11-09 01:58:25+00:00,https://github.com/tensorflow/tensorflow/issues/78596,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteModelMaker', 'TFLite Model Maker related issues')]","[{'comment_id': 2435728151, 'issue_id': 2607858565, 'author': 'gaikwadrahul8', 'body': ""Hi, @mbilalyaq \r\n\r\nThank you for bringing this issue to our attention, I'm able to replicate the same behavior from my end here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/66cef09334aad13c3004a8fca7f2040f/tflite-issue-78596.ipynb) for reference so at the moment I would suggest you to please go with [MediaPipe Model Maker](https://ai.google.dev/edge/mediapipe/solutions/model_maker) by following this [example](https://ai.google.dev/edge/mediapipe/solutions/customization/image_classifier) while our relevant team fix this issue and once I got any update from relevant team will update you here\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 24, 16, 21, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157396, 'issue_id': 2607858565, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 1, 2, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985673, 'issue_id': 2607858565, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985712, 'issue_id': 2607858565, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78596"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78596"">No</a>', 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 27, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-24 16:21:58 UTC): Hi, @mbilalyaq 

Thank you for bringing this issue to our attention, I'm able to replicate the same behavior from my end here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/66cef09334aad13c3004a8fca7f2040f/tflite-issue-78596.ipynb) for reference so at the moment I would suggest you to please go with [MediaPipe Model Maker](https://ai.google.dev/edge/mediapipe/solutions/model_maker) by following this [example](https://ai.google.dev/edge/mediapipe/solutions/customization/image_classifier) while our relevant team fix this issue and once I got any update from relevant team will update you here

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-01 02:06:54 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-09 01:58:24 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-09 01:58:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78596"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78596"">No</a>

"
2607542967,issue,closed,completed,How to use a GPU with TFlite in Python with ubuntu and Cuda,"Hi, 
I have trained TensorFlow model and quantized it to float 16, saved the file as .tflite format. Tested both TensorFlow and tflite model with CPU it's working. Now want to use GPU for  tflite model with ubuntu (20.4), and follow the same steps provided in the GitHub link git https://github.com/tensorflow/tensorflow.git, for installing gpu_delegate.so file by using below command

**command: bazel build -c opt tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so**

Cuda Version | 11.4.0, Driver Version | 470.256.02, TensorFlow Version | 2.10.0, Python Version|3.8.0, Bazel Version | 7.4.0


GPU delegate is available for ubuntu to test quantized tflite model?
",KarthigaNattarasan,2024-10-23 07:16:45+00:00,['gaikwadrahul8'],2024-11-21 02:04:14+00:00,2024-11-21 02:04:14+00:00,https://github.com/tensorflow/tensorflow/issues/78591,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.10', '')]","[{'comment_id': 2434292042, 'issue_id': 2607542967, 'author': 'Venkat6871', 'body': 'Hi **@KarthigaNattarasan** ,\r\nThank you for raising your concern here. Yes, the GPU delegate is available for Ubuntu to test quantized TFLite models. However, you need to confirm that all the versions are compatible with TensorFlow 2.10.0. It seems you are using Bazel version 7.4.0, which is not compatible with TensorFlow 2.10.0. For TensorFlow 2.10.0, it is better to use Bazel 5.x to avoid build issues and achieve better results. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Please review it for compatibility versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 24, 4, 48, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436776141, 'issue_id': 2607542967, 'author': 'andrenatal', 'body': '@Venkat6871 but this would work with cuda, correct?', 'created_at': datetime.datetime(2024, 10, 25, 4, 5, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452796992, 'issue_id': 2607542967, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453935766, 'issue_id': 2607542967, 'author': 'KarthigaNattarasan', 'body': 'Hi @Venkat6871, \r\nplease share the appropriate documents for reference', 'created_at': datetime.datetime(2024, 11, 4, 6, 52, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454530489, 'issue_id': 2607542967, 'author': 'KarthigaNattarasan', 'body': ""Hi @Venkat6871,\r\nThanks for the reply, tried with Bazel Version | 5.1.1, but still not using GPU.\r\n\r\nError Info:\r\n'''GPU is available and configured for use\r\nEvaluating float16 quantized model...\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.'''\r\n\r\nWhile loading tflite model, it shows GPU is available but it's not utilizing GPU for model inference."", 'created_at': datetime.datetime(2024, 11, 4, 12, 0, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459766715, 'issue_id': 2607542967, 'author': 'gaikwadrahul8', 'body': ""Hi, @KarthigaNattarasan \r\n\r\nI apologize for the delayed response, I see you're not using the latest version of TensorFlow, if possible could you please try with below Tested build configurations and see is it working as expected or not ?\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.18.0 | 3.9-3.12 | Clang 17.0.6 | Bazel 6.5.0 | 9.3 | 12.5\r\n\r\nIf issue still persists could you please help us with your minimal code along with `libtensorflowlite_gpu_delegate.so` and model file (if possible) to reproduce the same behavior from our end to investigate this issue further ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 6, 13, 33, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206822, 'issue_id': 2607542967, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913826, 'issue_id': 2607542967, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 13, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-10-24 04:48:45 UTC): Hi **@KarthigaNattarasan** ,
Thank you for raising your concern here. Yes, the GPU delegate is available for Ubuntu to test quantized TFLite models. However, you need to confirm that all the versions are compatible with TensorFlow 2.10.0. It seems you are using Bazel version 7.4.0, which is not compatible with TensorFlow 2.10.0. For TensorFlow 2.10.0, it is better to use Bazel 5.x to avoid build issues and achieve better results. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Please review it for compatibility versions.
Thank you!

andrenatal on (2024-10-25 04:05:25 UTC): @Venkat6871 but this would work with cuda, correct?

github-actions[bot] on (2024-11-02 02:00:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

KarthigaNattarasan (Issue Creator) on (2024-11-04 06:52:58 UTC): Hi @Venkat6871, 
please share the appropriate documents for reference

KarthigaNattarasan (Issue Creator) on (2024-11-04 12:00:59 UTC): Hi @Venkat6871,
Thanks for the reply, tried with Bazel Version | 5.1.1, but still not using GPU.

Error Info:
'''GPU is available and configured for use
Evaluating float16 quantized model...
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.'''

While loading tflite model, it shows GPU is available but it's not utilizing GPU for model inference.

gaikwadrahul8 (Assginee) on (2024-11-06 13:33:17 UTC): Hi, @KarthigaNattarasan 

I apologize for the delayed response, I see you're not using the latest version of TensorFlow, if possible could you please try with below Tested build configurations and see is it working as expected or not ?

Version | Python version | Compiler | Build tools | cuDNN | CUDA
-- | -- | -- | -- | -- | --
tensorflow-2.18.0 | 3.9-3.12 | Clang 17.0.6 | Bazel 6.5.0 | 9.3 | 12.5

If issue still persists could you please help us with your minimal code along with `libtensorflowlite_gpu_delegate.so` and model file (if possible) to reproduce the same behavior from our end to investigate this issue further ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-14 02:01:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-21 02:04:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2605940314,issue,closed,completed,GPU Detection/PROBLEM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

> =2.2

### Custom code

Yes

### OS platform and distribution

UBUNTU 24.4

### Mobile device

_No response_

### Python version

>=9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6 /   9.5

### GPU model and memory

NVIDIA GeForce RTX 4060

### Current behavior?

Hello, everyone! 

TensorFlow managed to detect my GPU, an NVIDIA GeForce RTX 4060 Laptop GPU, which is a good sign as the hardware is working correctly.

However, I'm facing an issue: TensorFlow only detects the GPU when I install version 2.2. With newer versions, it doesn't detect the GPU. I've tried reinstalling several times, but nothing worked. Does anyone know what's going on?


nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Sep_12_02:18:05_PDT_2024
Cuda compilation tools, release 12.6, V12.6.77
Build cuda_12.6.r12.6/compiler.34841621_0

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

```shell
/home/nzakiese/PycharmProjects/pythonProject/.venv/bin/python /home/nzakiese/PycharmProjects/pythonProject/TESTT.py 
2024-10-22 07:52:02.196951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-22 07:52:02.258352: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0.
2024-10-22 07:52:02.260071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
2024-10-22 07:52:02.260079: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2024-10-22 07:52:02.589600: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
2024-10-22 07:52:02.589629: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
2024-10-22 07:52:02.589632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
[]
2024-10-22 07:52:04.743596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2024-10-22 07:52:04.743615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: nzakiese
2024-10-22 07:52:04.743618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: nzakiese
2024-10-22 07:52:04.743686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 560.35.3
2024-10-22 07:52:04.743698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 560.35.3
2024-10-22 07:52:04.743701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 560.35.3
```
",Villjoie,2024-10-22 16:31:56+00:00,['tilakrayal'],2024-11-08 02:00:57+00:00,2024-11-08 02:00:54+00:00,https://github.com/tensorflow/tensorflow/issues/78531,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.2', 'Issues related to TF 2.2')]","[{'comment_id': 2431739352, 'issue_id': 2605940314, 'author': 'tilakrayal', 'body': '@Villjoie,\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. In this case, you are trying to install TensorFlow v2.2 with CUDA 12.6 and cuDNN 9.5 which is not compatible. Also the current Tensorflow v2.17, and v2.2 is not supported.\r\nhttps://www.tensorflow.org/install/source#gpu\r\n\r\nSo I request to upgrade to the latest Tensorflow where most of the bugs will be resolved. Thank you!', 'created_at': datetime.datetime(2024, 10, 23, 11, 0, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867608, 'issue_id': 2605940314, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 31, 2, 2, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463606960, 'issue_id': 2605940314, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 8, 2, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607020, 'issue_id': 2605940314, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78531"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78531"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 2, 0, 56, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-23 11:00:56 UTC): @Villjoie,
Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. In this case, you are trying to install TensorFlow v2.2 with CUDA 12.6 and cuDNN 9.5 which is not compatible. Also the current Tensorflow v2.17, and v2.2 is not supported.
https://www.tensorflow.org/install/source#gpu

So I request to upgrade to the latest Tensorflow where most of the bugs will be resolved. Thank you!

github-actions[bot] on (2024-10-31 02:02:55 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-08 02:00:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-08 02:00:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78531"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78531"">No</a>

"
2605334318,issue,closed,completed,Build TensorFlowLiteSwift static XCFramework,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello! I need some help to build and install TFLite manually for iOS.

I have a C++ written library that uses TFLite core components. I need to use it in my iOS app, but the main problem is that this iOS app already uses TFLite. So the solution here is to build `TensorFlowLiteC` and `TensorFlowLiteSwift` from source and put it all together in the iOS app so i would use only one Version of these libs.
When I build `TensorFlowLiteC` through Bazel to get an output XCFramework, it hides some necessary symbols needed for the C++ library, so I removed the Strip Code phase from `BUILD.apple` in the iOS folder. I think I can use the full library without hiding any symbols in the iOS app.

But the main problem here is how to build `TensorFlowLiteSwift` to get it as an XCFramework. I've tried to build it through 
`bazel build -c opt --config=ios_fat -c opt --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 //tensorflow/lite/swift:TensorFlowLite_xcframework`
but as an output, I am getting an XCFramework without any headers and umbrella header files. I think this is incorrect because when I try to build the iOS app with those two XCFrameworks, I am getting an error.
<img width=""326"" alt=""Снимок экрана 2024-10-22 в 15 22 44"" src=""https://github.com/user-attachments/assets/6a48fc31-87de-442c-a755-718ad63ea7f6"">

I have tried to build with just TFLite version 2.15.0 from Git. It has another error with undefined symbols—that's why I need `TensorFlowLiteC` without hidden symbols. Also as requirement is that i need static libs.
I tryed to follow this [guid](https://ai.google.dev/edge/litert/build/ios) but no luck. I stucked at dealing with `TensorFlowLiteSwift`. 

I Think that some workaround exists because stable version can be provided like static lib.

Can anyone help me to achieve my goal?


### Standalone code to reproduce the issue

```shell
bazel build --config=ios_fat -c opt --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 \
//tensorflow/lite/ios:TensorFlowLiteC_xcframework --copt=-Os --subcommands

bazel build -c opt --config=ios_fat -c opt --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 //tensorflow/lite/swift:TensorFlowLite_xcframework --subcommands
```


### Relevant log output

_No response_",vaverkax,2024-10-22 12:36:45+00:00,['gaikwadrahul8'],2024-10-23 12:22:24+00:00,2024-10-23 12:21:49+00:00,https://github.com/tensorflow/tensorflow/issues/78511,"[('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2429253358, 'issue_id': 2605334318, 'author': 'vaverkax', 'body': ""One more thing: If i try to Build iOS app with my local TensorFlowLiteC XCFramework everything fine and works. But if i add \r\n`pod 'TensorFlowLiteSwift', :path => 'path/to/local/podspec'` \r\nthe error about `types.h` occurs. So it seems like TensorFlowLiteSwift is wrongly builded."", 'created_at': datetime.datetime(2024, 10, 22, 13, 13, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431948966, 'issue_id': 2605334318, 'author': 'vaverkax', 'body': ""So, after investigation of stable version of tfliteC i found that it includes `types.h` header file that was missing in my build. \r\nI've added to `BUILD.apple` in iOS folder to `tflite_ios_xcframework` func this header and everything works fine for me. \r\nGuess we can Close issue."", 'created_at': datetime.datetime(2024, 10, 23, 12, 21, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431949094, 'issue_id': 2605334318, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78511"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78511"">No</a>', 'created_at': datetime.datetime(2024, 10, 23, 12, 21, 51, tzinfo=datetime.timezone.utc)}]","vaverkax (Issue Creator) on (2024-10-22 13:13:07 UTC): One more thing: If i try to Build iOS app with my local TensorFlowLiteC XCFramework everything fine and works. But if i add 
`pod 'TensorFlowLiteSwift', :path => 'path/to/local/podspec'` 
the error about `types.h` occurs. So it seems like TensorFlowLiteSwift is wrongly builded.

vaverkax (Issue Creator) on (2024-10-23 12:21:49 UTC): So, after investigation of stable version of tfliteC i found that it includes `types.h` header file that was missing in my build. 
I've added to `BUILD.apple` in iOS folder to `tflite_ios_xcframework` func this header and everything works fine for me. 
Guess we can Close issue.

google-ml-butler[bot] on (2024-10-23 12:21:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78511"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78511"">No</a>

"
2605125733,issue,closed,completed,OOM when running `tf.raw_ops.DenseBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

All memory will be used up when running the following code

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1-me1CkT6-kEV9vhCsFRbygkVRNApaSM3?usp=sharing
```


### Relevant log output

```shell
Timestamp,Level,Message
""Oct 22, 2024, 7:05:36 PM"",WARNING,WARNING:root:kernel 82160fbc-3b27-42da-ad82-176fab1b542f restarted
""Oct 22, 2024, 7:05:36 PM"",INFO,""KernelRestarter: restarting kernel (1/5), keep random ports""
""Oct 22, 2024, 7:05:15 PM"",WARNING,2024-10-22 11:05:15.583047: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10007999174112 exceeds 10% of free system memory.
""Oct 22, 2024, 7:05:12 PM"",WARNING,2024-10-22 11:05:12.027196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
",LongZE666,2024-10-22 11:10:31+00:00,['tilakrayal'],2024-11-11 02:01:46+00:00,2024-11-11 02:01:42+00:00,https://github.com/tensorflow/tensorflow/issues/78508,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2432141979, 'issue_id': 2605125733, 'author': 'Karan-Nagure', 'body': 'Hey, I have run the colab notebook. I came up with the solution that we can reduce the size value. This value should be near about 1000 so it could be managable.\r\n\r\nI like to fix the bug.', 'created_at': datetime.datetime(2024, 10, 23, 13, 16, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434332373, 'issue_id': 2605125733, 'author': 'tilakrayal', 'body': '@LongZE666,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/65634 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 24, 5, 27, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439421341, 'issue_id': 2605125733, 'author': 'LongZE666', 'body': '@tilakrayal Thank you very much for your reply, I will take a look', 'created_at': datetime.datetime(2024, 10, 26, 7, 53, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453261531, 'issue_id': 2605125733, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 3, 2, 5, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467087339, 'issue_id': 2605125733, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 11, 2, 1, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467087413, 'issue_id': 2605125733, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78508"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78508"">No</a>', 'created_at': datetime.datetime(2024, 11, 11, 2, 1, 45, tzinfo=datetime.timezone.utc)}]","Karan-Nagure on (2024-10-23 13:16:39 UTC): Hey, I have run the colab notebook. I came up with the solution that we can reduce the size value. This value should be near about 1000 so it could be managable.

I like to fix the bug.

tilakrayal (Assginee) on (2024-10-24 05:27:50 UTC): @LongZE666,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/65634 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

LongZE666 (Issue Creator) on (2024-10-26 07:53:21 UTC): @tilakrayal Thank you very much for your reply, I will take a look

github-actions[bot] on (2024-11-03 02:05:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-11 02:01:42 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-11 02:01:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78508"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78508"">No</a>

"
2604745499,issue,closed,completed,Failed to load the native TensorFlow runtime.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My custom classifier isn't running.

Expectation:
Wanted to run a custom classifier 

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in
from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Users\user\fer-project\src\train.py"", line 3, in
from model import create_cnn_model
File ""C:\Users\user\fer-project\src\model.py"", line 3, in
import tensorflow as tf
File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow_init_.py"", line 38, in
from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow # pylint: disable=unused-import
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in
raise ImportError(
ImportError: Traceback (most recent call last):
File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in
from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

Failed to load the native TensorFlow runtime.
```


### Relevant log output

_No response_",philipakomolafe,2024-10-22 08:53:01+00:00,['Venkat6871'],2024-11-08 02:00:59+00:00,2024-11-08 02:00:55+00:00,https://github.com/tensorflow/tensorflow/issues/78485,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2431459613, 'issue_id': 2604745499, 'author': 'Venkat6871', 'body': 'Hi **@philipakomolafe** ,\r\nThere are at least 3 possible scenarios:\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 23, 9, 25, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867637, 'issue_id': 2604745499, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 31, 2, 2, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463606997, 'issue_id': 2604745499, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 8, 2, 0, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607080, 'issue_id': 2604745499, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78485"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78485"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 2, 0, 58, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-23 09:25:49 UTC): Hi **@philipakomolafe** ,
There are at least 3 possible scenarios:
You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-10-31 02:02:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-08 02:00:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-08 02:00:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78485"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78485"">No</a>

"
2604113765,issue,closed,completed,"What does the tensor ""transformer_layer_0/BroadcastTo"" do, and why is it quantized to INT32?","### 1. System information

- Linux Ubuntu 22.04
- TensorFlow 2.12 installed via pip

### 2. Code

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/github/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb): The KerasNLP workshop from IO2023

The link to the model I produced using the notebook is here: https://drive.google.com/file/d/1nSABgkHysrwkAn8K3H45Eqq646iNZOBp/view?usp=sharing

### Other info

I follow the IO 2023 workshop notebook above to use KerasNLP to produce a quantized GPT2 in the tflite format.

I then visualize the tflite model using `tensorflow/lite/tools/visualize.py`.

Most of the parameters are quantized to INT8, which is expected. I am confused about two things regarding the following tensors: `transformer_layer_0/BroadcastTo`, `transformer_layer_0/BroadcastTo1`, `transformer_layer_0/cached_multi_head_attention/ExpandDims`, (... for each ""transformer_layer"").

My first question is: What does a broadcast/ExpandDims ""tensor"" store? It seems to me that those are not trainable operations (e.g. doing broadcasting), so why would the tensors have a huge shape [1, 100, 100] (i.e. 10,000 elements)?

My second question is: Why are those tensors quantized to a higher precision (INT32 vs INT8 like other params)?

I really appreciate any help you can provide.",i3abghany,2024-10-22 03:18:29+00:00,['gaikwadrahul8'],2024-10-28 03:01:35+00:00,2024-10-28 03:01:33+00:00,https://github.com/tensorflow/tensorflow/issues/78475,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2432789974, 'issue_id': 2604113765, 'author': 'fergushenderson', 'body': 'For documentation on the semantics of the BroadcastTo and ExpandDims operations, see the following:\r\n\r\n- https://www.tensorflow.org/api_docs/python/tf/broadcast_to\r\n\r\n- https://www.tensorflow.org/api_docs/python/tf/expand_dims', 'created_at': datetime.datetime(2024, 10, 23, 16, 25, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432904076, 'issue_id': 2604113765, 'author': 'i3abghany', 'body': '@fergushenderson Thank you. I already know the operations, but I am asking why they have trainable parameters and why they are quantized to a higher bit width (int32 instead of int8).', 'created_at': datetime.datetime(2024, 10, 23, 17, 12, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435648237, 'issue_id': 2604113765, 'author': 'gaikwadrahul8', 'body': ""Hi, @i3abghany\r\n\r\nAs far I know `BroadcastTo` and `ExpandDims` tensors operations are used in the attention mechanism of transformers specifically to create attention masks and handle batched inputs, Those are not trainable parameters but necessary runtime tensors saves memory compared to storing separate masks for each batch and attention head\r\n\r\nAs per my understanding the **INT32** quantization is necessary for numerical stability in attention computations (Q * K multiplications) which is needed for accumulating products across sequence dimension which preserves precision for mask values and attention weights, query and key are typically **INT8** after quantization ( INT8 values range (127 to -128) when multiplied together which exceeds **INT8** range (-128 to 127) to avoid overflow in cumulative operations may be using **INT32** if I'm not wrong\r\n\r\nThis behavior will depend upon which quantization technique you apply on the model for more details please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization)\r\n\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 24, 15, 45, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440442696, 'issue_id': 2604113765, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78475"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78475"">No</a>', 'created_at': datetime.datetime(2024, 10, 28, 3, 1, 34, tzinfo=datetime.timezone.utc)}]","fergushenderson on (2024-10-23 16:25:11 UTC): For documentation on the semantics of the BroadcastTo and ExpandDims operations, see the following:

- https://www.tensorflow.org/api_docs/python/tf/broadcast_to

- https://www.tensorflow.org/api_docs/python/tf/expand_dims

i3abghany (Issue Creator) on (2024-10-23 17:12:48 UTC): @fergushenderson Thank you. I already know the operations, but I am asking why they have trainable parameters and why they are quantized to a higher bit width (int32 instead of int8).

gaikwadrahul8 (Assginee) on (2024-10-24 15:45:13 UTC): Hi, @i3abghany

As far I know `BroadcastTo` and `ExpandDims` tensors operations are used in the attention mechanism of transformers specifically to create attention masks and handle batched inputs, Those are not trainable parameters but necessary runtime tensors saves memory compared to storing separate masks for each batch and attention head

As per my understanding the **INT32** quantization is necessary for numerical stability in attention computations (Q * K multiplications) which is needed for accumulating products across sequence dimension which preserves precision for mask values and attention weights, query and key are typically **INT8** after quantization ( INT8 values range (127 to -128) when multiplied together which exceeds **INT8** range (-128 to 127) to avoid overflow in cumulative operations may be using **INT32** if I'm not wrong

This behavior will depend upon which quantization technique you apply on the model for more details please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization)

If I have missed something here please let me know.

Thank you for your cooperation and patience.

google-ml-butler[bot] on (2024-10-28 03:01:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78475"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78475"">No</a>

"
2603779268,issue,closed,completed,ModuleNotFoundError: No module named 'tensorflow.python.client',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.7/12.0.140

### GPU model and memory

RTX 3050 4 GB

### Current behavior?

Whenever I try to convert onnx model to tensorflow I get this error.

~~~shell
$ onnx-tf convert -i yolov7.onnx -o tensorflow-model/
~~~

output:

~~~
Traceback (most recent call last):
  File ""/home/metehan/miniconda3/envs/yolov7/bin/onnx-tf"", line 5, in <module>
    from onnx_tf.cli import main
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/__init__.py"", line 1, in <module>
    from . import backend
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/backend.py"", line 25, in <module>
    from onnx_tf.common import data_type
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/common/__init__.py"", line 14, in <module>
    from tensorflow.python.client import device_lib
ModuleNotFoundError: No module named 'tensorflow.python.client'
~~~

### Standalone code to reproduce the issue

```python
import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load(""yolov7-tinny.onnx"")
tf_rep = prepare(onnx_model)
tf_rep.export_graph(""tensorflow-model.pb"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/metehan/miniconda3/envs/yolov7/bin/onnx-tf"", line 5, in <module>
    from onnx_tf.cli import main
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/__init__.py"", line 1, in <module>
    from . import backend
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/backend.py"", line 25, in <module>
    from onnx_tf.common import data_type
  File ""/home/metehan/miniconda3/envs/yolov7/lib/python3.10/site-packages/onnx_tf/common/__init__.py"", line 14, in <module>
    from tensorflow.python.client import device_lib
ModuleNotFoundError: No module named 'tensorflow.python.client'
```
",metehanozdeniz,2024-10-21 22:18:51+00:00,['Venkat6871'],2024-11-09 01:58:30+00:00,2024-11-09 01:58:27+00:00,https://github.com/tensorflow/tensorflow/issues/78466,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2435328091, 'issue_id': 2603779268, 'author': 'Venkat6871', 'body': 'Hi **@metehanozdeniz** ,\r\nApologies for the delay. Could you please provide the model you are using? It would be helpful to replicate the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 24, 13, 39, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157434, 'issue_id': 2603779268, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 1, 2, 6, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985707, 'issue_id': 2603779268, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985754, 'issue_id': 2603779268, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78466"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78466"">No</a>', 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 29, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-24 13:39:28 UTC): Hi **@metehanozdeniz** ,
Apologies for the delay. Could you please provide the model you are using? It would be helpful to replicate the issue.
Thank you!

github-actions[bot] on (2024-11-01 02:06:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-09 01:58:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-09 01:58:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78466"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78466"">No</a>

"
2602743172,issue,closed,completed,`std::is_pod<T>` is deprecated in C++20,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

5844ceef9b1bfdfe5e289ec11f85472e0353ec9a

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

13.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For some reasons, I'm compiling tensorflow lite with C++20. This is perfectly fine for my use case and works well, the only issue is that I'm also compiling with `-Wdeprecated-declarations` and the console is spammed with that warning:

```
tensorflow/lite/core/api/flatbuffer_conversions.h: In member function 'T* tflite::BuiltinDataAllocator::AllocatePOD()':
tensorflow/lite/core/api/flatbuffer_conversions.h:47:24: warning: 'template<class _Tp> struct std::is_pod' is deprecated: use is_standard_layout && is_trivial instead [-Wdeprecated-declarations]
   47 |     static_assert(std::is_pod<T>::value, ""Builtin data structure must be POD."");
      |                        ^~~~~~
In file included from include/c++/12.3.1/bits/stl_pair.h:60,
                 from include/c++/12.3.1/utility:69,
                 from tensorflow/lite/core/c/common.h:1476,
                 from tensorflow/lite/c/common.h:31,
                 from signal/micro/kernels/filter_bank_square_root.h:18,
                 from signal/micro/kernels/filter_bank_square_root.cc:20:
include/c++/12.3.1/type_traits:757:5: note: declared here
  757 |     is_pod
      |     ^~~~~~
```

As you can see [here](https://github.com/tensorflow/tensorflow/blame/5844ceef9b1bfdfe5e289ec11f85472e0353ec9a/tensorflow/lite/core/api/flatbuffer_conversions.h#L45-L46), in the source code, this issue already covered by a FIXME. I would have open a PR to replace `is_pod` by `is_trivially_destructible`, but I don't know if that mysterious platform finally got support for it. I hope you guys have more information internally.


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_",LucasChollet,2024-10-21 14:31:21+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-04 20:22:57+00:00,2025-01-04 20:22:53+00:00,https://github.com/tensorflow/tensorflow/issues/78434,"[('awaiting review', 'Pull request awaiting review'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues')]","[{'comment_id': 2437852492, 'issue_id': 2602743172, 'author': 'gaikwadrahul8', 'body': ""Hi, @LucasChollet \r\n\r\nI apologize for the delayed response, Meanwhile, Thank you for bringing this issue to our attention could you please help me with exact steps which you followed before encountering that warning so I'll try to replicate the same behavior from my end and will discuss with relevant team about it ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 25, 13, 57, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438039520, 'issue_id': 2602743172, 'author': 'LucasChollet', 'body': ""Hello @gaikwadrahul8 \r\n\r\nI reported the issue here as that's where the code/issue comes from but my workflow actually involve tflite-micro.\r\nHere should be a full reproducer:\r\n\r\n```shell\r\ngit clone https://github.com/tensorflow/tflite-micro.git\r\ncd tflite-micro/\r\nmkdir gen\r\npython3 ../tflite-micro/tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py gen\r\ng++ -Igen -Igen/third_party/flatbuffers/include -std=c++20 -c gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc\r\n```\r\n\r\nYou're supposed to build all source files in the generated source directory (here `gen`) and link that into you're executable, but here I'm simply compiling one file that trigger the warning.\r\n\r\nAnother way of seeing it with a more normal workflow, is to use the `Makefile` they provide in the repo with a slight modification. So, to execute after the previous commands:\r\n\r\n```\r\ncp tensorflow/lite/micro/tools/project_generation/Makefile gen/\r\ncd gen\r\nrm tensorflow/lite/micro/span_test.cc tensorflow/lite/micro/static_vector_test.cc # Don't ask me why, but these two need to be deleted\r\nsed -i 's/c++17/c++20/' Makefile\r\nmake -j8\r\n```"", 'created_at': datetime.datetime(2024, 10, 25, 14, 53, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441849607, 'issue_id': 2602743172, 'author': 'gaikwadrahul8', 'body': 'Hi, @LucasChollet \r\n\r\nI apologize for the delayed response, thank you for providing the detailed steps to replicate the similar behavior from my end and I\'m able to replicate for reference I\'ve added output log below so need to discuss this issue internally and will update you, thank you for bringing this issue to our attention\r\n\r\n```\r\nbase) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-#78434/tflite-micro$ g++ -Igen -Igen/third_party/flatbuffers/include -std=c++20 -c gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc\r\nIn file included from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h:19,\r\n                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc:15:\r\ngen/tensorflow/lite/core/api/flatbuffer_conversions.h: In member function ‘T* tflite::BuiltinDataAllocator::AllocatePOD()’:\r\ngen/tensorflow/lite/core/api/flatbuffer_conversions.h:47:24: warning: ‘template<class _Tp> struct std::is_pod’ is deprecated: use is_standard_layout && is_trivial instead [-Wdeprecated-declarations]\r\n   47 |     static_assert(std::is_pod<T>::value, ""Builtin data structure must be POD."");\r\n      |                        ^~~~~~\r\nIn file included from gen/tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\n                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h:19,\r\n                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc:15:\r\n/usr/include/c++/11/type_traits:733:5: note: declared here\r\n  733 |     is_pod\r\n      |     ^~~~~~\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-#78434/tflite-micro$ \r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 28, 15, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468828222, 'issue_id': 2602743172, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 11, 11, 18, 51, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468884029, 'issue_id': 2602743172, 'author': 'pkgoogle', 'body': 'Hi @LucasChollet, what version of gcc/g++ are you using? 2.18 only supports Clang 17.0.6, is std::is_pod<T> deprecated in Clang 17.0.6? Can you use Clang to do your workflow instead?', 'created_at': datetime.datetime(2024, 11, 11, 19, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468912008, 'issue_id': 2602743172, 'author': 'LucasChollet', 'body': 'I\'m using tflite-micro and GCC to compile for an embedded platform. But I don\'t think the issue comes from a compiler version but more probably from a C++ version. I know that I\'m compiling with a more recent version of C++ than you guys are doing and that\'s why I\'m getting this warning (see the `-std=c++20` flag in the reproducer). If my request of stopping to use `std::is_pod` is not something that you\'re willing to fix because it\'s out of the scope of the ""normal"" usage, I understand. That being said, you\'ll have to fix it one day or another (when migrating), so I opened the issue because I would prefer if that day comes sooner than later :^).\r\n\r\nI\'m still ok to open a PR to fix it if you want.', 'created_at': datetime.datetime(2024, 11, 11, 19, 48, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471312894, 'issue_id': 2602743172, 'author': 'pkgoogle', 'body': 'Hi @LucasChollet, no worries, I only point that out because 17.0.6 partially supports C++20 and we only officially support that for now, so if Clang 17.0.6 supports this deprecation than this can be considered a bug. If you can compile your changes with 17.0.6, then I believe it will be accepted, if not then I believe it will probably be rejected until we do the full migration together.\r\n\r\nIf your workflow and tflite-micro supports a different compiler you may want to check with https://github.com/tensorflow/tflite-micro/issues if you want to change code there instead.', 'created_at': datetime.datetime(2024, 11, 12, 18, 50, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471372349, 'issue_id': 2602743172, 'author': 'LucasChollet', 'body': ""The paper that led to the deprecation of `std::is_pod` was published in 2017 and quickly implemented in clang/libcxx. [Here](https://libcxx.llvm.org/Status/Cxx20.html) it is stated that clang and libcxx are conform to that paper since version 7.0. However, `libcxx` doesn't annotate its functions with `[[deprecated]]` at all, which makes the bug unreproducible with `libcxx`. So while I might be able to compile with clang and libstdc++ (GNU's implementation of the standard library that contains actual `[[deprecated]]` attributes and thus provoke a warning in the compiler) to prove the point, that seems like it's too far from a normal use case to be considered worth fixing."", 'created_at': datetime.datetime(2024, 11, 12, 19, 22, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2474273780, 'issue_id': 2602743172, 'author': 'pkgoogle', 'body': 'PR opened for this: https://github.com/tensorflow/tensorflow/pull/79967', 'created_at': datetime.datetime(2024, 11, 13, 17, 24, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477275018, 'issue_id': 2602743172, 'author': 'LucasChollet', 'body': 'Thanks for taking care of that!', 'created_at': datetime.datetime(2024, 11, 14, 19, 45, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571402691, 'issue_id': 2602743172, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78434"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78434"">No</a>', 'created_at': datetime.datetime(2025, 1, 4, 20, 22, 56, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-25 13:57:16 UTC): Hi, @LucasChollet 

I apologize for the delayed response, Meanwhile, Thank you for bringing this issue to our attention could you please help me with exact steps which you followed before encountering that warning so I'll try to replicate the same behavior from my end and will discuss with relevant team about it ? 

Thank you for your cooperation and patience.

LucasChollet (Issue Creator) on (2024-10-25 14:53:55 UTC): Hello @gaikwadrahul8 

I reported the issue here as that's where the code/issue comes from but my workflow actually involve tflite-micro.
Here should be a full reproducer:

```shell
git clone https://github.com/tensorflow/tflite-micro.git
cd tflite-micro/
mkdir gen
python3 ../tflite-micro/tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py gen
g++ -Igen -Igen/third_party/flatbuffers/include -std=c++20 -c gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc
```

You're supposed to build all source files in the generated source directory (here `gen`) and link that into you're executable, but here I'm simply compiling one file that trigger the warning.

Another way of seeing it with a more normal workflow, is to use the `Makefile` they provide in the repo with a slight modification. So, to execute after the previous commands:

```
cp tensorflow/lite/micro/tools/project_generation/Makefile gen/
cd gen
rm tensorflow/lite/micro/span_test.cc tensorflow/lite/micro/static_vector_test.cc # Don't ask me why, but these two need to be deleted
sed -i 's/c++17/c++20/' Makefile
make -j8
```

gaikwadrahul8 (Assginee) on (2024-10-28 15:05:00 UTC): Hi, @LucasChollet 

I apologize for the delayed response, thank you for providing the detailed steps to replicate the similar behavior from my end and I'm able to replicate for reference I've added output log below so need to discuss this issue internally and will update you, thank you for bringing this issue to our attention

```
base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-#78434/tflite-micro$ g++ -Igen -Igen/third_party/flatbuffers/include -std=c++20 -c gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc
In file included from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h:19,
                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc:15:
gen/tensorflow/lite/core/api/flatbuffer_conversions.h: In member function ‘T* tflite::BuiltinDataAllocator::AllocatePOD()’:
gen/tensorflow/lite/core/api/flatbuffer_conversions.h:47:24: warning: ‘template<class _Tp> struct std::is_pod’ is deprecated: use is_standard_layout && is_trivial instead [-Wdeprecated-declarations]
   47 |     static_assert(std::is_pod<T>::value, ""Builtin data structure must be POD."");
      |                        ^~~~~~
In file included from gen/tensorflow/lite/core/api/flatbuffer_conversions.h:24,
                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.h:19,
                 from gen/tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc:15:
/usr/include/c++/11/type_traits:733:5: note: declared here
  733 |     is_pod
      |     ^~~~~~
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-#78434/tflite-micro$ 
```

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-11-11 18:51:10 UTC): Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-11-11 19:29:00 UTC): Hi @LucasChollet, what version of gcc/g++ are you using? 2.18 only supports Clang 17.0.6, is std::is_pod<T> deprecated in Clang 17.0.6? Can you use Clang to do your workflow instead?

LucasChollet (Issue Creator) on (2024-11-11 19:48:09 UTC): I'm using tflite-micro and GCC to compile for an embedded platform. But I don't think the issue comes from a compiler version but more probably from a C++ version. I know that I'm compiling with a more recent version of C++ than you guys are doing and that's why I'm getting this warning (see the `-std=c++20` flag in the reproducer). If my request of stopping to use `std::is_pod` is not something that you're willing to fix because it's out of the scope of the ""normal"" usage, I understand. That being said, you'll have to fix it one day or another (when migrating), so I opened the issue because I would prefer if that day comes sooner than later :^).

I'm still ok to open a PR to fix it if you want.

pkgoogle (Assginee) on (2024-11-12 18:50:10 UTC): Hi @LucasChollet, no worries, I only point that out because 17.0.6 partially supports C++20 and we only officially support that for now, so if Clang 17.0.6 supports this deprecation than this can be considered a bug. If you can compile your changes with 17.0.6, then I believe it will be accepted, if not then I believe it will probably be rejected until we do the full migration together.

If your workflow and tflite-micro supports a different compiler you may want to check with https://github.com/tensorflow/tflite-micro/issues if you want to change code there instead.

LucasChollet (Issue Creator) on (2024-11-12 19:22:06 UTC): The paper that led to the deprecation of `std::is_pod` was published in 2017 and quickly implemented in clang/libcxx. [Here](https://libcxx.llvm.org/Status/Cxx20.html) it is stated that clang and libcxx are conform to that paper since version 7.0. However, `libcxx` doesn't annotate its functions with `[[deprecated]]` at all, which makes the bug unreproducible with `libcxx`. So while I might be able to compile with clang and libstdc++ (GNU's implementation of the standard library that contains actual `[[deprecated]]` attributes and thus provoke a warning in the compiler) to prove the point, that seems like it's too far from a normal use case to be considered worth fixing.

pkgoogle (Assginee) on (2024-11-13 17:24:17 UTC): PR opened for this: https://github.com/tensorflow/tensorflow/pull/79967

LucasChollet (Issue Creator) on (2024-11-14 19:45:45 UTC): Thanks for taking care of that!

google-ml-butler[bot] on (2025-01-04 20:22:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78434"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78434"">No</a>

"
2602414922,issue,open,,No kernels registered for op `Conv2DBackpropInputV2`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The existence of operator `Conv2DBackpropInputV2` is described in the official website document.https://www.tensorflow.org/api_docs/python/tf/raw_ops/Conv2DBackpropInputV2.
However, during my actual execution, the following error message appears:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_sizes = tf.constant(1, shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInputV2(
    input=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```


### Relevant log output

```shell
2024-10-21 12:30:18.492624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/mnt/tests/Conv2DBackpropInput.py"", line 10, in <module>
    tf.raw_ops.Conv2DBackpropInputV2(
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/util/tf_export.py"", line 377, in wrapper
    return f(**kwargs)
           ^^^^^^^^^^^
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2030, in conv2d_backprop_input_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]
All kernels registered for op Conv2DBackpropInputV2:
  <no registered kernels>
 [Op:Conv2DBackpropInputV2] name:
```
",LongZE666,2024-10-21 12:34:35+00:00,['Venkat6871'],2024-11-13 04:11:47+00:00,,https://github.com/tensorflow/tensorflow/issues/78431,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2431406351, 'issue_id': 2602414922, 'author': 'Venkat6871', 'body': 'I tried to run your code on colab using Tensorflow 2.17.0 and nightly versions. And faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/579d3fcc1d9ca68f810bb8a1a1cdf4ad/78431_2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 23, 9, 8, 5, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-23 09:08:05 UTC): I tried to run your code on colab using Tensorflow 2.17.0 and nightly versions. And faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/579d3fcc1d9ca68f810bb8a1a1cdf4ad/78431_2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2602373601,issue,open,,Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow : `input_sizes` is a one-dimensional tensor and contains maximum values
Check fail：`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2

### Standalone code to reproduce the issue

```shell
Overflow:

import tensorflow as tf

input_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)

Check fail:
```python
import tensorflow as tf

input_sizes = tf.constant(1, shape=[2], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```
```


### Relevant log output

```shell
Overflow:

2024-10-21 11:40:49.417611: F tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)
Status: INVALID_ARGUMENT: Dimension -727379969 must be >= 0
Aborted (core dumped)
```

Check fail:
```
2024-10-21 11:16:29.937265: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)
Aborted (core dumped)
```
```
",LongZE666,2024-10-21 12:17:20+00:00,['tilakrayal'],2024-12-14 14:27:20+00:00,,https://github.com/tensorflow/tensorflow/issues/78428,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2431722291, 'issue_id': 2602373601, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/fcfd94987aa889de1c377da9fa71b005/untitled2190.ipynb).\r\n\r\n\r\n**Output Error:**\r\n```python\r\nInvalidArgumentError: {{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dimension -727379969 must be >= 0 [Op:Conv2DBackpropInput] name: \r\n```', 'created_at': datetime.datetime(2024, 10, 23, 10, 52, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497249967, 'issue_id': 2602373601, 'author': 'LongZE666', 'body': '@tilakrayal  The example in Check fail can still be triggered, right?', 'created_at': datetime.datetime(2024, 11, 25, 8, 32, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543132623, 'issue_id': 2602373601, 'author': 'Balkrishna5', 'body': ""As ,I am a second year student, if any thing wrong in solution ,glad to hear from you\r\n\r\n1.\r\ninput_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)\r\noverflow occurs because 999999999999 exceeds the maximum value for int32 , which 214483647. So, solution for above is, ensure input_size has realistic values for the expected input dimensions\r\n\r\n2.\r\nIssue 1: The input_sizes tensor's shape is [2], but TensorFlow expects it to have a shape of [4] (representing [batch_size, height, width, channels] for the input tensor).\r\n\r\nIssue 2: The filter_tensor tensor has a shape of [1], but TensorFlow expects a filter of at least rank 4 (shape [filter_height, filter_width, in_channels, out_channels]).\r\n\r\nsolution for above two issues will be:\r\nCorrect the shapes of both input_sizes and filter_tensor to meet TensorFlow's expectations:\r\n\r\ninput_sizes = tf.constant([1, 5, 5, 2], shape=[4], dtype=tf.int32)\r\nfilter_tensor = tf.constant(2, shape=[3, 3, 2, 2], dtype=tf.float32)"", 'created_at': datetime.datetime(2024, 12, 14, 14, 27, 18, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-23 10:52:52 UTC): I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/fcfd94987aa889de1c377da9fa71b005/untitled2190.ipynb).


**Output Error:**
```python
InvalidArgumentError: {{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dimension -727379969 must be >= 0 [Op:Conv2DBackpropInput] name: 
```

LongZE666 (Issue Creator) on (2024-11-25 08:32:47 UTC): @tilakrayal  The example in Check fail can still be triggered, right?

Balkrishna5 on (2024-12-14 14:27:18 UTC): As ,I am a second year student, if any thing wrong in solution ,glad to hear from you

1.
input_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)
overflow occurs because 999999999999 exceeds the maximum value for int32 , which 214483647. So, solution for above is, ensure input_size has realistic values for the expected input dimensions

2.
Issue 1: The input_sizes tensor's shape is [2], but TensorFlow expects it to have a shape of [4] (representing [batch_size, height, width, channels] for the input tensor).

Issue 2: The filter_tensor tensor has a shape of [1], but TensorFlow expects a filter of at least rank 4 (shape [filter_height, filter_width, in_channels, out_channels]).

solution for above two issues will be:
Correct the shapes of both input_sizes and filter_tensor to meet TensorFlow's expectations:

input_sizes = tf.constant([1, 5, 5, 2], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[3, 3, 2, 2], dtype=tf.float32)

"
2602201285,issue,open,,Overflow in `tf.raw_ops.Fill`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow in `tf.raw_ops.Fill` when there are too large values in `dims`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing
```


### Relevant log output

```shell
Kill
```
",LongZE666,2024-10-21 11:10:34+00:00,['Venkat6871'],2024-11-07 15:26:49+00:00,,https://github.com/tensorflow/tensorflow/issues/78427,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2428622139, 'issue_id': 2602201285, 'author': 'Venkat6871', 'body': ""Hi **@LongZE666** ,\r\nI am trying to open the Colab gist you provided, but I don't have access to it. Could you please grant access to your Colab gist?\r\nThank you!"", 'created_at': datetime.datetime(2024, 10, 22, 8, 33, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428631437, 'issue_id': 2602201285, 'author': 'LongZE666', 'body': '> ```shell\r\n> https://colab.research.google.com/drive/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing\r\n> ```\r\nI changed the permissions, can you try again? terribly sorry', 'created_at': datetime.datetime(2024, 10, 22, 8, 37, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431430129, 'issue_id': 2602201285, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\r\nThank you for providing access. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/c8b8334e515062ba892ca8fc6539548f/78427_2-17-0-nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 23, 9, 16, 1, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-22 08:33:56 UTC): Hi **@LongZE666** ,
I am trying to open the Colab gist you provided, but I don't have access to it. Could you please grant access to your Colab gist?
Thank you!

LongZE666 (Issue Creator) on (2024-10-22 08:37:35 UTC): I changed the permissions, can you try again? terribly sorry

Venkat6871 (Assginee) on (2024-10-23 09:16:01 UTC): Hi **@LongZE666** ,
Thank you for providing access. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/c8b8334e515062ba892ca8fc6539548f/78427_2-17-0-nightly-v.ipynb) here for reference.

Thank you!

"
2600171903,issue,closed,not_planned,android TensorFlow-Lite yolo11-obb run error,"**System information**
- OS Platform and Distribution (e.g., Android 14):
  implementation(""com.google.ai.edge.litert:litert-support:1.0.1"")
  implementation(""com.google.ai.edge.litert:litert-gpu:1.0.1"")
  implementation(""com.google.ai.edge.litert:litert-gpu-api:1.0.1"")
  implementation(""com.google.ai.edge.litert:litert-metadata:1.0.1"")


[
[best_int8.tflite.zip](https://github.com/user-attachments/files/17449551/best_int8.tflite.zip)
](url)
YOLO8n-obb.pt can run normally when converted to tflite model, but YOLO11n-obb.pt reports an error when converted to tflite model



source code

```
 public void init(Context context, String modelPath, List<String> labels) {
        this.labels = labels;
        CompatibilityList compatList = new CompatibilityList();
        Interpreter.Options options = new Interpreter.Options();
        options.setUseNNAPI(true);
        if (compatList.isDelegateSupportedOnThisDevice()) {
            GpuDelegate delegate = new GpuDelegate(compatList.getBestOptionsForThisDevice());
            options.addDelegate(delegate);
        } else {
            options.setNumThreads(4);
        }
        try {
            MappedByteBuffer mappedByteBuffer = FileUtil.loadMappedFile(context, modelPath);
            this.interpreter = new Interpreter(mappedByteBuffer, options);
            initializeTensorShapes();
        } catch (Exception e) {
            Log.e(""DetectorObb"", ""Error initializing TensorFlow Lite interpreter"", e);
            throw new RuntimeException(e);
        }
    }

```

```
If not calling 
GpuDelegate delegate=new GpuDelegate (compactList. getBestOptionsForThisDevice());
options.addDelegate(delegate); 

This code will not generate any errors



2024-10-20 16:00:34.037 13564-18621 libc                    com.example.opencv                   I  handling signal: 11
2024-10-20 16:00:34.037 13564-18621 libc                    com.example.opencv                   A  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x18 in tid 18621 (pool-2-thread-1), pid 13564 (.example.opencv)
2024-10-20 16:00:34.037 13564-18621 libc                    com.example.opencv                   I  debuggerd_dispatch_pseudothread start. crashing tid: 18621
2024-10-20 16:00:34.048 13564-18621 libc                    com.example.opencv                   I  crash_dump pid: 18733
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A  Cmdline: com.example.opencv
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A  pid: 13564, tid: 18621, name: pool-2-thread-1  >>> com.example.opencv <<<
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A        #17 pc 00000000001650ac  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A        #18 pc 0000000000164fe0  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A        #19 pc 00000000001607e8  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.377 18734-18734 DEBUG                   pid-18734                            A        #20 pc 000000000010f80c  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #21 pc 0000000000105fe4  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #22 pc 0000000000105a4c  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #23 pc 00000000001057d8  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #24 pc 00000000000ffdf4  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #25 pc 00000000000b6cd4  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #26 pc 00000000000b6748  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #27 pc 00000000000b7748  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #28 pc 0000000000320f64  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #29 pc 0000000000320908  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #30 pc 0000000000320524  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #31 pc 00000000000b3174  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_gpu_jni.so (offset 0x56c000) (BuildId: 85ee85b3b999e40f77f5ec31b04e320c)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #32 pc 0000000000325400  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #33 pc 00000000003259a4  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #34 pc 00000000003189e0  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #35 pc 000000000031bcac  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #36 pc 000000000031c1c8  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #37 pc 0000000000084f80  /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!libtensorflowlite_jni.so (offset 0x75c000) (Java_org_tensorflow_lite_NativeInterpreterWrapper_createInterpreter+692) (BuildId: 850a925e891f910bae3c83095332764b)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #44 pc 000000000009c134  [anon:dalvik-classes6.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes6.dex] (org.tensorflow.lite.NativeInterpreterWrapper.init+0)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #50 pc 000000000009be34  [anon:dalvik-classes6.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes6.dex] (org.tensorflow.lite.NativeInterpreterWrapper.<init>+0)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #56 pc 000000000009b7ec  [anon:dalvik-classes6.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes6.dex] (org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>+0)
2024-10-20 16:00:34.378 18734-18734 DEBUG                   pid-18734                            A        #62 pc 000000000009b618  [anon:dalvik-classes6.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes6.dex] (org.tensorflow.lite.Interpreter.<init>+0)
2024-10-20 16:00:34.379 18734-18734 DEBUG                   pid-18734                            A        #68 pc 000000000000152c  /data/data/com.example.opencv/code_cache/.overlay/base.apk/classes2.dex (com.example.tflite.DetectorObb.init+0)
2024-10-20 16:00:34.379 18734-18734 DEBUG                   pid-18734                            A        #74 pc 00000000000019cc  [anon:dalvik-classes4.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes4.dex] (com.example.opencv.CameraTestActivity.lambda$onCreate$0$com-example-opencv-CameraTestActivity+0)
2024-10-20 16:00:34.379 18734-18734 DEBUG                   pid-18734                            A        #80 pc 00000000000014c0  [anon:dalvik-classes4.dex extracted in memory from /data/app/~~0cf_c_6kX4_zxIPTT_wnqQ==/com.example.opencv-kkHJ40uyAxIPy0dkRtusBQ==/base.apk!classes4.dex] (com.example.opencv.CameraTestActivity$$ExternalSyntheticLambda3.run+0)

```




",sungerk,2024-10-20 08:17:07+00:00,"['arfaian', 'gaikwadrahul8', 'pkgoogle']",2025-01-27 22:12:48+00:00,2025-01-27 22:12:44+00:00,https://github.com/tensorflow/tensorflow/issues/78396,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2426520069, 'issue_id': 2600171903, 'author': 'gaikwadrahul8', 'body': 'Hi, @sungerk \r\n\r\nThank you for bringing this issue to our attention, if possible could you please help us with minimum code example with your Github repo along with complete steps to replicate the same behavior from our end to investigate this issue further ?\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 21, 12, 22, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426562562, 'issue_id': 2600171903, 'author': 'sungerk', 'body': '@gaikwadrahul8 \r\n\r\n[YOLOv8-TfLite-Object-Detector.zip](https://github.com/user-attachments/files/17460899/YOLOv8-TfLite-Object-Detector.zip)\r\n\r\n\r\nthis is my source code ,you can edit the  Constants.MODEL_PATH to choose mode\r\n\r\n\r\nthis is my covert model code\r\n```\r\nimport os\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\'yolov8n-obb.pt\')  \r\n\r\nmodel.export(format=""tflite"", imgsz=(640, 640),\r\n             optimize=True,\r\n             int8=True)\r\n```', 'created_at': datetime.datetime(2024, 10, 21, 12, 41, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432470710, 'issue_id': 2600171903, 'author': 'fergushenderson', 'body': '>         options.setUseNNAPI(true);\r\n\r\nNNAPI is [deprecated now](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide), so I recommend removing that line.', 'created_at': datetime.datetime(2024, 10, 23, 14, 45, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432501239, 'issue_id': 2600171903, 'author': 'sungerk', 'body': ""@fergushenderson \r\n\r\nI have tried commenting out this line of code. It's not useful, the key error is that \r\n\r\nGpuDelegate delegate=new GpuDelegate (compactList. getBestOptionsForThisDevice());\r\noptions.addDelegate(delegate);\r\n\ufeff\r\n.If the model converted from YOLOV8 does not generate errors, the model converted from YOLO11 will generate errors after loading the model for a while without inference"", 'created_at': datetime.datetime(2024, 10, 23, 14, 53, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435349319, 'issue_id': 2600171903, 'author': 'gaikwadrahul8', 'body': ""Hi, @sungerk \r\n\r\nI apologize for the delayed response, I was trying to replicate the similar behavior with your provided source code with `yolo11_obb.tflite` model and i'm getting below runtime error for reference I've added error log below \r\n\r\n**Here is runtime error log for reference :**\r\n\r\n```\r\n2024-10-24 17:33:24.023 29019-29057 libc                                                         A  Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 29057 (bt_stack_manage), pid 29019 (droid.bluetooth)\r\n2024-10-24 17:33:24.712 31578-31578 DEBUG                                                        A        #04 pc 0000000000448860  /apex/com.android.btservices/lib64/libbluetooth_jni.so (void bluetooth::log::fatal<unsigned char>(fmt::v10::basic_format_string<char, fmt::v10::type_identity<unsigned char>::type>, unsigned char&&, bluetooth::log_internal::source_location)+76) (BuildId: d30e2e8307dbbe890368d1d0b791ed4b)\r\n2024-10-24 17:41:23.632 31730-31748 libc                                                         A  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xc in tid 31748 (pool-2-thread-1), pid 31730 (an.yolov8tflite)\r\n```\r\nIf I have missed something here to reproduce the exact behavior as you mentioned in the issue template from my end please do let me know with exact steps which I'll try from my end again\r\n\r\nThank you for your cooperation and patience"", 'created_at': datetime.datetime(2024, 10, 24, 13, 47, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439213983, 'issue_id': 2600171903, 'author': 'sungerk', 'body': ""@gaikwadrahul8  Sorry for the late reply. There are no other steps. What's strange is why you don't have the error libtensorflowlite_jni.so (offset 0x75c000)?"", 'created_at': datetime.datetime(2024, 10, 26, 3, 16, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447312002, 'issue_id': 2600171903, 'author': 'gaikwadrahul8', 'body': ""Hi, @sungerk \r\nI apologize for the delayed response, even I'm not sure why I'm not getting the error related to **libtensorflowlite_jni.so (offset 0x75c000)** at the moment\r\n\r\nHi, @pkgoogle\r\nPlease take look into this issue. Thank you"", 'created_at': datetime.datetime(2024, 10, 30, 14, 16, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448128697, 'issue_id': 2600171903, 'author': 'pkgoogle', 'body': 'Hi @sungerk, I loaded your project and changed `const val MODEL_PATH = ""yolov8_obb.tflite""` to `const val MODEL_PATH = ""yolo11_obb.tflite""`, I was able to run it on emulator successfully.\r\n\r\nHere\'s my AVD settings. Have I missed something in reproducing this?\r\n\r\n![image](https://github.com/user-attachments/assets/3e8b007c-3716-4d6b-8bf1-244913ffa816)', 'created_at': datetime.datetime(2024, 10, 30, 19, 9, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448788716, 'issue_id': 2600171903, 'author': 'sungerk', 'body': '@pkgoogle  without ,but i use my mobile phone Redmi Note12 5G', 'created_at': datetime.datetime(2024, 10, 31, 1, 1, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450425303, 'issue_id': 2600171903, 'author': 'pkgoogle', 'body': 'I am unable to reproduce on emulator as shown above, @arfaian, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 10, 31, 17, 22, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614465682, 'issue_id': 2600171903, 'author': 'PlateAss', 'body': 'I have same error on Xiaomi Redmi K20 Pro.\nyolov8 is ok. yolo11(test on yolo11n,yolo11s) failed with gpu,but it can run on cpu.', 'created_at': datetime.datetime(2025, 1, 26, 15, 17, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617006996, 'issue_id': 2600171903, 'author': 'pkgoogle', 'body': 'Thanks for the additional information, we will be moving this issue to [LiteRT](https://github.com/google-ai-edge/litert). Please follow progress on that repo.', 'created_at': datetime.datetime(2025, 1, 27, 22, 12, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617007060, 'issue_id': 2600171903, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78396"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78396"">No</a>', 'created_at': datetime.datetime(2025, 1, 27, 22, 12, 46, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-21 12:22:24 UTC): Hi, @sungerk 

Thank you for bringing this issue to our attention, if possible could you please help us with minimum code example with your Github repo along with complete steps to replicate the same behavior from our end to investigate this issue further ?

Thank you for your cooperation and patience.

sungerk (Issue Creator) on (2024-10-21 12:41:49 UTC): @gaikwadrahul8 

[YOLOv8-TfLite-Object-Detector.zip](https://github.com/user-attachments/files/17460899/YOLOv8-TfLite-Object-Detector.zip)


this is my source code ,you can edit the  Constants.MODEL_PATH to choose mode


this is my covert model code
```
import os
from ultralytics import YOLO

model = YOLO('yolov8n-obb.pt')  

model.export(format=""tflite"", imgsz=(640, 640),
             optimize=True,
             int8=True)
```

fergushenderson on (2024-10-23 14:45:45 UTC): NNAPI is [deprecated now](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide), so I recommend removing that line.

sungerk (Issue Creator) on (2024-10-23 14:53:06 UTC): @fergushenderson 

I have tried commenting out this line of code. It's not useful, the key error is that 

GpuDelegate delegate=new GpuDelegate (compactList. getBestOptionsForThisDevice());
options.addDelegate(delegate);
﻿
.If the model converted from YOLOV8 does not generate errors, the model converted from YOLO11 will generate errors after loading the model for a while without inference

gaikwadrahul8 (Assginee) on (2024-10-24 13:47:46 UTC): Hi, @sungerk 

I apologize for the delayed response, I was trying to replicate the similar behavior with your provided source code with `yolo11_obb.tflite` model and i'm getting below runtime error for reference I've added error log below 

**Here is runtime error log for reference :**

```
2024-10-24 17:33:24.023 29019-29057 libc                                                         A  Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 29057 (bt_stack_manage), pid 29019 (droid.bluetooth)
2024-10-24 17:33:24.712 31578-31578 DEBUG                                                        A        #04 pc 0000000000448860  /apex/com.android.btservices/lib64/libbluetooth_jni.so (void bluetooth::log::fatal<unsigned char>(fmt::v10::basic_format_string<char, fmt::v10::type_identity<unsigned char>::type>, unsigned char&&, bluetooth::log_internal::source_location)+76) (BuildId: d30e2e8307dbbe890368d1d0b791ed4b)
2024-10-24 17:41:23.632 31730-31748 libc                                                         A  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xc in tid 31748 (pool-2-thread-1), pid 31730 (an.yolov8tflite)
```
If I have missed something here to reproduce the exact behavior as you mentioned in the issue template from my end please do let me know with exact steps which I'll try from my end again

Thank you for your cooperation and patience

sungerk (Issue Creator) on (2024-10-26 03:16:28 UTC): @gaikwadrahul8  Sorry for the late reply. There are no other steps. What's strange is why you don't have the error libtensorflowlite_jni.so (offset 0x75c000)?

gaikwadrahul8 (Assginee) on (2024-10-30 14:16:07 UTC): Hi, @sungerk 
I apologize for the delayed response, even I'm not sure why I'm not getting the error related to **libtensorflowlite_jni.so (offset 0x75c000)** at the moment

Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-30 19:09:49 UTC): Hi @sungerk, I loaded your project and changed `const val MODEL_PATH = ""yolov8_obb.tflite""` to `const val MODEL_PATH = ""yolo11_obb.tflite""`, I was able to run it on emulator successfully.

Here's my AVD settings. Have I missed something in reproducing this?

![image](https://github.com/user-attachments/assets/3e8b007c-3716-4d6b-8bf1-244913ffa816)

sungerk (Issue Creator) on (2024-10-31 01:01:44 UTC): @pkgoogle  without ,but i use my mobile phone Redmi Note12 5G

pkgoogle (Assginee) on (2024-10-31 17:22:48 UTC): I am unable to reproduce on emulator as shown above, @arfaian, can you please take a look? Thanks.

PlateAss on (2025-01-26 15:17:46 UTC): I have same error on Xiaomi Redmi K20 Pro.
yolov8 is ok. yolo11(test on yolo11n,yolo11s) failed with gpu,but it can run on cpu.

pkgoogle (Assginee) on (2025-01-27 22:12:44 UTC): Thanks for the additional information, we will be moving this issue to [LiteRT](https://github.com/google-ai-edge/litert). Please follow progress on that repo.

google-ml-butler[bot] on (2025-01-27 22:12:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78396"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78396"">No</a>

"
2600157833,issue,closed,completed,Failed to load the native TensorFlow runtime.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow-intel==2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 10 

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected the code to run as inteneded and generate a file within a certain folder within my project structure

### Standalone code to reproduce the issue

```shell
import pandas as pd
import os
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error  # For evaluation after predictions
import tensorflow as tf
import numpy as np
import pickle

# Load configuration
CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'C:/Users/path')

with open(CONFIG_PATH, 'r') as f:
    config = json.load(f)

def load_data(symbol):
    """"""
    Load processed data for a given stock symbol.
    """"""
    folder = os.path.join(os.path.dirname(__file__), 'C:/Users/path')
    file_path = os.path.join(folder, f'{symbol}.csv')
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f""Processed data for {symbol} not found."")
    
    df = pd.read_csv(file_path)
    return df

def preprocess_data(df):
    """"""
    Preprocess the data for LSTM. Scale it and reshape it for sequential training.
    """"""
    # Select features and target
    features = ['SMA_20', 'EMA_20', 'SMA_50', 'EMA_50', 'RSI', 'Volatility', 'ROC', 'MACD', 'Signal_Line']
    target = 'close'
    
    # Scaling the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(df[features])
    
    # Creating sequences
    sequence_length = 60  # Use past 60 days to predict the next day
    X, y = [], []
    
    for i in range(sequence_length, len(scaled_data)):
        X.append(scaled_data[i-sequence_length:i])
        y.append(df[target].values[i])
    
    X, y = np.array(X), np.array(y)
    
    # Reshape X to be (samples, time steps, features)
    X = np.reshape(X, (X.shape[0], X.shape[1], len(features)))
    
    return X, y, scaler

def build_lstm_model(input_shape):
    """"""
    Build an LSTM model for stock price prediction.
    """"""
    model = tf.keras.Sequential()
    
    # LSTM layers
    model.add(tf.keras.layers.LSTM(units=100, return_sequences=True, input_shape=input_shape))
    model.add(tf.keras.layers.Dropout(0.2))  # Add dropout to prevent overfitting
    model.add(tf.keras.layers.LSTM(units=100, return_sequences=False))
    model.add(tf.keras.layers.Dropout(0.2))
    
    # Dense layers
    model.add(tf.keras.layers.Dense(units=50))
    model.add(tf.keras.layers.Dense(units=1))  # Output layer predicting closing price
    
    model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])
    
    return model

def train_model(X, y):
    """"""
    Train the LSTM model.
    """"""
    # Build the model
    model = build_lstm_model((X.shape[1], X.shape[2]))
    
    # Train the model
    model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
    
    return model

def save_model(model, symbol, scaler):
    """"""
    Save the trained LSTM model and the scaler for later use.
    """"""
    folder = os.path.join(os.path.dirname(__file__), 'C:/Users/path')
    os.makedirs(folder, exist_ok=True)
    
    model_file_path = os.path.join(folder, f'{symbol}_lstm_model.h5')
    scaler_file_path = os.path.join(folder, f'{symbol}_scaler.pkl')
    
    # Save the model
    model.save(model_file_path)
    print(f""Model saved as {symbol}_lstm_model.h5"")
    
    # Save the scaler
    with open(scaler_file_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f""Scaler saved as {symbol}_scaler.pkl"")

def train_and_save_models():
    """"""
    Load data for each stock, preprocess it, train the LSTM model, and save the trained model.
    """"""
    stock_symbols = config['stocks']
    
    for symbol in stock_symbols:
        print(f""Training LSTM model for {symbol}..."")
        
        # Load and preprocess the data
        df = load_data(symbol)
        X, y, scaler = preprocess_data(df)
        
        # Train the model
        model = train_model(X, y)
        
        # Save the model and scaler
        save_model(model, symbol, scaler)

if __name__ == ""__main__"":
    train_and_save_models()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Program Files\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\path"", line 6, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
```
",ghost,2024-10-20 07:59:50+00:00,['Venkat6871'],2024-10-22 09:43:38+00:00,2024-10-22 09:43:34+00:00,https://github.com/tensorflow/tensorflow/issues/78395,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('subtype:cpu-intel', 'To track windows cpu issues')]","[{'comment_id': 2428317748, 'issue_id': 2600157833, 'author': 'Venkat6871', 'body': 'Hi **@nashole** ,\r\nApologies for the delay, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 22, 5, 51, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428801529, 'issue_id': 2600157833, 'author': 'mihaimaruseac', 'body': 'Closing as duplicate of #61887 (and similar other issues).', 'created_at': datetime.datetime(2024, 10, 22, 9, 43, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428801612, 'issue_id': 2600157833, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78395"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78395"">No</a>', 'created_at': datetime.datetime(2024, 10, 22, 9, 43, 36, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-22 05:51:59 UTC): Hi **@nashole** ,
Apologies for the delay, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

mihaimaruseac on (2024-10-22 09:43:34 UTC): Closing as duplicate of #61887 (and similar other issues).

google-ml-butler[bot] on (2024-10-22 09:43:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78395"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78395"">No</a>

"
2600053768,issue,open,,tflite-support build is failing for elinux_aarch6,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tflite-support 0.4.4

### Custom code

No

### OS platform and distribution

Ubuntu 22.04 Arm

### Mobile device

NXP i.mx8 plus

### Python version

3.10.12

### Bazel version

5.1.1

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

pip install tflite-support on the NXP i.mx8 plus board gets a very old version of the package (0.1.0)

I need the 0.4.4 wheel built for arm64, to make use of TextEmbedder.

I expect the bazel build to succeed on the Ubuntu 22.04 arm64 based machine that I have (AWS T4G EC2 instance) from where I plan to take the wheel and deploy to the NXP device. 

The build is failing.

### Standalone code to reproduce the issue

```shell
1. Obtain the source code from https://github.com/tensorflow/tflite-support/archive/refs/tags/v0.4.4.tar.gz

2. Install bazel using https://github.com/bazelbuild/bazelisk/releases/download/v1.22.0/bazelisk-linux-arm64

3. Modify tensorflow_lite_support/tools/pip_package/rpi/build_arm_pip_package.sh to remove build for elinux_armhf, as I need only the elinux_aarch64 to be built.

4. Run tensorflow_lite_support/tools/pip_package/rpi/build_arm_pip_package.sh

It results in the errors show in in the log output. Same issue occurs when building from nightly as well as 0.4.3.
```


### Relevant log output

```shell
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/types/BUILD.bazel:154:11: Compiling absl/types/bad_optional_access.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/types/bad_optional_access.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/escaping.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/escaping.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/internal/memutil.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/internal/memutil.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/string_view.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/string_view.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
Target //tensorflow_lite_support/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 31.453s, Critical Path: 0.47s
INFO: 33 processes: 30 internal, 3 local.
FAILED: Build did NOT complete successfully
```
",sixersri,2024-10-20 06:07:53+00:00,['gaikwadrahul8'],2024-10-28 14:40:56+00:00,,https://github.com/tensorflow/tensorflow/issues/78390,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues')]","[{'comment_id': 2432416110, 'issue_id': 2600053768, 'author': 'fergushenderson', 'body': ""`/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string`\r\n\r\nThis looks like some sort of cross compilation configuration issue: you are trying to execute a toolchain built for one architecture on a different architecture, and the file format isn't recognized, so it is trying to execute the binary for that architecture as a shell script, and then is getting a syntax error because it's actually a binary file not a shell script."", 'created_at': datetime.datetime(2024, 10, 23, 14, 30, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432849385, 'issue_id': 2600053768, 'author': 'sixersri', 'body': 'Thanks Fergus. I tried compiling on the NXP device itself which is running a Debian derivative linux. Same issue. \r\n\r\nSo, when I run the tensorflow_lite_support/tools/pip_package/build_pip_package.sh , the compilation succeeds. When I try to install that wheel on the NXP device via pip, it says unsupported format/platform. That gave me a hint that the tensorflow_lite_support/tools/pip_package/build_pip_package.sh is perhaps building for x86_64.\r\n\r\nThis is where I started running the tensorflow_lite_support/tools/pip_package/**rpi**/build_pip_package.sh , as this has the --config=elinux_aarch64 option set. On closer look, the rpi based build_pip_package.sh is referencing tensorflow_lite_support/tools/pip_package:build_pip_package - is this reference causing some x86_64 versus aarch64 compile issue?\r\n\r\nCan I ask what are the steps that I should be using to compile for an aarch64 target? is the **rpi**/build_pip_package.sh the right approach?', 'created_at': datetime.datetime(2024, 10, 23, 16, 53, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437588171, 'issue_id': 2600053768, 'author': 'fergushenderson', 'body': ""> Can I ask what are the steps that I should be using to compile for an aarch64 target?\r\n> is the rpi/build_pip_package.sh the right approach?\r\n\r\nI don't know off-hand, sorry."", 'created_at': datetime.datetime(2024, 10, 25, 11, 54, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441579912, 'issue_id': 2600053768, 'author': 'gaikwadrahul8', 'body': ""Hi, @sixersri \r\n\r\nI apologize for the delayed response, I was trying to replicate the same behavior from my end on **GCP VM Ubuntu (Arm64)** with your provided steps and I'm getting similar error for reference I've added output log below \r\n\r\n```\r\nERROR: /home/gaikwadrahul/tflite-#78390/tflite-support-0.4.4/tensorflow_lite_support/codegen/python/BUILD:10:17: Compiling tensorflow_lite_support/codegen/python/codegen_lib.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/461e71c01175b74d26435d70c17829c6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG ... (remaining 75 arguments skipped)\r\n/home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/461e71c01175b74d26435d70c17829c6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string\r\nTarget //tensorflow_lite_support/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.830s, Critical Path: 0.03s\r\nINFO: 2 processes: 2 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI tried below command and it's showing `ERROR: Config value 'aarch64' is not defined in any .rc file`\r\n\r\n```\r\ngaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22-arm64:~/tflite-#78390/tflite-support-0.4.4$ bazel build --verbose_failures --sandbox_debug --config=aarch64 //tensorflow_lite_support/tools/pip_package:build_pip_package\r\nERROR: Config value 'aarch64' is not defined in any .rc file\r\ngaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22-arm64:~/tflite-#78390/tflite-support-0.4.4$\r\n```\r\nThank you."", 'created_at': datetime.datetime(2024, 10, 28, 13, 23, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441782817, 'issue_id': 2600053768, 'author': 'sixersri', 'body': 'Thanks @gaikwadrahul8 . The presence of aarch64 config in rpi/build_pip_package.sh gave me the knowledge to try this. Please let me know how to compile for aarch64.', 'created_at': datetime.datetime(2024, 10, 28, 14, 40, 55, tzinfo=datetime.timezone.utc)}]","fergushenderson on (2024-10-23 14:30:22 UTC): `/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string`

This looks like some sort of cross compilation configuration issue: you are trying to execute a toolchain built for one architecture on a different architecture, and the file format isn't recognized, so it is trying to execute the binary for that architecture as a shell script, and then is getting a syntax error because it's actually a binary file not a shell script.

sixersri (Issue Creator) on (2024-10-23 16:53:26 UTC): Thanks Fergus. I tried compiling on the NXP device itself which is running a Debian derivative linux. Same issue. 

So, when I run the tensorflow_lite_support/tools/pip_package/build_pip_package.sh , the compilation succeeds. When I try to install that wheel on the NXP device via pip, it says unsupported format/platform. That gave me a hint that the tensorflow_lite_support/tools/pip_package/build_pip_package.sh is perhaps building for x86_64.

This is where I started running the tensorflow_lite_support/tools/pip_package/**rpi**/build_pip_package.sh , as this has the --config=elinux_aarch64 option set. On closer look, the rpi based build_pip_package.sh is referencing tensorflow_lite_support/tools/pip_package:build_pip_package - is this reference causing some x86_64 versus aarch64 compile issue?

Can I ask what are the steps that I should be using to compile for an aarch64 target? is the **rpi**/build_pip_package.sh the right approach?

fergushenderson on (2024-10-25 11:54:31 UTC): I don't know off-hand, sorry.

gaikwadrahul8 (Assginee) on (2024-10-28 13:23:10 UTC): Hi, @sixersri 

I apologize for the delayed response, I was trying to replicate the same behavior from my end on **GCP VM Ubuntu (Arm64)** with your provided steps and I'm getting similar error for reference I've added output log below 

```
ERROR: /home/gaikwadrahul/tflite-#78390/tflite-support-0.4.4/tensorflow_lite_support/codegen/python/BUILD:10:17: Compiling tensorflow_lite_support/codegen/python/codegen_lib.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/461e71c01175b74d26435d70c17829c6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG ... (remaining 75 arguments skipped)
/home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/461e71c01175b74d26435d70c17829c6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
Target //tensorflow_lite_support/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.830s, Critical Path: 0.03s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```
I tried below command and it's showing `ERROR: Config value 'aarch64' is not defined in any .rc file`

```
gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22-arm64:~/tflite-#78390/tflite-support-0.4.4$ bazel build --verbose_failures --sandbox_debug --config=aarch64 //tensorflow_lite_support/tools/pip_package:build_pip_package
ERROR: Config value 'aarch64' is not defined in any .rc file
gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22-arm64:~/tflite-#78390/tflite-support-0.4.4$
```
Thank you.

sixersri (Issue Creator) on (2024-10-28 14:40:55 UTC): Thanks @gaikwadrahul8 . The presence of aarch64 config in rpi/build_pip_package.sh gave me the knowledge to try this. Please let me know how to compile for aarch64.

"
2599148452,issue,open,,Multi-threaded execution throws an exception (using GPU).,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20241018

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Multi-threaded execution throws an exception (using GPU).

### Standalone code to reproduce the issue

```shell
import concurrent

import numpy as np
import tensorflow as tf
print(tf.__version__)

executor = concurrent.futures.ThreadPoolExecutor()


def sum(x, axis):
    return tf.reduce_sum(x, axis=axis)


futures = []

for i in range(1000):
    futures.clear()
    for _ in range(4):
        x = tf.convert_to_tensor(np.random.rand(100, 100))
        futures.append(executor.submit(sum, x, 1))
        x = tf.convert_to_tensor(np.random.rand(100))
        futures.append(executor.submit(sum, x, 0))
    concurrent.futures.wait(
        futures, return_when=concurrent.futures.ALL_COMPLETED
    )
    [future.result() for future in futures]
```


### Relevant log output

```shell
W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)
I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)

```
```
",nicolaspi,2024-10-19 13:02:32+00:00,['tilakrayal'],2024-11-07 06:51:37+00:00,,https://github.com/tensorflow/tensorflow/issues/78338,"[('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues')]","[{'comment_id': 2423836428, 'issue_id': 2599148452, 'author': 'nicolaspi', 'body': 'Potentially related: https://github.com/tensorflow/tensorflow/issues/72388', 'created_at': datetime.datetime(2024, 10, 19, 13, 3, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427188221, 'issue_id': 2599148452, 'author': 'tilakrayal', 'body': '@nicolaspi,\r\nThe PR which was raised for the issue has been merged and also tried to execute the mentioned code on both tensorflow v2.17 and tf-nightly, observed on tf-nightly the code was executed. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ab9434c9dab1de112d482293c2ad03ad/untitled2186.ipynb) and confirm whether the issue was resolved? Thank you!', 'created_at': datetime.datetime(2024, 10, 21, 16, 43, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427288020, 'issue_id': 2599148452, 'author': 'nicolaspi', 'body': '@tilakrayal Your gist is showing me an error when I run it (even for tf-nightly).', 'created_at': datetime.datetime(2024, 10, 21, 17, 13, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461407109, 'issue_id': 2599148452, 'author': 'tilakrayal', 'body': '@nicolaspi,\r\nI tried to execute with the latest tensorflow v2.18(GPU), and the code was executed without fail/error as 2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4aa3733f062a2d1ded0bc74e95205673/untitled2221.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 10, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461454499, 'issue_id': 2599148452, 'author': 'nicolaspi', 'body': '@tilakrayal I am still getting the error (with 2.18). Please note that the test is non-deterministic and there is a small probability of success.', 'created_at': datetime.datetime(2024, 11, 7, 6, 51, 34, tzinfo=datetime.timezone.utc)}]","nicolaspi (Issue Creator) on (2024-10-19 13:03:53 UTC): Potentially related: https://github.com/tensorflow/tensorflow/issues/72388

tilakrayal (Assginee) on (2024-10-21 16:43:21 UTC): @nicolaspi,
The PR which was raised for the issue has been merged and also tried to execute the mentioned code on both tensorflow v2.17 and tf-nightly, observed on tf-nightly the code was executed. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ab9434c9dab1de112d482293c2ad03ad/untitled2186.ipynb) and confirm whether the issue was resolved? Thank you!

nicolaspi (Issue Creator) on (2024-10-21 17:13:18 UTC): @tilakrayal Your gist is showing me an error when I run it (even for tf-nightly).

tilakrayal (Assginee) on (2024-11-07 06:10:34 UTC): @nicolaspi,
I tried to execute with the latest tensorflow v2.18(GPU), and the code was executed without fail/error as 2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4aa3733f062a2d1ded0bc74e95205673/untitled2221.ipynb). Thank you!

nicolaspi (Issue Creator) on (2024-11-07 06:51:34 UTC): @tilakrayal I am still getting the error (with 2.18). Please note that the test is non-deterministic and there is a small probability of success.

"
2598958998,issue,open,,[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

AlmaLinux 9.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.
The output is expected to be 0, since:
1. This behavior is not consistent with dtype=float64, where the output will be 0.
2. When input tensor contains only one value, the output will be 0.
3. The same code snippet will return different result on macOS, where the output is also 0.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

input = tf.constant(np.inf, dtype=tf.float64)
out = tf.math.reciprocal(input)
# tf.Tensor(0.0, shape=(), dtype=float64)
print(out)

input = tf.constant(np.inf, dtype=tf.complex128)
out = tf.math.reciprocal(input)
# tf.Tensor(0j, shape=(), dtype=complex128)
print(out)

input = tf.constant([np.inf, np.inf], dtype=tf.complex128)
out = tf.math.reciprocal(input)
# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)
print(out)
```


### Relevant log output

```shell
AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'
tf.Tensor(0.0, shape=(), dtype=float64)
tf.Tensor(0j, shape=(), dtype=complex128)
tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
```
",dlibk,2024-10-19 09:41:36+00:00,['Venkat6871'],2024-11-07 15:24:16+00:00,,https://github.com/tensorflow/tensorflow/issues/78298,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2425662750, 'issue_id': 2598958998, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/a81acfd2a335359feb3376c8d85958ff/78298_tf-2-17-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 21, 6, 1, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459491167, 'issue_id': 2598958998, 'author': 'VadisettyRahul', 'body': 'HI @dlibk @Venkat6871 \r\n\r\nA Custom Wrapper could be implemented, creating a function that will check if the input is infinite before applying the reciprocal. When it detects inf or -inf, the function will return 0 instead of NaN.\r\n\r\nWhat do you think?', 'created_at': datetime.datetime(2024, 11, 6, 11, 25, 17, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-21 06:01:23 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/a81acfd2a335359feb3376c8d85958ff/78298_tf-2-17-nightly-v.ipynb) here for reference.
Thank you!

VadisettyRahul on (2024-11-06 11:25:17 UTC): HI @dlibk @Venkat6871 

A Custom Wrapper could be implemented, creating a function that will check if the input is infinite before applying the reciprocal. When it detects inf or -inf, the function will return 0 instead of NaN.

What do you think?

"
2597960032,issue,closed,completed,Import TensorFlow Error: Conflict with Mesop and rules_python Causes AttributeError,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0, tf-nightly==2.18.0.dev20240906

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

Not specified

### GCC/compiler version

Not specified

### CUDA/cuDNN version

Not installed (running on CPU)

### GPU model and memory

Not applicable (running on CPU)

### Current behavior?

When I installed both TensorFlow and Mesop simultaneously, I encountered an error while importing TensorFlow. It seems that installing Mesop also installs rules_python, which causes TensorFlow to malfunction.

Specifically, at the following code:    
[tensorflow/tensorflow/python/platform/resource_loader.py]

Lines 117 to 119 in [bc90265]

 r = runfiles.Create() 
 new_fpath = r.Rlocation( 
     _os.path.abspath(_os.path.join('tensorflow', path)))

r becomes None, leading to an error.

According to rules_python:
https://github.com/bazelbuild/rules_python/blob/0.26.0/python/runfiles/runfiles.py#L40

It seems possible that r = runfiles.Create() can indeed return None.

While it seems the issue could be linked to Mesop installing rules_python, there might also be an underlying problem with TensorFlow. I would appreciate your help in investigating further.

### Standalone code to reproduce the issue

```shell
$ pip install ""tf-nightly==2.18.0.dev20240906"" ""mesop==0.12.3""
$ python -c ""import tensorflow""

# I run this in docker image of python:3.11.
```


### Relevant log output

```shell
2024-09-06 21:45:07.412511: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.415758: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.425482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1725659107.441013      78 cuda_dnn.cc:8322] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1725659107.445948      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-06 21:45:07.463219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/__init__.py"", line 53, in <module>
    from tensorflow._api.v2 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>
    from tensorflow._api.v2.compat.v1 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py"", line 47, in <module>
    from tensorflow._api.v2.compat.v1 import lite
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py"", line 9, in <module>
    from tensorflow._api.v2.compat.v1.lite import experimental
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.lite.experimental import authoring
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py"", line 8, in <module>
    from tensorflow.lite.python.authoring.authoring import compatible # line: 263
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/authoring/authoring.py"", line 42, in <module>
    from tensorflow.lite.python import convert
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/convert.py"", line 151, in <module>
    _deprecated_conversion_binary = _resource_loader.get_path_to_datafile(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/platform/resource_loader.py"", line 118, in get_path_to_datafile
    new_fpath = r.Rlocation(
                ^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'Rlocation'
```
",saradha04,2024-10-18 16:39:14+00:00,['tilakrayal'],2024-11-06 02:00:23+00:00,2024-11-06 02:00:20+00:00,https://github.com/tensorflow/tensorflow/issues/78201,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2427160290, 'issue_id': 2597960032, 'author': 'tilakrayal', 'body': ""@saradha04,\r\nAs a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might work and the Project can be build successfully.\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 10, 21, 16, 29, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443011860, 'issue_id': 2597960032, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458567557, 'issue_id': 2597960032, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 6, 2, 0, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458567677, 'issue_id': 2597960032, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78201"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78201"">No</a>', 'created_at': datetime.datetime(2024, 11, 6, 2, 0, 22, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-21 16:29:41 UTC): @saradha04,
As a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might work and the Project can be build successfully.

Thank you!

github-actions[bot] on (2024-10-29 02:02:51 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-06 02:00:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-06 02:00:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78201"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78201"">No</a>

"
2597297955,issue,open,,Duplicate Logs After tf.saved_model.save with Custom Logging Configurations,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Fedora Linux 40

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using tf.saved_model.save, log messages are duplicated in my application. This happens because TensorFlow adds its own handlers, which interfere with my custom logging setup.

TensorFlow should respect existing logging configurations and avoid adding unnecessary handlers. Some documentation to help align TensorFlow’s logging with user-defined settings would be great.

Setting logger.propagate = False works but requires extra setup.

My suggestion is to check for existing log handlers before adding new ones.

Thanks for looking into this.

### Standalone code to reproduce the issue

Here is the minimal code to reproduce the issue. You can check the code and results in this [colab notebook](https://colab.research.google.com/drive/1IB3IM81FCX9LJ0Ae8uW9p5OCvvJ986fm?usp=sharing) as well.

```python

import logging
import logging.config
import sys
from typing import Tuple
import tensorflow as tf
import numpy as np


def create_keras_sequential_model() -> tf.keras.Model:
    return tf.keras.Sequential(
        [
            tf.keras.layers.Input(shape=(10,)),
            tf.keras.layers.Dense(64, activation=""relu""),
            tf.keras.layers.Dense(1),
        ]
    )


def generate_random_data(num_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
    x = np.random.random((num_samples, 10))
    y = np.random.random((num_samples, 1))
    return x, y


def compile_model(model: tf.keras.Model) -> None:
    model.compile(optimizer=""sgd"", loss=""mean_absolute_error"", metrics=[""mae""])


def fit_model(
    model: tf.keras.Model, x: np.ndarray, y: np.ndarray, epochs: int = 10
) -> None:
    model.fit(x, y, epochs=epochs, verbose=0)


def create_compile_return_model() -> None:
    model = create_keras_sequential_model()
    compile_model(model)
    x, y = generate_random_data()
    fit_model(model, x, y)
    return model


def get_custom_formatter_logger() -> logging.Logger:
    log_format_with_time = ""%(asctime)s - %(levelname)s %(message)s""
    logHandler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter(log_format_with_time)
    logHandler.setFormatter(formatter)
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    logger.addHandler(logHandler)
    # logger.propagate = False #If you add this, then logs are not duplicated
    return logger


if __name__ == ""__main__"":

    logger = get_custom_formatter_logger()
    logger.info(""TF version: %s"", tf.__version__)
    logger.info(""Started"")
    keras_model = create_compile_return_model()
    logger.info(""Model created, compiled and trained. Now saving the model!"")
    tf.saved_model.save(keras_model, ""my_fake_model"")
    logger.info(""Model saved!"")
    logger.info(""Logs are now duplicated!"")
    logger.info(""Finished"")

```


### Relevant log output

```
2024-10-18 13:23:10,159 - INFO TF version: 2.17.0
2024-10-18 13:23:10,160 - INFO Started
2024-10-18 13:23:11,400 - INFO Model created, compiled and trained. Now saving the model!
2024-10-18 13:23:11,832 - INFO Model saved!
INFO:__main__:Model saved!
2024-10-18 13:23:11,832 - INFO Logs are now duplicated!
INFO:__main__:Logs are now duplicated!
2024-10-18 13:23:11,832 - INFO Finished
INFO:__main__:Finished
```
",arianmaghsoudnia,2024-10-18 11:41:23+00:00,['Venkat6871'],2024-10-22 05:25:49+00:00,,https://github.com/tensorflow/tensorflow/issues/78190,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2426100968, 'issue_id': 2597297955, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/9dc80b1c6b48d704bbb8b2e0b25bc2dd/78190_tf-2-17-0-nightly.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 21, 9, 21, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-21 09:21:00 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/9dc80b1c6b48d704bbb8b2e0b25bc2dd/78190_tf-2-17-0-nightly.ipynb) here for reference.
Thank you!

"
2595759154,issue,closed,completed,"`tf.keras.metrics.R2Score()`: ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>","As [also reported on Stack Overflow by another person](https://stackoverflow.com/questions/78056806/using-tf-keras-metrics-r2score-results-in-an-error-in-tensorflow).

### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Windows 10 64-bit

### Mobile device

_No response_

### Python version

3.11.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected to use `tf.keras.metrics.R2Score()` as easily as `tf.keras.metrics.RootMeanSquaredError()`, but I'm getting this instead:

> ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>

This happens in the following line:
> history = model.fit(trainDataForPrediction, trainDataTrueValues, epochs=300, batch_size=1, verbose=0)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

trainDataForPrediction = np.array([[[0.28358589],
  [0.30372512],
  [0.29780091],
  [0.33183642],
  [0.33120391],
  [0.33995099]],

 [[0.66235955],
  [0.35913154],
  [0.44153985],
  [0.32184616],
  [0.36265909],
  [0.3549683 ]],

 [[0.31142234],
  [0.66259034],
  [0.72903083],
  [0.77104302],
  [0.72910771],
  [0.75193211]],

 [[0.90720823],
  [0.72759569],
  [0.62614929],
  [0.69093327],
  [0.73826299],
  [0.72309858]],

 [[0.6095221 ],
  [0.77340943],
  [0.81678509],
  [0.80538922],
  [0.81804642],
  [0.8251804 ]],

 [[0.80261632],
  [0.71335692],
  [0.60358738],
  [0.64392465],
  [0.70798606],
  [0.68685222]],

 [[0.780457  ],
  [0.78226247],
  [0.90243802],
  [0.97144548],
  [0.94602405],
  [0.96125509]],

 [[0.79170093],
  [0.90229303],
  [0.8141366 ],
  [0.80853979],
  [0.78771087],
  [0.77839755]],

 [[0.61180146],
  [0.69044191],
  [0.5812535 ],
  [0.47918308],
  [0.46885173],
  [0.46438816]],

 [[0.62133159],
  [0.46832587],
  [0.45011011],
  [0.43797561],
  [0.46931518],
  [0.49728175]],

 [[0.59755109],
  [0.63108946],
  [0.64450683],
  [0.67581521],
  [0.66612456],
  [0.65878372]],

 [[0.71132382],
  [0.72192407],
  [0.71825596],
  [0.77809111],
  [0.71006228],
  [0.69495688]],

 [[0.4333941 ],
  [0.47057709],
  [0.46120598],
  [0.45281569],
  [0.44260477],
  [0.44248343]],

 [[0.52482055],
  [0.56745374],
  [0.65914372],
  [0.57337102],
  [0.69907015],
  [0.69499166]],

 [[0.51284886],
  [0.33358419],
  [0.31609008],
  [0.23997269],
  [0.08639418],
  [0.08413338]],

 [[0.26429119],
  [0.45916246],
  [0.44680846],
  [0.4449086 ],
  [0.52549847],
  [0.55313779]],

 [[0.4566679 ],
  [0.41502596],
  [0.66814448],
  [0.79515032],
  [0.82469515],
  [0.85223724]],

 [[0.8428317 ],
  [1.        ],
  [0.94682836],
  [0.95609914],
  [0.9859931 ],
  [0.95404273]],

 [[0.89188471],
  [0.92726165],
  [0.86781746],
  [0.8164678 ],
  [0.79838713],
  [0.79180823]],

 [[0.74514082],
  [0.63961743],
  [0.3959561 ],
  [0.65873789],
  [0.66951841],
  [0.69643851]],

 [[0.68831417],
  [0.74602272],
  [0.75794952],
  [0.62336891],
  [0.6014669 ],
  [0.57675219]],

 [[0.29146979],
  [0.41276609],
  [0.60938479],
  [0.7062046 ],
  [0.65504986],
  [0.66181513]],

 [[0.88358238],
  [0.72456903],
  [0.51257023],
  [0.40234096],
  [0.43700235],
  [0.42401991]],

 [[0.3591383 ],
  [0.69884845],
  [0.74231565],
  [0.72416779],
  [0.6708481 ],
  [0.6731879 ]],

 [[0.94124425],
  [0.83152508],
  [0.82366235],
  [0.80077871],
  [0.80614143],
  [0.79487525]],

 [[0.34668558],
  [0.23052622],
  [0.17238472],
  [0.2675286 ],
  [0.26344458],
  [0.28616361]],

 [[0.40986458],
  [0.35779146],
  [0.40335441],
  [0.44973167],
  [0.41253347],
  [0.37845105]],

 [[0.33203832],
  [0.27771177],
  [0.30814096],
  [0.16146156],
  [0.1718526 ],
  [0.18814805]],

 [[0.63741835],
  [0.66444711],
  [0.76911393],
  [0.7553838 ],
  [0.76645967],
  [0.76460272]],

 [[0.33085441],
  [0.44095143],
  [0.35532193],
  [0.43949481],
  [0.46892119],
  [0.4662825 ]],

 [[0.65113037],
  [0.91117417],
  [0.9335289 ],
  [0.89285049],
  [0.89504786],
  [0.91245301]],

 [[0.92886475],
  [0.7068268 ],
  [0.60644207],
  [0.57394975],
  [0.57464331],
  [0.54921686]],

 [[0.23284108],
  [0.22316557],
  [0.20152097],
  [0.45580923],
  [0.45287703],
  [0.45928762]],

 [[0.45123298],
  [0.39450794],
  [0.48112946],
  [0.32824454],
  [0.2944551 ],
  [0.30711642]],

 [[0.66849429],
  [0.6326308 ],
  [0.57193197],
  [0.50133743],
  [0.4672485 ],
  [0.44125429]],

 [[0.3493021 ],
  [0.43485091],
  [0.46408419],
  [0.75744247],
  [0.79000517],
  [0.80664802]],

 [[0.59350822],
  [0.55769807],
  [0.63017208],
  [0.26459647],
  [0.18448017],
  [0.14864757]],

 [[0.2809532 ],
  [0.22152394],
  [0.18470882],
  [0.23680976],
  [0.27114282],
  [0.28851017]],

 [[0.46479513],
  [0.52150761],
  [0.53962938],
  [0.56391452],
  [0.53710149],
  [0.5308821 ]],

 [[0.61977787],
  [0.53154372],
  [0.50561344],
  [0.48908166],
  [0.47152856],
  [0.48861851]],

 [[0.31765691],
  [0.5696298 ],
  [0.68688768],
  [0.611828  ],
  [0.59800787],
  [0.58199944]],

 [[0.59364795],
  [0.44172827],
  [0.31675594],
  [0.35414828],
  [0.36070871],
  [0.39298799]],

 [[0.19718846],
  [0.30401209],
  [0.51566878],
  [0.64076456],
  [0.65299798],
  [0.63290334]],

 [[0.72989215],
  [0.64011724],
  [0.60324933],
  [0.51062346],
  [0.45331722],
  [0.46125121]],

 [[0.549944  ],
  [0.34359706],
  [0.28630835],
  [0.37263408],
  [0.51816687],
  [0.53117809]],

 [[0.7357174 ],
  [0.82513636],
  [0.92903864],
  [0.83082154],
  [0.71830423],
  [0.68545151]]])

trainDataTrueValues = np.array([[0.33370854, 0.32896128, 0.338919  , 0.370148  , 0.41977692, 0.5521488 ],
 [0.365207  , 0.37061936, 0.37484066, 0.3478887 , 0.32885199, 0.30680109],
 [0.75690644, 0.76740645, 0.78093759, 0.80580592, 0.83506068, 0.9300879 ],
 [0.72214934, 0.71222063, 0.70721571, 0.72001991, 0.86853872, 0.78016653],
 [0.81758234, 0.81016924, 0.80366251, 0.81069042, 0.60300473, 0.67470109],
 [0.67958566, 0.68243936, 0.69163868, 0.74519473, 0.68240246, 0.657002  ],
 [0.96831789, 0.96380285, 0.967898  , 0.92530772, 0.9249375 , 0.93694259],
 [0.76195766, 0.74630911, 0.7047356 , 0.69865743, 0.64689554, 0.53129387],
 [0.47209114, 0.48193162, 0.50943131, 0.52597968, 0.65194851, 0.79167671],
 [0.52327628, 0.56134685, 0.60585979, 0.65919966, 0.59725093, 0.57757021],
 [0.65063778, 0.63845143, 0.6223349 , 0.59585136, 0.62452674, 0.66366742],
 [0.66665364, 0.644637  , 0.61860204, 0.60778969, 0.54817006, 0.53309155],
 [0.44629018, 0.43732508, 0.45198314, 0.40540066, 0.45934156, 0.44508884],
 [0.68928946, 0.69242095, 0.66407551, 0.65466724, 0.63588645, 0.62255665],
 [0.07655137, 0.07586849, 0.07615533, 0.09743152, 0.0912761 , 0.16081511],
 [0.57516233, 0.5752103 , 0.58274857, 0.60408212, 0.53677125, 0.38215918],
 [0.87645224, 0.89860724, 0.91237928, 0.90458273, 0.89167839, 0.86169194],
 [0.93496343, 0.91752935, 0.91871312, 0.93298075, 0.90635008, 0.93339182],
 [0.78620334, 0.77966131, 0.76595499, 0.80123668, 0.72281454, 0.67729982],
 [0.71433298, 0.73036564, 0.74620482, 0.71141186, 0.80361011, 0.84697337],
 [0.54462913, 0.5222716 , 0.51101144, 0.52252069, 0.42480727, 0.26657974],
 [0.6645752 , 0.66903111, 0.66718311, 0.66090196, 0.68263579, 0.85079916],
 [0.41923131, 0.42102118, 0.44039002, 0.48348755, 0.48306699, 0.36817135],
 [0.68148362, 0.67589061, 0.66555973, 0.69096076, 0.7228609 , 0.78776612],
 [0.7854791 , 0.78995575, 0.79338535, 0.72868889, 0.65642879, 0.55843462],
 [0.28512477, 0.27293793, 0.25881146, 0.26540709, 0.22930567, 0.33778585],
 [0.37266819, 0.37910424, 0.38644206, 0.36064735, 0.43564156, 0.35146986],
 [0.22257434, 0.23625543, 0.24991159, 0.28900138, 0.30654842, 0.42902441],
 [0.7425433 , 0.73684753, 0.73618661, 0.70964112, 0.67040764, 0.62850268],
 [0.45068071, 0.43816298, 0.4342914 , 0.45724268, 0.43607784, 0.55865807],
 [0.92236959, 0.93100085, 0.92969332, 0.93081777, 0.96367614, 0.89586521],
 [0.5410671 , 0.52699706, 0.52766478, 0.51691315, 0.43398716, 0.33637598],
 [0.46335955, 0.46776761, 0.46408167, 0.44867432, 0.43701596, 0.51659065],
 [0.30417175, 0.30733531, 0.30366558, 0.29330711, 0.36359768, 0.38749372],
 [0.42664812, 0.42056716, 0.43086576, 0.40887337, 0.42715668, 0.57628272],
 [0.82901749, 0.83238729, 0.82457459, 0.84872239, 0.79996528, 0.62709093],
 [0.12056511, 0.10651576, 0.10280307, 0.07995919, 0.07564526, 0.21194409],
 [0.30114611, 0.31115646, 0.31270446, 0.33757487, 0.40753736, 0.43746691],
 [0.52919291, 0.52264534, 0.51790728, 0.51318749, 0.44302725, 0.40943982],
 [0.48224635, 0.48409421, 0.48324061, 0.47385317, 0.55736301, 0.52762245],
 [0.57722016, 0.57718821, 0.5700168 , 0.59701639, 0.50802179, 0.44843445],
 [0.39456566, 0.38874   , 0.41657823, 0.39331915, 0.41278882, 0.39932694],
 [0.64290878, 0.65892973, 0.65161573, 0.61453231, 0.68637572, 0.70285098],
 [0.45496414, 0.43906435, 0.42968136, 0.4435593 , 0.38087753, 0.53327326],
 [0.53351884, 0.54998068, 0.56712283, 0.62159043, 0.74422592, 0.76377224],
 [0.67259377, 0.65934765, 0.64005251, 0.56716475, 0.41110739, 0.3281523 ]])

def createNeuralNetwork(hidden_units=9, dense_units=6, input_shape=(12-6,1), activation=['relu','sigmoid']):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(hidden_units,input_shape=input_shape,activation=activation[0]))
    model.add(tf.keras.layers.Dense(units=dense_units,activation=activation[1]))
    model.add(tf.keras.layers.Dense(units=dense_units,activation=activation[1]))
    model.add(tf.keras.layers.Dense(units=dense_units,activation=activation[1]))
    model.compile(loss='mse', metrics=['mae', tf.keras.metrics.RootMeanSquaredError(), 'mse', tf.keras.metrics.R2Score()], optimizer='adam')
    return model

model   = createNeuralNetwork()
history = model.fit(trainDataForPrediction, trainDataTrueValues, epochs=300, batch_size=1, verbose=0)
```


### Relevant log output
```
Traceback (most recent call last):

  File C:\ProgramData\miniconda3\envs\secas3\Lib\site-packages\spyder_kernels\customize\utils.py:209 in exec_encapsulate_locals
    exec_fun(compile(code_ast, filename, ""exec""), globals)

  File e:\poscomp\python\redeneuralsecas\neuralnetwork\untitled3.py:383
    history = model.fit(trainDataForPrediction, trainDataTrueValues, epochs=300, batch_size=1, verbose=0)

  File C:\ProgramData\miniconda3\envs\secas3\Lib\site-packages\keras\src\utils\traceback_utils.py:70 in error_handler
    raise e.with_traceback(filtered_tb) from None

  File C:\ProgramData\miniconda3\envs\secas3\Lib\site-packages\tensorflow\python\framework\ops.py:965 in __tf_tensor__
    raise ValueError(

ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>
```",A-Infor,2024-10-17 21:10:22+00:00,['tilakrayal'],2024-10-24 11:32:43+00:00,2024-10-24 11:32:39+00:00,https://github.com/tensorflow/tensorflow/issues/78126,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2422334528, 'issue_id': 2595759154, 'author': 'A-Infor', 'body': 'A simpler code that also gives the same error:\r\n\r\n```\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(10, activation=""relu""))\r\nmodel.add(keras.layers.Dense(1))\r\n\r\nmodel.compile(\r\n    optimizer=""adam"",\r\n    loss=keras.losses.MeanSquaredError(),\r\n    metrics=[keras.metrics.R2Score()]\r\n)\r\n\r\ndata = np.random.random((1000, 5))\r\nlabels = data.sum(axis=1)\r\n\r\nmodel.fit(data, labels, epochs=10)\r\n```\r\nSource: [DataScience Stack Exchange](https://datascience.stackexchange.com/a/129793/159260)', 'created_at': datetime.datetime(2024, 10, 18, 12, 13, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427153932, 'issue_id': 2595759154, 'author': 'tilakrayal', 'body': '@A-Infor,\r\nI tried to execute both codes on the latest TensorFlow v2.17 which consists of Keras3.0 and observed that both are executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2af1276474da4681ccee3cada4a2ab2b/untitled2185.ipynb). I request to upgrade to the latest TensorFlow v2.17. Thank you!', 'created_at': datetime.datetime(2024, 10, 21, 16, 26, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427638648, 'issue_id': 2595759154, 'author': 'A-Infor', 'body': 'I tested on TensorFlow 2.15 and noticed that it works fine.\r\nSeems to be a bug with version 2.13 .', 'created_at': datetime.datetime(2024, 10, 21, 20, 18, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430914130, 'issue_id': 2595759154, 'author': 'tilakrayal', 'body': ""@A-Infor,\r\n It's unlikely for TF 2.13 version to receive any bug fixes except when we have security patches. There is a high possibility that this was fixed with later TF versions. Perhaps you can use the latest tf version 2.15 & 2.17 where the issue was resolved for your case. Thank you!"", 'created_at': datetime.datetime(2024, 10, 23, 5, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431808728, 'issue_id': 2595759154, 'author': 'A-Infor', 'body': '> Perhaps you can use the latest tf version 2.15 & 2.17 where the issue was resolved for your case. Thank you!\r\n\r\nThe latest version of TensorFlow for Windows 64 on https://anaconda.org/search?q=platform:win-64+tensorflow is 2.13 .\r\nThere is no obvious way to use a newer TensorFlow. (I tested TensorFlow 2.15 on another computer that have Linux)', 'created_at': datetime.datetime(2024, 10, 23, 11, 29, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435026276, 'issue_id': 2595759154, 'author': 'A-Infor', 'body': '> The latest version of TensorFlow for Windows 64 on https://anaconda.org/search?q=platform:win-64+tensorflow is 2.13 . There is no obvious way to use a newer TensorFlow. (I tested TensorFlow 2.15 on another computer that have Linux)\r\n\r\nWrong. Just use `pip` instead of `conda` and you will be able to install TensorFlow 2.17.', 'created_at': datetime.datetime(2024, 10, 24, 11, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435026362, 'issue_id': 2595759154, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78126"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78126"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 11, 32, 42, tzinfo=datetime.timezone.utc)}]","A-Infor (Issue Creator) on (2024-10-18 12:13:58 UTC): A simpler code that also gives the same error:

```
from tensorflow import keras
import numpy as np

model = keras.Sequential()
model.add(keras.layers.Dense(10, activation=""relu""))
model.add(keras.layers.Dense(1))

model.compile(
    optimizer=""adam"",
    loss=keras.losses.MeanSquaredError(),
    metrics=[keras.metrics.R2Score()]
)

data = np.random.random((1000, 5))
labels = data.sum(axis=1)

model.fit(data, labels, epochs=10)
```
Source: [DataScience Stack Exchange](https://datascience.stackexchange.com/a/129793/159260)

tilakrayal (Assginee) on (2024-10-21 16:26:38 UTC): @A-Infor,
I tried to execute both codes on the latest TensorFlow v2.17 which consists of Keras3.0 and observed that both are executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2af1276474da4681ccee3cada4a2ab2b/untitled2185.ipynb). I request to upgrade to the latest TensorFlow v2.17. Thank you!

A-Infor (Issue Creator) on (2024-10-21 20:18:56 UTC): I tested on TensorFlow 2.15 and noticed that it works fine.
Seems to be a bug with version 2.13 .

tilakrayal (Assginee) on (2024-10-23 05:06:00 UTC): @A-Infor,
 It's unlikely for TF 2.13 version to receive any bug fixes except when we have security patches. There is a high possibility that this was fixed with later TF versions. Perhaps you can use the latest tf version 2.15 & 2.17 where the issue was resolved for your case. Thank you!

A-Infor (Issue Creator) on (2024-10-23 11:29:13 UTC): The latest version of TensorFlow for Windows 64 on https://anaconda.org/search?q=platform:win-64+tensorflow is 2.13 .
There is no obvious way to use a newer TensorFlow. (I tested TensorFlow 2.15 on another computer that have Linux)

A-Infor (Issue Creator) on (2024-10-24 11:32:40 UTC): Wrong. Just use `pip` instead of `conda` and you will be able to install TensorFlow 2.17.

google-ml-butler[bot] on (2024-10-24 11:32:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78126"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78126"">No</a>

"
2595681439,issue,closed,completed,Cannot include headers loader.h session.h,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.5.0

### GCC/compiler version

Clang 17.0.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

only CPU

### Current behavior?

During adding headers tensorflow/core/public/session.h and tensorflow/cc/saved_model/loader.h there are issues with EventCount.h from eigen libary.

Using MSVC 2022, but same error occurs while using LLVM/MSVC2019 compiler with my C++ application (in Qt 6.5.0)
and same thing happend when i was trying to use TF 2.9.3 GPU compiled with tested version of tools from TF official page .


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/saved_model/loader.h""

which results in

1) C:\Users\Kuba\tensorflow\bazel-bin\tensorflow\include\Eigen\src\ThreadPool\EventCount.h:130: error: C2059: syntax error: 'public'

2) C:\Users\Kuba\tensorflow\bazel-bin\tensorflow\include\Eigen\src\ThreadPool\EventCount.h:130: error: C2513: 'const unsigned __int64': no variable declaration before '='

3) C:\Users\Kuba\tensorflow\bazel-bin\tensorflow\include\Eigen\src\ThreadPool\EventCount.h:137: error: C2143: syntax error: missing ';' before '{'

4) C:\Users\Kuba\tensorflow\bazel-bin\tensorflow\include\Eigen\src\ThreadPool\EventCount.h:140: error: C2181: illegal else without preceding if statement
```


### Relevant log output

```shell
Below is eventcount.h with indicated which error occur in which place

// This file is part of Eigen, a lightweight C++ template library
// for linear algebra.
//
// Copyright (C) 2016 Dmitry Vyukov <dvyukov@google.com>
//
// This Source Code Form is subject to the terms of the Mozilla
// Public License v. 2.0. If a copy of the MPL was not distributed
// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.

#ifndef EIGEN_CXX11_THREADPOOL_EVENTCOUNT_H
#define EIGEN_CXX11_THREADPOOL_EVENTCOUNT_H

// IWYU pragma: private
#include ""./InternalHeaderCheck.h""

namespace Eigen {

// EventCount allows to wait for arbitrary predicates in non-blocking
// algorithms. Think of condition variable, but wait predicate does not need to
// be protected by a mutex. Usage:
// Waiting thread does:
//
//   if (predicate)
//     return act();
//   EventCount::Waiter& w = waiters[my_index];
//   ec.Prewait(&w);
//   if (predicate) {
//     ec.CancelWait(&w);
//     return act();
//   }
//   ec.CommitWait(&w);
//
// Notifying thread does:
//
//   predicate = true;
//   ec.Notify(true);
//
// Notify is cheap if there are no waiting threads. Prewait/CommitWait are not
// cheap, but they are executed only if the preceding predicate check has
// failed.
//
// Algorithm outline:
// There are two main variables: predicate (managed by user) and state_.
// Operation closely resembles Dekker mutual algorithm:
// https://en.wikipedia.org/wiki/Dekker%27s_algorithm
// Waiting thread sets state_ then checks predicate, Notifying thread sets
// predicate then checks state_. Due to seq_cst fences in between these
// operations it is guaranteed than either waiter will see predicate change
// and won't block, or notifying thread will see state_ change and will unblock
// the waiter, or both. But it can't happen that both threads don't see each
// other changes, which would lead to deadlock.
class EventCount {
 public:
  class Waiter;

  EventCount(MaxSizeVector<Waiter>& waiters) : state_(kStackMask), waiters_(waiters) {
    eigen_plain_assert(waiters.size() < (1 << kWaiterBits) - 1);
  }

  ~EventCount() {
    // Ensure there are no waiters.
    eigen_plain_assert(state_.load() == kStackMask);
  }

  // Prewait prepares for waiting.
  // After calling Prewait, the thread must re-check the wait predicate
  // and then call either CancelWait or CommitWait.
  void Prewait() {
    uint64_t state = state_.load(std::memory_order_relaxed);
    for (;;) {
      CheckState(state);
      uint64_t newstate = state + kWaiterInc;
      CheckState(newstate);
      if (state_.compare_exchange_weak(state, newstate, std::memory_order_seq_cst)) return;
    }
  }

  // CommitWait commits waiting after Prewait.
  void CommitWait(Waiter* w) {
    eigen_plain_assert((w->epoch & ~kEpochMask) == 0);
    w->state = Waiter::kNotSignaled;
    const uint64_t me = (w - &waiters_[0]) | w->epoch;
    uint64_t state = state_.load(std::memory_order_seq_cst);
    for (;;) {
      CheckState(state, true);
      uint64_t newstate;
      if ((state & kSignalMask) != 0) {
        // Consume the signal and return immediately.
        newstate = state - kWaiterInc - kSignalInc;
      } else {
        // Remove this thread from pre-wait counter and add to the waiter stack.
        newstate = ((state & kWaiterMask) - kWaiterInc) | me;
        w->next.store(state & (kStackMask | kEpochMask), std::memory_order_relaxed);
      }
      CheckState(newstate);
      if (state_.compare_exchange_weak(state, newstate, std::memory_order_acq_rel)) {
        if ((state & kSignalMask) == 0) {
          w->epoch += kEpochInc;
          Park(w);
        }
        return;
      }
    }
  }

  // CancelWait cancels effects of the previous Prewait call.
  void CancelWait() {
    uint64_t state = state_.load(std::memory_order_relaxed);
    for (;;) {
      CheckState(state, true);
      uint64_t newstate = state - kWaiterInc;
      // We don't know if the thread was also notified or not,
      // so we should not consume a signal unconditionally.
      // Only if number of waiters is equal to number of signals,
      // we know that the thread was notified and we must take away the signal.
      if (((state & kWaiterMask) >> kWaiterShift) == ((state & kSignalMask) >> kSignalShift)) newstate -= kSignalInc;
      CheckState(newstate);
      if (state_.compare_exchange_weak(state, newstate, std::memory_order_acq_rel)) return;
    }
  }

  // Notify wakes one or all waiting threads.
  // Must be called after changing the associated wait predicate.
  void Notify(bool notifyAll) {
    std::atomic_thread_fence(std::memory_order_seq_cst);
    uint64_t state = state_.load(std::memory_order_acquire);
    for (;;) {
      CheckState(state);
      const uint64_t waiters = (state & kWaiterMask) >> kWaiterShift;
1) 2) 3)      const uint64_t signals = (state & kSignalMask) >> kSignalShift;
      // Easy case: no waiters.
1)      if ((state & kStackMask) == kStackMask && waiters == signals) return;
      uint64_t newstate;
      if (notifyAll) {
        // Empty wait stack and set signal to number of pre-wait threads.
        newstate = (state & kWaiterMask) | (waiters << kSignalShift) | kStackMask;
1) 3)      } else if (signals < waiters) {
        // There is a thread in pre-wait state, unblock it.
        newstate = state + kSignalInc;
4)      } else {
        // Pop a waiter from list and unpark it.
        Waiter* w = &waiters_[state & kStackMask];
        uint64_t next = w->next.load(std::memory_order_relaxed);
        newstate = (state & (kWaiterMask | kSignalMask)) | next;
      }
      CheckState(newstate);
      if (state_.compare_exchange_weak(state, newstate, std::memory_order_acq_rel)) {
 1)       if (!notifyAll && (signals < waiters)) return;  // unblocked pre-wait thread
        if ((state & kStackMask) == kStackMask) return;
        Waiter* w = &waiters_[state & kStackMask];
        if (!notifyAll) w->next.store(kStackMask, std::memory_order_relaxed);
        Unpark(w);
        return;
      }
    }
  }

  class Waiter {
    friend class EventCount;
    // Align to 128 byte boundary to prevent false sharing with other Waiter
    // objects in the same vector.
    EIGEN_ALIGN_TO_BOUNDARY(128) std::atomic<uint64_t> next;
    EIGEN_MUTEX mu;
    EIGEN_CONDVAR cv;
    uint64_t epoch = 0;
    unsigned state = kNotSignaled;
    enum {
      kNotSignaled,
      kWaiting,
      kSignaled,
    };
  };

 private:
  // State_ layout:
  // - low kWaiterBits is a stack of waiters committed wait
  //   (indexes in waiters_ array are used as stack elements,
  //   kStackMask means empty stack).
  // - next kWaiterBits is count of waiters in prewait state.
  // - next kWaiterBits is count of pending signals.
  // - remaining bits are ABA counter for the stack.
  //   (stored in Waiter node and incremented on push).
  static const uint64_t kWaiterBits = 14;
  static const uint64_t kStackMask = (1ull << kWaiterBits) - 1;
  static const uint64_t kWaiterShift = kWaiterBits;
  static const uint64_t kWaiterMask = ((1ull << kWaiterBits) - 1) << kWaiterShift;
  static const uint64_t kWaiterInc = 1ull << kWaiterShift;
  static const uint64_t kSignalShift = 2 * kWaiterBits;
  static const uint64_t kSignalMask = ((1ull << kWaiterBits) - 1) << kSignalShift;
  static const uint64_t kSignalInc = 1ull << kSignalShift;
  static const uint64_t kEpochShift = 3 * kWaiterBits;
  static const uint64_t kEpochBits = 64 - kEpochShift;
  static const uint64_t kEpochMask = ((1ull << kEpochBits) - 1) << kEpochShift;
  static const uint64_t kEpochInc = 1ull << kEpochShift;
  std::atomic<uint64_t> state_;
  MaxSizeVector<Waiter>& waiters_;

  static void CheckState(uint64_t state, bool waiter = false) {
    static_assert(kEpochBits >= 20, ""not enough bits to prevent ABA problem"");
    const uint64_t waiters = (state & kWaiterMask) >> kWaiterShift;
1) 2)    const uint64_t signals = (state & kSignalMask) >> kSignalShift;
    eigen_plain_assert(waiters >= signals);
    eigen_plain_assert(waiters < (1 << kWaiterBits) - 1);
    eigen_plain_assert(!waiter || waiters > 0);
    (void)waiters;
1)    (void)signals;
  }

  void Park(Waiter* w) {
    EIGEN_MUTEX_LOCK lock(w->mu);
    while (w->state != Waiter::kSignaled) {
      w->state = Waiter::kWaiting;
      w->cv.wait(lock);
    }
  }

  void Unpark(Waiter* w) {
    for (Waiter* next; w; w = next) {
      uint64_t wnext = w->next.load(std::memory_order_relaxed) & kStackMask;
      next = wnext == kStackMask ? nullptr : &waiters_[internal::convert_index<size_t>(wnext)];
      unsigned state;
      {
        EIGEN_MUTEX_LOCK lock(w->mu);
        state = w->state;
        w->state = Waiter::kSignaled;
      }
      // Avoid notifying if it wasn't waiting.
      if (state == Waiter::kWaiting) w->cv.notify_one();
    }
  }

  EventCount(const EventCount&) = delete;
  void operator=(const EventCount&) = delete;
};

}  // namespace Eigen

#endif  // EIGEN_CXX11_THREADPOOL_EVENTCOUNT_H
```
",kubarybpl,2024-10-17 20:30:32+00:00,['Venkat6871'],2024-10-22 06:53:04+00:00,2024-10-22 06:53:00+00:00,https://github.com/tensorflow/tensorflow/issues/78125,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:core', 'issues related to core part of tensorflow'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2428401954, 'issue_id': 2595681439, 'author': 'kubarybpl', 'body': 'The issue is related to fact that word ""singals"" occurs in Qt as signal mechanism. Both libraries use signals as a identifier which leads to a namespace collision.\r\n\r\nThe solution is:\r\n\r\n#undef signals\r\n\r\n#include ""tensorflow/cc/saved_model/loader.h""\r\n#include ""tensorflow/core/public/session.h""\r\n\r\n#define signals', 'created_at': datetime.datetime(2024, 10, 22, 6, 53, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428402005, 'issue_id': 2595681439, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78125"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78125"">No</a>', 'created_at': datetime.datetime(2024, 10, 22, 6, 53, 2, tzinfo=datetime.timezone.utc)}]","kubarybpl (Issue Creator) on (2024-10-22 06:53:01 UTC): The issue is related to fact that word ""singals"" occurs in Qt as signal mechanism. Both libraries use signals as a identifier which leads to a namespace collision.

The solution is:

#undef signals

#include ""tensorflow/cc/saved_model/loader.h""
#include ""tensorflow/core/public/session.h""

#define signals

google-ml-butler[bot] on (2024-10-22 06:53:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78125"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78125"">No</a>

"
2594947603,issue,closed,completed,ResNet model has wrong output shape,"###  Issue

I applied the [tutorial](https://ai.google.dev/edge/litert/models/post_training_integer_quant) for post-training integer quantization to a ResNet model, but the conversion leads to a model with a wrong output shape of [   1    7    7 2048], instead of a 1000-element vector

### 1. System information

Google Colab, with the default tensorlow 2.17

### 2. Code

the image_array is a numpy array of shape (n, 224, 224, 3) and dtype float32

```
model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(image_array).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model_quant = converter.convert()

output_details = interpreter.get_output_details()[0]
for key in output_details.keys():
  print(key, output_details[key])
```",Lorsu,2024-10-17 14:40:06+00:00,['gaikwadrahul8'],2024-10-23 10:32:05+00:00,2024-10-23 10:32:01+00:00,https://github.com/tensorflow/tensorflow/issues/78111,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2425570570, 'issue_id': 2594947603, 'author': 'prabalai', 'body': '/assign', 'created_at': datetime.datetime(2024, 10, 21, 4, 42, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426465702, 'issue_id': 2594947603, 'author': 'gaikwadrahul8', 'body': 'Hi, @Lorsu \r\n\r\nI apologize for the delayed response, thank you for bringing this issue to our attention if possible could you please help us with exact Google colab notebook which you created with  **ResNet** model to replicate the same behavior from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 21, 11, 57, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2429469753, 'issue_id': 2594947603, 'author': 'gaikwadrahul8', 'body': ""Hi, @Lorsu \r\n\r\nIt seems like the issue you're facing is because you're using `include_top=False` in the `ResNet50` model which removes the final classification layers. To get the **1000** element output vector we need to include the top layers by setting `include_top=True` so model includes the final classification layers with output 1000 element vector corresponding to the **1000** ImageNet classes.This is useful when you want to use the model for ImageNet classification tasks.\r\nPlease refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/b871f283d9f579b03b6d959ca721b7f7/tflite-issue-78111.ipynb) \r\n\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 22, 14, 37, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431677926, 'issue_id': 2594947603, 'author': 'Lorsu', 'body': ""> Hi, @Lorsu\r\n> \r\n> It seems like the issue you're facing is because you're using `include_top=False` in the `ResNet50` model which removes the final classification layers. To get the **1000** element output vector we need to include the top layers by setting `include_top=True` so model includes the final classification layers with output 1000 element vector corresponding to the **1000** ImageNet classes.This is useful when you want to use the model for ImageNet classification tasks. Please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/b871f283d9f579b03b6d959ca721b7f7/tflite-issue-78111.ipynb)\r\n> \r\n> If I have missed something here please let me know.\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nyes, the issue is in the include_top parameter, thank you really much"", 'created_at': datetime.datetime(2024, 10, 23, 10, 32, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431678000, 'issue_id': 2594947603, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78111"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78111"">No</a>', 'created_at': datetime.datetime(2024, 10, 23, 10, 32, 3, tzinfo=datetime.timezone.utc)}]","prabalai on (2024-10-21 04:42:04 UTC): /assign

gaikwadrahul8 (Assginee) on (2024-10-21 11:57:19 UTC): Hi, @Lorsu 

I apologize for the delayed response, thank you for bringing this issue to our attention if possible could you please help us with exact Google colab notebook which you created with  **ResNet** model to replicate the same behavior from our end ? 

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-22 14:37:10 UTC): Hi, @Lorsu 

It seems like the issue you're facing is because you're using `include_top=False` in the `ResNet50` model which removes the final classification layers. To get the **1000** element output vector we need to include the top layers by setting `include_top=True` so model includes the final classification layers with output 1000 element vector corresponding to the **1000** ImageNet classes.This is useful when you want to use the model for ImageNet classification tasks.
Please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/b871f283d9f579b03b6d959ca721b7f7/tflite-issue-78111.ipynb) 

If I have missed something here please let me know.

Thank you for your cooperation and patience.

Lorsu (Issue Creator) on (2024-10-23 10:32:01 UTC): yes, the issue is in the include_top parameter, thank you really much

google-ml-butler[bot] on (2024-10-23 10:32:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78111"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78111"">No</a>

"
2594699295,issue,open,,"tf.linalg.expm fails to support half/float16 data type, which is inconsistent with doc","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/expm
tf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input = tf.constant(np.random.randn(1,1), dtype='float16')
out = tf.linalg.expm(input)
```
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]
All kernels registered for op MatrixSolve:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:MatrixSolve] name:
```
",drewshark,2024-10-17 13:09:08+00:00,['Venkat6871'],2024-11-07 15:07:12+00:00,,https://github.com/tensorflow/tensorflow/issues/78107,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2421171496, 'issue_id': 2594699295, 'author': 'drewshark', 'body': 'Hi, I noticed three similar issues:\r\nAPI tf.raw_ops.Inv does not support int8 input,\r\nAPI tf.raw_ops.MatrixSolveLs does not support float16 input,\r\nAPI tf.raw_ops.MatrixTriangularSolve does not support bfloat16 input.\r\n\r\nAccording to their documentation, these APIs should support the related input data type.\r\n\r\nHere is the reproducible code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntry:\r\n    x = tf.constant(np.random.randint(-10, 10, (1,1,1)), dtype=\'int8\')\r\n    tf.raw_ops.Inv(x=x)\r\nexcept Exception as e:\r\n    print(f""===> API tf.raw_ops.Inv should accept int8 according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/Inv, but crashes with the following error: {e}"")\r\n\r\ntry:\r\n    matrix = tf.constant(np.random.randn(0,0), dtype=\'float16\')\r\n    rhs = tf.constant(np.random.randn(0,0), dtype=\'float16\')\r\n    l2_regularizer = tf.constant(np.random.randint(-10, 10, ()), dtype=\'int16\')\r\n    fast = True\r\n    tf.raw_ops.MatrixSolveLs(matrix=matrix,rhs=rhs,l2_regularizer=l2_regularizer,fast=fast)\r\nexcept Exception as e:\r\n    print(f""===> API tf.raw_ops.MatrixSolveLs should accept float16/half matrix according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixSolveLs, but crashes with the following error: {e}"")\r\n\r\ntry:\r\n    matrix = tf.constant(np.random.randn(0,0), dtype=\'bfloat16\')\r\n    rhs = tf.constant(np.random.randn(0), dtype=\'bfloat16\')\r\n    lower = True\r\n    adjoint = True\r\n    tf.raw_ops.MatrixTriangularSolve(matrix=matrix,rhs=rhs,lower=lower,adjoint=adjoint)\r\nexcept Exception as e:\r\n    print(f""===> API tf.raw_ops.MatrixTriangularSolve should accept bfloat16 matrix according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixTriangularSolve, but crashes with the following error: {e}"")\r\n\r\n```\r\n\r\nI would appreciate it if you could also fix these three issues.', 'created_at': datetime.datetime(2024, 10, 18, 2, 55, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428563487, 'issue_id': 2594699295, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/dc72b39e9c375f7ca6a185f3046c4761/78107_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 22, 8, 10, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435557148, 'issue_id': 2594699295, 'author': 'aroun-coumar', 'body': ""It says \r\n\r\n```\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n```\r\n\r\nwhich means the MatrixSolve operation supports float16 (DT_HALF) only when using the XLA (Accelerated Linear Algebra) JIT compilation device, either on CPU or GPU.\r\n\r\nso you can use this\r\n\r\n`tf.config.optimizer.set_jit(True)`\r\n\r\nHope this helps!"", 'created_at': datetime.datetime(2024, 10, 24, 15, 6, 16, tzinfo=datetime.timezone.utc)}]","drewshark (Issue Creator) on (2024-10-18 02:55:31 UTC): Hi, I noticed three similar issues:
API tf.raw_ops.Inv does not support int8 input,
API tf.raw_ops.MatrixSolveLs does not support float16 input,
API tf.raw_ops.MatrixTriangularSolve does not support bfloat16 input.

According to their documentation, these APIs should support the related input data type.

Here is the reproducible code:

```
import tensorflow as tf
import numpy as np

try:
    x = tf.constant(np.random.randint(-10, 10, (1,1,1)), dtype='int8')
    tf.raw_ops.Inv(x=x)
except Exception as e:
    print(f""===> API tf.raw_ops.Inv should accept int8 according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/Inv, but crashes with the following error: {e}"")

try:
    matrix = tf.constant(np.random.randn(0,0), dtype='float16')
    rhs = tf.constant(np.random.randn(0,0), dtype='float16')
    l2_regularizer = tf.constant(np.random.randint(-10, 10, ()), dtype='int16')
    fast = True
    tf.raw_ops.MatrixSolveLs(matrix=matrix,rhs=rhs,l2_regularizer=l2_regularizer,fast=fast)
except Exception as e:
    print(f""===> API tf.raw_ops.MatrixSolveLs should accept float16/half matrix according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixSolveLs, but crashes with the following error: {e}"")

try:
    matrix = tf.constant(np.random.randn(0,0), dtype='bfloat16')
    rhs = tf.constant(np.random.randn(0), dtype='bfloat16')
    lower = True
    adjoint = True
    tf.raw_ops.MatrixTriangularSolve(matrix=matrix,rhs=rhs,lower=lower,adjoint=adjoint)
except Exception as e:
    print(f""===> API tf.raw_ops.MatrixTriangularSolve should accept bfloat16 matrix according to https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixTriangularSolve, but crashes with the following error: {e}"")

```

I would appreciate it if you could also fix these three issues.

Venkat6871 (Assginee) on (2024-10-22 08:10:22 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/dc72b39e9c375f7ca6a185f3046c4761/78107_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

aroun-coumar on (2024-10-24 15:06:16 UTC): It says 

```
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
```

which means the MatrixSolve operation supports float16 (DT_HALF) only when using the XLA (Accelerated Linear Algebra) JIT compilation device, either on CPU or GPU.

so you can use this

`tf.config.optimizer.set_jit(True)`

Hope this helps!

"
2594086834,issue,open,,DLPack with Int32 tensor on the GPU: inconsistent eager mode / graph mode / XLA,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-117097-gecf05620570 2.19.0-dev20241016

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.

Currently, the `device` field of a tensor created with:
```py
with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
```
*does* indicate it's a GPU tensor: `/job:localhost/replica:0/task:0/device:GPU:0`.

However, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.
There even seems to be a unit test validating this:
https://github.com/tensorflow/tensorflow/blob/d3de971a7348ecaefdbb920e580c37ebde10d780/tensorflow/python/dlpack/dlpack_test.py#L75-L78


However, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).

Would it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def f_eager(x):
    return x
f_graph = tf.function()(f_eager)
f_xla = tf.function(jit_compile=True)(f_eager)


with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
    print(""Original tensor:"", x.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(x)
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Default:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Eager:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Graph:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""XLA:"", x_.device)
```


### Relevant log output

```shell
Original tensor: /job:localhost/replica:0/task:0/device:GPU:0
Default: /job:localhost/replica:0/task:0/device:CPU:0
Eager: /job:localhost/replica:0/task:0/device:CPU:0
Graph: /job:localhost/replica:0/task:0/device:CPU:0
XLA: /job:localhost/replica:0/task:0/device:GPU:0
```
",merlinND,2024-10-17 08:59:51+00:00,['tilakrayal'],2024-10-22 08:06:11+00:00,,https://github.com/tensorflow/tensorflow/issues/78091,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('comp:gpu', 'GPU related issues')]","[{'comment_id': 2428551651, 'issue_id': 2594086834, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on TensorFlow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bca3e97f5d4b156e1545ba4b7fe4c7f2/untitled2187.ipynb).\r\n\r\nThe output for GPU is below\r\n```python\r\nOriginal tensor: /job:localhost/replica:0/task:0/device:GPU:0\r\nDefault: /job:localhost/replica:0/task:0/device:CPU:0\r\nEager: /job:localhost/replica:0/task:0/device:CPU:0\r\nGraph: /job:localhost/replica:0/task:0/device:CPU:0\r\nXLA: /job:localhost/replica:0/task:0/device:GPU:0\r\n```', 'created_at': datetime.datetime(2024, 10, 22, 8, 5, 21, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-22 08:05:21 UTC): I was able to reproduce the issue on TensorFlow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bca3e97f5d4b156e1545ba4b7fe4c7f2/untitled2187.ipynb).

The output for GPU is below
```python
Original tensor: /job:localhost/replica:0/task:0/device:GPU:0
Default: /job:localhost/replica:0/task:0/device:CPU:0
Eager: /job:localhost/replica:0/task:0/device:CPU:0
Graph: /job:localhost/replica:0/task:0/device:CPU:0
XLA: /job:localhost/replica:0/task:0/device:GPU:0
```

"
2588446767,issue,closed,completed,"TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux CentOS 7.9

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We used tensorflow for 3 class classifications. The code worked well on existed environment: Python 3.8.3 + tensorflow 2.13.1.
But when we tried on new enviroment: Python 3.12.4 + tensorflow 2.17.0, several errors blocked.

### Standalone code to reproduce the issue


Here are the simple code to reproduce the issue.
```
import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np


def build_model():
    metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
    loss = tf.keras.losses.sparse_categorical_crossentropy
    num_class = 3
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(num_class, activation='softmax')(lstm_out)
    model = tf.keras.Model(inputs, output)
    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen():
    while True:
        yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))

model = build_model()
cw = {0: 0.3, 1:2.5, 2:3.2}
model.fit(data_gen(), epochs=2, steps_per_epoch=10, class_weight=cw)
```

The first error we met:
```
Traceback (most recent call last):
  File ""/data/release/infinity_stock2/tf_model.py"", line 25, in <module>
    model.fit(data_gen(), epochs=2, steps_per_epoch=10, class_weight=cw)
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/trainers/data_adapters/__init__.py"", line 108, in get_data_adapter
    raise ValueError(
ValueError: Argument `class_weight` is not supported for Python generator inputs. Received: class_weight={0: 0.3, 1: 2.5, 2: 3.2}
```

To solve the issue, we used tf dataset to wrap the generator. 
```
import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np


def build_model():
    metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
    loss = tf.keras.losses.sparse_categorical_crossentropy
    num_class = 3
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(num_class, activation='softmax')(lstm_out)
    model = tf.keras.Model(inputs, output)
    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen():
    while True:
        yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))

model = build_model()
cw = {0: 0.3, 1:2.5, 2:3.2}
data = tf.data.Dataset.from_generator(data_gen, output_types=(tf.float32, tf.float32),  output_shapes=((None, 40, 36), (None, None)))
model.fit(data, epochs=2, steps_per_epoch=10, class_weight=cw)
```
And another error happened:
```
Traceback (most recent call last):
  File ""/data/release/infinity_stock2/tf_model.py"", line 26, in <module>
    model.fit(data, epochs=2, steps_per_epoch=10, class_weight=cw)
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/tensorflow/python/ops/cond_v2.py"", line 876, in error
    raise TypeError(
TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values.

true_fn output: Tensor(""cond/Identity:0"", shape=(None,), dtype=int64)
false_fn output: Tensor(""cond/Identity:0"", shape=(None,), dtype=int32)

Error details:
Tensor(""cond/Identity:0"", shape=(None,), dtype=int64) and Tensor(""cond/Identity:0"", shape=(None,), dtype=int32) have different types
```
It looks like class weight might not work well.

### Relevant log output

_No response_",henghamao,2024-10-15 11:16:10+00:00,['Venkat6871'],2025-02-02 07:52:20+00:00,2025-02-02 07:52:18+00:00,https://github.com/tensorflow/tensorflow/issues/77958,"[('type:bug', 'Bug'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2413622877, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'Another try, we removed class weight in fit(), another error happened.\r\nAll these code worked well on old enviroment Python 3.84. + tf 2.15, but failed on Python 3.12.4 + tf 2.17.0.\r\n```\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nimport numpy as np\r\n\r\n\r\ndef build_model():\r\n    metrics = [\'accuracy\', tf.keras.metrics.AUC(multi_label=True)]\r\n    loss = tf.keras.losses.sparse_categorical_crossentropy\r\n    num_class = 3\r\n    inputs = tf.keras.layers.Input(shape=(40, 36))\r\n    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)\r\n    output = Dense(num_class, activation=\'softmax\')(lstm_out)\r\n    model = tf.keras.Model(inputs, output)\r\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\r\n    model.summary()\r\n    return model\r\n\r\ndef data_gen():\r\n    while True:\r\n        yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))\r\n\r\nmodel = build_model()\r\ncw = {0: 0.3, 1:2.5, 2:3.2}\r\ndata = tf.data.Dataset.from_generator(data_gen, output_types=(tf.float32, tf.float32),  output_shapes=((None, 40, 36), (None, None)))\r\nmodel.fit(data, epochs=2, steps_per_epoch=10)#, class_weight=cw)\r\n```\r\nError log:\r\n```\r\nEpoch 1/2\r\nTraceback (most recent call last):\r\n  File ""/data/release/infinity_stock2/tf_model.py"", line 26, in <module>\r\n    model.fit(data, epochs=2, steps_per_epoch=10)#, class_weight=cw)\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py"", line 652, in sparse_categorical_crossentropy\r\n    raise ValueError(\r\nValueError: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, None), output.shape=(None, 3)\r\n```', 'created_at': datetime.datetime(2024, 10, 15, 11, 25, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415696647, 'issue_id': 2588446767, 'author': 'keshav861', 'body': ""@henghamao \r\n I modified the data_gen function to output not only the data (x_batch, y_batch) but also sample_weights, which are calculated based on the class_weights.\r\n\r\nInstead of class_weight, the model now uses sample_weights per batch in the generator, which handles class imbalance.\r\n\r\ncan you check this code out:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nimport numpy as np\r\n\r\n\r\ndef build_model():\r\n    metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]\r\n    loss = tf.keras.losses.sparse_categorical_crossentropy\r\n    num_class = 3\r\n    inputs = tf.keras.layers.Input(shape=(40, 36))\r\n    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)\r\n    output = Dense(num_class, activation='softmax')(lstm_out)\r\n    model = tf.keras.Model(inputs, output)\r\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\r\n    model.summary()\r\n    return model\r\n\r\ndef data_gen(class_weights):\r\n    while True:\r\n        x_batch = np.random.rand(128, 40, 36)\r\n        y_batch = np.random.randint(0, 3, (128, 1))\r\n        # Apply class weights to the labels\r\n        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)\r\n        yield x_batch, y_batch, sample_weights\r\n\r\nmodel = build_model()\r\ncw = {0: 0.3, 1: 2.5, 2: 3.2}\r\nmodel.fit(data_gen(cw), epochs=2, steps_per_epoch=10)\r\n```\r\n\r\nthis shows no error output for tis code\r\n![image](https://github.com/user-attachments/assets/cadbbaa0-0636-4dfd-aed0-ed0d4dec89fc)\r\n\r\nwhat do you say about this??"", 'created_at': datetime.datetime(2024, 10, 16, 4, 17, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415982969, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'Thanks for providing work around solutions!\r\nBy using sample weight, we could do the model training on tf 2.17.', 'created_at': datetime.datetime(2024, 10, 16, 7, 46, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418444215, 'issue_id': 2588446767, 'author': 'henghamao', 'body': '@keshav861 \r\nDoes sample_weight and class_weight behave the same in model training?\r\nWe observed different training results from tf 2.15 used class weight and tf 2.17 used sample weight.\r\ntf 2.17 stopped at epoch 1 with best val_loss, while tf 2.15 using class weight stopped at epoch 3 with best val loss. And thus, tf 2.15 got better training results.\r\nBTW, the training speed is about 25% faster on new enviroment python 3.12 + tf 2.17.', 'created_at': datetime.datetime(2024, 10, 17, 3, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422500002, 'issue_id': 2588446767, 'author': 'Venkat6871', 'body': 'Hi **@henghamao** ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow 2.13.0, and it worked fine. I also tested it with versions 2.17.0 and the nightly release, and I encountered the same issue. However, it works fine with an alternative approach. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/dbd119ef636ff94f9a5fb97052429194/77958_tf-2-13-2-17-nightly-v.ipynb) here for reference. You mentioned that the training speed is faster in this alternative way — this is because Keras 3 is working on JAX, which enhances performance compared to older versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 18, 13, 38, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422529400, 'issue_id': 2588446767, 'author': 'henghamao', 'body': ""@Venkat6871 \r\nThanks for the reply.\r\nI read the gist, the approch is almost the same as @keshav861 provided.\r\nThe code used 'sample weight' instead of 'class weight' to deal with imbalance data classifications.\r\nHowever, our question is whether sample weight behave exactly the same as class weight in model training.\r\nWe observed different training results from tf 2.13 used class weight and tf 2.17 used sample weight. Not sure it is caused by run to run variance.\r\nComparing early stop epoch, tf 2.17 using sample weight stopped at epoch 1 with best val_loss, while tf 2.13 using class weight stopped at epoch 3 with best val loss. tf 2.13 using class weight got better training results with smaller val_loss tested by our data."", 'created_at': datetime.datetime(2024, 10, 18, 13, 51, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456571411, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'Any updates on this issue?\r\nWill ""class weight\' be fixed in latest version of TF?', 'created_at': datetime.datetime(2024, 11, 5, 8, 47, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2593707411, 'issue_id': 2588446767, 'author': 'Mustafa-MS', 'body': ""I've been struggling for 2 days with this bug, I changed many things in my code, and the problem was the (class_weight) have a bug with tensorflow > 2.15\njust revert it back to 2.13 and the code is working great !"", 'created_at': datetime.datetime(2025, 1, 15, 18, 52, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594239133, 'issue_id': 2588446767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 0, 56, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594241698, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'We reported the issue 3 months ago, and seems the issue still not resolved.\nWe had to stay at old python version and old tf 2.13.', 'created_at': datetime.datetime(2025, 1, 16, 0, 58, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594242270, 'issue_id': 2588446767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 0, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594292757, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'Wrong operation to close the issue.\nAs the issue had not resolved, reopen it and warit for the response.', 'created_at': datetime.datetime(2025, 1, 16, 1, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596704391, 'issue_id': 2588446767, 'author': 'mihaimaruseac', 'body': 'Transforming `if` from a control block to an expression requires that both branches are present and have the same type. Please fix your code to pass the additional checks that have been added, this is working as intended.', 'created_at': datetime.datetime(2025, 1, 16, 19, 37, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597183802, 'issue_id': 2588446767, 'author': 'henghamao', 'body': '""Transforming if from a control block to an expression requires that both branches are present and have the same type. ""\nSorry, I did not catch the problem of the code. Could you please help clarify the issue?\nThe code worked correctly with tf 2.13, but failed with tf 2.15.', 'created_at': datetime.datetime(2025, 1, 17, 0, 30, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598527593, 'issue_id': 2588446767, 'author': 'mihaimaruseac', 'body': ""It was an error before and it got fixed.\n\nIn imperative constructs you can have\n```python\nif condition():\n  true_fn()\n# missing else\n```\nand\n```\nif condition():\n  true_fn() # returns a list of ints\nelse:\n  false_fn() # returns None / doesn't return a value\n```\n\nBut in functional constructs, you have an expression `cond(condition, true_fn, false_fn)`. Already need to pass both branches. If you want the code to do type inference correctly, then the type of the return value of `cond` needs to be consistent and to reach that you need to have the types of `true_fn` and `false_fn` be equivalent.\n\nUntil 2.13 this was not completely correctly done, but it got fixed."", 'created_at': datetime.datetime(2025, 1, 17, 14, 46, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598551293, 'issue_id': 2588446767, 'author': 'henghamao', 'body': 'From your description, it looks like a bug fix inside tf code caused the issue.\nOur concern is how to use class weight for new version of tf.\nThe previouse code did not work now.', 'created_at': datetime.datetime(2025, 1, 17, 14, 58, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629261341, 'issue_id': 2588446767, 'author': 'SanjaySG', 'body': ""@henghamao \nLooking into the keras/src/backend/tensorflow/trainer.py and keras/src/trainers/data_adapters code, `class_weight` and `sample_weight` should behave in a similar way for most common data types (I checked the code for ArrayDataAdapter and tf.dataset types, but haven't looked into other types). In fact, `class_weights` are converted into `sample_weights` internally in both implementations. For example: https://github.com/keras-team/keras/blob/master/keras/src/trainers/data_adapters/array_data_adapter.py#L66-L74. \n\nTo test this, I initialized two versions of the model in your code and set the initial weights for the models to be the same. Then I ran model.fit() on the first model with `sample_weights`, and on the second model with the `class_weights` (using the dictionary). Note that I set `shuffle=False` explicitly during the model.fit() call, so that each training step for both models uses the same data.  The metrics (accuracy, auc and loss) are identical for each epoch for both models. Here is the [gist](https://colab.research.google.com/gist/SanjaySG/e3ff0cb28cd6a28cb71e4e5a1790a178). \n\nA few caveats. I tested the code on TF 2.18.0. I'm not sure if an earlier version had a different implementation. Both the models in the gist pass the inputs and outputs to the model directly i.e. they do not use a generator. `class_weights` do not work with a generator and there was never support for it AFAIK [(source)](https://github.com/keras-team/keras/blob/master/keras/src/trainers/data_adapters/__init__.py#L112-L116). \n\nI hope this clears up your question on whether sample_weight and class_weight work the same. Let me know if you have any other doubts."", 'created_at': datetime.datetime(2025, 2, 2, 6, 14, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629285912, 'issue_id': 2588446767, 'author': 'henghamao', 'body': '@SanjaySG \nThanks!\nWe will compare model training results running with sample weight and class weight.', 'created_at': datetime.datetime(2025, 2, 2, 7, 52, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629285965, 'issue_id': 2588446767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>', 'created_at': datetime.datetime(2025, 2, 2, 7, 52, 19, tzinfo=datetime.timezone.utc)}]","henghamao (Issue Creator) on (2024-10-15 11:25:10 UTC): Another try, we removed class weight in fit(), another error happened.
All these code worked well on old enviroment Python 3.84. + tf 2.15, but failed on Python 3.12.4 + tf 2.17.0.
```

import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np


def build_model():
    metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
    loss = tf.keras.losses.sparse_categorical_crossentropy
    num_class = 3
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(num_class, activation='softmax')(lstm_out)
    model = tf.keras.Model(inputs, output)
    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen():
    while True:
        yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))

model = build_model()
cw = {0: 0.3, 1:2.5, 2:3.2}
data = tf.data.Dataset.from_generator(data_gen, output_types=(tf.float32, tf.float32),  output_shapes=((None, 40, 36), (None, None)))
model.fit(data, epochs=2, steps_per_epoch=10)#, class_weight=cw)
```
Error log:
```
Epoch 1/2
Traceback (most recent call last):
  File ""/data/release/infinity_stock2/tf_model.py"", line 26, in <module>
    model.fit(data, epochs=2, steps_per_epoch=10)#, class_weight=cw)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py"", line 652, in sparse_categorical_crossentropy
    raise ValueError(
ValueError: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, None), output.shape=(None, 3)
```

keshav861 on (2024-10-16 04:17:45 UTC): @henghamao 
 I modified the data_gen function to output not only the data (x_batch, y_batch) but also sample_weights, which are calculated based on the class_weights.

Instead of class_weight, the model now uses sample_weights per batch in the generator, which handles class imbalance.

can you check this code out:

```
import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np


def build_model():
    metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
    loss = tf.keras.losses.sparse_categorical_crossentropy
    num_class = 3
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(num_class, activation='softmax')(lstm_out)
    model = tf.keras.Model(inputs, output)
    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen(class_weights):
    while True:
        x_batch = np.random.rand(128, 40, 36)
        y_batch = np.random.randint(0, 3, (128, 1))
        # Apply class weights to the labels
        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)
        yield x_batch, y_batch, sample_weights

model = build_model()
cw = {0: 0.3, 1: 2.5, 2: 3.2}
model.fit(data_gen(cw), epochs=2, steps_per_epoch=10)
```

this shows no error output for tis code
![image](https://github.com/user-attachments/assets/cadbbaa0-0636-4dfd-aed0-ed0d4dec89fc)

what do you say about this??

henghamao (Issue Creator) on (2024-10-16 07:46:07 UTC): Thanks for providing work around solutions!
By using sample weight, we could do the model training on tf 2.17.

henghamao (Issue Creator) on (2024-10-17 03:54:00 UTC): @keshav861 
Does sample_weight and class_weight behave the same in model training?
We observed different training results from tf 2.15 used class weight and tf 2.17 used sample weight.
tf 2.17 stopped at epoch 1 with best val_loss, while tf 2.15 using class weight stopped at epoch 3 with best val loss. And thus, tf 2.15 got better training results.
BTW, the training speed is about 25% faster on new enviroment python 3.12 + tf 2.17.

Venkat6871 (Assginee) on (2024-10-18 13:38:13 UTC): Hi **@henghamao** ,
Apologies for the delay. I tried running your code on Colab using TensorFlow 2.13.0, and it worked fine. I also tested it with versions 2.17.0 and the nightly release, and I encountered the same issue. However, it works fine with an alternative approach. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/dbd119ef636ff94f9a5fb97052429194/77958_tf-2-13-2-17-nightly-v.ipynb) here for reference. You mentioned that the training speed is faster in this alternative way — this is because Keras 3 is working on JAX, which enhances performance compared to older versions.
Thank you!

henghamao (Issue Creator) on (2024-10-18 13:51:41 UTC): @Venkat6871 
Thanks for the reply.
I read the gist, the approch is almost the same as @keshav861 provided.
The code used 'sample weight' instead of 'class weight' to deal with imbalance data classifications.
However, our question is whether sample weight behave exactly the same as class weight in model training.
We observed different training results from tf 2.13 used class weight and tf 2.17 used sample weight. Not sure it is caused by run to run variance.
Comparing early stop epoch, tf 2.17 using sample weight stopped at epoch 1 with best val_loss, while tf 2.13 using class weight stopped at epoch 3 with best val loss. tf 2.13 using class weight got better training results with smaller val_loss tested by our data.

henghamao (Issue Creator) on (2024-11-05 08:47:44 UTC): Any updates on this issue?
Will ""class weight' be fixed in latest version of TF?

Mustafa-MS on (2025-01-15 18:52:55 UTC): I've been struggling for 2 days with this bug, I changed many things in my code, and the problem was the (class_weight) have a bug with tensorflow > 2.15
just revert it back to 2.13 and the code is working great !

google-ml-butler[bot] on (2025-01-16 00:56:04 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>

henghamao (Issue Creator) on (2025-01-16 00:58:05 UTC): We reported the issue 3 months ago, and seems the issue still not resolved.
We had to stay at old python version and old tf 2.13.

google-ml-butler[bot] on (2025-01-16 00:58:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>

henghamao (Issue Creator) on (2025-01-16 01:45:18 UTC): Wrong operation to close the issue.
As the issue had not resolved, reopen it and warit for the response.

mihaimaruseac on (2025-01-16 19:37:13 UTC): Transforming `if` from a control block to an expression requires that both branches are present and have the same type. Please fix your code to pass the additional checks that have been added, this is working as intended.

henghamao (Issue Creator) on (2025-01-17 00:30:15 UTC): ""Transforming if from a control block to an expression requires that both branches are present and have the same type. ""
Sorry, I did not catch the problem of the code. Could you please help clarify the issue?
The code worked correctly with tf 2.13, but failed with tf 2.15.

mihaimaruseac on (2025-01-17 14:46:57 UTC): It was an error before and it got fixed.

In imperative constructs you can have
```python
if condition():
  true_fn()
# missing else
```
and
```
if condition():
  true_fn() # returns a list of ints
else:
  false_fn() # returns None / doesn't return a value
```

But in functional constructs, you have an expression `cond(condition, true_fn, false_fn)`. Already need to pass both branches. If you want the code to do type inference correctly, then the type of the return value of `cond` needs to be consistent and to reach that you need to have the types of `true_fn` and `false_fn` be equivalent.

Until 2.13 this was not completely correctly done, but it got fixed.

henghamao (Issue Creator) on (2025-01-17 14:58:47 UTC): From your description, it looks like a bug fix inside tf code caused the issue.
Our concern is how to use class weight for new version of tf.
The previouse code did not work now.

SanjaySG on (2025-02-02 06:14:03 UTC): @henghamao 
Looking into the keras/src/backend/tensorflow/trainer.py and keras/src/trainers/data_adapters code, `class_weight` and `sample_weight` should behave in a similar way for most common data types (I checked the code for ArrayDataAdapter and tf.dataset types, but haven't looked into other types). In fact, `class_weights` are converted into `sample_weights` internally in both implementations. For example: https://github.com/keras-team/keras/blob/master/keras/src/trainers/data_adapters/array_data_adapter.py#L66-L74. 

To test this, I initialized two versions of the model in your code and set the initial weights for the models to be the same. Then I ran model.fit() on the first model with `sample_weights`, and on the second model with the `class_weights` (using the dictionary). Note that I set `shuffle=False` explicitly during the model.fit() call, so that each training step for both models uses the same data.  The metrics (accuracy, auc and loss) are identical for each epoch for both models. Here is the [gist](https://colab.research.google.com/gist/SanjaySG/e3ff0cb28cd6a28cb71e4e5a1790a178). 

A few caveats. I tested the code on TF 2.18.0. I'm not sure if an earlier version had a different implementation. Both the models in the gist pass the inputs and outputs to the model directly i.e. they do not use a generator. `class_weights` do not work with a generator and there was never support for it AFAIK [(source)](https://github.com/keras-team/keras/blob/master/keras/src/trainers/data_adapters/__init__.py#L112-L116). 

I hope this clears up your question on whether sample_weight and class_weight work the same. Let me know if you have any other doubts.

henghamao (Issue Creator) on (2025-02-02 07:52:04 UTC): @SanjaySG 
Thanks!
We will compare model training results running with sample weight and class weight.

google-ml-butler[bot] on (2025-02-02 07:52:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77958"">No</a>

"
2588139942,issue,closed,completed,Error Compiling TensorFlow: Invalid AVX512 Flag During Bazel Build,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04(x64)

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**Current Behavior:**

_While building TensorFlow using Bazel 6.5.0 on a x64 Ubuntu 22.04 machine with the following command_:
`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --jobs=24 --local_cpu_resources=12 --verbose_failures -s`

_The build fails with the error:_
```
gcc: error: unrecognized command-line option '-mavx512fp16'; did you mean '-mavx512bf16'?
Target //tensorflow/tools/pip_package:wheel failed to build.
```

**Expected Behavior:**

The build should successfully complete without encountering any issues, allowing TensorFlow to build and generate the desired wheel package on the x64 platform.

### Standalone code to reproduce the issue

```shell
To reproduce the issue, you can follow these steps on an x64 Ubuntu 22.04 machine:
1. Clone the TensorFlow repository:
   git clone https://github.com/tensorflow/tensorflow.git
   cd tensorflow
   git checkout v2.17.0

2. Install Bazel 6.5.0 and required dependencies.
3. Run the following Bazel build command:
   bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --jobs=24 --local_cpu_resources=12 --verbose_failures -s
```


### Relevant log output

```shell
Here is the relevant portion of the log output:

SUBCOMMAND: # @curl//:curl [action 'Compiling lib/vauth/oauth2.c [for tool]', configuration: 68326eafa7a63b7eaf0428a35495cf2cafd361e10c823
7afd816e7ee4ba618d5, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/86fb503c75b079be2d3bf5ec3140c43c/execroot/org_tensorflow && \
  exec env - \
    PATH=/workdir/env_py/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/bin:/root/bin:/root/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/curl/_objs/curl/oauth2.pic.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/curl/_objs/curl/oauth2.pic.o' -fPIC -DCURL_STATICLIB '-DBAZEL_CURRENT_REPOSITORY=""curl""' -iquote external/curl -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/curl -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/boringssl -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/boringssl -isystem external/curl/include -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/curl/include -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -isystem external/boringssl/src/include -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/boringssl/src/include -g0 -w -Iexternal/curl/lib -D_GNU_SOURCE -DBUILDING_LIBCURL -DHAVE_CONFIG_H -DCURL_DISABLE_FTP -DCURL_DISABLE_NTLM -DHAVE_LIBZ-DHAVE_ZLIB_H -Wno-string-plus-int '-DCURL_MAX_WRITE_SIZE=65536' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/curl/lib/vauth/oauth2.c -o bazel-out/k8-opt-exec-50AE0418/bin/external/curl/_objs/curl/oauth2.pic.o)
# Configuration: 68326eafa7a63b7eaf0428a35495cf2cafd361e10c8237afd816e7ee4ba618d5
# Execution platform: @local_execution_config_platform//:platform
ERROR: /root/.cache/bazel/_bazel_root/86fb503c75b079be2d3bf5ec3140c43c/external/XNNPACK/BUILD.bazel:669:36: Compiling src/f16-vbinary/gen/
f16-vsubc-avx512fp16-u64.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:avx512fp16_prod_microkernels)
  (cd /root/.cache/bazel/_bazel_root/86fb503c75b079be2d3bf5ec3140c43c/execroot/org_tensorflow && \
  exec env - \
    PATH=/workdir/env_py/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/bin:/root/bin:/root/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/workdir/env_py/bin/python3 \
    PYTHON_LIB_PATH=/workdir/env_py/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/XNNPACK/_objs/avx512fp16_prod_microkernels/f16-vsubc-avx512fp16-u64.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/XNNPACK/_objs/avx512fp16_prod_microkernels/f16-vsubc-avx512fp16-u64.pic.o' -fPIC -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DXNN_ENABLE_SRM_SME=0' '-DXNN_ENABLE_ARM_SME2=0' '-DBAZEL_CURRENT_REPOSITORY=""XNNPACK""' -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv
```
",rpushkarr,2024-10-15 09:13:27+00:00,['tilakrayal'],2024-11-02 02:00:54+00:00,2024-11-02 02:00:50+00:00,https://github.com/tensorflow/tensorflow/issues/77950,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2422738305, 'issue_id': 2588139942, 'author': 'tilakrayal', 'body': '@rpushkarr,\r\nAs per the official documentation, for the latest tensorflow v2.17.0, the compiler is Clang 17.0. Please try with the similar configuration.\r\ntensorflow-2.17.0, python -3.9-3.12, Clang 17.0.6 and Bazel 6.5.0\r\nhttps://www.tensorflow.org/install/source#cpu\r\n\r\nAlso as an alternative, use a Docker container from https://hub.docker.com/r/tensorflow/build/tags. It comes with Clang pre-installed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/70199\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 18, 15, 31, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439173955, 'issue_id': 2588139942, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 26, 1, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797031, 'issue_id': 2588139942, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797060, 'issue_id': 2588139942, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77950"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77950"">No</a>', 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-18 15:31:15 UTC): @rpushkarr,
As per the official documentation, for the latest tensorflow v2.17.0, the compiler is Clang 17.0. Please try with the similar configuration.
tensorflow-2.17.0, python -3.9-3.12, Clang 17.0.6 and Bazel 6.5.0
https://www.tensorflow.org/install/source#cpu

Also as an alternative, use a Docker container from https://hub.docker.com/r/tensorflow/build/tags. It comes with Clang pre-installed.

https://github.com/tensorflow/tensorflow/issues/70199

Thank you!

github-actions[bot] on (2024-10-26 01:59:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-02 02:00:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-02 02:00:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77950"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77950"">No</a>

"
2588076784,issue,open,,Cannot pass $LOCAL_CUDNN_PATH as /usr,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.rc0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

12

### CUDA/cuDNN version

8.9.0

### GPU model and memory

GTX 4090

### Current behavior?

If I pass `LOCAL_CUDNN_PATH`  as /usr, just like the docker image nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04, then tensorflow bazel would create a symlink: external/cudnn/include  as /usr/include, 
then, tensorflow would pass `-isystem external/cuda_cudnn/include`
after that, `#include_next` always skip the current directory external/cuda_cudnn/include, which is actually /usr/include, then bazel would report the error: 
```
/usr/local/include/c++/12/cstdlib:75:15: fatal error: stdlib.h: No such file or directory
 #include_next <stdlib.h>
               ^~~~~~~~~~
compilation terminated.
```
If I do not pass `LOCAL_CUDNN_PATH` and let tensorflow redownload a cudnn library, things would be very smooth.

### Standalone code to reproduce the issue

```shell
git submodule sync
git submodule update --init --recursive

export _GLIBCXX_USE_CXX11_ABI=1
. /work/conda_init.sh \
    && conda activate py3 \
    && HERMETIC_CUDA_VERSION=12.1.0 HERMETIC_CUDNN_VERSION=8.9.4.25 HERMETIC_CUDA_COMPUTE_CAPABILITIES=9.0  TF_NVCC_CLANG=1 TF_NEED_TENSORRT=1 LOCAL_CUDANN_PATH=/usr  LOCAL_CUDA_PATH=$CUDA_HOME  TF_NEED_CUDA=1  CLANG_CUDA_COMPILER_PATH=/llvm_release_17_with_nvptx/bin/clang ./configure
. /work/conda_init.sh \
    && conda activate py3 \
    && export LOCAL_CUDA_PATH=$CUDA_HOME \
    && bazel build --config=opt --copt=""-Iexternal/cuda_cudnn/include""   --copt=""-Wno-error=unused-command-line-argument""  //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel  --config=dbg --copt=""-Wno-int-conversion""   --copt=""-Wno-error=extra-semi"" --copt=""-Wno-gnu-include-next""  --copt=""-Wno-error=c23-extensions""  --copt=""-Wno-error=overlength-strings""  --copt=""--gcc-install-dir=/usr/lib/gcc/x86_64-linux-gnu/12"" --verbose_failures  --subcommands
```
```


### Relevant log output

_No response_",xuesu,2024-10-15 08:49:13+00:00,"['penpornk', 'Venkat6871']",2024-11-27 06:33:39+00:00,,https://github.com/tensorflow/tensorflow/issues/77948,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('comp:core', 'issues related to core part of tensorflow'), ('2.18.rc', '')]","[{'comment_id': 2418622669, 'issue_id': 2588076784, 'author': 'Venkat6871', 'body': 'Hi **@xuesu** ,\r\nApologies for the delay. Could you please provide the exact error you are encountering with your code? This will help us debug the issue more easily.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 17, 6, 18, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418845678, 'issue_id': 2588076784, 'author': 'xuesu', 'body': '```\r\n/usr/local/include/c++/12/cstdlib:75:15: fatal error: stdlib.h: No such file or directory\r\n #include_next <stdlib.h>\r\n               ^~~~~~~~~~\r\ncompilation terminated.\r\n```\r\nwhen `cmake --build .`, if I del -isystem external/cudnn/include, or change it into  -I external/cudnn/include, things would be smoothly', 'created_at': datetime.datetime(2024, 10, 17, 8, 3, 53, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-17 06:18:50 UTC): Hi **@xuesu** ,
Apologies for the delay. Could you please provide the exact error you are encountering with your code? This will help us debug the issue more easily.
Thank you!

xuesu (Issue Creator) on (2024-10-17 08:03:53 UTC): ```
/usr/local/include/c++/12/cstdlib:75:15: fatal error: stdlib.h: No such file or directory
 #include_next <stdlib.h>
               ^~~~~~~~~~
compilation terminated.
```
when `cmake --build .`, if I del -isystem external/cudnn/include, or change it into  -I external/cudnn/include, things would be smoothly

"
2587999533,issue,closed,completed,TensorFlow argmin function returns incorrect index when dealing with subnormal float values.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using the argmin function on an input array containing subnormal float values, TensorFlow incorrectly returns the index of 0.0 as the minimum value, even though a smaller subnormal value (-1.401298464324817e-45) exists in the array. Other deep learning frameworks (e.g., PyTorch, Chainer) correctly return the index of the subnormal value, but TensorFlow (and Keras) consistently return the index of 0.


### Standalone code to reproduce the issue

```shell
import torch
import tensorflow as tf
import numpy as np
from chainer import functions as F
import jax.numpy as jnp
import tensorflow.keras.backend as K

# Input data
input_data = [
    0.0,
    1.1754943508222875e-38,
    -1.401298464324817e-45,
    0.0,
    459367.0
]

# Test PyTorch
def test_pytorch_argmin(input_data):
    tensor = torch.tensor(input_data, dtype=torch.float32)
    result = torch.argmin(tensor).item()
    print(f""PyTorch argmin result: {result}"")
    return result

# Test TensorFlow
def test_tensorflow_argmin(input_data):
    tensor = tf.constant(input_data, dtype=tf.float32)
    result = tf.argmin(tensor).numpy()
    print(f""TensorFlow argmin result: {result}"")
    return result

# Test Keras using backend
def test_keras_argmin(input_data):
    tensor = K.constant(input_data, dtype=tf.float32)
    result = K.argmin(tensor, axis=-1).numpy()
    print(f""Keras argmin result: {result}"")
    return result

# Test Chainer
def test_chainer_argmin(input_data):
    tensor = np.array(input_data, dtype=np.float32)
    result = F.argmin(tensor).data
    print(f""Chainer argmin result: {result}"")
    return result

# Test JAX
def test_jax_argmin(input_data):
    tensor = jnp.array(input_data, dtype=jnp.float32)
    result = jnp.argmin(tensor).item()
    print(f""JAX argmin result: {result}"")
    return result

if __name__ == ""__main__"":
    pytorch_result = test_pytorch_argmin(input_data)
    tensorflow_result = test_tensorflow_argmin(input_data)
    keras_result = test_keras_argmin(input_data)
    chainer_result = test_chainer_argmin(input_data)
    jax_result = test_jax_argmin(input_data)

    print(""\nSummary of results:"")
    print(f""PyTorch argmin: {pytorch_result}"")
    print(f""TensorFlow argmin: {tensorflow_result}"")
    print(f""Keras argmin: {keras_result}"")
    print(f""Chainer argmin: {chainer_result}"")
    print(f""JAX argmin: {jax_result}"")

```
```


### Relevant log output

```shell
Summary of results:
PyTorch argmin: 2
TensorFlow argmin: 0
Keras argmin: 0
Chainer argmin: 2
JAX argmin: 0
```
```
",LilyDong0127,2024-10-15 08:15:27+00:00,['tilakrayal'],2024-11-02 02:00:57+00:00,2024-11-02 02:00:52+00:00,https://github.com/tensorflow/tensorflow/issues/77946,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2422729009, 'issue_id': 2587999533, 'author': 'tilakrayal', 'body': '@LilyDong0127,\r\nLooks like this is similar to the another issue which was raised for the Argmax.\r\nhttps://github.com/tensorflow/tensorflow/issues/77853\r\n\r\nAnd also calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero and subnormal numbers (like 1e-45) are flushed to zeros Thank you!', 'created_at': datetime.datetime(2024, 10, 18, 15, 26, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439173966, 'issue_id': 2587999533, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 26, 1, 59, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797047, 'issue_id': 2587999533, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797092, 'issue_id': 2587999533, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77946"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77946"">No</a>', 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 55, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-18 15:26:23 UTC): @LilyDong0127,
Looks like this is similar to the another issue which was raised for the Argmax.
https://github.com/tensorflow/tensorflow/issues/77853

And also calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero and subnormal numbers (like 1e-45) are flushed to zeros Thank you!

github-actions[bot] on (2024-10-26 01:59:54 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-02 02:00:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-02 02:00:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77946"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77946"">No</a>

"
2586428434,issue,open,,tf.math.special.bessel_* has inconsistent result with scipy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.

### Standalone code to reproduce the issue

```shell
import scipy
import numpy as np
import tensorflow as tf
x = tf.constant(-np.inf, dtype='float64')
print(""TF:"", tf.math.special.bessel_y1(x))
print(""Scipy: "", scipy.special.y1(x))
print(""TF:"", tf.math.special.bessel_y0(x))
print(""Scipy: "", scipy.special.y0(x))
print(""TF:"", tf.math.special.bessel_k0(x))
print(""Scipy: "", scipy.special.k0(x))
print(""TF:"", tf.math.special.bessel_k1(x))
print(""Scipy: "", scipy.special.k1(x))
```


### Relevant log output

```shell
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
```
",maybeLee,2024-10-14 15:45:19+00:00,['Venkat6871'],2024-10-22 07:17:27+00:00,,https://github.com/tensorflow/tensorflow/issues/77864,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2415915587, 'issue_id': 2586428434, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/7714b91a5cd2690b4676c7b121f657e9/77864_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 16, 7, 10, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-16 07:10:00 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/7714b91a5cd2690b4676c7b121f657e9/77864_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2586385312,issue,open,,"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When receiving this input:
```
x = tf.constant([[1,2],[2,3]])
```
`tf.math.is_strictly_increasing` outputs `False` instead of `True`.
Also, for a tensor with shape (1,3,3):
```
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
```
It's output is still `False` instead of `True` even when x's first dimension only has one element.
Based on the description ""Elements of x are compared in row-major order."", it seems that elements in x are compared along row (i.e., the first dimension).
Therefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([[1,2],[2,3]])
print(tf.math.is_strictly_increasing(x))  # False
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
print(tf.math.is_strictly_increasing(x))  # False
```
```


### Relevant log output

_No response_",maybeLee,2024-10-14 15:28:21+00:00,['tilakrayal'],2024-11-13 12:23:54+00:00,,https://github.com/tensorflow/tensorflow/issues/77863,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2418905197, 'issue_id': 2586385312, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it here.\r\n\r\n\r\n**Output in 2.17 and nightly:**\r\n\r\n```python\r\ntf.Tensor(False, shape=(), dtype=bool)\r\ntf.Tensor(False, shape=(), dtype=bool)\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 17, 8, 32, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2473463123, 'issue_id': 2586385312, 'author': 'jakubxy08', 'body': 'The description says: _""Elements of `x` are compared in row-major order.  The tensor `[x[0],...]`\r\n  is strictly increasing if for every adjacent pair we have `x[i] < x[i+1]`.\r\n  If `x` has less than two elements, it is trivially strictly increasing""_\r\n\r\nIt means that the multidimensional array is effectively flattened into a one-dimensional array in row-major order before comparison.\r\n\r\nThere is also element-wise comparison that probably reflect your scenario. It can be calculated with: \r\n\r\n```\r\na = tf.constant([[1, 2], [3, 4]])\r\nb = tf.constant([[2, 3], [4, 5]])\r\nresult = tf.math.less(a, b)\r\nprint(result)  # [[True, True], [True, True]]\r\n```', 'created_at': datetime.datetime(2024, 11, 13, 12, 23, 31, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-17 08:32:11 UTC): I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it here.


**Output in 2.17 and nightly:**

```python
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor(False, shape=(), dtype=bool)
```

Thank you!

jakubxy08 on (2024-11-13 12:23:31 UTC): The description says: _""Elements of `x` are compared in row-major order.  The tensor `[x[0],...]`
  is strictly increasing if for every adjacent pair we have `x[i] < x[i+1]`.
  If `x` has less than two elements, it is trivially strictly increasing""_

It means that the multidimensional array is effectively flattened into a one-dimensional array in row-major order before comparison.

There is also element-wise comparison that probably reflect your scenario. It can be calculated with: 

```
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[2, 3], [4, 5]])
result = tf.math.less(a, b)
print(result)  # [[True, True], [True, True]]
```

"
2585517359,issue,closed,completed, Failed to load the native TensorFlow runtime.,"ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[414], line 1
----> 1 import tensorflow

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\kabeer\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",Mammancy,2024-10-14 10:12:01+00:00,['Venkat6871'],2024-10-29 02:02:55+00:00,2024-10-29 02:02:55+00:00,https://github.com/tensorflow/tensorflow/issues/77854,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2410776402, 'issue_id': 2585517359, 'author': 'Venkat6871', 'body': 'Hi @Mammancy ,\r\nCould you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 10, 33, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058337, 'issue_id': 2585517359, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443011929, 'issue_id': 2585517359, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 10:33:49 UTC): Hi @Mammancy ,
Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue.
Thank you!

github-actions[bot] on (2024-10-22 02:02:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:02:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2585507563,issue,open,,argmax returns incorrect result for input containing Minimum number (TensorFlow 2.x),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Current Behavior:
When using tf.math.argmax on an input array that contains -0.0, the result is incorrect. Specifically, the function returns 1 (the index of -0.0) as the position of the maximum value, while the actual maximum value is 1.401298464324817e-45 at index 2.

The same behavior is observed in Keras and JAX, as both use TensorFlow internally for the argmax function.

Expected Behavior:
tf.math.argmax should return 2, as the value at index 2 (1.401298464324817e-45) is greater than both -1.0 and -0.0.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp
from tensorflow import keras

def test_argmax():
    # Input data
    input_data = np.array([-1.0, -0.0, 1.401298464324817e-45], dtype=np.float32)

    # PyTorch argmax
    pytorch_result = torch.argmax(torch.tensor(input_data, dtype=torch.float32)).item()
    print(f""PyTorch argmax result: {pytorch_result}"")

    # TensorFlow argmax
    tensorflow_result = tf.math.argmax(input_data).numpy()
    print(f""TensorFlow argmax result: {tensorflow_result}"")

    # Keras argmax (Keras internally uses TensorFlow, so should be the same)
    keras_result = keras.backend.argmax(input_data).numpy()
    print(f""Keras argmax result: {keras_result}"")

    # JAX argmax
    jax_result = jnp.argmax(input_data)
    print(f""JAX argmax result: {jax_result}"")

if __name__ == ""__main__"":
    test_argmax()

```


### Standalone code to reproduce the issue

```shell
PyTorch argmax result: 2
TensorFlow argmax result: 1
Keras argmax result: 1
JAX argmax result: 1

```
```


### Relevant log output

_No response_",LilyDong0127,2024-10-14 10:07:59+00:00,['tilakrayal'],2024-10-28 09:54:17+00:00,,https://github.com/tensorflow/tensorflow/issues/77853,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2422721419, 'issue_id': 2585507563, 'author': 'tilakrayal', 'body': '@LilyDong0127,\r\nIn TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero. So, the issue is not in argmax but in using FTZ mode in general on the CPU. Thank you!', 'created_at': datetime.datetime(2024, 10, 18, 15, 22, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439173986, 'issue_id': 2585507563, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 26, 1, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441102449, 'issue_id': 2585507563, 'author': 'LilyDong0127', 'body': ""> @LilyDong0127, In TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero. So, the issue is not in argmax but in using FTZ mode in general on the CPU. Thank you!\r\n\r\nThank you for your reply! But I don't think this is a problem with the FTZ mode. Logically, the execution effects on the CPU and GPU should be the same. However, there is no FTZ mode on the GPU, and if this mode is turned on by default, it may cause great misunderstanding to our users. Because users may not know that floating-point numbers will automatically return to zero when they are less than a certain threshold. This behavior may cause inconsistent results on different platforms and cause confusion and potential errors. I hope to explore this issue further, thank you!"", 'created_at': datetime.datetime(2024, 10, 28, 9, 54, 13, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-18 15:22:20 UTC): @LilyDong0127,
In TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero. So, the issue is not in argmax but in using FTZ mode in general on the CPU. Thank you!

github-actions[bot] on (2024-10-26 01:59:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

LilyDong0127 (Issue Creator) on (2024-10-28 09:54:13 UTC): Thank you for your reply! But I don't think this is a problem with the FTZ mode. Logically, the execution effects on the CPU and GPU should be the same. However, there is no FTZ mode on the GPU, and if this mode is turned on by default, it may cause great misunderstanding to our users. Because users may not know that floating-point numbers will automatically return to zero when they are less than a certain threshold. This behavior may cause inconsistent results on different platforms and cause confusion and potential errors. I hope to explore this issue further, thank you!

"
2585334077,issue,open,,argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.

Expected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp

def test_argsort():
    # Input data, hardcoded as float32
    input_data = np.array([
        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,
        5.960464477539063e-08, -2.0000100135803223, 1000000.0,
        722801.375, 0.0, -1.100000023841858
    ], dtype=np.float32)

    # PyTorch argsort
    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()
    print(f""PyTorch argsort result: {pytorch_result}"")

    # TensorFlow argsort
    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)
    print(f""TensorFlow argsort result: {tensorflow_result}"")

    # JAX argsort
    jax_result = jnp.argsort(input_data).astype(np.int32)
    print(f""JAX argsort result: {jax_result}"")

if __name__ == ""__main__"":
    test_argsort()

```

### Standalone code to reproduce the issue

```shell
PyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]
TensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]
JAX argsort result: [5 9 0 1 3 8 4 2 7 6]
```
Expected Behavior:
TensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.

Standalone Code to Reproduce the Issue:
The above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.

Relevant Log Output:
No error logs are generated, but the incorrect behavior is clearly shown in the sorting results.
```


### Relevant log output

_No response_",LilyDong0127,2024-10-14 09:02:22+00:00,['Venkat6871'],2024-10-28 09:53:34+00:00,,https://github.com/tensorflow/tensorflow/issues/77849,"[('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2425574372, 'issue_id': 2585334077, 'author': 'Venkat6871', 'body': 'Hi **@LilyDong0127** ,\r\nApologies for the delay, In TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero due to the use of FTZ (Flush-to-Zero) mode. Therefore, the issue is not with argsort() itself, but rather with how small floating-point numbers are handled in FTZ mode on the CPU.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 21, 4, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441100832, 'issue_id': 2585334077, 'author': 'LilyDong0127', 'body': ""> Hi **@LilyDong0127** , Apologies for the delay, In TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero due to the use of FTZ (Flush-to-Zero) mode. Therefore, the issue is not with argsort() itself, but rather with how small floating-point numbers are handled in FTZ mode on the CPU. Thank you!\r\n\r\nThank you for your reply! But I don't think this is a problem with the FTZ mode. Logically, the execution effects on the CPU and GPU should be the same. However, there is no FTZ mode on the GPU, and if this mode is turned on by default, it may cause great misunderstanding to our users. Because users may not know that floating-point numbers will automatically return to zero when they are less than a certain threshold. This behavior may cause inconsistent results on different platforms and cause confusion and potential errors. I hope to explore this issue further, thank you!"", 'created_at': datetime.datetime(2024, 10, 28, 9, 53, 32, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-21 04:46:00 UTC): Hi **@LilyDong0127** ,
Apologies for the delay, In TensorFlow, calculations performed on the CPU using floating-point numbers smaller than a certain threshold (like 1e-45) are rounded down to zero due to the use of FTZ (Flush-to-Zero) mode. Therefore, the issue is not with argsort() itself, but rather with how small floating-point numbers are handled in FTZ mode on the CPU.
Thank you!

LilyDong0127 (Issue Creator) on (2024-10-28 09:53:32 UTC): Thank you for your reply! But I don't think this is a problem with the FTZ mode. Logically, the execution effects on the CPU and GPU should be the same. However, there is no FTZ mode on the GPU, and if this mode is turned on by default, it may cause great misunderstanding to our users. Because users may not know that floating-point numbers will automatically return to zero when they are less than a certain threshold. This behavior may cause inconsistent results on different platforms and cause confusion and potential errors. I hope to explore this issue further, thank you!

"
2585234537,issue,closed,completed,can not use GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04.5 LTS


### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

gcc11.4

### CUDA/cuDNN version

use:
  pip install tensorflow[and-cuda]
![image](https://github.com/user-attachments/assets/73265078-7de5-408a-bbb5-e756c0d59034)


### GPU model and memory

![image](https://github.com/user-attachments/assets/23638da5-03a9-4e07-87bb-20ae3703719d)



### Standalone code to reproduce the issue

step1:  sudo apt install nvidia-driver-550
step2:  pip install tensorflow[and-cuda]
step3:  pip install tensorrt

step4:　python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'));print(tf.test.is_gpu_available())""



### Relevant log output
2024-10-15 08:04:10.453448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-15 08:04:10.465218: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-15 08:04:10.468774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-15 08:04:10.477538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-15 08:04:11.150132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT


![image](https://github.com/user-attachments/assets/e091dec3-976f-4c53-91da-31a3b0d05121)




",jandy0414,2024-10-14 08:31:57+00:00,['tilakrayal'],2024-12-22 02:05:49+00:00,2024-12-22 02:05:46+00:00,https://github.com/tensorflow/tensorflow/issues/77848,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('WIP', ''), ('comp:gpu', 'GPU related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2413216044, 'issue_id': 2585234537, 'author': 'jandy0414', 'body': 'when i run:\u3000python3 -c ""import tensorrt; print(tensorrt.__file__)""\r\nget response:\r\n![image](https://github.com/user-attachments/assets/02a80be1-9a84-4bb3-828b-7eeb18dc28e9)', 'created_at': datetime.datetime(2024, 10, 15, 8, 23, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421523182, 'issue_id': 2585234537, 'author': 'duyiwei7w', 'body': 'the same issue', 'created_at': datetime.datetime(2024, 10, 18, 6, 21, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440488664, 'issue_id': 2585234537, 'author': 'ramonpaolo', 'body': 'same issue', 'created_at': datetime.datetime(2024, 10, 28, 3, 44, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440499180, 'issue_id': 2585234537, 'author': 'tilakrayal', 'body': '@jandy0414,\r\nThank you for reporting the issue. Please allow some time to deepdive into the issue and provide the update on the same. Thank you!', 'created_at': datetime.datetime(2024, 10, 28, 3, 57, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461525258, 'issue_id': 2585234537, 'author': 'lucamorelli', 'body': 'I think i have a similar issue here.\r\nI downloaded the tensorflow docker image with gpu and try to run it with \r\n> docker run --gpus all -it tensorflow/tensorflow:latest-gpu\r\n\r\nand this is the error message I obtain\r\n![image](https://github.com/user-attachments/assets/75435cf5-01b0-438f-9f7e-76c9ac100c67)\r\n\r\nrunning the command I obtain a message about a couple of missing symbolic links, and if I try to run python I obtain the second error message', 'created_at': datetime.datetime(2024, 11, 7, 7, 40, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523304745, 'issue_id': 2585234537, 'author': 'tilakrayal', 'body': '@jandy0414,\r\nCould you please try to install the latest tensorflow v2.18 and also TensorRT support is disabled in CUDA builds for code health improvement.\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 6, 13, 54, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542662823, 'issue_id': 2585234537, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 14, 2, 5, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300071, 'issue_id': 2585234537, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300090, 'issue_id': 2585234537, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77848"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77848"">No</a>', 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 48, tzinfo=datetime.timezone.utc)}]","jandy0414 (Issue Creator) on (2024-10-15 08:23:26 UTC): when i run:　python3 -c ""import tensorrt; print(tensorrt.__file__)""
get response:
![image](https://github.com/user-attachments/assets/02a80be1-9a84-4bb3-828b-7eeb18dc28e9)

duyiwei7w on (2024-10-18 06:21:35 UTC): the same issue

ramonpaolo on (2024-10-28 03:44:07 UTC): same issue

tilakrayal (Assginee) on (2024-10-28 03:57:30 UTC): @jandy0414,
Thank you for reporting the issue. Please allow some time to deepdive into the issue and provide the update on the same. Thank you!

lucamorelli on (2024-11-07 07:40:25 UTC): I think i have a similar issue here.
I downloaded the tensorflow docker image with gpu and try to run it with 

and this is the error message I obtain
![image](https://github.com/user-attachments/assets/75435cf5-01b0-438f-9f7e-76c9ac100c67)

running the command I obtain a message about a couple of missing symbolic links, and if I try to run python I obtain the second error message

tilakrayal (Assginee) on (2024-12-06 13:54:38 UTC): @jandy0414,
Could you please try to install the latest tensorflow v2.18 and also TensorRT support is disabled in CUDA builds for code health improvement.

https://github.com/tensorflow/tensorflow/releases

Thank you!

github-actions[bot] on (2024-12-14 02:05:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-22 02:05:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-22 02:05:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77848"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77848"">No</a>

"
2584873781,issue,closed,completed,Import error with tensorflow 2.17,"### Issue type

Bug

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Windows 11

### Python version

3.9

### Current behavior?


`import tensorflow as tf`
`pip list | findstr tensorflow`
tensorflow                    2.17.0
tensorflow-intel              2.17.0
tensorflow-io-gcs-filesystem  0.31.0

Can anyone explain why am I encountering this error.

### Standalone code to reproduce the issue

![{7CA877A0-8409-496C-A979-8CD1D1C45FE5}](https://github.com/user-attachments/assets/3bca24b4-89af-4df0-883f-28d241fbca31)



### Relevant log output

_No response_",the-silent-geek,2024-10-14 06:08:57+00:00,['Venkat6871'],2024-10-29 02:02:59+00:00,2024-10-29 02:02:56+00:00,https://github.com/tensorflow/tensorflow/issues/77844,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410685606, 'issue_id': 2584873781, 'author': 'Venkat6871', 'body': 'Hi **@the-silent-geek** ,\r\nCould you please check all the compatibility versions and verify if you have imported all the necessary libraries? And In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 10, 8, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058373, 'issue_id': 2584873781, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443011959, 'issue_id': 2584873781, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012035, 'issue_id': 2584873781, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77844"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77844"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 58, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 10:08:44 UTC): Hi **@the-silent-geek** ,
Could you please check all the compatibility versions and verify if you have imported all the necessary libraries? And In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.

Thank you!

github-actions[bot] on (2024-10-22 02:02:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:02:56 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-29 02:02:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77844"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77844"">No</a>

"
2584750193,issue,closed,completed,Installing tensorflow version 2.17.0 and curl is getting installed of version 7.85.0 which is outdated. Is there way to remove critical Vulnerability for curl 7.85.0.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Install tensorflow version 2.17.0 and curl is getting installed of version 7.85.0 which is outdated. Having security vulnerabilities for tensorflow-io-gcs-filesystem/tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.

Getting below vulnerabilities:
https://github.com/advisories/GHSA-7xw9-w465-6x42
https://github.com/advisories/GHSA-grfr-78m7-q35q
https://github.com/advisories/GHSA-6xq7-qqp2-9mmc
https://github.com/advisories/GHSA-99j9-jf36-9747
https://github.com/advisories/GHSA-xvw3-6q4f-2gcv
https://github.com/advisories/GHSA-4j25-c9rf-fp5f
https://github.com/advisories/GHSA-75qm-2q4j-qx6g
https://github.com/advisories/GHSA-25m2-mpq4-29vh
https://github.com/advisories/GHSA-grfr-78m7-q35q
https://github.com/advisories/GHSA-98w6-hw73-ph8m
https://github.com/advisories/GHSA-6295-5j29-3cc8

### Standalone code to reproduce the issue


### Relevant log output

_No response_",adarshbilimagga,2024-10-14 05:03:57+00:00,['tilakrayal'],2024-11-06 09:48:14+00:00,2024-10-31 02:03:01+00:00,https://github.com/tensorflow/tensorflow/issues/77843,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2416769309, 'issue_id': 2584750193, 'author': 'tilakrayal', 'body': '@adarshbilimagga,\r\nThe mail brach has been moved from 7.85.0 to ""curl-8.6.0"". Could you please have a look at the below file  for the reference.\r\n```python\r\n\r\n    tf_http_archive(\r\n        name = ""curl"",\r\n        build_file = ""//third_party:curl.BUILD"",\r\n        sha256 = ""9c6db808160015f30f3c656c0dec125feb9dc00753596bf858a272b5dd8dc398"",\r\n        strip_prefix = ""curl-8.6.0"",\r\n        system_build_file = ""//third_party/systemlibs:curl.BUILD"",\r\n        urls = tf_mirror_urls(""https://curl.se/download/curl-8.6.0.tar.gz""),\r\n    )\r\n```\r\n\r\nMain branch: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L426-L435\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 16, 12, 57, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073500, 'issue_id': 2584750193, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867739, 'issue_id': 2584750193, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867801, 'issue_id': 2584750193, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77843"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77843"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458712082, 'issue_id': 2584750193, 'author': 'adarshbilimagga', 'body': '@tilakrayal ,\r\nBy default pip install tensorflow-io-gcs-filesystem will download the version which is available for windows i.e as shown below:\r\n**tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl** \r\nWith this version of tensorflow-io-gcs-filesystem it is referring the older version of curl library.\r\nIs there any way we will be able to get the latest version of tensorflow-io-gcs-filesystem wheel file for windows version.', 'created_at': datetime.datetime(2024, 11, 6, 4, 40, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459154550, 'issue_id': 2584750193, 'author': 'adarshbilimagga', 'body': '> @adarshbilimagga, The mail brach has been moved from 7.85.0 to ""curl-8.6.0"". Could you please have a look at the below file for the reference.\r\n> \r\n> ```python\r\n>     tf_http_archive(\r\n>         name = ""curl"",\r\n>         build_file = ""//third_party:curl.BUILD"",\r\n>         sha256 = ""9c6db808160015f30f3c656c0dec125feb9dc00753596bf858a272b5dd8dc398"",\r\n>         strip_prefix = ""curl-8.6.0"",\r\n>         system_build_file = ""//third_party/systemlibs:curl.BUILD"",\r\n>         urls = tf_mirror_urls(""https://curl.se/download/curl-8.6.0.tar.gz""),\r\n>     )\r\n> ```\r\n> \r\n> Main branch: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L426-L435\r\n> \r\n> Thank you!\r\n\r\nWe did check with latest version but as mentioned we are facing issue still.', 'created_at': datetime.datetime(2024, 11, 6, 9, 48, 13, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-16 12:57:08 UTC): @adarshbilimagga,
The mail brach has been moved from 7.85.0 to ""curl-8.6.0"". Could you please have a look at the below file  for the reference.
```python

    tf_http_archive(
        name = ""curl"",
        build_file = ""//third_party:curl.BUILD"",
        sha256 = ""9c6db808160015f30f3c656c0dec125feb9dc00753596bf858a272b5dd8dc398"",
        strip_prefix = ""curl-8.6.0"",
        system_build_file = ""//third_party/systemlibs:curl.BUILD"",
        urls = tf_mirror_urls(""https://curl.se/download/curl-8.6.0.tar.gz""),
    )
```

Main branch: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L426-L435

Thank you!

github-actions[bot] on (2024-10-24 02:01:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-31 02:03:01 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-31 02:03:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77843"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77843"">No</a>

adarshbilimagga (Issue Creator) on (2024-11-06 04:40:14 UTC): @tilakrayal ,
By default pip install tensorflow-io-gcs-filesystem will download the version which is available for windows i.e as shown below:
**tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl** 
With this version of tensorflow-io-gcs-filesystem it is referring the older version of curl library.
Is there any way we will be able to get the latest version of tensorflow-io-gcs-filesystem wheel file for windows version.

adarshbilimagga (Issue Creator) on (2024-11-06 09:48:13 UTC): We did check with latest version but as mentioned we are facing issue still.

"
2584335715,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"Traceback (most recent call last):
  File ""C:\python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<frozen runpy>"", line 189, in _run_module_as_main
  File ""<frozen runpy>"", line 148, in _get_module_details
  File ""<frozen runpy>"", line 112, in _get_module_details
  File ""C:\python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",yettey74,2024-10-13 22:45:58+00:00,['Venkat6871'],2024-10-30 02:02:15+00:00,2024-10-30 02:02:14+00:00,https://github.com/tensorflow/tensorflow/issues/77829,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2409244315, 'issue_id': 2584335715, 'author': 'yettey74', 'body': '**CURRENT TOML FILE**\r\n[tool.poetry]\r\nname = ""agi""\r\nversion = ""0.1.0""\r\ndescription = """"\r\nauthors = [""yettey74 <73868116+yettey74@users.noreply.github.com>""]\r\nreadme = ""README.md""\r\n\r\n[tool.poetry.dependencies]\r\npython = ""^3.12""\r\nnumpy = ""2.0.0""\r\npandas = ""2.2.3""\r\npanda = ""0.3.1""\r\nscipy = ""1.14.1""\r\nmatplotlib = ""3.9.2""\r\npyqt6 = ""6.7.1""\r\nopenai = ""1.51.2""\r\nnetworkx = ""3.3""\r\nwebsockets = ""13.1""\r\nrequests = ""2.32.3""\r\naiohttp = ""3.10.9""\r\nasynctest = ""0.13.0""\r\ntransformers = ""4.45.2""\r\ntorch = ""2.4.1""\r\nsphinx = ""^8.0.2""\r\ntensorflow = ""2.17.0""\r\nscikit-learn = ""^1.5.2""\r\ndatasets = ""^3.0.1""\r\nray = ""^2.37.0""\r\npyyaml = ""^6.0.2""\r\npillow = ""^10.4.0""\r\nbeautifulsoup4 = ""^4.12.3""\r\neasydict = ""^1.13""\r\nmarkdown = ""^3.7""\r\nvirtualenv = ""^20.26.6""\r\ntenacity = ""^9.0.0""\r\ncolorama = ""^0.4.6""\r\nregex = ""^2024.9.11""\r\ntiktoken = ""^0.8.0""\r\nwerkzeug = ""^3.0.4""\r\nfaiss-cpu = ""^1.9.0""\r\nwikipedia-api = ""^0.7.1""\r\nimportlib-metadata = ""^8.5.0""\r\nflask-socketio = ""^5.4.1""\r\nnpm = ""^0.1.1""\r\nnode = ""^1.2.2""\r\nspacy = ""^3.8.2""\r\n\r\n\r\n[tool.poetry.group.CollaborativeWorkspaceAgent.dependencies]\r\ndash = ""2.18.1""\r\nflask = ""3.0.3""\r\nstreamlit = ""1.39.0""\r\n\r\n\r\n[tool.poetry.group.EmotionalIntelligenceAgent.dependencies]\r\ntextblob = ""0.18.0.post0""\r\nnltk = ""3.9.1""\r\n\r\n\r\n[tool.poetry.group.EnvironmentalAdaptationAgent.dependencies]\r\npymatgen = ""2024.10.3""\r\n\r\n\r\n[tool.poetry.group.GoalSettingAgent.dependencies]\r\ngym = ""0.26.2""\r\n\r\n\r\n[tool.poetry.group.GravitySpecialistAgent.dependencies]\r\npygravity = ""0.1""\r\nastropy = ""6.1.4""\r\nrebound = ""4.4.3""\r\n\r\n\r\n[tool.poetry.group.MathsAgent.dependencies]\r\nsympy = ""1.13.3""\r\n\r\n\r\n[tool.poetry.group.OpticalSpecialistAgent.dependencies]\r\npyoptics = ""0.0.3""\r\npymat = ""0.0.2""\r\n\r\n\r\n[tool.poetry.group.PerformanceAgent.dependencies]\r\nline-profiler = ""4.1.3""\r\nmemory-profiler = ""0.61.0""\r\ncprofilev = ""1.0.7""\r\n\r\n\r\n[tool.poetry.group.PlotAgent.dependencies]\r\nplotly = ""5.9.0""\r\nbokeh = ""3.6.0""\r\nseaborn = ""0.13.2""\r\nmayavi = ""4.8.2""\r\nplotly-express = ""0.4.1""\r\n\r\n\r\n[tool.poetry.group.QuantumSpecialistAgent.dependencies]\r\nqiskit = ""1.2.4""\r\npyquantum = ""1.0.60""\r\n\r\n\r\n[tool.poetry.group.ReportAgent.dependencies]\r\npylint = ""3.3.1""\r\nsphinx = ""8.0.2""\r\nradon = ""6.0.1""\r\nflake8 = ""7.1.1""\r\nblack = ""24.10.0""\r\nmypy = ""1.11.2""\r\ncoverage = ""7.6.1""\r\n\r\n\r\n[tool.poetry.group.ResourceManagerAgent.dependencies]\r\ncachetools = ""5.5.0""\r\n\r\n\r\n[tool.poetry.group.TestAgentAgent.dependencies]\r\npytest = ""8.3.3""\r\npytest-cov = ""^5.0.0""\r\nunittest-xml-reporting = ""3.0.4""\r\nassertpy = ""1.1""\r\nparameterized = ""0.9.0""\r\nhtml-testrunner = ""1.2.1""\r\n\r\n\r\n[tool.poetry.group.ValidationAgent.dependencies]\r\nhypothesis = ""6.54.5""\r\n\r\n\r\n[tool.poetry.group.dev.dependencies]\r\npre-commit = ""^4.0.1""\r\n\r\n[build-system]\r\nrequires = [""poetry-core""]\r\nbuild-backend = ""poetry.core.masonry.api""', 'created_at': datetime.datetime(2024, 10, 13, 22, 46, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410477299, 'issue_id': 2584335715, 'author': 'Venkat6871', 'body': 'Hi @yettey74 ,\r\nCould you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue. \r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 8, 45, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411538114, 'issue_id': 2584335715, 'author': 'rolandweb3', 'body': '> Hi @yettey74 , Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue. Thank you!\r\n\r\nI have the same issue, what could be course:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\rolan\\Desktop\\Code\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module> \r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\rolan\\Desktop\\Code\\one.py"", line 24, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n  File ""C:\\Users\\rolan\\Desktop\\Code\\tf\\lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n  File ""C:\\Users\\rolan\\Desktop\\Code\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module> \r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File ""C:\\Users\\rolan\\Desktop\\Code\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module> \r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.', 'created_at': datetime.datetime(2024, 10, 14, 15, 4, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412840048, 'issue_id': 2584335715, 'author': 'Venkat6871', 'body': 'Hi **@yettey74** ,\r\nThere are at least 3 possible scenarios:\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nAlso in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 15, 3, 51, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430659810, 'issue_id': 2584335715, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 23, 2, 1, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445662910, 'issue_id': 2584335715, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 30, 2, 2, 14, tzinfo=datetime.timezone.utc)}]","yettey74 (Issue Creator) on (2024-10-13 22:46:39 UTC): **CURRENT TOML FILE**
[tool.poetry]
name = ""agi""
version = ""0.1.0""
description = """"
authors = [""yettey74 <73868116+yettey74@users.noreply.github.com>""]
readme = ""README.md""

[tool.poetry.dependencies]
python = ""^3.12""
numpy = ""2.0.0""
pandas = ""2.2.3""
panda = ""0.3.1""
scipy = ""1.14.1""
matplotlib = ""3.9.2""
pyqt6 = ""6.7.1""
openai = ""1.51.2""
networkx = ""3.3""
websockets = ""13.1""
requests = ""2.32.3""
aiohttp = ""3.10.9""
asynctest = ""0.13.0""
transformers = ""4.45.2""
torch = ""2.4.1""
sphinx = ""^8.0.2""
tensorflow = ""2.17.0""
scikit-learn = ""^1.5.2""
datasets = ""^3.0.1""
ray = ""^2.37.0""
pyyaml = ""^6.0.2""
pillow = ""^10.4.0""
beautifulsoup4 = ""^4.12.3""
easydict = ""^1.13""
markdown = ""^3.7""
virtualenv = ""^20.26.6""
tenacity = ""^9.0.0""
colorama = ""^0.4.6""
regex = ""^2024.9.11""
tiktoken = ""^0.8.0""
werkzeug = ""^3.0.4""
faiss-cpu = ""^1.9.0""
wikipedia-api = ""^0.7.1""
importlib-metadata = ""^8.5.0""
flask-socketio = ""^5.4.1""
npm = ""^0.1.1""
node = ""^1.2.2""
spacy = ""^3.8.2""


[tool.poetry.group.CollaborativeWorkspaceAgent.dependencies]
dash = ""2.18.1""
flask = ""3.0.3""
streamlit = ""1.39.0""


[tool.poetry.group.EmotionalIntelligenceAgent.dependencies]
textblob = ""0.18.0.post0""
nltk = ""3.9.1""


[tool.poetry.group.EnvironmentalAdaptationAgent.dependencies]
pymatgen = ""2024.10.3""


[tool.poetry.group.GoalSettingAgent.dependencies]
gym = ""0.26.2""


[tool.poetry.group.GravitySpecialistAgent.dependencies]
pygravity = ""0.1""
astropy = ""6.1.4""
rebound = ""4.4.3""


[tool.poetry.group.MathsAgent.dependencies]
sympy = ""1.13.3""


[tool.poetry.group.OpticalSpecialistAgent.dependencies]
pyoptics = ""0.0.3""
pymat = ""0.0.2""


[tool.poetry.group.PerformanceAgent.dependencies]
line-profiler = ""4.1.3""
memory-profiler = ""0.61.0""
cprofilev = ""1.0.7""


[tool.poetry.group.PlotAgent.dependencies]
plotly = ""5.9.0""
bokeh = ""3.6.0""
seaborn = ""0.13.2""
mayavi = ""4.8.2""
plotly-express = ""0.4.1""


[tool.poetry.group.QuantumSpecialistAgent.dependencies]
qiskit = ""1.2.4""
pyquantum = ""1.0.60""


[tool.poetry.group.ReportAgent.dependencies]
pylint = ""3.3.1""
sphinx = ""8.0.2""
radon = ""6.0.1""
flake8 = ""7.1.1""
black = ""24.10.0""
mypy = ""1.11.2""
coverage = ""7.6.1""


[tool.poetry.group.ResourceManagerAgent.dependencies]
cachetools = ""5.5.0""


[tool.poetry.group.TestAgentAgent.dependencies]
pytest = ""8.3.3""
pytest-cov = ""^5.0.0""
unittest-xml-reporting = ""3.0.4""
assertpy = ""1.1""
parameterized = ""0.9.0""
html-testrunner = ""1.2.1""


[tool.poetry.group.ValidationAgent.dependencies]
hypothesis = ""6.54.5""


[tool.poetry.group.dev.dependencies]
pre-commit = ""^4.0.1""

[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""

Venkat6871 (Assginee) on (2024-10-14 08:45:57 UTC): Hi @yettey74 ,
Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue. 
Thank you!

rolandweb3 on (2024-10-14 15:04:52 UTC): I have the same issue, what could be course:


Traceback (most recent call last):
  File ""C:\Users\rolan\Desktop\Code\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module> 
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\rolan\Desktop\Code\one.py"", line 24, in <module>
    from tensorflow.keras.models import Sequential
  File ""C:\Users\rolan\Desktop\Code\tf\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\rolan\Desktop\Code\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module> 
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\rolan\Desktop\Code\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module> 
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

Venkat6871 (Assginee) on (2024-10-15 03:51:55 UTC): Hi **@yettey74** ,
There are at least 3 possible scenarios:
You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.

Also in order to expedite the trouble-shooting process, could you please provide the following information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
Installed using virtualenv? pip? conda?:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
and the exact sequence of commands / steps that you executed before running into the problem.
https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-10-23 02:01:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-30 02:02:14 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2584041370,issue,closed,completed,AttributeError: 'NoneType' object has no attribute 'shape',"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.20

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow version: 2.17.0
Transformers version: 4.46.0.dev0
Keras version: 3.6.0

This problem can be solved by using TensorFlow version 2.11, but it is not solved because i'm trying to use Tensorflow version 2.17. The existing BERT model is implemented in version 2.17, but this also does not work in version 2.11 of Tensorflow... 

Therefore, I would like to solve this problem and make the entire code work in version 2.17.

### Standalone code to reproduce the issue

```shell
from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, TFBertModel
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D

from tensorflow.keras.models import Model
from keras_tuner import HyperParameters, RandomSearch

import numpy as np
import tensorflow as tf

import joblib, os
import pandas as pd
import tensorflow as tf
import numpy as np
import pickle

import numpy as np
import tensorflow as tf
from transformers import TFBertModel
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from keras_tuner import RandomSearch
from tensorflow.keras.models import Model

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Set memory growth
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

INPUT_DIR = ""../Output/proto_models_rev2""
# Load data
df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'))
df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'))
# df_test = processor.load_data()
X_train = df_train['review']
X_test = df_test['review']
y_train = df_train['polarity']
y_test = df_test['polarity']

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenizing the datasets
X_train_tokens = tokenizer(
    text=list(X_train),
    add_special_tokens=True,
    max_length=100,
    padding='max_length',
    truncation=True,
    return_tensors='tf',
    return_token_type_ids=False,
    return_attention_mask=True
)

X_test_tokens = tokenizer(
    text=list(X_test),
    add_special_tokens=True,
    max_length=100,
    padding='max_length',
    truncation=True,
    return_tensors='tf',
    return_token_type_ids=False,
    return_attention_mask=True
)


# # Define BERT Model
# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# # Use Hugging Face's AdamWeightDecay optimizer
# optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
# # Compile the model using a standard loss function
# loss_fn = SparseCategoricalCrossentropy(from_logits=True)
# bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])
# # Model Summary
# bert_model.summary()

# BERT Model Definition
def build_bert_model(hp):
    input_ids = Input(shape=(100,), dtype=tf.int32, name=""input_ids"")
    attention_mask = Input(shape=(100,), dtype=tf.int32, name=""attention_mask"")

    bert_model = TFBertModel.from_pretrained(""bert-base-uncased"")
    bert_output = bert_model(input_ids=input_ids, attention_mask=attention_mask)[0]
    
    pooled_output = GlobalAveragePooling1D()(bert_output)
    dense = Dense(units=hp.Int(""units"", min_value=32, max_value=128, step=32), activation='relu')(pooled_output)
    output = Dense(2, activation='softmax')(dense)
    
    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    
    learning_rate = hp.Choice(""learning_rate"", values=[1e-5, 2e-5, 3e-5])
    optimizer = Adam(learning_rate=learning_rate, weight_decay_rate=0.01)
    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[""accuracy""])
    
    return model

# Convert to TensorFlow Tensor
def create_tf_dataset(X, y, batch_size=32):
    input_ids = tf.convert_to_tensor(X[""input_ids""])
    attention_mask = tf.convert_to_tensor(X[""attention_mask""])
    y = tf.convert_to_tensor(y)
    
    dataset = tf.data.Dataset.from_tensor_slices(({""input_ids"": input_ids, ""attention_mask"": attention_mask}, y))
    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    return dataset

# Convert datasets
train_dataset = create_tf_dataset(X_train_tokens, y_train)
val_dataset = create_tf_dataset(X_test_tokens, y_test)

# Hyperparameter tuner
tuner = RandomSearch(
    build_bert_model,
    objective=""val_accuracy"",
    max_trials=5,
    executions_per_trial=1,
    directory=""bert_tuning"",
    project_name=""bert_sentiment_analysis""
)


# Perform the hyperparameter tuning search
tuner.search(
    train_dataset,
    validation_data=val_dataset,
    epochs=3
)
```


### Relevant log output

```shell
{
	""name"": ""RuntimeError"",
	""message"": ""Number of consecutive failures exceeded the limit of 3.
Traceback (most recent call last):
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\"", line 274, in _try_run_and_update_trial
    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\"", line 239, in _run_and_update_trial
    results = self.run_trial(trial, *fit_args, **fit_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 314, in run_trial
    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 232, in _build_and_fit_model
    model = self._try_build(hp)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 164, in _try_build
    model = self._build_hypermodel(hp)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 155, in _build_hypermodel
    model = self.hypermodel.build(hp)
  File \""/tmp/ipykernel_48514/733536575.py\"", line 91, in build_bert_model
    bert_model = TFBertModel.from_pretrained(\""bert-base-uncased\"")
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\"", line 1684, in from_pretrained
    model(model.dummy_inputs)  # build the network with dummy inputs
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\"", line 1130, in call
    outputs = self.bert(
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\"", line 871, in call
    encoder_outputs = self.encoder(
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/optree/ops.py\"", line 747, in tree_map
    return treespec.unflatten(map(func, *flat_args))
AttributeError: Exception encountered when calling TFBertMainLayer.call().

'NoneType' object has no attribute 'shape'

Arguments received by TFBertMainLayer.call():
  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)
  • attention_mask=None
  • token_type_ids=None
  • position_ids=None
  • head_mask=None
  • inputs_embeds=None
  • encoder_hidden_states=None
  • encoder_attention_mask=None
  • past_key_values=None
  • use_cache=True
  • output_attentions=False
  • output_hidden_states=False
  • return_dict=True
  • training=False
  • kwargs=<class 'inspect._empty'>
"",
	""stack"": ""---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[19], line 132
    121 tuner = RandomSearch(
    122     build_bert_model,
    123     objective=\""val_accuracy\"",
   (...)
    127     project_name=\""bert_sentiment_analysis\""
    128 )
    131 # Perform the hyperparameter tuning search
--> 132 tuner.search(
    133     train_dataset,
    134     validation_data=val_dataset,
    135     epochs=3
    136 )

File ~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:235, in BaseTuner.search(self, *fit_args, **fit_kwargs)
    233     self.on_trial_begin(trial)
    234     self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)
--> 235     self.on_trial_end(trial)
    236 self.on_search_end()

File ~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:339, in BaseTuner.on_trial_end(self, trial)
    333 def on_trial_end(self, trial):
    334     \""\""\""Called at the end of a trial.
    335 
    336     Args:
    337         trial: A `Trial` instance.
    338     \""\""\""
--> 339     self.oracle.end_trial(trial)
    340     self.save()

File ~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:108, in synchronized.<locals>.wrapped_func(*args, **kwargs)
    106     LOCKS[oracle].acquire()
    107     THREADS[oracle] = thread_name
--> 108 ret_val = func(*args, **kwargs)
    109 if need_acquire:
    110     THREADS[oracle] = None

File ~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:588, in Oracle.end_trial(self, trial)
    586 if not self._retry(trial):
    587     self.end_order.append(trial.trial_id)
--> 588     self._check_consecutive_failures()
    590 self._save_trial(trial)
    591 self.save()

File ~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:545, in Oracle._check_consecutive_failures(self)
    543     consecutive_failures = 0
    544 if consecutive_failures == self.max_consecutive_failed_trials:
--> 545     raise RuntimeError(
    546         \""Number of consecutive failures exceeded the limit \""
    547         f\""of {self.max_consecutive_failed_trials}.\
\""
    548         + (trial.message or \""\"")
    549     )

RuntimeError: Number of consecutive failures exceeded the limit of 3.
Traceback (most recent call last):
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\"", line 274, in _try_run_and_update_trial
    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\"", line 239, in _run_and_update_trial
    results = self.run_trial(trial, *fit_args, **fit_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 314, in run_trial
    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 232, in _build_and_fit_model
    model = self._try_build(hp)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 164, in _try_build
    model = self._build_hypermodel(hp)
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\"", line 155, in _build_hypermodel
    model = self.hypermodel.build(hp)
  File \""/tmp/ipykernel_48514/733536575.py\"", line 91, in build_bert_model
    bert_model = TFBertModel.from_pretrained(\""bert-base-uncased\"")
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\"", line 1684, in from_pretrained
    model(model.dummy_inputs)  # build the network with dummy inputs
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\"", line 1130, in call
    outputs = self.bert(
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\"", line 871, in call
    encoder_outputs = self.encoder(
  File \""/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/optree/ops.py\"", line 747, in tree_map
    return treespec.unflatten(map(func, *flat_args))
AttributeError: Exception encountered when calling TFBertMainLayer.call().

'NoneType' object has no attribute 'shape'

Arguments received by TFBertMainLayer.call():
  • input_ids=tf.Tensor(shape=(3, 5), dtype=int32)
  • attention_mask=None
  • token_type_ids=None
  • position_ids=None
  • head_mask=None
  • inputs_embeds=None
  • encoder_hidden_states=None
  • encoder_attention_mask=None
  • past_key_values=None
  • use_cache=True
  • output_attentions=False
  • output_hidden_states=False
  • return_dict=True
  • training=False
  • kwargs=<class 'inspect._empty'>
""
}
```
",Kim-William,2024-10-13 15:07:19+00:00,['Venkat6871'],2024-11-07 02:00:40+00:00,2024-11-07 02:00:37+00:00,https://github.com/tensorflow/tensorflow/issues/77826,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2413485501, 'issue_id': 2584041370, 'author': 'Venkat6871', 'body': ""Hi **@Kim-William** ,\r\nApologies for the delay. I tried to run your code on Colab using TensorFlow 2.17.0, and it is throwing the following error:``` No such file or directory: '../Output/proto_models_rev2/train_cleaned.csv'```. Could you please provide the CSV file associated with your code? This will make it easier for us to replicate and troubleshoot the issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f0c65ee292b0c59cd7330a113d6a7f7f/77826_tf-2-17-0-v.ipynb) here for reference.\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 10, 15, 10, 18, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423104370, 'issue_id': 2584041370, 'author': 'Kim-William', 'body': ""> Hi **@Kim-William** , Apologies for the delay. I tried to run your code on Colab using TensorFlow 2.17.0, and it is throwing the following error:` No such file or directory: '../Output/proto_models_rev2/train_cleaned.csv'`. Could you please provide the CSV file associated with your code? This will make it easier for us to replicate and troubleshoot the issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f0c65ee292b0c59cd7330a113d6a7f7f/77826_tf-2-17-0-v.ipynb) here for reference.\r\n> \r\n> Thank you!\r\n[test_cleaned.csv](https://github.com/user-attachments/files/17439887/test_cleaned.csv)\r\n[train_cleaned.csv](https://github.com/user-attachments/files/17439888/train_cleaned.csv)\r\n\r\n@Venkat6871 \r\nHere i attached the data set.\r\n\r\nplease check this :)"", 'created_at': datetime.datetime(2024, 10, 18, 19, 27, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427761822, 'issue_id': 2584041370, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">No</a>', 'created_at': datetime.datetime(2024, 10, 21, 21, 30, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428425985, 'issue_id': 2584041370, 'author': 'Venkat6871', 'body': 'Hi **@Kim-William** ,\r\nApologies for the delay, and thank you for providing the necessary files. I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b134ff955e997f35974092f9a0c6dbe9/77826_tf-2-17-0-v.ipynb) here for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 22, 7, 6, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445662933, 'issue_id': 2584041370, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 30, 2, 2, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461162239, 'issue_id': 2584041370, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 7, 2, 0, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461162318, 'issue_id': 2584041370, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">No</a>', 'created_at': datetime.datetime(2024, 11, 7, 2, 0, 39, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-15 10:18:23 UTC): Hi **@Kim-William** ,
Apologies for the delay. I tried to run your code on Colab using TensorFlow 2.17.0, and it is throwing the following error:``` No such file or directory: '../Output/proto_models_rev2/train_cleaned.csv'```. Could you please provide the CSV file associated with your code? This will make it easier for us to replicate and troubleshoot the issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f0c65ee292b0c59cd7330a113d6a7f7f/77826_tf-2-17-0-v.ipynb) here for reference.

Thank you!

Kim-William (Issue Creator) on (2024-10-18 19:27:35 UTC): [test_cleaned.csv](https://github.com/user-attachments/files/17439887/test_cleaned.csv)
[train_cleaned.csv](https://github.com/user-attachments/files/17439888/train_cleaned.csv)

@Venkat6871 
Here i attached the data set.

please check this :)

google-ml-butler[bot] on (2024-10-21 21:30:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">No</a>

Venkat6871 (Assginee) on (2024-10-22 07:06:34 UTC): Hi **@Kim-William** ,
Apologies for the delay, and thank you for providing the necessary files. I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b134ff955e997f35974092f9a0c6dbe9/77826_tf-2-17-0-v.ipynb) here for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

github-actions[bot] on (2024-10-30 02:02:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-07 02:00:37 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-07 02:00:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77826"">No</a>

"
2583961541,issue,closed,completed,Aborted (core dumped) in tf.raw_ops.IRFFT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the following code is entered, it will cause abort



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant(1, shape=5, dtype=tf.complex64)

fft_length = tf.constant(0, shape=[1], dtype=tf.int32)

output = tf.raw_ops.IRFFT(
    input=input_data,
    fft_length=fft_length
)
```


### Relevant log output

```shell
2024-10-13 13:12:27.281652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-13 13:12:27.282449: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-10-13 13:12:27.286344: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-10-13 13:12:27.298014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-13 13:12:27.316660: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-13 13:12:27.322124: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-13 13:12:27.335981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-13 13:12:28.402423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DUCC FFT c2r failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
",LongZE666,2024-10-13 13:12:55+00:00,['Venkat6871'],2024-10-29 02:03:03+00:00,2024-10-29 02:02:58+00:00,https://github.com/tensorflow/tensorflow/issues/77825,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410362660, 'issue_id': 2583961541, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\r\nI tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. And I have provided an alternative solution for this issue, and I hope it will be helpful for you. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/f854d589102f4a019bc06c93bfcbbb0f/77825_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 8, 1, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058402, 'issue_id': 2583961541, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012016, 'issue_id': 2583961541, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012101, 'issue_id': 2583961541, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77825"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77825"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 1, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 08:01:58 UTC): Hi **@LongZE666** ,
I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. And I have provided an alternative solution for this issue, and I hope it will be helpful for you. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/f854d589102f4a019bc06c93bfcbbb0f/77825_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

github-actions[bot] on (2024-10-22 02:02:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:02:58 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-29 02:03:01 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77825"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77825"">No</a>

"
2583958982,issue,open,,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If the value contained in fft_length is the maximum value, it will cause an abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)

tf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 13:04:53.308156: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements
Aborted (core dumped)
```
",LongZE666,2024-10-13 13:07:08+00:00,['Venkat6871'],2024-10-17 09:09:04+00:00,,https://github.com/tensorflow/tensorflow/issues/77824,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2413445274, 'issue_id': 2583958982, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/ebc35ba8d5396490d5c2cf20fa300672/77824_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 15, 10, 0, 4, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-15 10:00:04 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/ebc35ba8d5396490d5c2cf20fa300672/77824_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2583957260,issue,open,,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Since the value in fft_length is a maximum value, it will cause abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)

tf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 12:59:11.295197: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
",LongZE666,2024-10-13 13:04:05+00:00,['Venkat6871'],2024-10-17 09:08:05+00:00,,https://github.com/tensorflow/tensorflow/issues/77823,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410312185, 'issue_id': 2583957260, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/f86bf167d1c7c9020d9ca6ba7c0c0055/77823_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 7, 45, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 07:45:46 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/f86bf167d1c7c9020d9ca6ba7c0c0055/77823_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2583939874,issue,closed,completed,Traceback (most recent call last),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expect it to run without any problem

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""C:\Users\Rolanddev\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization 
routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\Rolanddev\Desktop\One.py"", line 24, in <module>
    from tensorflow.keras.models import Sequential
  File ""C:\Users\Rolanddev\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py"", line 38, 
in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Rolanddev\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Rolanddev\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization 
routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Relevant log output

_No response_",rolandweb3,2024-10-13 12:36:51+00:00,['Venkat6871'],2024-10-30 07:02:35+00:00,2024-10-29 02:02:59+00:00,https://github.com/tensorflow/tensorflow/issues/77822,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410914596, 'issue_id': 2583939874, 'author': 'Venkat6871', 'body': 'Hi **@rolandweb3** ,\r\nThere are at least 3 possible scenarios:\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nAlso in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 11, 20, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058423, 'issue_id': 2583939874, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012048, 'issue_id': 2583939874, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 2, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012161, 'issue_id': 2583939874, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77822"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77822"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 4, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 11:20:57 UTC): Hi **@rolandweb3** ,
There are at least 3 possible scenarios:
You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.

Also in order to expedite the trouble-shooting process, could you please provide the following information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
Installed using virtualenv? pip? conda?:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
and the exact sequence of commands / steps that you executed before running into the problem.
https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-10-22 02:02:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:02:59 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-29 02:03:04 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77822"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77822"">No</a>

"
2583886295,issue,closed,completed,Bug: tf.math.angle returns 0.0 for real NaN input instead of NaN System information,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

win32

### Python version

3.9.19

### Bazel version

bazel version

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Not applicable (No GPU support)

### GPU model and memory

No GPU (using CPU)

### Current behavior?

Current Behavior:
When using `tf.math.angle` with a real NaN input, TensorFlow and Keras return `0.0`, while other frameworks such as PyTorch, JAX, and Chainer return `NaN`. Returning `0.0` for NaN is unexpected, as the angle of NaN is undefined and should return NaN.


### Standalone code to reproduce the issue

```shell
import torch
import tensorflow as tf
import jax.numpy as jnp
import numpy as np

# Create a scalar NaN tensor with dtype float32
input_tensor = tf.constant(float('nan'), dtype=tf.float32)

# PyTorch: angle function for real NaN input
def pytorch_angle(x):
    return torch.angle(torch.tensor(x, dtype=torch.float32)).numpy()

# JAX: angle function for real NaN input
def jax_angle(x):
    return jnp.angle(jnp.array(x)).item()

# Chainer: angle function for real NaN input
def chainer_angle(x):
    return np.angle(x)

# TensorFlow: angle function for real NaN input
def tf_angle(x):
    return tf.math.angle(x).numpy()

# Keras: using TensorFlow's angle function for real NaN input
def keras_angle(x):
    return tf.math.angle(x).numpy()

# Testing angle function across frameworks
print(f""PyTorch angle result: {pytorch_angle(float('nan'))}"")
print(f""JAX angle result: {jax_angle(float('nan'))}"")
print(f""Chainer angle result: {chainer_angle(np.array(float('nan')))}"")
print(f""TensorFlow angle result: {tf_angle(input_tensor)}"")
print(f""Keras angle result: {keras_angle(input_tensor)}"")
```


### Relevant log output

```shell
Generated input: nan

PyTorch angle result: nan
JAX angle result: nan
Chainer angle result: nan
TensorFlow angle result: 0.0
Keras angle result: 0.0
```
",LilyDong0127,2024-10-13 11:11:38+00:00,['Venkat6871'],2024-10-29 02:03:08+00:00,2024-10-29 02:03:01+00:00,https://github.com/tensorflow/tensorflow/issues/77821,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2410293938, 'issue_id': 2583886295, 'author': 'Venkat6871', 'body': 'Hi **@LilyDong0127** ,\r\nI tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/d625a888c7d5ecd42a4709216e5e5353/77821_tf-2-17-0-nightly-v.ipynb) provided here for reference.\r\nSome related issues are already being tracked. Please review them as well, as they might be helpful. I am providing the [links](https://github.com/tensorflow/tensorflow/issues/50854) to those issues for your reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 7, 37, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058454, 'issue_id': 2583886295, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012077, 'issue_id': 2583886295, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012230, 'issue_id': 2583886295, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77821"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77821"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 7, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 07:37:18 UTC): Hi **@LilyDong0127** ,
I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/d625a888c7d5ecd42a4709216e5e5353/77821_tf-2-17-0-nightly-v.ipynb) provided here for reference.
Some related issues are already being tracked. Please review them as well, as they might be helpful. I am providing the [links](https://github.com/tensorflow/tensorflow/issues/50854) to those issues for your reference.

Thank you!

github-actions[bot] on (2024-10-22 02:02:23 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:03:00 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-29 02:03:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77821"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77821"">No</a>

"
2583788078,issue,closed,completed,Update CODE_OF_CONDUCT.md #77818,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

CODE OF CONDUCT

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello there,

Previous CODE_OF_CONDUCT is version 1.4,
I updated CODE_OF_CONDUCT to version 2.1.

please review my PR and suggest me some changes if needed.

please assign me this 

### Standalone code to reproduce the issue

```shell
Hello there,

Previous CODE_OF_CONDUCT is version 1.4,
I updated CODE_OF_CONDUCT to version 2.1.

please review my PR and suggest me some changes if needed.

please assign me this
```


### Relevant log output

```shell
assign me this
```
",Anandha-Vihari,2024-10-13 08:39:17+00:00,['tilakrayal'],2025-01-03 19:13:19+00:00,2025-01-03 19:13:19+00:00,https://github.com/tensorflow/tensorflow/issues/77820,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('invalid', 'Hacktoberfest spam PR')]","[{'comment_id': 2408885771, 'issue_id': 2583788078, 'author': 'Anandha-Vihari', 'body': '@tilakrayal why this bot auto assigned to you', 'created_at': datetime.datetime(2024, 10, 13, 8, 40, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416735395, 'issue_id': 2583788078, 'author': 'tilakrayal', 'body': '@Anandha-Vihari,\r\nIf you are encountering any difficulties related to TensorFlow, please share more details. Otherwise, feel free to close this issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 16, 12, 41, 58, tzinfo=datetime.timezone.utc)}]","Anandha-Vihari (Issue Creator) on (2024-10-13 08:40:07 UTC): @tilakrayal why this bot auto assigned to you

tilakrayal (Assginee) on (2024-10-16 12:41:58 UTC): @Anandha-Vihari,
If you are encountering any difficulties related to TensorFlow, please share more details. Otherwise, feel free to close this issue. Thank you!

"
2583783023,issue,closed,completed,Update CODE_OF_CONDUCT.md #77818,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

CODE OF CONDUCT

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello there,

Previous CODE_OF_CONDUCT is version 1.4,
I updated CODE_OF_CONDUCT to version 2.1.

please review my PR and suggest me some changes if needed.

### Standalone code to reproduce the issue

```shell
Hello there,

Previous CODE_OF_CONDUCT is version 1.4,
I updated CODE_OF_CONDUCT to version 2.1.

please review my PR and suggest me some changes if needed.
```


### Relevant log output

```shell
Hello there,

Previous CODE_OF_CONDUCT is version 1.4,
I updated CODE_OF_CONDUCT to version 2.1.

please review my PR and suggest me some changes if needed.
```
",Anandha-Vihari,2024-10-13 08:36:15+00:00,['Venkat6871'],2024-10-13 12:40:36+00:00,2024-10-13 08:37:55+00:00,https://github.com/tensorflow/tensorflow/issues/77819,"[('type:feature', 'Feature requests'), ('invalid', 'Hacktoberfest spam PR'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]",[],
2583243517,issue,closed,completed,Failed to load the native TensorFlow runtime.,"Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\fer-project\src\train.py"", line 3, in <module>
    from model import create_cnn_model
  File ""C:\Users\user\fer-project\src\model.py"", line 3, in <module>
    import tensorflow as tf
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.",philipakomolafe,2024-10-12 16:37:30+00:00,['Venkat6871'],2024-10-30 07:02:00+00:00,2024-10-29 02:03:02+00:00,https://github.com/tensorflow/tensorflow/issues/77797,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2410878224, 'issue_id': 2583243517, 'author': 'Venkat6871', 'body': 'Hi **@philipakomolafe** ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue. Also Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n#61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 11, 5, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058485, 'issue_id': 2583243517, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012110, 'issue_id': 2583243517, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 1, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 11:05:20 UTC): Hi **@philipakomolafe** ,
We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue. Also Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
#61887

Thank you!

github-actions[bot] on (2024-10-22 02:02:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:03:01 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2582435670,issue,closed,completed,TFLite concatenation operator should support float16 data type,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of concatenation](https://www.tensorflow.org/mlir/tfl_ops#operands_21) supports all data types, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/concatenation.cc;l=144?q=concatenation.cc&ss=chromium%2Fchromium%2Fsrc) doesn't support float16 data type.

",fujunwei,2024-10-12 02:58:26+00:00,['Venkat6871'],2024-10-12 02:58:46+00:00,2024-10-12 02:58:46+00:00,https://github.com/tensorflow/tensorflow/issues/77759,"[('comp:lite', 'TF Lite related issues')]",[],
2582209974,issue,closed,completed,"NVIDIA GPU not being detected, no error messages provided. All minimum requirements met.","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA GTX 1650 Max-Q | 4GB VRAM

### Current behavior?

I am in need of GPU support on my laptop for TensorFlow. I natively run Windows for all my development, and while WSL2 Tensorflow ""works,"" it is unbearingly slow when I try to run Jupyter notebooks connecting to WSL2 from PyCharm. So, the following steps are what I took to install Tensorflow 2.10, in hopes of getting GPU support.

- Installed Python 3.10
- Installed CUDA Toolkit v12.6 and v11.8
- Installed cuDNN v9.5 and cuDNN v8.9
- Verified my GPU has a compatible CUDA version (12.6) (see nvidia-smi output below)
```+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1650 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   52C    P0             12W /   35W |      19MiB /   4096MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
```
- Created a new PyCharm project with a venv based off of Python 3.10
- Targeted the project's venv and installed tensorflow with the line `python -m pip install ""tensorflow<2.11""`
- Attempted to verify the installation using `python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""`

Even after cross-referencing the documentation to ensure my versions are correct and installing any and all dependencies based on warning provided, my dGPU is still undetected and not shown in the Python commandline output. Any and all advice is appreciated, I'm willing to try anything at this point.

### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow as tf; print(tf.config.list_physical_devices())""
```


### Relevant log output

```shell
> []
```
",alexanderjalexander,2024-10-11 21:30:01+00:00,['tilakrayal'],2024-10-11 21:40:14+00:00,2024-10-11 21:40:12+00:00,https://github.com/tensorflow/tensorflow/issues/77729,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2408155092, 'issue_id': 2582209974, 'author': 'alexanderjalexander', 'body': 'Issue closed. After reproducing the steps one more time, it turns out the venv in question was not targeted properly. An alternate venv was targeted instead. Thus the GPU was not supported due to this problem.', 'created_at': datetime.datetime(2024, 10, 11, 21, 40, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408155116, 'issue_id': 2582209974, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77729"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77729"">No</a>', 'created_at': datetime.datetime(2024, 10, 11, 21, 40, 14, tzinfo=datetime.timezone.utc)}]","alexanderjalexander (Issue Creator) on (2024-10-11 21:40:12 UTC): Issue closed. After reproducing the steps one more time, it turns out the venv in question was not targeted properly. An alternate venv was targeted instead. Thus the GPU was not supported due to this problem.

google-ml-butler[bot] on (2024-10-11 21:40:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77729"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77729"">No</a>

"
2582019084,issue,closed,completed,How to install the TfLite after succeed the cmake build ?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Latest 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.9.20

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After build the Lite project succesffuly with cmake :
```
-- Building for XNNPACK_TARGET_PROCESSOR: x86_64
-- Generating microkernels.cmake
-- 
-- 3.21.9.0
-- Configuring done (7.7s)
-- Generating done (1.5s)
-- Build files have been written to: /home/mac/PycharmProjects/RealTimeFightDetect/tensorflow/tensorflow/lite/build
```
I didn't find any doc about how to install it, so I tried 
`>>>sudo cmake --install .`
I got this error :

```
-- Up-to-date: /usr/local/include/eigen3/unsupported/Eigen/CXX11/src/TensorSymmetry/StaticSymmetry.h
CMake Error at _deps/fft2d-build/cmake_install.cmake:46 (file):
  file INSTALL cannot find
  ""/home/mac/PycharmProjects/RealTimeFightDetect/tensorflow/tensorflow/lite/build/_deps/fft2d-build/libfft2d_fftsg.a"":
  No such file or directory.
Call Stack (most recent call first):
  cmake_install.cmake:62 (include)
```
And before trying to compile the project I also tried to install directly without success with different Python version from 3.9.x to 3.12.x :


```
>> sudo apt-get install python3-tflite-runtime
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 python3-tflite-runtime : Depends: python3 (< 3.10) but 3.12.3-0ubuntu2 is to be installed
E: Unable to correct problems, you have held broken packages.
```


### Standalone code to reproduce the issue

```shell
`>>>sudo cmake --install .`
```


### Relevant log output

_No response_",nassimus26,2024-10-11 19:06:29+00:00,['gaikwadrahul8'],2024-10-29 02:03:11+00:00,2024-10-29 02:03:03+00:00,https://github.com/tensorflow/tensorflow/issues/77718,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410989159, 'issue_id': 2582019084, 'author': 'gaikwadrahul8', 'body': ""Hi, @nassimus26 \r\n\r\nThank you for bringing this issue to our attention, as per [official documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#build-installable-package) To build an installable package that can be used as a dependency by another CMake project with `find_package(tensorflow-lite CONFIG)`, in your projects `CMakeLists.txt` file and I do not believe these instructions are meant to be used with `cmake --install .`\r\n\r\nRefer to CMake documentation for [find_package](https://cmake.org/cmake/help/latest/command/find_package.html) to learn more about handling and locating packages.\r\n\r\nIf I've missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 14, 11, 52, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058512, 'issue_id': 2582019084, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012138, 'issue_id': 2582019084, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443012304, 'issue_id': 2582019084, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77718"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77718"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 2, 3, 10, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-14 11:52:10 UTC): Hi, @nassimus26 

Thank you for bringing this issue to our attention, as per [official documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#build-installable-package) To build an installable package that can be used as a dependency by another CMake project with `find_package(tensorflow-lite CONFIG)`, in your projects `CMakeLists.txt` file and I do not believe these instructions are meant to be used with `cmake --install .`

Refer to CMake documentation for [find_package](https://cmake.org/cmake/help/latest/command/find_package.html) to learn more about handling and locating packages.

If I've missed something here please let me know.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-22 02:02:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-29 02:03:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-29 02:03:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77718"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77718"">No</a>

"
2581595249,issue,closed,completed,"Ho to deal with this Message : TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack","### 1. System information

- Linux Ubuntu 16.04
- TensorFlow installation 2.17

I want to convert this model and check if it could run on a Coral Edge TPU

### 2. Code
```
num_classes = 2
base_model = tf.keras.applications.MobileNetV2(
    include_top=False, weights='imagenet', input_tensor=None,
    input_shape=input_shape,
    pooling=None, 
)
for layer in base_model.layers:
    layer.trainable = False
base_model.summary()
cnn = models.Sequential()
cnn.add(base_model)
cnn.add(layers.GlobalAveragePooling2D())
cnn.add(layers.Dropout(0.2))
base_model.trainable = False
model = models.Sequential()
print(full_input_shape)
model.add(layers.TimeDistributed(cnn, input_shape=full_input_shape))
model.add(layers.LSTM(nbr_frame, return_sequences=True))
model.add(layers.TimeDistributed(layers.Dense(nbr_frame, activation='relu')))
model.add(layers.Flatten())
model.add(layers.Dense(164, activation='relu', name=""filter1""))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(24, activation='sigmoid', name=""filter2""))
model.add(layers.Dropout(0.1))
model.add(layers.Dense(num_classes, activation=""sigmoid"", name=""last""))
rms = optimizers.RMSprop()
metrics = [tf.keras.metrics.CategoricalAccuracy('accuracy', dtype=tf.float32)]
loss = tf.keras.losses.CategoricalCrossentropy()
model.compile(
    loss=loss,
    optimizer= rms,
    metrics=metrics
)
#####.....training the model ... ######
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.experimental_new_converter = True
tflite_model = converter.convert()
open(""fights.tflite"", ""wb"").write(tflite_model)
```

I am getting this message (I have no idea how to deal with this, I didn't find any relevant documentation about this message):

```
W0000 00:00:1728657905.522324    4234 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.
W0000 00:00:1728657905.522345    4234 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.
2024-10-11 16:45:05.522588: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpwe7b0rla
2024-10-11 16:45:05.543656: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-10-11 16:45:05.543697: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpwe7b0rla
2024-10-11 16:45:05.843740: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-10-11 16:45:06.979508: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpwe7b0rla
2024-10-11 16:45:07.522614: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 2000031 microseconds.
2024-10-11 16:45:10.755926: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3463] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack
Details:
	tf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x10xf32>>>) : {device = """"}
	tf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x10xf32>>>, tensor<i32>, tensor<?x10xf32>) -> (tensor<!tf_type.variant<tensor<?x10xf32>>>) : {device = """", resize_if_index_out_of_bounds = false}
	tf.TensorListStack(tensor<!tf_type.variant<tensor<?x10xf32>>>, tensor<2xi32>) -> (tensor<10x?x10xf32>) : {device = """", num_elements = 10 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
```",nassimus26,2024-10-11 14:59:51+00:00,['gaikwadrahul8'],2024-11-08 02:01:02+00:00,2024-11-08 02:00:59+00:00,https://github.com/tensorflow/tensorflow/issues/77704,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2411183894, 'issue_id': 2581595249, 'author': 'gaikwadrahul8', 'body': 'Hi, @nassimus26 \r\n\r\nI apologize for the delayed response, if possible could you please help us with Google colab to reproduce the same bahavior from our end to investigate this issue further from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 14, 13, 5, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058554, 'issue_id': 2581595249, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432510537, 'issue_id': 2581595249, 'author': 'fergushenderson', 'body': "">   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n\r\nTry removing that line?\r\n\r\nIn general that is needed for running on CPU, if your model contains TF ops that can't be converted into TF Lite ops.\r\nBut I don't think such ops will run on EdgeTPU.  If you remove that line, either it will now just work, or if not you should get some information from the converter about which ops can't be converted."", 'created_at': datetime.datetime(2024, 10, 23, 14, 55, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432968882, 'issue_id': 2581595249, 'author': 'gaikwadrahul8', 'body': ""Hi, @nassimus26\r\nHi, @fergushenderson Thank you for your pointers\r\n\r\nAs far I know that  message indicates that your model contains TensorFlow operations that are not natively supported in TFLite and requires the Flex delegate. Try using a simpler architecture that is known to work with Edge TPU. Use the Edge TPU compatibility checker tool and quantize your model to **INT8**, you'll have to replace `TimeDistributed` and `LSTM` with supported operations mentioned [here](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)\r\n\r\nYou can use Flex Delegate to avoid this message `TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack` but as you mentioned in the issue template after converting the model to TFLite you want to run on Coral Edge TPU so which is not recommended for Edge TPU because the Edge TPU does not currently support the Flex Delegate\r\n\r\nPlease refer this [supported operations on the Edge TPU](https://coral.ai/docs/edgetpu/models-intro/#supported-operations) in that `TimeDistributed` is not supported operations and unidirectional `LSTM` only supported\r\n\r\nIf I have missed something here please let me know. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 23, 17, 37, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867776, 'issue_id': 2581595249, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607085, 'issue_id': 2581595249, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 8, 2, 0, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607159, 'issue_id': 2581595249, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77704"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77704"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 2, 1, 1, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-14 13:05:41 UTC): Hi, @nassimus26 

I apologize for the delayed response, if possible could you please help us with Google colab to reproduce the same bahavior from our end to investigate this issue further from our end ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-22 02:02:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

fergushenderson on (2024-10-23 14:55:23 UTC): Try removing that line?

In general that is needed for running on CPU, if your model contains TF ops that can't be converted into TF Lite ops.
But I don't think such ops will run on EdgeTPU.  If you remove that line, either it will now just work, or if not you should get some information from the converter about which ops can't be converted.

gaikwadrahul8 (Assginee) on (2024-10-23 17:37:25 UTC): Hi, @nassimus26
Hi, @fergushenderson Thank you for your pointers

As far I know that  message indicates that your model contains TensorFlow operations that are not natively supported in TFLite and requires the Flex delegate. Try using a simpler architecture that is known to work with Edge TPU. Use the Edge TPU compatibility checker tool and quantize your model to **INT8**, you'll have to replace `TimeDistributed` and `LSTM` with supported operations mentioned [here](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)

You can use Flex Delegate to avoid this message `TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack` but as you mentioned in the issue template after converting the model to TFLite you want to run on Coral Edge TPU so which is not recommended for Edge TPU because the Edge TPU does not currently support the Flex Delegate

Please refer this [supported operations on the Edge TPU](https://coral.ai/docs/edgetpu/models-intro/#supported-operations) in that `TimeDistributed` is not supported operations and unidirectional `LSTM` only supported

If I have missed something here please let me know. 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-31 02:03:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-08 02:00:58 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-08 02:01:01 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77704"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77704"">No</a>

"
2581293352,issue,closed,completed,"cuFFT, cuDNN, cuBLAS errors    AND    TF-TRT Warning: Could not find TensorRT    AND    could not open file to read NUMA node","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Windows 11 Pro 23H2 WSL2 Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.4 (WSL install) + cudnn 8.9.7 (pip TF[and-cuda] install)

### GPU model and memory

Quadro P620

### Current behavior?

Outputs errors and warnings when running the standard TF test `python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""`

Errors about cuFFT, cuDNN, cuBLAS (""already been registered"").

Warning TensorRT not found and could not open file to read NUMA node.

Steps to reproduce on Windows11 WSL 2 Ubuntu:

* Install NVIDIA CUDA Toolkit for WSL as specified here https://docs.nvidia.com/cuda/wsl-user-guide/index.html
* Create venv and activate it
* pip install tensorRT
* pip install tf with CUDA (cudnn, cublas, cufft)
* test TF

The TF-nightly install does not find the GPU

### Standalone code to reproduce the issue

```shell
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda-repo-wsl-ubuntu-12-6-local_12.6.2-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-12-6-local_12.6.2-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-6

python3 -m venv .venv
source .venv/bin/activate

pip cache remove ""tensorrt*""
python3 -m pip install --upgrade pip
python3 -m pip install wheel
python3 -m pip install --upgrade tensorrt
python3 -m pip install --upgrade tensorrt-lean
python3 -m pip install --upgrade tensorrt-dispatch
python3 -m pip install tensorflow[and-cuda]

python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
2024-10-11 14:41:24.335456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-11 14:41:24.955115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-11 14:41:25.124162: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-11 14:41:26.366017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-11 14:41:34.469157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1728650506.662098    1240 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
I0000 00:00:1728650508.389075    1240 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
I0000 00:00:1728650508.389145    1240 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
",DenisUllmann,2024-10-11 12:43:27+00:00,['Venkat6871'],2024-10-23 08:55:43+00:00,2024-10-15 15:23:51+00:00,https://github.com/tensorflow/tensorflow/issues/77694,"[('type:build/install', 'Build and install issues'), ('comp:gpu', 'GPU related issues'), ('wsl2', 'Windows Subsystem for Linux'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2407711364, 'issue_id': 2581293352, 'author': 'DenisUllmann', 'body': 'When I perform a Debian install of TensorRT instead of a python installation, I get the same TF-TRT Warning.\r\n\r\nYet, `dpkg-query -W tensorrt` returns\r\n\r\n`tensorrt        10.5.0.18-1+cuda12.6`', 'created_at': datetime.datetime(2024, 10, 11, 16, 3, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409695949, 'issue_id': 2581293352, 'author': 'jandy0414', 'body': 'I have the same question, i think the CUDA version change to be 11.8  \r\n\r\nbut the question is still on:\r\n\r\n![image](https://github.com/user-attachments/assets/5a4ea953-1c79-41ba-ba5a-c1cc23fd5957)\r\n![image](https://github.com/user-attachments/assets/f6685a17-a730-47f1-a0d6-6bba723402c8)\r\n\r\n![image](https://github.com/user-attachments/assets/b4a3a09d-fab8-4377-9a40-e64c720c0c48)\r\n\r\n\r\nmy os version is :\r\n  ubuntu 22.04.5 lts  gcc 11.4\r\n\r\nmy tesoflow version :\r\n   2.17.0\r\n\r\npython : 3.10.12', 'created_at': datetime.datetime(2024, 10, 14, 2, 0, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410093071, 'issue_id': 2581293352, 'author': 'Venkat6871', 'body': 'Hi @DenisUllmann ,\r\nApologies for the delay, and thank you for raising your concern here. You may need to install a compatible version of TensorRT from NVIDIA, as it is an optimization library specifically developed by NVIDIA for their GPUs and is not bundled with the TensorFlow package. You can use the tensorflow[and-cuda] package to install TensorFlow with the required CUDA/cuDNN packages, but it seems TensorRT is not included.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ced28148be8cec1f234886f21c7e528c374032b8/tensorflow/tools/pip_package/setup.py#L151-L166\r\n \r\nSome related issues are already ongoing. Please go through them once, as I hope they will be helpful for you. I am providing the [links](https://github.com/tensorflow/tensorflow/issues/64809) to those issues here for your reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 6, 8, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410142603, 'issue_id': 2581293352, 'author': 'DenisUllmann', 'body': ""Hello @Venkat6871 ,\r\n\r\nThank you for your answer. As you see I get 3 issues with the installation of TF on WSL2, and you understood that the most critical one is about cudnn.\r\n\r\nI have actually already tried the `pip install tensorfol[and-cuda]` as described in my original message. Surprisingly TF do not find the cuda/cudnn packages even with this specific `pip install` that should them as dependencies of TF. I can't understand why. \r\n\r\nRegarding TensorRT, do you know which version is compatible with the current TF 2.17.0 as it is not specified in the pip install guide for TF and I also already installed TensorRT but TF can't find it. \r\n\r\nFinally, regarding the NUMA errors, it seams that I have to build a custom WSL2 core that supports NUMA.\r\n\r\nThanks in advance for your help."", 'created_at': datetime.datetime(2024, 10, 14, 6, 28, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414292729, 'issue_id': 2581293352, 'author': 'DenisUllmann', 'body': 'Hello, with the following installation (Cuda toolkit 12.3.2, cudnn 8.9.7) on WSL2, I still get the same errors and warnings but the training finally works on GPU WITH cudNN.\r\n\r\nErrors displayed about cublas, cudnn and cufft registers are sent by XLA, but at the end these libraries are active.\r\n\r\nThere are no available TensorRT version compatible with my GPU and cuda toolkit 12.3 at the same time.\r\n\r\nAnd Windows 11 Pro does not support NUMA, so XLA outputs Messages, but the setup works perfectly fine for training and inference.\r\n\r\nDelete and create symlinks for `libcuda` in windows, then restart WSL  (https://github.com/microsoft/WSL/issues/5663#issuecomment-1068499676)\r\n\r\n```\r\nC:\r\ncd \\Windows\\System32\\lxss\\lib\r\ndel libcuda.so\r\ndel libcuda.so.1\r\nmklink libcuda.so libcuda.so.1.1\r\nmklink libcuda.so.1 libcuda.so.1.1\r\nwsl --shutdown\r\nwsl\r\n```\r\n\r\nInstall CUDA toolkit for WSL 12.3 (without cuda driver, NVIDIA Container Toolkit for WSL should be installed beforehand)\r\n\r\n```\r\nsudo apt-key del 7fa2af80\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\r\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda-repo-wsl-ubuntu-12-3-local_12.3.2-1_amd64.deb\r\nsudo dpkg -i cuda-repo-wsl-ubuntu-12-3-local_12.3.2-1_amd64.deb\r\nsudo cp /var/cuda-repo-wsl-ubuntu-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\r\nsudo apt-get update\r\nsudo apt-get -y install cuda-toolkit-12-3\r\n```\r\n\r\nThen download cudnn 8.9.7 https://developer.nvidia.com/rdp/cudnn-archive#a-collapse897-120\r\n\r\n```\r\nsudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.7.29_1.0-1_amd64.deb\r\nsudo cp /var/cudnn-local-repo-ubuntu2204-8.9.7.29/cudnn-local-08A7D361-keyring.gpg /usr/share/keyrings/\r\nsudo apt-get update\r\nsudo apt-get -y install libcudnn8    (use apt-cache search cudnn)\r\nsudo apt-get -y install libcudnn8-dev\r\nsudo apt autoremove\r\n```\r\n\r\nAnd testing install:\r\n\r\n```\r\nfunction lib_installed() { /sbin/ldconfig -N -v $(sed \'s/:/ /\' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; }\r\nfunction check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; }\r\ncheck libcuda\r\ncheck libcudart\r\n```\r\n\r\n```\r\nfunction lib_installed() { /sbin/ldconfig -N -v $(sed \'s/:/ /\' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; }\r\nfunction check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; }\r\ncheck libcudnn \r\n```', 'created_at': datetime.datetime(2024, 10, 15, 15, 23, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414292813, 'issue_id': 2581293352, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77694"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77694"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 15, 23, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431362762, 'issue_id': 2581293352, 'author': 'DenisUllmann', 'body': 'For those who may be interested, I could get TF 2.17 to run in WSL with a GPU computing capability of 6.1: the installed configuration is Python 11, CUDA 12.1, TensorRT 8.6.1, SSE3, SEE4.1, SSE4.2, FMA, AVX, AVX2.\r\n\r\nCUDA 12.1 + TensorRT 8.6.1 is the right combination when your GPU is 6.1\r\n\r\nBazel version is 6.5.0\r\n\r\nGCC version is 12.3.0\r\n\r\nGet your system architecture:\r\n\r\n```\r\ngrep flags -m1 /proc/cpuinfo | cut -d "":"" -f 2 | tr \'[:upper:]\' \'[:lower:]\' | { read FLAGS; OPT=""-march=native""; for flag in $FLAGS; do case ""$flag"" in ""sse4_1"" | ""sse4_2"" | ""ssse3"" | ""fma"" | ""cx16"" | ""popcnt"" | ""avx"" | ""avx2"" | ""mfpmath"") OPT+="" -m$flag"";; esac; done; MODOPT=${OPT//_/\\.}; echo ""$MODOPT""; }\r\n```\r\n\r\nEg. I got: `-march=native -mssse3 -mfma -mcx16 -msse4.1 -msse4.2 -mpopcnt -mavx -mavx2`\r\n\r\nInstall install cuda toolkit 12.1 for wsl\r\n```\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\r\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb\r\nsudo dpkg -i cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb\r\nsudo cp /var/cuda-repo-wsl-ubuntu-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\r\nsudo apt-get update\r\nsudo apt-get -y install cuda\r\n```\r\n\r\nInstall cudnn 8.9.7 (download `*.deb` from nvidia)\r\n```\r\nsudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.7.29_1.0-1_amd64.deb\r\nsudo cp /var/cudnn-local-repo-ubuntu2204-8.9.7.29/cudnn-local-*-keyring.gpg /usr/share/keyrings/\r\nsudo apt-get update\r\nsudo apt-get -y install libcudnn8    (use apt-cache search cudnn)\r\nsudo apt-get -y install libcudnn8-dev\r\nsudo apt autoremove\r\n```\r\n\r\nInstall tensorrt 8.6.1 (because I have a 6.1 GPU)\r\n```\r\nos=""ubuntu2204""\r\ntag=""8.6.1-cuda-12.0""\r\nsudo dpkg -i nv-tensorrt-local-repo-${os}-${tag}_1.0-1_amd64.deb\r\nsudo cp /var/nv-tensorrt-local-repo-${os}-${tag}/*-keyring.gpg /usr/share/keyrings/\r\nsudo apt-get update\r\nsudo apt-get install tensorrt\r\n```\r\n\r\nUpdate python to 3.11\r\n```\r\nsudo apt update && sudo apt upgrade -y\r\nsudo apt install python3.11\r\nsudo update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.10 1\r\nsudo update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.11 2\r\nsudo update-alternatives --config python3\r\n```\r\nrestart WSL, update Ubuntu related python packages\r\n```\r\nsudo apt remove --purge python3-apt\r\nsudo apt autoclean\r\nsudo apt install python3-apt\r\nsudo apt-get install python3.11-distutils python3.11-dev python3.11-venv\r\ncurl -sS https://bootstrap.pypa.io/get-pip.py | python3.11\r\n```\r\n\r\nFollow these steps for the build : [https://www.tensorflow.org/install/source?_gl=1*gurby1*_up*MQ..*_ga*MTIxNDI1OTYyMi4xNzI5MDc5MzA2*_ga_W0YLR4190T*MTcyOTA3OTMwNi4xLjAuMTcyOTA3OTMwNi4wLjAuMA..#setup_for_linux_and_macos](tensorflow build from source)\r\n\r\nWhen configuring the build `./configure` don\'t use CLANG as the host compiler (it says it is partially supported with CUDA 12.1 and it fails at the end). \r\nFor the optimization flags, use the ones obtained previously about our system architecture.\r\nHere are my answers for the configuration:\r\n\r\n```\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is /*/.venv/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /*/.venv/lib/python3.11/site-packages\r\nPlease input the desired Python library path to use.  Default is [/*/.venv/lib/python3.11/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 12.1 in:\r\n    /usr/local/cuda-12.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-12.1/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 8.6.1 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [Y/n]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native -mssse3 -mfma -mcx16 -msse4.1 -msse4.2 -mpopcnt -mavx -mavx2\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\nexport the variables requested when attempting to build, eg.: `export TF_PYTHON_VERSION=3.11`\r\n\r\nAnd build with these options (no nccl and opt to build with SSE3, SEE4.1, SSE4.2, FMA, AVX, AVX2):\r\n\r\n`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=opt --config=nonccl --verbose_failures`\r\n\r\nAnd then `pip install` the built TF wheel.\r\n\r\nYou may need to `sudo apt-get install patchelf` before the build (in case the build fails because of it) but I am not sure, I may have installed it due to previous attemps to build.\r\n\r\nThe `import tensorflow` in python still outputs XLA related cuBLAS, cuDNN and cuFFT errors and NUMA warnings but we can ignore them as NUMA in currently not supported in WSL and all rquested CUDA libraries, including cuBLAS, cuDNN and cuFFT, are loaded when training.', 'created_at': datetime.datetime(2024, 10, 23, 8, 54, 48, tzinfo=datetime.timezone.utc)}]","DenisUllmann (Issue Creator) on (2024-10-11 16:03:35 UTC): When I perform a Debian install of TensorRT instead of a python installation, I get the same TF-TRT Warning.

Yet, `dpkg-query -W tensorrt` returns

`tensorrt        10.5.0.18-1+cuda12.6`

jandy0414 on (2024-10-14 02:00:57 UTC): I have the same question, i think the CUDA version change to be 11.8  

but the question is still on:

![image](https://github.com/user-attachments/assets/5a4ea953-1c79-41ba-ba5a-c1cc23fd5957)
![image](https://github.com/user-attachments/assets/f6685a17-a730-47f1-a0d6-6bba723402c8)

![image](https://github.com/user-attachments/assets/b4a3a09d-fab8-4377-9a40-e64c720c0c48)


my os version is :
  ubuntu 22.04.5 lts  gcc 11.4

my tesoflow version :
   2.17.0

python : 3.10.12

Venkat6871 (Assginee) on (2024-10-14 06:08:21 UTC): Hi @DenisUllmann ,
Apologies for the delay, and thank you for raising your concern here. You may need to install a compatible version of TensorRT from NVIDIA, as it is an optimization library specifically developed by NVIDIA for their GPUs and is not bundled with the TensorFlow package. You can use the tensorflow[and-cuda] package to install TensorFlow with the required CUDA/cuDNN packages, but it seems TensorRT is not included.

https://github.com/tensorflow/tensorflow/blob/ced28148be8cec1f234886f21c7e528c374032b8/tensorflow/tools/pip_package/setup.py#L151-L166
 
Some related issues are already ongoing. Please go through them once, as I hope they will be helpful for you. I am providing the [links](https://github.com/tensorflow/tensorflow/issues/64809) to those issues here for your reference.

Thank you!

DenisUllmann (Issue Creator) on (2024-10-14 06:28:01 UTC): Hello @Venkat6871 ,

Thank you for your answer. As you see I get 3 issues with the installation of TF on WSL2, and you understood that the most critical one is about cudnn.

I have actually already tried the `pip install tensorfol[and-cuda]` as described in my original message. Surprisingly TF do not find the cuda/cudnn packages even with this specific `pip install` that should them as dependencies of TF. I can't understand why. 

Regarding TensorRT, do you know which version is compatible with the current TF 2.17.0 as it is not specified in the pip install guide for TF and I also already installed TensorRT but TF can't find it. 

Finally, regarding the NUMA errors, it seams that I have to build a custom WSL2 core that supports NUMA.

Thanks in advance for your help.

DenisUllmann (Issue Creator) on (2024-10-15 15:23:51 UTC): Hello, with the following installation (Cuda toolkit 12.3.2, cudnn 8.9.7) on WSL2, I still get the same errors and warnings but the training finally works on GPU WITH cudNN.

Errors displayed about cublas, cudnn and cufft registers are sent by XLA, but at the end these libraries are active.

There are no available TensorRT version compatible with my GPU and cuda toolkit 12.3 at the same time.

And Windows 11 Pro does not support NUMA, so XLA outputs Messages, but the setup works perfectly fine for training and inference.

Delete and create symlinks for `libcuda` in windows, then restart WSL  (https://github.com/microsoft/WSL/issues/5663#issuecomment-1068499676)

```
C:
cd \Windows\System32\lxss\lib
del libcuda.so
del libcuda.so.1
mklink libcuda.so libcuda.so.1.1
mklink libcuda.so.1 libcuda.so.1.1
wsl --shutdown
wsl
```

Install CUDA toolkit for WSL 12.3 (without cuda driver, NVIDIA Container Toolkit for WSL should be installed beforehand)

```
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda-repo-wsl-ubuntu-12-3-local_12.3.2-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-12-3-local_12.3.2-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-3
```

Then download cudnn 8.9.7 https://developer.nvidia.com/rdp/cudnn-archive#a-collapse897-120

```
sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.7.29_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2204-8.9.7.29/cudnn-local-08A7D361-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install libcudnn8    (use apt-cache search cudnn)
sudo apt-get -y install libcudnn8-dev
sudo apt autoremove
```

And testing install:

```
function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; }
function check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; }
check libcuda
check libcudart
```

```
function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; }
function check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; }
check libcudnn 
```

google-ml-butler[bot] on (2024-10-15 15:23:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77694"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77694"">No</a>

DenisUllmann (Issue Creator) on (2024-10-23 08:54:48 UTC): For those who may be interested, I could get TF 2.17 to run in WSL with a GPU computing capability of 6.1: the installed configuration is Python 11, CUDA 12.1, TensorRT 8.6.1, SSE3, SEE4.1, SSE4.2, FMA, AVX, AVX2.

CUDA 12.1 + TensorRT 8.6.1 is the right combination when your GPU is 6.1

Bazel version is 6.5.0

GCC version is 12.3.0

Get your system architecture:

```
grep flags -m1 /proc/cpuinfo | cut -d "":"" -f 2 | tr '[:upper:]' '[:lower:]' | { read FLAGS; OPT=""-march=native""; for flag in $FLAGS; do case ""$flag"" in ""sse4_1"" | ""sse4_2"" | ""ssse3"" | ""fma"" | ""cx16"" | ""popcnt"" | ""avx"" | ""avx2"" | ""mfpmath"") OPT+="" -m$flag"";; esac; done; MODOPT=${OPT//_/\.}; echo ""$MODOPT""; }
```

Eg. I got: `-march=native -mssse3 -mfma -mcx16 -msse4.1 -msse4.2 -mpopcnt -mavx -mavx2`

Install install cuda toolkit 12.1 for wsl
```
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
```

Install cudnn 8.9.7 (download `*.deb` from nvidia)
```
sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.7.29_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2204-8.9.7.29/cudnn-local-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install libcudnn8    (use apt-cache search cudnn)
sudo apt-get -y install libcudnn8-dev
sudo apt autoremove
```

Install tensorrt 8.6.1 (because I have a 6.1 GPU)
```
os=""ubuntu2204""
tag=""8.6.1-cuda-12.0""
sudo dpkg -i nv-tensorrt-local-repo-${os}-${tag}_1.0-1_amd64.deb
sudo cp /var/nv-tensorrt-local-repo-${os}-${tag}/*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get install tensorrt
```

Update python to 3.11
```
sudo apt update && sudo apt upgrade -y
sudo apt install python3.11
sudo update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.10 1
sudo update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.11 2
sudo update-alternatives --config python3
```
restart WSL, update Ubuntu related python packages
```
sudo apt remove --purge python3-apt
sudo apt autoclean
sudo apt install python3-apt
sudo apt-get install python3.11-distutils python3.11-dev python3.11-venv
curl -sS https://bootstrap.pypa.io/get-pip.py | python3.11
```

Follow these steps for the build : [https://www.tensorflow.org/install/source?_gl=1*gurby1*_up*MQ..*_ga*MTIxNDI1OTYyMi4xNzI5MDc5MzA2*_ga_W0YLR4190T*MTcyOTA3OTMwNi4xLjAuMTcyOTA3OTMwNi4wLjAuMA..#setup_for_linux_and_macos](tensorflow build from source)

When configuring the build `./configure` don't use CLANG as the host compiler (it says it is partially supported with CUDA 12.1 and it fails at the end). 
For the optimization flags, use the ones obtained previously about our system architecture.
Here are my answers for the configuration:

```
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /*/.venv/bin/python3]:


Found possible Python library paths:
  /*/.venv/lib/python3.11/site-packages
Please input the desired Python library path to use.  Default is [/*/.venv/lib/python3.11/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Found CUDA 12.1 in:
    /usr/local/cuda-12.1/targets/x86_64-linux/lib
    /usr/local/cuda-12.1/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 8.6.1 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]:


Do you want to use clang as CUDA compiler? [Y/n]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native -mssse3 -mfma -mcx16 -msse4.1 -msse4.2 -mpopcnt -mavx -mavx2


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```

export the variables requested when attempting to build, eg.: `export TF_PYTHON_VERSION=3.11`

And build with these options (no nccl and opt to build with SSE3, SEE4.1, SSE4.2, FMA, AVX, AVX2):

`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=opt --config=nonccl --verbose_failures`

And then `pip install` the built TF wheel.

You may need to `sudo apt-get install patchelf` before the build (in case the build fails because of it) but I am not sure, I may have installed it due to previous attemps to build.

The `import tensorflow` in python still outputs XLA related cuBLAS, cuDNN and cuFFT errors and NUMA warnings but we can ignore them as NUMA in currently not supported in WSL and all rquested CUDA libraries, including cuBLAS, cuDNN and cuFFT, are loaded when training.

"
2581271735,issue,open,,Gradients of tf.linalg.expm not supported with JIT compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tested 2.17 and 2.10, both have the issue

### Custom code

Yes

### OS platform and distribution

Ubuntu

### Mobile device

_No response_

### Python version

tested 3.9 and 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Gradients of `tf.linalg.expm` can not be computed with JIT compilation. 

This is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

A = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)

@tf.function(jit_compile=True) #set jit_compile=False to make it work
def f(A):
    with tf.GradientTape() as tape:
        B = tf.linalg.expm(A)
    return tape.gradient(B, A)

f(A)
```


### Relevant log output

```shell
2024-10-11 11:17:27.281304: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.

Stack trace for op definition: 
File ""<frozen runpy>"", line 198, in _run_module_as_main
File ""<frozen runpy>"", line 88, in _run_code
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py"", line 18, in <module>
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py"", line 1075, in launch_instance
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py"", line 739, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py"", line 205, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py"", line 88, in _run
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"",
```
",felbecker,2024-10-11 12:31:35+00:00,['Venkat6871'],2024-10-21 09:23:35+00:00,,https://github.com/tensorflow/tensorflow/issues/77693,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2410872544, 'issue_id': 2581271735, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/9fe44a45fdb2707ab0fd7ed5c12a3cb4/untitled374.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 11, 3, 10, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-14 11:03:10 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/9fe44a45fdb2707ab0fd7ed5c12a3cb4/untitled374.ipynb) here for reference.
Thank you!

"
2580256434,issue,closed,completed,@Name,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",Bigfamily666,2024-10-11 02:16:45+00:00,['Venkat6871'],2024-10-27 02:05:44+00:00,2024-10-27 02:05:43+00:00,https://github.com/tensorflow/tensorflow/issues/77618,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2406408155, 'issue_id': 2580256434, 'author': 'Bigfamily666', 'body': '> **System information**\n> - Android Device information (use `adb shell getprop ro.build.fingerprint`\n>   if possible):\n> - TensorFlow Lite in Play Services SDK version (found in `build.gradle`):\n> - Google Play Services version\n>   (`Settings` > `Apps` > `Google Play Services` > `App details`):\n> \n> **Standalone code to reproduce the issue**\n> Provide a reproducible test case that is the bare minimum necessary to generate\n> the problem. If possible, please share a link to or attach code demonstrating\n> the problem.\n> \n> **Any other info / logs**\n> Include any logs or source code that would be helpful to diagnose the problem.\n> If including tracebacks, please include the full traceback. Large logs and files\n> should be attached.\n>', 'created_at': datetime.datetime(2024, 10, 11, 2, 17, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407179000, 'issue_id': 2580256434, 'author': 'Venkat6871', 'body': 'Hi **@Bigfamily666** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 11, 11, 8, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423475654, 'issue_id': 2580256434, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 19, 2, 0, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439804065, 'issue_id': 2580256434, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 27, 2, 5, 43, tzinfo=datetime.timezone.utc)}]","Bigfamily666 (Issue Creator) on (2024-10-11 02:17:43 UTC): 

Venkat6871 (Assginee) on (2024-10-11 11:08:01 UTC): Hi **@Bigfamily666** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

github-actions[bot] on (2024-10-19 02:00:47 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-27 02:05:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2579298210,issue,closed,not_planned,좋은거,https://github.com/jk96491/Advanced_Models/blob/master/Models%2FVAE_Model%2Fmain.py,hogomister,2024-10-10 16:16:19+00:00,['tilakrayal'],2024-10-11 14:32:53+00:00,2024-10-11 14:32:53+00:00,https://github.com/tensorflow/tensorflow/issues/77570,[],[],
2578348983,issue,open,,tf.custom_gradient for function with kwarg shows unexpected behavior,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3/8

### GPU model and memory

_No response_

### Current behavior?

I have a function that takes two tensors as inputs, one as argument and one as keyword argument.
The function has a custom gradient.

When ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.

When the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.


### Standalone code to reproduce the issue

```shell
@tf.custom_gradient
def func(x, y=0):
    z = 2*x + y
    def grad(dz):
        dx = 2*dz
        dy = dz
        return dx, dy
    return z, grad
x = tf.constant(2.)
y = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch([x, y])
    z = func(x, y=y) #func(x, y) does not generate the error
grads = tape.gradient(z, [x, y])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[129], line 14
     12     tape.watch([x, y])
     13     z = func(x, y=y)
---> 14 grads = tape.gradient(z, [x, y])

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)
   1060   output_gradients = (
   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(
   1062           output_gradients))
   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)
   1064                       for x in output_gradients]
-> 1066 flat_grad = imperative_grad.imperative_grad(
   1067     self._tape,
   1068     flat_targets,
   1069     flat_sources,
   1070     output_gradients=output_gradients,
   1071     sources_raw=flat_sources_raw,
   1072     unconnected_gradients=unconnected_gradients)
   1074 if not self._persistent:
   1075   # Keep track of watched variables before setting tape to None
   1076   self._watched_variables = self._tape.watched_variables()

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     63 except ValueError:
     64   raise ValueError(
     65       ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
---> 67 return pywrap_tfe.TFE_Py_TapeGradient(
     68     tape._tape,  # pylint: disable=protected-access
     69     target,
     70     sources,
     71     output_gradients,
     72     sources_raw,
     73     compat.as_str(unconnected_gradients.value))

File ~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)
    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(
    586     nest.flatten(input_grads))
    587 if len(flat_grads) != arg_count:
--> 588   raise ValueError(
    589       f""custom_gradient function expected to return {arg_count} ""
    590       f""gradients, but returned {len(flat_grads)} instead."")
    591 return flat_grads + variable_grads

ValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.
```
",jhoydis,2024-10-10 10:18:54+00:00,['Venkat6871'],2024-10-22 07:16:10+00:00,,https://github.com/tensorflow/tensorflow/issues/77559,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2407175649, 'issue_id': 2578348983, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/7abeab63d74f062929fd319f505e2d35/77559_2-17-0-and-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 11, 11, 5, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-11 11:05:55 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/7abeab63d74f062929fd319f505e2d35/77559_2-17-0-and-nightly-v.ipynb) here for reference.
Thank you!

"
2577559272,issue,closed,completed,Issue: Error Serving LSTM Model on CPU After GPU Training,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I trained a model using LSTM layers on a GPU for faster processing. However, when attempting to serve the model in a cloud environment with only CPU support, I encountered the following error:

`No OpKernel was registered to support Op 'CudnnRNNV3'.`

I understand that this error is related to GPU-specific operations (e.g., CuDNN) not being available on CPU.

**My Question:**
Is there a way to train a model using GPU and still successfully serve it on a CPU-only environment?

**What I’ve Tried:**
Exporting the model in SavedModel format
Converting the model to TensorFlow Lite (TFLite)

However, both approaches resulted in similar errors related to GPU dependencies.

Saving the model in .keras format after training with GPU, then using it in CPU only environment actually works, but very slow, because the same model trained with CPU and using TFLite format works almost instant. Is a very small model.

Any guidance on how to resolve this would be greatly appreciated!

### Standalone code to reproduce the issue

```shell
Basic model creation with a layer like tf.keras.layers.LSTM(256, return_sequences=True)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File "".../index.py"", line 114, in <module>
    main()
  File "".../index.py"", line 39, in main
    result = predict(model, metadata, values)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../model.py"", line 139, in predict
    prediction = model.serve(padded_sequence) #inference model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "".../.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNNV3' used by {{node StatefulPartitionedCall/sequential_1/lstm_1/CudnnRNNV3}} with these attrs: [rnn_mode=""lstm"", seed=0, input_mode=""linear_input"", num_proj=0, dropout=0, seed2=0, is_training=true, direction=""unidirectional"", time_major=false, T=DT_FLOAT]
```
",dev2xl,2024-10-10 04:39:14+00:00,['tilakrayal'],2024-11-04 13:51:43+00:00,2024-10-31 02:03:06+00:00,https://github.com/tensorflow/tensorflow/issues/77442,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2416064332, 'issue_id': 2577559272, 'author': 'tilakrayal', 'body': ""@dev2xl,\r\nCould you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. \r\nAlso It seems like somewhere in your code, you're using CudnnRNN and as you know AMD GPU is not compatible with cuda-toolkit and if you're not using CudnnRNN anywhere in your code so Could you please try to run the same code on Google Colab with GPU and check whether is it running as expected or not.\r\n\r\nThank  you!"", 'created_at': datetime.datetime(2024, 10, 16, 8, 19, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073571, 'issue_id': 2577559272, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867855, 'issue_id': 2577559272, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867897, 'issue_id': 2577559272, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77442"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77442"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 7, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-16 08:19:22 UTC): @dev2xl,
Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. 
Also It seems like somewhere in your code, you're using CudnnRNN and as you know AMD GPU is not compatible with cuda-toolkit and if you're not using CudnnRNN anywhere in your code so Could you please try to run the same code on Google Colab with GPU and check whether is it running as expected or not.

Thank  you!

github-actions[bot] on (2024-10-24 02:01:36 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-31 02:03:05 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-31 02:03:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77442"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77442"">No</a>

"
2576429918,issue,closed,completed,Tensorflow Import  Not working,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 11 Home (Version 23H2, OS Build 22631.4169)

### Mobile device

_No response_

### Python version

Error was reproduced on py 3.12.0 and py 3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected tensorflow to import as normal, but the terminal throws the same error every time. I've uninstalled + reinstalled Python (3.12.0 and 3.11.9 seperately, since the documentation said that tensorflow was supported up till py 3.11) multiple times, and done the same with tensorflow, tried the same thing with tf-nightly too.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\khiza\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\khiza\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\khiza\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\khiza\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```
",NexusHex,2024-10-09 16:40:29+00:00,['Venkat6871'],2024-12-19 15:44:45+00:00,2024-12-18 17:53:59+00:00,https://github.com/tensorflow/tensorflow/issues/77387,"[('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2406105759, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex can you please try the steps below and let us know if the issue persists\r\n![image](https://github.com/user-attachments/assets/69f6f091-0f9d-4f06-a5a4-b6413a24f343)', 'created_at': datetime.datetime(2024, 10, 10, 21, 53, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407147238, 'issue_id': 2576429918, 'author': 'Venkat6871', 'body': 'Hi **@NexusHex** ,\r\nCould you please try using the latest version of TensorFlow 2.18.0-rc1? I am providing the [documentation](https://github.com/tensorflow/tensorflow/releases) for the latest version here. Please let us know if the issue still persists.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 11, 10, 48, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412841419, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""> Hi @NexusHex can you please try the steps below and let us know if the issue persists ![image](https://private-user-images.githubusercontent.com/83710963/375556384-69f6f091-0f9d-4f06-a5a4-b6413a24f343.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjg5NjMyMjYsIm5iZiI6MTcyODk2MjkyNiwicGF0aCI6Ii84MzcxMDk2My8zNzU1NTYzODQtNjlmNmYwOTEtMGY5ZC00ZjA2LWE1YTQtYjY0MTNhMjRmMzQzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDAzMjg0NlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExMGZmNjVlYzA2YzdjMjBkNmIwMjMzZDliMzBmNjIxZjFlZmM2NDc1ODVkN2EyOTA5NzM1OGI4ODViZmY3NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.lOgbenONcxbfqRr7A11gqu9av1fa81q1NORy-59HA2o)\r\n\r\nHi mraunak, sorry for the long wait. I hope your Thanksgiving weekend went well. I attempted to do the instructions as listed (the paths for the venv weren't the same, so I created the venv from my user directory rather than from C:\\Python311\\python.exe) but the venv still doesn't recognize Tensorflow as a module:\r\n![{8B66024A-FFFC-4C4C-8368-B59F1E62BD5C}](https://github.com/user-attachments/assets/8879129b-f63c-4872-9a00-18fe6144c3b6)\r\n\r\nI called pip list to see if there was anything wrong, and saw that tf-nightly was still there from my first attempt importing Tensorflow, so I proceeded to delete all modules related to tf-nightly and try again (using the specific path this time):\r\n![{EED64C05-D774-4FE1-9A32-95A2E91397D9}](https://github.com/user-attachments/assets/2b6c2ee6-b337-4104-b08e-42324c1926e4)\r\n![{680A00CD-44C1-4C46-B07B-D3BE570622DE}](https://github.com/user-attachments/assets/b575b6af-fc21-4af1-b411-396d73e10817)\r\n![{DA414BCA-8C69-4A65-9CEA-0455CD7B1F46}](https://github.com/user-attachments/assets/95c48185-01dc-40f7-b12f-bb35a4c14d18)\r\n![{37E81077-25F4-4230-AD60-FB5B8F4D166B}](https://github.com/user-attachments/assets/a7ee6e99-8446-4866-952c-997e951b0c1e)\r\n![{C8F58779-12F9-432C-8426-DFD94EE9C0A6}](https://github.com/user-attachments/assets/70332ca4-7f59-4b1a-9b50-8f7ba69d0138)\r\nUnfortunately, none of this worked... I hope I followed all the instructions correctly, otherwise I'll wait for your reply.\r\nThanks :)"", 'created_at': datetime.datetime(2024, 10, 15, 3, 53, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412854475, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""> Hi **@NexusHex** , Could you please try using the latest version of TensorFlow 2.18.0-rc1? I am providing the [documentation](https://github.com/tensorflow/tensorflow/releases) for the latest version here. Please let us know if the issue still persists. Thank you!\r\n\r\nAfter attempting mraunak's solution, I tried yours in the same venv as the one I used before, here are the results:\r\n![{A999FB69-118D-44F1-9C03-A3FF7D870CEB}](https://github.com/user-attachments/assets/bd25d7d5-2ecb-4009-b526-17909740e8e0)\r\n![{1B629FF2-8C66-4F50-837D-0BA612F15290}](https://github.com/user-attachments/assets/45b8e6ae-f8f0-4c62-9686-7865c7f1db53)\r\n![{8C163423-71B8-4E5A-BE55-B828BDBF50DD}](https://github.com/user-attachments/assets/7ca9d106-952d-4f0c-9e4f-c81c50a8baf4)\r\n![{378CC51E-0568-45AA-BA7A-2A925A54E4EA}](https://github.com/user-attachments/assets/283781c9-1f91-4a01-a3b7-add22011dbbd)\r\nAnother dead end, but I'll wait for your response along with mraunak's too!"", 'created_at': datetime.datetime(2024, 10, 15, 4, 8, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413077336, 'issue_id': 2576429918, 'author': 'mraunak', 'body': ""Hi @NexusHex, Could you please download and install the Python package in a new location, somewhere on the C:\\ drive? I'm a bit puzzled since it's working fine on my end without needing any pip installation. I tried with the exact Python version that you have. \r\n\r\n![image](https://github.com/user-attachments/assets/d6cc5386-3e0e-4aa2-acd7-7e2ab57b295d)"", 'created_at': datetime.datetime(2024, 10, 15, 7, 13, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414393461, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""@mraunak I ran the code as shown from your example, and it came up with the same ModuleNotFoundError.\r\nFrom that point I decided to trace back all my steps, deleting all my temp files, any duplicate files, all packages linked to tensorflow, redownloading Python 3.11.9 and redownloading tf 2.18.0-rc1:\r\n![{C1022FDF-12F0-4222-B0F8-A807BF4FC6C1}](https://github.com/user-attachments/assets/cb8da2e4-9d6a-47a0-a464-b285ff2ce492)\r\n\r\nAfter I redownloaded everything I attempted to import the package as you normally would in the terminal:\r\n![{4C776B0B-BD69-47FB-BFA6-3BEC161C2070}](https://github.com/user-attachments/assets/a63f2762-2fe7-4040-aca8-178563179a34)\r\n\r\nThe same error happened again, so I tried it in a venv once again...\r\n![{09FDEC8E-4656-47CC-862D-574DD15037BA}](https://github.com/user-attachments/assets/2f031a86-8280-4df1-b410-8692424bd1d1)\r\n\r\nThis is pretty puzzling for me too, since I've never really had issues go this far with any packages."", 'created_at': datetime.datetime(2024, 10, 15, 15, 46, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415001033, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, can you please try to create a fresh virtual env and pip install the following packages with the same versions. This should work\r\n![image](https://github.com/user-attachments/assets/9ee0603e-997b-46f1-b1cf-bf41fc423fb2)', 'created_at': datetime.datetime(2024, 10, 15, 20, 38, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2419508183, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""Hi @mraunak, I did everything as per your instructions and ran 'import tensorflow as tf':\r\n![{9D889DDC-2214-4F8E-B4B3-2331344741E7}](https://github.com/user-attachments/assets/225a5752-6e9c-4e65-b078-8099f0eb4b02)\r\n![{2BDD1622-9F6E-4486-BE23-672CB4449C5B}](https://github.com/user-attachments/assets/9dd1a6f0-24e5-491a-8815-6823d04d638a)\r\n![{47368B12-C7D2-4B86-9598-9EDE5B71327F}](https://github.com/user-attachments/assets/6191fa5e-b3a5-46a5-93d5-87caeaabae9e)\r\n![{DB60FBF4-E571-4272-97AD-3165A8B936FC}](https://github.com/user-attachments/assets/5a7f23ba-667c-4aa2-a92f-ad36e610b5db)\r\n![{76C18621-568C-4FDC-9555-EAA7A02C5AF1}](https://github.com/user-attachments/assets/e50b028b-27bb-437c-8f03-b4f77b084495)\r\n![{D77584B1-6347-4F9B-AD43-769888FDB9EE}](https://github.com/user-attachments/assets/440d9634-5cd2-4967-80f1-840c9f0cb13e)\r\n![{0445604C-75F0-4868-BF72-D7A8DCE75620}](https://github.com/user-attachments/assets/ccb5bbfd-e1c6-4a2e-aaee-652a33a50b8c)\r\n![{749F0844-B486-4F31-9BC1-C823ABD7C151}](https://github.com/user-attachments/assets/586f051b-c3d2-4295-bc01-b8a3f055a1e5)\r\n![image](https://github.com/user-attachments/assets/4b83db69-4c98-43f1-bf2c-dba04559d2e4)\r\n![{9E736A75-DAB2-43B3-B8B9-6F097EEEC53A}](https://github.com/user-attachments/assets/0461c3a8-0c8e-44ab-982e-7128e193c26c)\r\n\r\nAfter installing all of this, I checked the pip list and uninstalled tensorflow-io-gcs-filesystem:\r\n![{555B2D88-C4F8-4E60-8268-7B7242BCDD1E}](https://github.com/user-attachments/assets/9253d5d6-838c-4c88-9b16-5b099a729c8f)\r\n\r\nNow that my pip list is the exact same as yours, I ran the code as normal and:\r\n![{94F90468-9805-4953-8E0E-A141DD4DEFB9}](https://github.com/user-attachments/assets/9816f7cf-5834-4e1c-85fc-17b6fa672db0)"", 'created_at': datetime.datetime(2024, 10, 17, 13, 10, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421156750, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Yeah, I was able to replicate the issue on my end on a different Windows machine. I just did pip install tensorflow and it fixed the issue. sorry, you need to have tensorflow-io-gcs-filesystem. \r\n\r\n![image](https://github.com/user-attachments/assets/2b7bacb4-4552-427a-8b28-f6815f98f965)\r\n![image](https://github.com/user-attachments/assets/047e2732-fcd6-41fc-ac76-3e3d9adbd740)', 'created_at': datetime.datetime(2024, 10, 18, 2, 41, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422609070, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Same result\r\n![{BDF59D90-3BD0-4F6B-8AE1-EADC510CD980}](https://github.com/user-attachments/assets/afd0e17f-829f-415e-b91b-9e7dff3bdc5a)\r\n![{3E66EE00-6F73-4C01-A9F5-8E0E8D068F5E}](https://github.com/user-attachments/assets/47906d38-6388-4713-bee0-81ca5f1bc556)\r\n![{04233737-A34E-44DC-BE8C-5F2C6E282C2E}](https://github.com/user-attachments/assets/9abbdd7e-f1dc-4b25-846e-b89e04a73220)\r\n![{325467A8-DEAC-4E39-8E55-BE9D169778C3}](https://github.com/user-attachments/assets/435aa101-8e44-4435-9cd2-86fb56575969)', 'created_at': datetime.datetime(2024, 10, 18, 14, 27, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2429962112, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""@mraunak @Venkat6871 Sorry to bother you, I hope you both enjoyed your weekend. I was wondering if either of you would be able to further assist me with this issue, as I've had no further success on my own trying to fix the problem. Thanks"", 'created_at': datetime.datetime(2024, 10, 22, 18, 25, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434463347, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, sorry for the delayed response, Could you please try using a different Python version, such as 3.12/3.10. Kindly follow these steps:\r\nCreate a virtual environment using Python 3.12/3.10.\r\nJust Install TensorFlow and import TensorFlow.\r\n\r\nIf the import fails, please let us know and share the output of pip list', 'created_at': datetime.datetime(2024, 10, 24, 7, 3, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434658463, 'issue_id': 2576429918, 'author': 'adamrickayzenanvil', 'body': 'I am having the same issue, it may be something to do with the type of processor? Tensorflow works fine on my laptop running an Intel processor, but fails in the exact same files and environments on a machine running an ARM processor.', 'created_at': datetime.datetime(2024, 10, 24, 8, 43, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435379003, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '> Hi @NexusHex, sorry for the delayed response, Could you please try using a different Python version, such as 3.12/3.10. Kindly follow these steps: Create a virtual environment using Python 3.12/3.10. Just Install TensorFlow and import TensorFlow.\r\n> \r\n> If the import fails, please let us know and share the output of pip list\r\n\r\nPython v3.12.7:\r\n![image](https://github.com/user-attachments/assets/66fdd51a-f44e-4bbf-a6df-239e9efae454)\r\n![{F27E7000-5A79-41DA-80D3-5D82A5734CE7}](https://github.com/user-attachments/assets/d4ea0a3f-65cc-41f2-aae3-5d79ea05f582)\r\n\r\nPython v3.10.7:\r\n![{8CECED48-C2B4-4D4E-A224-B05E2CC838D6}](https://github.com/user-attachments/assets/da0cae16-9de8-4e74-875f-26437ff189cd)\r\n![{A026C2B7-2C8A-45F2-B91F-2498307E05C5}](https://github.com/user-attachments/assets/c37efeac-2ff0-4c17-9094-6e081b1a26b6)', 'created_at': datetime.datetime(2024, 10, 24, 14, 0, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435383398, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""> I am having the same issue, it may be something to do with the type of processor? Tensorflow works fine on my laptop running an Intel processor, but fails in the exact same files and environments on a machine running an ARM processor.\r\n\r\nYeah, the package worked totally fine on another machine that I downloaded it on, but it's only my one that seems to have this problem. The thing is this machine was recently reset to factory settings, so I can't think of a way for the computer to be doing anything wrong."", 'created_at': datetime.datetime(2024, 10, 24, 14, 1, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436700175, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '> > Hi @NexusHex, sorry for the delayed response, Could you please try using a different Python version, such as 3.12/3.10. Kindly follow these steps: Create a virtual environment using Python 3.12/3.10. Just Install TensorFlow and import TensorFlow.\r\n> > If the import fails, please let us know and share the output of pip list\r\n> \r\n> Python v3.12.7: ![image](https://private-user-images.githubusercontent.com/140787406/379792812-66fdd51a-f44e-4bbf-a6df-239e9efae454.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk4MjI5MjgsIm5iZiI6MTcyOTgyMjYyOCwicGF0aCI6Ii8xNDA3ODc0MDYvMzc5NzkyODEyLTY2ZmRkNTFhLWY0NGUtNGJiZi1hNmRmLTIzOWU5ZWZhZTQ1NC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDI1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAyNVQwMjE3MDhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mNTgyMzk3NWZlMmM3ZjA4MDI5YWJkMzlhOTQ1NzNiZDE3N2FiNTAwZGM0YTE0ZTc3YTViM2Y4MWIxNDA3ZmM0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.Dg1aFFjm-fZ8QrAVDRsMv7Cf3PLcrQCguteNuQ1YxKc) ![{F27E7000-5A79-41DA-80D3-5D82A5734CE7}](https://private-user-images.githubusercontent.com/140787406/379792922-d4ea0a3f-65cc-41f2-aae3-5d79ea05f582.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk4MjI5MjgsIm5iZiI6MTcyOTgyMjYyOCwicGF0aCI6Ii8xNDA3ODc0MDYvMzc5NzkyOTIyLWQ0ZWEwYTNmLTY1Y2MtNDFmMi1hYWUzLTVkNzllYTA1ZjU4Mi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDI1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAyNVQwMjE3MDhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zNTRjOWIzMmUzZDFmYWUxYzBjNjAzOGRkMzY5Y2I3YzIzNjBlYWYyOWYyODlmOWNmYjk5NDc4Nzg2Njk3ODFiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.VnJj7fBNKYg1x5rLYlJZSV9GWxOujTnoaCgxlwertIs)\r\n> \r\n> Python v3.10.7: ![{8CECED48-C2B4-4D4E-A224-B05E2CC838D6}](https://private-user-images.githubusercontent.com/140787406/379801578-da0cae16-9de8-4e74-875f-26437ff189cd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk4MjI5MjgsIm5iZiI6MTcyOTgyMjYyOCwicGF0aCI6Ii8xNDA3ODc0MDYvMzc5ODAxNTc4LWRhMGNhZTE2LTlkZTgtNGU3NC04NzVmLTI2NDM3ZmYxODljZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDI1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAyNVQwMjE3MDhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mM2JkYWQ0OWQxZmY2YjRjNzAwNmNlYzhiZWRlMzEwOTFiZTllNzEwOTEyYTg3YzRmNGU5NGUxNzAzZjY4MjQxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.Z6dg5Sa19RipMyaQPo2elq4c7CSENOiEjpN8FmpkGYo) ![{A026C2B7-2C8A-45F2-B91F-2498307E05C5}](https://private-user-images.githubusercontent.com/140787406/379801655-c37efeac-2ff0-4c17-9094-6e081b1a26b6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk4MjI5MjgsIm5iZiI6MTcyOTgyMjYyOCwicGF0aCI6Ii8xNDA3ODc0MDYvMzc5ODAxNjU1LWMzN2VmZWFjLTJmZjAtNGMxNy05MDk0LTZlMDgxYjFhMjZiNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDI1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAyNVQwMjE3MDhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jY2U4OGE5YTRlNzM2NDJkNTQ1MjU2OWJhYTI1N2Y2YjNhOGFhYzBlMDRhZjAwZWY3ODQzNzUyNDVjMTM1ZmQ1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.xslxPNGn6Be2fqwBXTnsqyXbSF2U12yA3B66E8H8Adk)\r\n\r\nI forgot to include the pip list outputs, so I reproduced the error and got the pip list outputs:\r\n\r\nPython 3.12.7:\r\n![{4AE054EA-DE87-478D-9163-D34E6E2AC124}](https://github.com/user-attachments/assets/042ba617-035c-4790-bfa2-7267f6f47912)\r\n\r\n\r\nPython 3.10.7:\r\n![{B9E8E4AF-482C-4BC5-9290-401D48A23526}](https://github.com/user-attachments/assets/1b8eeadf-e63c-4572-86e4-2efe939db48a)', 'created_at': datetime.datetime(2024, 10, 25, 2, 46, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436701349, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""At this point personally I believe it is a problem with my machine, since it works on many of the other machines in my house and only this one refuses to let me download it. Perhaps more of my computer's specifications would help you out?"", 'created_at': datetime.datetime(2024, 10, 25, 2, 47, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437199463, 'issue_id': 2576429918, 'author': 'adamrickayzenanvil', 'body': ""> > I am having the same issue, it may be something to do with the type of processor? Tensorflow works fine on my laptop running an Intel processor, but fails in the exact same files and environments on a machine running an ARM processor.\r\n> \r\n> Yeah, the package worked totally fine on another machine that I downloaded it on, but it's only my one that seems to have this problem. The thing is this machine was recently reset to factory settings, so I can't think of a way for the computer to be doing anything wrong.\r\n\r\nWhich processor do you have?"", 'created_at': datetime.datetime(2024, 10, 25, 8, 28, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437831214, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""> > > I am having the same issue, it may be something to do with the type of processor? Tensorflow works fine on my laptop running an Intel processor, but fails in the exact same files and environments on a machine running an ARM processor.\r\n> > \r\n> > \r\n> > Yeah, the package worked totally fine on another machine that I downloaded it on, but it's only my one that seems to have this problem. The thing is this machine was recently reset to factory settings, so I can't think of a way for the computer to be doing anything wrong.\r\n> \r\n> Which processor do you have?\r\n\r\nDevice name\tLAPTOP-C47IURB2\r\nProcessor\tIntel(R) Celeron(R) N4000 CPU @ 1.10GHz   1.10 GHz\r\nInstalled RAM\t4.00 GB (3.83 GB usable)\r\nDevice ID\t64F0A5A2-9229-4105-AE7F-BD6AC370745B\r\nProduct ID\t00356-02181-06055-AAOEM\r\nSystem type\t64-bit operating system, x64-based processor\r\nPen and touch\tNo pen or touch input is available for this display\r\n\r\nThat's what my machine says from the System > About section of my Settings app"", 'created_at': datetime.datetime(2024, 10, 25, 13, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452394841, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""Hey there, are there any new developments in relation to my query? It's been a while since I've heard back from anyone. Thanks."", 'created_at': datetime.datetime(2024, 11, 1, 18, 36, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455797794, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, sorry for the delay, can you please try \r\npip install -U --force-reinstall tensorflow', 'created_at': datetime.datetime(2024, 11, 4, 22, 6, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455840764, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'No worries about the delay, I tried your solution out:\r\n![{A66B1235-4793-42A9-B998-DD5DDDD3FBC4}](https://github.com/user-attachments/assets/f31cfc73-a128-4a63-ae93-5f5b33d25bef)\r\n![{F37765E8-3014-4E68-A452-9971EAE80C22}](https://github.com/user-attachments/assets/ef671a0d-dc47-4007-8a73-f711c38e14ca)\r\n![{A6C767AC-4214-4AB4-B1C7-F32ACB14EBC2}](https://github.com/user-attachments/assets/88efdf22-ddbb-436c-9d9b-a9ede72e7219)\r\nUnfortunately throws the same error.', 'created_at': datetime.datetime(2024, 11, 4, 22, 34, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462586925, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '@mraunak', 'created_at': datetime.datetime(2024, 11, 7, 15, 51, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465847745, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, can you please try \r\n\r\npython -m pip install tensorflow\r\n\r\nand then\r\n![image](https://github.com/user-attachments/assets/cb6fffe3-9e8f-445d-9312-fdbeae2cd767)\r\n\r\nI believe there are multiple Python versions installed. If you have both Python 3.8 and Python 3.10 installed, running python3.8 -m pip install package_name might install a package specifically for Python 3.8. But if you later run python3.10 script.py, Python 3.10 might not recognize that package because it’s installed in the environment for Python 3.8', 'created_at': datetime.datetime(2024, 11, 8, 22, 28, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466008136, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Hi @mraunak, I only have Python 3.11.9 installed, and I made sure to delete the empty folders that were there for Python 3.10 and  3.12.\r\n![{9D7168BC-CF0B-4FB2-80FC-09E6EA495DF9}](https://github.com/user-attachments/assets/e49d2b5d-caeb-434b-b54c-5628dbd148ec)\r\n![{2094CACA-8969-49E1-A391-2ABC6A14101C}](https://github.com/user-attachments/assets/526f6f74-bb2f-4c7a-ad1e-00d07a31ddaa)\r\n![{790E3D93-87BE-4A97-AA25-D3A1E9F32003}](https://github.com/user-attachments/assets/26f8068a-824c-465f-a2fa-d5ef7e0f934e)\r\nSame result as every other attempt, sorry.', 'created_at': datetime.datetime(2024, 11, 9, 2, 59, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471533403, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, Since we’re unable to reproduce the issue on our end, troubleshooting is a bit challenging. Would it be possible for you to share the code on a cloud platform or somewhere accessible to us? This way, we can directly investigate and work on a solution', 'created_at': datetime.datetime(2024, 11, 12, 20, 36, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472223731, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""@mraunak I don't understand what you mean by 'code' since the command is generic for installing the package. I have a Google Drive where I have copied over all the contents of my version of Python that I have installed. Is that sufficient or do you need anything else specific?\r\n![{AF213175-A175-40CF-BA30-C96945B24CA2}](https://github.com/user-attachments/assets/6946307b-4a05-4e1a-b210-3b060c160247)"", 'created_at': datetime.datetime(2024, 11, 13, 2, 25, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482317814, 'issue_id': 2576429918, 'author': 'shangerxin', 'body': 'Hi @NexusHex , Is it possible to package your venv into an zip file and share with us? I cannot reproduce the issue on my end too. I have tried 3.10, 3.12\r\n\r\n![image](https://github.com/user-attachments/assets/74c3c76b-6954-4d38-a8aa-f9da5faa4349)\r\n![image](https://github.com/user-attachments/assets/ff1253eb-da26-4c9a-a61b-2e3a9bfedd19)', 'created_at': datetime.datetime(2024, 11, 18, 8, 55, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485767663, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""@shangerxin Sure, no worries. I have put the file in the same google drive (Github won't let me upload it straight to the thread since the file is too big) as I have posted on this thread already. Do you have an email address that you would like me to share the zip file to? @mraunak I could share it too you too if you'd like"", 'created_at': datetime.datetime(2024, 11, 19, 13, 46, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486422018, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Sorry NexusHex for the delayed response, I was traveling. yeah sure please share the file with me as well.', 'created_at': datetime.datetime(2024, 11, 19, 18, 13, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486465620, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'No worries mraunak, is there an email address I can share the drive folder to?', 'created_at': datetime.datetime(2024, 11, 19, 18, 36, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487139609, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi NexusHex, you can share the file to email mayankkumarraunak85@gmail.com', 'created_at': datetime.datetime(2024, 11, 20, 1, 42, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487143035, 'issue_id': 2576429918, 'author': 'shangerxin', 'body': ""> @shangerxin Sure, no worries. I have put the file in the same google drive (Github won't let me upload it straight to the thread since the file is too big) as I have posted on this thread already. Do you have an email address that you would like me to share the zip file to? @mraunak I could share it too you too if you'd like\r\n\r\n@NexusHex  Could you share the google drive link to us again or send the files to the Email shared by Mayank? Thank you."", 'created_at': datetime.datetime(2024, 11, 20, 1, 45, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488713300, 'issue_id': 2576429918, 'author': 'mraunak', 'body': ""Hi @NexusHex, can you please also share the output log from the command below?\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # Enables detailed TensorFlow logs\r\nimport tensorflow as tf"", 'created_at': datetime.datetime(2024, 11, 20, 14, 21, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491351293, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '@mraunak @shangerxin I have shared the zip file to the email mraunak has provided. Sorry for the delay.', 'created_at': datetime.datetime(2024, 11, 21, 14, 23, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491359333, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Also @mraunak here is the output log for the command you gave:\r\n![{44182794-0DD3-485D-9F44-9420015D07D2}](https://github.com/user-attachments/assets/dcbab5b5-38b6-4d39-a625-c88504ce0dd1)', 'created_at': datetime.datetime(2024, 11, 21, 14, 26, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504439419, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '@mraunak @shangerxin', 'created_at': datetime.datetime(2024, 11, 27, 17, 36, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517737964, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex,\r\nApologies for the delay; I was unable to reproduce the error on my end.\r\n\r\nCould you please try building the TensorFlow wheel using the steps outlined in this guide: [Building from Source on Windows](https://www.tensorflow.org/install/source_windows)? Once the wheel is built, try installing it and let me know if you encounter any issues during the process.\r\n\r\nIn the meantime, we will continue exploring the problem and will update you if we find any alternative solutions.', 'created_at': datetime.datetime(2024, 12, 4, 15, 17, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528920302, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""Hi @mraunak,\r\nI followed the instructions given to me and I have built the wheel, but when I try to create the Tensorflow package-builder, Bazel throws this error (this is after doing the command at this step - https://www.tensorflow.org/install/source_windows#build_the_package-builder):\r\n![image](https://github.com/user-attachments/assets/e864e1fb-1bca-4ca7-a1a9-0d97f4c2f8bb)\r\n\r\nI have done my best to follow all the instructions properly, and I have all the required things installed (I didn't do any of the optional steps). Is there another step I have to make to create the 'WORKSPACE file' it's talking about before I make the package-builder?"", 'created_at': datetime.datetime(2024, 12, 9, 17, 56, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541690253, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, \r\nyou need to navigate to the TensorFlow directory and execute the Bazel command\r\n\r\n![image](https://github.com/user-attachments/assets/d56082af-cfeb-4790-8b1b-9762bd74eb38)', 'created_at': datetime.datetime(2024, 12, 13, 15, 22, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541775545, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Also run the following commands if you get any error and check the Path below\r\n\r\nset BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\r\nset BAZEL_VS=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\r\nset BAZEL_VC=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\r\nset BAZEL_VC_FULL_VERSION=14.41.34120\r\nset BAZEL_LLVM=C:\\Program Files\\LLVM\r\nset PATH=C:\\Tools\\bazel;C:\\Python311\\Scripts;C:\\msys64;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Git\\usr\\bin;C:\\msys64\\usr\\bin;C:\\Windows\\SysWOW64;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Java\\jre1.8.0_251\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files (x86)\\PowerShell\\6\\;C:\\Windows\\system32\\config\\systemprofile\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files\\LLVM\\bin;%PATH%\r\nset PYTHON_BIN_PATH=C:\\Python311\\python.exe\r\nset PYTHON_LIB_PATH=C:\\Python311\\Lib\r\nset PYTHON_DIRECTORY=C:\\Python311\\Scripts\r\n\r\nnote: you will find BAZEL_VC_FULL_Version here: C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC', 'created_at': datetime.datetime(2024, 12, 13, 16, 7, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544331163, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Hey @mraunak,\r\n\r\nI have tried the suggestions you have given to me, and these were the results:\r\n__________________________\r\nFirst attempt -\r\n![image](https://github.com/user-attachments/assets/ecac9e68-bdc7-4a7a-be56-8d753d310adf)\r\n![{35763E9C-57FD-4134-BBED-F3A5159328CB}](https://github.com/user-attachments/assets/6c756361-10a9-4326-86b3-19f962a25b2f)\r\n![{A80FE46A-8F1A-4A94-998B-D40D0861B192}](https://github.com/user-attachments/assets/f755ddd2-25cd-4cca-a681-d2ac2cb9c70f)\r\n__________________________\r\nSecond attempt (I did all the commands that you gave in case an error happened) -\r\n![{DEC14DA7-4769-44B6-9C0A-0EA6A14EE0B3}](https://github.com/user-attachments/assets/e52d48ab-f13c-4214-8508-155195720ca6)\r\n![{524A05BA-F7DC-46AC-855F-FFD7D7EC4EAA}](https://github.com/user-attachments/assets/63a071b2-6962-47f4-9b52-e8328cbd2bc8)\r\n![{F0414D13-A08B-4876-9D2F-75252360E680}](https://github.com/user-attachments/assets/825a8805-ea99-41a0-9900-1bd8290d6e44)\r\n__________________________\r\nThird attempt (I tried the command you used in your venv example when you toldme to navigate to the tensorflow directory) -\r\n![{E2DD9151-A37A-447A-9C4D-4B26D4032106}](https://github.com/user-attachments/assets/5e722aec-2008-499c-873d-abb1f9482366)\r\n![{EA2C3464-7E85-480A-805B-FD22ACE6F59B}](https://github.com/user-attachments/assets/b3558a8d-f877-43db-9e1b-56a2fd6c3699)', 'created_at': datetime.datetime(2024, 12, 16, 2, 4, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546246641, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex,\r\nUse Python 3.11 to create a virtual environment:\r\npython3.11 -m venv tensorflow_build_env\r\ntensorflow_build_env\\Scripts\\activate\r\n\r\nExecute all the provided setup (set) commands and build commands within the activated virtual environment.', 'created_at': datetime.datetime(2024, 12, 16, 17, 37, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547256528, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Hi @mraunak, same errors after running Bazel in the venv:\r\n\r\n![{E20B48FD-2C97-4062-82D2-CF687ED04130}](https://github.com/user-attachments/assets/cc230f28-defd-4593-81de-74c388e5d155)\r\n![{FF3EBC32-3539-4C6A-8980-60B7A9AAEB9B}](https://github.com/user-attachments/assets/d626161f-7b90-4fa8-949d-c8fc13767173)\r\n![{1EFAD637-C46D-4A4A-9A11-5AB6C2AD602B}](https://github.com/user-attachments/assets/e1dfa875-0567-44f2-be82-f759f126b2e5)\r\n![{E2CACA83-18ED-4EBA-9085-1EF2E6D637BB}](https://github.com/user-attachments/assets/e0b2a087-adf3-4cfa-a90e-597a2c8cef37)', 'created_at': datetime.datetime(2024, 12, 17, 0, 39, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549199303, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Hi @NexusHex, \r\n\r\nVerify the Path Configuration--Check that the Path is set correctly and points to the intended location.\r\nValidate the Environment Path\r\n\r\nConfirm that the environment path is correctly configured and accessible.\r\n\r\nRun the Commands Inside the TensorFlow Repository\r\n\r\nNavigate to the TensorFlow repository and execute the following commands:\r\nset  \r\npython configure.py  \r\n\r\n![image](https://github.com/user-attachments/assets/e2f0c10b-1620-495d-b0a9-5c983a9fc5b6)', 'created_at': datetime.datetime(2024, 12, 17, 17, 56, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549249753, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': '@mraunak I ran the commands you listed above:\r\n![{22A33DFC-F6D9-42C3-8B65-995DC54AC42B}](https://github.com/user-attachments/assets/7a9b3a6f-a793-4ae8-bb0f-6b48b9bd5685)\r\n![{0821A030-DC1E-4725-931D-7E264A842EA4}](https://github.com/user-attachments/assets/9f8befa0-5102-401b-b616-348b892658bc)\r\n![{AE36011C-B807-4238-AE4A-5FC9A94EA403}](https://github.com/user-attachments/assets/cfb04b17-0750-4508-a61f-b9f826d21577)\r\n\r\nThese are my paths in the user variables:\r\n![{C0277318-451F-432D-BA6F-936AD62D8DBD}](https://github.com/user-attachments/assets/e9161b10-3653-4c47-9c55-f05dcc664df5)\r\nAnd these are the ones in the system variables:\r\n![{04BE2BF0-D014-44DC-B4F3-0E31D9D061B6}](https://github.com/user-attachments/assets/6e19b987-6dec-4a9b-b14f-bc8c75c113f8)', 'created_at': datetime.datetime(2024, 12, 17, 18, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549637580, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'your output of set command does not contain\r\n![image](https://github.com/user-attachments/assets/ea1a1363-1a39-432a-84ed-3cdf0288a865)\r\n\r\nSince Google officially uses bazel version 6.5.0. Please install the same version\r\nhttps://github.com/tensorflow/tensorflow/blob/master/.bazelversion', 'created_at': datetime.datetime(2024, 12, 17, 21, 10, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549649721, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'add python path in environment variables as well\r\nset PYTHON_BIN_PATH=C:\\Python311\\python.exe or C:\\users\\Khiza\\tensorflow_build_env\\Scripts\\python.exe\r\nset PYTHON_LIB_PATH=C:\\Python311\\Lib or C:\\users\\Khiza\\tensorflow_build_env\\LIB\r\nset PYTHON_DIRECTORY=C:\\Python311\\Scripts', 'created_at': datetime.datetime(2024, 12, 17, 21, 17, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551542078, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Hey @mraunak, I donwloaded the correct version of Bazel and deleted the other one:\r\n![{422CF75B-958E-458D-B0BC-F28BD1A1D512}](https://github.com/user-attachments/assets/5fab4713-30b3-48f5-bb75-9ecdba05af55)\r\n\r\nAnd I did all the same things once again:\r\n![{EB84C5A5-CE7E-4EEF-A63C-EF7E191A9A34}](https://github.com/user-attachments/assets/d4986b2d-bfa2-4f90-b4e1-38a188f32190)\r\n\r\nI applied the following new environment variables:\r\n![{DFAC5FF2-0EB2-49F9-B27E-50E75E9F9064}](https://github.com/user-attachments/assets/21b85774-2051-4514-8399-678a53cf2ed6)\r\n![{DFEC5266-09BC-4240-BBC9-8F19B9765F54}](https://github.com/user-attachments/assets/0c438aba-5fe6-481a-8c13-e47e785ab687)\r\nBut I ran into an issue while I was setting the BAZEL_VC variable. It seems like I only have the BuildTools for VS Redistributable 2022, so how would I go about installing the VC portion of the package?\r\n![{3D7FCE71-2621-464E-BB21-35668F1DEE94}](https://github.com/user-attachments/assets/b0fa858b-a922-46c9-8bb7-0835be700f03)', 'created_at': datetime.datetime(2024, 12, 18, 14, 57, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551797980, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Try to install Visual Studio Community 2022\r\n\r\nhttps://visualstudio.microsoft.com/vs/ \r\n\r\nor try the commands similar to the commands below\r\n![image](https://github.com/user-attachments/assets/8ed42907-88db-4b74-b8b4-af1a7122419e)', 'created_at': datetime.datetime(2024, 12, 18, 16, 41, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551881080, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""For Visual Studio Community 2022, I'm not sure what I should download alongside the program, unless I should just download it without any addons.\r\n![image](https://github.com/user-attachments/assets/5c328140-bf0d-479e-9a40-65797f544259)"", 'created_at': datetime.datetime(2024, 12, 18, 17, 20, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551942703, 'issue_id': 2576429918, 'author': 'mihaimaruseac', 'body': ""Duplicate of #19584\r\n\r\nPlease, don't post screenshots of errors. Instead, post the code in proper markdown format (3 backticks). This way, the errors are searchable and also more accessible.\r\n\r\n@mraunak please don't post screenshots with answer code. Post code in proper markdown blocks.\r\n\r\nBoth, please do a search before. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets."", 'created_at': datetime.datetime(2024, 12, 18, 17, 53, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551942755, 'issue_id': 2576429918, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77387"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77387"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 54, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551953559, 'issue_id': 2576429918, 'author': 'mraunak', 'body': 'Thank you very much @mihaimaruseac, yeah sure, I will take care of the points mentioned', 'created_at': datetime.datetime(2024, 12, 18, 18, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552665175, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': 'Thank you for the link to the original issue @mihaimaruseac. Sorry about the shortcomings in mentioning my errors with importing the library, and I will keep them in mind for the future. Have a great day, the both of you, and thank you @mraunak for guiding me through solving this problem to the best of your ability.', 'created_at': datetime.datetime(2024, 12, 19, 2, 50, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553980274, 'issue_id': 2576429918, 'author': 'mihaimaruseac', 'body': ""Thank you both, and indeed, lots of thanks to @mraunak for helping with windows issues.\r\n\r\nSadly this old CPU issue has been with TF ever before ~2019. It's not something we can reasonably fix as a lot of the code assumes AVX extensions are available. TF has some checker code to check if the CPU is modern enough but even that code gets compiled with a requirement for AVX so it fails :("", 'created_at': datetime.datetime(2024, 12, 19, 13, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554285887, 'issue_id': 2576429918, 'author': 'adamrickayzenanvil', 'body': 'I have this issue too but my CPU is extremely modern (Snapdragon(R) X Elite - X1E80100, ARM64 based). Is it likely there will be an update at some point which works with this CPU?', 'created_at': datetime.datetime(2024, 12, 19, 14, 25, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554296764, 'issue_id': 2576429918, 'author': 'mihaimaruseac', 'body': 'Oh, in your case the issue is that TF ARM build is too new. For this, can you open a new issue please?\r\n\r\nIt would be great if you can also identify which DLL does not load and why by looking at the system logs. Then, please post that in the new issue.', 'created_at': datetime.datetime(2024, 12, 19, 14, 27, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554359818, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""> Thank you both, and indeed, lots of thanks to @mraunak for helping with windows issues.\r\n> \r\n> Sadly this old CPU issue has been with TF ever before ~2019. It's not something we can reasonably fix as a lot of the code assumes AVX extensions are available. TF has some checker code to check if the CPU is modern enough but even that code gets compiled with a requirement for AVX so it fails :(\r\n\r\nAh, so that's the problem. The thing is that Tensorflow was running perfectly on all my other machines, so I guess that explains why the problem is only affecting my computer. Not sure how old my CPU is though; it is an Intel(R) Celeron(R) N4000 CPU @ 1.10GHz. Either way, it was interesting finding the issue since this was the first time that the library outright refused to be imported."", 'created_at': datetime.datetime(2024, 12, 19, 14, 39, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554625302, 'issue_id': 2576429918, 'author': 'mihaimaruseac', 'body': '> Intel(R) Celeron(R) N4000 CPU @ 1.10GHz.\r\n\r\nChecked on https://www.intel.com/content/www/us/en/products/sku/128988/intel-celeron-processor-n4000-4m-cache-up-to-2-60-ghz/specifications.html, and it does have AVX but stops short of AVX2 which is also needed.', 'created_at': datetime.datetime(2024, 12, 19, 15, 28, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554712778, 'issue_id': 2576429918, 'author': 'NexusHex', 'body': ""@mihaimaruseac Makes sense then. I'll check an alternative sometime soon from the issue you linked to me and see if that works better. Thanks :)"", 'created_at': datetime.datetime(2024, 12, 19, 15, 44, 44, tzinfo=datetime.timezone.utc)}]","mraunak on (2024-10-10 21:53:08 UTC): Hi @NexusHex can you please try the steps below and let us know if the issue persists
![image](https://github.com/user-attachments/assets/69f6f091-0f9d-4f06-a5a4-b6413a24f343)

Venkat6871 (Assginee) on (2024-10-11 10:48:10 UTC): Hi **@NexusHex** ,
Could you please try using the latest version of TensorFlow 2.18.0-rc1? I am providing the [documentation](https://github.com/tensorflow/tensorflow/releases) for the latest version here. Please let us know if the issue still persists.
Thank you!

NexusHex (Issue Creator) on (2024-10-15 03:53:49 UTC): Hi mraunak, sorry for the long wait. I hope your Thanksgiving weekend went well. I attempted to do the instructions as listed (the paths for the venv weren't the same, so I created the venv from my user directory rather than from C:\Python311\python.exe) but the venv still doesn't recognize Tensorflow as a module:
![{8B66024A-FFFC-4C4C-8368-B59F1E62BD5C}](https://github.com/user-attachments/assets/8879129b-f63c-4872-9a00-18fe6144c3b6)

I called pip list to see if there was anything wrong, and saw that tf-nightly was still there from my first attempt importing Tensorflow, so I proceeded to delete all modules related to tf-nightly and try again (using the specific path this time):
![{EED64C05-D774-4FE1-9A32-95A2E91397D9}](https://github.com/user-attachments/assets/2b6c2ee6-b337-4104-b08e-42324c1926e4)
![{680A00CD-44C1-4C46-B07B-D3BE570622DE}](https://github.com/user-attachments/assets/b575b6af-fc21-4af1-b411-396d73e10817)
![{DA414BCA-8C69-4A65-9CEA-0455CD7B1F46}](https://github.com/user-attachments/assets/95c48185-01dc-40f7-b12f-bb35a4c14d18)
![{37E81077-25F4-4230-AD60-FB5B8F4D166B}](https://github.com/user-attachments/assets/a7ee6e99-8446-4866-952c-997e951b0c1e)
![{C8F58779-12F9-432C-8426-DFD94EE9C0A6}](https://github.com/user-attachments/assets/70332ca4-7f59-4b1a-9b50-8f7ba69d0138)
Unfortunately, none of this worked... I hope I followed all the instructions correctly, otherwise I'll wait for your reply.
Thanks :)

NexusHex (Issue Creator) on (2024-10-15 04:08:31 UTC): After attempting mraunak's solution, I tried yours in the same venv as the one I used before, here are the results:
![{A999FB69-118D-44F1-9C03-A3FF7D870CEB}](https://github.com/user-attachments/assets/bd25d7d5-2ecb-4009-b526-17909740e8e0)
![{1B629FF2-8C66-4F50-837D-0BA612F15290}](https://github.com/user-attachments/assets/45b8e6ae-f8f0-4c62-9686-7865c7f1db53)
![{8C163423-71B8-4E5A-BE55-B828BDBF50DD}](https://github.com/user-attachments/assets/7ca9d106-952d-4f0c-9e4f-c81c50a8baf4)
![{378CC51E-0568-45AA-BA7A-2A925A54E4EA}](https://github.com/user-attachments/assets/283781c9-1f91-4a01-a3b7-add22011dbbd)
Another dead end, but I'll wait for your response along with mraunak's too!

mraunak on (2024-10-15 07:13:43 UTC): Hi @NexusHex, Could you please download and install the Python package in a new location, somewhere on the C:\ drive? I'm a bit puzzled since it's working fine on my end without needing any pip installation. I tried with the exact Python version that you have. 

![image](https://github.com/user-attachments/assets/d6cc5386-3e0e-4aa2-acd7-7e2ab57b295d)

NexusHex (Issue Creator) on (2024-10-15 15:46:40 UTC): @mraunak I ran the code as shown from your example, and it came up with the same ModuleNotFoundError.
From that point I decided to trace back all my steps, deleting all my temp files, any duplicate files, all packages linked to tensorflow, redownloading Python 3.11.9 and redownloading tf 2.18.0-rc1:
![{C1022FDF-12F0-4222-B0F8-A807BF4FC6C1}](https://github.com/user-attachments/assets/cb8da2e4-9d6a-47a0-a464-b285ff2ce492)

After I redownloaded everything I attempted to import the package as you normally would in the terminal:
![{4C776B0B-BD69-47FB-BFA6-3BEC161C2070}](https://github.com/user-attachments/assets/a63f2762-2fe7-4040-aca8-178563179a34)

The same error happened again, so I tried it in a venv once again...
![{09FDEC8E-4656-47CC-862D-574DD15037BA}](https://github.com/user-attachments/assets/2f031a86-8280-4df1-b410-8692424bd1d1)

This is pretty puzzling for me too, since I've never really had issues go this far with any packages.

mraunak on (2024-10-15 20:38:12 UTC): Hi @NexusHex, can you please try to create a fresh virtual env and pip install the following packages with the same versions. This should work
![image](https://github.com/user-attachments/assets/9ee0603e-997b-46f1-b1cf-bf41fc423fb2)

NexusHex (Issue Creator) on (2024-10-17 13:10:42 UTC): Hi @mraunak, I did everything as per your instructions and ran 'import tensorflow as tf':
![{9D889DDC-2214-4F8E-B4B3-2331344741E7}](https://github.com/user-attachments/assets/225a5752-6e9c-4e65-b078-8099f0eb4b02)
![{2BDD1622-9F6E-4486-BE23-672CB4449C5B}](https://github.com/user-attachments/assets/9dd1a6f0-24e5-491a-8815-6823d04d638a)
![{47368B12-C7D2-4B86-9598-9EDE5B71327F}](https://github.com/user-attachments/assets/6191fa5e-b3a5-46a5-93d5-87caeaabae9e)
![{DB60FBF4-E571-4272-97AD-3165A8B936FC}](https://github.com/user-attachments/assets/5a7f23ba-667c-4aa2-a92f-ad36e610b5db)
![{76C18621-568C-4FDC-9555-EAA7A02C5AF1}](https://github.com/user-attachments/assets/e50b028b-27bb-437c-8f03-b4f77b084495)
![{D77584B1-6347-4F9B-AD43-769888FDB9EE}](https://github.com/user-attachments/assets/440d9634-5cd2-4967-80f1-840c9f0cb13e)
![{0445604C-75F0-4868-BF72-D7A8DCE75620}](https://github.com/user-attachments/assets/ccb5bbfd-e1c6-4a2e-aaee-652a33a50b8c)
![{749F0844-B486-4F31-9BC1-C823ABD7C151}](https://github.com/user-attachments/assets/586f051b-c3d2-4295-bc01-b8a3f055a1e5)
![image](https://github.com/user-attachments/assets/4b83db69-4c98-43f1-bf2c-dba04559d2e4)
![{9E736A75-DAB2-43B3-B8B9-6F097EEEC53A}](https://github.com/user-attachments/assets/0461c3a8-0c8e-44ab-982e-7128e193c26c)

After installing all of this, I checked the pip list and uninstalled tensorflow-io-gcs-filesystem:
![{555B2D88-C4F8-4E60-8268-7B7242BCDD1E}](https://github.com/user-attachments/assets/9253d5d6-838c-4c88-9b16-5b099a729c8f)

Now that my pip list is the exact same as yours, I ran the code as normal and:
![{94F90468-9805-4953-8E0E-A141DD4DEFB9}](https://github.com/user-attachments/assets/9816f7cf-5834-4e1c-85fc-17b6fa672db0)

mraunak on (2024-10-18 02:41:18 UTC): Yeah, I was able to replicate the issue on my end on a different Windows machine. I just did pip install tensorflow and it fixed the issue. sorry, you need to have tensorflow-io-gcs-filesystem. 

![image](https://github.com/user-attachments/assets/2b7bacb4-4552-427a-8b28-f6815f98f965)
![image](https://github.com/user-attachments/assets/047e2732-fcd6-41fc-ac76-3e3d9adbd740)

NexusHex (Issue Creator) on (2024-10-18 14:27:22 UTC): Same result
![{BDF59D90-3BD0-4F6B-8AE1-EADC510CD980}](https://github.com/user-attachments/assets/afd0e17f-829f-415e-b91b-9e7dff3bdc5a)
![{3E66EE00-6F73-4C01-A9F5-8E0E8D068F5E}](https://github.com/user-attachments/assets/47906d38-6388-4713-bee0-81ca5f1bc556)
![{04233737-A34E-44DC-BE8C-5F2C6E282C2E}](https://github.com/user-attachments/assets/9abbdd7e-f1dc-4b25-846e-b89e04a73220)
![{325467A8-DEAC-4E39-8E55-BE9D169778C3}](https://github.com/user-attachments/assets/435aa101-8e44-4435-9cd2-86fb56575969)

NexusHex (Issue Creator) on (2024-10-22 18:25:19 UTC): @mraunak @Venkat6871 Sorry to bother you, I hope you both enjoyed your weekend. I was wondering if either of you would be able to further assist me with this issue, as I've had no further success on my own trying to fix the problem. Thanks

mraunak on (2024-10-24 07:03:43 UTC): Hi @NexusHex, sorry for the delayed response, Could you please try using a different Python version, such as 3.12/3.10. Kindly follow these steps:
Create a virtual environment using Python 3.12/3.10.
Just Install TensorFlow and import TensorFlow.

If the import fails, please let us know and share the output of pip list

adamrickayzenanvil on (2024-10-24 08:43:06 UTC): I am having the same issue, it may be something to do with the type of processor? Tensorflow works fine on my laptop running an Intel processor, but fails in the exact same files and environments on a machine running an ARM processor.

NexusHex (Issue Creator) on (2024-10-24 14:00:03 UTC): Python v3.12.7:
![image](https://github.com/user-attachments/assets/66fdd51a-f44e-4bbf-a6df-239e9efae454)
![{F27E7000-5A79-41DA-80D3-5D82A5734CE7}](https://github.com/user-attachments/assets/d4ea0a3f-65cc-41f2-aae3-5d79ea05f582)

Python v3.10.7:
![{8CECED48-C2B4-4D4E-A224-B05E2CC838D6}](https://github.com/user-attachments/assets/da0cae16-9de8-4e74-875f-26437ff189cd)
![{A026C2B7-2C8A-45F2-B91F-2498307E05C5}](https://github.com/user-attachments/assets/c37efeac-2ff0-4c17-9094-6e081b1a26b6)

NexusHex (Issue Creator) on (2024-10-24 14:01:41 UTC): Yeah, the package worked totally fine on another machine that I downloaded it on, but it's only my one that seems to have this problem. The thing is this machine was recently reset to factory settings, so I can't think of a way for the computer to be doing anything wrong.

NexusHex (Issue Creator) on (2024-10-25 02:46:22 UTC): I forgot to include the pip list outputs, so I reproduced the error and got the pip list outputs:

Python 3.12.7:
![{4AE054EA-DE87-478D-9163-D34E6E2AC124}](https://github.com/user-attachments/assets/042ba617-035c-4790-bfa2-7267f6f47912)


Python 3.10.7:
![{B9E8E4AF-482C-4BC5-9290-401D48A23526}](https://github.com/user-attachments/assets/1b8eeadf-e63c-4572-86e4-2efe939db48a)

NexusHex (Issue Creator) on (2024-10-25 02:47:49 UTC): At this point personally I believe it is a problem with my machine, since it works on many of the other machines in my house and only this one refuses to let me download it. Perhaps more of my computer's specifications would help you out?

adamrickayzenanvil on (2024-10-25 08:28:14 UTC): Which processor do you have?

NexusHex (Issue Creator) on (2024-10-25 13:48:00 UTC): Device name	LAPTOP-C47IURB2
Processor	Intel(R) Celeron(R) N4000 CPU @ 1.10GHz   1.10 GHz
Installed RAM	4.00 GB (3.83 GB usable)
Device ID	64F0A5A2-9229-4105-AE7F-BD6AC370745B
Product ID	00356-02181-06055-AAOEM
System type	64-bit operating system, x64-based processor
Pen and touch	No pen or touch input is available for this display

That's what my machine says from the System > About section of my Settings app

NexusHex (Issue Creator) on (2024-11-01 18:36:29 UTC): Hey there, are there any new developments in relation to my query? It's been a while since I've heard back from anyone. Thanks.

mraunak on (2024-11-04 22:06:46 UTC): Hi @NexusHex, sorry for the delay, can you please try 
pip install -U --force-reinstall tensorflow

NexusHex (Issue Creator) on (2024-11-04 22:34:51 UTC): No worries about the delay, I tried your solution out:
![{A66B1235-4793-42A9-B998-DD5DDDD3FBC4}](https://github.com/user-attachments/assets/f31cfc73-a128-4a63-ae93-5f5b33d25bef)
![{F37765E8-3014-4E68-A452-9971EAE80C22}](https://github.com/user-attachments/assets/ef671a0d-dc47-4007-8a73-f711c38e14ca)
![{A6C767AC-4214-4AB4-B1C7-F32ACB14EBC2}](https://github.com/user-attachments/assets/88efdf22-ddbb-436c-9d9b-a9ede72e7219)
Unfortunately throws the same error.

NexusHex (Issue Creator) on (2024-11-07 15:51:54 UTC): @mraunak

mraunak on (2024-11-08 22:28:16 UTC): Hi @NexusHex, can you please try 

python -m pip install tensorflow

and then
![image](https://github.com/user-attachments/assets/cb6fffe3-9e8f-445d-9312-fdbeae2cd767)

I believe there are multiple Python versions installed. If you have both Python 3.8 and Python 3.10 installed, running python3.8 -m pip install package_name might install a package specifically for Python 3.8. But if you later run python3.10 script.py, Python 3.10 might not recognize that package because it’s installed in the environment for Python 3.8

NexusHex (Issue Creator) on (2024-11-09 02:59:33 UTC): Hi @mraunak, I only have Python 3.11.9 installed, and I made sure to delete the empty folders that were there for Python 3.10 and  3.12.
![{9D7168BC-CF0B-4FB2-80FC-09E6EA495DF9}](https://github.com/user-attachments/assets/e49d2b5d-caeb-434b-b54c-5628dbd148ec)
![{2094CACA-8969-49E1-A391-2ABC6A14101C}](https://github.com/user-attachments/assets/526f6f74-bb2f-4c7a-ad1e-00d07a31ddaa)
![{790E3D93-87BE-4A97-AA25-D3A1E9F32003}](https://github.com/user-attachments/assets/26f8068a-824c-465f-a2fa-d5ef7e0f934e)
Same result as every other attempt, sorry.

mraunak on (2024-11-12 20:36:48 UTC): Hi @NexusHex, Since we’re unable to reproduce the issue on our end, troubleshooting is a bit challenging. Would it be possible for you to share the code on a cloud platform or somewhere accessible to us? This way, we can directly investigate and work on a solution

NexusHex (Issue Creator) on (2024-11-13 02:25:47 UTC): @mraunak I don't understand what you mean by 'code' since the command is generic for installing the package. I have a Google Drive where I have copied over all the contents of my version of Python that I have installed. Is that sufficient or do you need anything else specific?
![{AF213175-A175-40CF-BA30-C96945B24CA2}](https://github.com/user-attachments/assets/6946307b-4a05-4e1a-b210-3b060c160247)

shangerxin on (2024-11-18 08:55:03 UTC): Hi @NexusHex , Is it possible to package your venv into an zip file and share with us? I cannot reproduce the issue on my end too. I have tried 3.10, 3.12

![image](https://github.com/user-attachments/assets/74c3c76b-6954-4d38-a8aa-f9da5faa4349)
![image](https://github.com/user-attachments/assets/ff1253eb-da26-4c9a-a61b-2e3a9bfedd19)

NexusHex (Issue Creator) on (2024-11-19 13:46:14 UTC): @shangerxin Sure, no worries. I have put the file in the same google drive (Github won't let me upload it straight to the thread since the file is too big) as I have posted on this thread already. Do you have an email address that you would like me to share the zip file to? @mraunak I could share it too you too if you'd like

mraunak on (2024-11-19 18:13:46 UTC): Sorry NexusHex for the delayed response, I was traveling. yeah sure please share the file with me as well.

NexusHex (Issue Creator) on (2024-11-19 18:36:41 UTC): No worries mraunak, is there an email address I can share the drive folder to?

mraunak on (2024-11-20 01:42:28 UTC): Hi NexusHex, you can share the file to email mayankkumarraunak85@gmail.com

shangerxin on (2024-11-20 01:45:55 UTC): @NexusHex  Could you share the google drive link to us again or send the files to the Email shared by Mayank? Thank you.

mraunak on (2024-11-20 14:21:17 UTC): Hi @NexusHex, can you please also share the output log from the command below?

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # Enables detailed TensorFlow logs
import tensorflow as tf

NexusHex (Issue Creator) on (2024-11-21 14:23:21 UTC): @mraunak @shangerxin I have shared the zip file to the email mraunak has provided. Sorry for the delay.

NexusHex (Issue Creator) on (2024-11-21 14:26:24 UTC): Also @mraunak here is the output log for the command you gave:
![{44182794-0DD3-485D-9F44-9420015D07D2}](https://github.com/user-attachments/assets/dcbab5b5-38b6-4d39-a625-c88504ce0dd1)

NexusHex (Issue Creator) on (2024-11-27 17:36:59 UTC): @mraunak @shangerxin

mraunak on (2024-12-04 15:17:20 UTC): Hi @NexusHex,
Apologies for the delay; I was unable to reproduce the error on my end.

Could you please try building the TensorFlow wheel using the steps outlined in this guide: [Building from Source on Windows](https://www.tensorflow.org/install/source_windows)? Once the wheel is built, try installing it and let me know if you encounter any issues during the process.

In the meantime, we will continue exploring the problem and will update you if we find any alternative solutions.

NexusHex (Issue Creator) on (2024-12-09 17:56:15 UTC): Hi @mraunak,
I followed the instructions given to me and I have built the wheel, but when I try to create the Tensorflow package-builder, Bazel throws this error (this is after doing the command at this step - https://www.tensorflow.org/install/source_windows#build_the_package-builder):
![image](https://github.com/user-attachments/assets/e864e1fb-1bca-4ca7-a1a9-0d97f4c2f8bb)

I have done my best to follow all the instructions properly, and I have all the required things installed (I didn't do any of the optional steps). Is there another step I have to make to create the 'WORKSPACE file' it's talking about before I make the package-builder?

mraunak on (2024-12-13 15:22:55 UTC): Hi @NexusHex, 
you need to navigate to the TensorFlow directory and execute the Bazel command

![image](https://github.com/user-attachments/assets/d56082af-cfeb-4790-8b1b-9762bd74eb38)

mraunak on (2024-12-13 16:07:22 UTC): Also run the following commands if you get any error and check the Path below

set BAZEL_SH=C:\msys64\usr\bin\bash.exe
set BAZEL_VS=C:\Program Files\Microsoft Visual Studio\2022\Community
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Community\VC
set BAZEL_VC_FULL_VERSION=14.41.34120
set BAZEL_LLVM=C:\Program Files\LLVM
set PATH=C:\Tools\bazel;C:\Python311\Scripts;C:\msys64;C:\Program Files\Git\cmd;C:\Program Files\Git\usr\bin;C:\msys64\usr\bin;C:\Windows\SysWOW64;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Java\jre1.8.0_251\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files (x86)\PowerShell\6\;C:\Windows\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files\LLVM\bin;%PATH%
set PYTHON_BIN_PATH=C:\Python311\python.exe
set PYTHON_LIB_PATH=C:\Python311\Lib
set PYTHON_DIRECTORY=C:\Python311\Scripts

note: you will find BAZEL_VC_FULL_Version here: C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC

NexusHex (Issue Creator) on (2024-12-16 02:04:45 UTC): Hey @mraunak,

I have tried the suggestions you have given to me, and these were the results:
__________________________
First attempt -
![image](https://github.com/user-attachments/assets/ecac9e68-bdc7-4a7a-be56-8d753d310adf)
![{35763E9C-57FD-4134-BBED-F3A5159328CB}](https://github.com/user-attachments/assets/6c756361-10a9-4326-86b3-19f962a25b2f)
![{A80FE46A-8F1A-4A94-998B-D40D0861B192}](https://github.com/user-attachments/assets/f755ddd2-25cd-4cca-a681-d2ac2cb9c70f)
__________________________
Second attempt (I did all the commands that you gave in case an error happened) -
![{DEC14DA7-4769-44B6-9C0A-0EA6A14EE0B3}](https://github.com/user-attachments/assets/e52d48ab-f13c-4214-8508-155195720ca6)
![{524A05BA-F7DC-46AC-855F-FFD7D7EC4EAA}](https://github.com/user-attachments/assets/63a071b2-6962-47f4-9b52-e8328cbd2bc8)
![{F0414D13-A08B-4876-9D2F-75252360E680}](https://github.com/user-attachments/assets/825a8805-ea99-41a0-9900-1bd8290d6e44)
__________________________
Third attempt (I tried the command you used in your venv example when you toldme to navigate to the tensorflow directory) -
![{E2DD9151-A37A-447A-9C4D-4B26D4032106}](https://github.com/user-attachments/assets/5e722aec-2008-499c-873d-abb1f9482366)
![{EA2C3464-7E85-480A-805B-FD22ACE6F59B}](https://github.com/user-attachments/assets/b3558a8d-f877-43db-9e1b-56a2fd6c3699)

mraunak on (2024-12-16 17:37:34 UTC): Hi @NexusHex,
Use Python 3.11 to create a virtual environment:
python3.11 -m venv tensorflow_build_env
tensorflow_build_env\Scripts\activate

Execute all the provided setup (set) commands and build commands within the activated virtual environment.

NexusHex (Issue Creator) on (2024-12-17 00:39:29 UTC): Hi @mraunak, same errors after running Bazel in the venv:

![{E20B48FD-2C97-4062-82D2-CF687ED04130}](https://github.com/user-attachments/assets/cc230f28-defd-4593-81de-74c388e5d155)
![{FF3EBC32-3539-4C6A-8980-60B7A9AAEB9B}](https://github.com/user-attachments/assets/d626161f-7b90-4fa8-949d-c8fc13767173)
![{1EFAD637-C46D-4A4A-9A11-5AB6C2AD602B}](https://github.com/user-attachments/assets/e1dfa875-0567-44f2-be82-f759f126b2e5)
![{E2CACA83-18ED-4EBA-9085-1EF2E6D637BB}](https://github.com/user-attachments/assets/e0b2a087-adf3-4cfa-a90e-597a2c8cef37)

mraunak on (2024-12-17 17:56:09 UTC): Hi @NexusHex, 

Verify the Path Configuration--Check that the Path is set correctly and points to the intended location.
Validate the Environment Path

Confirm that the environment path is correctly configured and accessible.

Run the Commands Inside the TensorFlow Repository

Navigate to the TensorFlow repository and execute the following commands:
set  
python configure.py  

![image](https://github.com/user-attachments/assets/e2f0c10b-1620-495d-b0a9-5c983a9fc5b6)

NexusHex (Issue Creator) on (2024-12-17 18:17:18 UTC): @mraunak I ran the commands you listed above:
![{22A33DFC-F6D9-42C3-8B65-995DC54AC42B}](https://github.com/user-attachments/assets/7a9b3a6f-a793-4ae8-bb0f-6b48b9bd5685)
![{0821A030-DC1E-4725-931D-7E264A842EA4}](https://github.com/user-attachments/assets/9f8befa0-5102-401b-b616-348b892658bc)
![{AE36011C-B807-4238-AE4A-5FC9A94EA403}](https://github.com/user-attachments/assets/cfb04b17-0750-4508-a61f-b9f826d21577)

These are my paths in the user variables:
![{C0277318-451F-432D-BA6F-936AD62D8DBD}](https://github.com/user-attachments/assets/e9161b10-3653-4c47-9c55-f05dcc664df5)
And these are the ones in the system variables:
![{04BE2BF0-D014-44DC-B4F3-0E31D9D061B6}](https://github.com/user-attachments/assets/6e19b987-6dec-4a9b-b14f-bc8c75c113f8)

mraunak on (2024-12-17 21:10:15 UTC): your output of set command does not contain
![image](https://github.com/user-attachments/assets/ea1a1363-1a39-432a-84ed-3cdf0288a865)

Since Google officially uses bazel version 6.5.0. Please install the same version
https://github.com/tensorflow/tensorflow/blob/master/.bazelversion

mraunak on (2024-12-17 21:17:22 UTC): add python path in environment variables as well
set PYTHON_BIN_PATH=C:\Python311\python.exe or C:\users\Khiza\tensorflow_build_env\Scripts\python.exe
set PYTHON_LIB_PATH=C:\Python311\Lib or C:\users\Khiza\tensorflow_build_env\LIB
set PYTHON_DIRECTORY=C:\Python311\Scripts

NexusHex (Issue Creator) on (2024-12-18 14:57:09 UTC): Hey @mraunak, I donwloaded the correct version of Bazel and deleted the other one:
![{422CF75B-958E-458D-B0BC-F28BD1A1D512}](https://github.com/user-attachments/assets/5fab4713-30b3-48f5-bb75-9ecdba05af55)

And I did all the same things once again:
![{EB84C5A5-CE7E-4EEF-A63C-EF7E191A9A34}](https://github.com/user-attachments/assets/d4986b2d-bfa2-4f90-b4e1-38a188f32190)

I applied the following new environment variables:
![{DFAC5FF2-0EB2-49F9-B27E-50E75E9F9064}](https://github.com/user-attachments/assets/21b85774-2051-4514-8399-678a53cf2ed6)
![{DFEC5266-09BC-4240-BBC9-8F19B9765F54}](https://github.com/user-attachments/assets/0c438aba-5fe6-481a-8c13-e47e785ab687)
But I ran into an issue while I was setting the BAZEL_VC variable. It seems like I only have the BuildTools for VS Redistributable 2022, so how would I go about installing the VC portion of the package?
![{3D7FCE71-2621-464E-BB21-35668F1DEE94}](https://github.com/user-attachments/assets/b0fa858b-a922-46c9-8bb7-0835be700f03)

mraunak on (2024-12-18 16:41:06 UTC): Try to install Visual Studio Community 2022

https://visualstudio.microsoft.com/vs/ 

or try the commands similar to the commands below
![image](https://github.com/user-attachments/assets/8ed42907-88db-4b74-b8b4-af1a7122419e)

NexusHex (Issue Creator) on (2024-12-18 17:20:28 UTC): For Visual Studio Community 2022, I'm not sure what I should download alongside the program, unless I should just download it without any addons.
![image](https://github.com/user-attachments/assets/5c328140-bf0d-479e-9a40-65797f544259)

mihaimaruseac on (2024-12-18 17:53:59 UTC): Duplicate of #19584

Please, don't post screenshots of errors. Instead, post the code in proper markdown format (3 backticks). This way, the errors are searchable and also more accessible.

@mraunak please don't post screenshots with answer code. Post code in proper markdown blocks.

Both, please do a search before. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:54:01 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77387"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77387"">No</a>

mraunak on (2024-12-18 18:00:15 UTC): Thank you very much @mihaimaruseac, yeah sure, I will take care of the points mentioned

NexusHex (Issue Creator) on (2024-12-19 02:50:04 UTC): Thank you for the link to the original issue @mihaimaruseac. Sorry about the shortcomings in mentioning my errors with importing the library, and I will keep them in mind for the future. Have a great day, the both of you, and thank you @mraunak for guiding me through solving this problem to the best of your ability.

mihaimaruseac on (2024-12-19 13:29:00 UTC): Thank you both, and indeed, lots of thanks to @mraunak for helping with windows issues.

Sadly this old CPU issue has been with TF ever before ~2019. It's not something we can reasonably fix as a lot of the code assumes AVX extensions are available. TF has some checker code to check if the CPU is modern enough but even that code gets compiled with a requirement for AVX so it fails :(

adamrickayzenanvil on (2024-12-19 14:25:28 UTC): I have this issue too but my CPU is extremely modern (Snapdragon(R) X Elite - X1E80100, ARM64 based). Is it likely there will be an update at some point which works with this CPU?

mihaimaruseac on (2024-12-19 14:27:27 UTC): Oh, in your case the issue is that TF ARM build is too new. For this, can you open a new issue please?

It would be great if you can also identify which DLL does not load and why by looking at the system logs. Then, please post that in the new issue.

NexusHex (Issue Creator) on (2024-12-19 14:39:32 UTC): Ah, so that's the problem. The thing is that Tensorflow was running perfectly on all my other machines, so I guess that explains why the problem is only affecting my computer. Not sure how old my CPU is though; it is an Intel(R) Celeron(R) N4000 CPU @ 1.10GHz. Either way, it was interesting finding the issue since this was the first time that the library outright refused to be imported.

mihaimaruseac on (2024-12-19 15:28:37 UTC): Checked on https://www.intel.com/content/www/us/en/products/sku/128988/intel-celeron-processor-n4000-4m-cache-up-to-2-60-ghz/specifications.html, and it does have AVX but stops short of AVX2 which is also needed.

NexusHex (Issue Creator) on (2024-12-19 15:44:44 UTC): @mihaimaruseac Makes sense then. I'll check an alternative sometime soon from the issue you linked to me and see if that works better. Thanks :)

"
2576397680,issue,closed,completed,filters and kernel are mentioned as different terms,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

on the conv2d document kernels(as argument kernel_size) and filters(as argument filters) are mentioned as two different arguments despite referring to the same parameter in CNN's.Request to change the argument name both to filters or kernels to avoid any misinterpretation. 

### Standalone code to reproduce the issue

```shell
link to the aforementioned document:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D
```


### Relevant log output

_No response_",ShivamShinde27,2024-10-09 16:24:28+00:00,['tilakrayal'],2024-10-31 02:03:12+00:00,2024-10-31 02:03:07+00:00,https://github.com/tensorflow/tensorflow/issues/77386,"[('type:docs-bug', 'Document issues'), ('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2415869689, 'issue_id': 2576397680, 'author': 'tilakrayal', 'body': '@ShivamShinde27,\r\nFilters represent the number of output channels after convolution has been performed, while Kernel represents the size of a convolution filter being used to perform convolution on the image.\r\n A filter is a collection of kernels, although we use filter and kernels interchangeably. If you still need the change to happen, please raise the request in the keras-team/keras as this is related to keras. Thank you!', 'created_at': datetime.datetime(2024, 10, 16, 6, 41, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073619, 'issue_id': 2576397680, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867879, 'issue_id': 2576397680, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867978, 'issue_id': 2576397680, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77386"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77386"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 10, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-16 06:41:12 UTC): @ShivamShinde27,
Filters represent the number of output channels after convolution has been performed, while Kernel represents the size of a convolution filter being used to perform convolution on the image.
 A filter is a collection of kernels, although we use filter and kernels interchangeably. If you still need the change to happen, please raise the request in the keras-team/keras as this is related to keras. Thank you!

github-actions[bot] on (2024-10-24 02:01:38 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-31 02:03:06 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-31 02:03:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77386"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77386"">No</a>

"
2575897028,issue,open,,Tensorflow Distributed AlltoAll,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

Tf 2.9

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I am trying to find a workaround, or ideally an implementation of the MPI [AlltoAll](https://www.mpich.org/static/docs/v3.4/www3/MPI_Alltoall.html) primitive.

As far as I can tell, Tensorflow has an AlltoAll op, but only for TPUs: https://www.tensorflow.org/api_docs/python/tf/raw_ops/AllToAll.

In a distributed setup with 4 devices, this would allow me to go from
```bash
PerReplica:{
  0: [0 1 2 3],
  1: [0 1 2 3],
  2: [0 1 2 3],
  3: [0 1 2 3]
}
``` 
to 
```
PerReplica:{
  0: [0 0 0 0],
  1: [1 1 1 1],
  2: [2 2 2 2],
  3: [3 3 3 3]
}
```
with a single communication call. I have managed to get this working by sharding with DTensor, but I'm trying to stay within the `tf.distribute` setting. Is there an easy workaround?





### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
# tf.debugging.set_log_device_placement(True)
tf.config.set_visible_devices([], device_type='GPU')
# Create sharding mesh
def configure_virtual_cpus(ncpu):
    phy_devices = tf.config.list_physical_devices('CPU')
    tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

ndev = 4
configure_virtual_cpus(ndev)
devices = [f'CPU:{i}' for i in range(ndev)]
strategy = tf.distribute.MirroredStrategy(devices)

arr = np.arange(0,4)
arr_total = np.stack([arr]*4)


def value_fn(ctx):
    return arr_total[ctx.replica_id_in_sync_group]


arr_tf = strategy.experimental_distribute_values_from_function(value_fn)
print(arr_tf)
# How to achieve the AllToAll operation here?
```


### Relevant log output

```shell
PerReplica:{
  0: [0 1 2 3],
  1: [0 1 2 3],
  2: [0 1 2 3],
  3: [0 1 2 3]
}
```
",therooler,2024-10-09 13:09:12+00:00,['Venkat6871'],2024-10-11 10:39:11+00:00,,https://github.com/tensorflow/tensorflow/issues/77367,"[('type:feature', 'Feature requests'), ('comp:ops', 'OPs related issues')]",[],
2575807056,issue,closed,completed,Issue with Mask R-CNN and TFRecord containing both masks and bounding boxes,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am working with Mask R-CNN from the TensorFlow Model Zoo, and I am facing an issue when training with TFRecord files that include both masks and bounding boxes. My dataset is formatted as follows:

Each image has a corresponding mask image, which is used for generating bounding boxes.
I have included both the bounding box coordinates and the encoded mask in the TFRecord files.
Here is a snippet of how I prepare the TFRecord:

```
# Utility function for bounding box extraction from a mask
def mask_to_bounding_box(mask_path):
    # Load the binary mask (segmented object should have pixel value > 0)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        raise ValueError(f""Mask image {mask_path} could not be loaded."")
    
    # Find contours in the binary mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # List to store bounding boxes
    bounding_boxes = []

    # Iterate over each contour
    for contour in contours:
        # Get the bounding rectangle for each contour
        x, y, w, h = cv2.boundingRect(contour)
        bounding_boxes.append([x, y, x + w, y + h])  # (x_min, y_min, x_max, y_max)


    return bounding_boxes

def create_tf_example(image_path, mask_path, bboxes, labels):
    """"""
    Args:
      image_path (str): Path to the image file (JPEG or PNG).
      mask_path (str): Path to the corresponding mask file (PNG).
      bboxes (list of list): List of bounding boxes, each a list in [xmin, ymin, xmax, ymax] format, normalized to [0, 1].
      labels (list of int): List of integer labels corresponding to each bounding box.
    
    Returns:
      tf.train.Example: The TFRecord example containing image data and annotations.
    """"""

    # Read image
    with tf.io.gfile.GFile(image_path, 'rb') as fid:
        encoded_image = fid.read()
    
    image = cv2.imread(image_path, 0) 
    image_format = b'JPG'  # or b'png' depending on your image format
    height, width = image.shape  # You need to know the dimensions of your images
    
    # Read the mask (binary mask)
    with tf.io.gfile.GFile(mask_path, 'rb') as fid:
        encoded_mask = fid.read()

    # Prepare bounding box information
    xmins = []  # List of normalized xmin coordinates
    xmaxs = []  # List of normalized xmax coordinates
    ymins = []  # List of normalized ymin coordinates
    ymaxs = []  # List of normalized ymax coordinates
    sizes=[]

    class_text = [get_class_text_for_id(label_map_dict,labels).encode('utf-8')]
    class_id = [labels]

    #for bbox in bboxes:
    for bbox in bboxes:
        print(bbox)
        xmin, ymin, xmax, ymax = bbox
        size=(((xmax-xmin)/width)*((ymax-ymin)/height))
        assert xmin<xmax,""xmin<xmax olmalı""
        assert ymin<ymax,""ymin<ymax olmalı""
        assert size>0, ""Alan sıfırdan büyük olmalı""
        xmins.append(xmin/width)
        xmaxs.append(xmax/width)
        ymins.append(ymin/height)
        ymaxs.append(ymax/height)

        print(image_path,class_text,sizes)    
    # Prepare features for the TFRecord
    feature = {
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/source_id': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/encoded': dataset_util.bytes_feature(encoded_image),
        'image/format': dataset_util.bytes_feature(image_format),
        
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(class_text),
        'image/object/class/label': dataset_util.int64_list_feature(class_id),
        'image/encoded_mask': dataset_util.bytes_feature(encoded_mask)
    }

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example
```

However, when I try to train Mask R-CNN with this setup, I get the following error:

```
See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Traceback (most recent call last):
  File ""E:\PROJELER\DevamEden\DEN_IZ\tfgpu\models\research\object_detection\model_main_tf2.py"", line 114, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\platform\app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 308, in run
    _run_main(main, args)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""E:\PROJELER\DevamEden\DEN_IZ\tfgpu\models\research\object_detection\model_main_tf2.py"", line 105, in main
    model_lib_v2.train_loop(
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\object_detection\model_lib_v2.py"", line 685, in train_loop
    losses_dict = _dist_train_step(train_input_iter)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

2 root error(s) found.
  (0) INVALID_ARGUMENT:  indices[0] = 0 is not in [0, 0)
         [[{{node GatherV2_7}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[while/body/_1/IteratorGetNext]]
         [[while/body/_1/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/else/_6000/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_6260/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/_410]]
  (1) INVALID_ARGUMENT:  indices[0] = 0 is not in [0, 0)
         [[{{node GatherV2_7}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[while/body/_1/IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference__dist_train_step_93456]
```

The point that draws my attention is that this error does not appear when there are only masks in my TFRecord file, and I do not receive the error on models such as SSD that only work with bounding box. I get this error when I use both mask and bounding box information with Mask R-CNN. What could be the cause of this problem and how can I solve it?



### Standalone code to reproduce the issue

```shell
# Utility function for bounding box extraction from a mask
def mask_to_bounding_box(mask_path):
    # Load the binary mask (segmented object should have pixel value > 0)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        raise ValueError(f""Mask image {mask_path} could not be loaded."")
    
    # Find contours in the binary mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # List to store bounding boxes
    bounding_boxes = []

    # Iterate over each contour
    for contour in contours:
        # Get the bounding rectangle for each contour
        x, y, w, h = cv2.boundingRect(contour)
        bounding_boxes.append([x, y, x + w, y + h])  # (x_min, y_min, x_max, y_max)


    return bounding_boxes

def create_tf_example(image_path, mask_path, bboxes, labels):
    """"""
    Args:
      image_path (str): Path to the image file (JPEG or PNG).
      mask_path (str): Path to the corresponding mask file (PNG).
      bboxes (list of list): List of bounding boxes, each a list in [xmin, ymin, xmax, ymax] format, normalized to [0, 1].
      labels (list of int): List of integer labels corresponding to each bounding box.
    
    Returns:
      tf.train.Example: The TFRecord example containing image data and annotations.
    """"""

    # Read image
    with tf.io.gfile.GFile(image_path, 'rb') as fid:
        encoded_image = fid.read()
    
    image = cv2.imread(image_path, 0) 
    image_format = b'JPG'  # or b'png' depending on your image format
    height, width = image.shape  # You need to know the dimensions of your images
    
    # Read the mask (binary mask)
    with tf.io.gfile.GFile(mask_path, 'rb') as fid:
        encoded_mask = fid.read()

    # Prepare bounding box information
    xmins = []  # List of normalized xmin coordinates
    xmaxs = []  # List of normalized xmax coordinates
    ymins = []  # List of normalized ymin coordinates
    ymaxs = []  # List of normalized ymax coordinates
    sizes=[]

    class_text = [get_class_text_for_id(label_map_dict,labels).encode('utf-8')]
    class_id = [labels]

    #for bbox in bboxes:
    for bbox in bboxes:
        print(bbox)
        xmin, ymin, xmax, ymax = bbox
        size=(((xmax-xmin)/width)*((ymax-ymin)/height))
        assert xmin<xmax,""xmin<xmax olmalı""
        assert ymin<ymax,""ymin<ymax olmalı""
        assert size>0, ""Alan sıfırdan büyük olmalı""
        xmins.append(xmin/width)
        xmaxs.append(xmax/width)
        ymins.append(ymin/height)
        ymaxs.append(ymax/height)

        print(image_path,class_text,sizes)    
    # Prepare features for the TFRecord
    feature = {
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/source_id': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/encoded': dataset_util.bytes_feature(encoded_image),
        'image/format': dataset_util.bytes_feature(image_format),
        
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(class_text),
        'image/object/class/label': dataset_util.int64_list_feature(class_id),
        'image/encoded_mask': dataset_util.bytes_feature(encoded_mask)
    }

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example
```


### Relevant log output

_No response_",senaagacc,2024-10-09 12:36:34+00:00,['tilakrayal'],2024-10-27 02:05:49+00:00,2024-10-27 02:05:47+00:00,https://github.com/tensorflow/tensorflow/issues/77365,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:model', 'Model related issues'), ('TF 2.10', '')]","[{'comment_id': 2402220760, 'issue_id': 2575807056, 'author': 'senaagacc', 'body': 'Also, could there be a problem with the parameters in the Mask RCNN pipeline.config file, My file is as follows. I just changed num_classes and Paths.\r\n\r\n```\r\n# Mask R-CNN with Inception Resnet v2 (no atrous)\r\n# Sync-trained on COCO (with 8 GPUs) with batch size 16 (1024x1024 resolution)\r\n# Initialized from Imagenet classification checkpoint\r\n# TF2-Compatible, *Not* TPU-Compatible\r\n#\r\n# Achieves XXX mAP on COCO\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    number_of_stages: 3\r\n    num_classes: 11\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 1024\r\n        width: 1024\r\n        # pad_to_max_dimension: true\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \'faster_rcnn_inception_resnet_v2_keras\'\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 17\r\n    maxpool_kernel_size: 1\r\n    maxpool_stride: 1\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n        mask_height: 33\r\n        mask_width: 33\r\n        mask_prediction_conv_depth: 0\r\n        mask_prediction_num_conv_layers: 4\r\n        conv_hyperparams {\r\n          op: CONV\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.01\r\n            }\r\n          }\r\n        }\r\n        predict_instance_masks: true\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n    second_stage_mask_prediction_loss_weight: 4.0\r\n    resize_masks: false\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 16\r\n  num_steps: 200000\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.008\r\n          total_steps: 200000\r\n          warmup_learning_rate: 0.0\r\n          warmup_steps: 5000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: ""PATH""\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}\r\n\r\neval_config: {\r\n  metrics_set: ""coco_detection_metrics""\r\n  metrics_set: ""coco_mask_metrics""\r\n  eval_instance_masks: true\r\n  use_moving_averages: false\r\n  batch_size: 1\r\n  include_metrics_per_category: true\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: ""PATH""\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: ""PATH""\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}   \r\n```', 'created_at': datetime.datetime(2024, 10, 9, 12, 44, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408240253, 'issue_id': 2575807056, 'author': 'tilakrayal', 'body': '@senaagacc,\r\nIn the given code snippet you have defined the class and its methods but are not calling them anywhere. Could you please provide the complete code or the colab gist which helps to debug the issue and also please try to upgrade to the latest TensorFlow v2.17. Thank you!', 'created_at': datetime.datetime(2024, 10, 11, 23, 39, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423475688, 'issue_id': 2575807056, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 19, 2, 0, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439804108, 'issue_id': 2575807056, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 27, 2, 5, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439804144, 'issue_id': 2575807056, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77365"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77365"">No</a>', 'created_at': datetime.datetime(2024, 10, 27, 2, 5, 49, tzinfo=datetime.timezone.utc)}]","senaagacc (Issue Creator) on (2024-10-09 12:44:11 UTC): Also, could there be a problem with the parameters in the Mask RCNN pipeline.config file, My file is as follows. I just changed num_classes and Paths.

```
# Mask R-CNN with Inception Resnet v2 (no atrous)
# Sync-trained on COCO (with 8 GPUs) with batch size 16 (1024x1024 resolution)
# Initialized from Imagenet classification checkpoint
# TF2-Compatible, *Not* TPU-Compatible
#
# Achieves XXX mAP on COCO

model {
  faster_rcnn {
    number_of_stages: 3
    num_classes: 11
    image_resizer {
      fixed_shape_resizer {
        height: 1024
        width: 1024
        # pad_to_max_dimension: true
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2_keras'
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        mask_height: 33
        mask_width: 33
        mask_prediction_conv_depth: 0
        mask_prediction_num_conv_layers: 4
        conv_hyperparams {
          op: CONV
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.01
            }
          }
        }
        predict_instance_masks: true
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
    second_stage_mask_prediction_loss_weight: 4.0
    resize_masks: false
  }
}

train_config: {
  batch_size: 16
  num_steps: 200000
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 0.008
          total_steps: 200000
          warmup_learning_rate: 0.0
          warmup_steps: 5000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""PATH""
  }
  load_instance_masks: true
  mask_type: PNG_MASKS
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  metrics_set: ""coco_mask_metrics""
  eval_instance_masks: true
  use_moving_averages: false
  batch_size: 1
  include_metrics_per_category: true
}

eval_input_reader: {
  label_map_path: ""PATH""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""PATH""
  }
  load_instance_masks: true
  mask_type: PNG_MASKS
}   
```

tilakrayal (Assginee) on (2024-10-11 23:39:30 UTC): @senaagacc,
In the given code snippet you have defined the class and its methods but are not calling them anywhere. Could you please provide the complete code or the colab gist which helps to debug the issue and also please try to upgrade to the latest TensorFlow v2.17. Thank you!

github-actions[bot] on (2024-10-19 02:00:49 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-27 02:05:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-27 02:05:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77365"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77365"">No</a>

"
2575769733,issue,closed,completed,Issue with Mask R-CNN and TFRecord containing both masks and bounding boxes,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am working with Mask R-CNN from the TensorFlow Model Zoo, and I am facing an issue when training with TFRecord files that include both masks and bounding boxes. My dataset is formatted as follows:

Each image has a corresponding mask image, which is used for generating bounding boxes.
I have included both the bounding box coordinates and the encoded mask in the TFRecord files.
Here is a snippet of how I prepare the TFRecord:

```
# Utility function for bounding box extraction from a mask
def mask_to_bounding_box(mask_path):
    # Load the binary mask (segmented object should have pixel value > 0)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        raise ValueError(f""Mask image {mask_path} could not be loaded."")
    
    # Find contours in the binary mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # List to store bounding boxes
    bounding_boxes = []

    # Iterate over each contour
    for contour in contours:
        # Get the bounding rectangle for each contour
        x, y, w, h = cv2.boundingRect(contour)
        bounding_boxes.append([x, y, x + w, y + h])  # (x_min, y_min, x_max, y_max)


    return bounding_boxes

def create_tf_example(image_path, mask_path, bboxes, labels):
    """"""
    Args:
      image_path (str): Path to the image file (JPEG or PNG).
      mask_path (str): Path to the corresponding mask file (PNG).
      bboxes (list of list): List of bounding boxes, each a list in [xmin, ymin, xmax, ymax] format, normalized to [0, 1].
      labels (list of int): List of integer labels corresponding to each bounding box.
    
    Returns:
      tf.train.Example: The TFRecord example containing image data and annotations.
    """"""

    # Read image
    with tf.io.gfile.GFile(image_path, 'rb') as fid:
        encoded_image = fid.read()
    
    image = cv2.imread(image_path, 0) 
    image_format = b'JPG'  # or b'png' depending on your image format
    height, width = image.shape  # You need to know the dimensions of your images
    
    # Read the mask (binary mask)
    with tf.io.gfile.GFile(mask_path, 'rb') as fid:
        encoded_mask = fid.read()

    # Prepare bounding box information
    xmins = []  # List of normalized xmin coordinates
    xmaxs = []  # List of normalized xmax coordinates
    ymins = []  # List of normalized ymin coordinates
    ymaxs = []  # List of normalized ymax coordinates
    sizes=[]

    class_text = [get_class_text_for_id(label_map_dict,labels).encode('utf-8')]
    class_id = [labels]

    #for bbox in bboxes:
    for bbox in bboxes:
        print(bbox)
        xmin, ymin, xmax, ymax = bbox
        size=(((xmax-xmin)/width)*((ymax-ymin)/height))
        assert xmin<xmax,""xmin<xmax olmalı""
        assert ymin<ymax,""ymin<ymax olmalı""
        assert size>0, ""Alan sıfırdan büyük olmalı""
        xmins.append(xmin/width)
        xmaxs.append(xmax/width)
        ymins.append(ymin/height)
        ymaxs.append(ymax/height)

        print(image_path,class_text,sizes)    
    # Prepare features for the TFRecord
    feature = {
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/source_id': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/encoded': dataset_util.bytes_feature(encoded_image),
        'image/format': dataset_util.bytes_feature(image_format),
        
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(class_text),
        'image/object/class/label': dataset_util.int64_list_feature(class_id),
        'image/encoded_mask': dataset_util.bytes_feature(encoded_mask)
    }

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example
```

However, when I try to train Mask R-CNN with this setup, I get the following error:

```
See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Traceback (most recent call last):
  File ""E:\PROJELER\DevamEden\DEN_IZ\tfgpu\models\research\object_detection\model_main_tf2.py"", line 114, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\platform\app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 308, in run
    _run_main(main, args)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""E:\PROJELER\DevamEden\DEN_IZ\tfgpu\models\research\object_detection\model_main_tf2.py"", line 105, in main
    model_lib_v2.train_loop(
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\object_detection\model_lib_v2.py"", line 685, in train_loop
    losses_dict = _dist_train_step(train_input_iter)
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\caydoner\AppData\Local\anaconda3\envs\tfgpu\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

2 root error(s) found.
  (0) INVALID_ARGUMENT:  indices[0] = 0 is not in [0, 0)
         [[{{node GatherV2_7}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[while/body/_1/IteratorGetNext]]
         [[while/body/_1/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/else/_6000/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_6260/Loss/RPNLoss/Loss/huber_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/_410]]
  (1) INVALID_ARGUMENT:  indices[0] = 0 is not in [0, 0)
         [[{{node GatherV2_7}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[while/body/_1/IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference__dist_train_step_93456]
```

The point that draws my attention is that this error does not appear when there are only masks in my TFRecord file, and I do not receive the error on models such as SSD that only work with bounding box. I get this error when I use both mask and bounding box information with Mask R-CNN. What could be the cause of this problem and how can I solve it?



### Standalone code to reproduce the issue

```shell
# Utility function for bounding box extraction from a mask
def mask_to_bounding_box(mask_path):
    # Load the binary mask (segmented object should have pixel value > 0)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        raise ValueError(f""Mask image {mask_path} could not be loaded."")
    
    # Find contours in the binary mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # List to store bounding boxes
    bounding_boxes = []

    # Iterate over each contour
    for contour in contours:
        # Get the bounding rectangle for each contour
        x, y, w, h = cv2.boundingRect(contour)
        bounding_boxes.append([x, y, x + w, y + h])  # (x_min, y_min, x_max, y_max)


    return bounding_boxes

def create_tf_example(image_path, mask_path, bboxes, labels):
    """"""
    Args:
      image_path (str): Path to the image file (JPEG or PNG).
      mask_path (str): Path to the corresponding mask file (PNG).
      bboxes (list of list): List of bounding boxes, each a list in [xmin, ymin, xmax, ymax] format, normalized to [0, 1].
      labels (list of int): List of integer labels corresponding to each bounding box.
    
    Returns:
      tf.train.Example: The TFRecord example containing image data and annotations.
    """"""

    # Read image
    with tf.io.gfile.GFile(image_path, 'rb') as fid:
        encoded_image = fid.read()
    
    image = cv2.imread(image_path, 0) 
    image_format = b'JPG'  # or b'png' depending on your image format
    height, width = image.shape  # You need to know the dimensions of your images
    
    # Read the mask (binary mask)
    with tf.io.gfile.GFile(mask_path, 'rb') as fid:
        encoded_mask = fid.read()

    # Prepare bounding box information
    xmins = []  # List of normalized xmin coordinates
    xmaxs = []  # List of normalized xmax coordinates
    ymins = []  # List of normalized ymin coordinates
    ymaxs = []  # List of normalized ymax coordinates
    sizes=[]

    class_text = [get_class_text_for_id(label_map_dict,labels).encode('utf-8')]
    class_id = [labels]

    #for bbox in bboxes:
    for bbox in bboxes:
        print(bbox)
        xmin, ymin, xmax, ymax = bbox
        size=(((xmax-xmin)/width)*((ymax-ymin)/height))
        assert xmin<xmax,""xmin<xmax olmalı""
        assert ymin<ymax,""ymin<ymax olmalı""
        assert size>0, ""Alan sıfırdan büyük olmalı""
        xmins.append(xmin/width)
        xmaxs.append(xmax/width)
        ymins.append(ymin/height)
        ymaxs.append(ymax/height)

        print(image_path,class_text,sizes)    
    # Prepare features for the TFRecord
    feature = {
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/source_id': dataset_util.bytes_feature(tf.compat.as_bytes(image_path.encode('utf-8'))),
        'image/encoded': dataset_util.bytes_feature(encoded_image),
        'image/format': dataset_util.bytes_feature(image_format),
        
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(class_text),
        'image/object/class/label': dataset_util.int64_list_feature(class_id),
        'image/encoded_mask': dataset_util.bytes_feature(encoded_mask)
    }

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example
```


### Relevant log output

_No response_",senaagacc,2024-10-09 12:20:38+00:00,['Venkat6871'],2024-10-11 10:45:10+00:00,2024-10-11 10:45:07+00:00,https://github.com/tensorflow/tensorflow/issues/77363,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:model', 'Model related issues'), ('TF 2.10', '')]","[{'comment_id': 2407119017, 'issue_id': 2575769733, 'author': 'Venkat6871', 'body': 'Hi **@senaagacc** ,\r\nLooks like this is duplicate of issue #77365. Can you please close this issue, since it is already being tracked there?\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 11, 10, 29, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407142579, 'issue_id': 2575769733, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77363"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77363"">No</a>', 'created_at': datetime.datetime(2024, 10, 11, 10, 45, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-11 10:29:58 UTC): Hi **@senaagacc** ,
Looks like this is duplicate of issue #77365. Can you please close this issue, since it is already being tracked there?
Thank you!

google-ml-butler[bot] on (2024-10-11 10:45:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77363"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77363"">No</a>

"
2575680467,issue,closed,completed,OOM when trying to save several concatenated datasets as one sharded dataset,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

-

### GPU model and memory

_No response_

### Current behavior?

Hi,

I am trying to create a single, sharded (100 shards) tf.data.dataset from several tf.data.dataset(s) of overall 282GB size. Those separate datasets have been created by converting a list of parquet files. Each of those separate datasets should in the end be one of the shards of the overall dataset. I got a field in them (which has the same value for each example in a given dataset) that actually will be used in the save-method as an argument which will determine the shard number.

In order to generate the sharded dataset, I concat all separate ones and try to save them. What happens is that for the first 9 minutes several shards are being created without using more than 1% of the overall 128GB of memory on the node I got. There is absolutly no increase in memory usage visible.

After that tho, memory usage suddenly starts to increase until the system OOMs. Since the issue to occur requires huge files which I can not share, I just put the relevant code snipptes here.

My questions here are the following:

1) Is there a way to create this sharded dataset without going oom? 
2) If i would not create this sharded dataset and just directly feed the concatenated separate datasets to model training without doing shuffles, prefetches and such, would I go OOM or would it actually run through?
3) If the above solutions are the wrong track, what would be the correct one to train with this data?

I am really hoping for help. 




### Standalone code to reproduce the issue

```shell
sharded = tf.data.Dataset.load(""data/tf_datasets/converted/{data_type}/{partition}"".format(data_type=data_type, partition=0)).unbatch()
for partition in range(1,partition_count):
    loaded_ds = tf.data.Dataset.load(""data/tf_datasets/converted/{data_type}/{partition}"".format(data_type=data_type, partition=partition)).unbatch()
    sharded = sharded.concatenate(loaded_ds)

        
sharded.save(""data/tf_datasets/sharded/{data_type}"".format(data_type=data_type), shard_func=lambda x, y, z, a: a)
```


### Relevant log output

_No response_",PowerToThePeople111,2024-10-09 11:42:38+00:00,['tilakrayal'],2024-10-09 15:53:15+00:00,2024-10-09 15:53:13+00:00,https://github.com/tensorflow/tensorflow/issues/77359,"[('type:performance', 'Performance Issue')]","[{'comment_id': 2402713007, 'issue_id': 2575680467, 'author': 'PowerToThePeople111', 'body': 'Ok, I found a solution to the problem. It is to just skip the step of trying to save a sharded dataset and just directly feed the concatenated separate datasets to the model. At least 1 epoch of training on the complete set only brought me to 9% of memory usage. I hope it will stay that way. :)\r\n\r\nI think it is still quite surprising tho that saving a merged dataset seems to be harder than to train on it.', 'created_at': datetime.datetime(2024, 10, 9, 15, 53, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402713088, 'issue_id': 2575680467, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77359"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77359"">No</a>', 'created_at': datetime.datetime(2024, 10, 9, 15, 53, 15, tzinfo=datetime.timezone.utc)}]","PowerToThePeople111 (Issue Creator) on (2024-10-09 15:53:13 UTC): Ok, I found a solution to the problem. It is to just skip the step of trying to save a sharded dataset and just directly feed the concatenated separate datasets to the model. At least 1 epoch of training on the complete set only brought me to 9% of memory usage. I hope it will stay that way. :)

I think it is still quite surprising tho that saving a merged dataset seems to be harder than to train on it.

google-ml-butler[bot] on (2024-10-09 15:53:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77359"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77359"">No</a>

"
2575549236,issue,open,,Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1 (model saved), 2.17.0 (model loaded)

### Custom code

Yes

### OS platform and distribution

(Official Docker Image) Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10 (model saved),  3.11.0rc1 (model loaded)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

## Description

I have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.

## Steps to reproduce

1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:
   - A simple Keras sequential model is created and trained on random data.
   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).
   
2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:
   - The models are loaded using TensorFlow’s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.

3. **Observe the errors**:
   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.
   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.
   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!

## Expected behavior

While I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.


### Standalone code to reproduce the issue

## Minimal example to reproduce the issue

A minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https://github.com/arianmaghsoudnia/reproduce-tf-model-compat-issue)

Please follow the steps in the README file.



### Relevant log output

_No response_",arianmaghsoudnia,2024-10-09 10:50:39+00:00,['Venkat6871'],2024-10-24 07:13:58+00:00,,https://github.com/tensorflow/tensorflow/issues/77356,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2402423787, 'issue_id': 2575549236, 'author': 'arianmaghsoudnia', 'body': 'I found out the issue is probably with the optimizer. When I use `optimizer=""sgd""` for instance, instead of `optimizer=""adam""`, I can save and load model with the `tf.saved_model` without problems.', 'created_at': datetime.datetime(2024, 10, 9, 13, 56, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2425614878, 'issue_id': 2575549236, 'author': 'Venkat6871', 'body': 'Hi **@arianmaghsoudnia** ,\r\nApologies for the delay, and thank you for your patience. Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 21, 5, 21, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434480204, 'issue_id': 2575549236, 'author': 'arianmaghsoudnia', 'body': ""@Venkat6871 Thank you for reviewing this. I’d like to highlight that saving a model with TensorFlow's `tf.saved_model.save` and then attempting to load it using `tf.saved_model.load` currently results in a failure. Since this issue relates specifically to TensorFlow and not Keras, I believe it should be addressed here.\r\n\r\nAs for Keras, they’ve entirely dropped support for TensorFlow models. Strange decision, but that’s a separate discussion."", 'created_at': datetime.datetime(2024, 10, 24, 7, 13, 56, tzinfo=datetime.timezone.utc)}]","arianmaghsoudnia (Issue Creator) on (2024-10-09 13:56:33 UTC): I found out the issue is probably with the optimizer. When I use `optimizer=""sgd""` for instance, instead of `optimizer=""adam""`, I can save and load model with the `tf.saved_model` without problems.

Venkat6871 (Assginee) on (2024-10-21 05:21:30 UTC): Hi **@arianmaghsoudnia** ,
Apologies for the delay, and thank you for your patience. Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

arianmaghsoudnia (Issue Creator) on (2024-10-24 07:13:56 UTC): @Venkat6871 Thank you for reviewing this. I’d like to highlight that saving a model with TensorFlow's `tf.saved_model.save` and then attempting to load it using `tf.saved_model.load` currently results in a failure. Since this issue relates specifically to TensorFlow and not Keras, I believe it should be addressed here.

As for Keras, they’ve entirely dropped support for TensorFlow models. Strange decision, but that’s a separate discussion.

"
2575105426,issue,open,,tf.nn.conv2d terminates process with invalid input shape instead of raising an exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.

I expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define invalid input tensor and kernel
input_tensor = [[1.0, 2.0, 3.0]]
kernel = [[0.5, 0.5], [0.5, 0.5]]

try:
    # Create TensorFlow constants
    input_tf = tf.constant(input_tensor, dtype=tf.float32)
    kernel_tf = tf.constant(kernel, dtype=tf.float32)
    
    # Attempt to perform convolution, expecting an error
    output_tf = tf.nn.conv2d(
        tf.expand_dims(input_tf, axis=0), 
        tf.expand_dims(kernel_tf, axis=0), 
        strides=[1, 1, 1, 1], 
        padding='VALID'
    )
    
    print(""TensorFlow Output:"", output_tf.numpy())
except Exception as e:
    print(""TensorFlow Error:"", e)
```


### Relevant log output

```shell
2024-10-09 14:53:31.118589: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)
```
",LegendBug,2024-10-09 07:52:38+00:00,['tilakrayal'],2024-11-07 06:13:18+00:00,,https://github.com/tensorflow/tensorflow/issues/77336,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2401754103, 'issue_id': 2575105426, 'author': 'AdvaitDongre', 'body': ""To resolve this issue, the input tensor and kernel must be reshaped into 4D tensors:\r\n\r\n- **Input Tensor**: The input must be reshaped into `[batch_size, height, width, channels]` format. For a 1x3 input, this can be done using:\r\n  ```python\r\n  input_tf = tf.reshape(input_tf, [1, 1, 3, 1])  # batch_size=1, height=1, width=3, channels=1\r\n  ```\r\n  \r\n- **Kernel Tensor**: Similarly, the kernel must be reshaped into `[filter_height, filter_width, input_channels, output_channels]`. For a 2x2 kernel, this can be done as follows:\r\n  ```python\r\n  kernel_tf = tf.reshape(kernel_tf, [2, 2, 1, 1])  # filter_height=2, filter_width=2, input_channels=1, output_channels=1\r\n  ```\r\n\r\nBy reshaping both the input and kernel to their required 4D shapes, the convolution operation can be performed without errors.\r\n\r\nHere is the link of the colab in which i tried, and the it wasn't throwing any errors [here](https://colab.research.google.com/drive/1gbszfjs1iOyErCXVlFPGOLQUqT882krm#scrollTo=P7Cxc3cOfLVX)\r\n\r\nlet me know if it helps in your issue"", 'created_at': datetime.datetime(2024, 10, 9, 9, 2, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401783176, 'issue_id': 2575105426, 'author': 'LegendBug', 'body': ""> To resolve this issue, the input tensor and kernel must be reshaped into 4D tensors:\r\n> \r\n> * **Input Tensor**: The input must be reshaped into `[batch_size, height, width, channels]` format. For a 1x3 input, this can be done using:\r\n>   ```python\r\n>   input_tf = tf.reshape(input_tf, [1, 1, 3, 1])  # batch_size=1, height=1, width=3, channels=1\r\n>   ```\r\n> * **Kernel Tensor**: Similarly, the kernel must be reshaped into `[filter_height, filter_width, input_channels, output_channels]`. For a 2x2 kernel, this can be done as follows:\r\n>   ```python\r\n>   kernel_tf = tf.reshape(kernel_tf, [2, 2, 1, 1])  # filter_height=2, filter_width=2, input_channels=1, output_channels=1\r\n>   ```\r\n> \r\n> By reshaping both the input and kernel to their required 4D shapes, the convolution operation can be performed without errors.\r\n> \r\n> Here is the link of the colab in which i tried, and the it wasn't throwing any errors [here](https://colab.research.google.com/drive/1gbszfjs1iOyErCXVlFPGOLQUqT882krm#scrollTo=P7Cxc3cOfLVX)\r\n> \r\n> let me know if it helps in your issue\r\n\r\nThank you for your suggestion! Reshaping the input and kernel tensors to 4D indeed resolves the issue for this specific case. However, my concern is more about TensorFlow's robustness. Ideally, when passing invalid input shapes to tf.nn.conv2d, TensorFlow should raise a catchable Python exception (e.g., ValueError or TypeError) rather than terminating the process. This would allow for better error handling without the program crashing.\r\n\r\nLet me know if this makes sense, and thank you again for your help!"", 'created_at': datetime.datetime(2024, 10, 9, 9, 15, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402071403, 'issue_id': 2575105426, 'author': 'AdvaitDongre', 'body': ""You're very right. Currently, TensorFlow's behavior of terminating the process with a non-catchable error (rather than raising a Python exception like ValueError or TypeError) prevents graceful error handling. A more robust approach would be to throw a catchable Python exception that informs the user of invalid tensor shapes without crashing the entire program. i think you should tag someone from their team"", 'created_at': datetime.datetime(2024, 10, 9, 11, 35, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418637490, 'issue_id': 2575105426, 'author': 'tilakrayal', 'body': '@LegendBug,\r\nI tried to execute the mentioned code on tensorflow v2.17 & tf-nightly and observed that it was providing the error **convolution input must be 4-dimensional: [1,1,3] [Op:Conv2D]** rather than the crash/abort. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/8a28644e180fadcee90b639f9da1febe/untitled2180.ipynb) and confirm whether you are also facing the same. Thank you!', 'created_at': datetime.datetime(2024, 10, 17, 6, 29, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418969879, 'issue_id': 2575105426, 'author': 'LegendBug', 'body': ""> @LegendBug, I tried to execute the mentioned code on tensorflow v2.17 & tf-nightly and observed that it was providing the error **convolution input must be 4-dimensional: [1,1,3] [Op:Conv2D]** rather than the crash/abort. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/8a28644e180fadcee90b639f9da1febe/untitled2180.ipynb) and confirm whether you are also facing the same. Thank you!\r\n\r\nThank you for your response.\r\n\r\nUsing Colab's built-in configuration (CPU + Python 3), I indeed observed the expected error message. However, when I connect Colab to my local device, the program still crashes unexpectedly.\r\n\r\nYou can see my Colab runtime records [here](https://colab.research.google.com/gist/LegendBug/74c8bbacbf157a5781d33eea9ce8c7fd/untitled2180.ipynb). I attempted to run two similar programs—the first one produced output normally, while the second one caused a crash. Since Colab seems to disconnect sessions on crashes, the output is not complete, attached is my complete output at runtime locally.\r\n[output.log](https://github.com/user-attachments/files/17409204/output.log)\r\n\r\nPlease let me know if there's any additional information needed. Thank you again for your assistance!"", 'created_at': datetime.datetime(2024, 10, 17, 9, 2, 11, tzinfo=datetime.timezone.utc)}]","AdvaitDongre on (2024-10-09 09:02:58 UTC): To resolve this issue, the input tensor and kernel must be reshaped into 4D tensors:

- **Input Tensor**: The input must be reshaped into `[batch_size, height, width, channels]` format. For a 1x3 input, this can be done using:
  ```python
  input_tf = tf.reshape(input_tf, [1, 1, 3, 1])  # batch_size=1, height=1, width=3, channels=1
  ```
  
- **Kernel Tensor**: Similarly, the kernel must be reshaped into `[filter_height, filter_width, input_channels, output_channels]`. For a 2x2 kernel, this can be done as follows:
  ```python
  kernel_tf = tf.reshape(kernel_tf, [2, 2, 1, 1])  # filter_height=2, filter_width=2, input_channels=1, output_channels=1
  ```

By reshaping both the input and kernel to their required 4D shapes, the convolution operation can be performed without errors.

Here is the link of the colab in which i tried, and the it wasn't throwing any errors [here](https://colab.research.google.com/drive/1gbszfjs1iOyErCXVlFPGOLQUqT882krm#scrollTo=P7Cxc3cOfLVX)

let me know if it helps in your issue

LegendBug (Issue Creator) on (2024-10-09 09:15:38 UTC): Thank you for your suggestion! Reshaping the input and kernel tensors to 4D indeed resolves the issue for this specific case. However, my concern is more about TensorFlow's robustness. Ideally, when passing invalid input shapes to tf.nn.conv2d, TensorFlow should raise a catchable Python exception (e.g., ValueError or TypeError) rather than terminating the process. This would allow for better error handling without the program crashing.

Let me know if this makes sense, and thank you again for your help!

AdvaitDongre on (2024-10-09 11:35:20 UTC): You're very right. Currently, TensorFlow's behavior of terminating the process with a non-catchable error (rather than raising a Python exception like ValueError or TypeError) prevents graceful error handling. A more robust approach would be to throw a catchable Python exception that informs the user of invalid tensor shapes without crashing the entire program. i think you should tag someone from their team

tilakrayal (Assginee) on (2024-10-17 06:29:56 UTC): @LegendBug,
I tried to execute the mentioned code on tensorflow v2.17 & tf-nightly and observed that it was providing the error **convolution input must be 4-dimensional: [1,1,3] [Op:Conv2D]** rather than the crash/abort. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/8a28644e180fadcee90b639f9da1febe/untitled2180.ipynb) and confirm whether you are also facing the same. Thank you!

LegendBug (Issue Creator) on (2024-10-17 09:02:11 UTC): Thank you for your response.

Using Colab's built-in configuration (CPU + Python 3), I indeed observed the expected error message. However, when I connect Colab to my local device, the program still crashes unexpectedly.

You can see my Colab runtime records [here](https://colab.research.google.com/gist/LegendBug/74c8bbacbf157a5781d33eea9ce8c7fd/untitled2180.ipynb). I attempted to run two similar programs—the first one produced output normally, while the second one caused a crash. Since Colab seems to disconnect sessions on crashes, the output is not complete, attached is my complete output at runtime locally.
[output.log](https://github.com/user-attachments/files/17409204/output.log)

Please let me know if there's any additional information needed. Thank you again for your assistance!

"
2574652316,issue,closed,completed,How is TensorFlow tested before release?,"Hello，TensorFlow is a very good deep learning framework. I would like to ask, **how is TensorFlow tested after adding new features? What are the test cases and testing strategies?**
Is there a testing process and testing report?
Looking forward to your response, thank you.",HLH13297997663,2024-10-09 03:42:34+00:00,['Venkat6871'],2024-11-01 02:07:02+00:00,2024-11-01 02:07:02+00:00,https://github.com/tensorflow/tensorflow/issues/77316,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2418615658, 'issue_id': 2574652316, 'author': 'Venkat6871', 'body': 'Hi **@HLH13297997663** ,\r\nApologies for the delay. Please refer to this [documentation](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) for guidance on how to contribute.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 17, 6, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436658107, 'issue_id': 2574652316, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 25, 2, 2, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157536, 'issue_id': 2574652316, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 1, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-17 06:15:32 UTC): Hi **@HLH13297997663** ,
Apologies for the delay. Please refer to this [documentation](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) for guidance on how to contribute.
Thank you!

github-actions[bot] on (2024-10-25 02:02:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-01 02:07:01 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2574304285,issue,open,,RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab with Python 3.10.12
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
v2.17.0

### 2. Code

```
import tensorflow as tf

saved_model_dir = '/content/saved_model'

num_calibration_steps = 100

input = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)
dummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)

def representative_dataset_gen():
    for _ in range(num_calibration_steps):
        yield [dummy_input, input] #model has 2 input tensors

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
  tf.lite.OpsSet.SELECT_TF_OPS
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

# Save the quantized model to a local file
with open('quantized_model.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```

### 3. Failure after conversion

After converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` 
```
interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
interpreter.allocate_tensors()
```
I get the following error:

```
RuntimeError                              Traceback (most recent call last)
[<ipython-input-7-b6b80a3bdf94>](https://localhost:8080/#) in <cell line: 6>()
      4 interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
      5 print(interpreter.get_input_details())
----> 6 interpreter.allocate_tensors()

[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py](https://localhost:8080/#) in allocate_tensors(self)
    535   def allocate_tensors(self):
    536     self._ensure_safe()
--> 537     return self._interpreter.AllocateTensors()
    538 
    539   def _safe_to_run(self):

RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.
```

Could someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. 
The closest to this error is this [issue](https://github.com/tensorflow/tensorflow/issues/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.
",isuchy,2024-10-08 22:02:42+00:00,"['nutsiepully', 'gaikwadrahul8', 'pkgoogle']",2024-11-05 18:34:21+00:00,,https://github.com/tensorflow/tensorflow/issues/77293,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2402489803, 'issue_id': 2574304285, 'author': 'gaikwadrahul8', 'body': ""Hi, @isuchy\r\n\r\nThank you for bringing this issue to our attention, if possible could you please help us with your **saved_model** or Google colab notebook which you used to get `saved_model` so I'll try to replicate the same behavior from my end and will help you further ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 9, 14, 21, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402558021, 'issue_id': 2574304285, 'author': 'isuchy', 'body': ""Hello @gaikwadrahul8,\r\n\r\nThank you for your fast response.\r\nI've uploaded the [saved_model](https://drive.google.com/drive/folders/1mFCValz21_xAB0tQMYWrPs1LKYmPq9Jk?usp=sharing), please try in your environment to see if you can reproduce the error."", 'created_at': datetime.datetime(2024, 10, 9, 14, 48, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402715278, 'issue_id': 2574304285, 'author': 'gaikwadrahul8', 'body': ""Hi, @isuchy \r\n\r\nThank you for providing the **saved_model** access, I'm able to reproduce the same error `RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.` from my end and will need to dig more into this issue for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/914c71ec05378080caaed72b2ce9cb08/test-77293.ipynb)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 9, 15, 54, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411234446, 'issue_id': 2574304285, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 10, 14, 13, 13, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2425246103, 'issue_id': 2574304285, 'author': 'isuchy', 'body': 'Hi, @gaikwadrahul8, @pkgoogle\r\nhave you had a chance to check it out? Does it look like a more serious problem with the XNNPACK runtime implementation?', 'created_at': datetime.datetime(2024, 10, 20, 22, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430388598, 'issue_id': 2574304285, 'author': 'pkgoogle', 'body': 'Hi @isuchy, can you convert to a pytorch model and use [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch)? Let us know if that works better for you.', 'created_at': datetime.datetime(2024, 10, 22, 21, 55, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439050744, 'issue_id': 2574304285, 'author': 'AtomicCactus', 'body': 'I\'m running into something very similar with a simple MobileNetV3 based model:\r\n\r\n```\r\nException has occurred: RuntimeError\r\nfailed to create XNNPACK runtimeNode number 230 (TfLiteXNNPackDelegate) failed to prepare.\r\n  File ""tflite_test.py"", line 20, in <module>\r\n    interpreter.allocate_tensors()\r\nRuntimeError: failed to create XNNPACK runtimeNode number 230 (TfLiteXNNPackDelegate) failed to prepare.\r\n```\r\n\r\nGoing to give ai-edge-torch a shot in the meantime..\r\n\r\n> UPDATE: Nope, ai-edge-torch doesn\'t support int8 with a calibration dataset. So we\'re a bit stuck..', 'created_at': datetime.datetime(2024, 10, 25, 23, 49, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442268624, 'issue_id': 2574304285, 'author': 'pkgoogle', 'body': 'Hi @isuchy, I ran your script and I noticed this model needs flexops... in what runtime environment are you running inference? Can you ensure your have followed direction here? https://ai.google.dev/edge/litert/models/ops_select', 'created_at': datetime.datetime(2024, 10, 28, 17, 59, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445389652, 'issue_id': 2574304285, 'author': 'isuchy', 'body': ""Hi @pkgoogle,\r\n\r\n> I ran your script and I noticed this model needs flexops... in what runtime environment are you running inference? Can you ensure your have followed direction here? https://ai.google.dev/edge/litert/models/ops_select\r\n\r\nI ran the model inference in Google Colab and as it says here [https://ai.google.dev/edge/litert/models/ops_select#python](https://ai.google.dev/edge/litert/models/ops_select#python), for LiteRT runtime it is enough to have the TensorFlow package installed. But later I would like to deploy the model on a Raspberry Pi 5, running on Python as well.\r\n\r\n\r\n> Hi @isuchy, can you convert to a pytorch model and use [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch)? Let us know if that works better for you.\r\n\r\nI also tried ai_edge_torch as you recommended, it worked well for converting from the Pytorch model, but its output is directly the TFLite format (.tflite), with no way to set parameters to perform post-training int8 quantization. If I wanted to use tf.lite.TFLiteConverter, it does not support creating a TFLiteConverter object from a TFLite format model, only the following [methods](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#methods) are supported \r\n- experimental_from_jax\r\n- from_concrete_functions\r\n- from_keras_model\r\n- from_saved_model\r\n\r\nSo I haven't actually figured out how to quantize a float32 .tflite model to be able to perform inference on that model."", 'created_at': datetime.datetime(2024, 10, 29, 21, 45, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447967261, 'issue_id': 2574304285, 'author': 'pkgoogle', 'body': 'I was also able to replicate on tf-nightly [here](https://colab.sandbox.google.com/gist/pkgoogle/23805e1949f1c4b9848030df54322841/tf_77293.ipynb).\r\n\r\n@nutsiepully, can you please take a look? Thanks.\r\n\r\n@isuchy For the ai-edge-torch workflow, you would need to do TFL quantization [here](https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md#quantization).', 'created_at': datetime.datetime(2024, 10, 30, 18, 7, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455813854, 'issue_id': 2574304285, 'author': 'isuchy', 'body': 'Hi @pkgoogle,\r\n\r\nI tried the quantization as you advised in your comment and I was also inspired by this [article](https://medium.com/axinc-ai/convert-models-from-pytorch-to-tflite-with-ai-edge-torch-0e85623f8d56).\r\n\r\n> @isuchy For the ai-edge-torch workflow, you would need to do TFL quantization [here](https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md#quantization).\r\n\r\n```\r\nimport torch\r\nimport torchvision\r\nimport ai_edge_torch\r\n\r\nfrom ai_edge_torch.quantize import pt2e_quantizer\r\nfrom ai_edge_torch.quantize import quant_config\r\nfrom torch.ao.quantization import quantize_pt2e\r\n\r\npretrained_weights = torch.load(\'/content/model.pth\', weights_only=True)\r\nresnet50 = torchvision.models.resnet50(weights=pretrained_weights)\r\nsample_inputs = (torch.randn(1, 3, 640, 640),)\r\n\r\nquantizer = pt2e_quantizer.PT2EQuantizer().set_global(\r\n    pt2e_quantizer.get_symmetric_quantization_config()\r\n)\r\ngraphModule = torch.export.export(resnet50.eval(), sample_inputs).module()\r\nmodel = quantize_pt2e.prepare_pt2e(graphModule, quantizer)\r\n\r\ntorch.ao.quantization.move_exported_model_to_eval(model)\r\n\r\nmodel = quantize_pt2e.convert_pt2e(model, fold_quantize=False)\r\n\r\nwith_quantizer = ai_edge_torch.convert(\r\n    model,\r\n    sample_inputs,\r\n    quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),\r\n)\r\nwith_quantizer.export(""resnet50_int8.tflite"")\r\n```\r\nThe model is saved as a tflite file after conversion and quantization, but several warnings are raised during the run. \r\n\r\n```\r\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for \'weights\' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\r\n  warnings.warn(msg)\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_8) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_9) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_10) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_11) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_12) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_13) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_14) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_15) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_16) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_17) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_18) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_19) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_20) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_21) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_22) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_23) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_24) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_25) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_26) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_27) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_28) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_29) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_30) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_31) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_32) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_33) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_34) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_35) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_36) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_37) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_38) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_39) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_40) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_41) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_42) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_43) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_44) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_45) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_46) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_47) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_48) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_49) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_50) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_51) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_52) on an already erased node\r\n  warnings.warn(f""erase_node({to_erase}) on an already erased node"")\r\n/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \r\n  warnings.warn(\r\nWARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\r\n```\r\n\r\nI don\'t understand the warning message that the model is converted in training mode. I think the model is set in evaluation mode by calling `resnet50.eval()`. \r\nEven calling `torch.ao.quantization.move_exported_model_to_eval(model)` should ensure that the GrapModule is exported to the eval mode.\r\nI know this is out of the question, but could you please give me some hints, if you have any experience with that, why is it like it happens? Thank you.', 'created_at': datetime.datetime(2024, 11, 4, 22, 17, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457896370, 'issue_id': 2574304285, 'author': 'pkgoogle', 'body': 'Hi @isuchy, as far as I can tell you did it correctly, I think this is just a bug for now.', 'created_at': datetime.datetime(2024, 11, 5, 18, 34, 20, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-09 14:21:48 UTC): Hi, @isuchy

Thank you for bringing this issue to our attention, if possible could you please help us with your **saved_model** or Google colab notebook which you used to get `saved_model` so I'll try to replicate the same behavior from my end and will help you further ? 

Thank you for your cooperation and patience.

isuchy (Issue Creator) on (2024-10-09 14:48:30 UTC): Hello @gaikwadrahul8,

Thank you for your fast response.
I've uploaded the [saved_model](https://drive.google.com/drive/folders/1mFCValz21_xAB0tQMYWrPs1LKYmPq9Jk?usp=sharing), please try in your environment to see if you can reproduce the error.

gaikwadrahul8 (Assginee) on (2024-10-09 15:54:17 UTC): Hi, @isuchy 

Thank you for providing the **saved_model** access, I'm able to reproduce the same error `RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.` from my end and will need to dig more into this issue for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/914c71ec05378080caaed72b2ce9cb08/test-77293.ipynb)

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-14 13:13:29 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you.

isuchy (Issue Creator) on (2024-10-20 22:01:53 UTC): Hi, @gaikwadrahul8, @pkgoogle
have you had a chance to check it out? Does it look like a more serious problem with the XNNPACK runtime implementation?

pkgoogle (Assginee) on (2024-10-22 21:55:53 UTC): Hi @isuchy, can you convert to a pytorch model and use [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch)? Let us know if that works better for you.

AtomicCactus on (2024-10-25 23:49:02 UTC): I'm running into something very similar with a simple MobileNetV3 based model:

```
Exception has occurred: RuntimeError
failed to create XNNPACK runtimeNode number 230 (TfLiteXNNPackDelegate) failed to prepare.
  File ""tflite_test.py"", line 20, in <module>
    interpreter.allocate_tensors()
RuntimeError: failed to create XNNPACK runtimeNode number 230 (TfLiteXNNPackDelegate) failed to prepare.
```

Going to give ai-edge-torch a shot in the meantime..

pkgoogle (Assginee) on (2024-10-28 17:59:39 UTC): Hi @isuchy, I ran your script and I noticed this model needs flexops... in what runtime environment are you running inference? Can you ensure your have followed direction here? https://ai.google.dev/edge/litert/models/ops_select

isuchy (Issue Creator) on (2024-10-29 21:45:58 UTC): Hi @pkgoogle,


I ran the model inference in Google Colab and as it says here [https://ai.google.dev/edge/litert/models/ops_select#python](https://ai.google.dev/edge/litert/models/ops_select#python), for LiteRT runtime it is enough to have the TensorFlow package installed. But later I would like to deploy the model on a Raspberry Pi 5, running on Python as well.



I also tried ai_edge_torch as you recommended, it worked well for converting from the Pytorch model, but its output is directly the TFLite format (.tflite), with no way to set parameters to perform post-training int8 quantization. If I wanted to use tf.lite.TFLiteConverter, it does not support creating a TFLiteConverter object from a TFLite format model, only the following [methods](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#methods) are supported 
- experimental_from_jax
- from_concrete_functions
- from_keras_model
- from_saved_model

So I haven't actually figured out how to quantize a float32 .tflite model to be able to perform inference on that model.

pkgoogle (Assginee) on (2024-10-30 18:07:33 UTC): I was also able to replicate on tf-nightly [here](https://colab.sandbox.google.com/gist/pkgoogle/23805e1949f1c4b9848030df54322841/tf_77293.ipynb).

@nutsiepully, can you please take a look? Thanks.

@isuchy For the ai-edge-torch workflow, you would need to do TFL quantization [here](https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md#quantization).

isuchy (Issue Creator) on (2024-11-04 22:17:28 UTC): Hi @pkgoogle,

I tried the quantization as you advised in your comment and I was also inspired by this [article](https://medium.com/axinc-ai/convert-models-from-pytorch-to-tflite-with-ai-edge-torch-0e85623f8d56).


```
import torch
import torchvision
import ai_edge_torch

from ai_edge_torch.quantize import pt2e_quantizer
from ai_edge_torch.quantize import quant_config
from torch.ao.quantization import quantize_pt2e

pretrained_weights = torch.load('/content/model.pth', weights_only=True)
resnet50 = torchvision.models.resnet50(weights=pretrained_weights)
sample_inputs = (torch.randn(1, 3, 640, 640),)

quantizer = pt2e_quantizer.PT2EQuantizer().set_global(
    pt2e_quantizer.get_symmetric_quantization_config()
)
graphModule = torch.export.export(resnet50.eval(), sample_inputs).module()
model = quantize_pt2e.prepare_pt2e(graphModule, quantizer)

torch.ao.quantization.move_exported_model_to_eval(model)

model = quantize_pt2e.convert_pt2e(model, fold_quantize=False)

with_quantizer = ai_edge_torch.convert(
    model,
    sample_inputs,
    quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),
)
with_quantizer.export(""resnet50_int8.tflite"")
```
The model is saved as a tflite file after conversion and quantization, but several warnings are raised during the run. 

```
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_8) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_9) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_10) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_11) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_12) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_13) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_14) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_15) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_16) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_17) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_18) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_19) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_20) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_21) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_22) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_23) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_24) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_25) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_26) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_27) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_28) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_29) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_30) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_31) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_32) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_33) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_34) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_35) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_36) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_37) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_38) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_39) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_40) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_41) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_42) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_43) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_44) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_45) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_46) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_47) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_48) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_49) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_50) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_51) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_52) on an already erased node
  warnings.warn(f""erase_node({to_erase}) on an already erased node"")
/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point 
  warnings.warn(
WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.
```

I don't understand the warning message that the model is converted in training mode. I think the model is set in evaluation mode by calling `resnet50.eval()`. 
Even calling `torch.ao.quantization.move_exported_model_to_eval(model)` should ensure that the GrapModule is exported to the eval mode.
I know this is out of the question, but could you please give me some hints, if you have any experience with that, why is it like it happens? Thank you.

pkgoogle (Assginee) on (2024-11-05 18:34:20 UTC): Hi @isuchy, as far as I can tell you did it correctly, I think this is just a bug for now.

"
2574149429,issue,closed,completed,ValueError Failued to convert a NumPy array to a tensor... using Embedding,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Deep Learning OSS Nvidia Driver AMI GPU TensorFlow 2.15 (Ubuntu 20.04) 20240319

### Current behavior?

When attempting to train a model with an embedding, I get the error: 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).

An example dataset demonstrating the problem is attached.

Additional Information:

OS: Ubuntu 20.04.6 LTS (Deep Learning OSS Nvidia Driver AMI GPU TensorFlow 2.15 (Ubuntu 20.04) 20240319)
TensorFlow Version: 2.15.0
NumPy: 1.26.4
Pandas: 2.2.3
Python: 3.10.13

[training_labels.csv](https://github.com/user-attachments/files/17298913/training_labels.csv)
[training.csv](https://github.com/user-attachments/files/17298917/training.csv)

I hunted around a lot to try to sort this out. In my initial effort, I could see the field was coming in as a string. I added the converter shown and it appears from the outputs that's now as expected. Everything else that I could find on this error suggested using dtype=np.float32 in the read_csv() call to cast the data, but I've done that without success.


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import tensorflow as tf
import pandas as pd
import numpy as np


def convert(item):
    item = item[1:-1]    # remove `[ ]`
    item = item.strip()  # remove spaces at the end
    item = np.fromstring(item, sep=' ')  # convert string to `numpy.array`
    return item

print(""TensorFlow Version: ""+tf.__version__)

X_train = pd.read_csv('training.csv',converters={'fld7':convert})
y_train = pd.read_csv('training_labels.csv')
print(f""{X_train.shape=}"")
print(f""{y_train.shape=}"")

x_row=X_train.iloc[0]
x_val=x_row['fld7']
print(f""{type(x_row)=} {x_row=}"")
print(f""{type(x_val)=} {x_val=}"")


model = tf.keras.Sequential([
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dense(2048, activation='relu'),
    tf.keras.layers.Dense(2048, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss=tf.keras.losses.binary_crossentropy,
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]
    )

history = model.fit(X_train, y_train, epochs=25)
```


### Relevant log output

```shell
TensorFlow Version: 2.15.0
X_train.shape=(10, 134)
y_train.shape=(10, 1)
type(x_row)=<class 'pandas.core.series.Series'> x_row=fld0      0.571351
fld1             1
fld2             1
fld3             0
fld4             0
            ...   
fld129           0
fld130           0
fld131           0
fld132           0
fld133           0
Name: 0, Length: 134, dtype: object
type(x_val)=<class 'numpy.ndarray'> x_val=array([0.0756, 0.0756, 0.1176, 0.0672, 0.0588, 0.0756, 0.0672, 0.0504,
       0.0336, 0.1008, 0.0252, 0.0252, 0.0252, 0.0672, 0.0252, 0.0252,
       0.0168, 0.0084, 0.    , 0.    , 0.    , 0.0084, 0.0084, 0.    ,
       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,
       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.042 ])

2024-10-08 20:14:52.337621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-10-08 20:14:52.339066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20723 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2024-10-08 20:14:52.339066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20723 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
Traceback (most recent call last):
  File ""/home/ubuntu/new_model/testcase.py"", line 44, in <module>
    history = model.fit(X_train, y_train, epochs=25)
  File ""/opt/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/tensorflow/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py"", line 103, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
```
",gsexton,2024-10-08 20:32:11+00:00,['Venkat6871'],2024-10-11 20:05:37+00:00,2024-10-11 20:05:33+00:00,https://github.com/tensorflow/tensorflow/issues/77283,"[('type:bug', 'Bug'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2406585657, 'issue_id': 2574149429, 'author': 'Venkat6871', 'body': 'Hi **@gsexton** ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow v2.15.0 and encountered the same issue. Additionally, I tested with the latest versions and faced a different issue, but I found an alternative solution that worked for me. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e4ed8002e59627d6f6ef875542f43752/77283_tf-2-15-2-17-nightly-v.ipynb) here for your reference. Please try using the latest versions as they tend to provide more accurate results.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 11, 5, 34, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408053916, 'issue_id': 2574149429, 'author': 'gsexton', 'body': '@Venkat6871 Thank you so much for your help. I genuinely appreciate it. I incorporated your suggestion and it works well.', 'created_at': datetime.datetime(2024, 10, 11, 20, 5, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408054105, 'issue_id': 2574149429, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77283"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77283"">No</a>', 'created_at': datetime.datetime(2024, 10, 11, 20, 5, 35, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-11 05:34:08 UTC): Hi **@gsexton** ,
Apologies for the delay. I tried running your code on Colab using TensorFlow v2.15.0 and encountered the same issue. Additionally, I tested with the latest versions and faced a different issue, but I found an alternative solution that worked for me. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e4ed8002e59627d6f6ef875542f43752/77283_tf-2-15-2-17-nightly-v.ipynb) here for your reference. Please try using the latest versions as they tend to provide more accurate results.

Thank you!

gsexton (Issue Creator) on (2024-10-11 20:05:27 UTC): @Venkat6871 Thank you so much for your help. I genuinely appreciate it. I incorporated your suggestion and it works well.

google-ml-butler[bot] on (2024-10-11 20:05:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77283"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77283"">No</a>

"
2573482429,issue,open,,fatal error: 'NEON_2_SSE.h' file not found - macOS x86_64 build tensorflowlite_c library,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

AppleClang 15.0.0.15000309

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When trying to build the tensorflowlite_c lib according to [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md) guide, the build fails on macOS x86_64 plattform with the following error code:
```output
 In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/common.h:35:
/Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""  // IWYU pragma: export
         ^~~~~~~~~~~~~~
1 error generated.
```

### Standalone code to reproduce the issue

The error happens when I build using github workflows. [This](https://github.com/faressc/tflite-c-lib/actions/runs/11238016239/job/31241774171) is the action run.

### Relevant log output

```shell
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/common.h:35:
/Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""  // IWYU pragma: export
         ^~~~~~~~~~~~~~
1 error generated.
```
",faressc,2024-10-08 15:02:52+00:00,"['terryheo', 'gaikwadrahul8', 'pkgoogle']",2024-11-07 19:07:48+00:00,,https://github.com/tensorflow/tensorflow/issues/77264,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2402573189, 'issue_id': 2573482429, 'author': 'gaikwadrahul8', 'body': ""Hi, @faressc \r\n\r\nThank you for bringing this issue to our attention, I'm using below versions and steps but I'm getting different error if possible could you please help me with your versions and exact steps which you're following from official documentation which will be helpful to replicate same behavior from our end ?\r\n\r\n**Below are the versions I'm using:**\r\n\r\n```\r\nbash-3.2$ bazel --version\r\nbazel 7.3.1-homebrew\r\nbash-3.2$ xcode-select -p\r\n/Applications/Xcode.app/Contents/Developer\r\nbash-3.2$ xcodebuild -version\r\nXcode 16.0\r\nBuild version 16A242d\r\nbash-3.2$ cmake --version\r\ncmake version 3.30.2\r\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n```\r\n\r\n**I am following below steps :**\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\nmkdir tflite_build\r\ncd tflite_build\r\ncmake ../tensorflow_src/tensorflow/lite/c\r\ncmake --build . -j\r\n```\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 9, 14, 54, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408520266, 'issue_id': 2573482429, 'author': 'faressc', 'body': 'Hi there,\r\nThank you for your quick response! I am building tflite with the github runners. You can inspect all steps here: https://github.com/faressc/tflite-c-lib in the workflow file. Here is the versions the runner is using:\r\n```output\r\n/Applications/Xcode_15.4.app/Contents/Developer\r\nXcode 15.4\r\nBuild version 15F31d\r\ncmake version 3.30.4\r\n\r\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n```\r\n\r\nBazel is not installed I guess, but I am using only cmake anyway (or does it need bazel under the hood?).\r\n\r\nHere is what steps I am following on the runner:\r\n\r\n```bash\r\n# clone and checkout actions\r\nmkdir ${{ env.BUILD_DIR }}\r\ncd ${{ env.BUILD_DIR }}\r\ncmake ../tensorflow/tensorflow/lite/c -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_OSX_ARCHITECTURES=x86_64\r\n# cd .. (because new step in github action)\r\ncmake --build ${{ env.BUILD_DIR }} --config Release --parallel 4\r\n```', 'created_at': datetime.datetime(2024, 10, 12, 10, 52, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435011455, 'issue_id': 2573482429, 'author': 'gaikwadrahul8', 'body': 'Hi, @faressc\r\n\r\nI apologize for the delayed response, I am able to replicate the same behavior from my end for reference I\'ve added error log below so we\'ll have to dig more into this issue and will update you, thank you for bringing this issue to our attention I really appreciate your valuable time and efforts\r\n\r\n**Here is error log for reference :**\r\n\r\n```\r\n[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/serialization.cc.o\r\n[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/telemetry.cc.o\r\n[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/utils.cc.o\r\n[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/xnnpack/quantization_util.cc.o\r\nIn file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:\r\nIn file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:\r\nIn file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/common.h:35:\r\n/Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: \'NEON_2_SSE.h\' file not found\r\n#include ""NEON_2_SSE.h""  // IWYU pragma: export\r\n         ^~~~~~~~~~~~~~\r\n1 error generated.\r\nmake[2]: *** [tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/xnnpack/quantization_util.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[1]: *** [tensorflow-lite/CMakeFiles/tensorflow-lite.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 24, 11, 24, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443691810, 'issue_id': 2573482429, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 29, 9, 30, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445068438, 'issue_id': 2573482429, 'author': 'pkgoogle', 'body': ""I'm not able to replicate on M1, most likely specific to x86_64. @terryheo, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 10, 29, 18, 43, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458567751, 'issue_id': 2573482429, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 6, 2, 0, 24, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-09 14:54:37 UTC): Hi, @faressc 

Thank you for bringing this issue to our attention, I'm using below versions and steps but I'm getting different error if possible could you please help me with your versions and exact steps which you're following from official documentation which will be helpful to replicate same behavior from our end ?

**Below are the versions I'm using:**

```
bash-3.2$ bazel --version
bazel 7.3.1-homebrew
bash-3.2$ xcode-select -p
/Applications/Xcode.app/Contents/Developer
bash-3.2$ xcodebuild -version
Xcode 16.0
Build version 16A242d
bash-3.2$ cmake --version
cmake version 3.30.2
CMake suite maintained and supported by Kitware (kitware.com/cmake).
```

**I am following below steps :**

```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir tflite_build
cd tflite_build
cmake ../tensorflow_src/tensorflow/lite/c
cmake --build . -j
```
If I have missed something here please let me know.

Thank you for your cooperation and patience.

faressc (Issue Creator) on (2024-10-12 10:52:41 UTC): Hi there,
Thank you for your quick response! I am building tflite with the github runners. You can inspect all steps here: https://github.com/faressc/tflite-c-lib in the workflow file. Here is the versions the runner is using:
```output
/Applications/Xcode_15.4.app/Contents/Developer
Xcode 15.4
Build version 15F31d
cmake version 3.30.4

CMake suite maintained and supported by Kitware (kitware.com/cmake).
```

Bazel is not installed I guess, but I am using only cmake anyway (or does it need bazel under the hood?).

Here is what steps I am following on the runner:

```bash
# clone and checkout actions
mkdir ${{ env.BUILD_DIR }}
cd ${{ env.BUILD_DIR }}
cmake ../tensorflow/tensorflow/lite/c -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_OSX_ARCHITECTURES=x86_64
# cd .. (because new step in github action)
cmake --build ${{ env.BUILD_DIR }} --config Release --parallel 4
```

gaikwadrahul8 (Assginee) on (2024-10-24 11:24:57 UTC): Hi, @faressc

I apologize for the delayed response, I am able to replicate the same behavior from my end for reference I've added error log below so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention I really appreciate your valuable time and efforts

**Here is error log for reference :**

```
[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/serialization.cc.o
[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/telemetry.cc.o
[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/utils.cc.o
[ 63%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/xnnpack/quantization_util.cc.o
In file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:
In file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:
In file included from /Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/common.h:35:
/Users/runner/work/TFLite-C-Build-issue/TFLite-C-Build-issue/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""  // IWYU pragma: export
         ^~~~~~~~~~~~~~
1 error generated.
make[2]: *** [tensorflow-lite/CMakeFiles/tensorflow-lite.dir/delegates/xnnpack/quantization_util.cc.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [tensorflow-lite/CMakeFiles/tensorflow-lite.dir/all] Error 2
make: *** [all] Error 2
```

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-29 09:30:27 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-29 18:43:51 UTC): I'm not able to replicate on M1, most likely specific to x86_64. @terryheo, can you please take a look? Thanks.

github-actions[bot] on (2024-11-06 02:00:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2572307114,issue,closed,completed,CUDA and cUDNN version compatability for tensorflow 2.17.0?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I wanted to check what is the cuda and cUDNN version compatibility required for tensorflow 2.17.0?

### Standalone code to reproduce the issue

```shell
I wanted to check what is the cuda and cUDNN version compatibility required for tensorflow 2.17.0?
```


### Relevant log output

_No response_",prajsri99,2024-10-08 07:15:25+00:00,['Venkat6871'],2024-10-24 02:01:43+00:00,2024-10-24 02:01:40+00:00,https://github.com/tensorflow/tensorflow/issues/77216,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2401464774, 'issue_id': 2572307114, 'author': 'Venkat6871', 'body': 'Hi **@prajsri99** ,\r\nThank you for raising your concern here. GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin. This is clearly mentioned in the [documentation](https://www.tensorflow.org/install/source_windows), so please review it for further details.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 6, 47, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418337244, 'issue_id': 2572307114, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 17, 2, 1, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073683, 'issue_id': 2572307114, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073750, 'issue_id': 2572307114, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77216"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77216"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-09 06:47:55 UTC): Hi **@prajsri99** ,
Thank you for raising your concern here. GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin. This is clearly mentioned in the [documentation](https://www.tensorflow.org/install/source_windows), so please review it for further details.
Thank you!

github-actions[bot] on (2024-10-17 02:01:33 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:40 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:01:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77216"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77216"">No</a>

"
2572273323,issue,open,,"tensorflow.python.ops.signal.dct_ops.dct aborts with ""Assertion failure no zero-sized FFTs""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)
Please find the [gist](https://colab.research.google.com/drive/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.eager import def_function
from tensorflow.python.framework import tensor_spec
from tensorflow.python.ops.signal import dct_ops

def test_with_dynamic_dimensions(dct_type, norm, shape, dtype):
    @def_function.function
    def func(signals):
        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)
    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)
    f = func.get_concrete_function(signals_spec)
    f(np.zeros([0], dtype=dtype))
test_with_dynamic_dimensions(3, None, [3], np.float32)
```


### Relevant log output

```shell
DUCC FFT c2r failed: 
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
",cybersupersoap,2024-10-08 06:57:49+00:00,['tilakrayal'],2024-10-10 11:58:07+00:00,,https://github.com/tensorflow/tensorflow/issues/77211,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2404894235, 'issue_id': 2572273323, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f897390b50f359265257d04757a1d1be/untitled2162.ipynb) and screenshot for the reference.\r\n\r\n![Screenshot 2024-10-10 5 27 40 PM](https://github.com/user-attachments/assets/2b9a8469-4a8e-4789-a591-074754a53ddb)', 'created_at': datetime.datetime(2024, 10, 10, 11, 57, 58, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-10 11:57:58 UTC): I was able to reproduce the issue on both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f897390b50f359265257d04757a1d1be/untitled2162.ipynb) and screenshot for the reference.

![Screenshot 2024-10-10 5 27 40 PM](https://github.com/user-attachments/assets/2b9a8469-4a8e-4789-a591-074754a53ddb)

"
2572259861,issue,open,,tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.core.example import example_pb2
from tensorflow.core.example import feature_pb2
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import parsing_ops
example = example_pb2.Example
feature = feature_pb2.Feature
features = lambda d: feature_pb2.Features(feature=d)
bytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))
int64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))
float_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))
feature_list = lambda l: feature_pb2.FeatureList(feature=l)
feature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)
sequence_example = example_pb2.SequenceExample

def testSequenceExampleListWithWrongShapeFails():
    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))
    serialized = original.SerializeToString()
    parsing_ops.parse_single_sequence_example(**
        ({
            'example_name': 'in1',
            'serialized': ops.convert_to_tensor(serialized),
            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}
        }))
testSequenceExampleListWithWrongShapeFails()
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
",cybersupersoap,2024-10-08 06:52:07+00:00,['Venkat6871'],2024-10-22 07:14:45+00:00,,https://github.com/tensorflow/tensorflow/issues/77210,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues')]","[{'comment_id': 2401400911, 'issue_id': 2572259861, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/eef861e6a49a875f537ac41fdb917098/77210_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 6, 14, 44, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-09 06:14:44 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/eef861e6a49a875f537ac41fdb917098/77210_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2571494657,issue,closed,completed,"Bazel installer not run on recent Bazel version, kindly provide widly accepted CMake installer","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Linux linux 6.11.2-arch1-1 #1 SMP PREEMPT_DYNAMIC Fri, 04 Oct 2024 21:51:11 +0000 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

bazel 7.3.2

### GCC/compiler version

gcc (GCC) 14.2.1 20240910

### CUDA/cuDNN version

CUDA Version: 12.6 

### GPU model and memory

NVIDIA GeForce RTX 3050 - 583MiB /   8192MiB

### Current behavior?

bazel is configured to version 6.5 on tensorflow git files that is not cabable to run because current version is 7.3.2
please provide CMake installer since this is a C based lib.

### Standalone code to reproduce the issue

```shell
$bazel build --config=/linux/tensorflow/runtime //tensorflow/tools/lib_package:libtensorflow
ERROR: The project you're trying to build requires Bazel 6.5.0 (specified in /linux/tensorflow/.bazelversion), but it wasn't found in /usr/bin.

Bazel binaries for all official releases can be downloaded from here:
  https://github.com/bazelbuild/bazel/releases

Please put the downloaded Bazel binary into this location:
  /usr/bin/bazel-6.5.0-linux-x86_64
```


### Relevant log output

_No response_",Reyadeyat,2024-10-07 21:16:47+00:00,['tilakrayal'],2024-10-24 02:01:42+00:00,2024-10-24 02:01:42+00:00,https://github.com/tensorflow/tensorflow/issues/77176,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2402263747, 'issue_id': 2571494657, 'author': 'tilakrayal', 'body': '@Reyadeyat,\r\nAs per the official documentation, the latest tensorflow v2.17 supports the Bazel 6.5.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations\r\nhttps://www.tensorflow.org/install/source#gpu\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 13, 0, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418337278, 'issue_id': 2571494657, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 17, 2, 1, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073728, 'issue_id': 2571494657, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-09 13:00:51 UTC): @Reyadeyat,
As per the official documentation, the latest tensorflow v2.17 supports the Bazel 6.5.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations
https://www.tensorflow.org/install/source#gpu

Thank you!

github-actions[bot] on (2024-10-17 02:01:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:42 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2571219230,issue,open,,`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data,"I've noticed this hitting on our ubsan builds recently:

```
../../third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null
../../build/linux/debian_bullseye_amd64-sysroot/usr/include/string.h:44:28: note: nonnull attribute specified here
    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:3
    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/lite/string_util.cc:37:28
    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/task_utils.h:125:13
    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/processor/universal_sentence_encoder_preprocessor.cc:58:3
    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:174:25
    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/base_task_api.h:146:5
    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:169:10
    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave/components/ai_chat/core/browser/text_embedder.cc:271:49
    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave/components/ai_chat/core/browser/text_embedder.cc:287:19
    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave/components/ai_chat/core/browser/text_embedder_unittest.cc:67:58
    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base/functional/callback.h:156:12
    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base/task/common/task_annotator.cc:202:34
    #12 0x5a36d49dcfe9 in RunTask<(lambda at ../../base/task/thread_pool/task_tracker.cc:678:35)> base/task/common/task_annotator.h:90:5
    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:677:19
    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:662:3
    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/task/thread_pool/task_tracker.cc:520:5
    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/test/task_environment.cc:1028:46
    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base/task/thread_pool/task_tracker.cc:415:5
    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base/task/thread_pool/worker_thread.cc:493:36
    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base/task/thread_pool/worker_thread.cc:379:3
    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base/task/thread_pool/worker_thread.cc:359:7
    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base/threading/platform_thread_posix.cc:101:13
    #22 0x7695b109ca93 in start_thread nptl/pthread_create.c:447:8
    #23 0x7695b1129c3b in clone3 misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78
```",cdesouza-chromium,2024-10-07 18:57:35+00:00,['Venkat6871'],2024-10-22 06:04:44+00:00,,https://github.com/tensorflow/tensorflow/issues/77168,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('awaiting PR merge', 'awaiting PR merge')]","[{'comment_id': 2423819758, 'issue_id': 2571219230, 'author': 'cdesouza-chromium', 'body': '@Venkat6871 do you think you could help me getting this PR reviewed?', 'created_at': datetime.datetime(2024, 10, 19, 12, 34, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428332977, 'issue_id': 2571219230, 'author': 'Venkat6871', 'body': 'Hi **@cdesouza-chromium** ,\r\nApologies for the delay. Your PR is currently under review. Once it is merged, your issue should be resolved.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 22, 6, 4, 43, tzinfo=datetime.timezone.utc)}]","cdesouza-chromium (Issue Creator) on (2024-10-19 12:34:06 UTC): @Venkat6871 do you think you could help me getting this PR reviewed?

Venkat6871 (Assginee) on (2024-10-22 06:04:43 UTC): Hi **@cdesouza-chromium** ,
Apologies for the delay. Your PR is currently under review. Once it is merged, your issue should be resolved.
Thank you!

"
2570913882,issue,open,,Failed to build `tensorflow_cc` in Windows when linking,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

6.5.0

### GCC/compiler version

Clang 18.1.7

### CUDA/cuDNN version

No

### GPU model and memory

_No response_

### Current behavior?

I built the C++ library in Windows with LLVM/Clang 18.1.7 by command:
```bash
bazel build --config=release_cpu_windows --config=win_clang //tensorflow:tensorflow_cc
```
All compilation works well, but linking at the final task failed. It seems a lot of symbols lost when linking, such as `Session`, `SavedModelBundleInterface`, et al. They were all basic functions or classes and should not be missed.

CUDA was excluded.

I have tried many LLVM/Clang versions from 17 to 19. Looks like it has nothing to do with the compiler version.

### Standalone code to reproduce the issue

```shell
bazel build --config=release_cpu_windows --config=win_clang //tensorflow:tensorflow_cc
```


### Relevant log output

```shell
D:/tensorflow/tensorflow/BUILD:1316:21: Linking tensorflow/tensorflow_cc.dll failed: (Exit 1): lld-link.exe failed: error executing command (from target //tensorflow:tensorflow_cc.dll) 
  cd /d D:/output_base/execroot/org_tensorflow
  SET CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe
    SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.26100.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.26100.0\\um\x64;C:\Program Files\LLVM\lib\clang\18\lib\windows
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\DiagnosticsHub\Collector;C:\Program Files (x86)\Windows Kits\10\bin\10.0.26100.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\vcpkg
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python312/python.exe
    SET PYTHON_LIB_PATH=C:/Python312/Lib/site-packages
    SET TEMP=E:\tmp
    SET TF2_BEHAVIOR=1
    SET TMP=E:\tmp
  C:\Program Files\LLVM\bin\lld-link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params
# Configuration: ee29c35b2efb4ddb1fe39799ba1e7aae463cd78c5b2bb106c9b875ad299c989f
# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: duplicate symbol: TF_DataTypeSize
>>> defined at tf_datatype.lo.lib(tf_datatype.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewBuffer
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_GetCode
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_Message
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewBufferFromString
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_SetStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteBuffer
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorData
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_SetPayload
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NumDims
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_Dim
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_AllocateTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorBitcastFrom
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorElementCount
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_ForEachPayload
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: error: <root>: undefined symbol: class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)
lld-link: error: <root>: undefined symbol: public: virtual __cdecl tensorflow::SavedModelBundleInterface::~SavedModelBundleInterface(void)
lld-link: error: <root>: undefined symbol: bool __cdecl tensorflow::MaybeSavedModelDirectory(class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>> const &)
lld-link: error: <root>: undefined symbol: int `private: static class lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::btree_node<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>> * __cdecl absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNode(void)'::`2'::$TSS0
lld-link: error: <root>: undefined symbol: `public: class std::unique_ptr<class tensorflow::RunHandler, struct std::default_delete<class tensorflow::RunHandler>> __cdecl tensorflow::RunHandlerPool::Impl::Get(__int64, __int64, class tensorflow::RunOptions_Experimental_RunHandlerPoolOptions const &)'::`2'::`local static thread guard'{2}
lld-link: error: <root>: undefined symbol: class std::unordered_map<enum tensorflow::DataType, enum tensorflow::FullTypeId, struct tensorflow::DataTypeHasher, struct std::equal_to<enum tensorflow::DataType>, class std::allocator<struct std::pair<enum tensorflow::DataType const, enum tensorflow::FullTypeId>>> *tensorflow::DT_TO_FT
lld-link: error: <root>: undefined symbol: private: static class std::vector<class tsl::core::RefCountPtr<class tensorflow::Rendezvous>, class std::allocator<class tsl::core::RefCountPtr<class tensorflow::Rendezvous>>> &tensorflow::LocalRendezvous::aborted_rendezs_
lld-link: error: <root>: undefined symbol: private: static class tsl::mutex &tensorflow::LocalRendezvous::aborted_rendezs_mu_
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<2> *tensorflow::metrics::eager_client_error_counter
lld-link: error: <root>: undefined symbol: struct absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNodeType *`private: static class absl::lts_20230802::container_internal::btree_node<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>> * __cdecl absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNode(void)'::`2'::empty_node
lld-link: error: <root>: undefined symbol: private: static class tsl::mutex tensorflow::tfdbg::DebugEventsWriter::factory_mu_
lld-link: error: <root>: undefined symbol: char const *tensorflow::kDisableJitKernelsEnvVar
lld-link: error: <root>: undefined symbol: public: static __int64 tensorflow::CollectiveExecutor::kInvalidId
lld-link: error: <root>: undefined symbol: char const *tensorflow::kJitKernelLabel
lld-link: error: <root>: undefined symbol: public: static class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>> const tensorflow::LogMemory::kLogMemoryLabel
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<5> *tensorflow::metrics::mlir_bridge_first_phase_counter
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<1> *tensorflow::metrics::mlir_second_phase_count
lld-link: error: <root>: undefined symbol: void * (__cdecl *nsync::nsync_malloc_ptr_)(unsigned __int64)
lld-link: error: <root>: undefined symbol: struct nsync::lock_type_s *nsync::nsync_reader_type_
lld-link: error: <root>: undefined symbol: struct nsync::lock_type_s *nsync::nsync_writer_type_
lld-link: error: too many errors emitted, stopping now (use /errorlimit:0 to see all errors)
```
",sxlllslgh,2024-10-07 16:24:35+00:00,['tilakrayal'],2025-02-04 08:17:59+00:00,,https://github.com/tensorflow/tensorflow/issues/77156,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.18.rc', '')]","[{'comment_id': 2408242744, 'issue_id': 2570913882, 'author': 'tilakrayal', 'body': '@mraunak', 'created_at': datetime.datetime(2024, 10, 11, 23, 44, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408589753, 'issue_id': 2570913882, 'author': 'sxlllslgh', 'body': 'Some more information I found.\r\n\r\n**_Direct reason:_**\r\nThe exported symbol definition file are not complete. There are many reasons leading to this error:\r\n\r\n**Case 1:** Class `tensorflow::NewSession` exists in `tensorflow/core/common_runtime/session.cc`, which belongs to target `//tensorflow/core/common_runtime/session` in [`tensorflow/core/common_runtime/BUILD`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/BUILD#1829). But the target is not delcared in `tensorflow_cc` target in [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#1279) file (start about line 1307 in current master branch). It seems like many transitive dependencies must be declared manually. I\'m not sure, but I think it\'s due to the imperfections of bazel.\r\n\r\n**Case 2:** Class `tensorflow::SavedModelBundleInterface` exists in `master/tensorflow/cc/saved_model/loader.cc`, which belongs to target [`//tensorflow/cc/saved_model:loader`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/BUILD#104). This target have been added to the `roots` in [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#1279) file, but owing to no `srcs` declared in this target, no `loader.lib` will be generated, so the `link.exe` cannot find this symbol. By adding `srcs` attribute, this error will be disappeared.\r\n```\r\ncc_library(\r\n    name = ""loader"",\r\n    hdrs = [""loader.h""],\r\n    srcs = [""loader.cc""], // Add this line\r\n    deps = [\r\n    ...\r\n```\r\n\r\n**Case 3:** External dependency [`btree`](https://github.com/tensorflow/tensorflow/blob/master/third_party/absl/system.absl.container.BUILD#201) in `absl` missed. It seems bazel will not generate any `.lib` file of this target, even I have built abseil library by my self from its source. This is very strange: if this library is a header-only library, the `tensorflow_cc.dll` should not depend it; if not, bazel should generate `.lib` file for it.', 'created_at': datetime.datetime(2024, 10, 12, 14, 46, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439212456, 'issue_id': 2570913882, 'author': 'TheAsherbot', 'body': '@mraunak and @sxlllslgh Were either one of you able to fix this problem? If so, do you remember the steps to follow? Thank you for your time!', 'created_at': datetime.datetime(2024, 10, 26, 3, 15, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439217339, 'issue_id': 2570913882, 'author': 'ELundby45', 'body': ""I've ran into the exact same linking errors and unfortunately have not found a solution."", 'created_at': datetime.datetime(2024, 10, 26, 3, 20, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439713811, 'issue_id': 2570913882, 'author': 'sxlllslgh', 'body': ""> @mraunak and @sxlllslgh Were either one of you able to fix this problem? If so, do you remember the steps to follow? Thank you for your time! \n\nAs what I said above, adding all missed dependencies manually is a very very complex and time-consuming procedure. I only fix less than 10 errors within half a month! Further, if any modification of interfaces occurred in the future, the dependency might be updated manually too. It's ridiculous. I think the fundamental resolution is to hope bazel to compile all libs rather than only explicit dependencies."", 'created_at': datetime.datetime(2024, 10, 26, 19, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458264774, 'issue_id': 2570913882, 'author': 'mraunak', 'body': 'Hi, apologies for the delay. The issue stems from a missing configuration for Windows (see [tensorflow/tensorflow/BUILD#L1315](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L1315)). It works correctly on Linux, and I’m currently investigating the fix for Windows. I’ll have a solution soon.', 'created_at': datetime.datetime(2024, 11, 5, 22, 26, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496023057, 'issue_id': 2570913882, 'author': 'TheAsherbot', 'body': '@mraunak, By any chance were you able to solve this problem?', 'created_at': datetime.datetime(2024, 11, 24, 14, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498703885, 'issue_id': 2570913882, 'author': 'mraunak', 'body': 'Hi @TheAsherbot, @vam-google is working to fix the issue. we will have an update soon.', 'created_at': datetime.datetime(2024, 11, 25, 18, 5, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513750906, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@mraunak , waiting your package to fix this issue, we need to import works to windows platform, this is an obstacle.', 'created_at': datetime.datetime(2024, 12, 3, 7, 29, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516936662, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@sxlllslgh can u share with me a temp solution to build out tensorflow_cc.dll, thanks.', 'created_at': datetime.datetime(2024, 12, 4, 10, 48, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517468547, 'issue_id': 2570913882, 'author': 'sxlllslgh', 'body': ""> @sxlllslgh can u share with me a temp solution to build out tensorflow_cc.dll, thanks.\r\n\r\nSorry, I can't. As I said above, there are too many missed dependencies when building it. I'm also waiting for offical patches."", 'created_at': datetime.datetime(2024, 12, 4, 13, 52, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533857893, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@mraunak waiting for the patches.....', 'created_at': datetime.datetime(2024, 12, 11, 7, 9, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551949059, 'issue_id': 2570913882, 'author': 'mraunak', 'body': 'Hi @weijianghn,\r\n\r\nApologies for the delayed response.\r\n\r\nThe team working on this task has raised a few questions, as outlined below:\r\nWhy is there a need to build libtensorflow_cc on Windows?\r\nAs far as we know, this artifact is not available on Windows\r\nlibtensorflow_cc is typically a Linux-specific artifact, while on Windows, only pywrap_tensorflow_internal.dll is available.\r\nCould you provide some clarity on this?\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 12, 18, 17, 57, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552783440, 'issue_id': 2570913882, 'author': 'TheAsherbot', 'body': '@mraunak I am not weijianghn, but I thought I should give the reason why I am looking at using libtensorflow_cc on windows. I prefer to use visual studios as my c++ compiler. I also do not have a dedicated linux computer, though I do have Windows Subsystem for Linux which is limited on what it can do. If I am missing something that allows me to use libtensorflow_cc  easily on linux, please let me know.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 19, 4, 53, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553608140, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': ""> Hi @weijianghn,\r\n> \r\n> Apologies for the delayed response.\r\n> \r\n> The team working on this task has raised a few questions, as outlined below: Why is there a need to build libtensorflow_cc on Windows? As far as we know, this artifact is not available on Windows libtensorflow_cc is typically a Linux-specific artifact, while on Windows, only pywrap_tensorflow_internal.dll is available. Could you provide some clarity on this?\r\n> \r\n> Thanks!\r\nHi @mraunak \r\nWe want to deploy our product on windows platform,  we need to use c++ tensorflow API, that means we need libtensorflow_cc.  Actually, most of terminal device are based on windows platform.  We know keras is based on python, but we don't want to use python.  why the team doesn't want to support C++ anymore?\r\nThanks a lot\r\nBR"", 'created_at': datetime.datetime(2024, 12, 19, 12, 8, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572597487, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@mraunak  could you share with us the latest information about libtensorflow_cc? \r\nThanks a lot\r\nBR', 'created_at': datetime.datetime(2025, 1, 6, 8, 47, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573715481, 'issue_id': 2570913882, 'author': 'mihaimaruseac', 'body': ""So, the windows wheel is built slightly differently than on Linux and MacOS, due to mostly technical issues (at first, the Windows wheel was only offering the Python API whereas the other two were also offering some C++ and raw ops support -- this was due to maximum size limits when creating archives on Windows). As a result of that, not all targets can be compiled on every operating system.\r\n\r\nWe used to have some settings under `platform/` to error when an invalid target was being compiled but during migrations in the past few years those got lost. And, they might have also been incomplete.\r\n\r\nBut it is certain that `//tensorflow:tensorflow_cc` was not supposed to be a target that can be compiled under Windows.\r\n\r\nI would suggest either using a remote VM (could also be local, via VirtualBox, VMWare, etc.) that has a Linux operating system. You can then use Visual Studio to edit and build these files remotely.\r\n\r\nAlternatively, I think WSL should work, but I am not 100% certain. It's been multiple years since I last compiled TF on Windows.\r\n\r\nIf you need the full power of C++ API on Windows, I think sending some PRs to fix that issue would be accepted (you can tag me or @mraunak or both for review), but I am unsure if the OSS team at Google has cycles to work on this by itself."", 'created_at': datetime.datetime(2025, 1, 6, 18, 53, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591315475, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@sxlllslgh They need PRs to fix that issue.  Do you have any alternative method\n@mihaimaruseac  how to submit PR to OSS team', 'created_at': datetime.datetime(2025, 1, 14, 23, 31, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2593045267, 'issue_id': 2570913882, 'author': 'mihaimaruseac', 'body': 'In this repository, mostly. You might need to also look at tensorflow/build repository for some build configurations, though I think most of the stuff is now migrated here.', 'created_at': datetime.datetime(2025, 1, 15, 14, 42, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594377814, 'issue_id': 2570913882, 'author': 'weijianghn', 'body': '@mihaimaruseac  that means Google will not resolve build error in windows, right?', 'created_at': datetime.datetime(2025, 1, 16, 3, 5, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595865892, 'issue_id': 2570913882, 'author': 'mihaimaruseac', 'body': '@mraunak is helping with fixes for Windows build errors, but we cannot support all possible configurations. This issue is well outside of what we can support, sadly.', 'created_at': datetime.datetime(2025, 1, 16, 14, 24, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633183224, 'issue_id': 2570913882, 'author': 'werner-rammer', 'body': ""I'd like to add my use-case to the  discussion: in our simulation model we use the C++ version of TensorFlow for inference. The model itself is written in C++ and it uses the C++ API of TensorFlow (https://github.com/edfm-tum/SVD). I do think that tensorflow_cc.lib / tensorflow_cc.dll are relevant and important for many users! There are some research groups that want to use our model, but cannot use Linux servers.\n I was able to compile an old version on Windows (TF 1.4, very painful), and on Linux there is this fantastic repo (https://github.com/ika-rwth-aachen/libtensorflow_cc). @sxlllslgh is providing prebuilt binaries for Windows (many thanks!), but there seem to be some symbols missing (I try to use the 2.5.0 version).\nIn my case these are:\n```\ntensorflow::Scope::~Scope(void)\ntensorflow::Scope::NewRootScope(void)\ntensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const\ntensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const\ntensorflow::ops::TopK::TopK(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\ntensorflow::Scope::graph(void)const\n\n```\nI would really appreciate if a building on Windows would be fixed!"", 'created_at': datetime.datetime(2025, 2, 4, 8, 17, 57, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-11 23:44:21 UTC): @mraunak

sxlllslgh (Issue Creator) on (2024-10-12 14:46:46 UTC): Some more information I found.

**_Direct reason:_**
The exported symbol definition file are not complete. There are many reasons leading to this error:

**Case 1:** Class `tensorflow::NewSession` exists in `tensorflow/core/common_runtime/session.cc`, which belongs to target `//tensorflow/core/common_runtime/session` in [`tensorflow/core/common_runtime/BUILD`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/BUILD#1829). But the target is not delcared in `tensorflow_cc` target in [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#1279) file (start about line 1307 in current master branch). It seems like many transitive dependencies must be declared manually. I'm not sure, but I think it's due to the imperfections of bazel.

**Case 2:** Class `tensorflow::SavedModelBundleInterface` exists in `master/tensorflow/cc/saved_model/loader.cc`, which belongs to target [`//tensorflow/cc/saved_model:loader`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/BUILD#104). This target have been added to the `roots` in [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#1279) file, but owing to no `srcs` declared in this target, no `loader.lib` will be generated, so the `link.exe` cannot find this symbol. By adding `srcs` attribute, this error will be disappeared.
```
cc_library(
    name = ""loader"",
    hdrs = [""loader.h""],
    srcs = [""loader.cc""], // Add this line
    deps = [
    ...
```

**Case 3:** External dependency [`btree`](https://github.com/tensorflow/tensorflow/blob/master/third_party/absl/system.absl.container.BUILD#201) in `absl` missed. It seems bazel will not generate any `.lib` file of this target, even I have built abseil library by my self from its source. This is very strange: if this library is a header-only library, the `tensorflow_cc.dll` should not depend it; if not, bazel should generate `.lib` file for it.

TheAsherbot on (2024-10-26 03:15:26 UTC): @mraunak and @sxlllslgh Were either one of you able to fix this problem? If so, do you remember the steps to follow? Thank you for your time!

ELundby45 on (2024-10-26 03:20:30 UTC): I've ran into the exact same linking errors and unfortunately have not found a solution.

sxlllslgh (Issue Creator) on (2024-10-26 19:35:00 UTC): As what I said above, adding all missed dependencies manually is a very very complex and time-consuming procedure. I only fix less than 10 errors within half a month! Further, if any modification of interfaces occurred in the future, the dependency might be updated manually too. It's ridiculous. I think the fundamental resolution is to hope bazel to compile all libs rather than only explicit dependencies.

mraunak on (2024-11-05 22:26:17 UTC): Hi, apologies for the delay. The issue stems from a missing configuration for Windows (see [tensorflow/tensorflow/BUILD#L1315](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L1315)). It works correctly on Linux, and I’m currently investigating the fix for Windows. I’ll have a solution soon.

TheAsherbot on (2024-11-24 14:08:36 UTC): @mraunak, By any chance were you able to solve this problem?

mraunak on (2024-11-25 18:05:15 UTC): Hi @TheAsherbot, @vam-google is working to fix the issue. we will have an update soon.

weijianghn on (2024-12-03 07:29:15 UTC): @mraunak , waiting your package to fix this issue, we need to import works to windows platform, this is an obstacle.

weijianghn on (2024-12-04 10:48:08 UTC): @sxlllslgh can u share with me a temp solution to build out tensorflow_cc.dll, thanks.

sxlllslgh (Issue Creator) on (2024-12-04 13:52:21 UTC): Sorry, I can't. As I said above, there are too many missed dependencies when building it. I'm also waiting for offical patches.

weijianghn on (2024-12-11 07:09:33 UTC): @mraunak waiting for the patches.....

mraunak on (2024-12-18 17:57:40 UTC): Hi @weijianghn,

Apologies for the delayed response.

The team working on this task has raised a few questions, as outlined below:
Why is there a need to build libtensorflow_cc on Windows?
As far as we know, this artifact is not available on Windows
libtensorflow_cc is typically a Linux-specific artifact, while on Windows, only pywrap_tensorflow_internal.dll is available.
Could you provide some clarity on this?

Thanks!

TheAsherbot on (2024-12-19 04:53:23 UTC): @mraunak I am not weijianghn, but I thought I should give the reason why I am looking at using libtensorflow_cc on windows. I prefer to use visual studios as my c++ compiler. I also do not have a dedicated linux computer, though I do have Windows Subsystem for Linux which is limited on what it can do. If I am missing something that allows me to use libtensorflow_cc  easily on linux, please let me know.

Thank you!

weijianghn on (2024-12-19 12:08:40 UTC): Hi @mraunak 
We want to deploy our product on windows platform,  we need to use c++ tensorflow API, that means we need libtensorflow_cc.  Actually, most of terminal device are based on windows platform.  We know keras is based on python, but we don't want to use python.  why the team doesn't want to support C++ anymore?
Thanks a lot
BR

weijianghn on (2025-01-06 08:47:15 UTC): @mraunak  could you share with us the latest information about libtensorflow_cc? 
Thanks a lot
BR

mihaimaruseac on (2025-01-06 18:53:26 UTC): So, the windows wheel is built slightly differently than on Linux and MacOS, due to mostly technical issues (at first, the Windows wheel was only offering the Python API whereas the other two were also offering some C++ and raw ops support -- this was due to maximum size limits when creating archives on Windows). As a result of that, not all targets can be compiled on every operating system.

We used to have some settings under `platform/` to error when an invalid target was being compiled but during migrations in the past few years those got lost. And, they might have also been incomplete.

But it is certain that `//tensorflow:tensorflow_cc` was not supposed to be a target that can be compiled under Windows.

I would suggest either using a remote VM (could also be local, via VirtualBox, VMWare, etc.) that has a Linux operating system. You can then use Visual Studio to edit and build these files remotely.

Alternatively, I think WSL should work, but I am not 100% certain. It's been multiple years since I last compiled TF on Windows.

If you need the full power of C++ API on Windows, I think sending some PRs to fix that issue would be accepted (you can tag me or @mraunak or both for review), but I am unsure if the OSS team at Google has cycles to work on this by itself.

weijianghn on (2025-01-14 23:31:43 UTC): @sxlllslgh They need PRs to fix that issue.  Do you have any alternative method
@mihaimaruseac  how to submit PR to OSS team

mihaimaruseac on (2025-01-15 14:42:20 UTC): In this repository, mostly. You might need to also look at tensorflow/build repository for some build configurations, though I think most of the stuff is now migrated here.

weijianghn on (2025-01-16 03:05:41 UTC): @mihaimaruseac  that means Google will not resolve build error in windows, right?

mihaimaruseac on (2025-01-16 14:24:07 UTC): @mraunak is helping with fixes for Windows build errors, but we cannot support all possible configurations. This issue is well outside of what we can support, sadly.

werner-rammer on (2025-02-04 08:17:57 UTC): I'd like to add my use-case to the  discussion: in our simulation model we use the C++ version of TensorFlow for inference. The model itself is written in C++ and it uses the C++ API of TensorFlow (https://github.com/edfm-tum/SVD). I do think that tensorflow_cc.lib / tensorflow_cc.dll are relevant and important for many users! There are some research groups that want to use our model, but cannot use Linux servers.
 I was able to compile an old version on Windows (TF 1.4, very painful), and on Linux there is this fantastic repo (https://github.com/ika-rwth-aachen/libtensorflow_cc). @sxlllslgh is providing prebuilt binaries for Windows (many thanks!), but there seem to be some symbols missing (I try to use the 2.5.0 version).
In my case these are:
```
tensorflow::Scope::~Scope(void)
tensorflow::Scope::NewRootScope(void)
tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const
tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const
tensorflow::ops::TopK::TopK(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)
tensorflow::Scope::graph(void)const

```
I would really appreciate if a building on Windows would be fixed!

"
2570663327,issue,closed,completed,DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed,"{
	""name"": ""ImportError"",
	""message"": ""Traceback (most recent call last):
  File \""c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."",
	""stack"": ""---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62, in <module>
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Input In [6], in <cell line: 2>()
      1 #import keras libraries and packages
----> 2 import keras
      3 from keras.models import Sequential
      4 from keras.layers import Dense

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\__init__.py:20, in <module>
      1 # Copyright 2015 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the \""License\"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 \""\""\""Implementation of the Keras API, the high-level API of TensorFlow.
     16 
     17 Detailed documentation and user guides are available at
     18 [keras.io](https://keras.io).
     19 \""\""\""
---> 20 from keras import distribute
     21 from keras import models
     22 from keras.engine.input_layer import Input

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\distribute\\__init__.py:18, in <module>
      1 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the \""License\"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 \""\""\""Keras' Distribution Strategy library.\""\""\""
---> 18 from keras.distribute import sidecar_evaluator

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:17, in <module>
      1 # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the \""License\"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 \""\""\""Python module for evaluation loop.\""\""\""
---> 17 import tensorflow.compat.v2 as tf
     19 # isort: off
     20 from tensorflow.python.platform import tf_logging as logging

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\__init__.py:37, in <module>
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\__init__.py:36, in <module>
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77, in <module>
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\
\
Failed to load the native TensorFlow runtime.\
'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\
'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File \""c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.""
}",dambeebu,2024-10-07 14:46:02+00:00,['Venkat6871'],2024-10-24 02:01:44+00:00,2024-10-24 02:01:43+00:00,https://github.com/tensorflow/tensorflow/issues/77150,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2398935575, 'issue_id': 2570663327, 'author': 'Venkat6871', 'body': 'Hi @**dambeebu** ,\r\n\r\nCould you please try using the latest version of TensorFlow 2.18.0-rc1? I am providing the [documentation](https://github.com/tensorflow/tensorflow/releases) for the latest version here. Please let us know if the issue still persists.\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced. \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 8, 6, 11, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573150, 'issue_id': 2570663327, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073774, 'issue_id': 2570663327, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 43, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-08 06:11:01 UTC): Hi @**dambeebu** ,

Could you please try using the latest version of TensorFlow 2.18.0-rc1? I am providing the [documentation](https://github.com/tensorflow/tensorflow/releases) for the latest version here. Please let us know if the issue still persists.
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced. 

Thank you!

github-actions[bot] on (2024-10-16 02:02:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2570050419,issue,open,,Error building speech_commands with the ARM platform,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

Armv7

### Python version

3.12.3

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current behavior?

The following error occurs when building
`external/gemmlowp/meta/streams_arm_32.h:1535:3: error: 'asm' operand has impossible constraints`



### Standalone code to reproduce the issue

```shell
bazel build --config=elinux_armhf //tensorflow/examples/speech_commands:recognize_commands
```


### Relevant log output

```shell
~/tensorflow_src$ bazel build --config=elinux_armhf //tensorflow/examples/speech_commands:recognize_commands
INFO: Reading 'startup' options from /home/linus/tensorflow_src/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=115
INFO: Reading rc options for 'build' from /home/linus/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/linus/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /home/linus/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/linus/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:elinux_armhf in file /home/linus/tensorflow_src/.bazelrc: --config=elinux --cpu=armhf --copt -mfp16-format=ieee
INFO: Found applicable config definition build:elinux in file /home/linus/tensorflow_src/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:linux in file /home/linus/tensorflow_src/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/linus/tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/examples/speech_commands:recognize_commands (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/linus/tensorflow_src/tensorflow/core/kernels/BUILD:6327:11: Compiling tensorflow/core/kernels/meta_support.cc failed: (Exit 1): arm-none-linux-gnueabihf-gcc failed: error executing command (from target //tensorflow/core/kernels:meta_support) /home/linus/.cache/bazel/_bazel_linus/dfed2ce84bdb0ad21d45a92561825dca/external/armhf_linux_toolchain/bin/arm-none-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 135 arguments skipped)
In file included from external/gemmlowp/meta/streams.h:307,
                 from external/gemmlowp/meta/quantized_mul_kernels.h:22,
                 from ./tensorflow/core/kernels/meta_support.h:21,
                 from tensorflow/core/kernels/meta_support.cc:18:
external/gemmlowp/meta/streams_arm_32.h: In static member function 'static void gemmlowp::meta::GemmExecutorPackRHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 2; int n = 4; int k = 8; int m_leftovers = 0; int n_leftovers = 0; int k_leftovers = 0]':
external/gemmlowp/meta/streams_arm_32.h:1535:3: error: 'asm' operand has impossible constraints
 1535 |   asm volatile(
      |   ^~~
Target //tensorflow/examples/speech_commands:recognize_commands failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 107.917s, Critical Path: 91.70s
INFO: 13 processes: 7 internal, 6 local.
FAILED: Build did NOT complete successfully
```
",tonghyun,2024-10-07 10:44:12+00:00,['tilakrayal'],2024-12-07 19:46:24+00:00,,https://github.com/tensorflow/tensorflow/issues/77142,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2436466309, 'issue_id': 2570050419, 'author': 'pkgoogle', 'body': ""Hi @tilakrayal, this isn't a lite issue as far as I can tell. Please reroute appropriately. Thanks."", 'created_at': datetime.datetime(2024, 10, 24, 22, 42, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445827565, 'issue_id': 2570050419, 'author': 'tonghyun', 'body': ""> Hi @tilakrayal, this isn't a lite issue as far as I can tell. Please reroute appropriately. Thanks.\r\n\r\n@pkgoogle @tilakrayal\r\nLooking forward to reading your response. thank you."", 'created_at': datetime.datetime(2024, 10, 30, 4, 35, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503481406, 'issue_id': 2570050419, 'author': 'rishik1001', 'body': ""> > Hi @tilakrayal, this isn't a lite issue as far as I can tell. Please reroute appropriately. Thanks.\r\n> \r\n> @pkgoogle @tilakrayal Looking forward to reading your response. thank you.\r\n\r\nAny solution for above error ?"", 'created_at': datetime.datetime(2024, 11, 27, 10, 18, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525291253, 'issue_id': 2570050419, 'author': 'ghost', 'body': 'I have the same error. Does anyone have a possible solution?', 'created_at': datetime.datetime(2024, 12, 7, 19, 46, 22, tzinfo=datetime.timezone.utc)}]","pkgoogle on (2024-10-24 22:42:21 UTC): Hi @tilakrayal, this isn't a lite issue as far as I can tell. Please reroute appropriately. Thanks.

tonghyun (Issue Creator) on (2024-10-30 04:35:43 UTC): @pkgoogle @tilakrayal
Looking forward to reading your response. thank you.

rishik1001 on (2024-11-27 10:18:16 UTC): Any solution for above error ?

ghost on (2024-12-07 19:46:22 UTC): I have the same error. Does anyone have a possible solution?

"
2569936583,issue,closed,completed,TensorFlow Lite label_image fails to cross-compile with cmake,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc0

### Custom code

No

### OS platform and distribution

WSL Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

clang 17.0.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I got the error

> [riscv64-linux/bin/ld: cannot find -lprotobuf: No such file or directory](riscv64-unknown-linux-gnu-clang++: error: linker command failed with exit code 1 (use -v to see invocation))


### Standalone code to reproduce the issue

```shell
$ mkdir workspace && cd workspace
$ mkdir flatc_build && mkdir tflite_build

$ wget https://raw.githubusercontent.com/google/XNNPACK/refs/heads/master/cmake/riscv64.toolchain
$ wget https://github.com/riscv-collab/riscv-gnu-toolchain/releases/download/2023.11.20/riscv64-glibc-ubuntu-20.04-llvm-nightly-2023.11.20-nightly.tar.gz
$ tar zxvf riscv64-glibc-ubuntu-20.04-llvm-nightly-2023.11.20-nightly.tar.gz

$ git clone https://github.com/tensorflow/tensorflow
$ cmake -B ./flatc_build -S ./tensorflow/tensorflow/lite/tools/cmake/native_tools/flatbuffers
$ cmake --build ./flatc_build -j6

$ cmake -B ./tflite_build \
-S ./tensorflow/tensorflow/lite/ \
-DCMAKE_TOOLCHAIN_FILE=$(pwd)/riscv64.toolchain \
-DRISCV_TOOLCHAIN_ROOT=$(pwd)/riscv \
-DRISCV_QEMU_ROOT=$(pwd)/riscv \
-DCMAKE_BUILD_TYPE=Release \
-DTFLITE_HOST_TOOLS_DIR=./flatc_build/flatbuffers-flatc/bin \
-DTFLITE_KERNEL_TEST=OFF

$ cmake --build ./tflite_build -j6
$ cmake --build ./tflite_build -t label_image -j6
```


### Relevant log output

```shell
[100%] Linking CXX executable label_image
/home/xxx/workspace/tflite_build/../riscv/bin/riscv64-unknown-linux-gnu-ld: cannot find -lprotobuf: No such file or directory
riscv64-unknown-linux-gnu-clang++: error: linker command failed with exit code 1 (use -v to see invocation)
```
",yhng3010,2024-10-07 09:55:20+00:00,['Venkat6871'],2025-01-09 09:50:38+00:00,2025-01-09 09:50:34+00:00,https://github.com/tensorflow/tensorflow/issues/77137,"[('type:bug', 'Bug'), ('awaiting PR merge', 'awaiting PR merge'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2396467238, 'issue_id': 2569936583, 'author': 'yhng3010', 'body': 'The problem is that label_image should link to built protobuf, but the target_link_libraries in tensorflow/lite/examples/label_image/CMakeLists.txt are filled with the wrong library name.\r\nIn the cmake files of protobuf, you can see that the built library is named libprotobuf instead of protobuf:\r\n```\r\n$ cd tflite_build/protobuf/cmake\r\n$ ag ""add_library""  \r\nlibprotobuf.cmake\r\n102:add_library(libprotobuf ${protobuf_SHARED_OR_STATIC}\r\n134:add_library(protobuf::libprotobuf ALIAS libprotobuf)\r\n\r\nlibprotobuf-lite.cmake\r\n89:add_library(libprotobuf-lite ${protobuf_SHARED_OR_STATIC}\r\n118:add_library(protobuf::libprotobuf-lite ALIAS libprotobuf-lite)\r\n\r\ntests.cmake\r\n30:  add_library(gmock STATIC\r\n35:  add_library(gmock_main STATIC ""${googlemock_source_dir}/src/gmock_main.cc"")\r\n38:  add_library(GTest::gmock ALIAS gmock)\r\n39:  add_library(GTest::gmock_main ALIAS gmock_main)\r\n127:add_library(protobuf-lite-test-common STATIC\r\n141:add_library(protobuf-test-common STATIC\r\n\r\nlibprotoc.cmake\r\n113:add_library(libprotoc ${protobuf_SHARED_OR_STATIC}\r\n136:add_library(protobuf::libprotoc ALIAS libprotoc)\r\n```\r\nI have confirmed that label_image can be built with the change shown below.\r\n```\r\ndiff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\nindex 2fcb09ce96e..07ab2343ae5 100644\r\n--- a/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n@@ -84,5 +84,5 @@ target_compile_options(label_image\r\n target_link_libraries(label_image\r\n   tensorflow-lite\r\n   profiling_info_proto\r\n-  protobuf\r\n+  libprotobuf\r\n )\r\n```', 'created_at': datetime.datetime(2024, 10, 7, 9, 58, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579628753, 'issue_id': 2569936583, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77137"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77137"">No</a>', 'created_at': datetime.datetime(2025, 1, 9, 9, 50, 37, tzinfo=datetime.timezone.utc)}]","yhng3010 (Issue Creator) on (2024-10-07 09:58:47 UTC): The problem is that label_image should link to built protobuf, but the target_link_libraries in tensorflow/lite/examples/label_image/CMakeLists.txt are filled with the wrong library name.
In the cmake files of protobuf, you can see that the built library is named libprotobuf instead of protobuf:
```
$ cd tflite_build/protobuf/cmake
$ ag ""add_library""  
libprotobuf.cmake
102:add_library(libprotobuf ${protobuf_SHARED_OR_STATIC}
134:add_library(protobuf::libprotobuf ALIAS libprotobuf)

libprotobuf-lite.cmake
89:add_library(libprotobuf-lite ${protobuf_SHARED_OR_STATIC}
118:add_library(protobuf::libprotobuf-lite ALIAS libprotobuf-lite)

tests.cmake
30:  add_library(gmock STATIC
35:  add_library(gmock_main STATIC ""${googlemock_source_dir}/src/gmock_main.cc"")
38:  add_library(GTest::gmock ALIAS gmock)
39:  add_library(GTest::gmock_main ALIAS gmock_main)
127:add_library(protobuf-lite-test-common STATIC
141:add_library(protobuf-test-common STATIC

libprotoc.cmake
113:add_library(libprotoc ${protobuf_SHARED_OR_STATIC}
136:add_library(protobuf::libprotoc ALIAS libprotoc)
```
I have confirmed that label_image can be built with the change shown below.
```
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 2fcb09ce96e..07ab2343ae5 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -84,5 +84,5 @@ target_compile_options(label_image
 target_link_libraries(label_image
   tensorflow-lite
   profiling_info_proto
-  protobuf
+  libprotobuf
 )
```

google-ml-butler[bot] on (2025-01-09 09:50:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77137"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77137"">No</a>

"
2569668162,issue,closed,completed,Floating point exception (core dumped) with onednn opt,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

linux 

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I don't know why but tensorflow allows a layer generating tensor with shapes containing 0, which will lead to 'Floating point exception (core dumped)' when oneDNN Opt is on. 

In the code below a MaxPooling1D should have generate a tensor with shape [3,5,0], while the engine crashed due to the bug I mentioned above.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1jkQOOOjU2B60Qe8MvaEISXObuHBxm-I9?usp=sharing
```


### Relevant log output

```shell
The colab engine crashed.
If running locally, the error msg is:

Floating point exception (core dumped)
```
",Shuo-Sun20,2024-10-07 08:00:54+00:00,['tilakrayal'],2024-10-09 08:19:47+00:00,2024-10-09 08:19:43+00:00,https://github.com/tensorflow/tensorflow/issues/77131,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2396274203, 'issue_id': 2569668162, 'author': 'AdvaitDongre', 'body': ""heyy, i think these are some suggestions from my side which could help, let me know which one would help you:\r\n1. try updating the tensorflow to latest version\r\n2. try disabling oneDNN optimization `TF_DISABLE_MKL=1`\r\n3. try checking tensor shapes before pooling, just ensure they've valid dim and dosen't have zero's in it"", 'created_at': datetime.datetime(2024, 10, 7, 8, 35, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396677353, 'issue_id': 2569668162, 'author': 'Shuo-Sun20', 'body': ""@AdvaitDongre \r\nActually, I've tried them.\r\n1. I'v used the latest version tf 2.17\r\n2. Disabling oneDNN can avoid this problem but makes the executing process slow.\r\n3. I'm executing a random experiment, and the shape are not determined before execution.\r\n\r\nI thought the layer API could throw an exception which can be caught by 'try-except' block (as in Pytorch), but this floating point exception just makes the whole program crash."", 'created_at': datetime.datetime(2024, 10, 7, 11, 35, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399162730, 'issue_id': 2569668162, 'author': 'AdvaitDongre', 'body': 'ahh got it, \r\n1. can you try prechecking for zero dimensions? \r\ngpt code for it:\r\n`def safe_layer(layer, input_tensor):\r\n    shape = tf.shape(input_tensor)\r\n    if tf.reduce_any(shape == 0):\r\n        print(f""Skipping layer {layer.name} due to zero dimension in shape {shape}"")\r\n        return input_tensor\r\n    try:\r\n        return layer(input_tensor)\r\n    except Exception as e:\r\n        print(f""Error applying layer {layer.name}: {e}"")\r\n        return input_tensor`\r\n\r\n2. you could also try using tf.debugging.assert \r\ndocs: [https://www.tensorflow.org/api_docs/python/tf/debugging/Assert](tf.debugging.assert)\r\n\r\nbut i think pre-checking would fix your error, let me know whether it does or not', 'created_at': datetime.datetime(2024, 10, 8, 8, 15, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399678985, 'issue_id': 2569668162, 'author': 'tilakrayal', 'body': '@Shuo-Sun20,\r\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have changed some steps and the code was executed without crash/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/99396471aaa4cf5e4d54cdb58dd808df/untitled2155.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 8, 12, 14, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401659304, 'issue_id': 2569668162, 'author': 'Shuo-Sun20', 'body': ""@AdvaitDongre \r\nThank you for your advice.\r\nUnfortunately, this error can not be caught by 'try-except' block. It just crashes the whole program. Therefore the code you provided can not work.\r\n\r\n@tilakrayal \r\nThank you for the check.\r\nIt seems that something goes wrongly when keras updating from 2.0 to 3.0. I'll turn to keras and report this issue."", 'created_at': datetime.datetime(2024, 10, 9, 8, 19, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401659804, 'issue_id': 2569668162, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77131"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77131"">No</a>', 'created_at': datetime.datetime(2024, 10, 9, 8, 19, 45, tzinfo=datetime.timezone.utc)}]","AdvaitDongre on (2024-10-07 08:35:27 UTC): heyy, i think these are some suggestions from my side which could help, let me know which one would help you:
1. try updating the tensorflow to latest version
2. try disabling oneDNN optimization `TF_DISABLE_MKL=1`
3. try checking tensor shapes before pooling, just ensure they've valid dim and dosen't have zero's in it

Shuo-Sun20 (Issue Creator) on (2024-10-07 11:35:39 UTC): @AdvaitDongre 
Actually, I've tried them.
1. I'v used the latest version tf 2.17
2. Disabling oneDNN can avoid this problem but makes the executing process slow.
3. I'm executing a random experiment, and the shape are not determined before execution.

I thought the layer API could throw an exception which can be caught by 'try-except' block (as in Pytorch), but this floating point exception just makes the whole program crash.

AdvaitDongre on (2024-10-08 08:15:06 UTC): ahh got it, 
1. can you try prechecking for zero dimensions? 
gpt code for it:
`def safe_layer(layer, input_tensor):
    shape = tf.shape(input_tensor)
    if tf.reduce_any(shape == 0):
        print(f""Skipping layer {layer.name} due to zero dimension in shape {shape}"")
        return input_tensor
    try:
        return layer(input_tensor)
    except Exception as e:
        print(f""Error applying layer {layer.name}: {e}"")
        return input_tensor`

2. you could also try using tf.debugging.assert 
docs: [https://www.tensorflow.org/api_docs/python/tf/debugging/Assert](tf.debugging.assert)

but i think pre-checking would fix your error, let me know whether it does or not

tilakrayal (Assginee) on (2024-10-08 12:14:30 UTC): @Shuo-Sun20,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras

import tf_keras as keras
```

Also I have changed some steps and the code was executed without crash/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/99396471aaa4cf5e4d54cdb58dd808df/untitled2155.ipynb).

Thank you!

Shuo-Sun20 (Issue Creator) on (2024-10-09 08:19:31 UTC): @AdvaitDongre 
Thank you for your advice.
Unfortunately, this error can not be caught by 'try-except' block. It just crashes the whole program. Therefore the code you provided can not work.

@tilakrayal 
Thank you for the check.
It seems that something goes wrongly when keras updating from 2.0 to 3.0. I'll turn to keras and report this issue.

google-ml-butler[bot] on (2024-10-09 08:19:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77131"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77131"">No</a>

"
2568461373,issue,closed,not_planned,Proposal: Rename TensorFlow to Tensor Flow,"I would like to propose that the name TensorFlow be changed to ""Tensor Flow,"" with a space between the words. My reasoning is that this could provide better readability and clarity for new users.
While I understand TensorFlow has established branding, I believe this small change might make the name more intuitive for the general public. I'd appreciate the community's thoughts on this.",hrishi-2407,2024-10-06 04:48:16+00:00,['Venkat6871'],2025-01-03 19:14:08+00:00,2025-01-03 19:13:55+00:00,https://github.com/tensorflow/tensorflow/issues/77101,"[('type:feature', 'Feature requests')]",[],
2568222170,issue,closed,completed,tflite conversion from tensorflow concrete function drops input argument names.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 24.04):
- TensorFlow installation (pip package or built from source): Pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-version: 2.17.0

### 2. Code

```
import tensorflow as tf

    print(""tf-version:"", tf.__version__)

    class Test(tf.Module):
        def __init__(self, encoder_dim: int):
            super().__init__(name=None)

            self.final_projection = tf.keras.layers.Dense(
                encoder_dim, name=""final_projection""
            )

        @tf.function
        def stats(self, inputs: Dict[str, tf.Tensor]):
            x = inputs[""a""] + inputs[""b""]

            print(""x"", x)
            return {
                ""x"": x,
            }

    model = Test(32)

    inputs = {""a"": tf.constant(2), ""b"": tf.constant(5)}
    # Convert the concrete functions using TFLiteConverter
    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [model.stats.get_concrete_function(inputs)], model
    )
    tflite_model = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    signatures = interpreter.get_signature_list()
    print(""[Interpreter] signatures: "", signatures)
```

This produces 

```
[Interpreter] signatures:  {'serving_default': {'inputs': ['inputs', 'inputs_1'], 'outputs': ['x']}}
```

I would expect it to produce 

[Interpreter] signatures:  {'serving_default': {'inputs': ['a', 'b'], 'outputs': ['x']}}


Full logs


```
.venv ❯  python3 src/train.py

2024-10-05 19:04:36.611417: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-05 19:04:37.118668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
tf-version: 2.17.0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1728147877.548136  482617 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.
W0000 00:00:1728147877.548157  482617 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.
2024-10-05 19:04:37.548504: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpuxyu4mv2
2024-10-05 19:04:37.548619: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-10-05 19:04:37.548626: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpuxyu4mv2
2024-10-05 19:04:37.550161: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-10-05 19:04:37.550313: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-10-05 19:04:37.556608: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpuxyu4mv2
2024-10-05 19:04:37.559865: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 11361 microseconds.
2024-10-05 19:04:37.566004: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-10-05 19:04:37.573757: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3531] Estimated count of arithmetic ops: 1  ops, equivalently 0  MACs
[Interpreter] signatures:  {'serving_default': {'inputs': ['inputs', 'inputs_1'], 'outputs': ['x']}}
```",jonasrsv42,2024-10-05 17:06:16+00:00,['gaikwadrahul8'],2024-10-31 19:08:17+00:00,2024-10-31 19:08:13+00:00,https://github.com/tensorflow/tensorflow/issues/77094,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2397437417, 'issue_id': 2568222170, 'author': 'gaikwadrahul8', 'body': 'Hi, @jonasrsv42\r\n\r\nI apologize for the delayed response, I replaced the dictionary input `(Dict[str, tf.Tensor])` with two separate tensor inputs `a and b`. These are passed using the `input_signature` of the `tf.function` which defines the `input shapes, types and names`. By explicitly naming the inputs in the `TensorSpec` of the `tf.function` TensorFlow Lite will retain these names when creating the TFLite model. The `get_concrete_function()` now does not require a dictionary as input. It automatically takes in the tensor a and b.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(""tf-version:"", tf.__version__)\r\n\r\nclass Test(tf.Module):\r\n    def __init__(self, encoder_dim: int):\r\n        super().__init__(name=None)\r\n\r\n        self.final_projection = tf.keras.layers.Dense(\r\n            encoder_dim, name=""final_projection""\r\n        )\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec(shape=(), dtype=tf.int32, name=""a""),\r\n        tf.TensorSpec(shape=(), dtype=tf.int32, name=""b"")\r\n    ])\r\n    def stats(self, a: tf.Tensor, b: tf.Tensor):\r\n        x = a + b\r\n\r\n        print(""x:"", x)\r\n        return {\r\n            ""x"": x,\r\n        }\r\n\r\nmodel = Test(32)\r\n\r\n# Convert the concrete function using TFLiteConverter\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n    [model.stats.get_concrete_function()], model\r\n)\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\nsignatures = interpreter.get_signature_list()\r\nprint(""[Interpreter] signatures: "", signatures)\r\n```\r\n\r\n**Output Log :**\r\n\r\n```\r\ntf-version: 2.17.0\r\nx: Tensor(""add:0"", shape=(), dtype=int32)\r\nx: Tensor(""add:0"", shape=(), dtype=int32)\r\n[Interpreter] signatures:  {\'serving_default\': {\'inputs\': [\'a\', \'b\'], \'outputs\': [\'x\']}}\r\n```\r\nIf I\'ve missed something please let me know.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 7, 16, 54, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398785114, 'issue_id': 2568222170, 'author': 'jonasrsv42', 'body': 'Thank you for the response! \r\n\r\nUnfortunately the reason why I am using a dictionary is because my real use-case involves ~50 variables. Typing them all out manually is not very feasible. I tried to rely on argument ordering but it seems it gets scrambled. \r\n\r\nI have also tried with *args and **kwargs but it seems to suffer from the same issue. \r\n\r\nWhat is the recommended approach for exporting tflite models with a large number of arguments, is manually typing the `TensorSpec` the only supported path? \r\n\r\nIt does seem dictionary output works fine', 'created_at': datetime.datetime(2024, 10, 8, 4, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402822372, 'issue_id': 2568222170, 'author': 'gaikwadrahul8', 'body': 'Hi, @jonasrsv42\r\n\r\nI tried something different approach so please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/4a8b6d7140f7fbdeaa2cf30cf62880ca/test-77094.ipynb) and please refer our official documentation for [Signatures in LiteRT](https://ai.google.dev/edge/litert/models/signatures) which may help you solve your issue.\r\n\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 9, 16, 46, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2404026215, 'issue_id': 2568222170, 'author': 'jonasrsv42', 'body': ""In the gist your `input_x` tensor is being renamed to `inputs_y`. I believe the order is not preserved in tflite model, at-least that's what I have observed so that `input_x` != `inputs_x` in forall x. \r\n\r\nIs this a bug? Is your input argument supposed to get renamed? Is order supposed to be preserved?"", 'created_at': datetime.datetime(2024, 10, 10, 5, 0, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431915125, 'issue_id': 2568222170, 'author': 'gaikwadrahul8', 'body': 'Hi, @jonasrsv42 \r\n\r\nI apologize for the delayed response, I did some code modifications so please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/7f0a894ff29168b46e11f0e00852af99/tflite-77094.ipynb) and let me know is it working as expected or not ?\r\n\r\nif I have missed something here or If issue still persists please let me know.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 23, 12, 9, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448867924, 'issue_id': 2568222170, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450630920, 'issue_id': 2568222170, 'author': 'jonasrsv42', 'body': 'Yes this workaround seems to work! :) Thanks', 'created_at': datetime.datetime(2024, 10, 31, 19, 8, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450631014, 'issue_id': 2568222170, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77094"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77094"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 19, 8, 16, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-07 16:54:54 UTC): Hi, @jonasrsv42

I apologize for the delayed response, I replaced the dictionary input `(Dict[str, tf.Tensor])` with two separate tensor inputs `a and b`. These are passed using the `input_signature` of the `tf.function` which defines the `input shapes, types and names`. By explicitly naming the inputs in the `TensorSpec` of the `tf.function` TensorFlow Lite will retain these names when creating the TFLite model. The `get_concrete_function()` now does not require a dictionary as input. It automatically takes in the tensor a and b.


```
import tensorflow as tf

print(""tf-version:"", tf.__version__)

class Test(tf.Module):
    def __init__(self, encoder_dim: int):
        super().__init__(name=None)

        self.final_projection = tf.keras.layers.Dense(
            encoder_dim, name=""final_projection""
        )

    @tf.function(input_signature=[
        tf.TensorSpec(shape=(), dtype=tf.int32, name=""a""),
        tf.TensorSpec(shape=(), dtype=tf.int32, name=""b"")
    ])
    def stats(self, a: tf.Tensor, b: tf.Tensor):
        x = a + b

        print(""x:"", x)
        return {
            ""x"": x,
        }

model = Test(32)

# Convert the concrete function using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [model.stats.get_concrete_function()], model
)
tflite_model = converter.convert()
interpreter = tf.lite.Interpreter(model_content=tflite_model)
signatures = interpreter.get_signature_list()
print(""[Interpreter] signatures: "", signatures)
```

**Output Log :**

```
tf-version: 2.17.0
x: Tensor(""add:0"", shape=(), dtype=int32)
x: Tensor(""add:0"", shape=(), dtype=int32)
[Interpreter] signatures:  {'serving_default': {'inputs': ['a', 'b'], 'outputs': ['x']}}
```
If I've missed something please let me know.

Thank you for your cooperation and patience.

jonasrsv42 (Issue Creator) on (2024-10-08 04:09:15 UTC): Thank you for the response! 

Unfortunately the reason why I am using a dictionary is because my real use-case involves ~50 variables. Typing them all out manually is not very feasible. I tried to rely on argument ordering but it seems it gets scrambled. 

I have also tried with *args and **kwargs but it seems to suffer from the same issue. 

What is the recommended approach for exporting tflite models with a large number of arguments, is manually typing the `TensorSpec` the only supported path? 

It does seem dictionary output works fine

gaikwadrahul8 (Assginee) on (2024-10-09 16:46:50 UTC): Hi, @jonasrsv42

I tried something different approach so please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/4a8b6d7140f7fbdeaa2cf30cf62880ca/test-77094.ipynb) and please refer our official documentation for [Signatures in LiteRT](https://ai.google.dev/edge/litert/models/signatures) which may help you solve your issue.

If I have missed something here please let me know.

Thank you for your cooperation and patience.

jonasrsv42 (Issue Creator) on (2024-10-10 05:00:03 UTC): In the gist your `input_x` tensor is being renamed to `inputs_y`. I believe the order is not preserved in tflite model, at-least that's what I have observed so that `input_x` != `inputs_x` in forall x. 

Is this a bug? Is your input argument supposed to get renamed? Is order supposed to be preserved?

gaikwadrahul8 (Assginee) on (2024-10-23 12:09:30 UTC): Hi, @jonasrsv42 

I apologize for the delayed response, I did some code modifications so please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/7f0a894ff29168b46e11f0e00852af99/tflite-77094.ipynb) and let me know is it working as expected or not ?

if I have missed something here or If issue still persists please let me know.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-31 02:03:08 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

jonasrsv42 (Issue Creator) on (2024-10-31 19:08:12 UTC): Yes this workaround seems to work! :) Thanks

google-ml-butler[bot] on (2024-10-31 19:08:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77094"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77094"">No</a>

"
2568196602,issue,closed,completed,Import Module Error: Undefined Symbol in TensorFlow Lite Runtime After Build with Blaze and Flex support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

FROM public.ecr.aws/lambda/python:3.12

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

12.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I successfully built the TensorFlow Lite pip package with Flex using Blaze. However, when I attempted to load the TensorFlow Lite runtime, I encountered the following error:

`Runtime.ImportModuleError: Unable to import module 'entry': /var/lang/lib/python3.12/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: _ZN10tensorflow11CSRMatMulOpIN5Eigen16ThreadPoolDeviceEfEC2EPNS_20OpKernelConstructionE`

I built inside docker image with the following command

`CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native`

### Standalone code to reproduce the issue

```shell
import tflite_runtime.interpreter as tflite

filepath = os.path.join(os.path.dirname(__file__), f""models/model.tflite"")
interpreter = tflite.Interpreter(model_path=filepath)
```


### Relevant log output

```shell
2024-10-05 11:50:35 [ERROR] Runtime.ImportModuleError: Unable to import module 'entry': /var/lang/lib/python3.12/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: _ZN10tensorflow11CSRMatMulOpIN5Eigen16ThreadPoolDeviceEfEC2EPNS_20OpKernelConstructionE
Traceback (most recent call last):
```
",dev2xl,2024-10-05 16:01:39+00:00,['Venkat6871'],2024-10-22 02:02:37+00:00,2024-10-22 02:02:33+00:00,https://github.com/tensorflow/tensorflow/issues/77092,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2396053673, 'issue_id': 2568196602, 'author': 'Venkat6871', 'body': 'Hi **@dev2xl** ,\r\nThank you for raising this issue. It seems there is a version mismatch, which is causing compatibility issues. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Please go through it and recheck all the compatibility versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 7, 6, 52, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412661946, 'issue_id': 2568196602, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058690, 'issue_id': 2568196602, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428058745, 'issue_id': 2568196602, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77092"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77092"">No</a>', 'created_at': datetime.datetime(2024, 10, 22, 2, 2, 35, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-07 06:52:50 UTC): Hi **@dev2xl** ,
Thank you for raising this issue. It seems there is a version mismatch, which is causing compatibility issues. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Please go through it and recheck all the compatibility versions.
Thank you!

github-actions[bot] on (2024-10-15 02:02:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-22 02:02:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-22 02:02:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77092"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77092"">No</a>

"
2568129388,issue,closed,completed,A dynamic link library (DLL) initialization routine failed.,"ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:
",Adedeji-hub,2024-10-05 14:35:44+00:00,['tilakrayal'],2024-10-24 02:01:45+00:00,2024-10-24 02:01:44+00:00,https://github.com/tensorflow/tensorflow/issues/77089,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2399665729, 'issue_id': 2568129388, 'author': 'tilakrayal', 'body': '@Adedeji-hub,\r\nCould you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 8, 12, 8, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573186, 'issue_id': 2568129388, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073795, 'issue_id': 2568129388, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-08 12:08:04 UTC): @Adedeji-hub,
Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which helps to debug the issue. Thank you!

github-actions[bot] on (2024-10-16 02:02:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2567469278,issue,open,,Can't compile Tensorflow 2.17 from source for cpu on fedora 40 : undefined reference,"
## I'm trying to compile Tensorflow 2.17 on a new fresh install of Fedora40 lxqt desktop (official spin).

#### what i've donne (all command as root):

 - Fresh Fedora install
 - dnf update
 - reboot
 - dnf install python3-devel g++ gcc cmake python3-pip git
   eigen3-devel
 - pip install -U --user pip
 - pip install -U  pip six numpy wheel setuptools mock
 - wget -O bazel
   https://github.com/bazelbuild/bazelisk/releases/download/v1.22.0/bazelisk-linux-amd64
 - mv bazel /bin/
 - chmod 555 /bin/bazel
 - git clone https://github.com/tensorflow/tensorflow.git
 - cd tensorflow
 - git checkout r2.17
 - export TF_PYTHON_VERSION=3.12
 - export LD_LIBRARY_PATH=/usr/local/lib
 - ./configure # <- answer default value but use GCC and say no to Cuda build, and
   used this optimisation flags: -march=native -mtune=native -O3
 - bazel build //tensorflow/tools/pip_package:wheel
   --repo_env=WHEEL_NAME=tensorflow_cpu --config=nonccl --config=opt --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""

#### After a while a get this error:
```
ERROR: /home/brd/tensorflow/tensorflow/BUILD:1318:21: Linking tensorflow/libtensorflow_cc.so.2.17.1 failed: (Exit 1): gcc failed: error executing command (from target //tensorflow:libtensorflow_cc.so.2.17.1) /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.17.1-2.params
/usr/bin/ld.gold: warning: bazel-out/k8-opt/bin/external/local_tsl/tsl/platform/cloud/_objs/gcs_file_system/gcs_file_system.pic.o: conflicting default version definition for _ZZZN3tsl17RamFileBlockCacheC4EmmmSt8functionIFN4absl12lts_202308026StatusERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEmmPcPmEEPNS_3EnvEENKUliPKcE0_clEiSK_E17vmodule_activated@@tensorflow
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/local_tsl/tsl/platform/cloud/_objs/gcs_file_system/gcs_file_system.pic.o: previous definition of _ZZZN3tsl17RamFileBlockCacheC4EmmmSt8functionIFN4absl12lts_202308026StatusERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEmmPcPmEEPNS_3EnvEENKUliPKcE0_clEiSK_E17vmodule_activated@@tensorflow here
bazel-out/k8-opt/bin/tensorflow/core/kernels/mkl/_objs/mkl_sparse_matrix_matmul_op/mkl_sparse_matrix_matmul_op.pic.o:mkl_sparse_matrix_matmul_op.cc:function tensorflow::register_kernel_0::{lambda(tensorflow::KernelDef const*)#1}::operator()(tensorflow::KernelDef const*) const::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*):(.text._ZZNK10tensorflowL17register_kernel_0MUlPKNS_9KernelDefEE_clES3_ENUlPNS_20OpKernelConstructionEE_4_FUNES6_+0x18d): error: undefined reference to 'tensorflow::CSRMatMulOp<Eigen::ThreadPoolDevice, float>::CSRMatMulOp(tensorflow::OpKernelConstruction*)'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:wheel failed to build
```
**how do i solve : undefined reference to 'tensorflow::CSRMatMulOp<Eigen::ThreadPoolDevice, float>::CSRMatMulOp(tensorflow::OpKernelConstruction\*)'?**

---
#### Version:
Fedora 40 lxqt desktop kernel 6.10.11-200.fc40.x86_64
(runing in virtualbox with: 8 cores and 23Gb of ram)

gcc (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3)

g++ (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3)

Python 3.12.6

GNU Make 4.4.1

cmake version 3.28.2

Bazelisk version: v1.22.0
Build label: 6.5.0
",Brandonn-Etheve,2024-10-04 22:01:41+00:00,"['penpornk', 'tilakrayal']",2024-10-09 12:53:38+00:00,,https://github.com/tensorflow/tensorflow/issues/77072,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:core', 'issues related to core part of tensorflow'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2395053880, 'issue_id': 2567469278, 'author': 'Brandonn-Etheve', 'body': ""Using the same configuration and procedure building 2.16 is working with pip_package:build_pip_package instead of pip_package:wheel. but it's like there no pip_package:build_pip_package in 2.17."", 'created_at': datetime.datetime(2024, 10, 5, 13, 12, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402241220, 'issue_id': 2567469278, 'author': 'tilakrayal', 'body': '@Brandonn-Etheve,\r\nCould you please try whether this issue is happening with the latest tensorflow version [2.18.0-rc1](https://github.com/tensorflow/tensorflow/releases/tag/v2.18.0-rc1) and update if you are facing the same. Thank you!', 'created_at': datetime.datetime(2024, 10, 9, 12, 53, 37, tzinfo=datetime.timezone.utc)}]","Brandonn-Etheve (Issue Creator) on (2024-10-05 13:12:17 UTC): Using the same configuration and procedure building 2.16 is working with pip_package:build_pip_package instead of pip_package:wheel. but it's like there no pip_package:build_pip_package in 2.17.

tilakrayal (Assginee) on (2024-10-09 12:53:37 UTC): @Brandonn-Etheve,
Could you please try whether this issue is happening with the latest tensorflow version [2.18.0-rc1](https://github.com/tensorflow/tensorflow/releases/tag/v2.18.0-rc1) and update if you are facing the same. Thank you!

"
2567265805,issue,closed,completed,MultiHeadAttention layer broken by exception when inside Model or Layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Having MultiHeadAttention outside class works:
```
x = tf.keras.layers.MultiHeadAttention(
    num_heads=1,
    key_dim=1,
    attention_axes=-1,
)(x, x, x)
```
When organized inside custom Model/Layer traises exception located at `site-packages\tensorflow\python\ops\special_math_ops.py` line 1288. Commenting the exception makes `model.summary()` work:
```
if output_labels and len(set(output_labels)) != len(output_labels):
    raise ValueError(
        'Output subscripts contain a label appearing more than once: {}'.format(
            equation))

```
It doesn't happen with other layers.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf


class Block(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.att = tf.keras.layers.MultiHeadAttention(
            num_heads=1,
            key_dim=1,
            attention_axes=-1,
            # output_shape=tuple((units,)),
        )
        return

    def call(self, inputs):
        x = self.att(inputs, inputs, inputs)
        return x


my_model_inputs = tf.keras.Input((8, 8, 8, 1))
x = my_model_inputs
x = tf.keras.layers.Flatten()(x)
x = Block()(x)
my_model = tf.keras.Model(my_model_inputs, x)
my_model.summary(expand_nested=True, show_trainable=True)
```


### Relevant log output

```shell
# Errors are like this:

Could not automatically infer the output shape / dtype of 'block' (of type Block). Either the `Block.call()` method is incorrect, or you need to implement the `Block.compute_output_spec() / compute_output_shape()` method. Error encountered:

Exception encountered when calling MultiHeadAttention.call().

Output subscripts contain a label appearing more than once: abcdef,abcdef->abcdeff
# this too:
Output subscripts contain a label appearing more than once: abc,abc->abcc
```
",jm-willy,2024-10-04 20:13:16+00:00,['Venkat6871'],2024-11-13 10:29:27+00:00,2024-11-13 10:29:23+00:00,https://github.com/tensorflow/tensorflow/issues/77057,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.16', '')]","[{'comment_id': 2395996486, 'issue_id': 2567265805, 'author': 'Venkat6871', 'body': 'Hi **@jm-willy** ,\r\nI reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so that we can analyze it further?\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 7, 6, 16, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396484111, 'issue_id': 2567265805, 'author': 'jm-willy', 'body': ""Here: https://colab.research.google.com/drive/1kd5_mrlNL-rSNFbBL0ReuUNnGUf7YYw_?usp=sharing\r\n\r\nAlso, what different error did you find??\r\n\r\n**PD**: I think I may know the origin of some errors of `MultiHeadAttention` layer. The layer call can take 3 arguments: `call(x,x,x)`, as far as I know, it's the only layer that takes more than one raw tensor. All other layers either take a list of inputs or take a concatenated input to then split it. \r\n\r\nThis difference in `MultiHeadAttention.call()` is probably confronting lots of shape checks and related errors all over `tensorflow` and `keras`."", 'created_at': datetime.datetime(2024, 10, 7, 10, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472554219, 'issue_id': 2567265805, 'author': 'Venkat6871', 'body': 'Hi **@jm-willy** ,\r\nApologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow version 2.18.0 and faced the same issue. I found an alternative solution that worked for me, which I hope will help you as well. Please refer to the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cafe2daa591fb142ae85d57de158cad0/77057_tf-2-18-0-v.ipynb) here for more details.\r\nIf you have further questions, please consider raising this issue in the Keras repository, as it relates to Keras.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 13, 6, 23, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2473116351, 'issue_id': 2567265805, 'author': 'jm-willy', 'body': ""Thank you!! Can't believe it lacked a plain `compute_output_shape` with `layers.Layer`. Still gives a horrible warning, but it looks more like a keras implementation problem, probably lacking a more complex `compute_output_shape` method, which I tried the first but didn't got right."", 'created_at': datetime.datetime(2024, 11, 13, 10, 29, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2473116426, 'issue_id': 2567265805, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77057"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77057"">No</a>', 'created_at': datetime.datetime(2024, 11, 13, 10, 29, 25, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-07 06:16:53 UTC): Hi **@jm-willy** ,
I reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so that we can analyze it further?
Thank you!

jm-willy (Issue Creator) on (2024-10-07 10:06:00 UTC): Here: https://colab.research.google.com/drive/1kd5_mrlNL-rSNFbBL0ReuUNnGUf7YYw_?usp=sharing

Also, what different error did you find??

**PD**: I think I may know the origin of some errors of `MultiHeadAttention` layer. The layer call can take 3 arguments: `call(x,x,x)`, as far as I know, it's the only layer that takes more than one raw tensor. All other layers either take a list of inputs or take a concatenated input to then split it. 

This difference in `MultiHeadAttention.call()` is probably confronting lots of shape checks and related errors all over `tensorflow` and `keras`.

Venkat6871 (Assginee) on (2024-11-13 06:23:06 UTC): Hi **@jm-willy** ,
Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow version 2.18.0 and faced the same issue. I found an alternative solution that worked for me, which I hope will help you as well. Please refer to the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cafe2daa591fb142ae85d57de158cad0/77057_tf-2-18-0-v.ipynb) here for more details.
If you have further questions, please consider raising this issue in the Keras repository, as it relates to Keras.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.
Thank you!

jm-willy (Issue Creator) on (2024-11-13 10:29:23 UTC): Thank you!! Can't believe it lacked a plain `compute_output_shape` with `layers.Layer`. Still gives a horrible warning, but it looks more like a keras implementation problem, probably lacking a more complex `compute_output_shape` method, which I tried the first but didn't got right.

google-ml-butler[bot] on (2024-11-13 10:29:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77057"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77057"">No</a>

"
2566857445,issue,closed,completed,Custom activation functions cause TensorFlow to crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Mint 22

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected the code to run. Instead, TensorFlow crashes. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.utils import get_custom_objects
from tensorflow.keras.layers import Activation

def fourier_activation_lambda(freq):
    fn = lambda x : tf.sin(freq*x)
    return(fn)

freq = 1.0
fourier = fourier_activation_lambda(freq)

get_custom_objects()[""fourier""] = Activation(fourier)

print(3*""\n"")
print(f""After addition: {get_custom_objects()=}"")

x_input = tf.keras.Input(shape=[5])
activation = ""fourier""
layer_2 = tf.keras.layers.Dense(100, input_shape = [5],
                                activation=activation,
                                )(x_input)

model = tf.keras.Model(inputs=x_input, outputs=layer_2)
model.compile(optimizer='adam', loss='mse')
model.summary()
```


### Relevant log output

```shell
# Output of print statement:
get_custom_objects()={'fourier': <Activation name=activation, built=False>}

# Error message:
Traceback (most recent call last):
  File ""/home/orca/Downloads/minimal_tf_err.py"", line 20, in <module>
    layer_2 = tf.keras.layers.Dense(100, input_shape = [5],
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/orca/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py"", line 89, in __init__
    self.activation = activations.get(activation)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/orca/.local/lib/python3.12/site-packages/keras/src/activations/__init__.py"", line 104, in get
    raise ValueError(
ValueError: Could not interpret activation function identifier: fourier
```
",AtticusBeachy,2024-10-04 17:17:45+00:00,['tilakrayal'],2024-10-08 14:14:51+00:00,2024-10-08 14:14:47+00:00,https://github.com/tensorflow/tensorflow/issues/77048,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2398885763, 'issue_id': 2566857445, 'author': 'tilakrayal', 'body': '@AtticusBeachy,\r\nLooks like this issue is more related to Keras. Could you please raise the request in the Keras-team/keras repo from [here](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 10, 8, 5, 40, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399979206, 'issue_id': 2566857445, 'author': 'AtticusBeachy', 'body': 'Okay, I have reposted in the [Keras repo](https://github.com/keras-team/keras/issues/20333).', 'created_at': datetime.datetime(2024, 10, 8, 14, 14, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399979305, 'issue_id': 2566857445, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77048"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77048"">No</a>', 'created_at': datetime.datetime(2024, 10, 8, 14, 14, 50, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-08 05:40:33 UTC): @AtticusBeachy,
Looks like this issue is more related to Keras. Could you please raise the request in the Keras-team/keras repo from [here](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!

AtticusBeachy (Issue Creator) on (2024-10-08 14:14:47 UTC): Okay, I have reposted in the [Keras repo](https://github.com/keras-team/keras/issues/20333).

google-ml-butler[bot] on (2024-10-08 14:14:50 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77048"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77048"">No</a>

"
2566404258,issue,open,,NotImplementedError from tf.constant in trivial case,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. 
I get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array.`
I am not trying to use numpy, this is an internal error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
import numpy as np

class CustomModel(keras.models.Model):
    def call(self, inputs):
        inputs_shape = tf.shape(inputs)
        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError
        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK

model = CustomModel()
model.compile(run_eagerly=False, loss=""mse"")  # OK if run_eagerly=True
model.fit(np.array([[0.0]]), np.array([[0.0]]))
```
```


### Relevant log output

```shell
{
	""name"": ""NotImplementedError"",
	""message"": ""Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)"",
	""stack"": ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[6], line 13
     11 model = CustomModel()
     12 model.compile(run_eagerly=False, loss=\""mse\"")  # OK if run_eagerly=True
---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))

File /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

Cell In[6], line 8, in CustomModel.call(self, inputs)
      6 def call(self, inputs):
      7     inputs_shape = tf.shape(inputs)
----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)
   2979 @array_function_dispatch(_prod_dispatcher)
   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,
   2981          initial=np._NoValue, where=np._NoValue):
   2982     \""\""\""
   2983     Return the product of array elements over a given axis.
   2984 
   (...)
   3098     10
   3099     \""\""\""
-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
   3101                           keepdims=keepdims, initial=initial, where=where)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85         else:
     86             return reduction(axis=axis, out=out, **passkwargs)
---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

NotImplementedError: Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)""
}
```
",richardwhitehead,2024-10-04 13:38:25+00:00,['Venkat6871'],2025-01-03 11:56:15+00:00,,https://github.com/tensorflow/tensorflow/issues/77045,"[('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2395963902, 'issue_id': 2566404258, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.16.1, 2.17.0 and the nightly version. I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/aa1beb985ebb689d83cf7acdd08b350e/77045_tf-2-16-2-17-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 7, 5, 51, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568753199, 'issue_id': 2566404258, 'author': 'Venkat6871', 'body': 'Hi **@richardwhitehead** ,\r\nApologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The `tf.constant` function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue.\r\nTo resolve this, you can use `tf.fill` or `tf.ones` multiplied by the desired value instead of `tf.constant`. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/74e9c150b388d53b31aacb2cc2456d33/77045_tf_2-18-0-v.ipynb) attached here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 3, 6, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569112792, 'issue_id': 2566404258, 'author': 'richardwhitehead', 'body': 'Thanks. Maybe a note in the documentation could be added?On 3 Jan 2025 06:35, Venkat6871 ***@***.***> wrote:\r\nHi @richardwhitehead ,\r\nApologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The tf.constant function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue.\r\nTo resolve this, you can use tf.fill or tf.ones multiplied by the desired value instead of tf.constant. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the gist attached here for your reference.\r\nThank you!\r\n\r\n—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2025, 1, 3, 11, 56, 12, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-07 05:51:04 UTC): I tried running your code on Colab using TensorFlow v2.16.1, 2.17.0 and the nightly version. I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/aa1beb985ebb689d83cf7acdd08b350e/77045_tf-2-16-2-17-nightly-v.ipynb) here for reference.
Thank you!

Venkat6871 (Assginee) on (2025-01-03 06:35:33 UTC): Hi **@richardwhitehead** ,
Apologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The `tf.constant` function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue.
To resolve this, you can use `tf.fill` or `tf.ones` multiplied by the desired value instead of `tf.constant`. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/74e9c150b388d53b31aacb2cc2456d33/77045_tf_2-18-0-v.ipynb) attached here for your reference.
Thank you!

richardwhitehead (Issue Creator) on (2025-01-03 11:56:12 UTC): Thanks. Maybe a note in the documentation could be added?On 3 Jan 2025 06:35, Venkat6871 ***@***.***> wrote:
Hi @richardwhitehead ,
Apologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The tf.constant function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue.
To resolve this, you can use tf.fill or tf.ones multiplied by the desired value instead of tf.constant. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the gist attached here for your reference.
Thank you!

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>

"
2565340019,issue,open,,Request to bring back GPU compatibility checks for TFLite `model_analyzer`,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf_nightly == 2.19.0.dev20241003

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using the nightly version I discover that the GPU compatibility checks are deprecated for the [model_analyzer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc#L443) tool in TF lite, following [this PR](https://github.com/tensorflow/tensorflow/pull/74830).

Currently the code for checking GPU compatibility is deprecated, but the output still prints ""Your model is compatible with GPU delegate"" (because there are essentially no checks). IMO this is actually confusing. I would suggest to change the output print to ""Skipping GPU compatibility as it is deprecated"", or just deprecate the `gpu_compatibility` boolean flag.

The previous logic is handy enough for my use case to expose non-compatible operators beforehand, and apply the tunings required manually on the `.tflite` graph, so curious why it is deprecated and what are the plans moving forward. Thanks!

### Standalone code to reproduce the issue

```shell
Attached PR regarding deprecation: https://github.com/tensorflow/tensorflow/pull/74830
```


### Relevant log output

_No response_",gudgud96,2024-10-04 03:08:37+00:00,"['lina128', 'terryheo', 'gaikwadrahul8', 'pkgoogle']",2024-10-30 21:59:03+00:00,,https://github.com/tensorflow/tensorflow/issues/77042,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2392713598, 'issue_id': 2565340019, 'author': 'gudgud96', 'body': 'Further confirmed that this is deprecated since `tensorflow==2.18.0rc0`.', 'created_at': datetime.datetime(2024, 10, 4, 3, 15, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397388277, 'issue_id': 2565340019, 'author': 'gaikwadrahul8', 'body': ""Hi, @gudgud96 \r\n\r\nThank you for bringing this issue to our attention, I see this PR https://github.com/tensorflow/tensorflow/pull/74830 was merged on September 12, 2024 for `Deprecating GPU compatibility experimental feature from both tf/compiler/mlir/lite:flatbuffer_export and tf/lite/python/analyzer_wrapper:model_analyzer` and it's showing below messages so will need to check and will update you.\r\n\r\n**1. For TFLite runtime version 2.18.0-rc0** please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/083419460da9a2f45ecb3f36dcb9ce53/tf-2-18-0rc0-model_analyzer.ipynb)\r\n\r\n```\r\nYour model looks compatible with GPU delegate on TFLite runtime version 2.18.0-rc0.\r\nThis does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.\r\n```\r\n**2. For TFLite runtime version 2.19.0-dev20241003** please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ce88c11478f701c33a0bd1cbf16efafb/tf-nightly-model_analyzer.ipynb)\r\n\r\n```\r\nYour model looks compatible with GPU delegate on TFLite runtime version 2.19.0-dev20241003.\r\nThis does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.\r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 7, 16, 29, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413586519, 'issue_id': 2565340019, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nPlease take look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 10, 15, 11, 7, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448379689, 'issue_id': 2565340019, 'author': 'pkgoogle', 'body': 'Seems like we have future plans for something better. @terryheo assigning since you were mentioned on the PR. Thanks.', 'created_at': datetime.datetime(2024, 10, 30, 21, 4, 50, tzinfo=datetime.timezone.utc)}]","gudgud96 (Issue Creator) on (2024-10-04 03:15:05 UTC): Further confirmed that this is deprecated since `tensorflow==2.18.0rc0`.

gaikwadrahul8 (Assginee) on (2024-10-07 16:29:24 UTC): Hi, @gudgud96 

Thank you for bringing this issue to our attention, I see this PR https://github.com/tensorflow/tensorflow/pull/74830 was merged on September 12, 2024 for `Deprecating GPU compatibility experimental feature from both tf/compiler/mlir/lite:flatbuffer_export and tf/lite/python/analyzer_wrapper:model_analyzer` and it's showing below messages so will need to check and will update you.

**1. For TFLite runtime version 2.18.0-rc0** please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/083419460da9a2f45ecb3f36dcb9ce53/tf-2-18-0rc0-model_analyzer.ipynb)

```
Your model looks compatible with GPU delegate on TFLite runtime version 2.18.0-rc0.
This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.
```
**2. For TFLite runtime version 2.19.0-dev20241003** please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ce88c11478f701c33a0bd1cbf16efafb/tf-nightly-model_analyzer.ipynb)

```
Your model looks compatible with GPU delegate on TFLite runtime version 2.19.0-dev20241003.
This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.
```

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-15 11:07:42 UTC): Hi, @pkgoogle

Please take look into this issue. Thank you.

pkgoogle (Assginee) on (2024-10-30 21:04:50 UTC): Seems like we have future plans for something better. @terryheo assigning since you were mentioned on the PR. Thanks.

"
2564251382,issue,closed,completed,failed to load the native TensorFlow runtime,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Microsoft Windows 10.0

### Mobile device

_No response_

### Python version

3.12.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

it shows that an import error "" DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/admin/1.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/admin/1.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",arzappu,2024-10-03 14:44:14+00:00,['Venkat6871'],2024-10-19 02:00:56+00:00,2024-10-19 02:00:53+00:00,https://github.com/tensorflow/tensorflow/issues/77016,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2393111776, 'issue_id': 2564251382, 'author': 'Venkat6871', 'body': 'Hi **@arzappu**  ,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 4, 8, 15, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408301513, 'issue_id': 2564251382, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 12, 1, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423475744, 'issue_id': 2564251382, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 19, 2, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423475780, 'issue_id': 2564251382, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77016"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77016"">No</a>', 'created_at': datetime.datetime(2024, 10, 19, 2, 0, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-04 08:15:20 UTC): Hi **@arzappu**  ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

github-actions[bot] on (2024-10-12 01:59:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-19 02:00:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-19 02:00:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77016"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/77016"">No</a>

"
2562365056,issue,open,,TFlite compilation crashes on MacOS (error: _Float16 is not supported on this target),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

MacOS 15.0

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

Apple clang version 16.0.0 (clang-1600.0.26.3)
XCode 16.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling TF_lite v2.18.0-rc0 from source, using the command suggested in the documentation leads to crash (log below):

```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native```

### Standalone code to reproduce the issue

```shell
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```


### Relevant log output

```shell
TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \
    ZERO_AR_DATE=1 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' 'DEBUG_PREFIX_MAP_PWD=.' -iquote external/XNNPACK -iquote bazel-out/darwin-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/darwin-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/darwin-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/darwin-opt/bin/external/cpuinfo -iquote external/FP16 -iquote bazel-out/darwin-opt/bin/external/FP16 -Ibazel-out/darwin-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/darwin-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/darwin-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/XNNPACK/include -isystem bazel-out/darwin-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/darwin-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/darwin-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/darwin-opt/bin/external/FXdiv/include -isystem external/cpuinfo/include -isystem bazel-out/darwin-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/darwin-opt/bin/external/cpuinfo/src -isystem external/FP16/include -isystem bazel-out/darwin-opt/bin/external/FP16/include -MD -MF bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.d -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DBAZEL_CURRENT_REPOSITORY=""XNNPACK""' '-frandom-seed=bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks -no-canonical-prefixes -pthread -DGRPC_BAZEL_BUILD -w -O3 '-march=native' -Iinclude -Isrc '-DXNN_ENABLE_CPUINFO=1' '-std=c99' -O2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -target x86_64-apple-macosx15.0 -c external/XNNPACK/src/configs/cmul-config.c -o bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o)
# Configuration: 7ffafbfebd31d6f3229fc3b4603178937ffcc0387347731c7b47fcb97e2cd76d
# Execution platform: @local_execution_config_platform//:platform
ERROR: /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK//:sse_prod_microkernels) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)
In file included from bazel-out/darwin-opt/bin/external/XNNPACK/sse_prod_microkernels.c:1:
In file included from external/XNNPACK/src/xnnpack/avgpool.h:15:
In file included from external/XNNPACK/src/xnnpack/microparams.h:12:
In file included from external/XNNPACK/src/xnnpack/math.h:21:
In file included from bazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16/fp16/fp16.h:10:
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:27: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:8: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:28: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:38: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                      ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:8: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:27: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:8: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:27: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:8: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:28: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:8: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:27: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:8: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:28: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:8: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:28: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:8: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:31: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                               ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:41: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                         ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 119.893s, Critical Path: 3.17s
INFO: 342 processes: 313 internal, 29 local.
FAILED: Build did NOT complete successfully
```
",feranick,2024-10-02 18:20:47+00:00,"['terryheo', 'gaikwadrahul8', 'pkgoogle']",2024-10-29 17:54:04+00:00,,https://github.com/tensorflow/tensorflow/issues/76976,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2394036072, 'issue_id': 2562365056, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nThank you for bringing this issue to our attention, I was trying to replicate the same issue from my end but I was encountering the different issue so if you don't mind could you please help me with exact steps which you followed before encountering this issue so I'll try to replicate the same behavior from my end also ?\r\n\r\nThank you for your cooperation"", 'created_at': datetime.datetime(2024, 10, 4, 16, 10, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394258139, 'issue_id': 2562365056, 'author': 'feranick', 'body': 'Sure, thing. The crash happens with the following code, which is what is indicated in the [official site](https://ai.google.dev/edge/litert/build/cmake_pip):\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout v2.18.0-rc0\r\nPYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native\r\n```\r\n\r\n[log_bazel.txt](https://github.com/user-attachments/files/17262037/log_bazel.txt)\r\n\r\n\r\nIf I run it with cmake:\r\n\r\n\r\n```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native```\r\n\r\n\r\nI get the another error, as in the following log:\r\n[log_cmake.txt](https://github.com/user-attachments/files/17262018/log_cmake.txt)', 'created_at': datetime.datetime(2024, 10, 4, 17, 39, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394263920, 'issue_id': 2562365056, 'author': 'feranick', 'body': 'This may be an issue with XCode 16.0 (which I am using) while compiling XNNPACK. A similar error in other software using XNNPACK was seen and fixed:\r\n\r\nhttps://github.com/microsoft/onnxruntime/pull/22294', 'created_at': datetime.datetime(2024, 10, 4, 17, 43, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399527144, 'issue_id': 2562365056, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick\r\n\r\nI apologize for the delayed response, I'm able to replicate the same behavior from my end for reference I've added error log below and it seems like this is an issue with `XCode 16.0 `while compiling XNNPACK so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.\r\n\r\n```\r\n(cd /private/var/tmp/_bazel_gaikwadrahul/bbf47ac5a9b5699942dfbbcf52ff3662/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=15.0 \\\r\n    PATH=/Users/gaikwadrahul/miniconda3/bin:/Users/gaikwadrahul/Downloads/google-cloud-sdk/bin:/Users/gaikwadrahul/.nvm/versions/node/v20.15.1/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/git/git-google/bin:/usr/local/git/current/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin \\\r\n    PYTHON_BIN_PATH=/opt/homebrew/bin/python3.12 \\\r\n    PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \\\r\n    ZERO_AR_DATE=1 \\\r\n  external/local_config_cc/libtool @bazel-out/darwin-opt/bin/external/XNNPACK/liboperators.a-2.params)\r\n# Configuration: 03bc32a137f4cc6ee22da5a9cdc56cdcc0ecaa1e39a68e9962f856d35d4b4862\r\n# Execution platform: @local_execution_config_platform//:platform\r\nERROR: /private/var/tmp/_bazel_gaikwadrahul/bbf47ac5a9b5699942dfbbcf52ff3662/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK//:sse_prod_microkernels) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)\r\nIn file included from bazel-out/darwin-opt/bin/external/XNNPACK/sse_prod_microkernels.c:1:\r\nIn file included from external/XNNPACK/src/xnnpack/avgpool.h:15:\r\nIn file included from external/XNNPACK/src/xnnpack/microparams.h:12:\r\nIn file included from external/XNNPACK/src/xnnpack/math.h:21:\r\nIn file included from bazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16/fp16/fp16.h:10:\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:27: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:8: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:28: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:38: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                      ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:8: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:27: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:8: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:27: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:8: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:28: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:8: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:27: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:8: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:28: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:8: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:28: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:8: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:31: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                               ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:41: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                         ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nError in child process '/usr/bin/xcrun'. 1\r\nTarget //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 34.503s, Critical Path: 9.50s\r\nINFO: 293 processes: 104 internal, 189 local.\r\nFAILED: Build did NOT complete successfully\r\nbash-3.2$ \r\n\r\n```\r\n\r\nEDIT : I tried with cmake command and I'm getting same error messages which we're getting with bazel command please refer this error [output log](https://pastebin.com/wPfAT101)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 8, 10, 58, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409114290, 'issue_id': 2562365056, 'author': 'feranick', 'body': 'The same error message happens when building regular Tensorflow 2.18.0-rc', 'created_at': datetime.datetime(2024, 10, 13, 20, 28, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411230897, 'issue_id': 2562365056, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 14, 13, 11, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442426275, 'issue_id': 2562365056, 'author': 'pkgoogle', 'body': 'Hi @feranick, I was able to successfully build using 2.18.0-rc2, can you try with that and let me know if that works?', 'created_at': datetime.datetime(2024, 10, 28, 19, 17, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442580852, 'issue_id': 2562365056, 'author': 'feranick', 'body': ""Hi @pkgoogle. It still doesn't work for me, same error message on TF 2.18.0. Note, I am using MacOSX 15.1, XCode 16. Apple clang version 16.0.0 (clang-1600.0.26.3)."", 'created_at': datetime.datetime(2024, 10, 28, 20, 39, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442635822, 'issue_id': 2562365056, 'author': 'feranick', 'body': '@pkgoogle Again, this seems to be an issue related to the XNNPACK as seen elsewhere. \r\n\r\nhttps://github.com/microsoft/onnxruntime/pull/22294', 'created_at': datetime.datetime(2024, 10, 28, 21, 10, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444970527, 'issue_id': 2562365056, 'author': 'pkgoogle', 'body': ""I'm on Sonoma 14.7 M1. Xcode 15.0.1 (15A507)\r\n\r\n```sh\r\n$ clang --version\r\nHomebrew clang version 18.1.4\r\nTarget: arm64-apple-darwin23.6.0\r\nThread model: posix\r\n```\r\n\r\nIt seems we haven't updated our Compiler table for MacOS CPU... but updating clang might be a fast solution. @terryheo can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 10, 29, 17, 53, 23, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-04 16:10:15 UTC): Hi, @feranick 

Thank you for bringing this issue to our attention, I was trying to replicate the same issue from my end but I was encountering the different issue so if you don't mind could you please help me with exact steps which you followed before encountering this issue so I'll try to replicate the same behavior from my end also ?

Thank you for your cooperation

feranick (Issue Creator) on (2024-10-04 17:39:43 UTC): Sure, thing. The crash happens with the following code, which is what is indicated in the [official site](https://ai.google.dev/edge/litert/build/cmake_pip):
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.18.0-rc0
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```

[log_bazel.txt](https://github.com/user-attachments/files/17262037/log_bazel.txt)


If I run it with cmake:


```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native```


I get the another error, as in the following log:
[log_cmake.txt](https://github.com/user-attachments/files/17262018/log_cmake.txt)

feranick (Issue Creator) on (2024-10-04 17:43:51 UTC): This may be an issue with XCode 16.0 (which I am using) while compiling XNNPACK. A similar error in other software using XNNPACK was seen and fixed:

https://github.com/microsoft/onnxruntime/pull/22294

gaikwadrahul8 (Assginee) on (2024-10-08 10:58:10 UTC): Hi, @feranick

I apologize for the delayed response, I'm able to replicate the same behavior from my end for reference I've added error log below and it seems like this is an issue with `XCode 16.0 `while compiling XNNPACK so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.

```
(cd /private/var/tmp/_bazel_gaikwadrahul/bbf47ac5a9b5699942dfbbcf52ff3662/execroot/org_tensorflow && \
  exec env - \
    APPLE_SDK_PLATFORM=MacOSX \
    APPLE_SDK_VERSION_OVERRIDE=15.0 \
    PATH=/Users/gaikwadrahul/miniconda3/bin:/Users/gaikwadrahul/Downloads/google-cloud-sdk/bin:/Users/gaikwadrahul/.nvm/versions/node/v20.15.1/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/git/git-google/bin:/usr/local/git/current/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin \
    PYTHON_BIN_PATH=/opt/homebrew/bin/python3.12 \
    PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages \
    TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \
    ZERO_AR_DATE=1 \
  external/local_config_cc/libtool @bazel-out/darwin-opt/bin/external/XNNPACK/liboperators.a-2.params)
# Configuration: 03bc32a137f4cc6ee22da5a9cdc56cdcc0ecaa1e39a68e9962f856d35d4b4862
# Execution platform: @local_execution_config_platform//:platform
ERROR: /private/var/tmp/_bazel_gaikwadrahul/bbf47ac5a9b5699942dfbbcf52ff3662/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK//:sse_prod_microkernels) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)
In file included from bazel-out/darwin-opt/bin/external/XNNPACK/sse_prod_microkernels.c:1:
In file included from external/XNNPACK/src/xnnpack/avgpool.h:15:
In file included from external/XNNPACK/src/xnnpack/microparams.h:12:
In file included from external/XNNPACK/src/xnnpack/math.h:21:
In file included from bazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16/fp16/fp16.h:10:
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:27: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:8: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:28: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:38: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                      ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:8: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:27: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:8: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:27: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:8: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:28: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:8: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:27: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:8: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:28: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:8: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:28: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:8: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:31: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                               ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:41: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                         ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 34.503s, Critical Path: 9.50s
INFO: 293 processes: 104 internal, 189 local.
FAILED: Build did NOT complete successfully
bash-3.2$ 

```

EDIT : I tried with cmake command and I'm getting same error messages which we're getting with bazel command please refer this error [output log](https://pastebin.com/wPfAT101)

Thank you for your cooperation and patience.

feranick (Issue Creator) on (2024-10-13 20:28:57 UTC): The same error message happens when building regular Tensorflow 2.18.0-rc

gaikwadrahul8 (Assginee) on (2024-10-14 13:11:55 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-28 19:17:24 UTC): Hi @feranick, I was able to successfully build using 2.18.0-rc2, can you try with that and let me know if that works?

feranick (Issue Creator) on (2024-10-28 20:39:02 UTC): Hi @pkgoogle. It still doesn't work for me, same error message on TF 2.18.0. Note, I am using MacOSX 15.1, XCode 16. Apple clang version 16.0.0 (clang-1600.0.26.3).

feranick (Issue Creator) on (2024-10-28 21:10:04 UTC): @pkgoogle Again, this seems to be an issue related to the XNNPACK as seen elsewhere. 

https://github.com/microsoft/onnxruntime/pull/22294

pkgoogle (Assginee) on (2024-10-29 17:53:23 UTC): I'm on Sonoma 14.7 M1. Xcode 15.0.1 (15A507)

```sh
$ clang --version
Homebrew clang version 18.1.4
Target: arm64-apple-darwin23.6.0
Thread model: posix
```

It seems we haven't updated our Compiler table for MacOS CPU... but updating clang might be a fast solution. @terryheo can you please take a look? Thanks.

"
2561613048,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am getting the error ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. when I try to import tensorflow. I've tried installing the c++ redistributables, tried different versions of Python, different versions of Tensorflow, different environments including a completely new venv, but I'm at a loss. Any ideas?

### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
ImportError: Traceback (most recent call last):
  File ""C:\Users\NAME\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.        
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",adamrickayzenanvil,2024-10-02 13:07:42+00:00,['Venkat6871'],2024-12-18 17:54:44+00:00,2024-12-18 17:54:41+00:00,https://github.com/tensorflow/tensorflow/issues/76961,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2393109056, 'issue_id': 2561613048, 'author': 'Venkat6871', 'body': 'Hi @adamrickayzenanvil ,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 4, 8, 13, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393144894, 'issue_id': 2561613048, 'author': 'adamrickayzenanvil', 'body': ""Hi,\r\n\r\nMy processor is a 'Processor\tSnapdragon(R) X Elite - X1E80100 - Qualcomm(R) Oryon(TM) CPU, 4012 Mhz, 12 Core(s), 12 Logical Processor(s)'. It apparently does support AVX2 instructions. We did run into issues installing the redistributable due to the ARM architecture but our company's IT team say they have installed it now.\r\n![image](https://github.com/user-attachments/assets/6d1343ce-4af7-42cc-8e02-2ecc950336b2)\r\n\r\n\r\nThe versions are \r\ntensorboard                  2.17.1\r\ntensorboard-data-server      0.7.2\r\ntensorflow                   2.17.0\r\ntensorflow-estimator         2.12.0\r\ntensorflow-intel             2.17.0\r\ntensorflow-io-gcs-filesystem 0.31.0\r\n\r\nMy python version is: Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32\r\n\r\nI'm not sure how to check if the libraries are installed elsewhere, though I have tried installing using a venv, my system etc.\r\n\r\nMany thanks!"", 'created_at': datetime.datetime(2024, 10, 4, 8, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446109301, 'issue_id': 2561613048, 'author': 'Venkat6871', 'body': 'Hi **@adamrickayzenanvil** ,\r\nApologies for the delay, and thank you for your patience. Could you please check with the new version (TF-2.18.0-v) and let us know if the issue still persists?\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 30, 8, 2, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446278454, 'issue_id': 2561613048, 'author': 'adamrickayzenanvil', 'body': 'Thanks @Venkat6871, unfortunately the issue is still going on', 'created_at': datetime.datetime(2024, 10, 30, 9, 16, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551943903, 'issue_id': 2561613048, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 54, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551943944, 'issue_id': 2561613048, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76961"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76961"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 54, 43, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-04 08:13:51 UTC): Hi @adamrickayzenanvil ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

adamrickayzenanvil (Issue Creator) on (2024-10-04 08:32:40 UTC): Hi,

My processor is a 'Processor	Snapdragon(R) X Elite - X1E80100 - Qualcomm(R) Oryon(TM) CPU, 4012 Mhz, 12 Core(s), 12 Logical Processor(s)'. It apparently does support AVX2 instructions. We did run into issues installing the redistributable due to the ARM architecture but our company's IT team say they have installed it now.
![image](https://github.com/user-attachments/assets/6d1343ce-4af7-42cc-8e02-2ecc950336b2)


The versions are 
tensorboard                  2.17.1
tensorboard-data-server      0.7.2
tensorflow                   2.17.0
tensorflow-estimator         2.12.0
tensorflow-intel             2.17.0
tensorflow-io-gcs-filesystem 0.31.0

My python version is: Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32

I'm not sure how to check if the libraries are installed elsewhere, though I have tried installing using a venv, my system etc.

Many thanks!

Venkat6871 (Assginee) on (2024-10-30 08:02:56 UTC): Hi **@adamrickayzenanvil** ,
Apologies for the delay, and thank you for your patience. Could you please check with the new version (TF-2.18.0-v) and let us know if the issue still persists?
Thank you!

adamrickayzenanvil (Issue Creator) on (2024-10-30 09:16:50 UTC): Thanks @Venkat6871, unfortunately the issue is still going on

mihaimaruseac on (2024-12-18 17:54:41 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:54:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76961"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76961"">No</a>

"
2561610080,issue,closed,completed,AttributeError when importing tensorflow in python 3.9 and django 4.2 project,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17 and tf 2.13

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

Ubuntu 22.04

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When i try to run my server locally (windows 11) using manage.py runserver or on my ubuntu 22.04 server using uwsgi I am getting an error output from theenviron module as soon as tf tries to run to get the site package directories. If i remove tf and my new endpoint then everything runs and works if i add tf back it immediateloy starts failing again. I have tried this with tf 2.17.0 and tf 2.13.1 with the exact same result.

The code below i obviously have a whole project, but the endpoint in its own file set up like it is below causes it to fail to run with the same error. If I comment out the tf import then everything runs completely fine, but of course then i can't have my image classifier on my server.

It may be worth noting that whemn i had this as a script that iran directly it works just fine, this is only an issue when running it as part of my django rest framework api. 

Any help would be appreciated. 

### Standalone code to reproduce the issue

```shell
from rest_framework.decorators import api_view
from rest_framework.response import Response
import tensorflow as tf

@api_view(['POST'])
def MY_ENDPOINT(request):
    return Response()
```
```


### Relevant log output

```shell
File ""C:\Development\MY-SITE\views.py"", line 13, in <module>
    import tensorflow as tf
  File ""C:\Development\MY-SITE\venv3942\lib\site-packages\tensorflow\__init__.py"", line 415, in <module>
    _site_packages_dirs += [p for p in _sys.path if ""site-packages"" in p]
  File ""C:\Development\MY-SITE\venv3942\lib\site-packages\tensorflow\__init__.py"", line 415, in <listcomp>
    _site_packages_dirs += [p for p in _sys.path if ""site-packages"" in p]
  File ""C:\Development\MY-SITE\venv3942\lib\site-packages\environ\environ.py"", line 1068, in __contains__
    return item.__root__.startswith(base_path)
AttributeError: 'str' object has no attribute '__root__'
```


EDIT:

I can m ake it work by editing the tensorflow __init__.py locally to add a try/except like so, but i am not sure what other negative impacts that might have.

```
# Get sitepackages directories for the python installation.
_site_packages_dirs = []
if _site.ENABLE_USER_SITE and _site.USER_SITE is not None:
  _site_packages_dirs += [_site.USER_SITE]
try:
  _site_packages_dirs += [p for p in _sys.path if ""site-packages"" in p]
except:
  pass
```",bgriffin29,2024-10-02 13:07:02+00:00,['tilakrayal'],2024-10-24 02:01:48+00:00,2024-10-24 02:01:45+00:00,https://github.com/tensorflow/tensorflow/issues/76960,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2402231309, 'issue_id': 2561610080, 'author': 'tilakrayal', 'body': '@bgriffin29,\r\nLooks like this issue is not related to tensorflow installation and during the import of tensorflow. I suspect that the error was occured due to unittest tries to set path variables and somehow it is passing them in as strings instead of path objects.\r\nhttps://github.com/joke2k/django-environ/issues/197\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 12, 49, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418337340, 'issue_id': 2561610080, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 17, 2, 1, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073820, 'issue_id': 2561610080, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073881, 'issue_id': 2561610080, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76960"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76960"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-09 12:49:03 UTC): @bgriffin29,
Looks like this issue is not related to tensorflow installation and during the import of tensorflow. I suspect that the error was occured due to unittest tries to set path variables and somehow it is passing them in as strings instead of path objects.
https://github.com/joke2k/django-environ/issues/197

Thank you!

github-actions[bot] on (2024-10-17 02:01:38 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:45 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:01:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76960"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76960"">No</a>

"
2561463809,issue,closed,completed,TensorFlow 2.17.0 fails to install with grpcio,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Windows 11/ wsl  2.1.5.0

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

I'm trying to install tesnsorflow 2.17 under the same environment with protobuf==5.27.2 grpcio-tools==1.64
and getting this error:

`tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.2 which is incompatible.`

Downgrading protobuf is not a option for me, are there any workarounds this issue


### Standalone code to reproduce the issue

```shell
pip install protobuf==5.27.2
pip install grpcio-tools==1.64
pip install tensorflow
```


### Relevant log output

```shell
The conflict is caused by:
    The user requested protobuf==5.27.2
    grpcio-tools 1.64.0 depends on protobuf<6.0dev and >=5.26.1
    tensorflow 2.17.0 depends on protobuf!=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.20.3
    The user requested protobuf==5.27.2
    grpcio-tools 1.64.0 depends on protobuf<6.0dev and >=5.26.1
    tensorflow 2.16.2 depends on protobuf!=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.20.3
    The user requested protobuf==5.27.2
    grpcio-tools 1.64.0 depends on protobuf<6.0dev and >=5.26.1
    tensorflow 2.16.1 depends on protobuf!=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.20.3

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip to attempt to solve the dependency conflict
```
",p1x31,2024-10-02 12:18:41+00:00,['Venkat6871'],2024-10-18 02:02:01+00:00,2024-10-18 02:01:58+00:00,https://github.com/tensorflow/tensorflow/issues/76957,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2390607396, 'issue_id': 2561463809, 'author': 'Venkat6871', 'body': 'Hi **@p1x31** ,\r\nThank you for raising the issue here. It is crucial to check all compatibility versions for everything to work smoothly. In your case, while downgrading the `protobuf` version is a common solution.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 3, 6, 15, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406393652, 'issue_id': 2561463809, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 11, 2, 1, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421079996, 'issue_id': 2561463809, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 18, 2, 1, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421080048, 'issue_id': 2561463809, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76957"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76957"">No</a>', 'created_at': datetime.datetime(2024, 10, 18, 2, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-03 06:15:44 UTC): Hi **@p1x31** ,
Thank you for raising the issue here. It is crucial to check all compatibility versions for everything to work smoothly. In your case, while downgrading the `protobuf` version is a common solution.
Thank you!

github-actions[bot] on (2024-10-11 02:01:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-18 02:01:58 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-18 02:02:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76957"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76957"">No</a>

"
2560229330,issue,open,,Tf_lite fails to compile in v2.18.0-rc0 in Ubuntu,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

ubuntu 22.04, ubuntu 24.04

### Mobile device

_No response_

### Python version

3.10, 3.12

### Bazel version

6.5

### GCC/compiler version

_No response_

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current behavior?

When follow the [normal instructions to build tf_lite](https://ai.google.dev/edge/litert/build/cmake_pip) as indicated, compilation fails, log is attached.

### Standalone code to reproduce the issue

```shell
Follow the normal instructions to build tf_lite as indicated here: https://ai.google.dev/edge/litert/build/cmake_pip

Compilation fails, log is attached.
```


### Relevant log output
Full log:
[ubuntu22.04.log](https://github.com/user-attachments/files/17216590/ubuntu22.04.log)
[ubuntu24.04.log](https://github.com/user-attachments/files/17216593/ubuntu24.04.log)


```
/usr/bin/ranlib libmicrokernels-prod.a
gmake[3]: Leaving directory '/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.12/cmake_build'
[ 69%] Built target microkernels-prod
gmake[2]: Leaving directory '/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.12/cmake_build'
gmake[1]: *** [CMakeFiles/Makefile2:1526: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2
gmake[1]: Leaving directory '/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.12/cmake_build'
gmake: *** [Makefile:208: _pywrap_tensorflow_interpreter_wrapper] Error 2

make: *** [Makefile:72: docker-build] Error 2
make: Leaving directory '/home/----/Software/tensorflow-dir/tflite_runtime/tensorflow/tensorflow/lite/tools/pip_package'
```
",feranick,2024-10-01 21:16:02+00:00,"['terryheo', 'pkgoogle']",2025-02-07 07:06:01+00:00,,https://github.com/tensorflow/tensorflow/issues/76908,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release'), ('2.18.rc', '')]","[{'comment_id': 2392194843, 'issue_id': 2560229330, 'author': 'feranick', 'body': 'Tagging: https://github.com/tensorflow/tensorflow/tree/v2.18.0-rc0', 'created_at': datetime.datetime(2024, 10, 3, 19, 42, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394035101, 'issue_id': 2560229330, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nThank you for bringing this issue to our attention, I was trying to replicate the same issue from my end but I was encountering the different issue so if you don't mind could you please help me with exact steps which you followed before encountering this issue so I'll try to replicate the same behavior from my end also ?\r\n\r\nThank you for your cooperation"", 'created_at': datetime.datetime(2024, 10, 4, 16, 9, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394217560, 'issue_id': 2560229330, 'author': 'feranick', 'body': 'Sure, thing. The crash happens with the following code:\r\n```git clone https://github.com/tensorflow/tensorflow.git```\r\n```cd tensorflow```\r\n```git checkout v2.18.0-rc0```\r\n```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native```\r\n\r\nIf I run it with bazel, it works just fine:\r\n\r\n```\r\nPYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native\r\n```', 'created_at': datetime.datetime(2024, 10, 4, 17, 33, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394303215, 'issue_id': 2560229330, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nThank you for providing the steps I tried with r2.17 and it's working as expected for reference I've added output log below so let me try with [2.18.0rc0 pre-release](https://pypi.org/project/tensorflow/2.18.0rc0/) and will update you it is working or not. Thank you\r\n\r\n**1. With  r2.17** \r\n\r\n```\r\nOutput can be found here:\r\n+ find /home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist\r\n/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist\r\n/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.17.1-cp312-cp312-linux_x86_64.whl\r\n/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.17.1.linux-x86_64.tar.gz\r\n+ [[ '' != \\y ]]\r\n+ exit 0\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-76908/tensorflow_src$ \r\n```"", 'created_at': datetime.datetime(2024, 10, 4, 18, 5, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394323300, 'issue_id': 2560229330, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nI'm able to replicate the similar behavior I've added output log below for reference so we'll have to dig more into this issue, thank you for bringing this issue to our attention I really appreciate your valuable time and efforts.\r\n\r\n```\r\nERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/f3c0f0f4ffcd0642798974de9e5b017d/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/avxvnniint8_prod_microkernels.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:avxvnniint8_prod_microkernels) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 114 arguments skipped)\r\ngcc: error: unrecognized command-line option '-mavxvnniint8'; did you mean '-mavxvnni'?\r\nTarget //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 81.080s, Critical Path: 21.74s\r\nINFO: 504 processes: 287 internal, 217 local.\r\nFAILED: Build did NOT complete successfully\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-76908/tensorflow_src$ \r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 4, 18, 18, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394330764, 'issue_id': 2560229330, 'author': 'feranick', 'body': 'Thank you. I am not sure, though, that is the same error. I have seen the same error (unrecognized flags) in the past for TF proper when trying to compile on Ubuntu 22.04, and it was due to the compiler being too old (v13). Using Ubuntu 24.04 (with its newer compiler v18), that flag error does no longer occur, but the one I reported does.', 'created_at': datetime.datetime(2024, 10, 4, 18, 22, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411226471, 'issue_id': 2560229330, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 14, 13, 10, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439017112, 'issue_id': 2560229330, 'author': 'pkgoogle', 'body': 'I tried with rc2, I seem to be failing around the same part:\r\n\r\n```sh\r\n""CMakeFiles/microkernels-prod.dir/src/x64-transposec/gen/x64-transposec-4x2-scalar-int.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x64-transposec/gen/x64-transposec-4x4-reuse-multi-avx.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx2-u128.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx512skx-vpshufb-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx512vbmi-vpermx2b-u128.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-scalar-u4.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packq/x8-packq-scalar-f32qp8-u1.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x16-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x32-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x4-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x8-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-16x16-reuse-mov-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-2x4-scalar-int.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-32x32-reuse-switch-avx2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x2-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x2-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x3-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x3-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x4-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x4-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-xm-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-xm-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-copy/xx-copy-scalar-memcpy.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-fill/xx-fill-scalar-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-fill/xx-fill-sse2-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-pad/xx-pad-p16-sse2-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-pad/xx-pad-p4-scalar-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-transposev/xx-transposev-1x1-scalar-memcpy.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2-k-over-64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2-k-over-2048.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-4.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-8.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-32.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-2048.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/vlog.c.o""\r\n/usr/bin/ranlib libmicrokernels-prod.a\r\ngmake[3]: Leaving directory \'xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build\'\r\n[ 76%] Built target microkernels-prod\r\ngmake[2]: Leaving directory \'\'xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build\'\r\ngmake[1]: *** [CMakeFiles/Makefile2:1526: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2\r\ngmake[1]: Leaving directory \'\'xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build\'\r\ngmake: *** [Makefile:208: _pywrap_tensorflow_interpreter_wrapper] Error 2\r\n```\r\n\r\n@terryheo, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 10, 25, 23, 1, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471669698, 'issue_id': 2560229330, 'author': 'feranick', 'body': 'This seems to occur only in darwin for x86_64. Compilation proceeds fine on aarch64 for MacOS (i.e. Apple M1, etc).', 'created_at': datetime.datetime(2024, 11, 12, 21, 59, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475828069, 'issue_id': 2560229330, 'author': 'fergushenderson', 'body': 'Regarding the log message in \r\n<https://github.com/tensorflow/tensorflow/issues/76908#issuecomment-2439017112>:\r\nthe important error message is earlier in the log.', 'created_at': datetime.datetime(2024, 11, 14, 9, 26, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642099624, 'issue_id': 2560229330, 'author': 'CaptainDario', 'body': 'Building tf lite v2.18 also fails for me on macOS x86 but works fine on ARM.', 'created_at': datetime.datetime(2025, 2, 7, 7, 6, tzinfo=datetime.timezone.utc)}]","feranick (Issue Creator) on (2024-10-03 19:42:08 UTC): Tagging: https://github.com/tensorflow/tensorflow/tree/v2.18.0-rc0

gaikwadrahul8 on (2024-10-04 16:09:42 UTC): Hi, @feranick 

Thank you for bringing this issue to our attention, I was trying to replicate the same issue from my end but I was encountering the different issue so if you don't mind could you please help me with exact steps which you followed before encountering this issue so I'll try to replicate the same behavior from my end also ?

Thank you for your cooperation

feranick (Issue Creator) on (2024-10-04 17:33:53 UTC): Sure, thing. The crash happens with the following code:
```git clone https://github.com/tensorflow/tensorflow.git```
```cd tensorflow```
```git checkout v2.18.0-rc0```
```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native```

If I run it with bazel, it works just fine:

```
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```

gaikwadrahul8 on (2024-10-04 18:05:59 UTC): Hi, @feranick 

Thank you for providing the steps I tried with r2.17 and it's working as expected for reference I've added output log below so let me try with [2.18.0rc0 pre-release](https://pypi.org/project/tensorflow/2.18.0rc0/) and will update you it is working or not. Thank you

**1. With  r2.17** 

```
Output can be found here:
+ find /home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.17.1-cp312-cp312-linux_x86_64.whl
/home/gaikwadrahul/tflite-76908/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.17.1.linux-x86_64.tar.gz
+ [[ '' != \y ]]
+ exit 0
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-76908/tensorflow_src$ 
```

gaikwadrahul8 on (2024-10-04 18:18:37 UTC): Hi, @feranick 

I'm able to replicate the similar behavior I've added output log below for reference so we'll have to dig more into this issue, thank you for bringing this issue to our attention I really appreciate your valuable time and efforts.

```
ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/f3c0f0f4ffcd0642798974de9e5b017d/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/avxvnniint8_prod_microkernels.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:avxvnniint8_prod_microkernels) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 114 arguments skipped)
gcc: error: unrecognized command-line option '-mavxvnniint8'; did you mean '-mavxvnni'?
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 81.080s, Critical Path: 21.74s
INFO: 504 processes: 287 internal, 217 local.
FAILED: Build did NOT complete successfully
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/tflite-76908/tensorflow_src$ 
```

Thank you for your cooperation and patience.

feranick (Issue Creator) on (2024-10-04 18:22:56 UTC): Thank you. I am not sure, though, that is the same error. I have seen the same error (unrecognized flags) in the past for TF proper when trying to compile on Ubuntu 22.04, and it was due to the compiler being too old (v13). Using Ubuntu 24.04 (with its newer compiler v18), that flag error does no longer occur, but the one I reported does.

gaikwadrahul8 on (2024-10-14 13:10:40 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-25 23:01:28 UTC): I tried with rc2, I seem to be failing around the same part:

```sh
""CMakeFiles/microkernels-prod.dir/src/x64-transposec/gen/x64-transposec-4x2-scalar-int.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x64-transposec/gen/x64-transposec-4x4-reuse-multi-avx.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx2-u128.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx512skx-vpshufb-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-avx512vbmi-vpermx2b-u128.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-lut/gen/x8-lut-scalar-u4.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packq/x8-packq-scalar-f32qp8-u1.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x16-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x32-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x4-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-packw/gen/x8-packw-x8-gemm-goi-scalar-u2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-16x16-reuse-mov-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-2x4-scalar-int.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-transposec/gen/x8-transposec-32x32-reuse-switch-avx2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x2-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x2-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x3-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x3-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x4-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-x4-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-xm-scalar.c.o"" ""CMakeFiles/microkernels-prod.dir/src/x8-zip/x8-zip-xm-sse2.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-copy/xx-copy-scalar-memcpy.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-fill/xx-fill-scalar-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-fill/xx-fill-sse2-u64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-pad/xx-pad-p16-sse2-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-pad/xx-pad-p4-scalar-u16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/xx-transposev/xx-transposev-1x1-scalar-memcpy.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2-k-over-64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2-k-over-2048.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-4.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-8.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-16.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-32.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-64.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/exp2minus-k-over-2048.c.o"" ""CMakeFiles/microkernels-prod.dir/src/tables/vlog.c.o""
/usr/bin/ranlib libmicrokernels-prod.a
gmake[3]: Leaving directory 'xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
[ 76%] Built target microkernels-prod
gmake[2]: Leaving directory ''xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
gmake[1]: *** [CMakeFiles/Makefile2:1526: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2
gmake[1]: Leaving directory ''xxxxxxxxx/git/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
gmake: *** [Makefile:208: _pywrap_tensorflow_interpreter_wrapper] Error 2
```

@terryheo, can you please take a look? Thanks.

feranick (Issue Creator) on (2024-11-12 21:59:43 UTC): This seems to occur only in darwin for x86_64. Compilation proceeds fine on aarch64 for MacOS (i.e. Apple M1, etc).

fergushenderson on (2024-11-14 09:26:36 UTC): Regarding the log message in 
<https://github.com/tensorflow/tensorflow/issues/76908#issuecomment-2439017112>:
the important error message is earlier in the log.

CaptainDario on (2025-02-07 07:06:00 UTC): Building tf lite v2.18 also fails for me on macOS x86 but works fine on ARM.

"
2559769287,issue,open,,Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

MRE
-------
The following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:
```python
import tensorflow as tf

def cumsum(xs):
    return tf.scan(
        lambda a, x: a + x, elems=xs
    )

@tf.function(jit_compile=True)
def vec_cumsum(xs):
    return tf.vectorized_map(cumsum, elems=xs)

xs_batched = tf.reshape(tf.range(30), (3, 10))
vec_cumsum(xs_batched)
```

__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.

__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.

In JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.

May be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?

### Standalone code to reproduce the issue

```shell
Colab MRE: https://colab.research.google.com/drive/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-26ea491cd046> in <cell line: 13>()
     11 
     12 # Fails with ""No registered 'TensorListReserve'""""
---> 13 vec_cumsum(xs_batched)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
```
",chrism0dwk,2024-10-01 17:01:36+00:00,['Venkat6871'],2024-10-10 10:01:36+00:00,,https://github.com/tensorflow/tensorflow/issues/76891,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2390561538, 'issue_id': 2559769287, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/9ae3c160eacc131193f419627edb618a/76891_tf-nightly-v.ipynb) here for reference.\r\nThank you', 'created_at': datetime.datetime(2024, 10, 3, 5, 33, 20, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-03 05:33:20 UTC): I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/9ae3c160eacc131193f419627edb618a/76891_tf-nightly-v.ipynb) here for reference.
Thank you

"
2557559950,issue,closed,completed,TF model trained with 2.7 has error in later versions with no OpKernel error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Models trained with TF 2.7 is not able to run in later versions, like 2.16.1, with ""No OpKernel was registered to support Op 'BoostedTreesBucketize'"" error.

### Standalone code to reproduce the issue

We are using Nvidia's Triton model inference platform, when using the latest image which builds upon TF 2.16.1 to load certain models trained with TF 2.7, it runs into error as below
""No OpKernel was registered to support Op 'BoostedTreesBucketize' used by {{node transform/transform/bucketize/apply_buckets/assign_buckets_all_shapes/assign_buckets/BoostedTreesBucketize}} with these attrs: [_output_shapes=[[?]], num_features=1]""  

However, BoostedTreesBucketize shows unchanged since TF 2.7 to the latest version: https://github.com/tensorflow/tensorflow/blob/v2.17.0/tensorflow/core/ops/boosted_trees_ops.cc#L844. 

Is there any change around boosted tree ops or this specific op after 2.7 which leads to this error?

Detailed logs:
```shell
tensorflow/cc/saved_model/loader.cc:337] SavedModel load for tags { serve }; Status: fail: INVALID_ARGUMENT: No OpKernel was registered to support Op 'BoostedTreesBucketize' used by {{node transform/transform/bucketize/apply_buckets/assign_buckets_all_shapes/assign_buckets/BoostedTreesBucketize}} with these attrs: [_output_shapes=[[?]], num_features=1]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels> [[transform/transform/bucketize/apply_buckets/assign_buckets_all_shapes/assign_buckets/BoostedTreesBucketize]]""
```


### Relevant log output

_No response_",zhaoting-wu,2024-09-30 20:16:43+00:00,['tilakrayal'],2024-11-23 02:02:53+00:00,2024-11-23 02:02:51+00:00,https://github.com/tensorflow/tensorflow/issues/76826,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.16', '')]","[{'comment_id': 2398873496, 'issue_id': 2557559950, 'author': 'tilakrayal', 'body': '@zhaoting-wu,\r\nThere are multiple changes/updates that happened from tensorflow v2.7 to latest versions 2.16 and 2.17. Also from tensorflow 2.16 the Keras has been moved from Keras2.0 to Keras3.0. I suggest to convert the code to tensorflow v2.16, 2.17 apis and test the model.\r\n\r\nhttps://keras.io/keras_3/\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 8, 5, 29, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402851039, 'issue_id': 2557559950, 'author': 'zhaoting-wu', 'body': ""Hi @tilakrayal, thanks for the response! \r\n\r\nBased on the log, we noticed the errored model called `bucketize` transform in Tensorflow so that it touched the `BoostedTreesBucketize` op (`[[transform/transform/bucketize/apply_buckets/assign_buckets_all_shapes/assign_buckets/BoostedTreesBucketize]]`), but `bucketize` transform doesn't show as deprecated as `BoostedTreesBucketize`. Does that mean `bucketize` just started to call other substitute ops after deprecating `BoostedTreesBucketize`? \r\n\r\nFrom the [release note](https://github.com/tensorflow/tensorflow/releases/tag/v2.8.0), `BoostedTreesBucketize` appears deprecated since TF 2.8 and removed since TF 2.9: ```\r\nDue to security issues (see section below), all boosted trees code has been deprecated. Users should switch to [TensorFlow Decision Forests](https://github.com/tensorflow/decision-forests). TF's boosted trees code will be eliminated before the branch cut for TF 2.9 and will no longer be present since that release.\r\n```.  We tried Triton image built on TF 2.9 and this error starts to show up. Does that mean this issue can be resolved by simply switching our TF dependency for model training to TF2.9+, but still keeping the call of `bucketize`?"", 'created_at': datetime.datetime(2024, 10, 9, 17, 2, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461556356, 'issue_id': 2557559950, 'author': 'tilakrayal', 'body': '@zhaoting-wu,\r\nAs mentioned in the release document, the BoostedTreesBucketize appears deprecated since TF 2.8 and removed since TF 2.9.  Switching from deprecated `tf.raw_ops.BoostedTreesBucketize` to `tf.searchsorted` in `tft.apply_buckets`. This fixes bug with large int64 values being incorrectly mapped and allows to simplify `tft.apply_buckets` implementation.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 7, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805661, 'issue_id': 2557559950, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212331, 'issue_id': 2557559950, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212351, 'issue_id': 2557559950, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76826"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76826"">No</a>', 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-08 05:29:23 UTC): @zhaoting-wu,
There are multiple changes/updates that happened from tensorflow v2.7 to latest versions 2.16 and 2.17. Also from tensorflow 2.16 the Keras has been moved from Keras2.0 to Keras3.0. I suggest to convert the code to tensorflow v2.16, 2.17 apis and test the model.

https://keras.io/keras_3/

Thank you!

zhaoting-wu (Issue Creator) on (2024-10-09 17:02:11 UTC): Hi @tilakrayal, thanks for the response! 

Based on the log, we noticed the errored model called `bucketize` transform in Tensorflow so that it touched the `BoostedTreesBucketize` op (`[[transform/transform/bucketize/apply_buckets/assign_buckets_all_shapes/assign_buckets/BoostedTreesBucketize]]`), but `bucketize` transform doesn't show as deprecated as `BoostedTreesBucketize`. Does that mean `bucketize` just started to call other substitute ops after deprecating `BoostedTreesBucketize`? 

From the [release note](https://github.com/tensorflow/tensorflow/releases/tag/v2.8.0), `BoostedTreesBucketize` appears deprecated since TF 2.8 and removed since TF 2.9: ```
Due to security issues (see section below), all boosted trees code has been deprecated. Users should switch to [TensorFlow Decision Forests](https://github.com/tensorflow/decision-forests). TF's boosted trees code will be eliminated before the branch cut for TF 2.9 and will no longer be present since that release.
```.  We tried Triton image built on TF 2.9 and this error starts to show up. Does that mean this issue can be resolved by simply switching our TF dependency for model training to TF2.9+, but still keeping the call of `bucketize`?

tilakrayal (Assginee) on (2024-11-07 07:59:00 UTC): @zhaoting-wu,
As mentioned in the release document, the BoostedTreesBucketize appears deprecated since TF 2.8 and removed since TF 2.9.  Switching from deprecated `tf.raw_ops.BoostedTreesBucketize` to `tf.searchsorted` in `tft.apply_buckets`. This fixes bug with large int64 values being incorrectly mapped and allows to simplify `tft.apply_buckets` implementation.

Thank you!

github-actions[bot] on (2024-11-15 02:05:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-23 02:02:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76826"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76826"">No</a>

"
2556242630,issue,open,,Multithreading is not working with teansorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow==2.15.0.post1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

python:3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

when I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code

# Make predictions
outputs = model_obj(inputs)

and also inorder to reduce the docker image size I am using 
RUN pip3 install torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch_stable.html


before installing all the dependencies
Flask==2.2.5
g2p-en==2.1.0
gunicorn==21.2.0
jellyfish==1.0.3
kenlm==0.2.0
nltk==3.8.1
numpy==1.26.3
pandas==2.2.0
python-dotenv==1.0.1
requests==2.31.0
scikit-learn==1.4.0
semantic-router==0.0.17
semantic-router[fastembed]
sentence-transformers==2.3.1
tensorflow==2.15.0.post1
tensorflow-hub==0.16.0
theano==1.0.5
transformers==4.37.2
Werkzeug==2.2.2


please tell me why is my code is getting blocked if I use more than 1 thread.

### Standalone code to reproduce the issue

```shell
def intent_prediction(self, sentence, thresold_score):
        logger.info(f""Threshold Score: {thresold_score}"")
        try:
            model_obj = intent_object_dict[self.model]
        except KeyError:
            logger.error(""Model not found in intent_object_dict"")
            model_obj = self.load_model()

        if INTENT_MODEL == ""cohere"":
            score, intent = self.cohere_intent_prediction(sentence, model_obj)
        else:
            score, intent = self.Bert_intent_prediction(
                sentence, model_obj, thresold_score
            )
        return score, intent

def Bert_intent_prediction(self, sentence, model_obj, thresold_score):
        inputs = self.load_BERT_tokenizer(sentence)
        # Make predictions
        outputs = model_obj(inputs)
        # Get predicted class
        probabilities = tf.nn.softmax(outputs.logits, axis=1)
        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]
        matching_score = probabilities[0][predicted_class].numpy()
        try:
            intent_data = intent_label_dict[self.model]
        except Exception as e:
            logger.error(f""error while getting label: {e}"")
            intent_data = self.get_intent_labels()

        if matching_score >= thresold_score:
            logger.info(
                f""Matched Main Intent:\
    {intent_data[predicted_class]},\
    SCORE :{matching_score}""
            )
            logger.info(f""Matched Sentence: {intent_data[predicted_class]}"")
        else:
            logger.info(f""Intent not matched, score is {matching_score}"")

        intent = intent_data[predicted_class]

        return str(matching_score), intent
```


### Relevant log output

```shell
30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|
    __init__.py:171|Enter into PUNC for intent...
30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|
    phrase_sim.py:72|Threshold Score: 0.7


after this the code is blocked
```
",KunduruJayasimhareddy,2024-09-30 10:37:06+00:00,['Venkat6871'],2025-01-15 10:18:45+00:00,,https://github.com/tensorflow/tensorflow/issues/76794,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2382848982, 'issue_id': 2556242630, 'author': 'KunduruJayasimhareddy', 'body': ""also if I use the latest tensorflow I am getting below error\r\n\r\nterminate called after throwing an instance of 'std::runtime_error' askai-punc-service.1.iydr14592ivd@nladmin-TravelMate-P214-53 | what(): random_device could not be read askai-punc-service.1.iydr14592ivd@nladmin-TravelMate-P214-53 | Aborted \r\n(core dumped)"", 'created_at': datetime.datetime(2024, 9, 30, 10, 52, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385965042, 'issue_id': 2556242630, 'author': 'Venkat6871', 'body': 'Hi **@KunduruJayasimhareddy** ,\r\nI tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/66be48f2388da2636ef0f417d6f45ec5/76794_tf-2-17-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 13, 44, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386066751, 'issue_id': 2556242630, 'author': 'KunduruJayasimhareddy', 'body': 'I am trying to serve using uvicorn and gunicorn and when I use more than 1 workers I am encountering that issue.', 'created_at': datetime.datetime(2024, 10, 1, 14, 4, 12, tzinfo=datetime.timezone.utc)}]","KunduruJayasimhareddy (Issue Creator) on (2024-09-30 10:52:08 UTC): also if I use the latest tensorflow I am getting below error

terminate called after throwing an instance of 'std::runtime_error' askai-punc-service.1.iydr14592ivd@nladmin-TravelMate-P214-53 | what(): random_device could not be read askai-punc-service.1.iydr14592ivd@nladmin-TravelMate-P214-53 | Aborted 
(core dumped)

Venkat6871 (Assginee) on (2024-10-01 13:44:19 UTC): Hi **@KunduruJayasimhareddy** ,
I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/66be48f2388da2636ef0f417d6f45ec5/76794_tf-2-17-0-nightly-v.ipynb) here for your reference.
Thank you!

KunduruJayasimhareddy (Issue Creator) on (2024-10-01 14:04:12 UTC): I am trying to serve using uvicorn and gunicorn and when I use more than 1 workers I am encountering that issue.

"
2555060290,issue,closed,completed,ExponentialMovingAverage doesn't work with KerasVariable,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

### Description
tf.train.ExponentialMovingAverage apply() method takes list of variables. It expects list of tf.Variable and doesn't work with list of KerasVariable because of dtypes
tf.keras.layers.Dense trainable_variables property returns list of `KerasVariable`, KerasVariable dtype is `str`
tf.Variable dtype is `tf.Dtype`

### Expected behavior
I suppose tf.keras layer or model should return list of KerasVariable, so tf.train.ExponentialMovingAverage.apply() should support list of KerasVariables as argument



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

layer = tf.keras.layers.Dense(2)
ema = tf.train.ExponentialMovingAverage(decay=0.999)

x = tf.ones((3, 3))
y = layer(x)

# apply() works with tf.Variable, but doesn't work with KerasVariable
# because of dtype (tf.float32 (tf.Dtype) vs 'float32' (str) respectively)
ema.apply(layer.trainable_variables)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\TestProj\test.py"", line 11, in <module>
    ema.apply(layer.trainable_variables)
  File ""C:\TestProj\venv\Lib\site-packages\tensorflow\python\training\moving_averages.py"", line 542, in apply
    if var.dtype.base_dtype not in [
       ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'base_dtype'
```
",Grizzzlyy,2024-09-29 18:20:52+00:00,['tilakrayal'],2024-10-01 10:36:58+00:00,2024-10-01 10:36:55+00:00,https://github.com/tensorflow/tensorflow/issues/76771,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385046095, 'issue_id': 2555060290, 'author': 'tilakrayal', 'body': '@Grizzzlyy,\r\nTensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install tf-keras i.e., keras2.0 which the code was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bb326b36728f4528e1016d384638090a/untitled2141.ipynb).\r\n\r\n```python\r\n!pip install tf-keras\r\nimport tf_keras as keras\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 7, 52, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385062043, 'issue_id': 2555060290, 'author': 'Grizzzlyy', 'body': '@tilakrayal \nYep, that works. Thank you', 'created_at': datetime.datetime(2024, 10, 1, 8, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385113617, 'issue_id': 2555060290, 'author': 'tilakrayal', 'body': '@Grizzzlyy,\r\nGlad the issue got resolved. Could you please feel free to move this issue to closed status? Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 8, 24, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385431109, 'issue_id': 2555060290, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76771"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76771"">No</a>', 'created_at': datetime.datetime(2024, 10, 1, 10, 36, 57, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 07:52:28 UTC): @Grizzzlyy,
Tensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install tf-keras i.e., keras2.0 which the code was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bb326b36728f4528e1016d384638090a/untitled2141.ipynb).

```python
!pip install tf-keras
import tf_keras as keras
```

Thank you!

Grizzzlyy (Issue Creator) on (2024-10-01 08:00:46 UTC): @tilakrayal 
Yep, that works. Thank you

tilakrayal (Assginee) on (2024-10-01 08:24:02 UTC): @Grizzzlyy,
Glad the issue got resolved. Could you please feel free to move this issue to closed status? Thank you!

google-ml-butler[bot] on (2024-10-01 10:36:57 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76771"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76771"">No</a>

"
2554487967,issue,closed,completed,Problem in mnist.tflite file ,"I had downloaded corect file from https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/codelabs/digit_classifier/ml/step2_train_ml_model.ipynb#scrollTo=Q_Z5yLxrwbpI. I Put this file to final verions (end ) of android application . But a still got this error 
"" E  FATAL EXCEPTION: pool-2-thread-1 (Ask Gemini)
                                                                                                    Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 8694
                                                                                                    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'FULLY_CONNECTED' version '12'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?
                                                                                                    
                                                                                                    Registration failed.
                                                                                                    """,DanielVacha-dv,2024-09-28 20:42:27+00:00,['gaikwadrahul8'],2024-11-20 13:19:34+00:00,2024-10-24 02:01:47+00:00,https://github.com/tensorflow/tensorflow/issues/76743,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2383216472, 'issue_id': 2554487967, 'author': 'gaikwadrahul8', 'body': ""Hi, @DanielVacha-dv\r\n\r\nThank you for bringing this issue to our attention, I believe you're following the exact steps mentioned in this [codelab tutorial ](https://developer.android.com/codelabs/digit-classifier-tflite#0) in that it's using the older version of TensorFlow so could you please try with latest version by adding below line, if you want to use GPU then please add this line `implementation 'org.tensorflow:tensorflow-lite-gpu:+'` or else ignore it\r\n\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:+'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:+'      // Import the GPU delegate plugin Library for GPU inference\r\n}\r\n```\r\n\r\nWe do have [TensorFlow Lite Digit Classification Demo Application](https://github.com/tensorflow/examples/tree/master/lite/examples/digit_classifier/android) if you want to give it try \r\n\r\nPlease let us know after trying with latest version is it resolving your issue or not ? if issue still persists please let us know with error log and steps which you followed before encountering the error message that will help us to investigate your issue further. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 30, 13, 35, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384023108, 'issue_id': 2554487967, 'author': 'DanielVacha-dv', 'body': 'Hello gaikwadrahul8\r\nThank you for your reply. You\'re right, I tried to follow the instructions here https://developer.android.com/codelabs/digit-classifier-tflite?hl=en#0\r\nand also in https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/codelabs/digit_classifier/ml/step2_train_ml_model.ipynb#scrollTo=C4ASalaLIbu2 as accurately as I could.\r\nAfter cloning the project from the Git repository, I immediately ran the variant in the finish directory and added the mnist.tflite file to the desired location. I had problems during the build, so I changed the parameters in the build.gradle file.\r\n The next day after sending the email, I couldn\'t be satisfied with the failure and that\'s why I started, at least superficially, into the issue of creating models and asked Gemini as well.\r\nGemini advised me to edit the part with the model settings\r\n""converter.target_spec.supported_ops = [\r\n tf.lite.OpsSet.TFLITE_BUILTINS, # enable LiteRT ops.\r\n tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]"" but it still didn\'t work. Finally, out of desperation, I tried not to save the model with tflite_quantized_model , f.write(tflite_quantized_model) but with this tflite_float_model. Only then I was able to run the application without crashing.\r\n Following your advice, I cloned the https://github.com/tensorflow/examples.git repository again today to a new location and tried the recommended steps, but that also resulted in a translation error. I don\'t know if it\'s helpful, but I\'m attaching the build.gradle file that allowed me to run the application and also the model (mnist.tflite) that contains tflite_float_model.\r\nThank you for your cooperation\r\nRegards, Daniel\r\n[build.gradle.txt](https://github.com/user-attachments/files/17195834/build.gradle.txt)\r\n[mnist.tflite.txt](https://github.com/user-attachments/files/17195839/mnist.tflite.txt)', 'created_at': datetime.datetime(2024, 9, 30, 19, 47, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399731053, 'issue_id': 2554487967, 'author': 'gaikwadrahul8', 'body': ""Hi, @DanielVacha-dv \r\n\r\nThank you for trying things and detailed analysis about issues so I'll try from my end and see is it working as expected or not and will update you.\r\n\r\n**EDIT :** I tried with gradle version` 7.6.2 `instead of `8.4` so I changed this line `distributionUrl=https\\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle version `7.6.2` and our relevant team will fix this issue with gradle version `8.4` soon\r\n\r\n**Here is output screenshot for reference :**\r\n\r\n![image](https://github.com/user-attachments/assets/5d82a7b4-4c6c-4543-8987-f5831c66163f)\r\n\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 8, 12, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573271, 'issue_id': 2554487967, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073873, 'issue_id': 2554487967, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073966, 'issue_id': 2554487967, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76743"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76743"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 50, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-30 13:35:35 UTC): Hi, @DanielVacha-dv

Thank you for bringing this issue to our attention, I believe you're following the exact steps mentioned in this [codelab tutorial ](https://developer.android.com/codelabs/digit-classifier-tflite#0) in that it's using the older version of TensorFlow so could you please try with latest version by adding below line, if you want to use GPU then please add this line `implementation 'org.tensorflow:tensorflow-lite-gpu:+'` or else ignore it

```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:+'
    implementation 'org.tensorflow:tensorflow-lite-gpu:+'      // Import the GPU delegate plugin Library for GPU inference
}
```

We do have [TensorFlow Lite Digit Classification Demo Application](https://github.com/tensorflow/examples/tree/master/lite/examples/digit_classifier/android) if you want to give it try 

Please let us know after trying with latest version is it resolving your issue or not ? if issue still persists please let us know with error log and steps which you followed before encountering the error message that will help us to investigate your issue further. 

Thank you for your cooperation and patience.

DanielVacha-dv (Issue Creator) on (2024-09-30 19:47:42 UTC): Hello gaikwadrahul8
Thank you for your reply. You're right, I tried to follow the instructions here https://developer.android.com/codelabs/digit-classifier-tflite?hl=en#0
and also in https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/codelabs/digit_classifier/ml/step2_train_ml_model.ipynb#scrollTo=C4ASalaLIbu2 as accurately as I could.
After cloning the project from the Git repository, I immediately ran the variant in the finish directory and added the mnist.tflite file to the desired location. I had problems during the build, so I changed the parameters in the build.gradle file.
 The next day after sending the email, I couldn't be satisfied with the failure and that's why I started, at least superficially, into the issue of creating models and asked Gemini as well.
Gemini advised me to edit the part with the model settings
""converter.target_spec.supported_ops = [
 tf.lite.OpsSet.TFLITE_BUILTINS, # enable LiteRT ops.
 tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]"" but it still didn't work. Finally, out of desperation, I tried not to save the model with tflite_quantized_model , f.write(tflite_quantized_model) but with this tflite_float_model. Only then I was able to run the application without crashing.
 Following your advice, I cloned the https://github.com/tensorflow/examples.git repository again today to a new location and tried the recommended steps, but that also resulted in a translation error. I don't know if it's helpful, but I'm attaching the build.gradle file that allowed me to run the application and also the model (mnist.tflite) that contains tflite_float_model.
Thank you for your cooperation
Regards, Daniel
[build.gradle.txt](https://github.com/user-attachments/files/17195834/build.gradle.txt)
[mnist.tflite.txt](https://github.com/user-attachments/files/17195839/mnist.tflite.txt)

gaikwadrahul8 (Assginee) on (2024-10-08 12:37:55 UTC): Hi, @DanielVacha-dv 

Thank you for trying things and detailed analysis about issues so I'll try from my end and see is it working as expected or not and will update you.

**EDIT :** I tried with gradle version` 7.6.2 `instead of `8.4` so I changed this line `distributionUrl=https\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle version `7.6.2` and our relevant team will fix this issue with gradle version `8.4` soon

**Here is output screenshot for reference :**

![image](https://github.com/user-attachments/assets/5d82a7b4-4c6c-4543-8987-f5831c66163f)


Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-16 02:02:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:47 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:01:50 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76743"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76743"">No</a>

"
2554411747,issue,closed,completed,Leakage of personal info to BCI,1,hogomister,2024-09-28 17:11:05+00:00,['tilakrayal'],2024-09-28 17:13:33+00:00,2024-09-28 17:12:39+00:00,https://github.com/tensorflow/tensorflow/issues/76738,[],[],
2554247086,issue,closed,completed,Segmentation fault (core dumped) in `tf.raw_ops.FakeParam`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

With invalid input, tf.raw_ops.FakeParam triggers a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
fake_param = tf.raw_ops.FakeParam(dtype='string', shape=())
```


### Relevant log output

```shell
2024-09-28 22:02:02.412815: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 22:02:02.424900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 22:02:02.439240: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 22:02:02.443601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 22:02:02.454230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 22:02:03.122361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-09-28 22:02:03.765934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6
2024-09-28 22:02:03.766553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22456 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 14:06:51+00:00,['Venkat6871'],2024-10-16 02:02:36+00:00,2024-10-16 02:02:34+00:00,https://github.com/tensorflow/tensorflow/issues/76734,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384901788, 'issue_id': 2554247086, 'author': 'Venkat6871', 'body': 'Hi **@x0w3n** ,\r\nI tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I am not facing any issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/cc82aa67648260a385f607db5dec26e8/76734_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 26, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123014, 'issue_id': 2554247086, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 0, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573326, 'issue_id': 2554247086, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573369, 'issue_id': 2554247086, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76734"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76734"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 36, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-01 06:26:04 UTC): Hi **@x0w3n** ,
I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I am not facing any issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/cc82aa67648260a385f607db5dec26e8/76734_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

github-actions[bot] on (2024-10-09 02:00:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76734"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76734"">No</a>

"
2554221365,issue,closed,completed,Aborted (core dumped) in `tf.data.Dataset.from_generator`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.data.Dataset.from_generator triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import random

def generate_invalid_dataset():
    data = []
    for _ in range(10):
        size = random.randint(1, 5)
        data.append(np.random.randint(10, size=(size,)))
    return data

def test():
    dataset = generate_invalid_dataset()
    dataset = tf.data.Dataset.from_generator(lambda: iter(dataset), output_types=tf.int32)
    dataset = dataset.unbatch()
    dataset = dataset.apply(tf.data.experimental.ignore_errors())
    iterator = iter(dataset)

    try:
        for _ in range(10):
            next_element = next(iterator)
    except Exception as e:
        print(f""Caught an error: {e}"")

test()
```


### Relevant log output

```shell
2024-09-28 21:22:49.792052: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:22:49.854705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:22:49.933196: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:22:49.957165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:22:50.015970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From *.py:17: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.
Instructions for updating:
Use output_signature instead
2024-09-28 21:22:57.817999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2158 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:22:57.820421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
WARNING:tensorflow:From *.py:19: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.ignore_errors` instead.
2024-09-28 21:23:23.222693: F external/local_tsl/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
Aborted (core dumped)
```
",x0w3n,2024-09-28 13:25:51+00:00,['tilakrayal'],2024-12-04 14:56:06+00:00,2024-10-15 02:02:29+00:00,https://github.com/tensorflow/tensorflow/issues/76732,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382433299, 'issue_id': 2554221365, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/60149) where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 9, 30, 8, 22, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491112, 'issue_id': 2554221365, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662044, 'issue_id': 2554221365, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662097, 'issue_id': 2554221365, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76732"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76732"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 31, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-30 08:22:57 UTC): @x0w3n,
I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/60149) where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

github-actions[bot] on (2024-10-08 02:01:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:29 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76732"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76732"">No</a>

"
2554218867,issue,open,,Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data_source_name = ""sqlite:///path/to/correct_database.db""

query = ""SELECT id, name FROM my_table""
output_types = (tf.int64, tf.string)
dataset = tf.data.experimental.SqlDataset(
    'sqlite', data_source_name, query, output_types)

for element in dataset:
    print(element)
```


### Relevant log output

```shell
2024-09-28 21:18:33.844482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:18:33.907260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:18:33.986019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:18:34.009755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:18:34.068897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:18:38.768599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:18:38.769172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:18:39.132534: W tensorflow/core/kernels/data/experimental/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:///path/to/correct_database.db) failed: unable to open database file
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 13:20:59+00:00,['Venkat6871'],2024-10-17 09:02:57+00:00,,https://github.com/tensorflow/tensorflow/issues/76731,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384884446, 'issue_id': 2554218867, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/38b42902e2dcb4e77bc21af45606439d/76731_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 12, 48, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-01 06:12:48 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/38b42902e2dcb4e77bc21af45606439d/76731_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2554215305,issue,open,,Aborted (core dumped) in `tf.linalg.det/slogdet/logdet/cholesky/inv`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.linalg.det/slogdet/logdet/cholesky/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_input = tf.zeros([])
tf.linalg.det(invalid_input)    # crash
tf.linalg.slogdet(invalid_input)  # crash
tf.linalg.cholesky(invalid_input)  # crash
tf.linalg.logdet(invalid_input)  # crash
tf.linalg.inv(invalid_input)  # crash
```


### Relevant log output

```shell
2024-09-28 21:11:10.188752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:11:10.199880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:11:10.213635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:11:10.221654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:11:10.279720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:11:17.015480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:11:17.015957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:11:17.154391: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-09-28 13:15:48+00:00,['tilakrayal'],2024-10-09 04:10:21+00:00,,https://github.com/tensorflow/tensorflow/issues/76730,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385522878, 'issue_id': 2554215305, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI tried to execute the mentioned code on both [CPU](https://colab.research.google.com/gist/tilakrayal/620980be07924edd25a0dc5a5e4b51cd/untitled2146.ipynb) and GPU, in CPU the code was executed with the error output and on GPU the same code was aborted. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/64cdf1175bd3a7c663392343f525cb99/untitled2147.ipynb)\r\n\r\n\r\n![Screenshot 2024-10-01 4 54 44 PM](https://github.com/user-attachments/assets/296a0225-8144-46ee-87f7-fc52bd783655)', 'created_at': datetime.datetime(2024, 10, 1, 11, 25, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 11:25:51 UTC): @x0w3n,
I tried to execute the mentioned code on both [CPU](https://colab.research.google.com/gist/tilakrayal/620980be07924edd25a0dc5a5e4b51cd/untitled2146.ipynb) and GPU, in CPU the code was executed with the error output and on GPU the same code was aborted. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/64cdf1175bd3a7c663392343f525cb99/untitled2147.ipynb)


![Screenshot 2024-10-01 4 54 44 PM](https://github.com/user-attachments/assets/296a0225-8144-46ee-87f7-fc52bd783655)

"
2554212734,issue,open,,Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:
tf.raw_ops.ResourceScatterNdUpdate
tf.raw_ops.ResourceScatterNdAdd
tf.raw_ops.ResourceScatterNdSub
tf.raw_ops.ResourceScatterNdMax
tf.raw_ops.ResourceScatterNdMin

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

resource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)
resource_handle = resource_var.handle

indices = np.array([[2, 1], [1, 2]], dtype=np.int32)
updates = np.array([10, 20], dtype=np.float32)
tf.raw_ops.ResourceScatterNdUpdate(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)

tf.raw_ops.ResourceScatterNdAdd(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdSub(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMax(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMin(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
```


### Relevant log output

```shell
2024-09-28 21:06:23.445185: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:06:23.508056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:06:23.583640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:06:23.607538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:06:23.664877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:06:31.527466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:06:31.527985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:06:31.782114: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
",x0w3n,2024-09-28 13:09:06+00:00,['Venkat6871'],2024-10-17 09:00:28+00:00,,https://github.com/tensorflow/tensorflow/issues/76729,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384879157, 'issue_id': 2554212734, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/b72a086e7d257d3c4dd1f748e47c5b96/76729_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 8, 32, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-01 06:08:32 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/b72a086e7d257d3c4dd1f748e47c5b96/76729_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2554210767,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.BiasAdd/tf.nn.bias_add`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs And the gpu is available, tf.raw_ops.BiasAdd/tf.nn.bias_add triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.zeros([1, 2], dtype=tf.float32)
data_format = 'NCHW'
bias = tf.ones([2], dtype=tf.float32)
bias_add = tf.raw_ops.BiasAdd(value=input_tensor, bias=bias, data_format=data_format)
```


### Relevant log output

```shell
2024-09-28 21:01:39.889919: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:01:39.902000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:01:39.916516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:01:39.920867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:01:39.931797: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:01:40.599888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-09-28 21:01:41.259588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6
2024-09-28 21:01:41.260235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22456 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6
2024-09-28 21:01:41.612863: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 2)
Aborted
```
",x0w3n,2024-09-28 13:03:32+00:00,['tilakrayal'],2024-10-16 02:02:40+00:00,2024-10-16 02:02:35+00:00,https://github.com/tensorflow/tensorflow/issues/76728,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384745577, 'issue_id': 2554210767, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62146 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 4, 10, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123048, 'issue_id': 2554210767, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 0, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573360, 'issue_id': 2554210767, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573436, 'issue_id': 2554210767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76728"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76728"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 39, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 04:10:19 UTC): @x0w3n,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62146 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

github-actions[bot] on (2024-10-09 02:00:59 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76728"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76728"">No</a>

"
2554203474,issue,open,,Aborted (core dumped) in `tf.io.encode_png`/`tf.compat.v1.image.encode_png`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The crash was triggered when an illegal image was passed to tf.io.encode_png/tf.compat.v1.image.encode_png

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

image = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)

encoded_image = tf.compat.v1.image.encode_png(image) # crash
tf.io.encode_png(image, compression=-1, name=None) #crash
```


### Relevant log output

```shell
2024-09-28 20:48:36.270008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:48:36.332972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:48:36.411391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:48:36.428306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:48:36.438336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:48:41.475588: F tensorflow/core/lib/png/png_io.cc:350] 'image' Must be non NULL
Aborted (core dumped)
```
",x0w3n,2024-09-28 12:49:42+00:00,['Venkat6871'],2024-11-06 06:31:18+00:00,,https://github.com/tensorflow/tensorflow/issues/76726,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384833664, 'issue_id': 2554203474, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/4c720faf81813fb0f8f5c00b41e40359/76726_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 5, 30, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456604537, 'issue_id': 2554203474, 'author': 'yugborana', 'body': '@Venkat6871 \n\nI would like to work and know more on this issue....\n\nI think this core dump is occuring due to incorrect image dimensions/ memory allocation failures/Incorrect data types or Batch Processing errors.  \n\nSo, adding a new function to image_ops_impl.py, handling all these error cases maybe could solve this issue.\n\nWould like to know get more inputs on this issue?', 'created_at': datetime.datetime(2024, 11, 5, 9, 2, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458819943, 'issue_id': 2554203474, 'author': 'Venkat6871', 'body': 'Hi **@yugborana** ,\r\nCould you please raise a PR for the respective issue? This way, you will be able to contribute to TensorFlow.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 6, 31, 16, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-01 05:30:18 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/4c720faf81813fb0f8f5c00b41e40359/76726_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

yugborana on (2024-11-05 09:02:56 UTC): @Venkat6871 

I would like to work and know more on this issue....

I think this core dump is occuring due to incorrect image dimensions/ memory allocation failures/Incorrect data types or Batch Processing errors.  

So, adding a new function to image_ops_impl.py, handling all these error cases maybe could solve this issue.

Would like to know get more inputs on this issue?

Venkat6871 (Assginee) on (2024-11-06 06:31:16 UTC): Hi **@yugborana** ,
Could you please raise a PR for the respective issue? This way, you will be able to contribute to TensorFlow.
Thank you!

"
2554202027,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.Einsum`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.raw_ops.Einsum triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define a 3D tensor
g = tf.random.normal([10, 20, 30])
w = tf.random.normal([20, 30])

res = tf.raw_ops.Einsum(equation='ijk,jk->ijl', inputs=[g, w])
print(res.shape)
```


### Relevant log output

```shell
2024-09-28 20:42:44.280244: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:42:44.343703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:42:44.420579: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:42:44.441517: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:42:44.488538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:42:51.666942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:42:51.667588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:42:52.086477: F ./tensorflow/core/kernels/transpose_functor.h:171] Check failed: in.dims() == perm.size() (2 vs. 3)
Aborted (core dumped)
```
",x0w3n,2024-09-28 12:45:38+00:00,['tilakrayal'],2024-10-15 02:02:35+00:00,2024-10-15 02:02:31+00:00,https://github.com/tensorflow/tensorflow/issues/76725,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2383746745, 'issue_id': 2554202027, 'author': 'tilakrayal', 'body': ""@x0w3n,\r\nCould you please confirm whether the crash is happening with the 2-dimension input as well. I tried and the code was executed without any issues.\r\n\r\n```python\r\ng = tf.random.normal([10, 20])\r\nw = tf.random.normal([20, 30])\r\n\r\nres = tf.raw_ops.Einsum(equation='ij,jk->il', inputs=[g, w])\r\nprint(res.shape)\r\n```\r\n\r\nAlso this Op is not intended to be called by the user; instead users should call **tf.einsum** directly. It is a hidden Op used by [tf.einsum](https://www.tensorflow.org/api_docs/python/tf/einsum).\r\n\r\nAlso Operations are applied to the input(s) according to the rules which are documented here.\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/Einsum  \r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 9, 30, 17, 12, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491192, 'issue_id': 2554202027, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 1, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662091, 'issue_id': 2554202027, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662162, 'issue_id': 2554202027, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76725"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76725"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 34, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-30 17:12:59 UTC): @x0w3n,
Could you please confirm whether the crash is happening with the 2-dimension input as well. I tried and the code was executed without any issues.

```python
g = tf.random.normal([10, 20])
w = tf.random.normal([20, 30])

res = tf.raw_ops.Einsum(equation='ij,jk->il', inputs=[g, w])
print(res.shape)
```

Also this Op is not intended to be called by the user; instead users should call **tf.einsum** directly. It is a hidden Op used by [tf.einsum](https://www.tensorflow.org/api_docs/python/tf/einsum).

Also Operations are applied to the input(s) according to the rules which are documented here.
https://www.tensorflow.org/api_docs/python/tf/raw_ops/Einsum  

Thank you!

github-actions[bot] on (2024-10-08 02:01:55 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:31 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76725"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76725"">No</a>

"
2554200616,issue,open,,Floating point exception (core dumped) in `tf.nn.depth_to_space`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.depth_to_space triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
try:
    # Create an empty tensor
    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)
    # arg_0 = tf.identity(arg_0_tensor)
    arg_1 = 536870912
    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)
except Exception as e:
    print(""Error:"", str(e))
```


### Relevant log output

```shell
2024-09-28 20:41:05.888017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:41:05.950498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:41:06.028236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:41:06.052072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:41:06.111011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:41:11.970896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:41:11.973176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Floating point exception (core dumped)
```
",x0w3n,2024-09-28 12:41:49+00:00,['Venkat6871'],2024-10-17 08:56:59+00:00,,https://github.com/tensorflow/tensorflow/issues/76724,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382959766, 'issue_id': 2554200616, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b1094a7f631bdf0ccbf54711facd3394/767242_tf-2-17-0-nightly-v.ipynb) here for reference.\r\n<img width=""1728"" alt=""Screenshot 2024-09-30 at 5 13 39\u202fPM"" src=""https://github.com/user-attachments/assets/f9f18ba7-fb4a-40d4-a49d-bdd927594329"">\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 11, 45, 5, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-30 11:45:05 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b1094a7f631bdf0ccbf54711facd3394/767242_tf-2-17-0-nightly-v.ipynb) here for reference.
<img width=""1728"" alt=""Screenshot 2024-09-30 at 5 13 39 PM"" src=""https://github.com/user-attachments/assets/f9f18ba7-fb4a-40d4-a49d-bdd927594329"">

Thank you!

"
2554197998,issue,closed,completed,Segmentation fault (core dumped) in `tf.python.framework.importer`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.python.framework.importer triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import importer

try:
    arg_0_0 = ""Placeholder:0""
    arg_1_1 = ""add:0""
    arg_0 = [arg_0_0, arg_1_1]
    arg_1 = None
    arg_2 = None
    out = importer._GatherReturnElements(arg_0, arg_1, arg_2)
except Exception as e:
    print(""Error:"", str(e))
```


### Relevant log output

```shell
2024-09-28 20:31:16.069665: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:31:16.132409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:31:16.209869: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:31:16.233126: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:31:16.292093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 12:34:35+00:00,['tilakrayal'],2024-10-16 02:02:42+00:00,2024-10-16 02:02:37+00:00,https://github.com/tensorflow/tensorflow/issues/76723,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2384947106, 'issue_id': 2554197998, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/61570 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 56, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123085, 'issue_id': 2554197998, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573392, 'issue_id': 2554197998, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573490, 'issue_id': 2554197998, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76723"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76723"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 41, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 06:56:30 UTC): @x0w3n,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/61570 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

github-actions[bot] on (2024-10-09 02:01:00 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:37 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76723"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76723"">No</a>

"
2554195605,issue,open,,Aborted (core dumped) in `tf.nn.max_pool/tf.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.max_pool triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_kernel_size = -1
invalid_operation = tf.nn.max_pool(
    tf.random.normal([1, 32, 32, 3]),
    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],
    strides=[1, 2, 2, 1],
    padding='SAME'
)
```

```
import tensorflow as tf
import sys

ksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize
input_tensor = tf.random.normal(shape=(2, 10, 4))
result = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')

```


### Relevant log output

```shell
2024-09-28 20:26:47.491907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:26:47.554171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:26:47.606570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:26:47.610539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:26:47.639739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:26:54.579839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:26:54.582099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:26:55.563477: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
F0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) 
*** Check failure stack trace: ***
Aborted (core dumped)
```
",x0w3n,2024-09-28 12:28:21+00:00,['Venkat6871'],2024-10-01 07:06:20+00:00,,https://github.com/tensorflow/tensorflow/issues/76722,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382933098, 'issue_id': 2554195605, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version with both CPU and GPU, With the CPU, it is not crashing, but with the GPU, I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/71721ea6f45fc91d33ccb7d24a5dc7dd/76722_tf-2-17-0-nightly-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/7821ae3e461243ab0e5800ff98844de9/76722_tf-2-17-0-gpu-v.ipynb) here for reference.\r\n<img width=""1728"" alt=""Screenshot 2024-09-30 at 4 39 05\u202fPM"" src=""https://github.com/user-attachments/assets/0b7f3332-86f2-49ec-b69d-b758ef5f4e71"">\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 11, 31, 52, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-30 11:31:52 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version with both CPU and GPU, With the CPU, it is not crashing, but with the GPU, I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/71721ea6f45fc91d33ccb7d24a5dc7dd/76722_tf-2-17-0-nightly-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/7821ae3e461243ab0e5800ff98844de9/76722_tf-2-17-0-gpu-v.ipynb) here for reference.
<img width=""1728"" alt=""Screenshot 2024-09-30 at 4 39 05 PM"" src=""https://github.com/user-attachments/assets/0b7f3332-86f2-49ec-b69d-b758ef5f4e71"">

Thank you!

"
2554194048,issue,closed,completed,Segmentation fault (core dumped) in `tf.data.experimental.service.WorkerServer`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.data.experimental.service.WorkerServer triggered a crash, when the start parameter is false.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
config = tf.data.experimental.service.WorkerConfig(
    dispatcher_address='invalid_address',
    worker_address=None,
    port=0,
    protocol=None,
    heartbeat_interval_ms=None,
    dispatcher_timeout_ms=None,
    data_transfer_protocol=None,
    data_transfer_address=None
)
def test():
    tf.data.experimental.service.WorkerServer(config,start=False)
test()
```


### Relevant log output

```shell
2024-09-28 20:23:15.476342: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:23:15.538580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:23:15.617490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:23:15.641378: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:23:15.701061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 12:23:55+00:00,['tilakrayal'],2024-11-07 13:18:11+00:00,2024-10-24 02:01:48+00:00,https://github.com/tensorflow/tensorflow/issues/76721,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2401279514, 'issue_id': 2554194048, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nCould you please confirm whether the same segmentation fault is happening when we use **tf.data.experimental.service.WorkerServer(config,start=True)**. I tried in another environment where with the True it was executed without the crash. \r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/service/WorkerServer\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 4, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418337391, 'issue_id': 2554194048, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 17, 2, 1, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073904, 'issue_id': 2554194048, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434074047, 'issue_id': 2554194048, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76721"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76721"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462220210, 'issue_id': 2554194048, 'author': 'rivershah', 'body': 'Please reopen. Problem persists\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ndispatcher = tf.data.experimental.service.DispatchServer()\r\ndispatcher_address = dispatcher.target.split(""://"")[1]\r\n\r\nworker = tf.data.experimental.service.WorkerServer(\r\n    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address)\r\n)\r\n\r\ndataset = tf.data.Dataset.range(10)\r\nprint(f""dataset: {dataset} {dataset.element_spec}"")\r\ndataset_id = tf.data.experimental.service.register_dataset(\r\n    dispatcher_address,\r\n    dataset,\r\n    compression=None,\r\n)\r\nprint(f""dataset_id: {dataset_id}"")\r\n\r\ndataset = tf.data.experimental.service.from_dataset_id(\r\n    processing_mode=""parallel_epochs"",\r\n    service=dispatcher.target,\r\n    dataset_id=dataset_id,\r\n    element_spec=dataset.element_spec,\r\n)\r\n\r\nprint(list(dataset.as_numpy_iterator()))\r\n\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730985417.669184  115264 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730985417.675189  115264 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\ntensorflow: 2.18.0\r\nI0000 00:00:1730985420.891233  115264 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\r\ndataset: <_RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> TensorSpec(shape=(), dtype=tf.int64, name=None)\r\ndataset_id: b\'1000\'\r\nSegmentation fault (core dumped)\r\n```', 'created_at': datetime.datetime(2024, 11, 7, 13, 18, 9, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-09 04:16:00 UTC): @x0w3n,
Could you please confirm whether the same segmentation fault is happening when we use **tf.data.experimental.service.WorkerServer(config,start=True)**. I tried in another environment where with the True it was executed without the crash. 
https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/WorkerServer

Thank you!

github-actions[bot] on (2024-10-17 02:01:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:01:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76721"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76721"">No</a>

rivershah on (2024-11-07 13:18:09 UTC): Please reopen. Problem persists

```
import tensorflow as tf

print(tf.__version__)

dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]

worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address)
)

dataset = tf.data.Dataset.range(10)
print(f""dataset: {dataset} {dataset.element_spec}"")
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher_address,
    dataset,
    compression=None,
)
print(f""dataset_id: {dataset_id}"")

dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec,
)

print(list(dataset.as_numpy_iterator()))

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730985417.669184  115264 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730985417.675189  115264 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
tensorflow: 2.18.0
I0000 00:00:1730985420.891233  115264 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9
dataset: <_RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> TensorSpec(shape=(), dtype=tf.int64, name=None)
dataset_id: b'1000'
Segmentation fault (core dumped)
```

"
2554193143,issue,closed,completed,Segmentation fault (core dumped) in `tf.data.experimental.service.DispatchServer`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.data.experimental.service.DispatchServer triggered a crash, when the start parameter is false.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def test():
    tf.data.experimental.service.DispatchServer(start=False)
test()
```


### Relevant log output

```shell
2024-09-28 20:20:16.983171: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:20:17.013139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:20:17.063675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:20:17.087886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:20:17.147364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 12:21:27+00:00,['Venkat6871'],2024-11-07 13:25:51+00:00,2024-10-15 02:02:33+00:00,https://github.com/tensorflow/tensorflow/issues/76720,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382863113, 'issue_id': 2554193143, 'author': 'Venkat6871', 'body': 'Hi **@x0w3n** ,\r\nI tried running your code on Colab using TensorFlow v2.17.0 & the nightly version and faced the same issue. I also tried an alternative approach. If we start the server manually, then it works fine. Here, I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/07d0c607c5221bc63813d6086604f37b/76720_tf-2-17-0-nightly-v.ipynb) for your reference. I hope it will be useful for you.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 10, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491264, 'issue_id': 2554193143, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 1, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662133, 'issue_id': 2554193143, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662233, 'issue_id': 2554193143, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76720"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76720"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462225757, 'issue_id': 2554193143, 'author': 'rivershah', 'body': 'Tensorflow team how are you letting these segfaults through and not responding with proper fixes. Fix the problem. Fix your unit tests', 'created_at': datetime.datetime(2024, 11, 7, 13, 20, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-30 10:58:40 UTC): Hi **@x0w3n** ,
I tried running your code on Colab using TensorFlow v2.17.0 & the nightly version and faced the same issue. I also tried an alternative approach. If we start the server manually, then it works fine. Here, I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/07d0c607c5221bc63813d6086604f37b/76720_tf-2-17-0-nightly-v.ipynb) for your reference. I hope it will be useful for you.
Thank you!

github-actions[bot] on (2024-10-08 02:01:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76720"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76720"">No</a>

rivershah on (2024-11-07 13:20:46 UTC): Tensorflow team how are you letting these segfaults through and not responding with proper fixes. Fix the problem. Fix your unit tests

"
2554191777,issue,closed,completed,Aborted (core dumped) in `tf.math.bincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The crash was triggered when tf.math.bincount processed a large input.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

large_start_number = 2**31
large_end_number = large_start_number + 1000
large_numbers = tf.range(start=large_start_number, limit=large_end_number)
result = tf.math.bincount(large_numbers.numpy())
print(result)
```


### Relevant log output

```shell
2024-09-28 20:15:38.161313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:15:38.225002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:15:38.303991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:15:38.328271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:15:38.388883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:15:42.209550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:15:42.210099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:15:43.290480: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes)
Status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -8589929985
Aborted (core dumped)
```
",x0w3n,2024-09-28 12:17:43+00:00,['tilakrayal'],2024-10-16 02:02:46+00:00,2024-10-16 02:02:39+00:00,https://github.com/tensorflow/tensorflow/issues/76719,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385580544, 'issue_id': 2554191777, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI tried to execute the mentioned code on tensorflow v2.17, tf-nightly and observed that it was crashed due to memory allocation when trying the largest number ""Allocation of 8589938592 exceeds 10% of free system memory.""\r\nWhen I tried with the normal numerics it was working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/030446ffb118735ca3e746b12474c066/untitled2149.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 11, 55, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123114, 'issue_id': 2554191777, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573424, 'issue_id': 2554191777, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573558, 'issue_id': 2554191777, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76719"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76719"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 45, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 11:55:41 UTC): @x0w3n,
I tried to execute the mentioned code on tensorflow v2.17, tf-nightly and observed that it was crashed due to memory allocation when trying the largest number ""Allocation of 8589938592 exceeds 10% of free system memory.""
When I tried with the normal numerics it was working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/030446ffb118735ca3e746b12474c066/untitled2149.ipynb).

Thank you!

github-actions[bot] on (2024-10-09 02:01:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76719"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76719"">No</a>

"
2554189172,issue,open,,Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.profiler.experimental.Profile triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

profiler_options = tf.profiler.experimental.ProfilerOptions(
    host_tracer_level=999,
    python_tracer_level=-1,
    device_tracer_level=10,
    delay_ms=None
)

with tf.profiler.experimental.Profile(None, options=profiler_options):
    a = tf.constant(1)
    b = tf.constant(2)
    c = a + b
    print(c.numpy())
```


### Relevant log output

```shell
2024-09-28 20:07:36.902909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:07:36.998027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:07:37.002984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",x0w3n,2024-09-28 12:11:00+00:00,['Venkat6871'],2025-01-16 14:57:29+00:00,,https://github.com/tensorflow/tensorflow/issues/76718,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382035057, 'issue_id': 2554189172, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version with both CPU and GPU, and I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/be9632319a32d8622a8f61fc892d2393/76718_tf-2-17-0-v-cpu.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/48ac1b537c9e189de2a1bc0942047d8d/76718_tf-2-17-0-v-gpu.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 4, 48, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595950277, 'issue_id': 2554189172, 'author': 'Zettelkasten', 'body': ""I'm running into the same issue. Is there any update or a workaround?"", 'created_at': datetime.datetime(2025, 1, 16, 14, 57, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-30 04:48:52 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version with both CPU and GPU, and I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/be9632319a32d8622a8f61fc892d2393/76718_tf-2-17-0-v-cpu.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/48ac1b537c9e189de2a1bc0942047d8d/76718_tf-2-17-0-v-gpu.ipynb) here for reference.
Thank you!

Zettelkasten on (2025-01-16 14:57:28 UTC): I'm running into the same issue. Is there any update or a workaround?

"
2554186006,issue,closed,completed,A crash is triggered when unknown flags are set.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A crash is triggered when unknown flags are set.

### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
os.environ[""TF_XLA_FLAGS""] = ""--tf_xla_force_host_platform_device_count=8""
tensor = tf.random.normal([100, 100])
```


### Relevant log output

```shell
2024-09-28 12:01:04.057803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 12:01:04.058256: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-28 12:01:04.060890: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-28 12:01:04.066512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1727524864.075697     254 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1727524864.078793     254 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 12:01:04.088010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/root/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}""
2024-09-28 12:01:05.220749: F external/local_xla/xla/parse_flags_from_env.cc:224] Unknown flags in TF_XLA_FLAGS: --tf_xla_force_host_platform_device_count=8
Aborted (core dumped)
```
",x0w3n,2024-09-28 12:03:04+00:00,['tilakrayal'],2024-10-16 02:02:48+00:00,2024-10-16 02:02:40+00:00,https://github.com/tensorflow/tensorflow/issues/76717,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:xla', 'XLA'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385603545, 'issue_id': 2554186006, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nAs mentioned when the unknown flags are set for **TF_XLA_FLAGS**  it was as providing the error **""Unknown flags in TF_XLA_FLAGS: --tf_xla_force_host_platform_device_count=8""** which was expected. Could you please provide the opinions on the same to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 12, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123140, 'issue_id': 2554186006, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573460, 'issue_id': 2554186006, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573615, 'issue_id': 2554186006, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76717"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76717"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 12:07:17 UTC): @x0w3n,
As mentioned when the unknown flags are set for **TF_XLA_FLAGS**  it was as providing the error **""Unknown flags in TF_XLA_FLAGS: --tf_xla_force_host_platform_device_count=8""** which was expected. Could you please provide the opinions on the same to debug the issue. Thank you!

github-actions[bot] on (2024-10-09 02:01:03 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:40 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76717"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76717"">No</a>

"
2553624121,issue,open,,Crash when calling TFSMLayer object during TF_lite conversion,"### 1. System information

- Linux Ubuntu 24.04
- TF 2.17.0 (pip) and Keras 3.5.0

### 2. Code

A TF model is first exported using Keras 3.5.0:

```model.export(model_name)```

Then when the following method is called below, the conversion crashes in TF 2.17.0 (log below). It works fine with TF 2.16.2.

```
def makeQuantizedTFmodel(A, dP):
    import tensorflow as tf    
    A2 = tf.cast(A, tf.float32)
    A = tf.data.Dataset.from_tensor_slices((A2)).batch(1)
    
    def representative_dataset_gen():
        for input_value in A.take(100):
            yield[input_value]
  
            import keras
            model = keras.layers.TFSMLayer(model_name), call_endpoint='serve')    
            converter = tf.lite.TFLiteConverter.from_keras_model(model)    

            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.representative_dataset = representative_dataset_gen
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            tflite_quant_model = converter.convert()

            with open(model_name+'.tflite', 'wb') as o:
                   o.write(tflite_quant_model)
```

The error with TF 2.17.0:

```
Traceback (most recent call last):
  File ""/home/nicola/test/DML/DataML.py"", line 1008, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/nicola/test/DML/DataML.py"", line 160, in main
    train(sys.argv[2], None, None)
  File ""/home/nicola/test/DML/DataML.py"", line 398, in train
    makeQuantizedTFmodel(A, dP)
  File ""/home/nicola/test/DML/libDataML.py"", line 231, in makeQuantizedTFmodel
    tflite_quant_model = converter.convert()
                         ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1231, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1183, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1749, in convert
    self._freeze_keras_model()
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1690, in _freeze_keras_model
    input_signature = _model_input_signature(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/tflite_keras_util.py"", line 119, in model_input_signature
    input_specs = model._get_save_spec(  # pylint: disable=protected-access
                  ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TFSMLayer' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?
```

",feranick,2024-09-27 19:44:16+00:00,"['sirakiin', 'gaikwadrahul8', 'pkgoogle']",2025-02-03 20:07:04+00:00,,https://github.com/tensorflow/tensorflow/issues/76675,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2383162831, 'issue_id': 2553624121, 'author': 'gaikwadrahul8', 'body': 'Hi, @feranick\r\n\r\nThank you for bringing this issue to our attention, I was trying to replicate the same behavior from my end with `TensorFlow version 2.17.0 ` and I did not encounter error which is mentioned in the issue template for your reference I have attached [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ca6167e8c96be86276f498c364d096d6/test-76675.ipynb)\r\n\r\nPlease let me know if I have missed something here.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 30, 13, 15, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383297717, 'issue_id': 2553624121, 'author': 'feranick', 'body': ""Thanks for the quick reply. Please find the [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) with the code to convert a TF model to a tflite. It works on TF2.16.2, not on TF 2.17.0. (The same script in github is available [here](https://github.com/feranick/DataML/blob/master/src/utilities/ConvertToTFLiteK3.py\r\n)). \r\n\r\nAttached here are the zipped models created with tf2.16.2 and tf2.17.0. After unzipping, you can try to convert them using the script above. Again, conversion of either TF model works with this script (which suggests that how you made the model doesn't seem to be the issue) when using TF on 2.16.2 but not with TF 2.17.0 (with error in this report). \r\n\r\n[k3_tf_2.16.2_model_regressor.zip](https://github.com/user-attachments/files/17191135/k3_tf_2.16.2_model_regressor.zip)\r\n[k3_tf_2.17.0_model_regressor.zip](https://github.com/user-attachments/files/17191136/k3_tf_2.17.0_model_regressor.zip)\r\n\r\nPlease let me know if you need anything else."", 'created_at': datetime.datetime(2024, 9, 30, 14, 2, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386204954, 'issue_id': 2553624121, 'author': 'feranick', 'body': 'The issue is still present in TF nightly (2024/10/01) provisional v. 2.19.0 and 2.18.0-rc0', 'created_at': datetime.datetime(2024, 10, 1, 14, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392196101, 'issue_id': 2553624121, 'author': 'feranick', 'body': 'Tagging: https://github.com/tensorflow/tensorflow/tree/v2.18.0-rc0\r\nTagging ""v2.18.0-rc0""', 'created_at': datetime.datetime(2024, 10, 3, 19, 42, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393709853, 'issue_id': 2553624121, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nI apologize for the delayed response, I was trying to replicate the same behavior from my end with your this [script](https://github.com/feranick/DataML/blob/master/src/utilities/ConvertToTFLiteK3.py) but I'm getting different error so if possible could you please give me access to your [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) to replicate same behavior from my end ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 4, 13, 27, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393890326, 'issue_id': 2553624121, 'author': 'feranick', 'body': 'Thanks and no worries. You should be able to access the [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV). Just run it, and the error should appear. \r\n\r\nI used the colab AI feature that suggested a completely different way to do the conversion, that does not run on Keras but it is built in TF. Anyway, I added both version of the method *BROKEN"" and ""SUGGESTED"" and you can comment/uncomment the last line to run either one. The Broken one is the one that triggers the issue.\r\n\r\nPlease note that there is nothing special about the code in the broken call, I am using code that used to work fine in TF 2.16.2 and it is the one suggested.', 'created_at': datetime.datetime(2024, 10, 4, 14, 50, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397527749, 'issue_id': 2553624121, 'author': 'gaikwadrahul8', 'body': ""Hi, @feranick \r\n\r\nThank you for providing the access to your [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) and I'm able to replicate the similar behavior from my end `AttributeError: 'TFSMLayer' object has no attribute '_get_save_spec'`so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 7, 17, 43, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413774906, 'issue_id': 2553624121, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 15, 12, 26, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442572963, 'issue_id': 2553624121, 'author': 'pkgoogle', 'body': 'I was able to replicate as above, this issue did happen before in more general cases (partially caused by Keras 2 to 3). Hi @sirakiin, can you please take a look?', 'created_at': datetime.datetime(2024, 10, 28, 20, 35, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631618139, 'issue_id': 2553624121, 'author': 'jmarrietar', 'body': ""Hi @pkgoogle @sirakiin 👋 , do you have a chance to look into this issue? I also have the `'_get_save_spec'.`  problem. \n\nThanks!"", 'created_at': datetime.datetime(2025, 2, 3, 17, 23, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631960388, 'issue_id': 2553624121, 'author': 'pkgoogle', 'body': ""Hi @feranick, @jmarrietar, what's the whole workflow expected here? Reason I ask is the preferred workflow now would be to save in PyTorch (or perhaps pytorch-backed Keras), reload the model in PyTorch, then convert using [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch). Can you guys switch to that workflow or is there a reason you guys can't?"", 'created_at': datetime.datetime(2025, 2, 3, 20, 6, 29, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-30 13:15:20 UTC): Hi, @feranick

Thank you for bringing this issue to our attention, I was trying to replicate the same behavior from my end with `TensorFlow version 2.17.0 ` and I did not encounter error which is mentioned in the issue template for your reference I have attached [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ca6167e8c96be86276f498c364d096d6/test-76675.ipynb)

Please let me know if I have missed something here.

Thank you for your cooperation and patience.

feranick (Issue Creator) on (2024-09-30 14:02:37 UTC): Thanks for the quick reply. Please find the [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) with the code to convert a TF model to a tflite. It works on TF2.16.2, not on TF 2.17.0. (The same script in github is available [here](https://github.com/feranick/DataML/blob/master/src/utilities/ConvertToTFLiteK3.py
)). 

Attached here are the zipped models created with tf2.16.2 and tf2.17.0. After unzipping, you can try to convert them using the script above. Again, conversion of either TF model works with this script (which suggests that how you made the model doesn't seem to be the issue) when using TF on 2.16.2 but not with TF 2.17.0 (with error in this report). 

[k3_tf_2.16.2_model_regressor.zip](https://github.com/user-attachments/files/17191135/k3_tf_2.16.2_model_regressor.zip)
[k3_tf_2.17.0_model_regressor.zip](https://github.com/user-attachments/files/17191136/k3_tf_2.17.0_model_regressor.zip)

Please let me know if you need anything else.

feranick (Issue Creator) on (2024-10-01 14:45:32 UTC): The issue is still present in TF nightly (2024/10/01) provisional v. 2.19.0 and 2.18.0-rc0

feranick (Issue Creator) on (2024-10-03 19:42:55 UTC): Tagging: https://github.com/tensorflow/tensorflow/tree/v2.18.0-rc0
Tagging ""v2.18.0-rc0""

gaikwadrahul8 (Assginee) on (2024-10-04 13:27:48 UTC): Hi, @feranick 

I apologize for the delayed response, I was trying to replicate the same behavior from my end with your this [script](https://github.com/feranick/DataML/blob/master/src/utilities/ConvertToTFLiteK3.py) but I'm getting different error so if possible could you please give me access to your [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) to replicate same behavior from my end ? 

Thank you for your cooperation and patience.

feranick (Issue Creator) on (2024-10-04 14:50:23 UTC): Thanks and no worries. You should be able to access the [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV). Just run it, and the error should appear. 

I used the colab AI feature that suggested a completely different way to do the conversion, that does not run on Keras but it is built in TF. Anyway, I added both version of the method *BROKEN"" and ""SUGGESTED"" and you can comment/uncomment the last line to run either one. The Broken one is the one that triggers the issue.

Please note that there is nothing special about the code in the broken call, I am using code that used to work fine in TF 2.16.2 and it is the one suggested.

gaikwadrahul8 (Assginee) on (2024-10-07 17:43:45 UTC): Hi, @feranick 

Thank you for providing the access to your [gist-file](https://colab.research.google.com/drive/1jLqjeYl03UmWTWG5NJj5uSEr1ygNO0rF#scrollTo=xITwjDtpOOPV) and I'm able to replicate the similar behavior from my end `AttributeError: 'TFSMLayer' object has no attribute '_get_save_spec'`so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. 

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-15 12:26:47 UTC): Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-28 20:35:47 UTC): I was able to replicate as above, this issue did happen before in more general cases (partially caused by Keras 2 to 3). Hi @sirakiin, can you please take a look?

jmarrietar on (2025-02-03 17:23:33 UTC): Hi @pkgoogle @sirakiin 👋 , do you have a chance to look into this issue? I also have the `'_get_save_spec'.`  problem. 

Thanks!

pkgoogle (Assginee) on (2025-02-03 20:06:29 UTC): Hi @feranick, @jmarrietar, what's the whole workflow expected here? Reason I ask is the preferred workflow now would be to save in PyTorch (or perhaps pytorch-backed Keras), reload the model in PyTorch, then convert using [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch). Can you guys switch to that workflow or is there a reason you guys can't?

"
2553395369,issue,closed,not_planned,TF-Lite converter crashes when quantizing GRU layer,"On Apple M1 Pro with MacOs Sonoma 14.1.2 (23B92)
TensorFlow version 2.16.1 (still an issue with TF 2.17.0)

sources + assets:
[Gru_quantization_bug.zip](https://github.com/user-attachments/files/17167511/Gru_quantization_bug.zip)

The zip contain a TF saved model and a convert.py script that converts it to a TF-Lite mode, but it crashes

removing either line 11 or 12 from the script makes the compilation pass

",bas-aarts,2024-09-27 17:18:48+00:00,"['paulinesho', 'gaikwadrahul8', 'pkgoogle']",2025-01-22 21:52:38+00:00,2025-01-22 21:52:35+00:00,https://github.com/tensorflow/tensorflow/issues/76672,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2382236392, 'issue_id': 2553395369, 'author': 'tilakrayal', 'body': '@bas-aarts,\r\nI guess this might be due to Keras3.0 which contains with tensorflow v2.17. Could you please try to use **!pip install tf-keras** and **import tf_keras as keras** which imports Keras2.0 and try to execute the code. Thank you!', 'created_at': datetime.datetime(2024, 9, 30, 6, 40, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383680301, 'issue_id': 2553395369, 'author': 'bas-aarts', 'body': ""That's what I did from the start. The saved model is generated using tf_keras"", 'created_at': datetime.datetime(2024, 9, 30, 16, 38, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383702190, 'issue_id': 2553395369, 'author': 'bas-aarts', 'body': 'same happens for LSTM layers btw', 'created_at': datetime.datetime(2024, 9, 30, 16, 50, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385546766, 'issue_id': 2553395369, 'author': 'gaikwadrahul8', 'body': ""Hi, @bas-aarts \r\n\r\nI apologize for the delayed response, I am able to replicate the same behavior from my end with `TensorFlow version 2.17.0` for reference I've added output log below and I also tried with TensorFlow version `2.16.2` and `2.15.1` so we'll have to dig more into this issue, thank you for bringing this issue to our attention\r\n\r\n**1. Output log with TensorFlow version `2.17.0`** \r\n\r\n```\r\ngaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1727781369.076345   62413 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\r\nW0000 00:00:1727781369.076376   62413 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\r\n2024-10-01 16:46:09.076870: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model\r\n2024-10-01 16:46:09.078139: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\r\n2024-10-01 16:46:09.078145: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: ./saved_model\r\n2024-10-01 16:46:09.082964: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-10-01 16:46:09.083624: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\r\n2024-10-01 16:46:09.093334: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: ./saved_model\r\n2024-10-01 16:46:09.100636: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 23768 microseconds.\r\n2024-10-01 16:46:09.109433: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-10-01 16:46:09.133967: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3531] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs\r\nSegmentation fault: 11\r\n```\r\n**2. Output log with TensorFlow version `2.16.2`** \r\n\r\n```\r\n(tf-2.16) gaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1727781968.787166   74219 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\r\nW0000 00:00:1727781968.787198   74219 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\r\n2024-10-01 16:56:08.787706: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model\r\n2024-10-01 16:56:08.788994: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\r\n2024-10-01 16:56:08.789000: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./saved_model\r\n2024-10-01 16:56:08.796175: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-10-01 16:56:08.796800: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\r\n2024-10-01 16:56:08.806372: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./saved_model\r\n2024-10-01 16:56:08.813352: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 25648 microseconds.\r\n2024-10-01 16:56:08.821567: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-10-01 16:56:08.845925: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3064] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs\r\nSegmentation fault: 11\r\n```\r\n**3. Output log with TensorFlow version `2.15.1`** \r\n\r\n```\r\n(tf-2.15) gaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py\r\n2024-10-01 16:59:25.527363: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\r\n2024-10-01 16:59:25.527391: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\r\n2024-10-01 16:59:25.527864: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model\r\n2024-10-01 16:59:25.529174: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\r\n2024-10-01 16:59:25.529180: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./saved_model\r\n2024-10-01 16:59:25.531466: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-10-01 16:59:25.532094: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\r\n2024-10-01 16:59:25.544428: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: ./saved_model\r\n2024-10-01 16:59:25.551775: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 23913 microseconds.\r\n2024-10-01 16:59:25.562335: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nSummary on the non-converted ops:\r\n---------------------------------\r\n * Accepted dialects: tfl, builtin, func\r\n * Non-Converted Ops: 20, Total Ops 62, % non-converted = 32.26 %\r\n * 20 ARITH ops\r\n\r\n- arith.constant:   20 occurrences  (f32: 6, i32: 14)\r\n\r\n  (i1: 1, i32: 1)\r\n\r\n\r\n  (f32: 4, i32: 2)\r\n  (f32: 1, i32: 2)\r\n  (f32: 3)\r\n  (f32: 1)\r\n  (i1: 1)\r\n  (f32: 2)\r\n  (f32: 3)\r\n\r\n  (f32: 2, i32: 2)\r\n  (f32: 2)\r\n  (f32: 1)\r\n  (f32: 1)\r\n  (f32: 1)\r\n  (f32: 1)\r\n  (i32: 1)\r\n\r\n2024-10-01 16:59:25.590713: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs\r\nSegmentation fault: 11\r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 1, 11, 38, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411219237, 'issue_id': 2553395369, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 14, 13, 9, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438966970, 'issue_id': 2553395369, 'author': 'pkgoogle', 'body': 'Hi @bas-aarts, are you willing to use transformers/LLMs? Those workflows are better supported.', 'created_at': datetime.datetime(2024, 10, 25, 22, 29, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445442478, 'issue_id': 2553395369, 'author': 'bas-aarts', 'body': ""That's not possible. This is triggered when converting an existing onnx model to TF. I do not control the model that is being converted"", 'created_at': datetime.datetime(2024, 10, 29, 22, 28, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448008534, 'issue_id': 2553395369, 'author': 'pkgoogle', 'body': 'I was also able to replicate on tf-nightly.\r\n\r\n@paulinesho can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 10, 30, 18, 22, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596062569, 'issue_id': 2553395369, 'author': 'NidaaElsayed', 'body': 'Hello, \n\nIs there an update on this issue ? \nGRU and LSTM can not be quantized with tensorflow 2.18 either. A segmentation fault still occurs.\n\nConverting LSTM to float is also not resulting ina Unidirectional Sequence LSTM operator anymore but a While operator.', 'created_at': datetime.datetime(2025, 1, 16, 15, 41, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608346834, 'issue_id': 2553395369, 'author': 'pkgoogle', 'body': ""Hi All, we'll move this issue to LiteRT for now."", 'created_at': datetime.datetime(2025, 1, 22, 21, 52, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608346882, 'issue_id': 2553395369, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76672"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76672"">No</a>', 'created_at': datetime.datetime(2025, 1, 22, 21, 52, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-09-30 06:40:26 UTC): @bas-aarts,
I guess this might be due to Keras3.0 which contains with tensorflow v2.17. Could you please try to use **!pip install tf-keras** and **import tf_keras as keras** which imports Keras2.0 and try to execute the code. Thank you!

bas-aarts (Issue Creator) on (2024-09-30 16:38:34 UTC): That's what I did from the start. The saved model is generated using tf_keras

bas-aarts (Issue Creator) on (2024-09-30 16:50:26 UTC): same happens for LSTM layers btw

gaikwadrahul8 (Assginee) on (2024-10-01 11:38:11 UTC): Hi, @bas-aarts 

I apologize for the delayed response, I am able to replicate the same behavior from my end with `TensorFlow version 2.17.0` for reference I've added output log below and I also tried with TensorFlow version `2.16.2` and `2.15.1` so we'll have to dig more into this issue, thank you for bringing this issue to our attention

**1. Output log with TensorFlow version `2.17.0`** 

```
gaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1727781369.076345   62413 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.
W0000 00:00:1727781369.076376   62413 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.
2024-10-01 16:46:09.076870: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model
2024-10-01 16:46:09.078139: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-10-01 16:46:09.078145: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: ./saved_model
2024-10-01 16:46:09.082964: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-10-01 16:46:09.083624: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-10-01 16:46:09.093334: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: ./saved_model
2024-10-01 16:46:09.100636: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 23768 microseconds.
2024-10-01 16:46:09.109433: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-10-01 16:46:09.133967: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3531] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs
Segmentation fault: 11
```
**2. Output log with TensorFlow version `2.16.2`** 

```
(tf-2.16) gaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1727781968.787166   74219 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.
W0000 00:00:1727781968.787198   74219 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.
2024-10-01 16:56:08.787706: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model
2024-10-01 16:56:08.788994: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-10-01 16:56:08.789000: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./saved_model
2024-10-01 16:56:08.796175: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-10-01 16:56:08.796800: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2024-10-01 16:56:08.806372: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./saved_model
2024-10-01 16:56:08.813352: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 25648 microseconds.
2024-10-01 16:56:08.821567: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-10-01 16:56:08.845925: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3064] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs
Segmentation fault: 11
```
**3. Output log with TensorFlow version `2.15.1`** 

```
(tf-2.15) gaikwadrahul-macbookpro2:gru_crash gaikwadrahul$ python3 convert.py
2024-10-01 16:59:25.527363: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
2024-10-01 16:59:25.527391: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.
2024-10-01 16:59:25.527864: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./saved_model
2024-10-01 16:59:25.529174: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-10-01 16:59:25.529180: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./saved_model
2024-10-01 16:59:25.531466: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-10-01 16:59:25.532094: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-10-01 16:59:25.544428: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: ./saved_model
2024-10-01 16:59:25.551775: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 23913 microseconds.
2024-10-01 16:59:25.562335: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 20, Total Ops 62, % non-converted = 32.26 %
 * 20 ARITH ops

- arith.constant:   20 occurrences  (f32: 6, i32: 14)

  (i1: 1, i32: 1)


  (f32: 4, i32: 2)
  (f32: 1, i32: 2)
  (f32: 3)
  (f32: 1)
  (i1: 1)
  (f32: 2)
  (f32: 3)

  (f32: 2, i32: 2)
  (f32: 2)
  (f32: 1)
  (f32: 1)
  (f32: 1)
  (f32: 1)
  (i32: 1)

2024-10-01 16:59:25.590713: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 3642  ops, equivalently 1821  MACs
Segmentation fault: 11
```

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-10-14 13:09:14 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-25 22:29:11 UTC): Hi @bas-aarts, are you willing to use transformers/LLMs? Those workflows are better supported.

bas-aarts (Issue Creator) on (2024-10-29 22:28:10 UTC): That's not possible. This is triggered when converting an existing onnx model to TF. I do not control the model that is being converted

pkgoogle (Assginee) on (2024-10-30 18:22:20 UTC): I was also able to replicate on tf-nightly.

@paulinesho can you please take a look? Thanks.

NidaaElsayed on (2025-01-16 15:41:37 UTC): Hello, 

Is there an update on this issue ? 
GRU and LSTM can not be quantized with tensorflow 2.18 either. A segmentation fault still occurs.

Converting LSTM to float is also not resulting ina Unidirectional Sequence LSTM operator anymore but a While operator.

pkgoogle (Assginee) on (2025-01-22 21:52:35 UTC): Hi All, we'll move this issue to LiteRT for now.

google-ml-butler[bot] on (2025-01-22 21:52:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76672"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76672"">No</a>

"
2553311549,issue,closed,completed,TensorFlowLiteSwift iOS 18 specific issues of bad memory access,"Pod versions:
- TensorFlowLiteSwift: 2.17.0
- TensorFlowLiteSelectTfOps: 0.0.1-nightly.20240927 or 2.17.0

Reproducible on: iOS 18 (Sim and Physical device)

Followed [this](https://ai.google.dev/edge/litert/models/ops_select#ios) guide for TensorFlow operators.

Load the model using below code - This works just fine
```swift
private static func loadModel() -> Interpreter? {
    let resource = ModelResource.model
    guard let modelPath = Bundle.main.path(forResource: resource.fileName,
                                           ofType: resource.fileExtension) else {
        fatalError(""Could not locate \(resource.fileName).\(resource.fileExtension) within main bundle"")
    }
    do {
        let interpreter = try Interpreter(modelPath: modelPath)
        try interpreter.allocateTensors()
        return interpreter
    } catch {
        fatalError(""Failed to load model: \(error.localizedDescription)"")
    }
}
```

When trying to load the model from local disk storage using below code,
```swift
private static func loadModel() -> Interpreter? {
    let resource = ModelResource.model
    guard let modelURL = resource.url else {
        log.error(""Could not locate \(resource.fileName).\(resource.fileExtension) within main bundle"")
        return nil
    }
    
    do {
        let interpreter = try Interpreter(modelData: Data(contentsOf: modelURL))
        try interpreter.allocateTensors()
        return interpreter
    } catch {
        log.error(""Failed to load model: \(error.localizedDescription)"")
        return nil
    }
}
```

Getting memory address error while invoking `interpreter.allocateTensors()`
> Thread 14: EXC_BAD_ACCESS (code=1, address=0x7fc453e9d338).

Stack trace of 'TensorFlowLite.Interpreter.allocateTensors()'
```
ProjectName.debug.dylib`TensorFlowLite.Interpreter.allocateTensors() throws -> ():
    0x10bdf9580 <+0>:  pushq  %rbp
    0x10bdf9581 <+1>:  movq   %rsp, %rbp
    0x10bdf9584 <+4>:  leaq   0xbb8a5(%rip), %rdi       ; TfLiteInterpreterAllocateTensors
    0x10bdf958b <+11>: movl   $0x4, %esi
    0x10bdf9590 <+16>: callq  0x10bdf95a0               ; merged TensorFlowLite.Interpreter.invoke() throws -> ()
->  0x10bdf9595 <+21>: popq   %rbp
    0x10bdf9596 <+22>: retq   
    0x10bdf9597 <+23>: nopw   (%rax,%rax)
```",nishchal-v,2024-09-27 16:24:46+00:00,['pkgoogle'],2024-11-13 02:01:17+00:00,2024-11-13 02:01:15+00:00,https://github.com/tensorflow/tensorflow/issues/76669,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('iOS', ''), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2383157729, 'issue_id': 2553311549, 'author': 'nishchal-v', 'body': 'The solution for now is to use\r\n```swift\r\nlet interpreter = try Interpreter(modelPath: modelURL.path(percentEncoded: false))\r\n```\r\n\r\nStill, the initialisation using model data is an issue.', 'created_at': datetime.datetime(2024, 9, 30, 13, 13, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413739351, 'issue_id': 2553311549, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 15, 12, 10, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442533457, 'issue_id': 2553311549, 'author': 'pkgoogle', 'body': 'Hi @nishchal-v, can you please share the model which produces this error? If you wish a reduced/smaller model which reproduces the issue is sufficient. Thanks for your help.', 'created_at': datetime.datetime(2024, 10, 28, 20, 14, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456067303, 'issue_id': 2553311549, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 5, 2, 0, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472173863, 'issue_id': 2553311549, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472174001, 'issue_id': 2553311549, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76669"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76669"">No</a>', 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 16, tzinfo=datetime.timezone.utc)}]","nishchal-v (Issue Creator) on (2024-09-30 13:13:07 UTC): The solution for now is to use
```swift
let interpreter = try Interpreter(modelPath: modelURL.path(percentEncoded: false))
```

Still, the initialisation using model data is an issue.

gaikwadrahul8 on (2024-10-15 12:10:16 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-28 20:14:58 UTC): Hi @nishchal-v, can you please share the model which produces this error? If you wish a reduced/smaller model which reproduces the issue is sufficient. Thanks for your help.

github-actions[bot] on (2024-11-05 02:00:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-13 02:01:14 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-13 02:01:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76669"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76669"">No</a>

"
2552268940,issue,closed,completed,How to determines which GEMM used in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


Where is the code that determines which GEMM to use in TensorFlow, such as using Eigen or oneDNN, and how does it decide to use different types of GEMM?

### Standalone code to reproduce the issue

```shell
Such as arm_gemm, brgemm, etc.
```


### Relevant log output

_No response_",nanzh-19,2024-09-27 07:55:34+00:00,['tilakrayal'],2024-10-16 02:02:51+00:00,2024-10-16 02:02:41+00:00,https://github.com/tensorflow/tensorflow/issues/76632,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2384829982, 'issue_id': 2552268940, 'author': 'tilakrayal', 'body': '@nanzh-19,\r\nCan you please elaborate about the statement which was mentioned. Also, please specify the Use Cases for this feature. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 5, 28, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123175, 'issue_id': 2552268940, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573487, 'issue_id': 2552268940, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573681, 'issue_id': 2552268940, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76632"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76632"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-01 05:28:38 UTC): @nanzh-19,
Can you please elaborate about the statement which was mentioned. Also, please specify the Use Cases for this feature. Thank you!

github-actions[bot] on (2024-10-09 02:01:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76632"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76632"">No</a>

"
2552014237,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 from tensorflow.keras import datasets
      2 import matplotlob.pyplot as plt
      3 import numpy as np

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

### Standalone code to reproduce the issue

```shell
from tensorflow.keras import datasets
```


### Relevant log output

_No response_",Ashraful-Dowla,2024-09-27 05:13:34+00:00,['Venkat6871'],2024-10-22 06:36:21+00:00,2024-10-22 06:36:18+00:00,https://github.com/tensorflow/tensorflow/issues/76627,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype:cpu-intel', 'To track windows cpu issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2380828971, 'issue_id': 2552014237, 'author': 'akbism', 'body': 'I am also facing exactly the same error.', 'created_at': datetime.datetime(2024, 9, 28, 16, 52, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381101210, 'issue_id': 2552014237, 'author': 'Ashraful-Dowla', 'body': ""> I am also facing exactly the same error.\r\nI also couldn't resolve the issue. I need a dataset from that library. I found another way to import the dataset."", 'created_at': datetime.datetime(2024, 9, 29, 4, 20, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384947591, 'issue_id': 2552014237, 'author': 'Venkat6871', 'body': 'Hi **@Ashraful-Dowla** ,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 6, 56, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386601942, 'issue_id': 2552014237, 'author': 'Ashraful-Dowla', 'body': 'Hello Venkat, My PC does not support AVX2 instructions. Is there alternative to use tensorflow library in my PC?', 'created_at': datetime.datetime(2024, 10, 1, 17, 45, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428322044, 'issue_id': 2552014237, 'author': 'Venkat6871', 'body': 'Hi **@Ashraful-Dowla** ,\r\nApologies for the delay. According to the documentation, there is no alternative; you must follow AVX2 instructions for optimal results.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 22, 5, 55, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428376003, 'issue_id': 2552014237, 'author': 'Ashraful-Dowla', 'body': 'Thanks for your reply @Venkat6871!', 'created_at': datetime.datetime(2024, 10, 22, 6, 36, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428376067, 'issue_id': 2552014237, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76627"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76627"">No</a>', 'created_at': datetime.datetime(2024, 10, 22, 6, 36, 20, tzinfo=datetime.timezone.utc)}]","akbism on (2024-09-28 16:52:19 UTC): I am also facing exactly the same error.

Ashraful-Dowla (Issue Creator) on (2024-09-29 04:20:55 UTC): I also couldn't resolve the issue. I need a dataset from that library. I found another way to import the dataset.

Venkat6871 (Assginee) on (2024-10-01 06:56:50 UTC): Hi **@Ashraful-Dowla** ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

Ashraful-Dowla (Issue Creator) on (2024-10-01 17:45:51 UTC): Hello Venkat, My PC does not support AVX2 instructions. Is there alternative to use tensorflow library in my PC?

Venkat6871 (Assginee) on (2024-10-22 05:55:51 UTC): Hi **@Ashraful-Dowla** ,
Apologies for the delay. According to the documentation, there is no alternative; you must follow AVX2 instructions for optimal results.
Thank you!

Ashraful-Dowla (Issue Creator) on (2024-10-22 06:36:18 UTC): Thanks for your reply @Venkat6871!

google-ml-butler[bot] on (2024-10-22 06:36:20 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76627"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76627"">No</a>

"
2551950150,issue,closed,completed,TF-Lite converter bug in 2.17 for TransposeConv with quantized weights,"On Apple M1 Pro with MacOs Sonoma 14.1.2 (23B92)
TensorFlow version 2.17.0 (not an issue with TF 2.16.1)
I ran this one a Samsung Galaxy S24 Ultra

sources + assets:
[TransposeConv_bug.zip](https://github.com/user-attachments/files/17158711/TransposeConv_bug.zip)

The zip contains a TF saved model and a convert.py script that converts it to a TF-Lite mode.
the zip file contains two TF-Lite models, and created with TF 2.16.1, and the other with 2.17.0.
convert.py also runs the TF-Lite model, but it needs to run on a target with GPU and/or NPU delegates to really see the issue.

When running the TF 2.16.1 TF-Lite model, the model runs on the NPU (Qualcomm QNN delegate).
When running the TF 2.17.0 TF-Lite model, the model runs on the CPU, using the XNNpack/TFLite delegates.

we see this log from TF-Lite on the device:
[tflite.log](https://github.com/user-attachments/files/17158819/tflite.log)
the interesting lines are:
`[27/Sept/2024:11:32:40 +08:00: profiler/warning] [job_id: j87gj31pd] [model.tflite] [tflite] tensorflow/lite/kernels/transpose_conv.cc:487 affine_quantization->scale->size != weights->dims->data[affine_quantization->quantized_dimension] (1 != 4)
`
and
`[27/Sept/2024:11:32:40 +08:00: profiler/warning] [job_id: j87gj31pd] [model.tflite] [tflite] tensorflow/lite/kernels/transpose_conv.cc:487 affine_quantization->scale->size != weights->dims->data[affine_quantization->quantized_dimension] (1 != 4)
`
the first one fails the NPU delegate check and the latter the GPU delegate check.

the check here is:
`https://github.com/tensorflow/tensorflow/blob/9e4fc3d09c298ca56bd11f72d2f1beb622a0f76b/tensorflow/lite/kernels/transpose_conv.cc#L485-L487`

This check is performed only when the Transpose Conv layer has float input but integral weights. This was not the case in TF 2.16, where the weights were first dequantized.
the check makes it looks like this must be a per channel quantized weight, which is clearly not the case.

So the converter generates code that the runtime forces back to the slower CPU delegates",bas-aarts,2024-09-27 04:10:05+00:00,['pkgoogle'],2024-11-06 07:20:33+00:00,2024-11-06 07:20:30+00:00,https://github.com/tensorflow/tensorflow/issues/76624,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('awaiting PR merge', 'awaiting PR merge'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385877800, 'issue_id': 2551950150, 'author': 'gaikwadrahul8', 'body': ""Hi, @bas-aarts\r\n\r\nI apologize for the delayed response, I was trying to replicate the same behavior from my end so to confirm, did you use [QAI Hub](https://app.aihub.qualcomm.com/docs/) with Samsung Galaxy S24 Ultra device or actual physical device ? \r\n\r\nIf you don't mind could you please guide me to replicate the same behavior from my end also. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 1, 13, 29, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397520021, 'issue_id': 2551950150, 'author': 'bas-aarts', 'body': ""Yes I did use AI Hub, but this does use an actual physical Samsung Galaxy S24 Ultra.\r\nThe AI Hub TFLite runtime uses theTF-Lite GPU delegate, as well as the QNN NPU delegate.\r\n\r\nTo reproduce you'll need a TF-Lite runtime that can use either of these, and possible opt-in using:\r\n`  interpreter.experimental_delegate = tf.lite.experimental.load_delegate('some delegate shared library')`\r\n\r\nI see a PR above, which aI hope means you were able to reproduce the issue."", 'created_at': datetime.datetime(2024, 10, 7, 17, 39, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411202990, 'issue_id': 2551950150, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\n\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 14, 13, 7, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438964109, 'issue_id': 2551950150, 'author': 'pkgoogle', 'body': 'The current expectation is that the PR will resolve this issue.', 'created_at': datetime.datetime(2024, 10, 25, 22, 26, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455943251, 'issue_id': 2551950150, 'author': 'bas-aarts', 'body': 'since the PR is still in draft, any idea in which release I can expect the fix?', 'created_at': datetime.datetime(2024, 11, 4, 23, 56, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457900108, 'issue_id': 2551950150, 'author': 'pkgoogle', 'body': 'Hi @bas-aarts, feel free to take the code/diff from the PR if you need it sooner, I would expect the fix to land 1 or 2 releases from now.', 'created_at': datetime.datetime(2024, 11, 5, 18, 36, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458880570, 'issue_id': 2551950150, 'author': 'advaitjain', 'body': 'Should be fixed with https://github.com/tensorflow/tensorflow/commit/dde56340610b37d2f2696b654be50a74dd25ff84', 'created_at': datetime.datetime(2024, 11, 6, 7, 20, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458880607, 'issue_id': 2551950150, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76624"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76624"">No</a>', 'created_at': datetime.datetime(2024, 11, 6, 7, 20, 32, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-10-01 13:29:47 UTC): Hi, @bas-aarts

I apologize for the delayed response, I was trying to replicate the same behavior from my end so to confirm, did you use [QAI Hub](https://app.aihub.qualcomm.com/docs/) with Samsung Galaxy S24 Ultra device or actual physical device ? 

If you don't mind could you please guide me to replicate the same behavior from my end also. 

Thank you for your cooperation and patience.

bas-aarts (Issue Creator) on (2024-10-07 17:39:31 UTC): Yes I did use AI Hub, but this does use an actual physical Samsung Galaxy S24 Ultra.
The AI Hub TFLite runtime uses theTF-Lite GPU delegate, as well as the QNN NPU delegate.

To reproduce you'll need a TF-Lite runtime that can use either of these, and possible opt-in using:
`  interpreter.experimental_delegate = tf.lite.experimental.load_delegate('some delegate shared library')`

I see a PR above, which aI hope means you were able to reproduce the issue.

gaikwadrahul8 on (2024-10-14 13:07:43 UTC): Hi, @pkgoogle 

Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-25 22:26:13 UTC): The current expectation is that the PR will resolve this issue.

bas-aarts (Issue Creator) on (2024-11-04 23:56:22 UTC): since the PR is still in draft, any idea in which release I can expect the fix?

pkgoogle (Assginee) on (2024-11-05 18:36:30 UTC): Hi @bas-aarts, feel free to take the code/diff from the PR if you need it sooner, I would expect the fix to land 1 or 2 releases from now.

advaitjain on (2024-11-06 07:20:31 UTC): Should be fixed with https://github.com/tensorflow/tensorflow/commit/dde56340610b37d2f2696b654be50a74dd25ff84

google-ml-butler[bot] on (2024-11-06 07:20:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76624"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76624"">No</a>

"
2551113356,issue,closed,completed,Python Failed to load the native TensorFlow runtime Idle Shell. ,"OS is Windows 10 and Python version 3.11.7, Pip version is 23.2.1 and Tensorflow 2.17. Installing to run NLP neural programs for cognitive linguistic modelling.
Python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license()"" for more information.
import tensorflow as tf
Traceback (most recent call last):
  File ""E:\Python\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""E:\Python\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""E:\Python\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""E:\Python\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


",NMB1967,2024-09-26 17:00:32+00:00,['Venkat6871'],2025-01-03 17:00:36+00:00,2025-01-03 17:00:33+00:00,https://github.com/tensorflow/tensorflow/issues/76581,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('subtype:cpu-intel', 'To track windows cpu issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2378678606, 'issue_id': 2551113356, 'author': 'Venkat6871', 'body': 'Hi **@NMB1967** ,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n[#61887](https://github.com/tensorflow/tensorflow/issues/61887)\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 8, 6, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380672953, 'issue_id': 2551113356, 'author': 'NMB1967', 'body': ""Thank you for your feedback. So tensor version and compatible version is 2.17  on OS Windows 10 and Python version 3.11.7 64 bit, Pip version is 23.2.1.\r\nI've added more details if that helps below. I will look at the guidance you gave.\r\n\r\nMore details on Python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)] on win32.\r\nI had install Python 3.12 and Tensorflow 2.16.1 but had the same error message when importing tensorflow into python.\r\nRegards laptop details:\r\nSystem Manufacturer:       Dell Inc.\r\nSystem Model:              Latitude E5500\r\nSystem Type:               x64-based PC\r\nProcessor(s):              1 Processor(s) Installed.\r\nIntel64 Family 6 Model 15 Stepping 13 GenuineIntel ~2001 Mhz\r\nBIOS Version:              Dell Inc. A17, 27/09/2011"", 'created_at': datetime.datetime(2024, 9, 28, 14, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380672976, 'issue_id': 2551113356, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">No</a>', 'created_at': datetime.datetime(2024, 9, 28, 14, 57, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380674442, 'issue_id': 2551113356, 'author': 'NMB1967', 'body': ""Thank you for your feedback. So tensor version and compatible version is 2.17 on OS Windows 10 and Python version 3.11.7 64 bit, Pip version is 23.2.1.\r\nI've added more details if that helps below. I will look at the guidance you gave.\r\n\r\nMore details on Python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec 4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)] on win32.\r\nI had install Python 3.12 and Tensorflow 2.16.1 but had the same error message when importing tensorflow into python.\r\nRegards laptop details:\r\nSystem Manufacturer: Dell Inc.\r\nSystem Model: Latitude E5500\r\nSystem Type: x64-based PC\r\nProcessor(s): 1 Processor(s) Installed.\r\nIntel64 Family 6 Model 15 Stepping 13 GenuineIntel ~2001 Mhz\r\nBIOS Version: Dell Inc. A17, 27/09/2011"", 'created_at': datetime.datetime(2024, 9, 28, 14, 59, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569543646, 'issue_id': 2551113356, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 17, 0, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569543692, 'issue_id': 2551113356, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 17, 0, 35, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-27 08:06:31 UTC): Hi **@NMB1967** ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
[#61887](https://github.com/tensorflow/tensorflow/issues/61887)


Thank you!

NMB1967 (Issue Creator) on (2024-09-28 14:57:48 UTC): Thank you for your feedback. So tensor version and compatible version is 2.17  on OS Windows 10 and Python version 3.11.7 64 bit, Pip version is 23.2.1.
I've added more details if that helps below. I will look at the guidance you gave.

More details on Python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)] on win32.
I had install Python 3.12 and Tensorflow 2.16.1 but had the same error message when importing tensorflow into python.
Regards laptop details:
System Manufacturer:       Dell Inc.
System Model:              Latitude E5500
System Type:               x64-based PC
Processor(s):              1 Processor(s) Installed.
Intel64 Family 6 Model 15 Stepping 13 GenuineIntel ~2001 Mhz
BIOS Version:              Dell Inc. A17, 27/09/2011

google-ml-butler[bot] on (2024-09-28 14:57:50 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">No</a>

NMB1967 (Issue Creator) on (2024-09-28 14:59:38 UTC): Thank you for your feedback. So tensor version and compatible version is 2.17 on OS Windows 10 and Python version 3.11.7 64 bit, Pip version is 23.2.1.
I've added more details if that helps below. I will look at the guidance you gave.

More details on Python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec 4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)] on win32.
I had install Python 3.12 and Tensorflow 2.16.1 but had the same error message when importing tensorflow into python.
Regards laptop details:
System Manufacturer: Dell Inc.
System Model: Latitude E5500
System Type: x64-based PC
Processor(s): 1 Processor(s) Installed.
Intel64 Family 6 Model 15 Stepping 13 GenuineIntel ~2001 Mhz
BIOS Version: Dell Inc. A17, 27/09/2011

mihaimaruseac on (2025-01-03 17:00:33 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2025-01-03 17:00:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76581"">No</a>

"
2550819386,issue,closed,completed,"Model build uses incorrect input shape, since 2.16.1","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Suppose that you have a model that splits its `input` as follows:
```
x = intuts[:,:,:-1]
mask = intuts[:,:,-1:]
```
Let further `x` be used in a custom layer: `y=layer(x)`. This layer should be build using the shape of `x`. However, since TF version 2.16.1, the layer fails to be build correctly. See a very simple example on google collab below. It was verified that in TF version 2.15 and below everything works as expected.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ULMSpyTnkCFmg0OZYa_8hte2rnqY6iVx?usp=sharing
```


### Relevant log output

_No response_",ml-inr,2024-09-26 14:46:51+00:00,['tilakrayal'],2024-09-27 14:06:34+00:00,2024-09-27 14:06:29+00:00,https://github.com/tensorflow/tensorflow/issues/76576,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2378392777, 'issue_id': 2550819386, 'author': 'tilakrayal', 'body': '@ml-inr,\r\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3be9dfcccdeea4b1e45ba329458a4bdb/untitled2133.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 4, 47, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378878127, 'issue_id': 2550819386, 'author': 'ml-inr', 'body': 'Hi! After switching to `tf-keras` everything works, thanks!\r\n\r\nSo, is this a known keras3.0 or TF-keras3.0 interaction bug? Are there plans to fix this? I understand that keras3.0 is now an independent project. I can post this issue on the keras page, if this makes sense.', 'created_at': datetime.datetime(2024, 9, 27, 9, 42, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378934949, 'issue_id': 2550819386, 'author': 'tilakrayal', 'body': '@ml-inr,\r\nYeah, as this is the bug in keras3.0, please try to raise the issue in [keras-team/keras](https://github.com/keras-team/keras/issues) repo & also it is resolved in tf-keras please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 9, 27, 10, 13, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379367478, 'issue_id': 2550819386, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76576"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76576"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 14, 6, 32, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-27 04:47:59 UTC): @ml-inr,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras

import tf_keras as keras
```

Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3be9dfcccdeea4b1e45ba329458a4bdb/untitled2133.ipynb).

Thank you!

ml-inr (Issue Creator) on (2024-09-27 09:42:46 UTC): Hi! After switching to `tf-keras` everything works, thanks!

So, is this a known keras3.0 or TF-keras3.0 interaction bug? Are there plans to fix this? I understand that keras3.0 is now an independent project. I can post this issue on the keras page, if this makes sense.

tilakrayal (Assginee) on (2024-09-27 10:13:49 UTC): @ml-inr,
Yeah, as this is the bug in keras3.0, please try to raise the issue in [keras-team/keras](https://github.com/keras-team/keras/issues) repo & also it is resolved in tf-keras please feel free to move this issue to closed status. Thank you!

google-ml-butler[bot] on (2024-09-27 14:06:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76576"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76576"">No</a>

"
2550200041,issue,closed,completed,tensoflow,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.",tawhidnazari57,2024-09-26 10:37:13+00:00,['Venkat6871'],2024-09-28 15:54:30+00:00,2024-09-26 10:37:26+00:00,https://github.com/tensorflow/tensorflow/issues/76560,[],[],
2549274067,issue,closed,completed,"Why, after using OneDNN, did I find that the GEMM used in the call stack extracted with perf is arm_gemm from ARM Compute instead of BRGEMM from OneDNN?","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf serving 2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

arm_gemm accounts for 65% of the runtime.

### Standalone code to reproduce the issue

```shell
This was discovered during performance analysis of a recommendation model while running inference on TensorFlow Serving.
```


### Relevant log output

_No response_",nanzh-19,2024-09-26 01:23:34+00:00,['tilakrayal'],2024-09-27 07:07:33+00:00,2024-09-27 07:07:29+00:00,https://github.com/tensorflow/tensorflow/issues/76525,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2376844008, 'issue_id': 2549274067, 'author': 'tilakrayal', 'body': '@nanzh-19,\r\nLooks like this issue is more related to tf-serving. Could you please raise the request in the tf-serving repo from here for the quick resolution.\r\n\r\nhttps://github.com/tensorflow/serving/issues\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 26, 12, 41, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376865173, 'issue_id': 2549274067, 'author': 'nanzh-19', 'body': 'ok, thank you.', 'created_at': datetime.datetime(2024, 9, 26, 12, 50, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378383022, 'issue_id': 2549274067, 'author': 'tilakrayal', 'body': '@nanzh-19,\r\nCould you please feel free to move this issue to closed status, since it is already being tracked there? Thank you!', 'created_at': datetime.datetime(2024, 9, 27, 4, 35, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378553119, 'issue_id': 2549274067, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76525"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76525"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 7, 7, 31, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-26 12:41:03 UTC): @nanzh-19,
Looks like this issue is more related to tf-serving. Could you please raise the request in the tf-serving repo from here for the quick resolution.

https://github.com/tensorflow/serving/issues

Thank you!

nanzh-19 (Issue Creator) on (2024-09-26 12:50:44 UTC): ok, thank you.

tilakrayal (Assginee) on (2024-09-27 04:35:05 UTC): @nanzh-19,
Could you please feel free to move this issue to closed status, since it is already being tracked there? Thank you!

google-ml-butler[bot] on (2024-09-27 07:07:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76525"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76525"">No</a>

"
2549227387,issue,closed,completed,TFLite concatenation operator should support float16 data type,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of concatenation](https://www.tensorflow.org/mlir/tfl_ops#operands_21) supports all data types, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/concatenation.cc;l=144?q=concatenation.cc&ss=chromium%2Fchromium%2Fsrc) doesn't support float16 data type.

",fujunwei,2024-09-26 00:30:50+00:00,"['gaikwadrahul8', 'pkgoogle']",2024-11-14 02:01:29+00:00,2024-11-14 02:01:28+00:00,https://github.com/tensorflow/tensorflow/issues/76521,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2378413339, 'issue_id': 2549227387, 'author': 'Venkat6871', 'body': 'Hi **@fujunwei** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 5, 11, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394860034, 'issue_id': 2549227387, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 5, 2, 0, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408321840, 'issue_id': 2549227387, 'author': 'fujunwei', 'body': '> Hi **@fujunwei** , We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced. Thank you!\r\n\r\nI request it because the document is not match with kernel implementation, do we plan to support it? thanks.', 'created_at': datetime.datetime(2024, 10, 12, 3, 1, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432808889, 'issue_id': 2549227387, 'author': 'gaikwadrahul8', 'body': ""Hi, @fujunwei\r\n\r\nI apologize for the delayed response, I was trying to use concatenation operator with `float16` data type but it's automatically using `float32` instead of `float16` please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/d0701cc91bf81dc6e0e7abdbf4addeb9/tflite-76521.ipynb) and I see there is no implementation for `float16` data type in the [source code](https://github.com/tensorflow/tensorflow/blob/ed1ba3af378316519dea147f5af965bb98860a0e/tensorflow/lite/kernels/concatenation.cc#L144) so most probably this issue will be considered as feature request and will update you soon, thank you for bringing this issue to our attention.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 23, 16, 33, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433943312, 'issue_id': 2549227387, 'author': 'fujunwei', 'body': 'Thanks so much.', 'created_at': datetime.datetime(2024, 10, 24, 0, 40, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447283825, 'issue_id': 2549227387, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 30, 14, 5, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448053240, 'issue_id': 2549227387, 'author': 'pkgoogle', 'body': 'Hi @fujunwei, the output documentation seems to match the datatypes supported but the operands says any type. Do you actually need/want float16 dtype to be supported or did you want the documentation to match correctly? Thanks.', 'created_at': datetime.datetime(2024, 10, 30, 18, 35, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461162421, 'issue_id': 2549227387, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 7, 2, 0, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206939, 'issue_id': 2549227387, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-09-27 05:11:11 UTC): Hi **@fujunwei** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

github-actions[bot] on (2024-10-05 02:00:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

fujunwei (Issue Creator) on (2024-10-12 03:01:08 UTC): I request it because the document is not match with kernel implementation, do we plan to support it? thanks.

gaikwadrahul8 (Assginee) on (2024-10-23 16:33:41 UTC): Hi, @fujunwei

I apologize for the delayed response, I was trying to use concatenation operator with `float16` data type but it's automatically using `float32` instead of `float16` please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/d0701cc91bf81dc6e0e7abdbf4addeb9/tflite-76521.ipynb) and I see there is no implementation for `float16` data type in the [source code](https://github.com/tensorflow/tensorflow/blob/ed1ba3af378316519dea147f5af965bb98860a0e/tensorflow/lite/kernels/concatenation.cc#L144) so most probably this issue will be considered as feature request and will update you soon, thank you for bringing this issue to our attention.

Thank you for your cooperation and patience.

fujunwei (Issue Creator) on (2024-10-24 00:40:31 UTC): Thanks so much.

gaikwadrahul8 (Assginee) on (2024-10-30 14:05:36 UTC): Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-30 18:35:58 UTC): Hi @fujunwei, the output documentation seems to match the datatypes supported but the operands says any type. Do you actually need/want float16 dtype to be supported or did you want the documentation to match correctly? Thanks.

github-actions[bot] on (2024-11-07 02:00:42 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-14 02:01:28 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2548034160,issue,closed,completed,Failed to load the native TensorFlow runtime.,"please help ..complete error below..

Traceback (most recent call last):
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\43664\PycharmProjects\pythonProjectDigitalBildungskarenz\Keras.py"", line 20, in <module>
    from keras.models import Sequential
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\__init__.py"", line 4, in <module>
    from keras.api import DTypePolicy
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\api\__init__.py"", line 8, in <module>
    from keras.api import activations
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\api\activations\__init__.py"", line 7, in <module>
    from keras.src.activations import deserialize
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\__init__.py"", line 1, in <module>
    from keras.src import activations
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\activations\__init__.py"", line 3, in <module>
    from keras.src.activations.activations import elu
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\activations\activations.py"", line 1, in <module>
    from keras.src import backend
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\__init__.py"", line 9, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\common\__init__.py"", line 2, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\common\dtypes.py"", line 5, in <module>
    from keras.src.backend.common.variables import standardize_dtype
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\common\variables.py"", line 11, in <module>
    from keras.src.utils.module_utils import tensorflow as tf
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\__init__.py"", line 1, in <module>
    from keras.src.utils.audio_dataset_utils import audio_dataset_from_directory
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\audio_dataset_utils.py"", line 4, in <module>
    from keras.src.utils import dataset_utils
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\dataset_utils.py"", line 9, in <module>
    from keras.src import tree
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\tree\__init__.py"", line 1, in <module>
    from keras.src.tree.tree_api import assert_same_structure
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\tree\tree_api.py"", line 6, in <module>
    from keras.src.tree import optree_impl as tree_impl
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\tree\optree_impl.py"", line 17, in <module>
    from tensorflow.python.trackable.data_structures import ListWrapper
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\43664\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

Process finished with exit code 1
",coco364,2024-09-25 13:40:10+00:00,['tilakrayal'],2024-10-24 02:01:56+00:00,2024-10-24 02:01:50+00:00,https://github.com/tensorflow/tensorflow/issues/76473,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2376100611, 'issue_id': 2548034160, 'author': 'tilakrayal', 'body': '@coco364,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue. Also Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 26, 7, 4, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646559, 'issue_id': 2548034160, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393355887, 'issue_id': 2548034160, 'author': 'coco364', 'body': 'Thank you for your answer! I solve this with google colab. Thanks\xa0Sent using the mobile mail appOn 04.10.24 at 04:01, github-actions[bot] wrote:\r\n                \r\n            \r\n            \r\n                \r\n                    From: ""github-actions[bot]"" ***@***.***>Date: 4. October 2024To: ""tensorflow/tensorflow"" ***@***.***>Cc: ""Mention"" ***@***.***>,""coco364"" ***@***.***>Subject: Re: [tensorflow/tensorflow] Failed to load the native TensorFlow runtime. (Issue #76473)\r\n                \r\n                \r\nThis issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.\r\n\r\n—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 10, 4, 10, 15, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401281014, 'issue_id': 2548034160, 'author': 'tilakrayal', 'body': '@coco364,\r\nGlad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 10, 9, 4, 17, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418337423, 'issue_id': 2548034160, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 17, 2, 1, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434073959, 'issue_id': 2548034160, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434074125, 'issue_id': 2548034160, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76473"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76473"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 56, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-26 07:04:42 UTC): @coco364,
We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue. Also Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-10-04 02:01:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

coco364 (Issue Creator) on (2024-10-04 10:15:22 UTC): Thank you for your answer! I solve this with google colab. Thanks Sent using the mobile mail appOn 04.10.24 at 04:01, github-actions[bot] wrote:
                
            
            
                
                    From: ""github-actions[bot]"" ***@***.***>Date: 4. October 2024To: ""tensorflow/tensorflow"" ***@***.***>Cc: ""Mention"" ***@***.***>,""coco364"" ***@***.***>Subject: Re: [tensorflow/tensorflow] Failed to load the native TensorFlow runtime. (Issue #76473)
                
                
This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>

tilakrayal (Assginee) on (2024-10-09 04:17:40 UTC): @coco364,
Glad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!

github-actions[bot] on (2024-10-17 02:01:43 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:01:49 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:01:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76473"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76473"">No</a>

"
2547539652,issue,open,,Failed to synchronize the stop event: CUDA_ERROR_UNKNOWN: unknown error,"Hello together,

I am trying to run the new-benchmark AI for my new platform:
https://pypi.org/project/new-ai-benchmark/

My System configuration is as follows:
*  TF Version: 2.17.0
*  Platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35
*  CPU: 13th Gen Intel(R) Core(TM) i9-13900E
*  CPU RAM: 62 GB
*  GPU/0: NVIDIA L4
*  GPU RAM: 20.3 GB
*  CUDA Version: 12.4
*  CUDA Build: V12.4.131

arun@arun:~$ nvidia-smi
Wed Sep 25 12:52:43 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L4                      On  |   00000000:01:00.0 Off |                    0 |
| N/A   55C    P8             13W /   72W |      14MiB /  23034MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1373      G   /usr/lib/xorg/Xorg                              4MiB |
+-----------------------------------------------------------------------------------------+


While executing the program, at any point of time while running, it fails to complete.
```

2024-09-25 10:02:40.968881: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1857] failed to synchronize the stop event: CUDA_ERROR_UNKNOWN: unknown error
E0000 00:00:1727251360.968902    4778 gpu_timer.cc:162] INTERNAL: Error destroying CUDA event: CUDA_ERROR_UNKNOWN: unknown error
E0000 00:00:1727251360.968910    4778 gpu_timer.cc:168] INTERNAL: Error destroying CUDA event: CUDA_ERROR_UNKNOWN: unknown error
2024-09-25 10:02:40.969770: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_152/block2/unit_8/bottleneck_v2/conv2/Conv2D}}]]
2024-09-25 10:02:40.969784: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_152/block2/unit_8/bottleneck_v2/conv2/Conv2D}}]]
[[output/_3]]
2024-09-25 10:02:40.969798: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16128423615390660009
2024-09-25 10:02:40.992816: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992867: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992874: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992880: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1401, in _do_call
    return fn(*args)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1384, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1477, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
```

In another instance, there is another reason for failure:

```
Failed to enqueue async memcpy from device to host: CUDA_ERROR_UNKNOWN: unknown error; host dst: 0x763a34bfcf48; GPU src: 0x76387c443400; size: 8=0x8
2024-09-25 11:53:30.387757: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1595] failed to free device memory at 0x76387c443400; result: CUDA_ERROR_UNKNOWN: unknown error
2024-09-25 11:53:30.387762: W tensorflow/core/kernels/gpu_utils.cc:88] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to memcpy from device to host.'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2024-09-25 11:53:30.388810: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D}}]]
2024-09-25 11:53:30.388848: F ./tensorflow/core/kernels/conv_2d_gpu.h:1028] Non-OK-status: GpuLaunchKernel( SwapDimension1And2InTensor3UsingTiles<T, kNumThreads, kTileSize, kTileSize, conjugate>, total_tiles_count, kNumThreads, 0, d.stream(), input, input_dims, output)
Status: INTERNAL: unknown error
```

Could you please help in resolving the issue? 

Thank you
Arun",arun-kumark,2024-09-25 10:01:45+00:00,['Venkat6871'],2024-09-27 06:55:07+00:00,,https://github.com/tensorflow/tensorflow/issues/76456,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2378445813, 'issue_id': 2547539652, 'author': 'Venkat6871', 'body': 'Hi **@arun-kumark** ,\r\nApologies for the delay. There seems to be a version mismatch, please check the version compatibility. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Could you also please explain the exact steps you followed?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 5, 46, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378489463, 'issue_id': 2547539652, 'author': 'arun-kumark', 'body': 'Hi Venkat, \r\nI am setting up the environment on my freshly installed Ubuntu 22.04LTS using the Lambda:\r\nhttps://lambdalabs.com/blog/nvidia-ngc-tutorial-run-pytorch-docker-container-using-nvidia-container-toolkit-on-ubuntu\r\n\r\nThis container set up the CUDA, TensorFlow and CuDNN in reliable way.\r\n\r\nI prepared two setups, one with the L4 Nvidia GPU card and another on my ZBook with RTX3000 GPU.\r\n\r\nOn RTX3000 GPU notebook, the things go smooth, and benchmarking test passes.\r\n\r\nWhile running exactly the same scripts, they never completes and break away in the mid.\r\n\r\nI am using the following repository to test the Ai benchmarking score:\r\nhttps://pypi.org/project/new-ai-benchmark/\r\n\r\nDo you have further comments to test/share.\r\n\r\nThank you for the document, yes my setup meets the matching installation with Lambda:\r\n_```\r\n<html>\r\n<body>\r\n<!--StartFragment-->\r\ntensorflow-2.17.0 | 3.9-3.12 | Clang 17.0.6 | Bazel 6.5.0 | 8.9 | 12.3\r\n-- | -- | -- | -- | -- | --\r\n\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>\r\n```_\r\n\r\nMy Python version is 3.10.12, which comes by default with Ubuntu 22.04LTS. But the same version works well on other notebook.\r\nthanks \r\nArun', 'created_at': datetime.datetime(2024, 9, 27, 6, 22, 26, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-27 05:46:31 UTC): Hi **@arun-kumark** ,
Apologies for the delay. There seems to be a version mismatch, please check the version compatibility. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference. Could you also please explain the exact steps you followed?
Thank you!

arun-kumark (Issue Creator) on (2024-09-27 06:22:26 UTC): Hi Venkat, 
I am setting up the environment on my freshly installed Ubuntu 22.04LTS using the Lambda:
https://lambdalabs.com/blog/nvidia-ngc-tutorial-run-pytorch-docker-container-using-nvidia-container-toolkit-on-ubuntu

This container set up the CUDA, TensorFlow and CuDNN in reliable way.

I prepared two setups, one with the L4 Nvidia GPU card and another on my ZBook with RTX3000 GPU.

On RTX3000 GPU notebook, the things go smooth, and benchmarking test passes.

While running exactly the same scripts, they never completes and break away in the mid.

I am using the following repository to test the Ai benchmarking score:
https://pypi.org/project/new-ai-benchmark/

Do you have further comments to test/share.

Thank you for the document, yes my setup meets the matching installation with Lambda:
_```
<html>
<body>
<!--StartFragment-->
tensorflow-2.17.0 | 3.9-3.12 | Clang 17.0.6 | Bazel 6.5.0 | 8.9 | 12.3
-- | -- | -- | -- | -- | --


<!--EndFragment-->
</body>
</html>
```_

My Python version is 3.10.12, which comes by default with Ubuntu 22.04LTS. But the same version works well on other notebook.
thanks 
Arun

"
2547096408,issue,closed,completed,Leakage  of personal info to BCI company,,hogomister,2024-09-25 06:51:09+00:00,['tilakrayal'],2024-09-25 06:52:18+00:00,2024-09-25 06:51:39+00:00,https://github.com/tensorflow/tensorflow/issues/76443,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2373180954, 'issue_id': 2547096408, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76443"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76443"">No</a>', 'created_at': datetime.datetime(2024, 9, 25, 6, 51, 40, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-09-25 06:51:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76443"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76443"">No</a>

"
2546752151,issue,closed,completed,"When I conveter a tensorflow model  to TFlite，I found that only some parameters are quantized as int8 , not all parameters，why?","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 22.04
- TensorFlow installation (pip package or built from source): Tensorflow 2.4
-            tensorboard               2.11.2                   pypi_0    pypi
              tensorboard-data-server   0.6.1                    pypi_0    pypi
              tensorboard-plugin-wit    1.8.1                    pypi_0    pypi
               tensorflow                2.4.0                    pypi_0    pypi
               tensorflow-estimator      2.4.0                    pypi_0    pypi
              tensorflow-model-optimization 0.7.3                    pypi_0    pypi
              termcolor                 1.1.0                    pypi_0    pypi
                tflite-runtime            2.7.0                    pypi_0    pypi
- TensorFlow library (version, if pip package or github SHA, if built from source): Tensorflow 2.4

### 2. Code
After conveter to .tflite，then I got the parameters。I found only the Dense layer's weight is int8，the parameter bias and Conv2D parameters are also float32。Accrodong to the conveter instruction， I think  all the parameters are int8 ,is that right? Thank you!
The code is define as follows :
  mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Define the model architecture
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(), #
  tf.keras.layers.Dense(10)  # 
])
model.summary()
# Train the digit classification model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(
                  from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=5,
  validation_data=(test_images, test_labels)
)

## then converter to tflite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model_quant = converter.convert()


",LNCC99,2024-09-25 02:22:38+00:00,['gaikwadrahul8'],2024-11-10 02:03:30+00:00,2024-11-10 02:03:30+00:00,https://github.com/tensorflow/tensorflow/issues/76422,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.4', 'for issues related to TF 2.4')]","[{'comment_id': 2383670741, 'issue_id': 2546752151, 'author': 'gaikwadrahul8', 'body': ""Hi, @LNCC99\r\n\r\nI apologize for the delayed response, I observed the same behavior as far I know it’s typical for certain parameters like biases to remain float32 after quantization and this is a designed behavior in TFLite\r\n\r\nWith `tf.lite.Optimize.DEFAULT`. It generally quantizes weights of layers like `Dense` and `Conv2D` to int8. However, biases are often kept as `float32` to maintain precision especially since they can affect the output significantly.This is a common practice to ensure that the models accuracy is not compromised.\r\n \r\nIf you want to enforce that all weights and biases are quantized to `int8`, you'll have to do something like below please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/c35891f5e9f328cec151bd65e83492a8/test-76422.ipynb) but you'll observe certain amount of drop in the model accuracy\r\n\r\nPlease refer our official documentation for [Model optimization](https://ai.google.dev/edge/litert/models/model_optimization) because we do have different quantization schemes you might want to use for your model.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n# Adding a representative dataset for better quantization\r\ndef representative_dataset():\r\n    for i in range(100):\r\n        yield [train_images[i:i+1].astype(np.float32)]  # Make sure to use float32\r\n\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_types = [tf.int8]  # Ensure int8 quantization\r\n\r\ntflite_model_quant = converter.convert()\r\n```\r\n \r\nI have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 30, 16, 33, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491426, 'issue_id': 2546752151, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398537962, 'issue_id': 2546752151, 'author': 'LNCC99', 'body': ""Hi, @gaikwadrahul8 \r\n      Thank you very much for your answer! Excepting the bias ,the weights of Conv2D are also float32 .I think the weights of Conv2D should be quantized as int8, not float32。Why the weights of Conv2D are also float32?\r\n      The weights and bias dtype is as following picture(the https://colab.sandbox.google.com/gist/gaikwadrahul8/c35891f5e9f328cec151bd65e83492a8/test-76422.ipynb):\r\n      The code print the dtype:\r\n      tensor_details = interpreter.get_tensor_details()\r\n      for tensor in tensor_details:\r\n            print(tensor)\r\n            tensor_name = tensor['name']\r\n            tensor_shape = tensor['shape']\r\n            tensor_type = tensor['dtype']\r\n   ![image](https://github.com/user-attachments/assets/fad16223-7b2f-4b2d-b332-4e9325ff47b1)\r\n\r\n\r\n\r\n    Thank you very much!"", 'created_at': datetime.datetime(2024, 10, 8, 2, 23, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413953406, 'issue_id': 2546752151, 'author': 'gaikwadrahul8', 'body': 'Hi, @LNCC99\r\n\r\nI apologize for the delayed response, just to confirm, are you printing above results with quantized model with my notebook to avoid confusion? if possible could you please try with https://netron.app/ by loading the quantized model and see is it still showing the weights and biases of `Conv2D` layer float32 to cross verify\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 15, 13, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418333908, 'issue_id': 2546752151, 'author': 'LNCC99', 'body': 'Hi，@gaikwadrahul8\r\n         Yes，the printing is with your notbook。The weights of Conv2D are still float32 in netron.app:\r\n         Thank you very much!\r\n![image](https://github.com/user-attachments/assets/6a800634-98c1-4777-a6dd-ffc0890565d9)', 'created_at': datetime.datetime(2024, 10, 17, 1, 59, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432210930, 'issue_id': 2546752151, 'author': 'gaikwadrahul8', 'body': ""Hi, @LNCC99\r\n\r\nI apologize for the delayed response, please refer this updated [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ee363d47c98da3cd98266216e0cb0877/tflite-76422.ipynb) and it seems like things are working as expected now I'm getting below output in netron app so please give it try from your end and let me know things are working as expected or not ?\r\n\r\n![image](https://github.com/user-attachments/assets/fa8071f6-bab5-4705-a6be-062df07ae04a)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 23, 13, 33, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434042967, 'issue_id': 2546752151, 'author': 'LNCC99', 'body': 'Hi,@gaikwadrahul8\r\n     Thank you for your answer。In your ""gist-file"",the quantization is fully-quantized, in this quantization ,the weights and the activate  are all int8,but I mean that using the dynamic range quantization.I want only to quantizate the weights but not contain the activates . In the dynamic range quantization ,why the weights of conv2D are also float32?', 'created_at': datetime.datetime(2024, 10, 24, 1, 40, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437772016, 'issue_id': 2546752151, 'author': 'gaikwadrahul8', 'body': ""Hi, @LNCC99 \r\n\r\nThank you for providing the more context, as far I know not all operations in a model are suitable for quantization and some operations may require high precision or may not benefit from quantization. TensorFlow Lite quantization tool will automatically determine which operations can be quantized and which should remain in floating-point format. \r\n\r\nDynamic range quantization (DRQ) can significantly reduce model size and improve inference speed so it doesn't necessarily convert all operations and layers to `int8` in DRQ\r\n\r\nThe specific operations and layers that are quantized will depend on the model architecture, hardware capabilities and the desired trade-off between model size, inference speed and accuracy.\r\n\r\nYou can choose quantization technique depends upon your use case and which thing you want to focus more like model size, inference speed or accuracy please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) for more information\r\n\r\nThank you."", 'created_at': datetime.datetime(2024, 10, 25, 13, 23, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797106, 'issue_id': 2546752151, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 0, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547335, 'issue_id': 2546752151, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 30, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-30 16:33:27 UTC): Hi, @LNCC99

I apologize for the delayed response, I observed the same behavior as far I know it’s typical for certain parameters like biases to remain float32 after quantization and this is a designed behavior in TFLite

With `tf.lite.Optimize.DEFAULT`. It generally quantizes weights of layers like `Dense` and `Conv2D` to int8. However, biases are often kept as `float32` to maintain precision especially since they can affect the output significantly.This is a common practice to ensure that the models accuracy is not compromised.
 
If you want to enforce that all weights and biases are quantized to `int8`, you'll have to do something like below please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/c35891f5e9f328cec151bd65e83492a8/test-76422.ipynb) but you'll observe certain amount of drop in the model accuracy

Please refer our official documentation for [Model optimization](https://ai.google.dev/edge/litert/models/model_optimization) because we do have different quantization schemes you might want to use for your model.

```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Adding a representative dataset for better quantization
def representative_dataset():
    for i in range(100):
        yield [train_images[i:i+1].astype(np.float32)]  # Make sure to use float32

converter.representative_dataset = representative_dataset
converter.target_spec.supported_types = [tf.int8]  # Ensure int8 quantization

tflite_model_quant = converter.convert()
```
 
I have missed something here please let me know.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-08 02:02:00 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

LNCC99 (Issue Creator) on (2024-10-08 02:23:53 UTC): Hi, @gaikwadrahul8 
      Thank you very much for your answer! Excepting the bias ,the weights of Conv2D are also float32 .I think the weights of Conv2D should be quantized as int8, not float32。Why the weights of Conv2D are also float32?
      The weights and bias dtype is as following picture(the https://colab.sandbox.google.com/gist/gaikwadrahul8/c35891f5e9f328cec151bd65e83492a8/test-76422.ipynb):
      The code print the dtype:
      tensor_details = interpreter.get_tensor_details()
      for tensor in tensor_details:
            print(tensor)
            tensor_name = tensor['name']
            tensor_shape = tensor['shape']
            tensor_type = tensor['dtype']
   ![image](https://github.com/user-attachments/assets/fad16223-7b2f-4b2d-b332-4e9325ff47b1)



    Thank you very much!

gaikwadrahul8 (Assginee) on (2024-10-15 13:41:00 UTC): Hi, @LNCC99

I apologize for the delayed response, just to confirm, are you printing above results with quantized model with my notebook to avoid confusion? if possible could you please try with https://netron.app/ by loading the quantized model and see is it still showing the weights and biases of `Conv2D` layer float32 to cross verify

Thank you for your cooperation and patience.

LNCC99 (Issue Creator) on (2024-10-17 01:59:02 UTC): Hi，@gaikwadrahul8
         Yes，the printing is with your notbook。The weights of Conv2D are still float32 in netron.app:
         Thank you very much!
![image](https://github.com/user-attachments/assets/6a800634-98c1-4777-a6dd-ffc0890565d9)

gaikwadrahul8 (Assginee) on (2024-10-23 13:33:55 UTC): Hi, @LNCC99

I apologize for the delayed response, please refer this updated [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ee363d47c98da3cd98266216e0cb0877/tflite-76422.ipynb) and it seems like things are working as expected now I'm getting below output in netron app so please give it try from your end and let me know things are working as expected or not ?

![image](https://github.com/user-attachments/assets/fa8071f6-bab5-4705-a6be-062df07ae04a)

Thank you for your cooperation and patience.

LNCC99 (Issue Creator) on (2024-10-24 01:40:48 UTC): Hi,@gaikwadrahul8
     Thank you for your answer。In your ""gist-file"",the quantization is fully-quantized, in this quantization ,the weights and the activate  are all int8,but I mean that using the dynamic range quantization.I want only to quantizate the weights but not contain the activates . In the dynamic range quantization ,why the weights of conv2D are also float32?

gaikwadrahul8 (Assginee) on (2024-10-25 13:23:45 UTC): Hi, @LNCC99 

Thank you for providing the more context, as far I know not all operations in a model are suitable for quantization and some operations may require high precision or may not benefit from quantization. TensorFlow Lite quantization tool will automatically determine which operations can be quantized and which should remain in floating-point format. 

Dynamic range quantization (DRQ) can significantly reduce model size and improve inference speed so it doesn't necessarily convert all operations and layers to `int8` in DRQ

The specific operations and layers that are quantized will depend on the model architecture, hardware capabilities and the desired trade-off between model size, inference speed and accuracy.

You can choose quantization technique depends upon your use case and which thing you want to focus more like model size, inference speed or accuracy please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) for more information

Thank you.

github-actions[bot] on (2024-11-02 02:00:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-10 02:03:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2546187535,issue,closed,completed,Documentation request: Proposed release schedule,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.17

### Custom code

No

### Current behavior?

I'm near a major release on a small project that uses TensorFlow, and I know that TF 2.18 will switch to Numpy 2.0 (according to the 2.17 release notes) but it's not clear when that will be. If 2.18 will be out in a couple weeks, I'll hold off on my update since Numpy 2.0 could introduce breaking changes. But if 2.18 is going to come out in mid-2025, then I can go ahead and release. 

It would be nice to have an estimated release timeline in the documentation, perhaps in RELEASE.md, or (even better) a description of how release timing is decided like PEP 602. Of course, this needn't be a binding schedule set in stone, but it would give me a sense of when the latest goodness from the nightly branch will make its way to a stable branch (especially the end of the extremely-enthusiastic ""Skipping the delay kernel"" messages!).

Cheers,
Charles.
",mmtrebuchet,2024-09-24 19:19:49+00:00,['tilakrayal'],2025-01-11 02:01:47+00:00,2025-01-11 02:01:44+00:00,https://github.com/tensorflow/tensorflow/issues/76407,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]","[{'comment_id': 2563687902, 'issue_id': 2546187535, 'author': 'tilakrayal', 'body': '@mmtrebuchet,\r\nThe latest TensorFlow v2.18 now supports and is compiled with NumPy 2.0 by default. Please take a look at the official release document for reference.\r\nhttps://github.com/tensorflow/tensorflow/releases\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 27, 13, 9, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2570002028, 'issue_id': 2546187535, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 4, 2, 0, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000686, 'issue_id': 2546187535, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000730, 'issue_id': 2546187535, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76407"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76407"">No</a>', 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 46, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-27 13:09:51 UTC): @mmtrebuchet,
The latest TensorFlow v2.18 now supports and is compiled with NumPy 2.0 by default. Please take a look at the official release document for reference.
https://github.com/tensorflow/tensorflow/releases

Thank you!

github-actions[bot] on (2025-01-04 02:00:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-11 02:01:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-11 02:01:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76407"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76407"">No</a>

"
2544960498,issue,open,,Build Failure with ml_dtypes 0.4.0 on Power Architecture,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

linux/ppc64le

### Mobile device

_No response_

### Python version

3.9, 3.10, 3.11,3.12

### Bazel version

6.5.2

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When attempting to build TensorFlow on the Power architecture with ml-dtypes==0.4.0, the build fails due to an incompatibility with numpy 2.0.0rc1. On power, there is no wheel for ml_dtypes on pypi. Hence, Tensorflow build tries to build ml_dtypes from its source, which fails due to build time depencency on numpy 2.0.0rc1. https://github.com/jax-ml/ml_dtypes/blob/v0.4.0/pyproject.toml#L52.  Numpy 2.0.0rc1 is not available for any architetcure on pypi.
We had rasied this concern with ml_dtypes and they have fixed it in ml_dtypes 0.4.1 version.

To resolve this issue, we recommend updating the following files:

1. tensorflow/tools/pip_package/setup.py

Update the ml_dtypes version requirement from 0.4.0 to 0.4.1.
Current section: ml_dtypes >= 0.4.0, < 0.5.0

2. ci/official/requirements_updater/requirements.in

Update the ml_dtypes version requirement from 0.4.0 to 0.4.1.
Current section: ml_dtypes >= 0.4.0, < 0.5.0

After these changes, it should reflect in the requirements_lock.txt for different Python versions.

### Standalone code to reproduce the issue

```shell
Architecture: ppc64le (Power)
Python Version: 3.x
TensorFlow Version: master
ml_dtypes Version: 0.4.0
pip Version: 24.2

Steps to reproduce:
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu

The error related to numpy==2.0.0rc1 will occur during the installation.
```


### Relevant log output

```shell
ERROR: /tensorflow/WORKSPACE:53:13: fetching whl_library rule //external:pypi_ml_dtypes: Traceback (most recent call last):
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/pypi/whl_library.bzl"", line 294, column 35, in _whl_library_impl
                repo_utils.execute_checked(
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/repo_utils.bzl"", line 182, column 29, in _execute_checked
                return _execute_internal(fail_on_error = True, *args, **kwargs)
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/repo_utils.bzl"", line 123, column 13, in _execute_internal
                fail((
Error in fail: repo.execute: whl_library.ResolveRequirement(pypi_ml_dtypes, ml-dtypes==0.4.0): end: failure:
  command: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/python_ppc64le-unknown-linux-gnu/bin/python3 -m python.private.pypi.whl_installer.wheel_installer --requirement ml-dtypes==0.4.0 --isolated --extra_pip_args ""{\""arg\"":[]}"" --pip_data_exclude ""{\""arg\"":[]}"" --environment ""{\""arg\"":{}}""
  return code: 1
  working dir: <default: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi_ml_dtypes>
  timeout: 600
  environment:
PYTHONPATH=""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__build:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__click:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__colorama:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__importlib_metadata:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__installer:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__more_itertools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__packaging:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pep517:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pip:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pip_tools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pyproject_hooks:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__setuptools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__tomli:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__wheel:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__zipp""
CPPFLAGS=""""
===== stdout start =====
Collecting ml-dtypes==0.4.0 (from -r /tmp/tmpae6v73x5 (line 1))
  Using cached ml_dtypes-0.4.0.tar.gz (692 kB)
  Installing build dependencies: started
  Installing build dependencies: finished with status 'error'
===== stdout end =====
===== stderr start =====
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [3 lines of output]
      ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11
      ERROR: Could not find a version that satisfies the requirement numpy==2.0.0rc1 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1)
      ERROR: No matching distribution found for numpy==2.0.0rc1
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error
```
",sandeepgupta12,2024-09-24 10:08:57+00:00,['Venkat6871'],2024-10-08 05:07:48+00:00,,https://github.com/tensorflow/tensorflow/issues/76379,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues')]","[{'comment_id': 2378427098, 'issue_id': 2544960498, 'author': 'Venkat6871', 'body': 'Hi **@sandeepgupta12** ,\r\nApologies for the delay. You are using the master branch, so could you please confirm which version you are using TensorFlow 2.17.0 or 2.18.0?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 5, 26, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382609405, 'issue_id': 2544960498, 'author': 'sandeepgupta12', 'body': 'Hi @Venkat6871, thanks for reply. We are working on top of the tree (master head commit). It is better to get this fixed in any upcoming new release, may it be 2.18 or 2.19', 'created_at': datetime.datetime(2024, 9, 30, 9, 30, 34, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-27 05:26:36 UTC): Hi **@sandeepgupta12** ,
Apologies for the delay. You are using the master branch, so could you please confirm which version you are using TensorFlow 2.17.0 or 2.18.0?
Thank you!

sandeepgupta12 (Issue Creator) on (2024-09-30 09:30:34 UTC): Hi @Venkat6871, thanks for reply. We are working on top of the tree (master head commit). It is better to get this fixed in any upcoming new release, may it be 2.18 or 2.19

"
2544696386,issue,closed,completed,"gen_nn_ops.fractional_max_pool_grad aborts with ""malloc_consolidate(): invalid chunk size""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `crash issue` in TensorFlow when I used API `gen_nn_ops.fractional_max_pool_grad` . 
I have confirmed that above the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build). Please find the [gist](https://colab.research.google.com/drive/16XRisnbPuTfwdeRHkdZsJIEaDs9RHYGs?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.ops import nn_ops
import tensorflow as tf

_PRNG = np.random.RandomState(341261)
num_batches =1
row_window_size=2
col_window_size=2
num_rows = (row_window_size - 1) * 5 + 1
num_cols = (col_window_size - 1) * 7 + 1
input_shape = (num_batches, num_rows, num_cols, 1)
sess = tf.compat.v1.Session()
with sess.as_default():
    x = np.arange(48, dtype=np.float32)
    _PRNG.shuffle(x)
    input_tensor =  x.reshape(input_shape)
    window_size = [1, row_window_size, col_window_size, 1]
    stride_size = [1, 65536, col_window_size - 1, 1] 
    padding = 'VALID'
    output_tensor = nn_ops.max_pool(input_tensor, window_size, stride_size, padding)
    output_backprop = _PRNG.randint(100, size=output_tensor.shape)
    row_seq = list(range(0, num_rows, row_window_size - 1))
    col_seq = list(range(0, num_cols, col_window_size - 1))
    row_seq[-1] += 1
    col_seq[-1] += 1
    gen_nn_ops.fractional_max_pool_grad(input_tensor, output_tensor, output_backprop, row_seq, col_seq, overlapping=True)
```


### Relevant log output

```shell
malloc_consolidate(): invalid chunk size
Aborted (core dumped)
```
",cybersupersoap,2024-09-24 08:17:49+00:00,['tilakrayal'],2024-10-11 02:01:13+00:00,2024-10-11 02:01:10+00:00,https://github.com/tensorflow/tensorflow/issues/76362,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2374009182, 'issue_id': 2544696386, 'author': 'tilakrayal', 'body': '@cybersupersoap,\r\nThank you for reporting the issue. I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/59394) and https://github.com/tensorflow/tensorflow/issues/59405 where a similar issue has been proposed and it is still open.Also I request to follow the similar issue which has been proposed to have the updates on the same. Thank you!', 'created_at': datetime.datetime(2024, 9, 25, 12, 55, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390349587, 'issue_id': 2544696386, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406393778, 'issue_id': 2544696386, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 11, 2, 1, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406393809, 'issue_id': 2544696386, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76362"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76362"">No</a>', 'created_at': datetime.datetime(2024, 10, 11, 2, 1, 11, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-25 12:55:25 UTC): @cybersupersoap,
Thank you for reporting the issue. I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/59394) and https://github.com/tensorflow/tensorflow/issues/59405 where a similar issue has been proposed and it is still open.Also I request to follow the similar issue which has been proposed to have the updates on the same. Thank you!

github-actions[bot] on (2024-10-03 02:01:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-11 02:01:09 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-11 02:01:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76362"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76362"">No</a>

"
2544354722,issue,closed,completed,AttributeError: module 'tensorflow' has no attribute 'estimator',"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

MacOS

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

AttributeError: module 'tensorflow' has no attribute 'estimator'

### Standalone code to reproduce the issue

```shell
Cell In[15], line 2
      1 # Build the model
----> 2 model = tf.estimator.DNNClassifier(
      3     feature_columns=feature_columns,
      4     hidden_units=[64, 32],
      5     n_classes=2,
      6     model_dir='./model_dir')
      8 # Train the model
      9 model.train(input_fn=train_input_fn, steps=5000)

AttributeError: module 'tensorflow' has no attribute 'estimator'
```


### Relevant log output

_No response_",mkuangdotcom,2024-09-24 05:25:12+00:00,['Venkat6871'],2024-09-26 03:11:41+00:00,2024-09-26 03:11:38+00:00,https://github.com/tensorflow/tensorflow/issues/76349,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2373347273, 'issue_id': 2544354722, 'author': 'Venkat6871', 'body': 'Hi **@mkuangdotcom** ,\r\nThank you for raising the issue here. You are using a deprecated API, which is clearly mentioned in the documentation. Estimators will not be available in TensorFlow 2.16 or later. I am providing the [documentation](https://www.tensorflow.org/guide/estimator) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 25, 8, 1, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375762955, 'issue_id': 2544354722, 'author': 'mkuangdotcom', 'body': 'Thanks for the info!', 'created_at': datetime.datetime(2024, 9, 26, 3, 11, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375763039, 'issue_id': 2544354722, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76349"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76349"">No</a>', 'created_at': datetime.datetime(2024, 9, 26, 3, 11, 40, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-25 08:01:16 UTC): Hi **@mkuangdotcom** ,
Thank you for raising the issue here. You are using a deprecated API, which is clearly mentioned in the documentation. Estimators will not be available in TensorFlow 2.16 or later. I am providing the [documentation](https://www.tensorflow.org/guide/estimator) for your reference.
Thank you!

mkuangdotcom (Issue Creator) on (2024-09-26 03:11:38 UTC): Thanks for the info!

google-ml-butler[bot] on (2024-09-26 03:11:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76349"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76349"">No</a>

"
2542425675,issue,closed,completed,"Using the c++ API,  how do  set  fp16 on  cpu？","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.30

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi 
      using cpu backend,  we want to set fp16 to Invoke, 
using below code,  What setting parameters do I need to add 

thansk


### Standalone code to reproduce the issue

```shell
#include <memory>
#include <string>
#include <vector>

#include ""tensorflow/lite/c/c_api.h""
#include ""tensorflow/lite/c/c_api_experimental.h""
#include ""tensorflow/lite/core/c/common.h""


#define STB_IMAGE_IMPLEMENTATION
#include ""stb_image.h""

#define STB_IMAGE_WRITE_IMPLEMENTATION
#include ""stb_image_write.h""

int main(void)
{
uint8_t *image = stbi_load(""192.jpg"", 192, 192, 3, 3);
int tmpSize = (192* 192*3);
float *in = (float *)malloc(tmpSize**sizeof(float)); 
float *out = (float *)malloc(tmpSize**sizeof(float)); 
for (int i = 0; i < tmpSize; i++)
{
    in[i]   = (float)(image[i] - 0.0f) / 255.0f;
}
//init model
TfLiteModel model = TfLiteModelCreateFromFile(info->ld.model_name);
TfLiteInterpreterOptions options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreterOptionsSetNumThreads(options, info->param.number_of_threads);
//
TfLiteInterpreter interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);

//invoke
TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
TfLiteTensorCopyFromBuffer(input_tensor, (void *)in, tmpSize*sizeof(float));
TfLiteInterpreterInvoke(interpreter);

//get output
TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 0);
TfLiteTensorCopyToBuffer(output_tensor, (void *)Out, tmpSize*sizeof(float) );

//deinit
TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);

}
```


### Relevant log output

_No response_",keke444,2024-09-23 12:03:26+00:00,['gaikwadrahul8'],2024-10-16 02:02:54+00:00,2024-10-16 02:02:44+00:00,https://github.com/tensorflow/tensorflow/issues/76315,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2380357013, 'issue_id': 2542425675, 'author': 'ravinm123', 'body': ""model = model.cast('float16')"", 'created_at': datetime.datetime(2024, 9, 28, 2, 16, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380357097, 'issue_id': 2542425675, 'author': 'ravinm123', 'body': ""# Enable mixed precision in TensorFlow for GPU\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)"", 'created_at': datetime.datetime(2024, 9, 28, 2, 16, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386006595, 'issue_id': 2542425675, 'author': 'gaikwadrahul8', 'body': ""Hi, @keke444 \r\n\r\nI apologize for the delayed response, I think you can use this [SetAllowFp16PrecisionForFp32](https://ai.google.dev/edge/api/tflite/cc/class/tflite/impl/interpreter)\r\n\r\n```\r\nvoid SetAllowFp16PrecisionForFp32(\r\n  bool allow\r\n)\r\n```\r\nAllow `float16` precision for `FP32` calculation when possible.\r\n\r\nDefault: not allow.\r\n\r\nWARNING: This API is deprecated: prefer controlling this via delegate options, e.g. `tflite::StatefulNnApiDelegate::Options::allow_fp16' or TfLiteGpuDelegateOptionsV2::is_precision_loss_allowed. This method will be removed in a future release.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 1, 13, 51, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123235, 'issue_id': 2542425675, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573536, 'issue_id': 2542425675, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573729, 'issue_id': 2542425675, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76315"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76315"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 54, tzinfo=datetime.timezone.utc)}]","ravinm123 on (2024-09-28 02:16:25 UTC): model = model.cast('float16')

ravinm123 on (2024-09-28 02:16:42 UTC): # Enable mixed precision in TensorFlow for GPU
from tensorflow.keras.mixed_precision import experimental as mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)

gaikwadrahul8 (Assginee) on (2024-10-01 13:51:16 UTC): Hi, @keke444 

I apologize for the delayed response, I think you can use this [SetAllowFp16PrecisionForFp32](https://ai.google.dev/edge/api/tflite/cc/class/tflite/impl/interpreter)

```
void SetAllowFp16PrecisionForFp32(
  bool allow
)
```
Allow `float16` precision for `FP32` calculation when possible.

Default: not allow.

WARNING: This API is deprecated: prefer controlling this via delegate options, e.g. `tflite::StatefulNnApiDelegate::Options::allow_fp16' or TfLiteGpuDelegateOptionsV2::is_precision_loss_allowed. This method will be removed in a future release.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-09 02:01:08 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:54 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76315"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76315"">No</a>

"
2542299878,issue,open,,Build Failure on AWS Graviton3 with Custom oneDNN (oneDNN-3.6-rc): Invalid Preprocessing Directives in dnnl_config.h,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

6.5.0

### GCC/compiler version

gcc version 11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am unable to build TensorFlow with the latest oneDNN or custom oneDNN (oneDNN-3.6-rc) on AWS Graviton3 (aarch64) CPU. The build process fails with several compilation errors related to invalid preprocessing directives in the dnnl_config.h file.

I expected the build to complete successfully with the custom oneDNN settings, allowing TensorFlow to run efficiently on the AWS Graviton3 (aarch64) architecture.

### Standalone code to reproduce the issue

```shell
Clone the TensorFlow repository:

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.17.0

Modify the relevant files as follows:

Update oneDNN version in tensorflow/workspace2.bzl.
Adjust mkldnn_acl.BUILD for versioning.

root@8c5bdc6a1bd7:/workdir/tensorflow# git diff
diff --git a/tensorflow/workspace2.bzl b/tensorflow/workspace2.bzl
index fd29dff05f3..7ed30157970 100644
--- a/tensorflow/workspace2.bzl
+++ b/tensorflow/workspace2.bzl
@@ -205,36 +205,24 @@ def _tf_repositories():
     tf_http_archive(
         name = ""onednn"",
         build_file = ""//third_party/mkl_dnn:mkldnn_v1.BUILD"",
-        sha256 = ""5131ac559a13daa6e2784d20ab24e4607e55aa6da973518086326a647d389425"",
-        strip_prefix = ""oneDNN-3.4.2"",
-        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.4.2.tar.gz""),
+       sha256 = ""568428621a4912dd2159eaee97f646259c655acc271dc57bd75478daa9672ea5"",
+       strip_prefix = ""oneDNN-3.6-rc"",
+       urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.6-rc.tar.gz""),
     )
 
     tf_http_archive(
         name = ""mkl_dnn_acl_compatible"",
         build_file = ""//third_party/mkl_dnn:mkldnn_acl.BUILD"",
-        patch_file = [
-            ""//third_party/mkl_dnn:onednn_acl_threadcap.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_reorder.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_thread_local_scheduler.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_fp32_bf16_reorder.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_bf16_capability_detection_for_ubuntu20.04.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_indirect_conv.patch"",
-        ],
-        sha256 = ""2f76b407ef8893cca71340f88cd800019a1f14f8ac1bbdbb89a84be1370b52e3"",
-        strip_prefix = ""oneDNN-3.2.1"",
-        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.2.1.tar.gz""),
+       sha256 = ""568428621a4912dd2159eaee97f646259c655acc271dc57bd75478daa9672ea5"",
+        strip_prefix = ""oneDNN-3.6-rc"",
+        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.6-rc.tar.gz""),
     )
 
     tf_http_archive(
         name = ""compute_library"",
-        patch_file = [
-            ""//third_party/compute_library:compute_library.patch"",
-            ""//third_party/compute_library:acl_thread_local_scheduler.patch"",
-        ],
-        sha256 = ""c4ca329a78da380163b2d86e91ba728349b6f0ee97d66e260a694ef37f0b0d93"",
-        strip_prefix = ""ComputeLibrary-23.05.1"",
-        urls = tf_mirror_urls(""https://github.com/ARM-software/ComputeLibrary/archive/v23.05.1.tar.gz""),
+        sha256 = ""e7e1b554129748c3aadf1a85de48d332afbef7c6c0c3c5be77a1cfb58311c57b"",
+        strip_prefix = ""ComputeLibrary-24.08.1"",
+        urls = tf_mirror_urls(""https://github.com/ARM-software/ComputeLibrary/archive/refs/tags/v24.08.1.tar.gz"")
     )
 
     tf_http_archive(
diff --git a/third_party/mkl_dnn/mkldnn_acl.BUILD b/third_party/mkl_dnn/mkldnn_acl.BUILD
index d67b62a98d2..083b3d7a627 100644
--- a/third_party/mkl_dnn/mkldnn_acl.BUILD
+++ b/third_party/mkl_dnn/mkldnn_acl.BUILD
@@ -128,8 +128,8 @@ expand_template(
     out = ""include/oneapi/dnnl/dnnl_version.h"",
     substitutions = {
         ""@DNNL_VERSION_MAJOR@"": ""3"",
-        ""@DNNL_VERSION_MINOR@"": ""2"",
-        ""@DNNL_VERSION_PATCH@"": ""1"",
+        ""@DNNL_VERSION_MINOR@"": ""6"",
+        ""@DNNL_VERSION_PATCH@"": ""0"",
         ""@DNNL_VERSION_HASH@"": ""N/A"",
     },
     template = ""include/oneapi/dnnl/dnnl_version.h.in"",
(END)


Attempt to build TensorFlow:

taskset -c 16-32 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=mkl_aarch64_threadpool --jobs=33 --local_cpu_resources=16 --verbose_failures -s
```


### Relevant log output

```shell
ERROR: /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/external/mkl_dnn_acl_compatible/BUILD.bazel:138:11: Compiling src/common/batch_normalization.cpp failed: (Exit 1): gcc failed: error executing command (from target @mkl_dnn_acl_compatible//:mkl_dnn_acl) 
  (cd /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.o' -fPIC -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=""mkl_dnn_acl_compatible""' -iquote external/mkl_dnn_acl_compatible -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible -iquote external/compute_library -iquote bazel-out/aarch64-opt/bin/external/compute_library -Ibazel-out/aarch64-opt/bin/external/compute_library/include/_virtual_includes/include -isystem external/mkl_dnn_acl_compatible/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include -isystem external/mkl_dnn_acl_compatible/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src -isystem external/mkl_dnn_acl_compatible/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/common -isystem external/mkl_dnn_acl_compatible/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem external/compute_library/arm_compute/runtime -isystem bazel-out/aarch64-opt/bin/external/compute_library/arm_compute/runtime -isystem external/compute_library/src/core/NEON/kernels/arm_gemm -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm -isystem external/compute_library/src/core/NEON/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/convolution/common -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common -isystem external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem external/compute_library/src/core/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/cpu/kernels/assembly -isystem external/compute_library/src/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/cpu/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/arm_conv -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++17' -fopenmp-simd -fexceptions -UUSE_MKL -UUSE_CBLAS -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/mkl_dnn_acl_compatible/src/common/batch_normalization.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.o)
# Configuration: 286713d3e237c869e8689debb2d6b060b16fc87de4d5e6ded144ba62ae251131
# Execution platform: @local_execution_config_platform//:platform
In file included from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_common_types.h:31,
                 from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_common.h:23,
                 from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl.h:23,
                 from external/mkl_dnn_acl_compatible/src/common/batch_normalization.cpp:18:
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:112:2: error: invalid preprocessing directive #cmakedefine
  112 | #cmakedefine DNNL_GPU_VENDOR DNNL_VENDOR_${DNNL_GPU_VENDOR}
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:158:2: error: invalid preprocessing directive #cmakedefine
  158 | #cmakedefine DNNL_SYCL_GENERIC
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:181:2: error: invalid preprocessing directive #cmakedefine
  181 | #cmakedefine DNNL_DISABLE_GPU_REF_KERNELS
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:195:2: error: invalid preprocessing directive #cmakedefine01
  195 | #cmakedefine01 BUILD_GROUP_NORMALIZATION
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:206:2: error: invalid preprocessing directive #cmakedefine01
  206 | #cmakedefine01 BUILD_SDPA
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:224:2: error: invalid preprocessing directive #cmakedefine01
  224 | #cmakedefine01 BUILD_XE2
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:226:2: error: invalid preprocessing directive #cmakedefine01
  226 | #cmakedefine01 BUILD_GEMM_KERNELS_ALL
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:227:2: error: invalid preprocessing directive #cmakedefine01
  227 | #cmakedefine01 BUILD_GEMM_KERNELS_NONE
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:228:2: error: invalid preprocessing directive #cmakedefine01
  228 | #cmakedefine01 BUILD_GEMM_SSE41
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:229:2: error: invalid preprocessing directive #cmakedefine01
  229 | #cmakedefine01 BUILD_GEMM_AVX2
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:230:2: error: invalid preprocessing directive #cmakedefine01
  230 | #cmakedefine01 BUILD_GEMM_AVX512
      |  ^~~~~~~~~~~~~
SUBCOMMAND: # @boringssl//:crypto [action 'Compiling src/crypto/pem/pem_lib.c [for tool]', configuration: 6c76bd453e22b21125a2028c36fb69b9de59167ea2a1dca88d8da721e8db0553, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.d '-frandom-seed=bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.o' -fPIC '-DBAZEL_CURRENT_REPOSITORY=""boringssl""' -iquote external/boringssl -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/src/include -g0 -w -DBORINGSSL_IMPLEMENTATION -Wa,--noexecstack -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common '-D_XOPEN_SOURCE=700' '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/boringssl/src/crypto/pem/pem_lib.c -o bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.o)
# Configuration: 6c76bd453e22b21125a2028c36fb69b9de59167ea2a1dca88d8da721e8db0553
# Execution platform: @local_execution_config_platform//:platform
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 165.542s, Critical Path: 28.45s
INFO: 5206 processes: 1083 internal, 4123 local.
FAILED: Build did NOT complete successfully
```
",deepeshfujitsu,2024-09-23 11:04:35+00:00,['Venkat6871'],2024-10-14 18:15:47+00:00,,https://github.com/tensorflow/tensorflow/issues/76311,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:mkl', 'MKL related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2372898568, 'issue_id': 2542299878, 'author': 'Venkat6871', 'body': '**@learning-to-play**', 'created_at': datetime.datetime(2024, 9, 25, 4, 34, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411942937, 'issue_id': 2542299878, 'author': 'Rohanjames1997', 'body': '@deepeshfujitsu \r\nThe latest supported version of oneDNN is 3.5 (#70822) \r\nWe have not looked into building TF with oneDNN 3.6 yet. Will provide a solution if/when we find one.', 'created_at': datetime.datetime(2024, 10, 14, 18, 15, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-25 04:34:31 UTC): **@learning-to-play**

Rohanjames1997 on (2024-10-14 18:15:46 UTC): @deepeshfujitsu 
The latest supported version of oneDNN is 3.5 (#70822) 
We have not looked into building TF with oneDNN 3.6 yet. Will provide a solution if/when we find one.

"
2542185201,issue,closed,completed, ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).,"code Link: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb#scrollTo=2l7zFL_XWIRu&uniqifier=1

code snipet: 
```
first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())
```


give error for this line 

`print('Normalized:', normalizer(first).numpy())`


First example: [[4 90.0 75.0 2125.0 14.5 74 False False True]]

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-20-0887d6979ce9>](https://localhost:8080/#) in <cell line: 3>()
      4   print('First example:', first)
      5   print()
----> 6   print('Normalized:', normalizer(first).numpy())

2 frames
[/usr/local/lib/python3.10/dist-packages/optree/ops.py](https://localhost:8080/#) in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    745     leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    746     flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> 747     return treespec.unflatten(map(func, *flat_args))
    748 
    749 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).",mn-48,2024-09-23 10:18:30+00:00,['tilakrayal'],2025-01-09 12:53:20+00:00,2024-09-24 08:11:42+00:00,https://github.com/tensorflow/tensorflow/issues/76309,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:model', 'Model related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2370273401, 'issue_id': 2542185201, 'author': 'tilakrayal', 'body': '@mn-48,\r\nThank you for reporting the issue. I tried to execute the mentioned official code with the alternative approach and it was executed without any issue/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/901bcfc7fce21fefec50d16e9d8cb746/regression.ipynb).\r\n\r\nI will request the PR in the tensorflow/docs for the similar changes. Thank you!', 'created_at': datetime.datetime(2024, 9, 24, 6, 9, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370371280, 'issue_id': 2542185201, 'author': 'mn-48', 'body': 'Thank you.\r\n\r\nOn Tue, Sep 24, 2024 at 12:09\u202fPM tilakrayal ***@***.***>\r\nwrote:\r\n\r\n> @mn-48 <https://github.com/mn-48>,\r\n> Thank you for reporting the issue. I tried to execute the mentioned\r\n> official code with the alternative approach and it was executed without any\r\n> issue/error. Kindly find the gist of it here\r\n> <https://colab.research.google.com/gist/tilakrayal/901bcfc7fce21fefec50d16e9d8cb746/regression.ipynb>\r\n> .\r\n>\r\n> I will request the PR in the tensorflow/docs for the similar changes.\r\n> Thank you!\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/76309#issuecomment-2370273401>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHLJUQ7PBLQI7RWXG7MUEHLZYD6ZVAVCNFSM6AAAAABOVXTBFCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZQGI3TGNBQGE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\r\n\r\n-- \r\n\r\nRegards,\r\nMd. Nazmul Hossain\r\nEmail: ***@***.***\r\nContact: +8801761777748', 'created_at': datetime.datetime(2024, 9, 24, 7, 8, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370433266, 'issue_id': 2542185201, 'author': 'tilakrayal', 'body': '@mn-48,\r\nIf the issue got resolved, Could you please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 9, 24, 7, 38, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370573551, 'issue_id': 2542185201, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 8, 11, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370712265, 'issue_id': 2542185201, 'author': 'mn-48', 'body': 'Yes\r\n\r\nOn Tue, 24 Sept 2024, 2:12 pm google-ml-butler[bot], <\r\n***@***.***> wrote:\r\n\r\n> Are you satisfied with the resolution of your issue?\r\n> Yes\r\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309>\r\n> No\r\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309>\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/76309#issuecomment-2370573551>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHLJUQ6S3P2AH7VIIDO77J3ZYENFTAVCNFSM6AAAAABOVXTBFCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZQGU3TGNJVGE>\r\n> .\r\n> You are receiving this because you modified the open/close state.Message\r\n> ID: ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 9, 24, 9, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580083505, 'issue_id': 2542185201, 'author': 'LadaF', 'body': 'I am still getting this error from the sheet from https://www.tensorflow.org/tutorials/keras/regression', 'created_at': datetime.datetime(2025, 1, 9, 12, 53, 18, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-24 06:09:06 UTC): @mn-48,
Thank you for reporting the issue. I tried to execute the mentioned official code with the alternative approach and it was executed without any issue/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/901bcfc7fce21fefec50d16e9d8cb746/regression.ipynb).

I will request the PR in the tensorflow/docs for the similar changes. Thank you!

mn-48 (Issue Creator) on (2024-09-24 07:08:30 UTC): Thank you.

On Tue, Sep 24, 2024 at 12:09 PM tilakrayal ***@***.***>
wrote:



-- 

Regards,
Md. Nazmul Hossain
Email: ***@***.***
Contact: +8801761777748

tilakrayal (Assginee) on (2024-09-24 07:38:09 UTC): @mn-48,
If the issue got resolved, Could you please feel free to move this issue to closed status. Thank you!

google-ml-butler[bot] on (2024-09-24 08:11:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76309"">No</a>

mn-48 (Issue Creator) on (2024-09-24 09:10:57 UTC): Yes

On Tue, 24 Sept 2024, 2:12 pm google-ml-butler[bot], <
***@***.***> wrote:

LadaF on (2025-01-09 12:53:18 UTC): I am still getting this error from the sheet from https://www.tensorflow.org/tutorials/keras/regression

"
2541314774,issue,closed,completed,TensorFlow Lite in Play Services issue,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",TAmulm,2024-09-22 23:53:19+00:00,['Venkat6871'],2024-10-10 02:01:24+00:00,2024-10-10 02:01:23+00:00,https://github.com/tensorflow/tensorflow/issues/76282,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2367024184, 'issue_id': 2541314774, 'author': 'TAmulm', 'body': 'تمام', 'created_at': datetime.datetime(2024, 9, 22, 23, 54, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367024326, 'issue_id': 2541314774, 'author': 'TAmulm', 'body': 'ما الذي تقدمه', 'created_at': datetime.datetime(2024, 9, 22, 23, 54, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370593679, 'issue_id': 2541314774, 'author': 'Venkat6871', 'body': 'Hi **@TAmulm** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 24, 8, 21, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387488504, 'issue_id': 2541314774, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 2, 2, 1, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748500, 'issue_id': 2541314774, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 23, tzinfo=datetime.timezone.utc)}]","TAmulm (Issue Creator) on (2024-09-22 23:54:05 UTC): تمام

TAmulm (Issue Creator) on (2024-09-22 23:54:33 UTC): ما الذي تقدمه

Venkat6871 (Assginee) on (2024-09-24 08:21:16 UTC): Hi **@TAmulm** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

github-actions[bot] on (2024-10-02 02:01:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-10 02:01:23 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2541225445,issue,closed,completed,Tensorflow terminates during prediction with no message,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

During prediction the application just stop in the 
![image](https://github.com/user-attachments/assets/334f6315-08b0-435d-a671-12de00f40e15)

I make many predictions during the application run and it is not the same place it stops

How can I add more trace/logging to see, what is causing the error, and get some output to report to the developers


### Standalone code to reproduce the issue

```shell
I currently do not have a standalone reproduceable test, as it seems to be ralated to a specific computer, as the same application works on other PC's
```


### Relevant log output

```shell
Using keras 3.4.1
```
",ThorvaldAagaard,2024-09-22 19:32:15+00:00,['tilakrayal'],2024-10-16 13:49:53+00:00,2024-10-16 13:49:49+00:00,https://github.com/tensorflow/tensorflow/issues/76279,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2368235549, 'issue_id': 2541225445, 'author': 'tilakrayal', 'body': '@ThorvaldAagaard,\r\nCan you please share a reproducible code/colab gist that supports your statement which helps to debug the issue in an effective way and provide the update? Thank you!', 'created_at': datetime.datetime(2024, 9, 23, 13, 23, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368427809, 'issue_id': 2541225445, 'author': 'ThorvaldAagaard', 'body': 'I have not been able to reproduce it on other machines, so it could be a hardware problem, but even as a hardware problem I am interested in (like you) to debug the keras/tensorflow code to find the statement, when the application terminates.\r\n\r\nMy problem is that when I call predict with verbose=1 I just get a progress indicator, that stops and the application exit without any type of message.\r\n\r\nI would expect the code to have some error handling, that should catch even hardware errors.\r\n\r\nEven on the same machine it is not the same predict, that fails, and currently the application has been running for more that 1,5 hours without any problem, so I am really mystified.\r\n\r\nCan I active some kind of trace of the code?', 'created_at': datetime.datetime(2024, 9, 23, 14, 20, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385049061, 'issue_id': 2541225445, 'author': 'tilakrayal', 'body': '@ThorvaldAagaard,\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 7, 54, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385246409, 'issue_id': 2541225445, 'author': 'ThorvaldAagaard', 'body': 'I assume it is a hardware problem and currently the machine is being at service.\r\n\r\nI do not expect you to be able to solve the problem, but as I just call predict and see the progress bar stopping in the  midle and the application terminates without any message I am looking for a way to trace the calls made, so it is possible to get a hint about where there are missing dome sort of handler.\r\n\r\nHopefully I will not see the problem again when I get the machine back. \r\n\r\nSo having a verbose=3 printing each method called would be one way to go, but also hint about what I can do would be nice.\r\n\r\nMy application is lorseker\\ben at github but I have not seen the error on other machines.', 'created_at': datetime.datetime(2024, 10, 1, 9, 14, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416886645, 'issue_id': 2541225445, 'author': 'tilakrayal', 'body': '@ThorvaldAagaard,\r\nIn Keras, verbose is an optional argument in various methods, including fit(), evaluate(), and predict(). It is used to set the logging level during the model training and validation process. The verbose argument can take one of three integer values: 0, 1, or 2.\r\n\r\nverbose=0: Silent mode - no output during training.\r\nverbose=1: Progress bar mode - displays a progress bar with training and validation metrics (default).\r\nverbose=2: One line per epoch - shows a summary of training and validation metrics after each epoch.\r\n\r\nOnce you get the hardware please try to test your code. and also from the above conversation, it looks more related to Keras. So please raise the new issue in Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 10, 16, 13, 44, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416907189, 'issue_id': 2541225445, 'author': 'ThorvaldAagaard', 'body': 'My power supply and my CPU has been replaced, and I have not seen the termination of the application since.\r\n\r\nIt seems verboce is for user, and you do not have anything to help a developer, is the correct?\r\nStill scary, that the application just terminates without any message, but I will close this, as I will not being developing Keras.', 'created_at': datetime.datetime(2024, 10, 16, 13, 49, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416907268, 'issue_id': 2541225445, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76279"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76279"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 13, 49, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-23 13:23:24 UTC): @ThorvaldAagaard,
Can you please share a reproducible code/colab gist that supports your statement which helps to debug the issue in an effective way and provide the update? Thank you!

ThorvaldAagaard (Issue Creator) on (2024-09-23 14:20:30 UTC): I have not been able to reproduce it on other machines, so it could be a hardware problem, but even as a hardware problem I am interested in (like you) to debug the keras/tensorflow code to find the statement, when the application terminates.

My problem is that when I call predict with verbose=1 I just get a progress indicator, that stops and the application exit without any type of message.

I would expect the code to have some error handling, that should catch even hardware errors.

Even on the same machine it is not the same predict, that fails, and currently the application has been running for more that 1,5 hours without any problem, so I am really mystified.

Can I active some kind of trace of the code?

tilakrayal (Assginee) on (2024-10-01 07:54:01 UTC): @ThorvaldAagaard,
Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using. Thank you!

ThorvaldAagaard (Issue Creator) on (2024-10-01 09:14:45 UTC): I assume it is a hardware problem and currently the machine is being at service.

I do not expect you to be able to solve the problem, but as I just call predict and see the progress bar stopping in the  midle and the application terminates without any message I am looking for a way to trace the calls made, so it is possible to get a hint about where there are missing dome sort of handler.

Hopefully I will not see the problem again when I get the machine back. 

So having a verbose=3 printing each method called would be one way to go, but also hint about what I can do would be nice.

My application is lorseker\ben at github but I have not seen the error on other machines.

tilakrayal (Assginee) on (2024-10-16 13:44:24 UTC): @ThorvaldAagaard,
In Keras, verbose is an optional argument in various methods, including fit(), evaluate(), and predict(). It is used to set the logging level during the model training and validation process. The verbose argument can take one of three integer values: 0, 1, or 2.

verbose=0: Silent mode - no output during training.
verbose=1: Progress bar mode - displays a progress bar with training and validation metrics (default).
verbose=2: One line per epoch - shows a summary of training and validation metrics after each epoch.

Once you get the hardware please try to test your code. and also from the above conversation, it looks more related to Keras. So please raise the new issue in Keras-team/keras repo for the quick resolution. Thank you!

ThorvaldAagaard (Issue Creator) on (2024-10-16 13:49:49 UTC): My power supply and my CPU has been replaced, and I have not seen the termination of the application since.

It seems verboce is for user, and you do not have anything to help a developer, is the correct?
Still scary, that the application just terminates without any message, but I will close this, as I will not being developing Keras.

google-ml-butler[bot] on (2024-10-16 13:49:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76279"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76279"">No</a>

"
2540756847,issue,closed,completed,Importing tensorflow,"ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 9
      7 from sklearn.metrics import r2_score,mean_squared_error
      8 from sklearn.pipeline import Pipeline
----> 9 import tensorflow as tf

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\CHUKS\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.",obinna-Muonanu,2024-09-22 07:36:43+00:00,['Venkat6871'],2024-10-10 02:01:27+00:00,2024-10-10 02:01:24+00:00,https://github.com/tensorflow/tensorflow/issues/76274,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2365778851, 'issue_id': 2540756847, 'author': 'obinna-Muonanu', 'body': 'I got this error when trying to import tensorflow', 'created_at': datetime.datetime(2024, 9, 22, 7, 37, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2366828449, 'issue_id': 2540756847, 'author': 'mihaimaruseac', 'body': 'Please search for similar issues in the repo.', 'created_at': datetime.datetime(2024, 9, 22, 15, 3, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370122562, 'issue_id': 2540756847, 'author': 'Venkat6871', 'body': 'Hi **@obinna-Muonanu** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nSome similar issues are being discussed, and I am providing those [links](https://github.com/tensorflow/tensorflow/issues/74725), [link2](https://github.com/tensorflow/tensorflow/issues/74405) here for your reference. Please go through them. I hope they will be helpful for you.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 24, 4, 24, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387488528, 'issue_id': 2540756847, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 2, 2, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748519, 'issue_id': 2540756847, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748563, 'issue_id': 2540756847, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76274"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76274"">No</a>', 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 26, tzinfo=datetime.timezone.utc)}]","obinna-Muonanu (Issue Creator) on (2024-09-22 07:37:28 UTC): I got this error when trying to import tensorflow

mihaimaruseac on (2024-09-22 15:03:35 UTC): Please search for similar issues in the repo.

Venkat6871 (Assginee) on (2024-09-24 04:24:10 UTC): Hi **@obinna-Muonanu** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Some similar issues are being discussed, and I am providing those [links](https://github.com/tensorflow/tensorflow/issues/74725), [link2](https://github.com/tensorflow/tensorflow/issues/74405) here for your reference. Please go through them. I hope they will be helpful for you.
Thank you!

github-actions[bot] on (2024-10-02 02:01:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-10 02:01:24 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-10 02:01:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76274"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76274"">No</a>

"
2540633536,issue,closed,completed,Reproducing Benchmark Tool performance in actual code,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

3.11

### Bazel version

7.3.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
adb shell taskset f0 /data/local/tmp/benchmark_model \                                                                                                                                                         master ✱
  --graph=/data/local/tmp/output.tflite \
--enable_op_profiling=false --input_layer_shape=1,32,32,1 --input_layer=inputs_0 --use_xnnpack=true --num_threads=1```
```
Timings:
Inference timings in us: Init: 18578, First inference: 3520, Warmup (avg): 3358.58, Inference (avg): **3403.45**

```
adb shell taskset f0 /data/local/tmp/benchmark_model \                                                                                                                                                         master ✱
  --graph=/data/local/tmp/output.tflite \
--enable_op_profiling=false --input_layer_shape=1,32,32,1 --input_layer=inputs_0 --use_xnnpack=true --num_threads=3```
```

Inference timings in us: Init: 20703, First inference: 2024, Warmup (avg): 1348.85, Inference (avg): **1363.29**


When i write the custom inference code with options to set the number of threads. 

1. Settings the number of threads > 1 using `builder.SetNumThreads(num_threads)` has no effect, while `benchmark_model` is able to reflect the improvements. 

```
        model_ = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());
        if (!model_) {
            return false;
        }

        // Create the interpreter
        tflite::ops::builtin::BuiltinOpResolver resolver;
        tflite::InterpreterBuilder builder(*model_, resolver);
        if (builder.SetNumThreads(num_threads) != kTfLiteOk) {
            TFLITE_LOG(ERROR) << ""Failed to set thread number"";
            return kTfLiteError;
        }
        builder(&interpreter_);
        if (!interpreter_) {
            return false;
        }
        // Apply XNNPACK delegate if requested
        if (use_xnnpack) {
            TfLiteXNNPackDelegateOptions xnnpack_options =
                TfLiteXNNPackDelegateOptionsDefault();
            xnnpack_delegate_ = TfLiteXNNPackDelegateCreate(&xnnpack_options);
            if (interpreter_->ModifyGraphWithDelegate(xnnpack_delegate_) !=
                kTfLiteOk) {
                std::cerr << ""Failed to apply XNNPACK delegate."" << std::endl;
                return false;
            }
        }

```

### Standalone code to reproduce the issue

```shell
adb shell taskset f0 /data/local/tmp/benchmark_model \                                                                                                                                                         master ✱
  --graph=/data/local/tmp/output.tflite \
--enable_op_profiling=false --input_layer_shape=1,32,32,1 --input_layer=inputs_0 --use_xnnpack=true --num_threads=3```
```
```


### Relevant log output

_No response_",nrupatunga,2024-09-22 04:25:22+00:00,['gaikwadrahul8'],2024-09-23 15:52:43+00:00,2024-09-23 15:52:40+00:00,https://github.com/tensorflow/tensorflow/issues/76261,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2368705853, 'issue_id': 2540633536, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76261"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76261"">No</a>', 'created_at': datetime.datetime(2024, 9, 23, 15, 52, 41, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-09-23 15:52:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76261"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76261"">No</a>

"
2539867354,issue,open,,tensorflow pjrt plugin,"I've developed a pjrt plugin using https://openxla.org/xla/pjrt_integration as a resource, which is working with jax. 

How do I load this plugin and run tensorflow models? Is there some kind of registration step, similar to jax? If I load the plugin into `import jax._src.xla_bridge` and call `tf.config.list_physical_devices()` I don't see my device, so I imagine it's something else. ",AleksKnezevic,2024-09-21 01:25:04+00:00,"['cheshire', 'wangpengmit']",2025-01-31 08:29:15+00:00,,https://github.com/tensorflow/tensorflow/issues/76188,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:model', 'Model related issues'), ('comp:xla', 'XLA')]","[{'comment_id': 2370445109, 'issue_id': 2539867354, 'author': 'Venkat6871', 'body': 'Hi **@AleksKnezevic** ,\r\nThank you for the collaboration. Could you please elaborate on your feature and specify the use cases for it?\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 24, 7, 43, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371048690, 'issue_id': 2539867354, 'author': 'AleksKnezevic', 'body': ""Hi @Venkat6871 thanks for getting back to me! We're developing an mlir based [compiler](https://github.com/tenstorrent/tt-mlir) for our accelerator and I'd like to integrate it into tensorflow. As I understand it, pjrt is the interface to do this. \r\n\r\nI have developed a pjrt plugin (using IREE as an example) that I can load into jax using `import jax._src.xla_bridge`. What is the equivalent way of loading it into tensorflow?"", 'created_at': datetime.datetime(2024, 9, 24, 11, 53, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387821855, 'issue_id': 2539867354, 'author': 'AleksKnezevic', 'body': '@cheshire or @wangpengmit, any thoughts?', 'created_at': datetime.datetime(2024, 10, 2, 7, 40, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626635397, 'issue_id': 2539867354, 'author': 'lumontec', 'body': 'Also interested in this', 'created_at': datetime.datetime(2025, 1, 31, 8, 29, 14, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-09-24 07:43:50 UTC): Hi **@AleksKnezevic** ,
Thank you for the collaboration. Could you please elaborate on your feature and specify the use cases for it?

Thank you!

AleksKnezevic (Issue Creator) on (2024-09-24 11:53:17 UTC): Hi @Venkat6871 thanks for getting back to me! We're developing an mlir based [compiler](https://github.com/tenstorrent/tt-mlir) for our accelerator and I'd like to integrate it into tensorflow. As I understand it, pjrt is the interface to do this. 

I have developed a pjrt plugin (using IREE as an example) that I can load into jax using `import jax._src.xla_bridge`. What is the equivalent way of loading it into tensorflow?

AleksKnezevic (Issue Creator) on (2024-10-02 07:40:23 UTC): @cheshire or @wangpengmit, any thoughts?

lumontec on (2025-01-31 08:29:14 UTC): Also interested in this

"
2539277667,issue,open,,TensorFlow keeps creating threads when multi-GPU training （thread leak）,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

Nvidia A800 

### Current behavior?

I was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https://github.com/chinese-poetry/chinese-poetry.git) https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/

Before we start, I would give a baseline amount of this machine's threads :
![10](https://github.com/user-attachments/assets/53e16381-e50f-4054-8c2a-b790fe2e077b)

When starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. 

But with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat /proc/""this programs' pid""/status to check the number of threads):
![3](https://github.com/user-attachments/assets/56bbfaf0-f59c-48f1-ae19-a98d563ad000)
![tf](https://github.com/user-attachments/assets/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)
![2](https://github.com/user-attachments/assets/0d07b24d-e93e-451e-9e1e-51a26f7db60d)

Then, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):
![6](https://github.com/user-attachments/assets/508fed5c-8aec-4deb-8e68-74dbf6aa5613)
![4](https://github.com/user-attachments/assets/77507790-70fd-4abf-b72c-e9d831565879)
![5](https://github.com/user-attachments/assets/d8945e53-23d9-4fb7-b50d-37962b93a692)

When calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):
![8](https://github.com/user-attachments/assets/15b433cc-eefb-451f-a31c-2a286e17afec)
![8](https://github.com/user-attachments/assets/813835d2-3834-400a-b8fe-ec27f15d89ad)

I checked an similar issue in https://github.com/tensorflow/tensorflow/issues/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.





### Standalone code to reproduce the issue

```shell
Here is my code

import os

os.environ[""KERAS_BACKEND""] = ""tensorflow""  # or ""tensorflow"" or ""torch""

import keras_nlp
import keras
import tensorflow as tf
import time

keras.mixed_precision.set_global_policy(""mixed_float16"")

import os
import json
import datetime

train_ds = (
    tf.data.Dataset.from_tensor_slices(paragraphs)
    .batch(36)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)

strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1"", ""GPU:2"", ""GPU:3"", ""GPU:4"", ""GPU:5""])
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
    ""gpt2_base_en"",
    sequence_length=128,
)

# Open a strategy scope.
with strategy.scope():
# To speed up training and generation, we use preprocessor of length 128
# instead of full length 1024.
    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
        ""gpt2_base_en"", preprocessor=preprocessor
    )
    num_epochs = 5

    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        monitor=""accuracy"",
        # monitor=""i_loss"",
        mode=""min"",
        save_best_only=True,
        save_freq=""epoch""
    )
    learning_rate = 5e-4
    
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    gpt2_lm.compile(
        optimizer=keras.optimizers.Adam(learning_rate),
        loss=loss,
        weighted_metrics=[""accuracy""],
    )

    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[
        checkpoint_callback,
        tensorboard_callback,
    ],
                )
```


### Relevant log output

```shell
2024-09-21 00:24:25.840848: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm/gpt2_backbone/embeddings_dropout/dropout/random_uniform/RandomUniform
2024-09-21 00:24:25.846135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-09-21 00:24:26.559075: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.572007: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.573183: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581128: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581197: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.588190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
12024-09-21 00:25:01.986918: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.215951: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-09-21 00:25:02.420703: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.542130: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:03.595774: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

12024-09-21 00:25:04.071526: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:04.130504: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:05.333297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5054/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'
terminate called recursively
  what():  Resource temporarily unavailable
Aborted (core dumped)
```
",Jisencc,2024-09-20 17:14:03+00:00,['tilakrayal'],2024-10-16 08:09:35+00:00,,https://github.com/tensorflow/tensorflow/issues/76157,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.11', 'Issues related to TF 2.11')]","[{'comment_id': 2368098658, 'issue_id': 2539277667, 'author': 'tilakrayal', 'body': '@Jisencc,\r\nI can see that you are using tensorflow v2.11 which is pretty old. Could you please try using latest tensorflow v2.17 and update whether you are facing a similar issue? Thank you!', 'created_at': datetime.datetime(2024, 9, 23, 12, 39, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370011411, 'issue_id': 2539277667, 'author': 'Jisencc', 'body': 'Hi @tilakrayal,\r\n\r\nVery glad to hear your response. I would really like to try the tensorflow v2.17, and actually I have installed the latest tensorflow v2.17 in the initial round of using this machine, which cannot work on there.  When I checked the Nvidia Kernel Module in this machine, I found:\r\n**NVRM version: NVIDIA UNIX x86_64 Kernel Module  470.256.02**\r\nSo, for meeting the requirements, I had to downgrade the Nvidia driver to **NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4** (I had tried a lot of the latest Nvidia driver on this machine such as 550, and these were not worked, just 470 was fine to this machine).\r\n\r\nBased on this, I have to install the tensorflow v2.11 to meet the CUDA tookit Version: 11.4, which eventually worked fine. Other programs run on the same tensorflow env was fine and no problem, but when run the GPT-2,  this situation happend. I am so sorry for that.\r\n\r\nI compared the GPT-2 code of calling tensorflow multi-GPU training module with my previous projects run successfully on this machine, which was no any difference. \r\n\r\nI have checked the same issue at https://github.com/tensorflow/tensorflow/issues/62466, but they overcome this problem by revising .cc files, I cannot find this file in my machine. \r\n\r\nTherefore, if possible, could you please give me a way to overcome this issue by just revising .py files or adding some codes to close the corresponding code that may inducing the issue (an accessible way I can process). \r\n\r\nSorry to disturb again.', 'created_at': datetime.datetime(2024, 9, 24, 2, 57, 20, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-23 12:39:28 UTC): @Jisencc,
I can see that you are using tensorflow v2.11 which is pretty old. Could you please try using latest tensorflow v2.17 and update whether you are facing a similar issue? Thank you!

Jisencc (Issue Creator) on (2024-09-24 02:57:20 UTC): Hi @tilakrayal,

Very glad to hear your response. I would really like to try the tensorflow v2.17, and actually I have installed the latest tensorflow v2.17 in the initial round of using this machine, which cannot work on there.  When I checked the Nvidia Kernel Module in this machine, I found:
**NVRM version: NVIDIA UNIX x86_64 Kernel Module  470.256.02**
So, for meeting the requirements, I had to downgrade the Nvidia driver to **NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4** (I had tried a lot of the latest Nvidia driver on this machine such as 550, and these were not worked, just 470 was fine to this machine).

Based on this, I have to install the tensorflow v2.11 to meet the CUDA tookit Version: 11.4, which eventually worked fine. Other programs run on the same tensorflow env was fine and no problem, but when run the GPT-2,  this situation happend. I am so sorry for that.

I compared the GPT-2 code of calling tensorflow multi-GPU training module with my previous projects run successfully on this machine, which was no any difference. 

I have checked the same issue at https://github.com/tensorflow/tensorflow/issues/62466, but they overcome this problem by revising .cc files, I cannot find this file in my machine. 

Therefore, if possible, could you please give me a way to overcome this issue by just revising .py files or adding some codes to close the corresponding code that may inducing the issue (an accessible way I can process). 

Sorry to disturb again.

"
2537129955,issue,open,,"`tf.slice` triggers XLA recompilation on each call despite static shape, while `xla.dynamic_slice` does not","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240919

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.function` with `jit_compile=True`, I've observed that `tf.slice` leads to XLA recompilation at each function call, even when the tensor shape are static. Explicitly using `tensorflow.compiler.tf2xla.python.xla.dynamic_slice` instead resolves this issue and prevents recompilation.

Expected behavior: `tf.slice` should not trigger XLA recompilation when shapes are static, similar to `xla.dynamic_slice`.

Actual behavior: `tf.slice` causes XLA recompilation on each call leading to a 100x slower performance.

Question: Shouldn't `tf.slice` be automatically converted to `xla.dynamic_slice` or an equivalent XLA operation to avoid unnecessary recompilation?

### Standalone code to reproduce the issue

```shell
import time

import tensorflow as tf
import tensorflow.compiler.tf2xla.python.xla as xla

@tf.function(jit_compile=True)
def func_xla_slice(n):
  range = tf.range(100)
  slice = xla.dynamic_slice(range, [n], [50])
  return slice

@tf.function(jit_compile=True)
def func_tf_slice(n):
  range = tf.range(100)
  slice = tf.slice(range, [n], [50])
  return slice

if __name__ == '__main__':
  print(tf.version.VERSION)

  # tracing calls
  func_xla_slice(tf.constant(0))
  func_tf_slice(tf.constant(0))

  start = time.time()
  for i in range(1, 50):
    func_xla_slice(tf.constant(i))
  print(f""XLA slice took {(time.time() - start) * 1000:.0f} ms"")

  start = time.time()
  for i in range(1, 50):
    func_tf_slice(tf.constant(i))
  print(f""TF slice took {(time.time() - start) * 1000:.0f} ms"")
```


### Relevant log output

```shell
XLA slice took 19 ms
TF slice took 2016 ms
```
",nicolaspi,2024-09-19 18:50:42+00:00,['tilakrayal'],2024-09-24 07:39:00+00:00,,https://github.com/tensorflow/tensorflow/issues/76070,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:ops', 'OPs related issues'), ('comp:xla', 'XLA'), ('type:performance', 'Performance Issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2367228485, 'issue_id': 2537129955, 'author': 'yugborana', 'body': 'I would like to know more about this issue. \r\nI have a thought of solution that if we could make a custom wrapper function which wraps the xla.dynamic_slice operation. By using wrapper internally, maybe we will be able to address the recompilation issue. And then, Replace calls to tf.slice with xla_optimized_slice in XLA-compiled functions.', 'created_at': datetime.datetime(2024, 9, 23, 4, 44, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367517156, 'issue_id': 2537129955, 'author': 'tilakrayal', 'body': '@nicolaspi,\r\nAs mentioned the difference is more when I tried to execute the code on tensorflow v2.17(2.18.0-dev20240922), but whereas on tf-nightly, the time taken for the execution is reduced than the v2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2e63e01b9453bec85f2cb31b1f020ab7/untitled2121.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 9, 23, 8, 17, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367684918, 'issue_id': 2537129955, 'author': 'nicolaspi', 'body': ""@tilakrayal Thank you for the performance comparison. While the gap has narrowed in nightly, `tf.slice` is still significantly slower (28x) than `xla.dynamic_slice`. Why is the issue marked as _awaiting response from author_? The core problem remains unchanged and I'm not sure what additional input I can provide here. :)"", 'created_at': datetime.datetime(2024, 9, 23, 9, 33, 8, tzinfo=datetime.timezone.utc)}]","yugborana on (2024-09-23 04:44:11 UTC): I would like to know more about this issue. 
I have a thought of solution that if we could make a custom wrapper function which wraps the xla.dynamic_slice operation. By using wrapper internally, maybe we will be able to address the recompilation issue. And then, Replace calls to tf.slice with xla_optimized_slice in XLA-compiled functions.

tilakrayal (Assginee) on (2024-09-23 08:17:06 UTC): @nicolaspi,
As mentioned the difference is more when I tried to execute the code on tensorflow v2.17(2.18.0-dev20240922), but whereas on tf-nightly, the time taken for the execution is reduced than the v2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2e63e01b9453bec85f2cb31b1f020ab7/untitled2121.ipynb). Thank you!

nicolaspi (Issue Creator) on (2024-09-23 09:33:08 UTC): @tilakrayal Thank you for the performance comparison. While the gap has narrowed in nightly, `tf.slice` is still significantly slower (28x) than `xla.dynamic_slice`. Why is the issue marked as _awaiting response from author_? The core problem remains unchanged and I'm not sure what additional input I can provide here. :)

"
2536821888,issue,closed,completed,Get tflite of unified_detector,"<!--
As per our GitHub Policy (https://github.com/tensorflow/models/blob/master/ISSUES.md), we only address code bugs, documentation issues, and feature requests on GitHub.

We will automatically close questions and help related issues.

Please go to Stack Overflow (http://stackoverflow.com/questions/tagged/tensorflow-model-garden) for questions and help.

-->
hi, how can i get tflite model of unified_detector to run it faster on mobile?",josef821,2024-09-19 12:33:04+00:00,['gaikwadrahul8'],2024-10-31 02:03:19+00:00,2024-10-31 02:03:14+00:00,https://github.com/tensorflow/tensorflow/issues/76059,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2363148560, 'issue_id': 2536821888, 'author': 'Venkat6871', 'body': 'Hi **@josef821** ,\r\nThank you for bringing your issue here. To get the TFLite model of the Unified Detector, the official documentation provides detailed instructions. I am providing the [link](https://ai.google.dev/edge/litert) to the documentation here, please review it.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 20, 8, 21, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380351124, 'issue_id': 2536821888, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 28, 2, 0, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384397123, 'issue_id': 2536821888, 'author': 'josef821', 'body': 'Unified Detector get 5G ram and 30 sec to warmup and 6 to 10 second to run on every image? it can be use in mobile. is there any tips to run it with less memory and faster (like 1 to 3 sec)?', 'created_at': datetime.datetime(2024, 9, 30, 23, 23, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413733223, 'issue_id': 2536821888, 'author': 'gaikwadrahul8', 'body': ""Hi, @josef821 \r\n\r\nI apologize for the delayed response, it seems like the unified_detector model is quite large and resource-intensive which is why it's requiring 5G of RAM and taking 30 seconds to warm up and 6-10 seconds to run on every image. To run this model on a mobile device with less memory and faster inference time then you'll have to go with model optimization because edge devices often have limited memory or computational power. Various optimizations can be applied to models so that they can be run within these constraints. In addition, some optimizations allow the use of specialized hardware for accelerated inference. please refer to [Model Optimizations ](https://ai.google.dev/edge/litert/models/model_optimization)\r\n\r\n**TensorFlow Lite optimizations with different delegates which you can try :**\r\n\r\n1. Use the TensorFlow Lite GPU delegate: If the mobile device has a GPU use the TensorFlow Lite GPU delegate to accelerate inference. Please refer to [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu) \r\n2. Use the TensorFlow Lite NNAPI delegate: If the mobile device supports the Android Neural Networks API (NNAPI) use the TensorFlow Lite NNAPI delegate to accelerate inference.\r\n3. Use the TensorFlow Lite XNNPACK delegate: Use the TensorFlow Lite XNNPACK delegate to accelerate inference on devices with limited GPU capabilities. Please refer to [XNNPACK backend for TensorFlow Lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md)\r\n[Performance best practices](https://ai.google.dev/edge/litert/models/best_practices)\r\n\r\nYou can also do performance measurement by using benchmark tools to calculate statistics for the following important performance metrics for more information please refer to [official documentation](https://ai.google.dev/edge/litert/models/measurement)\r\n\r\n- Initialization time\r\n- Inference time of warmup state\r\n- Inference time of steady state\r\n- Memory usage during initialization time\r\n- Overall memory usage\r\n\r\nIf I have missed something here please let me know. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 15, 12, 7, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414770904, 'issue_id': 2536821888, 'author': 'josef821', 'body': ""thanks for your complete answer,\r\nCan you practically help me to do this and get a lighter model like tflite or onnx that will work faster? Can I request that if you are familiar with how this model works, do it for me. Is it possible to optimize this model and increase its execution speed? I'm not much of a professional in deep learning."", 'created_at': datetime.datetime(2024, 10, 15, 18, 50, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416470315, 'issue_id': 2536821888, 'author': 'gaikwadrahul8', 'body': ""Hi, @josef821\r\n\r\nYou're welcome, To confirm you've `unified_detector` model and want to convert to TFLite format which should work faster on mobile ? If possible could you please elaborate more what are you trying to achieve with your use case that will be helpful to understand better.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 10, 16, 10, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416603497, 'issue_id': 2536821888, 'author': 'josef821', 'body': 'i want to use it in any device like phone,pc,laptop and etc for text detection and then use tesseract for recognizing but faster and use less memory. i try to convert it to tflite. it create 80mb tflite, i try to run it with python tflite interpter, it get 30 sec and 4G ram. i think deeplab2 backbone is very complex. have you any tips?', 'created_at': datetime.datetime(2024, 10, 16, 11, 56, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417591733, 'issue_id': 2536821888, 'author': 'gaikwadrahul8', 'body': 'Hi, @josef821 \r\n\r\nThank you for providing the more details, The first step to optimize the speed and performance of text detection and recognition models on mobile devices is to choose the right model for your use case. Depending on the type, size, orientation and quality of the text you may need different models with various architectures, input sizes and output formats.\r\n\r\nPlease refer this [official blog](https://blog.tensorflow.org/2021/09/blog.tensorflow.org202109optical-character-recognition.html) and [OCR (Optical Character Recognition) Android](https://github.com/tensorflow/examples/tree/master/lite/examples/optical_character_recognition/android) example which may be good starting point, You can find TensorFlow Lite models( renamed as LiteRT now) on [Kaggle Models](https://www.kaggle.com/models?framework=tfLite) and you can try lightweight models like MobileNet or EfficientNet\r\n\r\nI would suggest you to go with techniques like model pruning, quantization or knowledge distillation to reduce the size of the model as I said in my previous comment. Smaller models typically require less memory and computational power making them more suitable for deployment on mobile devices. Thank you', 'created_at': datetime.datetime(2024, 10, 16, 18, 23, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434074018, 'issue_id': 2536821888, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 24, 2, 1, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448868060, 'issue_id': 2536821888, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448868181, 'issue_id': 2536821888, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76059"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76059"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 18, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-09-20 08:21:16 UTC): Hi **@josef821** ,
Thank you for bringing your issue here. To get the TFLite model of the Unified Detector, the official documentation provides detailed instructions. I am providing the [link](https://ai.google.dev/edge/litert) to the documentation here, please review it.

Thank you!

github-actions[bot] on (2024-09-28 02:00:47 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

josef821 (Issue Creator) on (2024-09-30 23:23:58 UTC): Unified Detector get 5G ram and 30 sec to warmup and 6 to 10 second to run on every image? it can be use in mobile. is there any tips to run it with less memory and faster (like 1 to 3 sec)?

gaikwadrahul8 (Assginee) on (2024-10-15 12:07:14 UTC): Hi, @josef821 

I apologize for the delayed response, it seems like the unified_detector model is quite large and resource-intensive which is why it's requiring 5G of RAM and taking 30 seconds to warm up and 6-10 seconds to run on every image. To run this model on a mobile device with less memory and faster inference time then you'll have to go with model optimization because edge devices often have limited memory or computational power. Various optimizations can be applied to models so that they can be run within these constraints. In addition, some optimizations allow the use of specialized hardware for accelerated inference. please refer to [Model Optimizations ](https://ai.google.dev/edge/litert/models/model_optimization)

**TensorFlow Lite optimizations with different delegates which you can try :**

1. Use the TensorFlow Lite GPU delegate: If the mobile device has a GPU use the TensorFlow Lite GPU delegate to accelerate inference. Please refer to [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu) 
2. Use the TensorFlow Lite NNAPI delegate: If the mobile device supports the Android Neural Networks API (NNAPI) use the TensorFlow Lite NNAPI delegate to accelerate inference.
3. Use the TensorFlow Lite XNNPACK delegate: Use the TensorFlow Lite XNNPACK delegate to accelerate inference on devices with limited GPU capabilities. Please refer to [XNNPACK backend for TensorFlow Lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md)
[Performance best practices](https://ai.google.dev/edge/litert/models/best_practices)

You can also do performance measurement by using benchmark tools to calculate statistics for the following important performance metrics for more information please refer to [official documentation](https://ai.google.dev/edge/litert/models/measurement)

- Initialization time
- Inference time of warmup state
- Inference time of steady state
- Memory usage during initialization time
- Overall memory usage

If I have missed something here please let me know. 

Thank you for your cooperation and patience.

josef821 (Issue Creator) on (2024-10-15 18:50:43 UTC): thanks for your complete answer,
Can you practically help me to do this and get a lighter model like tflite or onnx that will work faster? Can I request that if you are familiar with how this model works, do it for me. Is it possible to optimize this model and increase its execution speed? I'm not much of a professional in deep learning.

gaikwadrahul8 (Assginee) on (2024-10-16 10:59:53 UTC): Hi, @josef821

You're welcome, To confirm you've `unified_detector` model and want to convert to TFLite format which should work faster on mobile ? If possible could you please elaborate more what are you trying to achieve with your use case that will be helpful to understand better.

Thank you for your cooperation and patience.

josef821 (Issue Creator) on (2024-10-16 11:56:59 UTC): i want to use it in any device like phone,pc,laptop and etc for text detection and then use tesseract for recognizing but faster and use less memory. i try to convert it to tflite. it create 80mb tflite, i try to run it with python tflite interpter, it get 30 sec and 4G ram. i think deeplab2 backbone is very complex. have you any tips?

gaikwadrahul8 (Assginee) on (2024-10-16 18:23:22 UTC): Hi, @josef821 

Thank you for providing the more details, The first step to optimize the speed and performance of text detection and recognition models on mobile devices is to choose the right model for your use case. Depending on the type, size, orientation and quality of the text you may need different models with various architectures, input sizes and output formats.

Please refer this [official blog](https://blog.tensorflow.org/2021/09/blog.tensorflow.org202109optical-character-recognition.html) and [OCR (Optical Character Recognition) Android](https://github.com/tensorflow/examples/tree/master/lite/examples/optical_character_recognition/android) example which may be good starting point, You can find TensorFlow Lite models( renamed as LiteRT now) on [Kaggle Models](https://www.kaggle.com/models?framework=tfLite) and you can try lightweight models like MobileNet or EfficientNet

I would suggest you to go with techniques like model pruning, quantization or knowledge distillation to reduce the size of the model as I said in my previous comment. Smaller models typically require less memory and computational power making them more suitable for deployment on mobile devices. Thank you

github-actions[bot] on (2024-10-24 02:01:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-31 02:03:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-31 02:03:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76059"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76059"">No</a>

"
2535945908,issue,closed,completed,# text_classification_with_hub code does not run. give error for `model.add(hub_layer)`,"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub


code url: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb
```
model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.summary()
```



---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-9-b0a8e638d9e1>](https://localhost:8080/#) in <cell line: 2>()
      1 model = tf.keras.Sequential()
----> 2 model.add(hub_layer)
      3 model.add(tf.keras.layers.Dense(16, activation='relu'))
      4 model.add(tf.keras.layers.Dense(1))
      5 

[/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py](https://localhost:8080/#) in add(self, layer, rebuild)
     93                 layer = origin_layer
     94         if not isinstance(layer, Layer):
---> 95             raise ValueError(
     96                 ""Only instances of `keras.Layer` can be ""
     97                 f""added to a Sequential model. Received: {layer} ""

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7d1a15ed5600> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)",mn-48,2024-09-19 10:33:44+00:00,['tilakrayal'],2024-09-24 09:10:35+00:00,2024-09-24 08:12:30+00:00,https://github.com/tensorflow/tensorflow/issues/76042,"[('type:docs-bug', 'Document issues'), ('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2360665124, 'issue_id': 2535945908, 'author': 'mn-48', 'body': '@emadellaloggia your provided file is blocked and shows danger.', 'created_at': datetime.datetime(2024, 9, 19, 10, 55, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361081043, 'issue_id': 2535945908, 'author': 'tilakrayal', 'body': '@mn-48,\r\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3f0497dc9892f933fdfd706adc39dded/text_classification_with_hub.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 19, 14, 4, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370575148, 'issue_id': 2535945908, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 8, 12, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370711404, 'issue_id': 2535945908, 'author': 'mn-48', 'body': 'Yes\r\n\r\nOn Tue, 24 Sept 2024, 2:12 pm google-ml-butler[bot], <\r\n***@***.***> wrote:\r\n\r\n> Are you satisfied with the resolution of your issue?\r\n> Yes\r\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042>\r\n> No\r\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042>\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/76042#issuecomment-2370575148>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHLJUQZFYBK6R5HKIPRSQK3ZYENIXAVCNFSM6AAAAABOPTL5TCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZQGU3TKMJUHA>\r\n> .\r\n> You are receiving this because you modified the open/close state.Message\r\n> ID: ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 9, 24, 9, 10, 34, tzinfo=datetime.timezone.utc)}]","mn-48 (Issue Creator) on (2024-09-19 10:55:56 UTC): @emadellaloggia your provided file is blocked and shows danger.

tilakrayal (Assginee) on (2024-09-19 14:04:01 UTC): @mn-48,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras

import tf_keras as keras
```

Also I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3f0497dc9892f933fdfd706adc39dded/text_classification_with_hub.ipynb).

Thank you!

google-ml-butler[bot] on (2024-09-24 08:12:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76042"">No</a>

mn-48 (Issue Creator) on (2024-09-24 09:10:34 UTC): Yes

On Tue, 24 Sept 2024, 2:12 pm google-ml-butler[bot], <
***@***.***> wrote:

"
2535823703,issue,closed,completed,GPU use,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

Window

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9.2.1

### GPU model and memory

_No response_

### Current behavior?

hey i successfully setup cuda and able to use GPU in WSL , but my question is how i use it on my windows Conda Environment

### Standalone code to reproduce the issue

```shell
hey i successfully setup cuda and able to use GPU in WSL , but my question is how i use it on my windows Conda Environment
```


### Relevant log output

```shell
hey i successfully setup cuda and able to use GPU in WSL , but my question is how i use it on my windows Conda Environment
```
",Tarun0000,2024-09-19 09:41:10+00:00,['Venkat6871'],2024-10-10 02:01:30+00:00,2024-10-10 02:01:26+00:00,https://github.com/tensorflow/tensorflow/issues/76038,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2362995352, 'issue_id': 2535823703, 'author': 'Venkat6871', 'body': 'Hi **@Tarun0000** ,\r\nThank you for bringing your issue here. First, you need to check all the compatibility versions, as there are version mismatches in your setup. I have provided the [documentation](https://www.tensorflow.org/install/source_windows) for your reference—please review it. After that install tensorflow(2.10) and compatible cuda versions and you can use tensorflow in native windows.\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 20, 7, 14, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363380839, 'issue_id': 2535823703, 'author': 'Tarun0000', 'body': 'my cuda version is 12.6', 'created_at': datetime.datetime(2024, 9, 20, 10, 20, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370132707, 'issue_id': 2535823703, 'author': 'Venkat6871', 'body': 'Hi **@Tarun0000** ,\r\nThank you for your confirmation. Your CUDA version should be 11.2, and the cuDNN version should be 8.1 for TensorFlow 2.10. I have provided the [documentation](https://www.tensorflow.org/install/source_windows#gpu) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 24, 4, 34, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387488561, 'issue_id': 2535823703, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 2, 2, 1, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748569, 'issue_id': 2535823703, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748624, 'issue_id': 2535823703, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76038"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76038"">No</a>', 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 29, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-20 07:14:13 UTC): Hi **@Tarun0000** ,
Thank you for bringing your issue here. First, you need to check all the compatibility versions, as there are version mismatches in your setup. I have provided the [documentation](https://www.tensorflow.org/install/source_windows) for your reference—please review it. After that install tensorflow(2.10) and compatible cuda versions and you can use tensorflow in native windows.


Thank you!

Tarun0000 (Issue Creator) on (2024-09-20 10:20:48 UTC): my cuda version is 12.6

Venkat6871 (Assginee) on (2024-09-24 04:34:44 UTC): Hi **@Tarun0000** ,
Thank you for your confirmation. Your CUDA version should be 11.2, and the cuDNN version should be 8.1 for TensorFlow 2.10. I have provided the [documentation](https://www.tensorflow.org/install/source_windows#gpu) for your reference.
Thank you!

github-actions[bot] on (2024-10-02 02:01:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-10 02:01:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-10 02:01:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76038"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76038"">No</a>

"
2535661925,issue,open,,"tf.python.ops.array_ops.transpose aborts with ""Check failed: d >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops import array_ops

x =np.arange(0, 8).reshape([2, 4]).astype(np.float32)
y = np.array([-1, 0]).astype(np.int32)
array_ops.transpose(x, y,conjugate = False)
```


### Relevant log output

```shell
2024-09-19 16:16:30.137164: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",cybersupersoap,2024-09-19 08:31:38+00:00,['tilakrayal'],2024-12-10 18:27:15+00:00,,https://github.com/tensorflow/tensorflow/issues/76036,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2363143618, 'issue_id': 2535661925, 'author': 'tilakrayal', 'body': ""@cybersupersoap,\r\nI can see that you are trying to provide the input y as -1 which was the reason for crash. Permutes the dimensions according to the value of perm.\r\n\r\nThe returned tensor's dimension i will correspond to the input dimension perm[i]. If perm is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence, by default, this operation performs a regular matrix transpose on 2-D input Tensors.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/transpose\r\n\r\nIf you provide the -ve(-1) which will give 0-D leads to a crash.\r\n\r\nKindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4bc9d408abba854a13deffb7892fb64b/untitled2118.ipynb). Thank you!"", 'created_at': datetime.datetime(2024, 9, 20, 8, 18, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367110238, 'issue_id': 2535661925, 'author': 'cybersupersoap', 'body': ""hi~ @tilakrayal  Thank you for your prompt reply. I found that TensorFlow lacks value checking for invalid inputs of transpose operator, which leads to the crash. I'm concerned that the absence of this validation could result in a denial-of-service issue in TensorFlow."", 'created_at': datetime.datetime(2024, 9, 23, 1, 58, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532566662, 'issue_id': 2535661925, 'author': 'liujqian', 'body': ""> @cybersupersoap, I can see that you are trying to provide the input y as -1 which was the reason for crash. Permutes the dimensions according to the value of perm.\r\n> \r\n> The returned tensor's dimension i will correspond to the input dimension perm[i]. If perm is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence, by default, this operation performs a regular matrix transpose on 2-D input Tensors.\r\n> \r\n> https://www.tensorflow.org/api_docs/python/tf/transpose\r\n> \r\n> If you provide the -ve(-1) which will give 0-D leads to a crash.\r\n> \r\n> Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4bc9d408abba854a13deffb7892fb64b/untitled2118.ipynb). Thank you!\r\n\r\nEven if the user provides an invalid argument, the framework should not crash. Isn't this software engineering 101?"", 'created_at': datetime.datetime(2024, 12, 10, 18, 27, 14, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-20 08:18:30 UTC): @cybersupersoap,
I can see that you are trying to provide the input y as -1 which was the reason for crash. Permutes the dimensions according to the value of perm.

The returned tensor's dimension i will correspond to the input dimension perm[i]. If perm is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence, by default, this operation performs a regular matrix transpose on 2-D input Tensors.

https://www.tensorflow.org/api_docs/python/tf/transpose

If you provide the -ve(-1) which will give 0-D leads to a crash.

Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4bc9d408abba854a13deffb7892fb64b/untitled2118.ipynb). Thank you!

cybersupersoap (Issue Creator) on (2024-09-23 01:58:25 UTC): hi~ @tilakrayal  Thank you for your prompt reply. I found that TensorFlow lacks value checking for invalid inputs of transpose operator, which leads to the crash. I'm concerned that the absence of this validation could result in a denial-of-service issue in TensorFlow.

liujqian on (2024-12-10 18:27:14 UTC): Even if the user provides an invalid argument, the framework should not crash. Isn't this software engineering 101?

"
2535447118,issue,closed,completed,add a op to tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",huangmeimao,2024-09-19 06:49:22+00:00,['Venkat6871'],2024-10-04 02:01:37+00:00,2024-10-04 02:01:36+00:00,https://github.com/tensorflow/tensorflow/issues/76033,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2361065019, 'issue_id': 2535447118, 'author': 'Venkat6871', 'body': 'Hi **@huangmeimao** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 19, 13, 57, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259368, 'issue_id': 2535447118, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646654, 'issue_id': 2535447118, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 35, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-19 13:57:45 UTC): Hi **@huangmeimao** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

github-actions[bot] on (2024-09-27 02:01:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-04 02:01:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2535342990,issue,closed,completed,"Error: Only one input size may be -1, not both 0 and 1","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 10 LTSC

### Mobile device

_No response_

### Python version

3.12.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I used tfrecord file to generate the dataset and fit the model, I encountered the error 'Only one input size may be -1, not both 0 and 1'.
 I'm not sure if this issue is caused by a TensorFlow bug, as the main code works fine when I use local image and JSON label files for training.
If there is any way to resolve this issue, please let me know.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ntVuMurkZ4nmg-owgzYOd9hLA596ZwyE?usp=sharing
```


### Relevant log output

```shell
Total params: 35,040,706 (133.67 MB)
Trainable params: 35,040,706 (133.67 MB)
Non-trainable params: 0 (0.00 B)
Epoch 1/10
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1726724161.327751   51040 service.cc:146] XLA service 0x26fbeda47c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1726724161.328263   51040 service.cc:154]   StreamExecutor device (0): Host, Default Version
I0000 00:00:1726724161.340495   51040 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-09-19 13:36:01.640915: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at reshape_op.h:65 : INVALID_ARGUMENT: 
Only one input size may be -1, not both 0 and 1
2024-09-19 13:36:01.643767: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Only one input size may be -1, not both 0 and 1
         [[{{function_node __inference_one_step_on_data_2728}}{{node functional_1/flatten_1/Reshape}}]]
```
",Mochengvia,2024-09-19 05:43:24+00:00,['tilakrayal'],2024-10-19 09:33:48+00:00,2024-10-16 02:02:49+00:00,https://github.com/tensorflow/tensorflow/issues/76031,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2362913374, 'issue_id': 2535342990, 'author': 'tilakrayal', 'body': '@Mochengvia,\r\nI was facing a different issue/error while executing the mentioned code. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/03a0bf18a168a244fb522a96fafa9421/untitled2115.ipynb) and provide the complete dependencies which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 9, 20, 6, 16, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362932209, 'issue_id': 2535342990, 'author': 'Mochengvia', 'body': '@tilakrayal \r\nSorry, I forgot to upload my TFRecord file. I’ve updated the code, so you can now run it directly and see the results.\r\nhttps://colab.research.google.com/drive/1ntVuMurkZ4nmg-owgzYOd9hLA596ZwyE?usp=sharing', 'created_at': datetime.datetime(2024, 9, 20, 6, 31, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385111902, 'issue_id': 2535342990, 'author': 'tilakrayal', 'body': '@Mochengvia,\r\nTensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install **tf-keras** i.e., keras2.0 which the code was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a8c84c6b9375db93cbb7021679083c95/untitled2142.ipynb).\r\n\r\n```python\r\n!pip install tf-keras\r\nimport tf_keras as keras\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 8, 23, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123289, 'issue_id': 2535342990, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573637, 'issue_id': 2535342990, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573784, 'issue_id': 2535342990, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76031"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76031"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423720452, 'issue_id': 2535342990, 'author': 'Mochengvia', 'body': 'it works well, thanks.', 'created_at': datetime.datetime(2024, 10, 19, 9, 33, 46, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-20 06:16:37 UTC): @Mochengvia,
I was facing a different issue/error while executing the mentioned code. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/03a0bf18a168a244fb522a96fafa9421/untitled2115.ipynb) and provide the complete dependencies which helps to debug the issue. Thank you!

Mochengvia (Issue Creator) on (2024-09-20 06:31:23 UTC): @tilakrayal 
Sorry, I forgot to upload my TFRecord file. I’ve updated the code, so you can now run it directly and see the results.
https://colab.research.google.com/drive/1ntVuMurkZ4nmg-owgzYOd9hLA596ZwyE?usp=sharing

tilakrayal (Assginee) on (2024-10-01 08:23:13 UTC): @Mochengvia,
Tensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install **tf-keras** i.e., keras2.0 which the code was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a8c84c6b9375db93cbb7021679083c95/untitled2142.ipynb).

```python
!pip install tf-keras
import tf_keras as keras
```

Thank you!

github-actions[bot] on (2024-10-09 02:01:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:02:57 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76031"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/76031"">No</a>

Mochengvia (Issue Creator) on (2024-10-19 09:33:46 UTC): it works well, thanks.

"
2534256309,issue,open,,Code error when feature name has multiple `_`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

5.15.149-99.162.amzn2.x86_64

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expect there shouldn't be errors just by changing feature name. 

### Standalone code to reproduce the issue
This is a code sample that will work normally

```
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras import Model

import pandas as pd
import numpy as np

df = pd.DataFrame()
numeric_feature_name = 'a' * 27
categorical_feature_name = 'b' * 11
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

However, if I change the feature name, the same code will throw error

```
df = pd.DataFrame()
## Just change the feature name here
numeric_feature_name = 'a_b_c_d_e_f_g' 
categorical_feature_name = 'a_b_c_d_e_f'
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

We have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.
```


### Relevant log output

```shell
Epoch 1/10
2024-09-18 05:28:05.240962: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[14], line 8
      5 ds = ds.batch(32)
      6 ds_train = ds
----> 8 model.fit(
      9     ds_train,
     10     epochs=10,
     11     batch_size=300,
     12     verbose=1
     13 )

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

UnimplementedError: Graph execution error:

Detected at node functional_5_1/Cast defined at (most recent call last):
  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel_launcher.py"", line 18, in <module>

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/traitlets/config/application.py"", line 1075, in launch_instance

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 739, in start

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 534, in process_one

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 778, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 449, in do_execute

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/tmp/ipykernel_37779/4021243845.py"", line 8, in <module>

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 51, in train_step

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 901, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 46, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 167, in call

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 258, in _standardize_inputs

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 218, in _convert_inputs_to_tensors

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/core.py"", line 822, in convert_to_tensor

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 132, in convert_to_tensor

Cast string to float is not supported
	 [[{{node functional_5_1/Cast}}]] [Op:__inference_one_step_on_iterator_6125]
```
",jinqishen0725,2024-09-18 17:06:56+00:00,['Venkat6871'],2024-12-24 05:30:31+00:00,,https://github.com/tensorflow/tensorflow/issues/75996,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2362893883, 'issue_id': 2534256309, 'author': 'Venkat6871', 'body': 'Hi @jinqishen0725 ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow v2.15.0 and v2.17.0 and encountered a different issue. Could you please provide the gist with all the dependencies? This will help us analyze the problem more effectively. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6febcaaa9c58d4aa6a08d293feaf9526/75996_tf-2-15-0-2-17-0-v.ipynb) I created here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 20, 6, 1, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362957660, 'issue_id': 2534256309, 'author': 'jinqishen0725', 'body': '@Venkat6871 Oh, my bad that forgot to provide that function for embedding layer. Very sorry for that. It is a short one. \r\n```\r\ndef get_category_encoding_layer(vocab):\r\n    index = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None, oov_token=""[UNK]"")\r\n    encoder = tf.keras.layers.CategoryEncoding(num_tokens=len(vocab) + 1, output_mode=""one_hot"")\r\n    return lambda feature: encoder(index(feature))\r\n```\r\n\r\nI also created the gist ([link](https://colab.research.google.com/drive/1UKvcfstKAgtJK3l7z6f2-rb2A33KDDzF?usp=sharing)) like you mentioned. Hopefully that will be helpful.', 'created_at': datetime.datetime(2024, 9, 20, 6, 49, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370896454, 'issue_id': 2534256309, 'author': 'Venkat6871', 'body': 'Hi **@jinqishen0725** ,\r\nThank you for the provided code snippet. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I am facing the same issue. I also tried it on TensorFlow 2.15.0, but it is not working there either. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/7d3477c2e7a318bf88dba93a0f244fe2/75996_tf-2-15-0-2-17-0-nightly-v.ipynb) for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 24, 10, 36, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371726154, 'issue_id': 2534256309, 'author': 'jinqishen0725', 'body': 'Hi @Venkat6871, I am very sure it can be run under tensorflow 2.15.0. Please see the following [gist](https://colab.research.google.com/drive/1qM_j_ifGl48APNuWFkHvZjNp1UT1COy1?usp=sharing) that only include 2.15.0. For your error in the gist with 2.15.0, I actually cannot reproduce ...', 'created_at': datetime.datetime(2024, 9, 24, 16, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373451487, 'issue_id': 2534256309, 'author': 'Venkat6871', 'body': 'Hi **@jinqishen0725** ,\r\nI tried to run your code on Colab using TF v2.15.0 and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6eb1252802308fb3a0e2c5a78eff4463/75996_tf-2-15-0.ipynb) here for the reference. \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 25, 8, 46, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374673866, 'issue_id': 2534256309, 'author': 'jinqishen0725', 'body': '@Venkat6871 just to confirm, so in your new gist, looks like you did run both codes successfully with 2.15.0, but the same code failed when you upgrade to 2.17.0, is it correct? I don\'t see what is the same issue you faced in your new gist with 2.15.0. \r\n<img width=""780"" alt=""image"" src=""https://github.com/user-attachments/assets/53238bdb-6af3-48ad-bf80-339a74591e26"">', 'created_at': datetime.datetime(2024, 9, 25, 17, 11, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382126861, 'issue_id': 2534256309, 'author': 'Venkat6871', 'body': 'Hi @jinqishen0725 ,\r\nActually, the code you provided is running successfully in TensorFlow v2.15.0. However, in version 2.17.0, I am encountering the same problem you are facing. I apologize for the earlier miscommunication.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 5, 11, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387547861, 'issue_id': 2534256309, 'author': 'jinqishen0725', 'body': 'Thanks @Venkat6871 for confirming on this! In this case, this should be more likely to be a tensorflow package issue I suppose, is it correct?', 'created_at': datetime.datetime(2024, 10, 2, 3, 14, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401432430, 'issue_id': 2534256309, 'author': 'Venkat6871', 'body': 'Hi **@jinqishen0725** ,\r\nThough it is a TensorFlow package, the APIs are related to Keras. Moreover, TensorFlow 2.17.0 includes Keras 3.0, which might be the reason for the error or failure. Therefore, I suggest raising the issue with the Keras team or in the Keras [repository](https://github.com/keras-team/keras/issues) for a quicker resolution.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 9, 6, 33, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402191557, 'issue_id': 2534256309, 'author': 'jinqishen0725', 'body': ""@Venkat6871 So have you checked the issue further? What is the reason you think it is a Keras issue? If you just don't want to look further, can you loop in another person for checking? I just cannot imagine why letting someone looking at it is so hard ..."", 'created_at': datetime.datetime(2024, 10, 9, 12, 30, 42, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-20 06:01:55 UTC): Hi @jinqishen0725 ,
Apologies for the delay. I tried running your code on Colab using TensorFlow v2.15.0 and v2.17.0 and encountered a different issue. Could you please provide the gist with all the dependencies? This will help us analyze the problem more effectively. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6febcaaa9c58d4aa6a08d293feaf9526/75996_tf-2-15-0-2-17-0-v.ipynb) I created here for reference.

Thank you!

jinqishen0725 (Issue Creator) on (2024-09-20 06:49:55 UTC): @Venkat6871 Oh, my bad that forgot to provide that function for embedding layer. Very sorry for that. It is a short one. 
```
def get_category_encoding_layer(vocab):
    index = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None, oov_token=""[UNK]"")
    encoder = tf.keras.layers.CategoryEncoding(num_tokens=len(vocab) + 1, output_mode=""one_hot"")
    return lambda feature: encoder(index(feature))
```

I also created the gist ([link](https://colab.research.google.com/drive/1UKvcfstKAgtJK3l7z6f2-rb2A33KDDzF?usp=sharing)) like you mentioned. Hopefully that will be helpful.

Venkat6871 (Assginee) on (2024-09-24 10:36:10 UTC): Hi **@jinqishen0725** ,
Thank you for the provided code snippet. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I am facing the same issue. I also tried it on TensorFlow 2.15.0, but it is not working there either. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/7d3477c2e7a318bf88dba93a0f244fe2/75996_tf-2-15-0-2-17-0-nightly-v.ipynb) for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

jinqishen0725 (Issue Creator) on (2024-09-24 16:07:17 UTC): Hi @Venkat6871, I am very sure it can be run under tensorflow 2.15.0. Please see the following [gist](https://colab.research.google.com/drive/1qM_j_ifGl48APNuWFkHvZjNp1UT1COy1?usp=sharing) that only include 2.15.0. For your error in the gist with 2.15.0, I actually cannot reproduce ...

Venkat6871 (Assginee) on (2024-09-25 08:46:06 UTC): Hi **@jinqishen0725** ,
I tried to run your code on Colab using TF v2.15.0 and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/6eb1252802308fb3a0e2c5a78eff4463/75996_tf-2-15-0.ipynb) here for the reference. 
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

jinqishen0725 (Issue Creator) on (2024-09-25 17:11:43 UTC): @Venkat6871 just to confirm, so in your new gist, looks like you did run both codes successfully with 2.15.0, but the same code failed when you upgrade to 2.17.0, is it correct? I don't see what is the same issue you faced in your new gist with 2.15.0. 
<img width=""780"" alt=""image"" src=""https://github.com/user-attachments/assets/53238bdb-6af3-48ad-bf80-339a74591e26"">

Venkat6871 (Assginee) on (2024-09-30 05:11:48 UTC): Hi @jinqishen0725 ,
Actually, the code you provided is running successfully in TensorFlow v2.15.0. However, in version 2.17.0, I am encountering the same problem you are facing. I apologize for the earlier miscommunication.
Thank you!

jinqishen0725 (Issue Creator) on (2024-10-02 03:14:37 UTC): Thanks @Venkat6871 for confirming on this! In this case, this should be more likely to be a tensorflow package issue I suppose, is it correct?

Venkat6871 (Assginee) on (2024-10-09 06:33:17 UTC): Hi **@jinqishen0725** ,
Though it is a TensorFlow package, the APIs are related to Keras. Moreover, TensorFlow 2.17.0 includes Keras 3.0, which might be the reason for the error or failure. Therefore, I suggest raising the issue with the Keras team or in the Keras [repository](https://github.com/keras-team/keras/issues) for a quicker resolution.
Thank you!

jinqishen0725 (Issue Creator) on (2024-10-09 12:30:42 UTC): @Venkat6871 So have you checked the issue further? What is the reason you think it is a Keras issue? If you just don't want to look further, can you loop in another person for checking? I just cannot imagine why letting someone looking at it is so hard ...

"
2534242564,issue,closed,completed,Significant difference in Arena Memory requirements between the tflite interpreter and MicroInterpreter,"Greetings,

Thanks for maintaining such a cool project :rocket: 

I am using a tflite model at full precision (FP32) and noticing a significant difference in the arena memory requirement between the interpreter of Tflite and MicroInterpreter of TfliteMicro.

When I use the `PrintInterpreterState()` method in Tflite after allocating the tensors to interpreter (with Tflite cpp), I get the following logs suggesting the memory usage is 0.246 MB

```
-----------Subgraph-0 has 149 tensors and 72 nodes------------
1 Inputs: [0] -> 196608B (0.19MB)
4 Outputs: [125,146,127,148] -> 60928B (0.06MB)
.
.
.
--------------Subgraph-0 dump has completed--------------

--------------Memory Arena Status Start--------------
Total memory usage: 257536 bytes (0.246 MB)
- Total arena memory usage: 257536 bytes (0.246 MB)
- Total dynamic memory usage: 0 bytes (0.000 MB)

Subgraph#0   Arena (Normal)         257536 (100.00%)
--------------Memory Arena Status End--------------
```

But when I try something similar with MicroInterpreter, after allocating the tensors, and logging 
```interpreter->arena_used_bytes()```  I find out its using 1784036 bytes (1.784 MB)

Is there something that I am missing like the PrintInterpreterState() only calculates for the input, output and intermediate weight arrays, while the actual execution require more memory for the intermediate transformations/results for input data ?",abangwal,2024-09-18 16:58:42+00:00,['pkgoogle'],2024-11-09 01:58:38+00:00,2024-11-09 01:58:35+00:00,https://github.com/tensorflow/tensorflow/issues/75995,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue'), ('comp:micro', 'Related to TensorFlow Lite Microcontrollers')]","[{'comment_id': 2360167265, 'issue_id': 2534242564, 'author': 'tilakrayal', 'body': '@abangwal,\r\nCould you please share a reproducible code that supports your statement so that the issue can debug the issue in an effective way? Thank you!', 'created_at': datetime.datetime(2024, 9, 19, 7, 10, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360574647, 'issue_id': 2534242564, 'author': 'abangwal', 'body': 'Hey @tilakrayal ,\r\n\r\nThe model in context in BlazeFace (face_detection_front_128x128), at full precision FP32, I produced the model arena status for tflite interpreter with the [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) given in the tflite official repository,\r\n\r\n```cpp\r\n  // Run inference\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  printf(""\\n\\n=== Post-invoke Interpreter State ===\\n"");\r\n  tflite::PrintInterpreterState(interpreter.get()); //This is printing the Model arena status\r\n```\r\nNow coming on the tflite micro, tho my board is not in the officially supported platforms by the Tflite Micro as in [this list](https://ai.google.dev/edge/litert/microcontrollers/overview#supported_platforms) but the manufacturer have released their own adjusted arduino library for tflite micro ([Ameba_TensorFlowLite.zip](https://github.com/Ameba-AIoT/ameba-arduino-d/tree/master/Arduino_zip_libraries)) \r\n\r\nSo I converted the model to C source file with the help of `convert_bytes_to_c_source()` function from python utils.\r\n\r\nThen here is the code I used to access the `arena_used_bytes()`, its in arduino style\r\n```cpp\r\n#include <TensorFlowLite.h>\r\n\r\n#include ""tensorflow/lite/micro/all_ops_resolver.h""\r\n#include ""tensorflow/lite/micro/micro_error_reporter.h""\r\n#include ""tensorflow/lite/micro/micro_interpreter.h""\r\n#include ""tensorflow/lite/schema/schema_generated.h""\r\n#include ""tensorflow/lite/version.h""\r\n\r\n#include ""BlazeFace32.h""\r\n\r\n// Globals, used for compatibility with Arduino-style sketches.\r\nnamespace {\r\ntflite::ErrorReporter* error_reporter = nullptr;\r\nconst tflite::Model* model = nullptr;\r\ntflite::MicroInterpreter* interpreter = nullptr;\r\nTfLiteTensor* input = nullptr;\r\nTfLiteTensor* output = nullptr;\r\n\r\n// Create an area of memory to use for input, output, and intermediate arrays.\r\n// Minimum arena size, at the time of writing. After allocating tensors\r\n// you can retrieve this value by invoking interpreter.arena_used_bytes().\r\nconst int kModelArenaSize = 2048 * 1024;\r\n// Extra headroom for model + alignment + future interpreter changes.\r\nconst int kExtraArenaSize = 0;\r\nconst int kTensorArenaSize = kModelArenaSize + kExtraArenaSize;\r\nuint8_t tensor_arena[kTensorArenaSize];\r\n}  // namespace\r\n\r\nvoid setup() {\r\n\r\n  // Set up logging. Google style is to avoid globals or statics because of\r\n  // lifetime uncertainty, but since this has a trivial destructor it\'s okay.\r\n  // NOLINTNEXTLINE(runtime-global-variables)\r\n  static tflite::MicroErrorReporter micro_error_reporter;\r\n  error_reporter = &micro_error_reporter;\r\n\r\n  // Map the model into a usable data structure. This doesn\'t involve any\r\n  // copying or parsing, it\'s a very lightweight operation.\r\n  model = tflite::GetModel(BlazeFace32);\r\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n    TF_LITE_REPORT_ERROR(error_reporter,\r\n                         ""Model provided is schema version %d not equal ""\r\n                         ""to supported version %d."",\r\n                         model->version(), TFLITE_SCHEMA_VERSION);\r\n    return;\r\n  }\r\n  else {\r\n  TF_LITE_REPORT_ERROR(error_reporter,\r\n                         ""Model loaded \\n Model Vestion : %d \\n Supported version %d."",\r\n                         model->version(), TFLITE_SCHEMA_VERSION);\r\n  }\r\n\r\n  // This pulls in all the operation implementations we need.\r\n  // NOLINTNEXTLINE(runtime-global-variables)\r\n  static tflite::AllOpsResolver resolver;\r\n\r\n  // Build an interpreter to run the model with.\r\n  static tflite::MicroInterpreter static_interpreter(\r\n      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n  interpreter = &static_interpreter;\r\n\r\n  // Allocate memory from the tensor_arena for the model\'s tensors.\r\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n  if (allocate_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, ""AllocateTensors() failed"");\r\n    return;\r\n  }else{\r\n    TF_LITE_REPORT_ERROR(error_reporter, ""Arena in use : %d"",interpreter->arena_used_bytes());\r\n  }\r\n\r\n  // Obtain pointers to the model\'s input and output tensors.\r\n  input = interpreter->input(0);\r\n  output = interpreter->output(0);\r\n}\r\n\r\nvoid loop() {\r\n}\r\n```\r\nI think this should be enough to reproduce the findings, If not let me know what\'s missing :smile: .', 'created_at': datetime.datetime(2024, 9, 19, 10, 10, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393528110, 'issue_id': 2534242564, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 4, 11, 51, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436460297, 'issue_id': 2534242564, 'author': 'pkgoogle', 'body': 'Hi @abangwal, I believe https://github.com/tensorflow/tflite-micro/issues will be able to help you better.', 'created_at': datetime.datetime(2024, 10, 24, 22, 38, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157608, 'issue_id': 2534242564, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985817, 'issue_id': 2534242564, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985840, 'issue_id': 2534242564, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75995"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75995"">No</a>', 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-09-19 07:10:22 UTC): @abangwal,
Could you please share a reproducible code that supports your statement so that the issue can debug the issue in an effective way? Thank you!

abangwal (Issue Creator) on (2024-09-19 10:10:59 UTC): Hey @tilakrayal ,

The model in context in BlazeFace (face_detection_front_128x128), at full precision FP32, I produced the model arena status for tflite interpreter with the [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) given in the tflite official repository,

```cpp
  // Run inference
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get()); //This is printing the Model arena status
```
Now coming on the tflite micro, tho my board is not in the officially supported platforms by the Tflite Micro as in [this list](https://ai.google.dev/edge/litert/microcontrollers/overview#supported_platforms) but the manufacturer have released their own adjusted arduino library for tflite micro ([Ameba_TensorFlowLite.zip](https://github.com/Ameba-AIoT/ameba-arduino-d/tree/master/Arduino_zip_libraries)) 

So I converted the model to C source file with the help of `convert_bytes_to_c_source()` function from python utils.

Then here is the code I used to access the `arena_used_bytes()`, its in arduino style
```cpp
#include <TensorFlowLite.h>

#include ""tensorflow/lite/micro/all_ops_resolver.h""
#include ""tensorflow/lite/micro/micro_error_reporter.h""
#include ""tensorflow/lite/micro/micro_interpreter.h""
#include ""tensorflow/lite/schema/schema_generated.h""
#include ""tensorflow/lite/version.h""

#include ""BlazeFace32.h""

// Globals, used for compatibility with Arduino-style sketches.
namespace {
tflite::ErrorReporter* error_reporter = nullptr;
const tflite::Model* model = nullptr;
tflite::MicroInterpreter* interpreter = nullptr;
TfLiteTensor* input = nullptr;
TfLiteTensor* output = nullptr;

// Create an area of memory to use for input, output, and intermediate arrays.
// Minimum arena size, at the time of writing. After allocating tensors
// you can retrieve this value by invoking interpreter.arena_used_bytes().
const int kModelArenaSize = 2048 * 1024;
// Extra headroom for model + alignment + future interpreter changes.
const int kExtraArenaSize = 0;
const int kTensorArenaSize = kModelArenaSize + kExtraArenaSize;
uint8_t tensor_arena[kTensorArenaSize];
}  // namespace

void setup() {

  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(BlazeFace32);
  if (model->version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(error_reporter,
                         ""Model provided is schema version %d not equal ""
                         ""to supported version %d."",
                         model->version(), TFLITE_SCHEMA_VERSION);
    return;
  }
  else {
  TF_LITE_REPORT_ERROR(error_reporter,
                         ""Model loaded \n Model Vestion : %d \n Supported version %d."",
                         model->version(), TFLITE_SCHEMA_VERSION);
  }

  // This pulls in all the operation implementations we need.
  // NOLINTNEXTLINE(runtime-global-variables)
  static tflite::AllOpsResolver resolver;

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &static_interpreter;

  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, ""AllocateTensors() failed"");
    return;
  }else{
    TF_LITE_REPORT_ERROR(error_reporter, ""Arena in use : %d"",interpreter->arena_used_bytes());
  }

  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
  output = interpreter->output(0);
}

void loop() {
}
```
I think this should be enough to reproduce the findings, If not let me know what's missing :smile: .

gaikwadrahul8 on (2024-10-04 11:51:24 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-24 22:38:11 UTC): Hi @abangwal, I believe https://github.com/tensorflow/tflite-micro/issues will be able to help you better.

github-actions[bot] on (2024-11-01 02:07:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-09 01:58:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-09 01:58:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75995"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75995"">No</a>

"
2533784820,issue,closed,completed,Program gets stuck every time I try to use the GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow[and-cuda] 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

GRID Virtualized A100 (10240MiB)

### Current behavior?

Given that I am testing using a VM Ware ESXI7 server.
The code simply gets stuck when it tries to use the GPU, even in the simple definition of a neural network.
What I am noticing is that at most 13MiB of GPU memory is assigned for each execution with 0% of GPU usage, while the CPU usage remains at 100%.

### Standalone code to reproduce the issue

```shell
from tensorflow import keras
from keras import layers

nn_clf = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=[76]),
    layers.Dense(64, activation='relu'),
    layers.Dropout(rate=0.3),
    layers.Dense(15, activation='softmax'),
])
```


### Relevant log output

```shell
2024-09-18 15:19:29.718741: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-18 15:19:29.733155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-18 15:19:29.750246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-18 15:19:29.755153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-18 15:19:29.767986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-18 15:19:30.704303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
",santinoamato,2024-09-18 13:39:38+00:00,['Venkat6871'],2024-10-04 02:01:40+00:00,2024-10-04 02:01:37+00:00,https://github.com/tensorflow/tensorflow/issues/75987,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2360010960, 'issue_id': 2533784820, 'author': 'Venkat6871', 'body': 'Hi **@santinoamato** ,\r\nThank you for providing detailed information about your issue. Could you please double-check all the compatibility versions? I tried running your code on Colab using TensorFlow 2.17.0 [GPU], and I did not encounter any issues. Here, I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cabcb9ede3f3a032699f7c9fb539e080/75987_tf-2-17-0-gpu-v.ipynb) for reference. If I am mistaken, please let me know.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 19, 5, 29, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259400, 'issue_id': 2533784820, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646679, 'issue_id': 2533784820, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646723, 'issue_id': 2533784820, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75987"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75987"">No</a>', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 39, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-19 05:29:37 UTC): Hi **@santinoamato** ,
Thank you for providing detailed information about your issue. Could you please double-check all the compatibility versions? I tried running your code on Colab using TensorFlow 2.17.0 [GPU], and I did not encounter any issues. Here, I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cabcb9ede3f3a032699f7c9fb539e080/75987_tf-2-17-0-gpu-v.ipynb) for reference. If I am mistaken, please let me know.
Thank you!

github-actions[bot] on (2024-09-27 02:01:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-04 02:01:37 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-04 02:01:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75987"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75987"">No</a>

"
2533528551,issue,closed,completed,Warning! ***HDF5 library version mismatched error***,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tensorflow-gpu 2.9.0

### Custom code

Yes

### OS platform and distribution

Linux, ubuntu 22.04

### Mobile device

Linux, ubuntu 22.04

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 11.5

### GPU model and memory

_No response_

### Current behavior?

Hello,
I am trying to run my application and check GPU utilization, When trying to import h5py I get the following error. I have seen a lot of ""conda"" commands solving this issue,  but I am not supposed to use conda,  I don't use anaconda python on Ubuntu 22.04. HELP ME SOLVE THIS ISSUE, I HAVE TRIED TO UPDATE THE HDF5 TO LATEST VERSION i.e., 1.14.2 AS INDICATED IN THE WARNING MESSAGE , BUT STILL GET THE SAME WARNING. My LD_LIBRARY_PATH was set to a different location I changed it to ""/usr/local/hdf5/lib:"" but I get the same error.
Any help is greatly appreciated.

### Standalone code to reproduce the issue

```shell
- UserWarning: h5py is running against HDF5 1.10.7 when it was built against 1.14.2, this may cause problems
  _warn((""h5py is running against HDF5 {0} when it was built against {1}, ""
Warning! ***HDF5 library version mismatched error***
The HDF5 header files used to compile this application do not match
the version used by the HDF5 library to which this application is linked.
Data corruption or segmentation faults may occur if the application continues.
This can happen when an application was compiled by one version of HDF5 but
linked with a different version of static or shared HDF5 library.
You should recompile the application or check your shared library related
settings such as 'LD_LIBRARY_PATH'.
You can, at your own risk, disable this warning by setting the environment
variable 'HDF5_DISABLE_VERSION_CHECK' to a value of '1'.
Setting it to 2 or higher will suppress the warning messages totally.
Headers are 1.14.2, library is 1.10.7
            SUMMARY OF THE HDF5 CONFIGURATION
            =================================
```


### Relevant log output

```shell
Headers are 1.14.2, library is 1.10.7                                                                                                                                                   SUMMARY OF THE HDF5 CONFIGURATION                                                                                                                                           =================================                                                                                                                                                                                                                                                                                                           General Information:                                                                                                                                                        -------------------                                                                                                                                                                            HDF5 Version: 1.10.7                                                                                                                                                       Configured on: Wed, 08 Dec 2021 23:33:27 +0000                                                                                                                              Configured by: Debian                                                                                                                                                         Host system: x86_64-pc-linux-gnu                                                                                                                                      Uname information: Debian                                                                                                                                                            Byte sex: little-endian                                                                                                                                           Installation point: /usr   

            Using memory checker: no
 Memory allocation sanity checks: no
          Function stack tracing: no
                Use file locking: best-effort
       Strict file format checks: no
    Optimization instrumentation: no
Bye...
Aborted (core dumped)
```
",abivelu-prem,2024-09-18 11:54:53+00:00,['tilakrayal'],2024-10-16 02:03:01+00:00,2024-10-16 02:02:50+00:00,https://github.com/tensorflow/tensorflow/issues/75983,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2360163861, 'issue_id': 2533528551, 'author': 'tilakrayal', 'body': '@abivelu-prem,\r\nHDF5 library version mismatched error will be resolved with pip install h5py --upgrade --no-dependencies --force. You can ignore the above CUDA warnings if you do not have a GPU set up on your machine.\r\n\r\nPlease can you install h5py as below\r\n\r\n```python\r\n  pip install h5py --upgrade --no-dependencies --force\r\n```\r\n\r\nAlso tensorflow v2.9 is a pretty old version, could you please upgrade to the latest tensorflow v2.17. Thank you!', 'created_at': datetime.datetime(2024, 9, 19, 7, 8, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363504521, 'issue_id': 2533528551, 'author': 'abivelu-prem', 'body': '@tilakrayal  I have tried to install tensorflow with CUDA dependency using ""pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118"" and have uninstalled h5py package by ""pip uninstall h5py"" which prevented the HDF5 warning to pop up on my application, I do have a GPU set up on my machine and I am trying to utilize GPU. But its not utilizing as of now, due to the tensorflow issue I guess. your suggestion would be highly appreciated.', 'created_at': datetime.datetime(2024, 9, 20, 11, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385085197, 'issue_id': 2533528551, 'author': 'tilakrayal', 'body': '@abivelu-prem,\r\nFrom the above comment I can see that you are using to install tensorflow using a pip install torch which might be the reason for the error. Could you please try to install from the tensorflow official document.\r\nhttps://www.tensorflow.org/install/pip#step-by-step_instructions\r\n\r\nAnd also if you are facing issue while using the torch, please raise the issue in the respective forum. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 8, 11, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123324, 'issue_id': 2533528551, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573667, 'issue_id': 2533528551, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 16, 2, 2, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415573831, 'issue_id': 2533528551, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75983"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75983"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 2, 3, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-19 07:08:18 UTC): @abivelu-prem,
HDF5 library version mismatched error will be resolved with pip install h5py --upgrade --no-dependencies --force. You can ignore the above CUDA warnings if you do not have a GPU set up on your machine.

Please can you install h5py as below

```python
  pip install h5py --upgrade --no-dependencies --force
```

Also tensorflow v2.9 is a pretty old version, could you please upgrade to the latest tensorflow v2.17. Thank you!

abivelu-prem (Issue Creator) on (2024-09-20 11:24:29 UTC): @tilakrayal  I have tried to install tensorflow with CUDA dependency using ""pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118"" and have uninstalled h5py package by ""pip uninstall h5py"" which prevented the HDF5 warning to pop up on my application, I do have a GPU set up on my machine and I am trying to utilize GPU. But its not utilizing as of now, due to the tensorflow issue I guess. your suggestion would be highly appreciated.

tilakrayal (Assginee) on (2024-10-01 08:11:57 UTC): @abivelu-prem,
From the above comment I can see that you are using to install tensorflow using a pip install torch which might be the reason for the error. Could you please try to install from the tensorflow official document.
https://www.tensorflow.org/install/pip#step-by-step_instructions

And also if you are facing issue while using the torch, please raise the issue in the respective forum. Thank you!

github-actions[bot] on (2024-10-09 02:01:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-16 02:02:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-16 02:03:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75983"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75983"">No</a>

"
2532505876,issue,closed,completed,Build Error on Windows 11 for TensorFlow r2.17.0 C++,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.5

### Bazel version

6.5.0

### GCC/compiler version

MSVC 2022/CLANG 17.0.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to build a .dll, and dll_import_lib for TensorFlow for C++, but everything has been giving me errors, besides when building with Linux, but the files are not compatible with Windows, which has my preferred IDE, Visual Studios. I have tried both Clang 17.0.6, and MSVC 2022. It when it fails it says something about a package called snappy. I have attached the who console log so y'all can see the whole [error](https://github.com/user-attachments/files/17037138/Log.txt).

### Standalone code to reproduce the issue

```shell
python -m venv venv
venv\scripts\activate

pip3 install -U pip
pip3 install -U six numpy wheel packaging
pip3 install -U keras_preprocessing --no-deps

pacman -Syu
pacman -S git patch unzip
pacman -S git patch unzip rsync

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.17

set PATH=C:\Users\Asher\AppData\Local\Programs\Python\Python312;$env:PATH
set PATH=C:\Users\Asher\AppData\Local\Programs\Python\Python312\scripts;$env:PATH
set PYTHON_BIN_PATH=C:\GameDev\Tensorflow\WIndowsBuild\venv\Scripts\python.exe
set PYTHON_LIB_PATH=C:\GameDev\Tensorflow\WIndowsBuild\venv\lib\site-packages
set PYTHON_DIRECTORY=C:\GameDev\Tensorflow\WIndowsBuild\venv\Scripts

set BAZEL_SH=C:/msys64/usr/bin/bash.exe
set BAZEL_VS=""C:\Program Files\Microsoft Visual Studio\2022\Community\NotCatch""
set BAZEL_VS=""C:\Program Files\Microsoft Visual Studio\2022\Community\NotCatch\VC""
set Bazel_LLVM=C:\Program Files\LLVM
set Bazel_LLVM=C:\Program Files\LLVM\bin;$env:PATH

python ./configure.py (Default for everything)

bazel build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.12 //tensorflow:tensorflow_cc.dll //tensorflow:tensorflow_cc_dll_import_lib
```


### Relevant log output

```shell
(venv) C:\GameDev\Tensorflow\WIndowsBuild\tensorflow>bazel build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.12 //tensorflow:tensorflow_cc.dll //tensorflow:tensorflow_cc_dll_import_lib
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/GameDev/Tensorflow/WIndowsBuild/venv/Scripts/python.exe
INFO: Reading rc options for 'build' from c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from c:\gamedev\tensorflow\windowsbuild\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/GameDev/Tensorflow/WIndowsBuild/venv/scripts/python.exe --action_env PYTHON_LIB_PATH=C:/GameDev/Tensorflow/WIndowsBuild/venv/Lib/site-packages --python_path=C:/GameDev/Tensorflow/WIndowsBuild/venv/scripts/python.exe --action_env CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe --repo_env=CC=C:Program FilesLLVMbinclang.exe --repo_env=BAZEL_COMPILER=C:Program FilesLLVMbinclang.exe --copt=-Wno-gnu-offsetof-extensions --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:win_clang in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --copt=/clang:-Weverything --extra_toolchains=@local_config_cc//:cc-toolchain-x64_windows-clang-cl --extra_execution_platforms=//tensorflow/tools/toolchains/win:x64_windows-clang-cl --host_platform=//tensorflow/tools/toolchains/win:x64_windows-clang-cl --compiler=clang-cl --linkopt=/FORCE:MULTIPLE --host_linkopt=/FORCE:MULTIPLE
INFO: Found applicable config definition build:windows in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --host_copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:no_tfrt in file c:\gamedev\tensorflow\windowsbuild\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: C:/users/asher/_bazel_asher/sfsjrgy5/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.12
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed 2 targets (508 packages loaded, 34733 targets configured).
INFO: Found 2 targets...
ERROR: C:/users/asher/_bazel_asher/sfsjrgy5/external/snappy/BUILD.bazel:89:8: Executing genrule @snappy//:snappy_stubs_public_h failed: (Exit 1): bash.exe failed: error executing command (from target @snappy//:snappy_stubs_public_h)
  cd /d C:/users/asher/_bazel_asher/sfsjrgy5/execroot/org_tensorflow
  SET CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe
    SET PATH=C:\Users\Asher\AppData\Local\Microsoft\WindowsApps;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\GameDev\Tensorflow\WIndowsBuild\venv\Scripts;;C:\Program Files (x86)\Microsoft SDKs\Azure\CLI2\wbin;C:\Program Files\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\Windows Kits\10\Windows Performance Toolkit\;C:\Program Files\Git\cmd;C:\Program Files\NVIDIA Corporation\NVIDIA app\NvDLISR;C:\Users\Asher\AppData\Local\Programs\Python\Python312\Scripts\;C:\Users\Asher\AppData\Local\Programs\Python\Python312\;C:\Users\Asher\AppData\Local\Microsoft\WindowsApps;C:\Users\Asher\.dotnet\tools;c:\msys64\usr\bin;C:\GameDev\Microsoft VS Code\bin;C:\Users\Asher\AppData\Local\Programs\Python;C:\Users\Asher\AppData\Local\GitHubDesktop\bin;C:\GameDev\_PATH;C:\GameDev\_PATH\bazel;C:\Program Files\LLVM\bin;
    SET PYTHON_BIN_PATH=C:/GameDev/Tensorflow/WIndowsBuild/venv/scripts/python.exe
    SET PYTHON_LIB_PATH=C:/GameDev/Tensorflow/WIndowsBuild/venv/Lib/site-packages
    SET TF2_BEHAVIOR=1
  C:\Users\Asher\AppData\Local\Microsoft\WindowsApps\bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; sed -e 's/${\(.*\)_01}/\1/g' -e 's/${SNAPPY_MAJOR}/1/g' -e 's/${SNAPPY_MINOR}/1/g' -e 's/${SNAPPY_PATCHLEVEL}/4/g' external/snappy/snappy-stubs-public.h.in >bazel-out/x64_windows-opt/bin/external/snappy/snappy-stubs-public.h
# Configuration: fb060f0fcae4f016525bb6c2e801b4346ff5ec6f8937e0f1b331d636e4b55663
# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl
/bin/bash: line 1: source external/bazel_tools/tools/genrule/genrule-setup.sh; sed -e 's/${\(.*\)_01}/\1/g' -e 's/${SNAPPY_MAJOR}/1/g' -e 's/${SNAPPY_MINOR}/1/g' -e 's/${SNAPPY_PATCHLEVEL}/4/g' external/snappy/snappy-stubs-public.h.in >bazel-out/x64_windows-opt/bin/external/snappy/snappy-stubs-public.h: bad substitution
INFO: Elapsed time: 470.485s, Critical Path: 6.27s
INFO: 225 processes: 182 internal, 43 local.
FAILED: Build did NOT complete successfully
```
",TheAsherbot,2024-09-18 01:43:43+00:00,['Venkat6871'],2024-12-28 22:51:06+00:00,2024-12-28 22:51:02+00:00,https://github.com/tensorflow/tensorflow/issues/75964,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('type:performance', 'Performance Issue'), ('comp:core', 'issues related to core part of tensorflow'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2376097823, 'issue_id': 2532505876, 'author': 'Venkat6871', 'body': '**@learning-to-play** ,', 'created_at': datetime.datetime(2024, 9, 26, 7, 3, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399759670, 'issue_id': 2532505876, 'author': 'TheAsherbot', 'body': '@learning-to-play or @Venkat6871, Sorry to bother you, but has there been any updates on ways of resolving this error?', 'created_at': datetime.datetime(2024, 10, 8, 12, 50, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400256756, 'issue_id': 2532505876, 'author': 'learning-to-play', 'body': '@mraunak', 'created_at': datetime.datetime(2024, 10, 8, 16, 7, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400505321, 'issue_id': 2532505876, 'author': 'mraunak', 'body': ""Hi @TheAsherbot, Could you please confirm if you've successfully built the above packages using the latest master commit? This will help determine whether the issue lies with the path setup or the TensorFlow 2.17.0 wheels."", 'created_at': datetime.datetime(2024, 10, 8, 18, 6, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400945532, 'issue_id': 2532505876, 'author': 'TheAsherbot', 'body': '@mraunak I was able to reproduce this error on both v2.18.0-rc1, and the Master commit. I updated my version of Clang to 19.1.1, and clang 18.1.8, and I am still getting the same error.', 'created_at': datetime.datetime(2024, 10, 8, 22, 44, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405749463, 'issue_id': 2532505876, 'author': 'mraunak', 'body': 'Hi @TheAsherbot, I am not able to reproduce the above error. However, I am getting a linking error with the above target(still under investigation). Please try the following commands to set up the environment and delete the bazel cache before running the bazel command. Hopefully this should fix the above error.\r\nset BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\r\nset BAZEL_VS=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\r\nset BAZEL_VC=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\r\nset BAZEL_VC_FULL_VERSION=14.39.33519\r\nset BAZEL_LLVM=C:\\Program Files\\LLVM\r\nset PATH=c:\\Tools\\Bazel;C:\\Python312\\Scripts;C:\\msys64;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Git\\usr\\bin;C:\\msys64\\usr\\bin;C:\\Windows\\SysWOW64;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Java\\jre1.8.0_251\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files (x86)\\PowerShell\\6\\;C:\\Windows\\system32\\config\\systemprofile\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files\\LLVM\\bin;%PATH%\r\nset PYTHON_BIN_PATH=C:\\Python312\\python.exe\r\nset PYTHON_LIB_PATH=C:\\Python312\\Lib\r\nset PYTHON_DIRECTORY=C:\\Python312\\Scripts', 'created_at': datetime.datetime(2024, 10, 10, 18, 11, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406004401, 'issue_id': 2532505876, 'author': 'TheAsherbot', 'body': '@mraunak Sadly following those steps did not work. : (   On a 2nd PC I was able to get a linking error mentioned https://github.com/tensorflow/tensorflow/issues/77156. I think these errors are do to something one my PC. I will see if I can resolve the issue because it is only on my machine. Thank you for your help!', 'created_at': datetime.datetime(2024, 10, 10, 20, 43, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564533905, 'issue_id': 2532505876, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75964"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75964"">No</a>', 'created_at': datetime.datetime(2024, 12, 28, 22, 51, 4, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-26 07:03:27 UTC): **@learning-to-play** ,

TheAsherbot (Issue Creator) on (2024-10-08 12:50:19 UTC): @learning-to-play or @Venkat6871, Sorry to bother you, but has there been any updates on ways of resolving this error?

learning-to-play on (2024-10-08 16:07:14 UTC): @mraunak

mraunak on (2024-10-08 18:06:19 UTC): Hi @TheAsherbot, Could you please confirm if you've successfully built the above packages using the latest master commit? This will help determine whether the issue lies with the path setup or the TensorFlow 2.17.0 wheels.

TheAsherbot (Issue Creator) on (2024-10-08 22:44:27 UTC): @mraunak I was able to reproduce this error on both v2.18.0-rc1, and the Master commit. I updated my version of Clang to 19.1.1, and clang 18.1.8, and I am still getting the same error.

mraunak on (2024-10-10 18:11:11 UTC): Hi @TheAsherbot, I am not able to reproduce the above error. However, I am getting a linking error with the above target(still under investigation). Please try the following commands to set up the environment and delete the bazel cache before running the bazel command. Hopefully this should fix the above error.
set BAZEL_SH=C:\msys64\usr\bin\bash.exe
set BAZEL_VS=C:\Program Files\Microsoft Visual Studio\2022\Community
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Community\VC
set BAZEL_VC_FULL_VERSION=14.39.33519
set BAZEL_LLVM=C:\Program Files\LLVM
set PATH=c:\Tools\Bazel;C:\Python312\Scripts;C:\msys64;C:\Program Files\Git\cmd;C:\Program Files\Git\usr\bin;C:\msys64\usr\bin;C:\Windows\SysWOW64;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Java\jre1.8.0_251\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files (x86)\PowerShell\6\;C:\Windows\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files\LLVM\bin;%PATH%
set PYTHON_BIN_PATH=C:\Python312\python.exe
set PYTHON_LIB_PATH=C:\Python312\Lib
set PYTHON_DIRECTORY=C:\Python312\Scripts

TheAsherbot (Issue Creator) on (2024-10-10 20:43:30 UTC): @mraunak Sadly following those steps did not work. : (   On a 2nd PC I was able to get a linking error mentioned https://github.com/tensorflow/tensorflow/issues/77156. I think these errors are do to something one my PC. I will see if I can resolve the issue because it is only on my machine. Thank you for your help!

google-ml-butler[bot] on (2024-12-28 22:51:04 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75964"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75964"">No</a>

"
2532105425,issue,closed,completed,"Tensorflow ""Text generation with an RNN"" tutorial out of date","I was taking a look at the tutorial on generating text with an RNN from the tensorflow website [(https://www.tensorflow.org/text/tutorials/text_generation),](https://www.tensorflow.org/text/tutorials/text_generation) and clicked the option to run on Google Colab. But when I tried running the full code I got overwhelmed with a multitude of errors. I thought I might have been doing something wrong, so I searched online, but found out that you are supposed to be able to just run the code without modification.
I figured it could just be that the way the new versions of tensorflow worked may have changed.

Would be very helpful to me if it could be fixed
many thanks",abisdbest,2024-09-17 20:38:20+00:00,['tilakrayal'],2024-10-06 02:05:25+00:00,2024-10-06 02:05:22+00:00,https://github.com/tensorflow/tensorflow/issues/75945,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:model', 'Model related issues')]","[{'comment_id': 2358690110, 'issue_id': 2532105425, 'author': 'tilakrayal', 'body': '@abisdbest,\r\nThank you for reporting the issue. I observed the same that the code is failing in tensorflow v2.17. Please allow some time to deep dive into the issue. Thank you!', 'created_at': datetime.datetime(2024, 9, 18, 14, 50, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361148432, 'issue_id': 2532105425, 'author': 'tilakrayal', 'body': '@abisdbest,\r\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9ead5c2dda2f3bf649d3bb3d214ca160/text_generation.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 19, 14, 25, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361941723, 'issue_id': 2532105425, 'author': 'abisdbest', 'body': 'thank you very much!', 'created_at': datetime.datetime(2024, 9, 19, 18, 56, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362541033, 'issue_id': 2532105425, 'author': 'tilakrayal', 'body': '@abisdbest,\r\nI have created the PR for the changes required. https://github.com/tensorflow/text/pull/1313\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 9, 20, 1, 40, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380351156, 'issue_id': 2532105425, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 28, 2, 0, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395259997, 'issue_id': 2532105425, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 6, 2, 5, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395260026, 'issue_id': 2532105425, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75945"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75945"">No</a>', 'created_at': datetime.datetime(2024, 10, 6, 2, 5, 24, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-18 14:50:38 UTC): @abisdbest,
Thank you for reporting the issue. I observed the same that the code is failing in tensorflow v2.17. Please allow some time to deep dive into the issue. Thank you!

tilakrayal (Assginee) on (2024-09-19 14:25:59 UTC): @abisdbest,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras

import tf_keras as keras
```

Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9ead5c2dda2f3bf649d3bb3d214ca160/text_generation.ipynb).

Thank you!

abisdbest (Issue Creator) on (2024-09-19 18:56:42 UTC): thank you very much!

tilakrayal (Assginee) on (2024-09-20 01:40:57 UTC): @abisdbest,
I have created the PR for the changes required. https://github.com/tensorflow/text/pull/1313
Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status. Thank you!

github-actions[bot] on (2024-09-28 02:00:49 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-06 02:05:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-06 02:05:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75945"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75945"">No</a>

"
2530744420,issue,closed,completed,Is it possible to avoid configure/configure.py?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6.0 9.3.0

### GPU model and memory

Jetson AGX Orin

### Current behavior?

I'm doing a docker to build automatically tensorflow.
Is it possible to avoid configure that ask to the user for configuration?

### Standalone code to reproduce the issue

```shell
./configure or python3 configure.py
```


### Relevant log output

_No response_",johnnynunez,2024-09-17 10:23:57+00:00,['Venkat6871'],2024-09-19 11:09:43+00:00,2024-09-19 11:09:41+00:00,https://github.com/tensorflow/tensorflow/issues/75911,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2355227756, 'issue_id': 2530744420, 'author': 'johnnynunez', 'body': 'Example jax:\r\nhttps://github.com/google/jax/blob/main/build/build.py\r\n```bash\r\nBUILD_FLAGS=\'--enable_cuda --enable_nccl=False \'\r\nBUILD_FLAGS+=\'--cuda_compute_capabilities=""sm_87"" \'\r\nBUILD_FLAGS+=\'--cuda_version=12.6.0 --cudnn_version=9.4.0 \'\r\nBUILD_FLAGS+=\'--bazel_options=--repo_env=LOCAL_CUDA_PATH=""/usr/local/cuda-12.6"" \'\r\nBUILD_FLAGS+=\'--bazel_options=--repo_env=LOCAL_CUDNN_PATH=""/opt/nvidia/cudnn/"" \'\r\nBUILD_FLAGS+=\'--output_path=/opt/wheels \'\r\n    \r\npython3 build/build.py $BUILD_FLAGS\r\npython3 build/build.py $BUILD_FLAGS --build_gpu_kernel_plugin=cuda --build_gpu_plugin\r\n```', 'created_at': datetime.datetime(2024, 9, 17, 10, 25, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357346672, 'issue_id': 2530744420, 'author': 'johnnynunez', 'body': 'also tensorflow always use python 3.12...\r\nbecause now xla use HERMETIC_PYTHON_VERSION', 'created_at': datetime.datetime(2024, 9, 18, 2, 8, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359974747, 'issue_id': 2530744420, 'author': 'Venkat6871', 'body': 'Hi @johnnynunez ,\r\nApologies for the delay. Yes, please follow [documentation1](https://www.tensorflow.org/install/docker) and [documentation2](https://www.tensorflow.org/install/source) for a smooth installation. Also, kindly double-check all the compatibility versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 19, 4, 53, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360691151, 'issue_id': 2530744420, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75911"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75911"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 11, 9, 42, tzinfo=datetime.timezone.utc)}]","johnnynunez (Issue Creator) on (2024-09-17 10:25:46 UTC): Example jax:
https://github.com/google/jax/blob/main/build/build.py
```bash
BUILD_FLAGS='--enable_cuda --enable_nccl=False '
BUILD_FLAGS+='--cuda_compute_capabilities=""sm_87"" '
BUILD_FLAGS+='--cuda_version=12.6.0 --cudnn_version=9.4.0 '
BUILD_FLAGS+='--bazel_options=--repo_env=LOCAL_CUDA_PATH=""/usr/local/cuda-12.6"" '
BUILD_FLAGS+='--bazel_options=--repo_env=LOCAL_CUDNN_PATH=""/opt/nvidia/cudnn/"" '
BUILD_FLAGS+='--output_path=/opt/wheels '
    
python3 build/build.py $BUILD_FLAGS
python3 build/build.py $BUILD_FLAGS --build_gpu_kernel_plugin=cuda --build_gpu_plugin
```

johnnynunez (Issue Creator) on (2024-09-18 02:08:27 UTC): also tensorflow always use python 3.12...
because now xla use HERMETIC_PYTHON_VERSION

Venkat6871 (Assginee) on (2024-09-19 04:53:18 UTC): Hi @johnnynunez ,
Apologies for the delay. Yes, please follow [documentation1](https://www.tensorflow.org/install/docker) and [documentation2](https://www.tensorflow.org/install/source) for a smooth installation. Also, kindly double-check all the compatibility versions.
Thank you!

google-ml-butler[bot] on (2024-09-19 11:09:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75911"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75911"">No</a>

"
2528154019,issue,closed,completed,tensorflow lite windows c++ build error from source ,"### 1. System information

- OS Platform and Distribution : windows 11
- TensorFlow installation (pip package or built from source): built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): tag-2.16.2

### 2. Code
```
D:\tf\tensorflow>bazel build //tensorflow/lite:tensorflowlite.dll
INFO: Reading 'startup' options from d:\tf\tensorflow\.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\tf\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file d:\tf\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file d:\tf\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:no_tfrt in file d:\tf\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Analyzed target //tensorflow/lite:tensorflowlite.dll (144 packages loaded, 3163 targets configured).
ERROR: D:/tf/tensorflow/tensorflow/lite/schema/BUILD:196:22: declared output 'tensorflow/lite/schema/conversion_metadata_generated.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: D:/tf/tensorflow/tensorflow/lite/schema/BUILD:116:22: declared output 'tensorflow/lite/schema/schema_generated.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: D:/tf/tensorflow/tensorflow/lite/schema/BUILD:116:22: Generating flatbuffer files for schema_fbs_srcs: //tensorflow/lite/schema:schema_fbs_srcs failed: not all outputs were created or valid
ERROR: D:/tf/tensorflow/tensorflow/lite/schema/BUILD:196:22: Generating flatbuffer files for conversion_metadata_fbs_srcs: //tensorflow/lite/schema:conversion_metadata_fbs_srcs failed: not all outputs were created or valid
Target //tensorflow/lite:tensorflowlite.dll failed to build
INFO: Elapsed time: 99.890s, Critical Path: 32.56s
INFO: 436 processes: 5 internal, 431 local.
ERROR: Build did NOT complete successfully
```

I am trying to build tf lite dynamic or static build for windows but getting this error using bazel 7.3.1
I have also tried with the cmake but it is have too much work for now. I want to use the lib or dll in my msvc c++ project. 
I am not familiar with bazel and would like to have some assistance on the same. 
I have look for github, documentions, and stackoverflow for the same but wasn't able to resolve this issue.
",Ashish2000L,2024-09-16 11:06:41+00:00,['gaikwadrahul8'],2024-09-26 07:54:28+00:00,2024-09-26 07:54:24+00:00,https://github.com/tensorflow/tensorflow/issues/75840,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.16', '')]","[{'comment_id': 2353467905, 'issue_id': 2528154019, 'author': 'gaikwadrahul8', 'body': ""Hi, @Ashish2000L\r\n\r\nThank you for bringing this issue to our attention, To conform are you following the below mentioned steps if you're using the GPU ? if you're using only CPU then please make this flag `-DTFLITE_ENABLE_GPU=OFF `or please don't add that flag \r\n\r\nI see you're using the `Bazel version 7.3.1` could you please try with `Bazel version 6.5.0` and see are you able to build successfully or not ? \r\n\r\nIf possible please go with `WSL` option because you will face less errors\r\n\r\n```\r\ngit clone --single-branch --branch r2.17 https://github.com/tensorflow/tensorflow tensorflow_src\r\nmkdir tflite_build_x64\r\ncd tflite_build_x64\r\ncmake -DTFLITE_ENABLE_GPU=ON ..\\tensorflow_src\\tensorflow\\lite\r\ncmake --build . -j 8 --config Release\r\n```\r\n\r\nIf I have missed something here please let me know. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 16, 17, 8, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354431445, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""> Hi, @Ashish2000L\r\n> \r\n> Thank you for bringing this issue to our attention, To conform are you following the below mentioned steps if you're using the GPU ? if you're using only CPU then please make this flag `-DTFLITE_ENABLE_GPU=OFF `or please don't add that flag\r\n> \r\n> I see you're using the `Bazel version 7.3.1` could you please try with `Bazel version 6.5.0` and see are you able to build successfully or not ?\r\n> \r\n> If possible please go with `WSL` option because you will face less errors\r\n> \r\n> ```\r\n> git clone --single-branch --branch r2.17 https://github.com/tensorflow/tensorflow tensorflow_src\r\n> mkdir tflite_build_x64\r\n> cd tflite_build_x64\r\n> cmake -DTFLITE_ENABLE_GPU=ON ..\\tensorflow_src\\tensorflow\\lite\r\n> cmake --build . -j 8 --config Release\r\n> ```\r\n> \r\n> If I have missed something here please let me know.\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nIf I try building this on wsl won't it give me .so file instead of .dll since I want to make it work in my msvc 2017. Also, if possible can we make the static libs instead of dynamic one?"", 'created_at': datetime.datetime(2024, 9, 17, 3, 26, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354447591, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""I have tried using bazel 6.5.0, but after some progress it is showing just loading with no verbose for a long time:\r\n\r\n> D:\\tf\\tensorflow>bazel build //tensorflow/lite:tensorflowlite\r\nINFO: Reading 'startup' options from d:\\tf\\tensorflow\\.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=172\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages --python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file d:\\tf\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\tf\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file d:\\tf\\tensorflow\\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt\r\nINFO: Found applicable config definition build:monolithic in file d:\\tf\\tensorflow\\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\r\nINFO: Found applicable config definition build:no_tfrt in file d:\\tf\\tensorflow\\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\r\nLoading:"", 'created_at': datetime.datetime(2024, 9, 17, 3, 47, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355248407, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': '@gaikwadrahul8 I have downloaded cmake for the windows and tried building the static file using the cmake:- 3.30.0-rc3. \r\n\r\n>cmake -DTFLITE_ENABLE_GPU=OFF -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=Debug -DTFLITE_ENABLE_XNNPACK=ON -A Win32 ../tensorflow/tensorflow/lite\r\n\r\nI have tried adding the flattbuffers.lib , tendorflow-lite.lib and XNNPack.lib in my mscv external dependency folder but getting too many linker errors, can you help me why these linker error are coming. I have built the tensorflow:2.16.2\r\n\r\n> 1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43): warning C4244: \'argument\': conversion from \'SizeT\' to \'size_t\', possible loss of data\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43): warning C4244:         with\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43): warning C4244:         [\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43): warning C4244:             SizeT=unsigned __int64\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43): warning C4244:         ]\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,43):\r\n1>the template instantiation context (the oldest one first) is\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1421,58):\r\n1>\tsee reference to class template instantiation \'flatbuffers::FlatBufferBuilderImpl<true>\' being compiled\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1275,26):\r\n1>\tsee reference to class template instantiation \'flatbuffers::vector_downward<unsigned __int64>\' being compiled\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(269,8):\r\n1>\twhile compiling class template member function \'void flatbuffers::vector_downward<unsigned __int64>::reallocate(size_t)\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(145,49):\r\n1>\t\tsee the first reference to \'flatbuffers::vector_downward<unsigned __int64>::reallocate\' in \'flatbuffers::vector_downward<unsigned __int64>::ensure_space\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(152,19):\r\n1>\t\tsee the first reference to \'flatbuffers::vector_downward<unsigned __int64>::ensure_space\' in \'flatbuffers::vector_downward<unsigned __int64>::make_space\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(214,15):\r\n1>\t\tsee the first reference to \'flatbuffers::vector_downward<unsigned __int64>::make_space\' in \'flatbuffers::vector_downward<unsigned __int64>::fill\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1363,14):\r\n1>\t\tsee the first reference to \'flatbuffers::vector_downward<unsigned __int64>::fill\' in \'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1424,19):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl\' in \'flatbuffers::FlatBufferBuilderImpl<true>::CreateString\'\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,33): warning C4244: \'argument\': conversion from \'SizeT\' to \'size_t\', possible loss of data\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,33): warning C4244:         with\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,33): warning C4244:         [\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,33): warning C4244:             SizeT=unsigned __int64\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\vector_downward.h(278,33): warning C4244:         ]\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23): warning C4244: \'argument\': conversion from \'T\' to \'const size_t\', possible loss of data\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23): warning C4244:         with\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23): warning C4244:         [\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23): warning C4244:             T=flatbuffers::uoffset64_t\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23): warning C4244:         ]\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(265,23):\r\n1>the template instantiation context (the oldest one first) is\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(327,10):\r\n1>\tsee reference to function template instantiation \'size_t flatbuffers::Verifier::VerifyOffset<flatbuffers::uoffset64_t,flatbuffers::soffset64_t>(const size_t) const\' being compiled\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(266,12): warning C4244: \'return\': conversion from \'const T\' to \'size_t\', possible loss of data\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(266,12): warning C4244:         with\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(266,12): warning C4244:         [\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(266,12): warning C4244:             T=flatbuffers::uoffset64_t\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\verifier.h(266,12): warning C4244:         ]\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1405,39): warning C4244: \'=\': conversion from \'unsigned __int64\' to \'size_t\', possible loss of data\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1405,39):\r\n1>the template instantiation context (the oldest one first) is\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1426,7):\r\n1>\tsee reference to function template instantiation \'unsigned __int64 flatbuffers::FlatBufferBuilderImpl<true>::CalculateOffset<flatbuffers::Offset64<flatbuffers::String>::offset_type>(void)\' being compiled\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1425,3):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::CalculateOffset\' in \'flatbuffers::FlatBufferBuilderImpl<true>::CreateString\'\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(523,38): warning C4244: \'argument\': conversion from \'unsigned __int64\' to \'size_t\', possible loss of data\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(523,38):\r\n1>the template instantiation context (the oldest one first) is\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(520,8):\r\n1>\twhile compiling class template member function \'void flatbuffers::FlatBufferBuilderImpl<true>::PreAlign(size_t,size_t)\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(687,13):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::PreAlign\' in \'flatbuffers::FlatBufferBuilderImpl<true>::StartVector\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1438,37):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::StartVector\' in \'flatbuffers::FlatBufferBuilderImpl<true>::StartVector\'\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37): warning C4244: \'argument\': conversion from \'SizeT\' to \'size_t\', possible loss of data\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37): warning C4244:         with\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37): warning C4244:         [\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37): warning C4244:             SizeT=unsigned __int64\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37): warning C4244:         ]\r\n1>(compiling source file \'main.cpp\')\r\n1>D:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(295,37):\r\n1>the template instantiation context (the oldest one first) is\r\n1>\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(293,8):\r\n1>\twhile compiling class template member function \'void flatbuffers::FlatBufferBuilderImpl<true>::Align(size_t)\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(316,10):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::Align\' in \'flatbuffers::FlatBufferBuilderImpl<true>::PushElement\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1365,16):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::PushElement\' in \'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl\'\r\n1>\t\tD:\\testWork\\visualCodes\\aadhartensor\\dependencies\\include\\flatbuffers\\flatbuffer_builder.h(1424,19):\r\n1>\t\tsee the first reference to \'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl\' in \'flatbuffers::FlatBufferBuilderImpl<true>::CreateString\'\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ostream<char,struct std::char_traits<char> >::_Osfx(void)"" (?_Osfx@?$basic_ostream@DU?$char_traits@D@std@@@std@@QAEXXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: class std::basic_ostream<char,struct std::char_traits<char> > & __thiscall std::basic_ostream<char,struct std::char_traits<char> >::flush(void)"" (?flush@?$basic_ostream@DU?$char_traits@D@std@@@std@@QAEAAV12@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ios<char,struct std::char_traits<char> >::setstate(int,bool)"" (?setstate@?$basic_ios@DU?$char_traits@D@std@@@std@@QAEXH_N@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::~basic_streambuf<char,struct std::char_traits<char> >(void)"" (??1?$basic_streambuf@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gnavail(void)const "" (?_Gnavail@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBE_JXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pnavail(void)const "" (?_Pnavail@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBE_JXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: void __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Init(void)"" (?_Init@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEXXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::xsgetn(char *,__int64)"" (?xsgetn@?$basic_streambuf@DU?$char_traits@D@std@@@std@@MAE_JPAD_J@Z) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::xsputn(char const *,__int64)"" (?xsputn@?$basic_streambuf@DU?$char_traits@D@std@@@std@@MAE_JPBD_J@Z) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::char_traits<char> >(void)"" (??1?$basic_ios@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __thiscall std::basic_ios<char,struct std::char_traits<char> >::basic_ios<char,struct std::char_traits<char> >(void)"" (??0?$basic_ios@DU?$char_traits@D@std@@@std@@IAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ostream<char,struct std::char_traits<char> >::~basic_ostream<char,struct std::char_traits<char> >(void)"" (??1?$basic_ostream@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_istream<char,struct std::char_traits<char> >::~basic_istream<char,struct std::char_traits<char> >(void)"" (??1?$basic_istream@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)\r\n1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(locale0.obj) : error LNK2005: ""void __cdecl std::_Facet_Register(class std::_Facet_base *)"" (?_Facet_Register@std@@YAXPAV_Facet_base@1@@Z) already defined in msvcprtd.lib(locale0_implib.obj)\r\n1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Getgloballocale(void)"" (?_Getgloballocale@locale@std@@CAPAV_Locimp@12@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Init(bool)"" (?_Init@locale@std@@CAPAV_Locimp@12@_N@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_ctor(class std::_Locinfo *,char const *)"" (?_Locinfo_ctor@_Locinfo@std@@SAXPAV12@PBD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_dtor(class std::_Locinfo *)"" (?_Locinfo_dtor@_Locinfo@std@@SAXPAV12@@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: static unsigned int __cdecl std::codecvt<char,char,struct _Mbstatet>::_Getcat(class std::locale::facet const * *,class std::locale const *)"" (?_Getcat@?$codecvt@DDU_Mbstatet@@@std@@SAIPAPBVfacet@locale@2@PBV42@@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gninc(void)"" (?_Gninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pninc(void)"" (?_Pninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::gptr(void)const "" (?gptr@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gninc(void)"" (?_Gninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pninc(void)"" (?_Pninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::gptr(void)const "" (?gptr@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::_Lockit(int)"" (??0_Lockit@std@@QAE@H@Z) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::~_Lockit(void)"" (??1_Lockit@std@@QAE@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)\r\n1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for \'_ITERATOR_DEBUG_LEVEL\': value \'0\' doesn\'t match value \'2\' in main.obj\r\n1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for \'RuntimeLibrary\': value \'MT_StaticRelease\' doesn\'t match value \'MDd_DynamicDebug\' in main.obj\r\n1>LINK : warning LNK4098: defaultlib \'MSVCRTD\' conflicts with use of other libs; use /NODEFAULTLIB:library\r\n1>MSVCRT.lib(initializers.obj) : warning LNK4098: defaultlib \'libcmt.lib\' conflicts with use of other libs; use /NODEFAULTLIB:library\r\n1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>main.obj : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter\r\n1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>main.obj : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport\r\n1>tensorflow-lite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::ScopedSuppressDenormals(void)"" (??0ScopedSuppressDenormals@ruy@@QAE@XZ) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@Interpreter@impl@tflite@@QAE?AW4TfLiteStatus@@XZ)\r\n1>tensorflow-lite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals(void)"" (??1ScopedSuppressDenormals@ruy@@QAE@XZ) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@Interpreter@impl@tflite@@QAE?AW4TfLiteStatus@@XZ)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2019: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z) referenced in function ""public: static void __cdecl ruy::detail::ApplyMultiplierImpl<int,signed char,1>::Run(class ruy::MulParams<int,signed char> const &,int,int *)"" (?Run@?$ApplyMultiplierImpl@HC$00@detail@ruy@@SAXABV?$MulParams@HC@3@HPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2019: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z) referenced in function ""public: static void __cdecl ruy::PackImpl<64,struct ruy::FixedKernelLayout<0,4,16>,short,short,int,0>::Run(enum ruy::Tuning,struct ruy::Mat<short> const &,struct ruy::PMat<short> *,int,int)"" (?Run@?$PackImpl@$0EA@U?$FixedKernelLayout@$0A@$03$0BA@@ruy@@FFH$0A@@ruy@@SAXW4Tuning@2@ABU?$Mat@F@2@PAU?$PMat@F@2@HH@Z)\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z)\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z)\r\n1>tensorflow-lite.lib(lsh_projection.obj) : error LNK2019: unresolved external symbol ""unsigned __int64 __cdecl util::Fingerprint64(char const *,unsigned int)"" (?Fingerprint64@util@@YA_KPBDI@Z) referenced in function ""int __cdecl tflite::ops::builtin::lsh_projection::RunningSignBit(struct TfLiteTensor const *,struct TfLiteTensor const *,float)"" (?RunningSignBit@lsh_projection@builtin@ops@tflite@@YAHPBUTfLiteTensor@@0M@Z)\r\n1>tensorflow-lite.lib(rfft2d.obj) : error LNK2019: unresolved external symbol _rdft2d referenced in function ""void __cdecl tflite::ops::builtin::rfft2d::Rfft2dImpl(int,int,double * *,int *,double *)"" (?Rfft2dImpl@rfft2d@builtin@ops@tflite@@YAXHHPAPANPAHPAN@Z)\r\n1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg\r\n1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg\r\n1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg\r\n1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_create referenced in function ""public: struct pthreadpool * __thiscall tflite::CpuBackendContext::get_xnnpack_threadpool(void)"" (?get_xnnpack_threadpool@CpuBackendContext@tflite@@QAEPAUpthreadpool@@XZ)\r\n1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_create\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_destroy referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@tflite@@QAE@XZ)\r\n1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_destroy\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::Context(void)"" (??0Context@ruy@@QAE@XZ) referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@tflite@@QAE@XZ)\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::~Context(void)"" (??1Context@ruy@@QAE@XZ) referenced in function ""public: void * __thiscall ruy::Context::`scalar deleting destructor\'(unsigned int)"" (??_GContext@ruy@@QAEPAXI@Z)\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::set_max_num_threads(int)"" (?set_max_num_threads@Context@ruy@@QAEXH@Z) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::SetMaxNumThreads(int)"" (?SetMaxNumThreads@CpuBackendContext@tflite@@UAEXH@Z)\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::ClearPrepackedCache(void)"" (?ClearPrepackedCache@Context@ruy@@QAEXXZ) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::ClearCaches(void)"" (?ClearCaches@CpuBackendContext@tflite@@UAEXXZ)\r\n1>tensorflow-lite.lib(spectrogram.obj) : error LNK2019: unresolved external symbol _rdft referenced in function ""private: void __thiscall tflite::internal::Spectrogram::ProcessCoreFFT(void)"" (?ProcessCoreFFT@Spectrogram@internal@tflite@@AAEXXZ)\r\n1>XNNPACK.lib(hardware-config.obj) : error LNK2019: unresolved external symbol _cpuinfo_initialize referenced in function _xnn_init_hardware_config\r\n1>XNNPACK.lib(hardware-config.obj) : error LNK2001: unresolved external symbol _cpuinfo_isa\r\n1>XNNPACK.lib(resize-bilinear-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(prelu-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(batch-matrix-multiply-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(lut-elementwise-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(fully-connected-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(argmax-pooling-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(binary-elementwise-nd.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(resize-bilinear-nchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(global-average-pooling-ncw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(global-average-pooling-nwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(average-pooling-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(dynamic-fully-connected-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(unary-elementwise-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(convolution-nchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(convolution-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(deconvolution-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_with_thread referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_tile_1d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_with_thread referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_1d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d_with_thread referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d_tile_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d_tile_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_6d_tile_2d referenced in function _xnn_run_operator_with_index\r\n1>XNNPACK.lib(gemm-config.obj) : error LNK2019: unresolved external symbol _cpuinfo_get_core referenced in function _init_f32_gemm_config\r\n1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___free_dbg referenced in function ""public: static void __cdecl std::_Fac_node::operator delete(void *)"" (??3_Fac_node@std@@SAXPAX@Z)\r\n1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___malloc_dbg referenced in function ""public: static void * __cdecl std::_Fac_node::operator new(unsigned int)"" (??2_Fac_node@std@@SAPAXI@Z)\r\n\r\n\r\nbelow is my main code that I am using to predict something from my pre-existing model:\r\n\r\n`#include <iostream>\r\n#include <tensorflow/lite/interpreter.h>\r\n#include <tensorflow/lite/model.h>\r\n#include <tensorflow/lite/kernels/register.h>\r\n\r\n\r\nint main() {\r\n\r\n\ttflite::StderrReporter error_reporter;\r\n\tstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\r\n\tstd::unique_ptr<tflite::impl::FlatBufferModel> model = tflite::impl::FlatBufferModel::BuildFromFile(""model.tflite"", &error_reporter);\r\n\r\n\ttflite::ops::builtin::BuiltinOpResolver resolver;\r\n\ttflite::InterpreterBuilder builder(*model, resolver);\r\n\tbuilder(&interpreter);\r\n\r\n\tif (!interpreter) {\r\n\t\tstd::cerr << ""Failed to create interpreter"" << std::endl;\r\n\t\treturn -1;\r\n\t}\r\n\r\n\tTfLiteStatus status = interpreter->AllocateTensors();\r\n\r\n\tif (status != kTfLiteOk) {\r\n\t\tstd::cerr << ""Failed to allocate tensors.""<<status << std::endl;\r\n\t\treturn -1;\r\n\t}\r\nreturn 0;\r\n}`', 'created_at': datetime.datetime(2024, 9, 17, 10, 32, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355368560, 'issue_id': 2528154019, 'author': 'qbe', 'body': 'please wrap all your error messages in\r\n\r\n```\r\ncode blocks\r\nlike this\r\n```\r\n\r\nto avoid notifying users whose usernames appear in your error message', 'created_at': datetime.datetime(2024, 9, 17, 11, 10, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355433610, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""> please wrap all your error messages in\r\n> \r\n> ```\r\n> code blocks\r\n> like this\r\n> ```\r\n> \r\n> to avoid notifying users whose usernames appear in your error message\r\n\r\napologies, i'll do the same next time thanks"", 'created_at': datetime.datetime(2024, 9, 17, 11, 32, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355572217, 'issue_id': 2528154019, 'author': 'gaikwadrahul8', 'body': 'Hi, @Ashish2000L\r\n\r\nThank you for the detailed error log, could you please refer this [stackoverflow answer](https://stackoverflow.com/questions/71556391/tensorflow-static-c-api-library-how-to-link-with-10-sub-dependencies) which is for C++ build and please follow the instructions mentioned in this [video](https://www.youtube.com/watch?v=1IhMISYvZG0) carefully which will help you to solve your issue\r\n\r\nIf issue still persists after following the video instructions and stackoverflow answer steps please help us with error log to investigate this issue further\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 17, 12, 19, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357535836, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""```\r\n1>tensorflow-lite.lib(interpreter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(model_builder.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(register.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(subgraph.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(signature_runner.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(metadata_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(external_cpu_backend_context.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(root_profiler.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mmap_allocation_disabled.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(allocation.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(op_resolver.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(flatbuffer_conversions.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(platform_profiler.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(schema_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(elementwise.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(add.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(add_n.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(assign_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(atan2.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(pooling.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(batch_to_space_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(bidirectional_sequence_lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(bidirectional_sequence_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(broadcast_args.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(broadcast_to.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(bucketize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(call_once.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(cast.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(ceil.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(complex_support.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(concatenation.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(conv3d.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(cumsum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(densify.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(depth_to_space.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(dequantize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(div.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(activations.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(embedding_lookup.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(embedding_lookup_sparse.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(comparisons.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(exp.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(expand_dims.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(fake_quant.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(fill.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(floor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(floor_div.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(floor_mod.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(fully_connected.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(gather.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(gather_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(hashtable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(hashtable_find.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(hashtable_import.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(hashtable_size.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(if.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(l2norm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(local_response_norm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(logical.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(lsh_projection.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(matrix_diag.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(matrix_set_diag.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(maximum_minimum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(reduce.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(neg.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(one_hot.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(pack.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(pow.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(quantize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(random_ops.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(range.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(rank.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(read_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(reshape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(resize_bilinear.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(resize_nearest_neighbor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(reverse_sequence.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(reverse.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(rfft2d.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(basic_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(round.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(segment_sum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(select.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(shape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(sign.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(skip_gram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(space_to_batch_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(space_to_depth.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(split.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(split_v.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(squared_difference.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(squeeze.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(strided_slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(sub.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(svdf.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(tile.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(topk_v2.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(transpose.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(unidirectional_sequence_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(unique.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(unpack.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(unsorted_segment.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(var_handle.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(where.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(while.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(zeros_like.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(bitcast.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(bitwise_xor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(right_shift.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(dilate.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(tflite_with_xnnpack_optional.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(array.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(graph_info.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(arena_planner.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(runtime_shape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(common.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(kernel_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(resource_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(cpu_backend_gemm_eigen.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(transpose_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(kernel_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(initialization_status.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(eigen_support.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(comparisons.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(string_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(rng_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(stablehlo_elementwise.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(spectrogram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj\r\n```\r\n\r\n@gaikwadrahul8 \r\nI am also seeing these runtimelibrary missmatch errors as well after including all the libs also some other issues."", 'created_at': datetime.datetime(2024, 9, 18, 5, 32, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357744434, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""@gaikwadrahul8 after watching video steps I am getting error working with bazel and working on latest tensorflow r2.17 branch. \r\n\r\n```\r\nD:\\tf\\tensorflow>bazel --output_user_root=D:/bazel/bazel_temp_tensor build -c opt //tensorflow/lite:tensorflowlite\r\nINFO: Reading 'startup' options from d:\\tf\\tensorflow\\.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from d:\\tf\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env CLANG_COMPILER_PATH=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --repo_env=CC=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --repo_env=BAZEL_COMPILER=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --copt=-Wno-gnu-offsetof-extensions --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file d:\\tf\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\tf\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file d:\\tf\\tensorflow\\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt\r\nINFO: Found applicable config definition build:monolithic in file d:\\tf\\tensorflow\\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\r\nINFO: Found applicable config definition build:no_tfrt in file d:\\tf\\tensorflow\\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\r\nINFO: Analyzed target //tensorflow/lite:tensorflowlite (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: D:/tf/tensorflow/tensorflow/lite/core/c/BUILD:324:38: Compiling tensorflow/lite/core/c/common.cc failed: (Exit 2): cl.exe failed: error executing command (from target //tensorflow/lite/core/c:common)\r\n  cd /d D:/bazel/bazel_temp_tensor/7ekuv3jw/execroot/org_tensorflow\r\n  SET CLANG_COMPILER_PATH=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe\r\n    SET INCLUDE=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.38.33130\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.38.33130\\ATLMFC\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\r\n    SET PATH=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.38.33130\\bin\\HostX64\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Team Tools\\Performance Tools;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Team Tools\\DiagnosticsHub\\Collector;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22621.0\\\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\\\MSBuild\\Current\\Bin\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\Llvm\\x64\\bin;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\vcpkg\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages\r\n    SET TEMP=C:\\Users\\<USER>\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\<USER>\\AppData\\Local\\Temp\r\n    SET VSLANG=1033\r\n  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.38.33130\\bin\\HostX64\\x64\\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/core/c/_objs/common/common.obj.params\r\n# Configuration: 34ffb9d4dbc148eb4c3a1733abdc4d1230e52a1bb28afd53a7c6ad5f7415fe5c\r\n# Execution platform: @local_execution_config_platform//:platform\r\ncl : Command line error D8021 : invalid numeric argument '/Wno-gnu-offsetof-extensions'\r\nTarget //tensorflow/lite:tensorflowlite failed to build\r\nINFO: Elapsed time: 1.519s, Critical Path: 0.44s\r\nINFO: 55 processes: 55 internal.\r\nFAILED: Build did NOT complete successfully\r\n```"", 'created_at': datetime.datetime(2024, 9, 18, 7, 46, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358739103, 'issue_id': 2528154019, 'author': 'gaikwadrahul8', 'body': ""Hi, @Ashish2000L \r\n\r\nThank you for providing the error log, May I know which bazel version are you using ? if you're not using bazel version `6.5.0 `then please try with it and also downgrade the python version to `3.11` or `3.10` and see is it working as expected or not ? Thank you."", 'created_at': datetime.datetime(2024, 9, 18, 15, 8, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358769552, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""> Hi, @Ashish2000L\r\n> \r\n> Thank you for providing the error log, May I know which bazel version are you using ? if you're not using bazel version `6.5.0 `then please try with it and also downgrade the python version to `3.11` or `3.10` and see is it working as expected or not ? Thank you.\r\n\r\n@gaikwadrahul8 \r\nI have used python 3.11 and bazel 6.50 to build the name and then getting the same error log as shared in last conversation."", 'created_at': datetime.datetime(2024, 9, 18, 15, 21, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359976343, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': 'its really typical to build tf in windows directly so i have built tflite .so file in wsl using \r\n```\r\nbazel --output_user_root=/home/<USER>/bazel_out build --config=monolithic -c dbg //tensorflow/lite:tensorflowlite\r\n```\r\n@gaikwadrahul8 can you specify how we can get the include files as the documention says we have to extract the same from ourself from the source, but it will be best if you can specify what folders I need to extract so that I can directly link them without any issue of linking, or you can share some docs that specify paths that need to be extracted according to r2.17 branch.', 'created_at': datetime.datetime(2024, 9, 19, 4, 55, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360693112, 'issue_id': 2528154019, 'author': 'gaikwadrahul8', 'body': ""Hi, @Ashish2000L\r\n\r\nI believe you're able to see built library `libtensorflowlite.so` at this location :`<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/lite/libtensorflowlite.so` so as far I know you'll have to copy the `tensorflow` folder from this path `<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/` and you'll have to add `flatbuffers` folder either by cloning this repo :https://github.com/google/flatbuffers or download zip file from [here](https://github.com/google/flatbuffers/releases/tag/v24.3.25) and unzip it keep `flatbuffers` folder under the `include` folder in the C++ project \r\n\r\nCould you please give it try and see is it working as expected or not ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 19, 11, 10, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360768664, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': ""> Hi, @Ashish2000L\r\n> \r\n> I believe you're able to see built library `libtensorflowlite.so` at this location :`<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/lite/libtensorflowlite.so` so as far I know you'll have to copy the `tensorflow` folder from this path `<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/` and you'll have to add `flatbuffers` folder either by cloning this repo :https://github.com/google/flatbuffers or download zip file from [here](https://github.com/google/flatbuffers/releases/tag/v24.3.25) and unzip it keep `flatbuffers` folder under the `include` folder in the C++ project\r\n> \r\n> Could you please give it try and see is it working as expected or not ?\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\n@gaikwadrahul8 \r\nthe path you specified is not having main header files like interpreter.h, model.h etc. so I am thinking of extracting all the header files that are available in <root path>/tensorflow/lite folder. Is it the right practice for the same, will it give any linking error? Also I have cloned the flatbuffers and in the same I found the include folder and I have added the same include folder only in my include folder."", 'created_at': datetime.datetime(2024, 9, 19, 11, 48, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360816475, 'issue_id': 2528154019, 'author': 'gaikwadrahul8', 'body': 'Hi, @Ashish2000L \r\n\r\nCould you please refer this [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal) with [tensorflow/lite/examples/minimal/CMakeLists.txt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt) there is similar issue which got resolved by using the `CMakeLists.txt` from minimal example refer that issue https://github.com/tensorflow/tensorflow/issues/60779 which may help you to solve your issue.\r\n\r\nThank you.', 'created_at': datetime.datetime(2024, 9, 19, 12, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376195740, 'issue_id': 2528154019, 'author': 'Ashish2000L', 'body': 'thanks, it resolved the issue.', 'created_at': datetime.datetime(2024, 9, 26, 7, 54, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376195801, 'issue_id': 2528154019, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75840"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75840"">No</a>', 'created_at': datetime.datetime(2024, 9, 26, 7, 54, 26, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-16 17:08:34 UTC): Hi, @Ashish2000L

Thank you for bringing this issue to our attention, To conform are you following the below mentioned steps if you're using the GPU ? if you're using only CPU then please make this flag `-DTFLITE_ENABLE_GPU=OFF `or please don't add that flag 

I see you're using the `Bazel version 7.3.1` could you please try with `Bazel version 6.5.0` and see are you able to build successfully or not ? 

If possible please go with `WSL` option because you will face less errors

```
git clone --single-branch --branch r2.17 https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_build_x64
cd tflite_build_x64
cmake -DTFLITE_ENABLE_GPU=ON ..\tensorflow_src\tensorflow\lite
cmake --build . -j 8 --config Release
```

If I have missed something here please let me know. 

Thank you for your cooperation and patience.

Ashish2000L (Issue Creator) on (2024-09-17 03:26:16 UTC): If I try building this on wsl won't it give me .so file instead of .dll since I want to make it work in my msvc 2017. Also, if possible can we make the static libs instead of dynamic one?

Ashish2000L (Issue Creator) on (2024-09-17 03:47:44 UTC): I have tried using bazel 6.5.0, but after some progress it is showing just loading with no verbose for a long time:

INFO: Reading 'startup' options from d:\tf\tensorflow\.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=172
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages --python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\tf\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file d:\tf\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file d:\tf\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:no_tfrt in file d:\tf\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
Loading:

Ashish2000L (Issue Creator) on (2024-09-17 10:32:33 UTC): @gaikwadrahul8 I have downloaded cmake for the windows and tried building the static file using the cmake:- 3.30.0-rc3. 


I have tried adding the flattbuffers.lib , tendorflow-lite.lib and XNNPack.lib in my mscv external dependency folder but getting too many linker errors, can you help me why these linker error are coming. I have built the tensorflow:2.16.2

1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         with
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         [
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:             SizeT=unsigned __int64
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         ]
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43):
1>the template instantiation context (the oldest one first) is
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1421,58):
1>	see reference to class template instantiation 'flatbuffers::FlatBufferBuilderImpl<true>' being compiled
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1275,26):
1>	see reference to class template instantiation 'flatbuffers::vector_downward<unsigned __int64>' being compiled
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(269,8):
1>	while compiling class template member function 'void flatbuffers::vector_downward<unsigned __int64>::reallocate(size_t)'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(145,49):
1>		see the first reference to 'flatbuffers::vector_downward<unsigned __int64>::reallocate' in 'flatbuffers::vector_downward<unsigned __int64>::ensure_space'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(152,19):
1>		see the first reference to 'flatbuffers::vector_downward<unsigned __int64>::ensure_space' in 'flatbuffers::vector_downward<unsigned __int64>::make_space'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(214,15):
1>		see the first reference to 'flatbuffers::vector_downward<unsigned __int64>::make_space' in 'flatbuffers::vector_downward<unsigned __int64>::fill'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1363,14):
1>		see the first reference to 'flatbuffers::vector_downward<unsigned __int64>::fill' in 'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1424,19):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl' in 'flatbuffers::FlatBufferBuilderImpl<true>::CreateString'
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244: 'argument': conversion from 'SizeT' to 'size_t', possible loss of data
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         with
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         [
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:             SizeT=unsigned __int64
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         ]
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244: 'argument': conversion from 'T' to 'const size_t', possible loss of data
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         with
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         [
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:             T=flatbuffers::uoffset64_t
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         ]
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23):
1>the template instantiation context (the oldest one first) is
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(327,10):
1>	see reference to function template instantiation 'size_t flatbuffers::Verifier::VerifyOffset<flatbuffers::uoffset64_t,flatbuffers::soffset64_t>(const size_t) const' being compiled
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244: 'return': conversion from 'const T' to 'size_t', possible loss of data
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         with
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         [
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:             T=flatbuffers::uoffset64_t
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         ]
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1405,39): warning C4244: '=': conversion from 'unsigned __int64' to 'size_t', possible loss of data
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1405,39):
1>the template instantiation context (the oldest one first) is
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1426,7):
1>	see reference to function template instantiation 'unsigned __int64 flatbuffers::FlatBufferBuilderImpl<true>::CalculateOffset<flatbuffers::Offset64<flatbuffers::String>::offset_type>(void)' being compiled
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1425,3):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::CalculateOffset' in 'flatbuffers::FlatBufferBuilderImpl<true>::CreateString'
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(523,38): warning C4244: 'argument': conversion from 'unsigned __int64' to 'size_t', possible loss of data
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(523,38):
1>the template instantiation context (the oldest one first) is
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(520,8):
1>	while compiling class template member function 'void flatbuffers::FlatBufferBuilderImpl<true>::PreAlign(size_t,size_t)'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(687,13):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::PreAlign' in 'flatbuffers::FlatBufferBuilderImpl<true>::StartVector'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1438,37):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::StartVector' in 'flatbuffers::FlatBufferBuilderImpl<true>::StartVector'
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244: 'argument': conversion from 'SizeT' to 'size_t', possible loss of data
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         with
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         [
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:             SizeT=unsigned __int64
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         ]
1>(compiling source file 'main.cpp')
1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37):
1>the template instantiation context (the oldest one first) is
1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(293,8):
1>	while compiling class template member function 'void flatbuffers::FlatBufferBuilderImpl<true>::Align(size_t)'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(316,10):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::Align' in 'flatbuffers::FlatBufferBuilderImpl<true>::PushElement'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1365,16):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::PushElement' in 'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl'
1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1424,19):
1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl<true>::CreateStringImpl' in 'flatbuffers::FlatBufferBuilderImpl<true>::CreateString'
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ostream<char,struct std::char_traits<char> >::_Osfx(void)"" (?_Osfx@?$basic_ostream@DU?$char_traits@D@std@@@std@@QAEXXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: class std::basic_ostream<char,struct std::char_traits<char> > & __thiscall std::basic_ostream<char,struct std::char_traits<char> >::flush(void)"" (?flush@?$basic_ostream@DU?$char_traits@D@std@@@std@@QAEAAV12@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ios<char,struct std::char_traits<char> >::setstate(int,bool)"" (?setstate@?$basic_ios@DU?$char_traits@D@std@@@std@@QAEXH_N@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::~basic_streambuf<char,struct std::char_traits<char> >(void)"" (??1?$basic_streambuf@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gnavail(void)const "" (?_Gnavail@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBE_JXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pnavail(void)const "" (?_Pnavail@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBE_JXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: void __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Init(void)"" (?_Init@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEXXZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::xsgetn(char *,__int64)"" (?xsgetn@?$basic_streambuf@DU?$char_traits@D@std@@@std@@MAE_JPAD_J@Z) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::xsputn(char const *,__int64)"" (?xsputn@?$basic_streambuf@DU?$char_traits@D@std@@@std@@MAE_JPBD_J@Z) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::char_traits<char> >(void)"" (??1?$basic_ios@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __thiscall std::basic_ios<char,struct std::char_traits<char> >::basic_ios<char,struct std::char_traits<char> >(void)"" (??0?$basic_ios@DU?$char_traits@D@std@@@std@@IAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ostream<char,struct std::char_traits<char> >::~basic_ostream<char,struct std::char_traits<char> >(void)"" (??1?$basic_ostream@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_istream<char,struct std::char_traits<char> >::~basic_istream<char,struct std::char_traits<char> >(void)"" (??1?$basic_istream@DU?$char_traits@D@std@@@std@@UAE@XZ) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj)
1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(locale0.obj) : error LNK2005: ""void __cdecl std::_Facet_Register(class std::_Facet_base *)"" (?_Facet_Register@std@@YAXPAV_Facet_base@1@@Z) already defined in msvcprtd.lib(locale0_implib.obj)
1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Getgloballocale(void)"" (?_Getgloballocale@locale@std@@CAPAV_Locimp@12@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Init(bool)"" (?_Init@locale@std@@CAPAV_Locimp@12@_N@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_ctor(class std::_Locinfo *,char const *)"" (?_Locinfo_ctor@_Locinfo@std@@SAXPAV12@PBD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_dtor(class std::_Locinfo *)"" (?_Locinfo_dtor@_Locinfo@std@@SAXPAV12@@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: static unsigned int __cdecl std::codecvt<char,char,struct _Mbstatet>::_Getcat(class std::locale::facet const * *,class std::locale const *)"" (?_Getcat@?$codecvt@DDU_Mbstatet@@@std@@SAIPAPBVfacet@locale@2@PBV42@@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gninc(void)"" (?_Gninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pninc(void)"" (?_Pninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::gptr(void)const "" (?gptr@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@locale@std@@QAEIXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@_Locinfo@std@@QBE?AU_Cvtvec@@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@_Locinfo@std@@QBEPBDXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Gninc(void)"" (?_Gninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::_Pninc(void)"" (?_Pninc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IAEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@_Locinfo@std@@QBEPBGXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@ios_base@std@@QBEHXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::gptr(void)const "" (?gptr@?$basic_streambuf@DU?$char_traits@D@std@@@std@@IBEPADXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf<char,struct std::char_traits<char> >::sputc(char)"" (?sputc@?$basic_streambuf@DU?$char_traits@D@std@@@std@@QAEHD@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@ios_base@std@@QAE_J_J@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@ios_base@std@@QBE_JXZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::_Lockit(int)"" (??0_Lockit@std@@QAE@H@Z) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::~_Lockit(void)"" (??1_Lockit@std@@QAE@XZ) already defined in msvcprtd.lib(MSVCP140D.dll)
1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj
1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj
1>LINK : warning LNK4098: defaultlib 'MSVCRTD' conflicts with use of other libs; use /NODEFAULTLIB:library
1>MSVCRT.lib(initializers.obj) : warning LNK4098: defaultlib 'libcmt.lib' conflicts with use of other libs; use /NODEFAULTLIB:library
1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>main.obj : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter
1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>main.obj : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport
1>tensorflow-lite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::ScopedSuppressDenormals(void)"" (??0ScopedSuppressDenormals@ruy@@QAE@XZ) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@Interpreter@impl@tflite@@QAE?AW4TfLiteStatus@@XZ)
1>tensorflow-lite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals(void)"" (??1ScopedSuppressDenormals@ruy@@QAE@XZ) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@Interpreter@impl@tflite@@QAE?AW4TfLiteStatus@@XZ)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@ruy@@YAPAVCtx@1@PAVContext@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@Ctx@ruy@@QAEXXZ)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@Ctx@ruy@@QAEXW4PerformanceAdvisory@2@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@Ctx@ruy@@QAE?AW4Path@2@W432@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@Ctx@ruy@@QAEPAVAllocator@2@XZ)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@Allocator@ruy@@QAEPAXH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2019: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z) referenced in function ""public: static void __cdecl ruy::detail::ApplyMultiplierImpl<int,signed char,1>::Run(class ruy::MulParams<int,signed char> const &,int,int *)"" (?Run@?$ApplyMultiplierImpl@HC$00@detail@ruy@@SAXABV?$MulParams@HC@3@HPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@detail@ruy@@YAHHHH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit<16,16> const &)"" (?Kernel8bitAvx512SingleCol@ruy@@YAXABU?$KernelParams8bit@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat<16,16> const &)"" (?KernelFloatAvx512SingleCol@ruy@@YAXABU?$KernelParamsFloat@$0BA@$0BA@@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx2SingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx2SingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvx@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat<8,8> const &)"" (?KernelFloatAvxSingleCol@ruy@@YAXABU?$KernelParamsFloat@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvx@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit<8,8> const &)"" (?Kernel8bitAvxSingleCol@ruy@@YAXABU?$KernelParams8bit@$07$07@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@ruy@@YAXPBCC0HHHPACPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@ruy@@YAXPBM0HHHPAM@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@ruy@@YAXPBEHHPACHHHHHHHPAH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@ruy@@YAXPAVCtx@1@PAUTrMulParams@1@@Z)
1>tensorflow-lite.lib(conv.obj) : error LNK2019: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z) referenced in function ""public: static void __cdecl ruy::PackImpl<64,struct ruy::FixedKernelLayout<0,4,16>,short,short,int,0>::Run(enum ruy::Tuning,struct ruy::Mat<short> const &,struct ruy::PMat<short> *,int,int)"" (?Run@?$PackImpl@$0EA@U?$FixedKernelLayout@$0A@$03$0BA@@ruy@@FFH$0A@@ruy@@SAXW4Tuning@2@ABU?$Mat@F@2@PAU?$PMat@F@2@HH@Z)
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z)
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@ruy@@YAXPBF0HHHPAFPAH@Z)
1>tensorflow-lite.lib(lsh_projection.obj) : error LNK2019: unresolved external symbol ""unsigned __int64 __cdecl util::Fingerprint64(char const *,unsigned int)"" (?Fingerprint64@util@@YA_KPBDI@Z) referenced in function ""int __cdecl tflite::ops::builtin::lsh_projection::RunningSignBit(struct TfLiteTensor const *,struct TfLiteTensor const *,float)"" (?RunningSignBit@lsh_projection@builtin@ops@tflite@@YAHPBUTfLiteTensor@@0M@Z)
1>tensorflow-lite.lib(rfft2d.obj) : error LNK2019: unresolved external symbol _rdft2d referenced in function ""void __cdecl tflite::ops::builtin::rfft2d::Rfft2dImpl(int,int,double * *,int *,double *)"" (?Rfft2dImpl@rfft2d@builtin@ops@tflite@@YAXHHPAPANPAHPAN@Z)
1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg
1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg
1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg
1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_create referenced in function ""public: struct pthreadpool * __thiscall tflite::CpuBackendContext::get_xnnpack_threadpool(void)"" (?get_xnnpack_threadpool@CpuBackendContext@tflite@@QAEPAUpthreadpool@@XZ)
1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_create
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_destroy referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@tflite@@QAE@XZ)
1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_destroy
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::Context(void)"" (??0Context@ruy@@QAE@XZ) referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@tflite@@QAE@XZ)
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::~Context(void)"" (??1Context@ruy@@QAE@XZ) referenced in function ""public: void * __thiscall ruy::Context::`scalar deleting destructor'(unsigned int)"" (??_GContext@ruy@@QAEPAXI@Z)
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::set_max_num_threads(int)"" (?set_max_num_threads@Context@ruy@@QAEXH@Z) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::SetMaxNumThreads(int)"" (?SetMaxNumThreads@CpuBackendContext@tflite@@UAEXH@Z)
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::ClearPrepackedCache(void)"" (?ClearPrepackedCache@Context@ruy@@QAEXXZ) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::ClearCaches(void)"" (?ClearCaches@CpuBackendContext@tflite@@UAEXXZ)
1>tensorflow-lite.lib(spectrogram.obj) : error LNK2019: unresolved external symbol _rdft referenced in function ""private: void __thiscall tflite::internal::Spectrogram::ProcessCoreFFT(void)"" (?ProcessCoreFFT@Spectrogram@internal@tflite@@AAEXXZ)
1>XNNPACK.lib(hardware-config.obj) : error LNK2019: unresolved external symbol _cpuinfo_initialize referenced in function _xnn_init_hardware_config
1>XNNPACK.lib(hardware-config.obj) : error LNK2001: unresolved external symbol _cpuinfo_isa
1>XNNPACK.lib(resize-bilinear-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(prelu-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(batch-matrix-multiply-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(lut-elementwise-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(fully-connected-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(argmax-pooling-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(binary-elementwise-nd.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(resize-bilinear-nchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(global-average-pooling-ncw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(global-average-pooling-nwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(average-pooling-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(dynamic-fully-connected-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(unary-elementwise-nc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(convolution-nchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(convolution-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(deconvolution-nhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_with_thread referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_tile_1d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_with_thread referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_1d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d_with_thread referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d_tile_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d_tile_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(operator-run.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_6d_tile_2d referenced in function _xnn_run_operator_with_index
1>XNNPACK.lib(gemm-config.obj) : error LNK2019: unresolved external symbol _cpuinfo_get_core referenced in function _init_f32_gemm_config
1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___free_dbg referenced in function ""public: static void __cdecl std::_Fac_node::operator delete(void *)"" (??3_Fac_node@std@@SAXPAX@Z)
1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___malloc_dbg referenced in function ""public: static void * __cdecl std::_Fac_node::operator new(unsigned int)"" (??2_Fac_node@std@@SAPAXI@Z)


below is my main code that I am using to predict something from my pre-existing model:

`#include <iostream>
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/kernels/register.h>


int main() {

	tflite::StderrReporter error_reporter;
	std::unique_ptr<tflite::Interpreter> interpreter;

	std::unique_ptr<tflite::impl::FlatBufferModel> model = tflite::impl::FlatBufferModel::BuildFromFile(""model.tflite"", &error_reporter);

	tflite::ops::builtin::BuiltinOpResolver resolver;
	tflite::InterpreterBuilder builder(*model, resolver);
	builder(&interpreter);

	if (!interpreter) {
		std::cerr << ""Failed to create interpreter"" << std::endl;
		return -1;
	}

	TfLiteStatus status = interpreter->AllocateTensors();

	if (status != kTfLiteOk) {
		std::cerr << ""Failed to allocate tensors.""<<status << std::endl;
		return -1;
	}
return 0;
}`

qbe on (2024-09-17 11:10:06 UTC): please wrap all your error messages in

```
code blocks
like this
```

to avoid notifying users whose usernames appear in your error message

Ashish2000L (Issue Creator) on (2024-09-17 11:32:03 UTC): apologies, i'll do the same next time thanks

gaikwadrahul8 (Assginee) on (2024-09-17 12:19:32 UTC): Hi, @Ashish2000L

Thank you for the detailed error log, could you please refer this [stackoverflow answer](https://stackoverflow.com/questions/71556391/tensorflow-static-c-api-library-how-to-link-with-10-sub-dependencies) which is for C++ build and please follow the instructions mentioned in this [video](https://www.youtube.com/watch?v=1IhMISYvZG0) carefully which will help you to solve your issue

If issue still persists after following the video instructions and stackoverflow answer steps please help us with error log to investigate this issue further

Thank you for your cooperation and patience.

Ashish2000L (Issue Creator) on (2024-09-18 05:32:17 UTC): ```
1>tensorflow-lite.lib(interpreter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(model_builder.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(interpreter_builder.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(register.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(subgraph.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(signature_runner.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(metadata_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(external_cpu_backend_context.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(root_profiler.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mmap_allocation_disabled.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(allocation.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(op_resolver.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(flatbuffer_conversions.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(platform_profiler.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(schema_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mutable_op_resolver.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(elementwise.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(add.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(add_n.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(arg_min_max.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(assign_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(atan2.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(pooling.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(batch_to_space_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(batch_matmul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(bidirectional_sequence_lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(bidirectional_sequence_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(broadcast_args.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(broadcast_to.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(bucketize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(call_once.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(cast.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(ceil.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(complex_support.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(concatenation.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(conv3d.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(conv3d_transpose.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(cumsum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(densify.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(depth_to_space.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(depthwise_conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(dequantize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(div.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(dynamic_update_slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(activations.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(embedding_lookup.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(embedding_lookup_sparse.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(comparisons.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(exp.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(expand_dims.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(fake_quant.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(fill.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(floor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(floor_div.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(floor_mod.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(fully_connected.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(gather.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(gather_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(hashtable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(hashtable_find.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(hashtable_lookup.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(hashtable_import.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(hashtable_size.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(if.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(l2norm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(local_response_norm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(logical.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(lsh_projection.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(matrix_diag.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(matrix_set_diag.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(maximum_minimum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(reduce.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mirror_pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(neg.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(non_max_suppression.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(one_hot.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(pack.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(pow.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(quantize.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(random_ops.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(range.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(rank.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(read_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(reshape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(resize_bilinear.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(resize_nearest_neighbor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(reverse_sequence.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(reverse.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(rfft2d.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(basic_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(round.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(scatter_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(segment_sum.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(select.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(shape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(sign.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(skip_gram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(space_to_batch_nd.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(space_to_depth.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(sparse_to_dense.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(split.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(split_v.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(squared_difference.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(squeeze.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(strided_slice.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(sub.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(svdf.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(tile.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(topk_v2.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(transpose.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(transpose_conv.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(unidirectional_sequence_lstm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(unidirectional_sequence_rnn.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(unique.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(unpack.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(unsorted_segment.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(var_handle.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(where.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(while.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(zeros_like.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(bitcast.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(bitwise_xor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(right_shift.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(rng_bit_generator.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_add.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_multiply.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_min_max.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_pad.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_reduce_window.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_scatter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_gather.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(dilate.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(tflite_with_xnnpack_optional.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(numeric_verify.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(audio_spectrogram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(detection_postprocess.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(array.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(graph_info.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(arena_planner.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(runtime_shape.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(common.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(tensor_ctypes.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(kernel_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(cpu_backend_context.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(resource_variable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(cpu_backend_gemm_eigen.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(transpose_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(lstm_eval.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(kernel_utils.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(initialization_status.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(eigen_support.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(sparsity_format_converter.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(comparisons.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(string_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(fully_connected_reference.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(static_hashtable.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(rng_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(stablehlo_elementwise.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(tensor_slice_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(xnnpack_delegate.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(spectrogram.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mfcc.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(simple_memory_arena.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(portable_tensor_utils.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(quantization_util.cc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mfcc_dct.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
1>tensorflow-lite.lib(mfcc_mel_filterbank.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in main.obj
```

@gaikwadrahul8 
I am also seeing these runtimelibrary missmatch errors as well after including all the libs also some other issues.

Ashish2000L (Issue Creator) on (2024-09-18 07:46:36 UTC): @gaikwadrahul8 after watching video steps I am getting error working with bazel and working on latest tensorflow r2.17 branch. 

```
D:\tf\tensorflow>bazel --output_user_root=D:/bazel/bazel_temp_tensor build -c opt //tensorflow/lite:tensorflowlite
INFO: Reading 'startup' options from d:\tf\tensorflow\.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from d:\tf\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages --python_path=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe --action_env CLANG_COMPILER_PATH=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --repo_env=CC=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --repo_env=BAZEL_COMPILER=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe --copt=-Wno-gnu-offsetof-extensions --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\tf\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file d:\tf\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --enable_runfiles --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file d:\tf\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:no_tfrt in file d:\tf\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Analyzed target //tensorflow/lite:tensorflowlite (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: D:/tf/tensorflow/tensorflow/lite/core/c/BUILD:324:38: Compiling tensorflow/lite/core/c/common.cc failed: (Exit 2): cl.exe failed: error executing command (from target //tensorflow/lite/core/c:common)
  cd /d D:/bazel/bazel_temp_tensor/7ekuv3jw/execroot/org_tensorflow
  SET CLANG_COMPILER_PATH=C:Program FilesMicrosoft Visual Studio2022CommunityVCToolsLlvmbinclang.exe
    SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.38.33130\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.38.33130\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.38.33130\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\DiagnosticsHub\Collector;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\Llvm\x64\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\vcpkg
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/<USER>/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/site-packages
    SET TEMP=C:\Users\<USER>\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\<USER>\AppData\Local\Temp
    SET VSLANG=1033
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.38.33130\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/core/c/_objs/common/common.obj.params
# Configuration: 34ffb9d4dbc148eb4c3a1733abdc4d1230e52a1bb28afd53a7c6ad5f7415fe5c
# Execution platform: @local_execution_config_platform//:platform
cl : Command line error D8021 : invalid numeric argument '/Wno-gnu-offsetof-extensions'
Target //tensorflow/lite:tensorflowlite failed to build
INFO: Elapsed time: 1.519s, Critical Path: 0.44s
INFO: 55 processes: 55 internal.
FAILED: Build did NOT complete successfully
```

gaikwadrahul8 (Assginee) on (2024-09-18 15:08:33 UTC): Hi, @Ashish2000L 

Thank you for providing the error log, May I know which bazel version are you using ? if you're not using bazel version `6.5.0 `then please try with it and also downgrade the python version to `3.11` or `3.10` and see is it working as expected or not ? Thank you.

Ashish2000L (Issue Creator) on (2024-09-18 15:21:20 UTC): @gaikwadrahul8 
I have used python 3.11 and bazel 6.50 to build the name and then getting the same error log as shared in last conversation.

Ashish2000L (Issue Creator) on (2024-09-19 04:55:15 UTC): its really typical to build tf in windows directly so i have built tflite .so file in wsl using 
```
bazel --output_user_root=/home/<USER>/bazel_out build --config=monolithic -c dbg //tensorflow/lite:tensorflowlite
```
@gaikwadrahul8 can you specify how we can get the include files as the documention says we have to extract the same from ourself from the source, but it will be best if you can specify what folders I need to extract so that I can directly link them without any issue of linking, or you can share some docs that specify paths that need to be extracted according to r2.17 branch.

gaikwadrahul8 (Assginee) on (2024-09-19 11:10:47 UTC): Hi, @Ashish2000L

I believe you're able to see built library `libtensorflowlite.so` at this location :`<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/lite/libtensorflowlite.so` so as far I know you'll have to copy the `tensorflow` folder from this path `<YOUR_TENSORFLOW_ROOT>/bazel-bin/tensorflow/` and you'll have to add `flatbuffers` folder either by cloning this repo :https://github.com/google/flatbuffers or download zip file from [here](https://github.com/google/flatbuffers/releases/tag/v24.3.25) and unzip it keep `flatbuffers` folder under the `include` folder in the C++ project 

Could you please give it try and see is it working as expected or not ? 

Thank you for your cooperation and patience.

Ashish2000L (Issue Creator) on (2024-09-19 11:48:07 UTC): @gaikwadrahul8 
the path you specified is not having main header files like interpreter.h, model.h etc. so I am thinking of extracting all the header files that are available in <root path>/tensorflow/lite folder. Is it the right practice for the same, will it give any linking error? Also I have cloned the flatbuffers and in the same I found the include folder and I have added the same include folder only in my include folder.

gaikwadrahul8 (Assginee) on (2024-09-19 12:09:15 UTC): Hi, @Ashish2000L 

Could you please refer this [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal) with [tensorflow/lite/examples/minimal/CMakeLists.txt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt) there is similar issue which got resolved by using the `CMakeLists.txt` from minimal example refer that issue https://github.com/tensorflow/tensorflow/issues/60779 which may help you to solve your issue.

Thank you.

Ashish2000L (Issue Creator) on (2024-09-26 07:54:25 UTC): thanks, it resolved the issue.

google-ml-butler[bot] on (2024-09-26 07:54:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75840"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75840"">No</a>

"
2527493619,issue,closed,completed,tensorflow lite `AllocateTensors()` breaks on raspberry pi pico w,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15.0

### Custom code

Yes

### OS platform and distribution

Raspberry Pi Pico W

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

Apple clang version 14.0.3 (clang-1403.0.22.14.1)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to build a [Keras mdn-rnn model](https://pypi.org/project/keras-mdn-layer/) and get a very basic prediction program running on my Raspberry Pi Pico W, which has 264KB of SRAM, and 2MB of on-board flash memory. The Keras model is converted to a tflite model by `tflite_file = model_to_tflite(inference_mdrnn.model, model_keras_file)`. It's then converted to a `model.cc` by `xxd -i model.tflite > model.cc`.

I followed the tinyML book example to build my basic prediction program, but adding the line `TfLiteStatus allocate_status = interpreter->AllocateTensors();` breaks the program and I cannot see any serial output from the connected pico. I have included my program source. I'm wondering if `kTensorArenaSize` is not large enough or is too large. But I do not know whether this is the case and if it is how to set up a correct `kTensorArenaSize`. Any suggestions to clear the air will be greatly appreciated.

The converted `model.cc` file is [here](https://github.com/Zlisch/tflite-pico/blob/main/examples/impsy_model/musicMDRNN-dim4-layers2-units32-mixtures5-scale10.cc). The [CMakeList](https://github.com/Zlisch/tflite-pico/blob/main/examples/impsy_model/CMakeLists.txt) to generate uf2file.

### Standalone code to reproduce the issue

```shell
#include ""constants.h""
#include ""model.h""
#include ""main_functions.h""
#include ""tensorflow/lite/micro/micro_interpreter.h""
#include ""tensorflow/lite/micro/micro_log.h""
#include ""tensorflow/lite/micro/micro_mutable_op_resolver.h""
#include ""tensorflow/lite/micro/system_setup.h""
#include ""tensorflow/lite/schema/schema_generated.h""

// Globals, used for compatibility with Arduino-style sketches.
namespace {
const tflite::Model* model = nullptr;
tflite::MicroInterpreter* interpreter = nullptr;
TfLiteTensor* input = nullptr;
TfLiteTensor* output = nullptr;
int inference_count = 0;

constexpr int kTensorArenaSize = 60 * 1024; // 2 * 1024 for float model
uint8_t tensor_arena[kTensorArenaSize];
bool setupornot = false;
}  // namespace

// The name of this function is important for Arduino compatibility.
void setup() {
  printf(""Setting up ...... \n"");
  tflite::InitializeTarget();
}

void loop() {
  if (!setupornot) {
    printf(""Now setup ... \n"");

    tflite::InitializeTarget();

    // Map the model into a usable data structure. This doesn't involve any
    // copying or parsing, it's a very lightweight operation.
    model = tflite::GetModel(g_model_data);
    if (model->version() != TFLITE_SCHEMA_VERSION) {
      printf(
          ""Model provided is schema version %d not equal ""
          ""to supported version %d."",
          model->version(), TFLITE_SCHEMA_VERSION);
      return;
    }

    // This pulls in all the operation implementations we need.
    // NOLINTNEXTLINE(runtime-global-variables)
    static tflite::MicroMutableOpResolver<1> resolver;
    TfLiteStatus resolve_status = resolver.AddFullyConnected();
    if (resolve_status != kTfLiteOk) {
      printf(""Op resolution failed"");
      return;
    }

    // Build an interpreter to run the model with.
    static tflite::MicroInterpreter static_interpreter(
        model, resolver, tensor_arena, kTensorArenaSize);
    interpreter = &static_interpreter;

    // Allocate memory from the tensor_arena for the model's tensors.
    TfLiteStatus allocate_status = interpreter->AllocateTensors(); // broken
    if (allocate_status != kTfLiteOk) {
      printf(""AllocateTensors() failed"");
      return;
    }
    
    setupornot = true;
  }

  // Calculate an x value to feed into the model. We compare the current
  // inference_count to the number of inferences per cycle to determine
  // our position within the range of possible x values the model was
  // trained on, and use this to calculate a value.
  float position = static_cast<float>(inference_count) /
                   static_cast<float>(kInferencesPerCycle);
  float x = position * kXrange;
  printf(""x=%.6f\n"", x); 

  if (setupornot) {
    printf(""Has been setup ... \n"");
  } else {
    printf(""setupornot still false! \n"");
  }

  if (input == nullptr) {
    printf(""input is null!\n"");
  } else if (input->dims == nullptr) {
    printf(""input->dims is null!\n"");
  } else {
    printf(""Size of input->dims->size: %d\n"", input->dims->size);
  }

  if (interpreter == nullptr) {
    printf(""interpreter is null!\n"");
  } else {
    printf(""interpreter is NOT null!\n"");
  }

  // Increment the inference_counter, and reset it if we have reached
  // the total number per cycle
  // working
  inference_count += 1;
  if (inference_count >= kInferencesPerCycle) inference_count = 0;
}
```


### Relevant log output

```shell
serial output when excluding `tflite_file = model_to_tflite(inference_mdrnn.model, model_keras_file)`:

...
input is null!
interpreter is NOT null!
x=4.398230
Has been setup ...
input is null!
interpreter is NOT null!
x=4.712389
Has been setup ...
input is null!
interpreter is NOT null!
x=5.026548
Has been setup ...
...
```
",Zlisch,2024-09-16 04:47:52+00:00,['gaikwadrahul8'],2024-09-19 12:24:11+00:00,2024-09-19 12:24:08+00:00,https://github.com/tensorflow/tensorflow/issues/75831,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2355775294, 'issue_id': 2527493619, 'author': 'gaikwadrahul8', 'body': ""Hi, @Zlisch \r\n\r\nThank you for bringing this issue to our attention, as far I know there might be the MDN-RNN model you're using too complex for the limited memory of the raspberry pi pico w so please consider simplifying the model by reducing the number of layers, neurons or data types.\r\n\r\nThe `kTensorArenaSize` value might be insufficient. Experiment with increasing it gradually until allocation succeeds but be mindful of memory limitations and also check if there are any other processes or libraries consuming significant memory on raspberry pi pico w.\r\n\r\nPlease make sure that the input data you're providing to the model is valid and compatible with its expected format and dimensions. verify that the input tensor shape matches the model's expected input shape.\r\n\r\nIf possible optimize your model using techniques like quantization or pruning to reduce its memory footprint for that please follow our [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) which will help help you to optimize your model. \r\n\r\nIf issue still persists could you please post this issue in the [tflite-micro](https://github.com/tensorflow/tflite-micro/issues) repo for further help ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 17, 13, 20, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357310126, 'issue_id': 2527493619, 'author': 'Zlisch', 'body': 'Thanks for the reply @gaikwadrahul8\r\n\r\nI will try the optimization you mentioned to make the model as small as possible. I also noticed though `model.tflite` can be quite small, e.g. 41KB, the `model.cc` created by hex dump can be 251KB large. Is there anything that can be done?', 'created_at': datetime.datetime(2024, 9, 18, 1, 32, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358915921, 'issue_id': 2527493619, 'author': 'abangwal', 'body': 'Hey @Zlisch ,\r\nRegarding your query on model size blow-up on conversion from Tflite to C data array. This is happening because HEX dump naturally takes more space than the Tflite optimised storing format, but once you compile the model.cc file your executable should be close the original .tflite file.\r\n\r\nHere is the original thread regarding this issue...\r\nhttps://github.com/tensorflow/tensorflow/issues/43749#issuecomment-703227741', 'created_at': datetime.datetime(2024, 9, 18, 16, 26, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359995912, 'issue_id': 2527493619, 'author': 'Zlisch', 'body': ""I have tried with a much reduced [model](https://github.com/Zlisch/tflite-pico/blob/main/examples/impsy_model/musicMDRNN-dim4-layers1-units4-mixtures5-scale10.cc) (model size 13092B, # parameters = 369) but it still doesn't work. I've raised a [new issue](https://github.com/tensorflow/tflite-micro/issues/2686) to the tflite-micro repo."", 'created_at': datetime.datetime(2024, 9, 19, 5, 16, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360746845, 'issue_id': 2527493619, 'author': 'gaikwadrahul8', 'body': ""Hi, @Zlisch\r\n\r\nThank you for the update and trying things, I see you've posted this issue here https://github.com/tensorflow/tflite-micro/issues/2686 for further help so please feel free to close this issue from your end\r\n\r\nIf you need any further help with TensorFlow core or TensorFlow Lite (now renamed as [LiteRT](https://ai.google.dev/edge/litert))  please feel free to post your [issue](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 19, 11, 37, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360846545, 'issue_id': 2527493619, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75831"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75831"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 12, 24, 10, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-17 13:20:27 UTC): Hi, @Zlisch 

Thank you for bringing this issue to our attention, as far I know there might be the MDN-RNN model you're using too complex for the limited memory of the raspberry pi pico w so please consider simplifying the model by reducing the number of layers, neurons or data types.

The `kTensorArenaSize` value might be insufficient. Experiment with increasing it gradually until allocation succeeds but be mindful of memory limitations and also check if there are any other processes or libraries consuming significant memory on raspberry pi pico w.

Please make sure that the input data you're providing to the model is valid and compatible with its expected format and dimensions. verify that the input tensor shape matches the model's expected input shape.

If possible optimize your model using techniques like quantization or pruning to reduce its memory footprint for that please follow our [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) which will help help you to optimize your model. 

If issue still persists could you please post this issue in the [tflite-micro](https://github.com/tensorflow/tflite-micro/issues) repo for further help ? 

Thank you for your cooperation and patience.

Zlisch (Issue Creator) on (2024-09-18 01:32:59 UTC): Thanks for the reply @gaikwadrahul8

I will try the optimization you mentioned to make the model as small as possible. I also noticed though `model.tflite` can be quite small, e.g. 41KB, the `model.cc` created by hex dump can be 251KB large. Is there anything that can be done?

abangwal on (2024-09-18 16:26:56 UTC): Hey @Zlisch ,
Regarding your query on model size blow-up on conversion from Tflite to C data array. This is happening because HEX dump naturally takes more space than the Tflite optimised storing format, but once you compile the model.cc file your executable should be close the original .tflite file.

Here is the original thread regarding this issue...
https://github.com/tensorflow/tensorflow/issues/43749#issuecomment-703227741

Zlisch (Issue Creator) on (2024-09-19 05:16:33 UTC): I have tried with a much reduced [model](https://github.com/Zlisch/tflite-pico/blob/main/examples/impsy_model/musicMDRNN-dim4-layers1-units4-mixtures5-scale10.cc) (model size 13092B, # parameters = 369) but it still doesn't work. I've raised a [new issue](https://github.com/tensorflow/tflite-micro/issues/2686) to the tflite-micro repo.

gaikwadrahul8 (Assginee) on (2024-09-19 11:37:19 UTC): Hi, @Zlisch

Thank you for the update and trying things, I see you've posted this issue here https://github.com/tensorflow/tflite-micro/issues/2686 for further help so please feel free to close this issue from your end

If you need any further help with TensorFlow core or TensorFlow Lite (now renamed as [LiteRT](https://ai.google.dev/edge/litert))  please feel free to post your [issue](https://github.com/tensorflow/tensorflow/issues/new/choose)

Thank you for your cooperation and patience.

google-ml-butler[bot] on (2024-09-19 12:24:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75831"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75831"">No</a>

"
2527417965,issue,closed,completed,TensorFlow outputs in bursts. Pauses for half a second after running inference for 3 or 4 images,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.1, 2.17

### Custom code

Yes

### OS platform and distribution

Windows 11 x64

### Mobile device

_No response_

### Python version

3.10, 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2.2 / 8.1.1.33

### GPU model and memory

RTX 3080 (Notebook) / 8GB VRAM

### Current behavior?

Hello everyone!

I am receiving data for my neural network over WiFi. For example, I get one image, and I directly put it into the model. There is no queue implementation or any wait.

**I am running into an issue where my TensorFlow performs in bursts.**

For example, I will receive one image, put it in my model, TensorFlow will process it in 50 milliseconds, then next image, put it in my model, TensorFlow will process it in 50 milliseconds, then TensorFlow will pause for 0.5 seconds

Then I will receive four images, after receiving each image, TensorFlow will take about 50 milliseconds to process them, and then after the 4 images are done, TensorFlow will pause for 0.4 seconds

Then three images again, TensorFlow will pause for 0.4 seconds

And so on...

I tried the latest version of TensorFlow 2.17 (CPU only cause Windows) and also 2.10.1, the last Windows version to support GPU. (Yes, processing takes place on the GPU, I checked).

I also shared my code. I am not doing anything with the outputted data right now.

If I comment the ""result = model(savedimage)"", then I rapidly start receiving images from over the network so this is not a network issue. It's only that one line of code where I put data in my model, it performs in bursts and slows down performance

Why is this happening?
Is there some kind of optimization to make TensorFlow work in bursts? Is there a way to disable it?

Regards,
Jack

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from keras.models import load_model, Model
from tensorflow.keras.utils import load_img, img_to_array
from PIL import Image

import os
import socket
import cv2
from io import BytesIO



model = load_model(""PartitionedModel/YOLOv3Model.h5"", compile = False)


SERVER_HOST = ""0.0.0.0""
SERVER_PORT = 5001
BUFFER_SIZE = 8192

while(1):
	s = socket.socket()
	s.bind((SERVER_HOST, SERVER_PORT))
	s.listen(1)
	client_socket, address = s.accept()
	image_data = BytesIO()

	while True:
		bytes_read = client_socket.recv(BUFFER_SIZE)
		if not bytes_read:
			break
		image_data.write(bytes_read)

	client_socket.close()
	s.close()


	savedimage = Image.open(image_data)

	result = model(savedimage)
```


### Relevant log output

_No response_",jackfaubshner,2024-09-16 03:19:28+00:00,['tilakrayal'],2024-10-04 02:01:43+00:00,2024-10-04 02:01:40+00:00,https://github.com/tensorflow/tensorflow/issues/75828,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2360965326, 'issue_id': 2527417965, 'author': 'tilakrayal', 'body': '@jackfaubshner,\r\nFrom the query I can sense that you are sending the images one after the one. Generally it is preferable to make the images in a certain batch and then feed to the model which helps to increase the performance. Could you please try in that way and provide the update. Thank you!', 'created_at': datetime.datetime(2024, 9, 19, 13, 18, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259456, 'issue_id': 2527417965, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646724, 'issue_id': 2527417965, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646781, 'issue_id': 2527417965, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75828"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75828"">No</a>', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-19 13:18:23 UTC): @jackfaubshner,
From the query I can sense that you are sending the images one after the one. Generally it is preferable to make the images in a certain batch and then feed to the model which helps to increase the performance. Could you please try in that way and provide the update. Thank you!

github-actions[bot] on (2024-09-27 02:01:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-04 02:01:39 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-04 02:01:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75828"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75828"">No</a>

"
2527349727,issue,open,,`resource_create_op` operation can cause TensorFlow to crash.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import test_ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import array_ops_stack


sess = tf.compat.v1.Session()

@tf.function
def func():
    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')
    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')
    c = array_ops_stack.stack([r1, r2])
    s = array_ops.strided_slice(c, [1], [2 ** 32])
    with sess.as_default():
        test_ops.resource_create_op(s)

func()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)

The above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)
```
",Justobe,2024-09-16 01:45:51+00:00,['Venkat6871'],2024-09-20 10:14:22+00:00,,https://github.com/tensorflow/tensorflow/issues/75825,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2354456983, 'issue_id': 2527349727, 'author': 'Venkat6871', 'body': 'Hi @Justobe ,\r\nSorry for the dealy, I reproduced the code shared but facing different error. Could you please share the colab gist with all the dependencies to analyze more of it.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 17, 4, 0, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354471031, 'issue_id': 2527349727, 'author': 'Justobe', 'body': 'Sorry for the missing dependencies. I’ve updated the code, and the colab gist can be found [here](https://colab.research.google.com/drive/1ft3CN0GGagOPxheGUjwlIvFszfM4-0ze?usp=sharing).', 'created_at': datetime.datetime(2024, 9, 17, 4, 16, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354822368, 'issue_id': 2527349727, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/54b4dd80dae6ad8b582fc4c0e7df164d/75825_tf-2-17-0-nightly.ipynb) here for reference. Thank you!', 'created_at': datetime.datetime(2024, 9, 17, 8, 3, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-17 04:00:29 UTC): Hi @Justobe ,
Sorry for the dealy, I reproduced the code shared but facing different error. Could you please share the colab gist with all the dependencies to analyze more of it.
Thank you!

Justobe (Issue Creator) on (2024-09-17 04:16:20 UTC): Sorry for the missing dependencies. I’ve updated the code, and the colab gist can be found [here](https://colab.research.google.com/drive/1ft3CN0GGagOPxheGUjwlIvFszfM4-0ze?usp=sharing).

Venkat6871 (Assginee) on (2024-09-17 08:03:03 UTC): I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/54b4dd80dae6ad8b582fc4c0e7df164d/75825_tf-2-17-0-nightly.ipynb) here for reference. Thank you!

"
2527272279,issue,closed,completed,CVE-2023-30767,"Hello again!


[Public sources](https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00903.html) say that CVE-2023-30767 affects versions of `intel-tensorflow` before than 2.13.0. Here the advisory:

> Improper buffer restrictions in Intel(R) Optimization for TensorFlow before version 2.13.0 may allow an authenticated user to potentially enable escalation of privilege via local access.

_I think_ [this is the related fix PR](https://github.com/tensorflow/tensorflow/pull/59581), which is indeed included in `intel-tensorflow` 2.13.0 **and also** in `tensorflow` itself. 
However, I don't see this security issue mentioned in this repository. It does not appears on https://github.com/tensorflow/tensorflow/security, nor I found a TFSA entry for it, nor it is mentioned in Tensorflow 2.13.0 Release Notes.

Could you clarify this situation? Is TensorFlow affected by CVE-2023-30767 in versions before 2.13.0?


Thank you in advance",SCH227,2024-09-15 23:59:59+00:00,['tilakrayal'],2024-09-17 19:34:11+00:00,2024-09-17 19:34:08+00:00,https://github.com/tensorflow/tensorflow/issues/75823,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('subtype:cpu-intel', 'To track windows cpu issues')]","[{'comment_id': 2352265706, 'issue_id': 2527272279, 'author': 'mihaimaruseac', 'body': 'There are no more TFSAs filed in this repo, only CVEs and only for advisories that match the updated security posture.\r\n\r\nFor this specific CVE, the vulnerable code is in OneDNN, with is contributed by Intel and not active in `tensorflow` itself. If you use oneDNN then you are affected, otherwise not.', 'created_at': datetime.datetime(2024, 9, 16, 8, 11, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354170979, 'issue_id': 2527272279, 'author': 'SCH227', 'body': 'Thank you for the prompt response.\r\n\r\nWhat confuses me is, if this CVE only affects code contributed by Intel OneDNN fixed [here](https://github.com/Intel-tensorflow/tensorflow/commit/37f8b09e93ca68f6c97e62794e064f4307a8e00b), why [these same code changes](https://github.com/tensorflow/tensorflow/pull/59581) are also part of TensorFlow release 2.13.0 without it being affected. \r\nEven the branch name [Intel-tensorflow:security_fix_quantiz](https://github.com/Intel-tensorflow/tensorflow/tree/security_fix_quantiz) may suggest something related to security. \r\n\r\nI will appreciate if you help me understand this situation 🙏', 'created_at': datetime.datetime(2024, 9, 16, 22, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354526410, 'issue_id': 2527272279, 'author': 'mihaimaruseac', 'body': ""The code itself is not in use in production if you're not using OneDNN. It's similar to the following scenario\r\n\r\n```cc\r\nif (using_one_dnn) {\r\n  call_the_code_that_got_fixed();\r\n} else {\r\n  run_some_other_code();\r\n}\r\n```\r\n\r\nSince `using_one_dnn` is true only if using OneDNN, the vulnerability does not manifest anywhere else"", 'created_at': datetime.datetime(2024, 9, 17, 5, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356738222, 'issue_id': 2527272279, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75823"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75823"">No</a>', 'created_at': datetime.datetime(2024, 9, 17, 19, 34, 9, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-09-16 08:11:20 UTC): There are no more TFSAs filed in this repo, only CVEs and only for advisories that match the updated security posture.

For this specific CVE, the vulnerable code is in OneDNN, with is contributed by Intel and not active in `tensorflow` itself. If you use oneDNN then you are affected, otherwise not.

SCH227 (Issue Creator) on (2024-09-16 22:56:35 UTC): Thank you for the prompt response.

What confuses me is, if this CVE only affects code contributed by Intel OneDNN fixed [here](https://github.com/Intel-tensorflow/tensorflow/commit/37f8b09e93ca68f6c97e62794e064f4307a8e00b), why [these same code changes](https://github.com/tensorflow/tensorflow/pull/59581) are also part of TensorFlow release 2.13.0 without it being affected. 
Even the branch name [Intel-tensorflow:security_fix_quantiz](https://github.com/Intel-tensorflow/tensorflow/tree/security_fix_quantiz) may suggest something related to security. 

I will appreciate if you help me understand this situation 🙏

mihaimaruseac on (2024-09-17 05:03:03 UTC): The code itself is not in use in production if you're not using OneDNN. It's similar to the following scenario

```cc
if (using_one_dnn) {
  call_the_code_that_got_fixed();
} else {
  run_some_other_code();
}
```

Since `using_one_dnn` is true only if using OneDNN, the vulnerability does not manifest anywhere else

google-ml-butler[bot] on (2024-09-17 19:34:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75823"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75823"">No</a>

"
2526731457,issue,closed,completed,ruy::CpuInfo::Initialize() Null pointer dereference: SIGSEGV  0x0000000000000008,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow-lite:2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

Android

### Python version

_No response_

### Bazel version

6.5.0

### GCC/compiler version

Clang 8.0.7

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We recently upgraded TensorFlow Lite from 2.9.0 to 2.16.1 (the latest version from MavenCentral) and received a fresh native crash issue on Firebase Crashlytic. (Possibly related to #74043). But since the official release of the shared library on MavenCentral was stripped (#72877), we can't investigate further. Our team decided to rebuild TensorFlow Lite 2.16.1 from source, with the flag `tflite_keep_symbols=true` to keep the stack trace info more readable. The issue has about 30k crash events, affecting 7k users. We still, can not reproduce the crash locally.

![crashlytics_report](https://github.com/user-attachments/assets/8a99cbed-06bf-40e9-9d37-f8473c6eb12b)



### Standalone code to reproduce the issue

```shell
We use only two main functions to infer the model, the crash happened in both cases.


org.tensorflow.lite.InterpreterApi#run
```


```
org.tensorflow.lite.Interpreter#runSignature(java.util.Map<java.lang.String,java.lang.Object>, java.util.Map<java.lang.String,java.lang.Object>, java.lang.String)
```
```


### Relevant log output

```shell
null pointer dereference: SIGSEGV  0x0000000000000008
#00 pc 0x23ddd8 libtensorflowlite_jni.so (ruy::CpuInfo::Initialize() [cpuinfo.cc:66]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#01 pc 0x23ddd4 libtensorflowlite_jni.so (ruy::CpuInfo::Initialize() [cpuinfo.cc:64]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#02 pc 0x23de6c libtensorflowlite_jni.so (ruy::CpuInfo::NeonDotprod() [cpuinfo.cc:35]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#03 pc 0x23c2c8 libtensorflowlite_jni.so (ruy::Ctx::GetRuntimeEnabledPaths() [ctx.cc:125]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#04 pc 0x23c3b4 libtensorflowlite_jni.so (ruy::Ctx::SelectPath(ruy::Path) [ctx.cc:177]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#05 pc 0xa63b4 libtensorflowlite_jni.so (void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)49, float, float, float, float>(ruy::Mat<float> const&, ruy::Mat<float> const&, ruy::Mat<float> const&, ruy::MulParams<float, float> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*) [create_trmul_params.h:423]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#06 pc 0xa626c libtensorflowlite_jni.so (void ruy::MulFrontEnd<(ruy::Path)49, float, float, float, float>(ruy::Mat<float> const&, ruy::Mat<float> const&, ruy::MulParams<float, float> const&, ruy::Ctx*, ruy::Mat<float>*) [create_trmul_params.h:472]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#07 pc 0xa5cb8 libtensorflowlite_jni.so (tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<float, float, float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0>::Run(tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float*, tflite::cpu_backend_gemm::GemmParams<float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0> const&, tflite::CpuBackendContext*) [ruy.h:46]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#08 pc 0x12c9bc libtensorflowlite_jni.so (tflite::optimized_ops::FullyConnected(tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*) [optimized_ops.h:306]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#09 pc 0x12b108 libtensorflowlite_jni.so (TfLiteStatus tflite::ops::builtin::fully_connected::EvalFloat<(tflite::ops::builtin::fully_connected::KernelType)1>(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) [fully_connected.cc:1563]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#10 pc 0x129aa0 libtensorflowlite_jni.so (TfLiteStatus tflite::ops::builtin::fully_connected::Eval<(tflite::ops::builtin::fully_connected::KernelType)1>(TfLiteContext*, TfLiteNode*) [fully_connected.cc:1605]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#11 pc 0x3403b0 libtensorflowlite_jni.so (tflite::Subgraph::InvokeImpl() [subgraph.cc:1396]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#12 pc 0x33fd98 libtensorflowlite_jni.so (tflite::Subgraph::Invoke() [subgraph.cc:1581]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#13 pc 0x333bc8 libtensorflowlite_jni.so (tflite::impl::SignatureRunner::Invoke() [signature_runner.cc:82]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#14 pc 0x28a4c libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeSignatureRunnerWrapper_nativeInvoke [nativesignaturerunner_jni.cc:268]) (BuildId: 488448e6d222d2f499e4c2978695bdc8ea291e71)
#15 pc 0x351e30 libart.so (BuildId: ddcc440d4609d2099db9d20895487a78)
```
",ninh-huynh,2024-09-15 04:21:34+00:00,"['arfaian', 'pkgoogle']",2024-12-29 00:46:17+00:00,2024-11-26 06:32:12+00:00,https://github.com/tensorflow/tensorflow/issues/75815,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2355842756, 'issue_id': 2526731457, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nCould you please take look into this issue? Thank you.', 'created_at': datetime.datetime(2024, 9, 17, 13, 39, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356679975, 'issue_id': 2526731457, 'author': 'pkgoogle', 'body': ""Hi @ninh-huynh, I am unfamiliar with how firebase crashlytics sets up its environments -- at first glance thats what it kind of looks like to me -- an issue with their environment setup -- since you can't figure out a way to reproduce it locally. Perhaps Firebase support will be a better starting point: https://firebase.google.com/support/troubleshooter/contact?visit_id=638621964296946057-4039620740&rd=1 until we can get more information."", 'created_at': datetime.datetime(2024, 9, 17, 19, 1, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357550893, 'issue_id': 2526731457, 'author': 'ninh-huynh', 'body': 'Hi @pkgoogle\r\n\r\nWhat do you mean about ""Firebase crashlytics sets up its environments""? The Crashlytics library helps produce readable stack-trace from the native library from Firebase Console (a web service that helps our developer monitor crashes from user devices). We still need to build an unstripped library manually and then provide a path to Firebase Crashlytics.  \r\n\r\nYou can take a quick look at how it was easy to setup Crashlytics to an Android project here. https://firebase.google.com/docs/crashlytics/ndk-reports\r\n\r\nNot every crash/issue from users is always reproducible, as a developer we try to help them as much as possible. Can you take a look at the source code file, function name,... from stack-trace above and review the recent change of tensorflow to see if anything relates to the crash?', 'created_at': datetime.datetime(2024, 9, 18, 5, 43, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362270788, 'issue_id': 2526731457, 'author': 'pkgoogle', 'body': 'Hi @arfaian, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 9, 19, 21, 55, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454375008, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': ""We're seeing perhaps a related issue with Redmi and Vivo devices. It's affecting a large number of users. Is there a plan to address this?"", 'created_at': datetime.datetime(2024, 11, 4, 10, 44, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455580418, 'issue_id': 2526731457, 'author': 'pkgoogle', 'body': 'Hi @sebouh00, do you have any of these error logs?', 'created_at': datetime.datetime(2024, 11, 4, 19, 55, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455589844, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': ""Here's a sample crash log I'm seeing:\r\n\r\n```\r\n*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\npid: 0, tid: 6913 >>> com.sentiance.journeys2 <<<\r\n\r\nbacktrace:\r\n  #00  pc 0x000000000024fc48  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #01  pc 0x000000000024fca4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #02  pc 0x000000000024e7ac  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #03  pc 0x000000000024e83c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #04  pc 0x00000000000d9f5c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #05  pc 0x00000000000d93a4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #06  pc 0x00000000000d5858  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #07  pc 0x000000000030205c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #08  pc 0x0000000000301b5c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #09  pc 0x00000000002f6124  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #10  pc 0x000000000007f9c0  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+88) (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n  #11  pc 0x00000000004bc2a0  /data/misc/apexdata/com.android.art/dalvik-cache/arm64/boot.oat (art_jni_trampoline+112)\r\n  #12  pc 0x000000000077f00c  /apex/com.android.art/lib64/libart.so (nterp_helper+1948)\r\n  #13  pc 0x000000000142f2f8  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.NativeInterpreterWrapper.run+124)\r\n  #14  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)\r\n  #15  pc 0x000000000142e26e  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs+10)\r\n  #16  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)\r\n  #17  pc 0x000000000142e5f4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs)\r\n  #18  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)\r\n  ...\r\n```"", 'created_at': datetime.datetime(2024, 11, 4, 20, 0, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456898633, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': 'Additional crash log that I obtained from a breakpad minidump. Unfortunately, no symbols to resolve the addresses.\r\n```\r\nOperating system: Android\r\n                  0.0.0 Linux 4.14.186-g9ce5b7f15-dirty #1 SMP PREEMPT Sun Apr 24 16:54:14 CST 2022 aarch64\r\nCPU: arm64\r\n     8 CPUs\r\n\r\nGPU: UNKNOWN\r\n\r\nCrash reason:  SIGSEGV /SEGV_MAPERR\r\nCrash address: 0x8\r\nProcess uptime: not available\r\n\r\nThread 40 (crashed)\r\n 0  libtensorflowlite_jni.so + 0x24fc48\r\n     x0 = 0x0000000000000000    x1 = 0x00000075f6f9b664\r\n     x2 = 0x00000075e4fb2020    x3 = 0x00000075e4fb2008\r\n     x4 = 0x0000000000000000    x5 = 0xb4000077785b8390\r\n     x6 = 0x00000075e4fb20e8    x7 = 0x000000b400000708\r\n     x8 = 0xb4000077b859a030    x9 = 0x0000000000000006\r\n    x10 = 0x0000000000000050   x11 = 0xfffffffffffffff7\r\n    x12 = 0x0000000000000009   x13 = 0xb4000077185a36f9\r\n    x14 = 0x00000000fffffff7   x15 = 0x00000075f6cdef5c\r\n    x16 = 0x00000075f6fe81d8   x17 = 0x000000796e17b430\r\n    x18 = 0x00000075e49f4000   x19 = 0xb4000077785b843c\r\n    x20 = 0x0000000000000006   x21 = 0x0000000000000002\r\n    x22 = 0xb4000076f85e3fe0   x23 = 0x0000000000020000\r\n    x24 = 0xb40000779857a540   x25 = 0xb40000779857a540\r\n    x26 = 0x0000000000000000   x27 = 0x0000000000000000\r\n    x28 = 0x0000000000000000    fp = 0x0000000000000003\r\n     lr = 0x00000075f6efdc48    sp = 0x00000075e4fb1cf0\r\n     pc = 0x00000075f6efdc48\r\n    Found by: given as instruction pointer in context\r\n 1  libtensorflowlite_jni.so + 0x24fca4\r\n     sp = 0x00000075e4fb1d08    pc = 0x00000075f6efdca8\r\n    Found by: stack scanning\r\n 2  libtensorflowlite_jni.so + 0x24e7ac\r\n     sp = 0x00000075e4fb1d48    pc = 0x00000075f6efc7b0\r\n    Found by: stack scanning\r\n 3  libtensorflowlite_jni.so + 0x2656ec\r\n     sp = 0x00000075e4fb1db8    pc = 0x00000075f6f136f0\r\n    Found by: stack scanning\r\n 4  libtensorflowlite_jni.so + 0x24e83c\r\n     sp = 0x00000075e4fb1dc8    pc = 0x00000075f6efc840\r\n    Found by: stack scanning\r\n 5  libtensorflowlite_jni.so + 0xd9f5c\r\n     sp = 0x00000075e4fb1e08    pc = 0x00000075f6d87f60\r\n    Found by: stack scanning\r\n 6  libtensorflowlite_jni.so + 0xd93a4\r\n     sp = 0x00000075e4fb1e48    pc = 0x00000075f6d873a8\r\n    Found by: stack scanning\r\n```\r\nWe heavily rely on TensorFlow Lite in our library, and unfortunately this is impacting a lot of users.', 'created_at': datetime.datetime(2024, 11, 5, 11, 15, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456927211, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': ""Could it be related to the issue that's been addressed [here](https://github.com/google/ruy/commit/690c14c441387a4ea6e07a9ed89657cec8200b92)?"", 'created_at': datetime.datetime(2024, 11, 5, 11, 30, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457362245, 'issue_id': 2526731457, 'author': 'ninh-huynh', 'body': ""> Could it be related to the issue that's been addressed [here](https://github.com/google/ruy/commit/690c14c441387a4ea6e07a9ed89657cec8200b92)?\r\n\r\nThat sounds promising. Don't know if we could verify that issue locally. I can confirm that the issue affects our app mostly on Xiaomi & Vivo devices too."", 'created_at': datetime.datetime(2024, 11, 5, 14, 43, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459101237, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': '@ninh-huynh Is the crash that you experienced with the production build of TFL also at `libtensorflowlite_jni.so + 0x24fc48`?', 'created_at': datetime.datetime(2024, 11, 6, 9, 23, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461682070, 'issue_id': 2526731457, 'author': 'ninh-huynh', 'body': '> @ninh-huynh Is the crash that you experienced with the production build of TFL also at `libtensorflowlite_jni.so + 0x24fc48`?\r\n\r\nOur team currently use custom TF Lite on production (with tflite_keep_symbols=true), so the report from Firebase is fully human readable (It crash at cpuinfo.cc - Line 66). Seem related to google/ruy#349', 'created_at': datetime.datetime(2024, 11, 7, 9, 5, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461859203, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': ""@ninh-huynh Thanks. I just want to be sure our issues are related. As I don't have human readable stack traces, I was wondering whether my crash resembles yours, when you originally got it with the production TFL build. And whether after you switched to the custom TFL build in your app, the crash log that you shared from Firebase is the only TFL crash that you're seeing."", 'created_at': datetime.datetime(2024, 11, 7, 10, 25, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461945078, 'issue_id': 2526731457, 'author': 'ninh-huynh', 'body': ""> @ninh-huynh Thanks. I just want to be sure our issues are related. As I don't have human readable stack traces, I was wondering whether my crash resembles yours, when you originally got it with the production TFL build. And whether after you switched to the custom TFL build in your app, the crash log that you shared from Firebase is the only TFL crash that you're seeing.\r\n\r\nOur feature using TFL don't have any update recently. Just updating TFT to version 2.16.1 and bum, fresh crash appear without stacktrace. Also, I already suggest the TF team to turn that flag on (see #72877) but no response yet. As a client using the TFL library, I really really don't understand why they turn off that flag. As you can see, that crash non reproducable but user out there being impacted a lot. But they still don't have any suggestion to address this issue."", 'created_at': datetime.datetime(2024, 11, 7, 11, 4, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461965097, 'issue_id': 2526731457, 'author': 'ninh-huynh', 'body': '> @ninh-huynh Is the crash that you experienced with the production build of TFL also at `libtensorflowlite_jni.so + 0x24fc48`?\r\n\r\nHere is the original crash when I upgraded TFL 2.16.1 (I got it from the user using the old version of the app, before I rebuilt TFL). It also crashes at 0x24fc48\r\n\r\n```\r\n         null pointer dereference: SIGSEGV  0x0000000000000008\r\n#00 pc 0x24fc48 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#01 pc 0x24fca4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#02 pc 0x24e7ac libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#03 pc 0x335f44 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#04 pc 0x335eb4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#05 pc 0x24e83c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#06 pc 0xd9f5c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#07 pc 0x10754c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#08 pc 0x237fe libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#09 pc 0x27e5c4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#10 pc 0x1073d8 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#11 pc 0x15d7e0 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#12 pc 0x15c0dc libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#13 pc 0xb0e04 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#14 pc 0x15a108 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#15 pc 0x30205c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#16 pc 0x30205c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#17 pc 0x301b5c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#18 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#19 pc 0x2fad4c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#20 pc 0x2361c8 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#21 pc 0x803b8 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)\r\n#22 pc 0x351230 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#23 pc 0x35119c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#24 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#25 pc 0x5b879c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#26 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#27 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#28 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#29 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#30 pc 0x5be120 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#31 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#32 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#33 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#34 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#35 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#36 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#37 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#38 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#39 pc 0x236d80 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#40 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#41 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#42 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#43 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#44 pc 0x72218558\r\n#45 pc 0x6ffb9b1f8c\r\n#46 pc 0x6ffb9fe180\r\n#47 pc 0x6ffb691830\r\n#48 pc 0x6ffba02988\r\n#49 pc 0x6ffb69fab4\r\n#50 pc 0x6ffb69c108\r\n#51 pc 0x3cbc50 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#52 pc 0xc15ffc libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#53 pc 0x6ffb69d2c4\r\n#54 pc 0x33a7a4 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#55 pc 0x23a01c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#56 pc 0x480b4 libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n#57 pc 0x539078 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#58 pc 0x539108 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)\r\n#59 pc 0xd33ac libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n#60 pc 0xfba4c libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n#61 pc 0x326ffc libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n#62 pc 0x8e5f0 libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n#63 pc 0xfb97c libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)\r\n\r\n```', 'created_at': datetime.datetime(2024, 11, 7, 11, 13, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462033191, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': 'Thanks. Then it seems we have the same issue, and your symbolicated stack trace applies to all cases. We will likely patch it ourselves and use that patched version.', 'created_at': datetime.datetime(2024, 11, 7, 11, 48, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499773812, 'issue_id': 2526731457, 'author': 'gaikwadrahul8', 'body': ""Hi, @ninh-huynh \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/36\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 6, 31, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499775234, 'issue_id': 2526731457, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75815"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75815"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 6, 32, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563908937, 'issue_id': 2526731457, 'author': 'prilaga', 'body': 'Hi @gaikwadrahul8 \r\nI tried LiteRT v.1.0.1 and it has the same issue. The crash is not fixed.', 'created_at': datetime.datetime(2024, 12, 27, 17, 50, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563911077, 'issue_id': 2526731457, 'author': 'prilaga', 'body': 'Hi @ninh-huynh \r\nDid you find a solution to avoid the crash? Maybe you downgraded tensorflow version?\r\nThanks!', 'created_at': datetime.datetime(2024, 12, 27, 17, 53, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564006561, 'issue_id': 2526731457, 'author': 'prilaga', 'body': ""> Thanks. Then it seems we have the same issue, and your symbolicated stack trace applies to all cases. We will likely patch it ourselves and use that patched version.\r\n\r\nHi @sebouh00 , perhaps you have found some working solution? I have the same problem and I can't reproduce it on my devices. My android app randomly crashes."", 'created_at': datetime.datetime(2024, 12, 27, 20, 20, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564270876, 'issue_id': 2526731457, 'author': 'sebouh00', 'body': ""@prilaga Yes. We updated the `ruy` dependency to a version that fixes the crash, and rolled out our own TFL artifact. See [here](https://github.com/sentiance/tensorflow/commit/0aa8cf51def4232d777b56d56f4fe744f887ae1d). We first forked `ruy`, and applied some changes to the bazel build files to make it work with TFL's bazel version. See [here](https://github.com/sentiance/ruy/compare/c08ec52...cfa3b6f)."", 'created_at': datetime.datetime(2024, 12, 28, 8, 43, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564571390, 'issue_id': 2526731457, 'author': 'prilaga', 'body': '@sebouh00 Thank you for the detailed information!', 'created_at': datetime.datetime(2024, 12, 29, 0, 46, 16, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-17 13:39:52 UTC): Hi, @pkgoogle

Could you please take look into this issue? Thank you.

pkgoogle (Assginee) on (2024-09-17 19:01:19 UTC): Hi @ninh-huynh, I am unfamiliar with how firebase crashlytics sets up its environments -- at first glance thats what it kind of looks like to me -- an issue with their environment setup -- since you can't figure out a way to reproduce it locally. Perhaps Firebase support will be a better starting point: https://firebase.google.com/support/troubleshooter/contact?visit_id=638621964296946057-4039620740&rd=1 until we can get more information.

ninh-huynh (Issue Creator) on (2024-09-18 05:43:07 UTC): Hi @pkgoogle

What do you mean about ""Firebase crashlytics sets up its environments""? The Crashlytics library helps produce readable stack-trace from the native library from Firebase Console (a web service that helps our developer monitor crashes from user devices). We still need to build an unstripped library manually and then provide a path to Firebase Crashlytics.  

You can take a quick look at how it was easy to setup Crashlytics to an Android project here. https://firebase.google.com/docs/crashlytics/ndk-reports

Not every crash/issue from users is always reproducible, as a developer we try to help them as much as possible. Can you take a look at the source code file, function name,... from stack-trace above and review the recent change of tensorflow to see if anything relates to the crash?

pkgoogle (Assginee) on (2024-09-19 21:55:37 UTC): Hi @arfaian, can you please take a look? Thanks.

sebouh00 on (2024-11-04 10:44:39 UTC): We're seeing perhaps a related issue with Redmi and Vivo devices. It's affecting a large number of users. Is there a plan to address this?

pkgoogle (Assginee) on (2024-11-04 19:55:05 UTC): Hi @sebouh00, do you have any of these error logs?

sebouh00 on (2024-11-04 20:00:00 UTC): Here's a sample crash log I'm seeing:

```
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
pid: 0, tid: 6913 >>> com.sentiance.journeys2 <<<

backtrace:
  #00  pc 0x000000000024fc48  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #01  pc 0x000000000024fca4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #02  pc 0x000000000024e7ac  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #03  pc 0x000000000024e83c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #04  pc 0x00000000000d9f5c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #05  pc 0x00000000000d93a4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #06  pc 0x00000000000d5858  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #07  pc 0x000000000030205c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #08  pc 0x0000000000301b5c  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #09  pc 0x00000000002f6124  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #10  pc 0x000000000007f9c0  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/split_config.arm64_v8a.apk!libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+88) (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
  #11  pc 0x00000000004bc2a0  /data/misc/apexdata/com.android.art/dalvik-cache/arm64/boot.oat (art_jni_trampoline+112)
  #12  pc 0x000000000077f00c  /apex/com.android.art/lib64/libart.so (nterp_helper+1948)
  #13  pc 0x000000000142f2f8  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.NativeInterpreterWrapper.run+124)
  #14  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)
  #15  pc 0x000000000142e26e  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs+10)
  #16  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)
  #17  pc 0x000000000142e5f4  /data/app/~~G6Pm98TxHmmZaYljDtBtvw==/com.sentiance.journeys2-GwfAF6oW3sfoEBLO8RcMsw==/oat/arm64/base.vdex (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs)
  #18  pc 0x000000000077f7c4  /apex/com.android.art/lib64/libart.so (nterp_helper+3924)
  ...
```

sebouh00 on (2024-11-05 11:15:51 UTC): Additional crash log that I obtained from a breakpad minidump. Unfortunately, no symbols to resolve the addresses.
```
Operating system: Android
                  0.0.0 Linux 4.14.186-g9ce5b7f15-dirty #1 SMP PREEMPT Sun Apr 24 16:54:14 CST 2022 aarch64
CPU: arm64
     8 CPUs

GPU: UNKNOWN

Crash reason:  SIGSEGV /SEGV_MAPERR
Crash address: 0x8
Process uptime: not available

Thread 40 (crashed)
 0  libtensorflowlite_jni.so + 0x24fc48
     x0 = 0x0000000000000000    x1 = 0x00000075f6f9b664
     x2 = 0x00000075e4fb2020    x3 = 0x00000075e4fb2008
     x4 = 0x0000000000000000    x5 = 0xb4000077785b8390
     x6 = 0x00000075e4fb20e8    x7 = 0x000000b400000708
     x8 = 0xb4000077b859a030    x9 = 0x0000000000000006
    x10 = 0x0000000000000050   x11 = 0xfffffffffffffff7
    x12 = 0x0000000000000009   x13 = 0xb4000077185a36f9
    x14 = 0x00000000fffffff7   x15 = 0x00000075f6cdef5c
    x16 = 0x00000075f6fe81d8   x17 = 0x000000796e17b430
    x18 = 0x00000075e49f4000   x19 = 0xb4000077785b843c
    x20 = 0x0000000000000006   x21 = 0x0000000000000002
    x22 = 0xb4000076f85e3fe0   x23 = 0x0000000000020000
    x24 = 0xb40000779857a540   x25 = 0xb40000779857a540
    x26 = 0x0000000000000000   x27 = 0x0000000000000000
    x28 = 0x0000000000000000    fp = 0x0000000000000003
     lr = 0x00000075f6efdc48    sp = 0x00000075e4fb1cf0
     pc = 0x00000075f6efdc48
    Found by: given as instruction pointer in context
 1  libtensorflowlite_jni.so + 0x24fca4
     sp = 0x00000075e4fb1d08    pc = 0x00000075f6efdca8
    Found by: stack scanning
 2  libtensorflowlite_jni.so + 0x24e7ac
     sp = 0x00000075e4fb1d48    pc = 0x00000075f6efc7b0
    Found by: stack scanning
 3  libtensorflowlite_jni.so + 0x2656ec
     sp = 0x00000075e4fb1db8    pc = 0x00000075f6f136f0
    Found by: stack scanning
 4  libtensorflowlite_jni.so + 0x24e83c
     sp = 0x00000075e4fb1dc8    pc = 0x00000075f6efc840
    Found by: stack scanning
 5  libtensorflowlite_jni.so + 0xd9f5c
     sp = 0x00000075e4fb1e08    pc = 0x00000075f6d87f60
    Found by: stack scanning
 6  libtensorflowlite_jni.so + 0xd93a4
     sp = 0x00000075e4fb1e48    pc = 0x00000075f6d873a8
    Found by: stack scanning
```
We heavily rely on TensorFlow Lite in our library, and unfortunately this is impacting a lot of users.

sebouh00 on (2024-11-05 11:30:12 UTC): Could it be related to the issue that's been addressed [here](https://github.com/google/ruy/commit/690c14c441387a4ea6e07a9ed89657cec8200b92)?

ninh-huynh (Issue Creator) on (2024-11-05 14:43:13 UTC): That sounds promising. Don't know if we could verify that issue locally. I can confirm that the issue affects our app mostly on Xiaomi & Vivo devices too.

sebouh00 on (2024-11-06 09:23:34 UTC): @ninh-huynh Is the crash that you experienced with the production build of TFL also at `libtensorflowlite_jni.so + 0x24fc48`?

ninh-huynh (Issue Creator) on (2024-11-07 09:05:07 UTC): Our team currently use custom TF Lite on production (with tflite_keep_symbols=true), so the report from Firebase is fully human readable (It crash at cpuinfo.cc - Line 66). Seem related to google/ruy#349

sebouh00 on (2024-11-07 10:25:11 UTC): @ninh-huynh Thanks. I just want to be sure our issues are related. As I don't have human readable stack traces, I was wondering whether my crash resembles yours, when you originally got it with the production TFL build. And whether after you switched to the custom TFL build in your app, the crash log that you shared from Firebase is the only TFL crash that you're seeing.

ninh-huynh (Issue Creator) on (2024-11-07 11:04:32 UTC): Our feature using TFL don't have any update recently. Just updating TFT to version 2.16.1 and bum, fresh crash appear without stacktrace. Also, I already suggest the TF team to turn that flag on (see #72877) but no response yet. As a client using the TFL library, I really really don't understand why they turn off that flag. As you can see, that crash non reproducable but user out there being impacted a lot. But they still don't have any suggestion to address this issue.

ninh-huynh (Issue Creator) on (2024-11-07 11:13:35 UTC): Here is the original crash when I upgraded TFL 2.16.1 (I got it from the user using the old version of the app, before I rebuilt TFL). It also crashes at 0x24fc48

```
         null pointer dereference: SIGSEGV  0x0000000000000008
#00 pc 0x24fc48 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#01 pc 0x24fca4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#02 pc 0x24e7ac libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#03 pc 0x335f44 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#04 pc 0x335eb4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#05 pc 0x24e83c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#06 pc 0xd9f5c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#07 pc 0x10754c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#08 pc 0x237fe libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#09 pc 0x27e5c4 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#10 pc 0x1073d8 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#11 pc 0x15d7e0 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#12 pc 0x15c0dc libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#13 pc 0xb0e04 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#14 pc 0x15a108 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#15 pc 0x30205c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#16 pc 0x30205c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#17 pc 0x301b5c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#18 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#19 pc 0x2fad4c libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#20 pc 0x2361c8 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#21 pc 0x803b8 libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
#22 pc 0x351230 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#23 pc 0x35119c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#24 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#25 pc 0x5b879c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#26 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#27 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#28 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#29 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#30 pc 0x5be120 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#31 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#32 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#33 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#34 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#35 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#36 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#37 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#38 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#39 pc 0x236d80 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#40 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#41 pc 0x5b9d74 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#42 pc 0x5af57c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#43 pc 0x5b8f54 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#44 pc 0x72218558
#45 pc 0x6ffb9b1f8c
#46 pc 0x6ffb9fe180
#47 pc 0x6ffb691830
#48 pc 0x6ffba02988
#49 pc 0x6ffb69fab4
#50 pc 0x6ffb69c108
#51 pc 0x3cbc50 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#52 pc 0xc15ffc libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#53 pc 0x6ffb69d2c4
#54 pc 0x33a7a4 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#55 pc 0x23a01c libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#56 pc 0x480b4 libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)
#57 pc 0x539078 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#58 pc 0x539108 libart.so (BuildId: 2452917c4ff69cbb6e75e5512260946b)
#59 pc 0xd33ac libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)
#60 pc 0xfba4c libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)
#61 pc 0x326ffc libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)
#62 pc 0x8e5f0 libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)
#63 pc 0xfb97c libc.so (BuildId: 3908c7c57fa04c64df24425cf16523cf)

```

sebouh00 on (2024-11-07 11:48:39 UTC): Thanks. Then it seems we have the same issue, and your symbolicated stack trace applies to all cases. We will likely patch it ourselves and use that patched version.

gaikwadrahul8 on (2024-11-26 06:31:11 UTC): Hi, @ninh-huynh 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/36

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 06:32:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75815"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75815"">No</a>

prilaga on (2024-12-27 17:50:08 UTC): Hi @gaikwadrahul8 
I tried LiteRT v.1.0.1 and it has the same issue. The crash is not fixed.

prilaga on (2024-12-27 17:53:19 UTC): Hi @ninh-huynh 
Did you find a solution to avoid the crash? Maybe you downgraded tensorflow version?
Thanks!

prilaga on (2024-12-27 20:20:07 UTC): Hi @sebouh00 , perhaps you have found some working solution? I have the same problem and I can't reproduce it on my devices. My android app randomly crashes.

sebouh00 on (2024-12-28 08:43:36 UTC): @prilaga Yes. We updated the `ruy` dependency to a version that fixes the crash, and rolled out our own TFL artifact. See [here](https://github.com/sentiance/tensorflow/commit/0aa8cf51def4232d777b56d56f4fe744f887ae1d). We first forked `ruy`, and applied some changes to the bazel build files to make it work with TFL's bazel version. See [here](https://github.com/sentiance/ruy/compare/c08ec52...cfa3b6f).

prilaga on (2024-12-29 00:46:16 UTC): @sebouh00 Thank you for the detailed information!

"
2526730440,issue,open,,Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.ops import data_flow_ops
import tensorflow as tf

q = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())
elems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
tmp_var64 = elems[4:8]

sess = tf.compat.v1.Session()
with sess.as_default():
    q.dequeue_up_to([])
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",Justobe,2024-09-15 04:17:25+00:00,['tilakrayal'],2024-09-17 06:46:53+00:00,,https://github.com/tensorflow/tensorflow/issues/75814,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2354682809, 'issue_id': 2526730440, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6166e9678d2a17ab7339701fe283c34b/untitled2108.ipynb).\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/d2c02d9f-85d5-4475-bca7-6b46b2a196fb)', 'created_at': datetime.datetime(2024, 9, 17, 6, 45, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-17 06:45:53 UTC): I was able to reproduce the issue on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6166e9678d2a17ab7339701fe283c34b/untitled2108.ipynb).


![image](https://github.com/user-attachments/assets/d2c02d9f-85d5-4475-bca7-6b46b2a196fb)

"
2526435628,issue,closed,completed,    tf-nightly[and-cuda] depends on archived version of nvidia-cudnn-cu12==8.9.7.29,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240910-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64

### Custom code

No

### OS platform and distribution

Deep Learning ARM64 Base OSS Nvidia Driver GPU AMI (Ubuntu 22.04) 20240823

### Mobile device

_No response_

### Python version

Python 3.11.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

 12.2

### GPU model and memory

_No response_

### Current behavior?

I expected it to install a version of nvidia-cudnn-cu12 that is available. 

pip index versions nvidia-cudnn-cu12
nvidia-cudnn-cu12 (9.4.0.58)
Available versions: 9.4.0.58, 9.3.0.75, 9.2.1.18, 9.2.0.82, 9.1.1.17, 9.1.0.70.post1, 9.0.0.312.post1, 9.0.0.312

### Standalone code to reproduce the issue

```shell
Its very simple to reproduce.  Launch an instance of Deep Learning ARM64 Base OSS Nvidia Driver GPU AMI (Ubuntu 22.04) 20240823 on a g5g.xlarge. 
ami-0f3ed94d52ca29367 on AWS.  Try to install tensorflow[and-cuda] or tf-nightly[and-cuda] and it will fail because tf-nightly[and-cuda] 2.18.0.dev20240617 depends on nvidia-cudnn-cu12==8.9.7.29; extra == ""and-cuda"".  That version isn't available anymore...

pip index versions nvidia-cudnn-cu12
nvidia-cudnn-cu12 (9.4.0.58)
Available versions: 9.4.0.58, 9.3.0.75, 9.2.1.18, 9.2.0.82, 9.1.1.17, 9.1.0.70.post1, 9.0.0.312.post1, 9.0.0.312
```


### Relevant log output

_No response_",danshome,2024-09-14 13:51:51+00:00,['Venkat6871'],2024-09-14 15:57:39+00:00,2024-09-14 15:57:36+00:00,https://github.com/tensorflow/tensorflow/issues/75785,"[('type:bug', 'Bug')]","[{'comment_id': 2351024057, 'issue_id': 2526435628, 'author': 'danshome', 'body': 'Nevermind, just realized I was going about this all wrong...I think.   I need to run the tensorflow-gpu docker image instead and run my models there. Sorry for wasting your time.', 'created_at': datetime.datetime(2024, 9, 14, 15, 6, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351024070, 'issue_id': 2526435628, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>', 'created_at': datetime.datetime(2024, 9, 14, 15, 6, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351031049, 'issue_id': 2526435628, 'author': 'danshome', 'body': ""Okay, so maybe not. \r\n\r\nStatus: Downloaded newer image for tensorflow/tensorflow:latest-gpu\r\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\nexec /usr/bin/python: exec format error\r\n\r\nHere's my issue, according to AWS the recommended instance type for deep learning model training is the G5G instance type, there is only one AMI that AWS publishes that supports the G5G instance type, and it's the Deep Learning ARM64 Base OSS Nvidia Driver GPU AMI (Ubuntu 22.04) 20240823 on a g5g.xlarge. \r\nami-0f3ed94d52ca29367.  And it seems like tensorflow only publishes docker images for amd64.   Do you have any guidance here?"", 'created_at': datetime.datetime(2024, 9, 14, 15, 26, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351043120, 'issue_id': 2526435628, 'author': 'danshome', 'body': ""Nevermind, I spoke with AWS support and they said to use the G5 and not the G5G since tensorflow doesn't support it.   I'll leave this open, because it would be nice if tensorflow published ARM64 docker images."", 'created_at': datetime.datetime(2024, 9, 14, 15, 57, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351043131, 'issue_id': 2526435628, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>', 'created_at': datetime.datetime(2024, 9, 14, 15, 57, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351043252, 'issue_id': 2526435628, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>', 'created_at': datetime.datetime(2024, 9, 14, 15, 57, 38, tzinfo=datetime.timezone.utc)}]","danshome (Issue Creator) on (2024-09-14 15:06:10 UTC): Nevermind, just realized I was going about this all wrong...I think.   I need to run the tensorflow-gpu docker image instead and run my models there. Sorry for wasting your time.

google-ml-butler[bot] on (2024-09-14 15:06:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>

danshome (Issue Creator) on (2024-09-14 15:26:49 UTC): Okay, so maybe not. 

Status: Downloaded newer image for tensorflow/tensorflow:latest-gpu
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
exec /usr/bin/python: exec format error

Here's my issue, according to AWS the recommended instance type for deep learning model training is the G5G instance type, there is only one AMI that AWS publishes that supports the G5G instance type, and it's the Deep Learning ARM64 Base OSS Nvidia Driver GPU AMI (Ubuntu 22.04) 20240823 on a g5g.xlarge. 
ami-0f3ed94d52ca29367.  And it seems like tensorflow only publishes docker images for amd64.   Do you have any guidance here?

danshome (Issue Creator) on (2024-09-14 15:57:08 UTC): Nevermind, I spoke with AWS support and they said to use the G5 and not the G5G since tensorflow doesn't support it.   I'll leave this open, because it would be nice if tensorflow published ARM64 docker images.

google-ml-butler[bot] on (2024-09-14 15:57:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>

google-ml-butler[bot] on (2024-09-14 15:57:38 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75785"">No</a>

"
2526001616,issue,closed,completed,TensorFlow does not support AMD Graphics Cards on Windows,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Windows 10 64-bit

### Mobile device

_No response_

### Python version

3.12

### Bazel version

Not applicable (installed from binary)

### GCC/compiler version

Not applicable

### CUDA/cuDNN version

Not applicable (as I am using an AMD GPU)

### GPU model and memory

AMD Radeon RX 580, 8GB VRAM

### Current behavior?

TensorFlow does not detect or utilize my AMD GPU for training models on Windows. It defaults to CPU even though I have correctly installed the GPU drivers and AMD ROCm libraries where supported.
## Expected behavior:
TensorFlow should detect the AMD GPU and utilize it for model training to improve performance.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))

# Expected output:
# Num GPUs Available: 1

# Current output:
# Num GPUs Available: 0
```


### Relevant log output

```shell
2024-09-14 12:34:56.789123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_110.dll
2024-09-14 12:34:56.790123: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2024-09-14 12:34:56.790123: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up.
```
",dawoodshahzad07,2024-09-14 05:18:49+00:00,['tilakrayal'],2024-12-22 02:05:55+00:00,2024-12-22 02:05:52+00:00,https://github.com/tensorflow/tensorflow/issues/75773,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2352225548, 'issue_id': 2526001616, 'author': 'tilakrayal', 'body': '@dawoodshahzad07,\r\nTensorflow utilizes nvidia toolkit for utilizing GPU, this toolkit was specifically designed for Nvidia gpu’s. \r\nAnd tf-rocm was developed and maintained by AMD for using Tensorflow with AMD GPU, which is not handling the tensorflow team and also not mentioned in the official document.\r\n\r\nhttps://www.tensorflow.org/install/pip\r\n\r\nThank You.', 'created_at': datetime.datetime(2024, 9, 16, 7, 48, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352236615, 'issue_id': 2526001616, 'author': 'dawoodshahzad07', 'body': ""Thank you for clarifying TensorFlow's use of NVIDIA's toolkit and the details about tf-rocm for AMD GPUs. I appreciate the helpful insight regarding its maintenance and documentation."", 'created_at': datetime.datetime(2024, 9, 16, 7, 55, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523301672, 'issue_id': 2526001616, 'author': 'tilakrayal', 'body': '@dawoodshahzad07,\r\nAs Tensorflow utilizes nvidia toolkit for utilizing GPU, this toolkit was specifically designed for Nvidia gpu’s. That information is available in the official document and ROCM is not maintained by the tensorflow which cannot be updated. Please feel free to contribute if the mentioned information is required for the documentation. Thank you!', 'created_at': datetime.datetime(2024, 12, 6, 13, 52, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542662887, 'issue_id': 2526001616, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 14, 2, 5, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300127, 'issue_id': 2526001616, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300151, 'issue_id': 2526001616, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75773"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75773"">No</a>', 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 55, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-16 07:48:46 UTC): @dawoodshahzad07,
Tensorflow utilizes nvidia toolkit for utilizing GPU, this toolkit was specifically designed for Nvidia gpu’s. 
And tf-rocm was developed and maintained by AMD for using Tensorflow with AMD GPU, which is not handling the tensorflow team and also not mentioned in the official document.

https://www.tensorflow.org/install/pip

Thank You.

dawoodshahzad07 (Issue Creator) on (2024-09-16 07:55:11 UTC): Thank you for clarifying TensorFlow's use of NVIDIA's toolkit and the details about tf-rocm for AMD GPUs. I appreciate the helpful insight regarding its maintenance and documentation.

tilakrayal (Assginee) on (2024-12-06 13:52:49 UTC): @dawoodshahzad07,
As Tensorflow utilizes nvidia toolkit for utilizing GPU, this toolkit was specifically designed for Nvidia gpu’s. That information is available in the official document and ROCM is not maintained by the tensorflow which cannot be updated. Please feel free to contribute if the mentioned information is required for the documentation. Thank you!

github-actions[bot] on (2024-12-14 02:05:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-22 02:05:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-22 02:05:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75773"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75773"">No</a>

"
2521723080,issue,open,,`tf_cond.cond` and `tf.function` could cause an aborted issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `tf_cond.cond` and `tf.function`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import ops
from tensorflow.python.ops import cond as tf_cond
from tensorflow.python.ops import control_flow_assert
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.platform import test

import tensorflow as tf
sess = tf.compat.v1.Session()

@tf.function
def func1():
    with sess.as_default():
        with ops.device(test.gpu_device_name()):
            pred = constant_op.constant([True, False])

        def fn1():
            return control_flow_ops.no_op()

        def fn2():
            with ops.device('/cpu:0'):
                return control_flow_assert.Assert(False, ['Wrong!'])

        r = tf_cond.cond(pred, fn1, fn2)

func1()
```


### Relevant log output

```shell
> 2024-09-12 16:43:14.634438: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
> Aborted (core dumped)
```
",Justobe,2024-09-12 08:31:32+00:00,['Venkat6871'],2024-09-20 10:13:29+00:00,,https://github.com/tensorflow/tensorflow/issues/75625,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2348030689, 'issue_id': 2521723080, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/fef3f969d4629b2702b5d4fd66b4b257/75625_2-15-2-17-1-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 13, 4, 59, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-13 04:59:09 UTC): I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/fef3f969d4629b2702b5d4fd66b4b257/75625_2-15-2-17-1-nightly-v.ipynb) here for reference.
Thank you!

"
2521660963,issue,open,,Using `fft_ops`  would cause an `aborted issue`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `fft_ops`. The code is as follows:

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops.signal import fft_ops
import tensorflow as tf
tf.compat.v1.disable_eager_execution()


def _tf_ifft(x, rank, fft_length=None, feed_dict=None):
    with tf.compat.v1.Session() as sess:
        return sess.run(_tf_ifft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)

def _tf_ifft_for_rank(rank):
    if rank == 1:
        return fft_ops.irfft
    elif rank == 2:
        return fft_ops.irfft2d
    elif rank == 3:
        return fft_ops.irfft3d
    else:
        raise ValueError('invalid rank')


rank = 1
extra_dims = 0
np_rtype = np.float32
np_ctype = np.complex64
dims = rank + extra_dims
x = np.zeros((1,) * dims).astype(np_ctype)
tmp_var22 = _tf_ifft(x, rank).shape
```


### Relevant log output

```shell
DUCC FFT c2r failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
",Justobe,2024-09-12 08:06:13+00:00,['tilakrayal'],2024-10-14 03:24:32+00:00,,https://github.com/tensorflow/tensorflow/issues/75624,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2351351573, 'issue_id': 2521660963, 'author': 'Justobe', 'body': '@tilakrayal Hi~ could you reproduce this issue? : )', 'created_at': datetime.datetime(2024, 9, 15, 4, 13, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352661263, 'issue_id': 2521660963, 'author': 'tilakrayal', 'body': 'I was able to reproduce the code on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ec6e182a0574e96b08884842e986a3d1/untitled2106.ipynb).\r\n\r\n![Screenshot 2024-09-16 4 55 09 PM](https://github.com/user-attachments/assets/9036f4d9-94c5-4550-a32c-8bf05c35ce68)', 'created_at': datetime.datetime(2024, 9, 16, 11, 30, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409870839, 'issue_id': 2521660963, 'author': 'cybersupersoap', 'body': '@tilakrayal I found this issue also exists on `fft_ops.irfft2d` and `fft_ops.irfft3d`(i.e., setting rank=2 or rank=3), Please find [gist](https://colab.research.google.com/drive/1c9Kw1Ga_vJiGwbtPYe6BYPd5ElfBwaIT?usp=sharing) here for reference. \r\nThank you!', 'created_at': datetime.datetime(2024, 10, 14, 3, 24, 31, tzinfo=datetime.timezone.utc)}]","Justobe (Issue Creator) on (2024-09-15 04:13:37 UTC): @tilakrayal Hi~ could you reproduce this issue? : )

tilakrayal (Assginee) on (2024-09-16 11:30:54 UTC): I was able to reproduce the code on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ec6e182a0574e96b08884842e986a3d1/untitled2106.ipynb).

![Screenshot 2024-09-16 4 55 09 PM](https://github.com/user-attachments/assets/9036f4d9-94c5-4550-a32c-8bf05c35ce68)

cybersupersoap on (2024-10-14 03:24:31 UTC): @tilakrayal I found this issue also exists on `fft_ops.irfft2d` and `fft_ops.irfft3d`(i.e., setting rank=2 or rank=3), Please find [gist](https://colab.research.google.com/drive/1c9Kw1Ga_vJiGwbtPYe6BYPd5ElfBwaIT?usp=sharing) here for reference. 
Thank you!

"
2521630173,issue,open,,Using `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when I used API `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_random_index_shuffle_ops
from tensorflow.python.ops import math_ops

seed = (74, 117)
seed_dtype = dtypes.int32
max_index = 129
index_dtype = dtypes.int32
rounds = 4

seen = (max_index + 1) * [False]
seed = math_ops.cast([seed[0], seed[1], 42], seed_dtype)
for index in range(max_index + 1):
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=rounds)
    # rounds = -2 causes the segmentfault
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=-2)
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",Justobe,2024-09-12 07:51:54+00:00,['Venkat6871'],2024-09-20 10:12:23+00:00,,https://github.com/tensorflow/tensorflow/issues/75623,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2348023962, 'issue_id': 2521630173, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the **[gist](https://colab.research.google.com/gist/Venkat6871/6e20677b00df7d4bf1be058b5a3c7d8b/75623_tf-2-15-2-17-1-nightly-v.ipynb)** here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 13, 4, 47, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-13 04:47:46 UTC): I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the **[gist](https://colab.research.google.com/gist/Venkat6871/6e20677b00df7d4bf1be058b5a3c7d8b/75623_tf-2-15-2-17-1-nightly-v.ipynb)** here for reference.
Thank you!

"
2521618852,issue,open,,Got one aborted issue when using `data_flow_ops.MapStagingArea` ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `data_flow_ops.MapStagingArea`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.platform import test
import tensorflow as tf


with ops.Graph().as_default() as g:
    with ops.device('/cpu:0'):
        x = array_ops.placeholder(dtypes.float32)
        pi = array_ops.placeholder(dtypes.int64)
        gi = array_ops.placeholder(dtypes.int64)
        v = 2.0 * (array_ops.zeros([]) + x)
    with ops.device(test.gpu_device_name()):
        stager = data_flow_ops.MapStagingArea([dtypes.float32])
        stage = stager.put(pi, [v], [0])
        k, y = stager.get([])
        y = math_ops.reduce_max(math_ops.matmul(y, y))
g.finalize()
with tf.compat.v1.Session(graph=g) as sess:
    sess.run(stage, feed_dict={x: -1, pi: 0})
    for i in range(10):
        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})
```


### Relevant log output

```shell
2024-09-12 15:57:44.445189: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted (core dumped)
```
",Justobe,2024-09-12 07:47:04+00:00,['tilakrayal'],2024-09-17 06:46:44+00:00,,https://github.com/tensorflow/tensorflow/issues/75622,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2351351951, 'issue_id': 2521618852, 'author': 'Justobe', 'body': '@tilakrayal Hi~ could you reproduce this issue? : )', 'created_at': datetime.datetime(2024, 9, 15, 4, 14, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352246985, 'issue_id': 2521618852, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a4ad9a7e7bb966f6f9203b0a5c74e80f/untitled2105.ipynb).\r\n\r\n![Screenshot 2024-09-16 4 48 10 PM](https://github.com/user-attachments/assets/7fb4ad9b-8e98-4cdc-bfda-058f63ec1a7f)', 'created_at': datetime.datetime(2024, 9, 16, 8, 1, 11, tzinfo=datetime.timezone.utc)}]","Justobe (Issue Creator) on (2024-09-15 04:14:12 UTC): @tilakrayal Hi~ could you reproduce this issue? : )

tilakrayal (Assginee) on (2024-09-16 08:01:11 UTC): I was able to reproduce the issue on tensorflow v2.16, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a4ad9a7e7bb966f6f9203b0a5c74e80f/untitled2105.ipynb).

![Screenshot 2024-09-16 4 48 10 PM](https://github.com/user-attachments/assets/7fb4ad9b-8e98-4cdc-bfda-058f63ec1a7f)

"
2521607876,issue,open,,An `aborted issue` could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` . 

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()
with sess.as_default():
    a = math_ops.cast([2], dtypes.int32)
    b = math_ops.cast([1], dtypes.int32)
    value = np.random.rand(11, 11)
    array_ops.split(value, [a, b])
```


### Relevant log output

```shell
2024-09-12 15:49:03.711972: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
",Justobe,2024-09-12 07:42:00+00:00,['Venkat6871'],2024-09-20 10:11:33+00:00,,https://github.com/tensorflow/tensorflow/issues/75621,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2348020038, 'issue_id': 2521607876, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/7415f79c049e8cd1ddf2dbfda49933b0/75621_tf-2-15-2-17-1-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 13, 4, 41, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-13 04:41:55 UTC): I tried to run your code on Colab using TF v2.15.1, 2.17.1 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/7415f79c049e8cd1ddf2dbfda49933b0/75621_tf-2-15-2-17-1-nightly-v.ipynb) here for reference.
Thank you!

"
2520598339,issue,closed,completed,traceback issue,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when tryting to run appears

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\PC\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\PC\Desktop\SEÑAS\app.py"", line 12, in <module>
    import mediapipe as mp
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\__init__.py"", line 17, in <module>
    import mediapipe.tasks.python as tasks
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\tasks\python\__init__.py"", line 17, in <module>
    from . import audio
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\tasks\python\audio\__init__.py"", line 18, in <module>
    import mediapipe.tasks.python.audio.audio_classifier
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\tasks\python\audio\audio_classifier.py"", line 26, in <module>
    from mediapipe.tasks.python.audio.core import base_audio_task_api
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\tasks\python\audio\core\base_audio_task_api.py"", line 25, in <module>
    from mediapipe.tasks.python.core.optional_dependencies import doc_controls
  File ""C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\mediapipe\tasks\python\core\optional_dependencies.py"", line 20, in <module>
    from tensorflow.tools.docs import doc_controls
  File ""C:\Users\PC\AppData\Roaming\Python\Python311\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\PC\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\PC\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).


Failed to load the native TensorFlow runtime.
```
",kevo200,2024-09-11 19:27:55+00:00,['tilakrayal'],2024-09-30 02:03:31+00:00,2024-09-30 02:03:28+00:00,https://github.com/tensorflow/tensorflow/issues/75585,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2345607822, 'issue_id': 2520598339, 'author': 'tilakrayal', 'body': '@kevo200,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 12, 8, 29, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367112945, 'issue_id': 2520598339, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 23, 2, 1, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381863519, 'issue_id': 2520598339, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 30, 2, 3, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381863560, 'issue_id': 2520598339, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75585"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75585"">No</a>', 'created_at': datetime.datetime(2024, 9, 30, 2, 3, 30, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-12 08:29:02 UTC): @kevo200,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-09-23 02:01:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-30 02:03:27 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-30 02:03:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75585"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75585"">No</a>

"
2520347300,issue,closed,completed,Auto-Encoding Variational Bayes,[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114) implementation in [tensorflow/examples](https://github.com/tensorflow/examples).,AtrejuArtax,2024-09-11 17:34:37+00:00,['Venkat6871'],2024-09-28 02:00:57+00:00,2024-09-28 02:00:53+00:00,https://github.com/tensorflow/tensorflow/issues/75578,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2348387549, 'issue_id': 2520347300, 'author': 'Venkat6871', 'body': 'Hi **@AtrejuArtax** ,\r\n\r\nCould you please elaborate on your issue? We noticed that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled out. Could you please complete it, as this helps us analyze the problem more effectively? Please include the TensorFlow version, the steps you followed before encountering the error, or a standalone code/Colab gist to reproduce the issue.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 13, 8, 46, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364920878, 'issue_id': 2520347300, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380351204, 'issue_id': 2520347300, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 28, 2, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380351232, 'issue_id': 2520347300, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75578"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75578"">No</a>', 'created_at': datetime.datetime(2024, 9, 28, 2, 0, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-13 08:46:17 UTC): Hi **@AtrejuArtax** ,

Could you please elaborate on your issue? We noticed that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled out. Could you please complete it, as this helps us analyze the problem more effectively? Please include the TensorFlow version, the steps you followed before encountering the error, or a standalone code/Colab gist to reproduce the issue.

Thank you!

github-actions[bot] on (2024-09-21 01:58:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-28 02:00:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-28 02:00:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75578"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75578"">No</a>

"
2519550328,issue,closed,completed,Python Wheel generation of TensorFlow Lite 2.17 for ARMv7l 32 bits not working,"I have previously crosscompiled different versions of TensorFlow lite (2.14, 2.15.1, 2.16.2) for Python 3.10 using CMAKE and following the instructions from the [website ](https://ai.google.dev/edge/litert/build/cmake_pip). 

So far the only changes required were to just set my armhf flags as:

` echo ""ARMCC_FLAGS=\""-march=armv7-a -mfpu=neon-vfpv3 -funsafe-math-optimizations \
`
adjust the python version in the MakeFile (tensorflow/lite/tools/pip_package/Makefile) and run the make command as:

` make -C tensorflow/lite/tools/pip_package docker-build \ TENSORFLOW_TARGET=armhf PYTHON_VERSION=3.10
`
Now, I have tried the same approach for the release 2.17.0 ( ad6d8cc ) and although the build is executed and the wheel is generated without errors, I keep getting the following errorat the moment of importing the interpreter:

```
python3 simpletest.py 
Traceback (most recent call last):
  File ""/mnt/simpletest.py"", line 2, in <module>
    import tflite_runtime.interpreter as tflite
  File ""/usr/local/lib/python3.10/dist-packages/tflite_runtime/interpreter.py"", line 33, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /usr/local/lib/python3.10/dist-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: TfLiteXNNPackDelegateOptionsDefault
```
",paguilar-pxc,2024-09-11 11:59:31+00:00,"['terryheo', 'pkgoogle']",2024-11-26 06:42:52+00:00,2024-11-26 06:42:50+00:00,https://github.com/tensorflow/tensorflow/issues/75562,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2354139951, 'issue_id': 2519550328, 'author': 'gaikwadrahul8', 'body': 'Hi, @paguilar-pxc\r\n\r\nI apologize for the delayed response, if possible could you please help us with exact steps which you followed to replicate the same behavior from our end to investigate this issue further from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 16, 22, 28, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354684028, 'issue_id': 2519550328, 'author': 'paguilar-pxc', 'body': 'Hello @gaikwadrahul8, \r\n\r\nThe steps are not that different from my original comment. I write here a more detailed description:\r\n\r\n1. I downloaded the source code from the latest stable release (2.17) ([https://github.com/tensorflow/tensorflow/releases/tag/v2.17.0](url) \r\n\r\n2. Extract the content to a folder. \r\n\r\n3. Navigate to ""Makefile"" in `tensorflow-2.17.0/tensorflow/lite/tools/pip_package/Makefile`  and modify the base image, python version and numpy version to match those that I will be using\r\n\r\n![image](https://github.com/user-attachments/assets/906a98ec-8c70-41a5-933e-f8be8b40b00a)\r\n\r\n5. Then I proceed to navigate to ""download_toolchains.sh"" located in `tensorflow-2.17.0/tensorflow/lite/tools/cmake/download_toolchains.sh` and modify the flags that come by default for armh\r\n\r\n![image](https://github.com/user-attachments/assets/6b6eab30-46c5-45b9-9f8a-ef02734ecb0d)\r\n\r\n6. Finally, from within the folder `tensorflow-2.17.0` I run the command `make -C tensorflow/lite/tools/pip_package docker-build \\ TENSORFLOW_TARGET=armhf PYTHON_VERSION=3.10` \r\n\r\n7. After the process is finished, I take the Python wheel from `tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist` to my device\r\n\r\n8. From the device, it doesnt really matter what I try to run because I cannot even import the interpreter (see message in original post). \r\n\r\nSOME EXTRA INFORMATION: The target device is not a raspberry pi (hence the tweak of the flags). I have attempted further variations with exactly the same results (wheel succesfully built but cannot import the interpreter in the edge device). Some of the variants I tried were:\r\n\r\na. Using the latest source code instead of the stable release. Same results.\r\nb. Using a different base image (Ubuntu 20.04). Same results.\r\nc. Using different versions of numpy. Same results\r\nd. Specifying further flags that I know are present in my target hardware. Same results.\r\ne. Setting the cross compilation for Python 3.11 instead of 3.10. Same results. \r\nf. Trying to import the interpreter from the source file instead of using simply the python command. Same results.\r\ng. \r\n\r\nDue to the impossibility of getting it to work, I resorted to ask here for support. Notice that, just as I mentioned before, the process had always worked flawlessly for previous versions (both for Python 3.10 and 3.11) and only with the 2.17 and the up-to-date code is that I get that error with the interpreter.', 'created_at': datetime.datetime(2024, 9, 17, 6, 46, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358917207, 'issue_id': 2519550328, 'author': 'gaikwadrahul8', 'body': ""Hi, @paguilar-pxc\r\n\r\nThank you for providing the detailed steps to replicate similar behavior from our end, I see the wheel file is generating the successfully in the `dist` folder for reference I've added output log but while importing the interpreter I'm also getting the same error message so will have to dig more into this issue\r\n\r\n```\r\n(tf-test-1) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/issue-75562/tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist$ ls\r\ntflite_runtime-2.17.0-cp310-cp310-linux_armv7l.whl  tflite_runtime-2.17.0.linux-armv7l.tar.gz\r\n(tf-test-1) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/issue-75562/tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist$ \r\n```\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 18, 16, 27, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360083296, 'issue_id': 2519550328, 'author': 'paguilar-pxc', 'body': 'Hello @gaikwadrahul8,\r\n\r\nthank you for the feedback. It is good to see that you got to the same result when following my steps. \r\n\r\nI keep looking forward to your continued progress in finding solution or workaround. 👍\r\n\r\nThank you.', 'created_at': datetime.datetime(2024, 9, 19, 6, 16, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364311131, 'issue_id': 2519550328, 'author': 'MrChike', 'body': ""Hello Team, \r\n\r\nI'm new here and would be participating on this task. Look forward to working with you guys"", 'created_at': datetime.datetime(2024, 9, 20, 18, 35, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376439742, 'issue_id': 2519550328, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\n\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 26, 9, 33, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376744380, 'issue_id': 2519550328, 'author': 'DayMiaN14', 'body': 'Kan iemand my op weg helpen met de herkenning software van Tensorflow/Python\r\nJe kan me eventueel mailen naar privehobby@hotmail.com\r\nzoek hulp bij het programmeren van herkenning software \r\nalle hulp is welkom', 'created_at': datetime.datetime(2024, 9, 26, 12, 2, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2377915635, 'issue_id': 2519550328, 'author': 'pkgoogle', 'body': 'Seems to be a valid bug, @terryheo, can you please take a look?', 'created_at': datetime.datetime(2024, 9, 26, 20, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499788029, 'issue_id': 2519550328, 'author': 'gaikwadrahul8', 'body': ""Hi, @paguilar-pxc\r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/37\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 6, 42, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499788340, 'issue_id': 2519550328, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75562"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75562"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 6, 42, 51, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-16 22:28:46 UTC): Hi, @paguilar-pxc

I apologize for the delayed response, if possible could you please help us with exact steps which you followed to replicate the same behavior from our end to investigate this issue further from our end ? 

Thank you for your cooperation and patience.

paguilar-pxc (Issue Creator) on (2024-09-17 06:46:34 UTC): Hello @gaikwadrahul8, 

The steps are not that different from my original comment. I write here a more detailed description:

1. I downloaded the source code from the latest stable release (2.17) ([https://github.com/tensorflow/tensorflow/releases/tag/v2.17.0](url) 

2. Extract the content to a folder. 

3. Navigate to ""Makefile"" in `tensorflow-2.17.0/tensorflow/lite/tools/pip_package/Makefile`  and modify the base image, python version and numpy version to match those that I will be using

![image](https://github.com/user-attachments/assets/906a98ec-8c70-41a5-933e-f8be8b40b00a)

5. Then I proceed to navigate to ""download_toolchains.sh"" located in `tensorflow-2.17.0/tensorflow/lite/tools/cmake/download_toolchains.sh` and modify the flags that come by default for armh

![image](https://github.com/user-attachments/assets/6b6eab30-46c5-45b9-9f8a-ef02734ecb0d)

6. Finally, from within the folder `tensorflow-2.17.0` I run the command `make -C tensorflow/lite/tools/pip_package docker-build \ TENSORFLOW_TARGET=armhf PYTHON_VERSION=3.10` 

7. After the process is finished, I take the Python wheel from `tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist` to my device

8. From the device, it doesnt really matter what I try to run because I cannot even import the interpreter (see message in original post). 

SOME EXTRA INFORMATION: The target device is not a raspberry pi (hence the tweak of the flags). I have attempted further variations with exactly the same results (wheel succesfully built but cannot import the interpreter in the edge device). Some of the variants I tried were:

a. Using the latest source code instead of the stable release. Same results.
b. Using a different base image (Ubuntu 20.04). Same results.
c. Using different versions of numpy. Same results
d. Specifying further flags that I know are present in my target hardware. Same results.
e. Setting the cross compilation for Python 3.11 instead of 3.10. Same results. 
f. Trying to import the interpreter from the source file instead of using simply the python command. Same results.
g. 

Due to the impossibility of getting it to work, I resorted to ask here for support. Notice that, just as I mentioned before, the process had always worked flawlessly for previous versions (both for Python 3.10 and 3.11) and only with the 2.17 and the up-to-date code is that I get that error with the interpreter.

gaikwadrahul8 on (2024-09-18 16:27:32 UTC): Hi, @paguilar-pxc

Thank you for providing the detailed steps to replicate similar behavior from our end, I see the wheel file is generating the successfully in the `dist` folder for reference I've added output log but while importing the interpreter I'm also getting the same error message so will have to dig more into this issue

```
(tf-test-1) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/issue-75562/tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist$ ls
tflite_runtime-2.17.0-cp310-cp310-linux_armv7l.whl  tflite_runtime-2.17.0.linux-armv7l.tar.gz
(tf-test-1) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/issue-75562/tensorflow-2.17.0/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.10/dist$ 
```
Thank you for your cooperation and patience.

paguilar-pxc (Issue Creator) on (2024-09-19 06:16:52 UTC): Hello @gaikwadrahul8,

thank you for the feedback. It is good to see that you got to the same result when following my steps. 

I keep looking forward to your continued progress in finding solution or workaround. 👍

Thank you.

MrChike on (2024-09-20 18:35:54 UTC): Hello Team, 

I'm new here and would be participating on this task. Look forward to working with you guys

gaikwadrahul8 on (2024-09-26 09:33:05 UTC): Hi, @pkgoogle 

Please take look into this issue. Thank you

DayMiaN14 on (2024-09-26 12:02:12 UTC): Kan iemand my op weg helpen met de herkenning software van Tensorflow/Python
Je kan me eventueel mailen naar privehobby@hotmail.com
zoek hulp bij het programmeren van herkenning software 
alle hulp is welkom

pkgoogle (Assginee) on (2024-09-26 20:52:26 UTC): Seems to be a valid bug, @terryheo, can you please take a look?

gaikwadrahul8 on (2024-11-26 06:42:36 UTC): Hi, @paguilar-pxc
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/37

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 06:42:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75562"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75562"">No</a>

"
2519370689,issue,open,,MLIR quantizer produces asymmetric quantization for int16 activations,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0

### 2. Code

```
with open(""calibrated.tflite"", ""rb"") as model_file:
    calibrated_model = model_file.read()

inference_type = convert.convert_inference_tf_type_to_tflite_type(tf.dtypes.int16)
tflite_model = convert.mlir_quantize(calibrated_model,
                                     fully_quantize=True,
                                     inference_type=inference_type,
                                     input_data_type=tf.dtypes.int16,
                                     output_data_type=tf.dtypes.int16)

with open(""quantized.tflite"", ""wb"") as model_file:
    model_file.write(tflite_model)

```

Input file:
[calibrated.zip](https://github.com/user-attachments/files/16962415/calibrated.zip)

Output file:
[quantized.zip](https://github.com/user-attachments/files/16962422/quantized.zip)

### 3. Failure after conversion

The model cannot run on TFLite Micro with CMSIS-NN, as its fully connected kernel supports only symmetric quantization for int16 activations.

### 5. (optional) Any other info / logs

The quantizer already tries to take into account the int16 requirements here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/common/quantization_lib/quantization_utils.h#L238 and here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/common/quantization_lib/quantization_utils.h#L266
But it's not enough, as it should also set `narrow_range` before calling `quantfork::fakeQuantAttrsToType` here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/common/quantization_lib/quantization_utils.h#L248 and here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/common/quantization_lib/quantization_utils.h#L274
Otherwise the zero point might drift to -1, as it happens with the attached files.

",tagunil,2024-09-11 10:39:19+00:00,"['paulinesho', 'pkgoogle']",2024-09-27 15:51:04+00:00,,https://github.com/tensorflow/tensorflow/issues/75558,"[('stat:contribution welcome', 'Status - Contributions welcome'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2360952661, 'issue_id': 2519370689, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 19, 13, 13, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361883951, 'issue_id': 2519370689, 'author': 'pkgoogle', 'body': 'Hi @tagunil, would you be willing to submit a PR? As it seems you have already identified root cause here.', 'created_at': datetime.datetime(2024, 9, 19, 18, 24, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361907658, 'issue_id': 2519370689, 'author': 'tagunil', 'body': ""Hi @pkgoogle, I would be happy to submit a PR for this issue and a couple of others, but, unfortunately, we're still struggling with the CLA for the time being."", 'created_at': datetime.datetime(2024, 9, 19, 18, 36, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369295128, 'issue_id': 2519370689, 'author': 'pkgoogle', 'body': 'Hi @paulinesho, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 9, 23, 20, 19, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379591547, 'issue_id': 2519370689, 'author': 'tagunil', 'body': ""I've checked https://github.com/tensorflow/tensorflow/pull/76584 and that's what we need, thanks @paulinesho!"", 'created_at': datetime.datetime(2024, 9, 27, 15, 51, 3, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-19 13:13:03 UTC): Hi, @pkgoogle

Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-19 18:24:49 UTC): Hi @tagunil, would you be willing to submit a PR? As it seems you have already identified root cause here.

tagunil (Issue Creator) on (2024-09-19 18:36:59 UTC): Hi @pkgoogle, I would be happy to submit a PR for this issue and a couple of others, but, unfortunately, we're still struggling with the CLA for the time being.

pkgoogle (Assginee) on (2024-09-23 20:19:22 UTC): Hi @paulinesho, can you please take a look? Thanks.

tagunil (Issue Creator) on (2024-09-27 15:51:03 UTC): I've checked https://github.com/tensorflow/tensorflow/pull/76584 and that's what we need, thanks @paulinesho!

"
2519215561,issue,closed,completed,Bazel build fails on RISC-V (riscv64) architecture for TensorFlow 2.17.0,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux openeuler-riscv-4-2 6.6.0

### Mobile device

no

### Python version

3.11.6

### Bazel version

6.5.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current behavior?

I am trying to build TensorFlow 2.17.0 on a RISC-V (riscv64) architecture using Bazel 6.5.0. The Bazel version is installed from an RPM package that has successfully passed C++ and Java tests in examples.
When I run the following command to build the TensorFlow pip package:The build fails with the following error:RROR: An error occurred during the fetch of repository 'python': No platform declared for host OS linux on arch riscv64
It seems that the `riscv64` architecture is not supported in the current Bazel rules for Python toolchains.
### Full error log:ERROR: An error occurred during the fetch of repository 'python':
No platform declared for host OS linux on arch riscv64
ERROR: Error computing the main repository mapping: no such package '@python//': No platform declared for host OS linux on arch riscv64

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout tags/v2.17.0
./configure
export TF_PYTHON_VERSION=3.11
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
```


### Relevant log output

```shell
INFO: Reading 'startup' options from /home/chenweijia/tf/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=119
INFO: Reading rc options for 'clean' from /home/chenweijia/tf/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /home/chenweijia/tf/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'clean' from /home/chenweijia/tf/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/chenweijia/tf/venv00/bin/python3 --action_env PYTHON_LIB_PATH=/home/chenweijia/tf/venv00/lib/python3.11/site-packages --python_path=/home/chenweijia/tf/venv00/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/chenweijia/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/chenweijia/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/chenweijia/tf/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/chenweijia/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
(venv00) [openeuler@openeuler-riscv-4-2 tensorflow]$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
Starting local Bazel server and connecting to it...
... still trying to connect to local Bazel server (251492) after 10 seconds ...
INFO: Reading 'startup' options from /home/chenweijia/tf/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=119
INFO: Reading rc options for 'build' from /home/chenweijia/tf/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/chenweijia/tf/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/chenweijia/tf/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/chenweijia/tf/venv00/bin/python3 --action_env PYTHON_LIB_PATH=/home/chenweijia/tf/venv00/lib/python3.11/site-packages --python_path=/home/chenweijia/tf/venv00/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/chenweijia/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/chenweijia/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/chenweijia/tf/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/chenweijia/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository python instantiated at:
  /home/chenweijia/tf/tensorflow/WORKSPACE:45:27: in <toplevel>
  /home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/repositories.bzl:603:22: in python_register_toolchains
Repository rule toolchain_aliases defined at:
  /home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/private/toolchains_repo.bzl:236:36: in <toplevel>
ERROR: An error occurred during the fetch of repository 'python':
   Traceback (most recent call last):
        File ""/home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl
                host_platform = get_host_platform(os_name, arch)
        File ""/home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform
                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))
Error in fail: No platform declared for host OS linux on arch riscv64
ERROR: /home/chenweijia/tf/tensorflow/WORKSPACE:45:27: fetching toolchain_aliases rule //external:python: Traceback (most recent call last):
        File ""/home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl
                host_platform = get_host_platform(os_name, arch)
        File ""/home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform
                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))
Error in fail: No platform declared for host OS linux on arch riscv64
INFO: Repository bazel_skylib instantiated at:
  /home/chenweijia/tf/tensorflow/WORKSPACE:10:13: in <toplevel>
Repository rule http_archive defined at:
  /home/openeuler/.cache/bazel/_bazel_openeuler/6f797a0931e8bb7410946b7d443abe30/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in <toplevel>
ERROR: Error computing the main repository mapping: no such package '@python//': No platform declared for host OS linux on arch riscv64
Loading:
```
",hansu2022,2024-09-11 09:46:32+00:00,['tilakrayal'],2024-09-27 02:01:38+00:00,2024-09-27 02:01:35+00:00,https://github.com/tensorflow/tensorflow/issues/75555,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2345574216, 'issue_id': 2519215561, 'author': 'tilakrayal', 'body': '@hansu2022,\r\nLooks like a similar type of issue is more related to Bazel and raised in the Bazel repo which the issue is open state\r\nhttps://github.com/bazelbuild/bazel/issues/23018\r\n\r\nPlease take a look and try to follow the same issue for the updates. Thank you!', 'created_at': datetime.datetime(2024, 9, 12, 8, 17, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362557505, 'issue_id': 2519215561, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 20, 1, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259550, 'issue_id': 2519215561, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259600, 'issue_id': 2519215561, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75555"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75555"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-12 08:17:53 UTC): @hansu2022,
Looks like a similar type of issue is more related to Bazel and raised in the Bazel repo which the issue is open state
https://github.com/bazelbuild/bazel/issues/23018

Please take a look and try to follow the same issue for the updates. Thank you!

github-actions[bot] on (2024-09-20 01:59:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-27 02:01:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-27 02:01:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75555"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75555"">No</a>

"
2515972165,issue,closed,completed,Custom `tf.keras.metrics.Metric` example fails on GPU in TF 2.17 (but not on nightly): is it possible to get it to work on 2.17?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4

### Current behavior?

When running code (from *Deep Learning with Python*, for teaching) in Colab with the standard TF 2.17, defining a custom metric seems to fail to move variables to the GPU.

This goes away with TF nightly, but I wondered if there was something that could be specified in the class in 2.17 to solve the issue?

[Notebook here](https://drive.google.com/file/d/14rNRJnh6zHNHb14mcZA3F6nnywTk7LSu/view?usp=sharing).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def get_mnist_model():
    inputs = tf.keras.Input(shape=(28 * 28,))
    features = tf.keras.layers.Dense(512, activation=""relu"")(inputs)
    features = tf.keras.layers.Dropout(0.5)(features)
    outputs = tf.keras.layers.Dense(10, activation=""softmax"")(features)
    model = tf.keras.Model(inputs, outputs)
    return model

(images, labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
images = images.reshape((60000, 28 * 28)).astype(""float32"") / 255
test_images = test_images.reshape((10000, 28 * 28)).astype(""float32"") / 255
train_images, val_images = images[10000:], images[:10000]
train_labels, val_labels = labels[10000:], labels[:10000]

class RootMeanSquaredError(tf.keras.metrics.Metric):
    def __init__(self, name=""rmse"", **kwargs):
        super().__init__(name=name, **kwargs)
        self.mse_sum = self.add_weight(name=""mse_sum"", initializer=""zeros"")
        self.total_samples = self.add_weight(
            name=""total_samples"", initializer=""zeros"", dtype=""int32""
        )

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])
        mse = tf.reduce_sum(tf.square(y_true - y_pred))
        self.mse_sum.assign_add(mse)
        num_samples = tf.shape(y_pred)[0]
        self.total_samples.assign_add(num_samples)

    def result(self):
        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))

    def reset_state(self):
        self.mse_sum.assign(0.0)
        self.total_samples.assign(0)

model = get_mnist_model()
model.compile(
    optimizer=""rmsprop"",
    loss=""sparse_categorical_crossentropy"",
    metrics=[""accuracy"", RootMeanSquaredError()],
)
model.fit(
    train_images, train_labels, epochs=3, validation_data=(val_images, val_labels)
)
test_metrics = model.evaluate(test_images, test_labels)
```


### Relevant log output

```shell
Epoch 1/3
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-8c78bd9ca7c0> in <cell line: 7>()
      5     metrics=[""accuracy"", RootMeanSquaredError()],
      6 )
----> 7 model.fit(
      8     train_images, train_labels, epochs=3, validation_data=(val_images, val_labels)
      9 )

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-5-8c78bd9ca7c0>"", line 7, in <cell line: 7>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 318, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

Trying to access resource rmse/total_samples/29 (defined @ /usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py:31) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0
 Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_18234]
```
",jchwenger,2024-09-10 09:55:00+00:00,['Venkat6871'],2024-09-11 11:02:43+00:00,2024-09-11 11:02:40+00:00,https://github.com/tensorflow/tensorflow/issues/75465,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2343310352, 'issue_id': 2515972165, 'author': 'Venkat6871', 'body': 'Hi **@jchwenger** ,\r\nI tried to run your code on Colab using TF v2.16.1, 2.17.0 and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/134520654652320ce70b5dfb8e93f9d9/75465_tf-2-17-0-2-16-1-v.ipynb?authuser=1) here for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 11, 10, 55, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343323824, 'issue_id': 2515972165, 'author': 'jchwenger', 'body': 'Sounds good @Venkat6871, I will, thanks!', 'created_at': datetime.datetime(2024, 9, 11, 11, 2, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343323882, 'issue_id': 2515972165, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75465"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75465"">No</a>', 'created_at': datetime.datetime(2024, 9, 11, 11, 2, 41, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-11 10:55:15 UTC): Hi **@jchwenger** ,
I tried to run your code on Colab using TF v2.16.1, 2.17.0 and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/134520654652320ce70b5dfb8e93f9d9/75465_tf-2-17-0-2-16-1-v.ipynb?authuser=1) here for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras.
Thank you!

jchwenger (Issue Creator) on (2024-09-11 11:02:40 UTC): Sounds good @Venkat6871, I will, thanks!

google-ml-butler[bot] on (2024-09-11 11:02:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75465"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75465"">No</a>

"
2515048013,issue,open,,How to pack TFRT into wheel? And use it in saved_model_cli.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Current behavior?

I have been compiled whole TF successfully by commands. And there is a tfrt directory in ./bazel-bin/tensorflow/core.
But after I run build_pip_package, the tfrt package didn't show up at /tensorflow/core/ in wheel.

I want to use TFRT in serving and somewhere else.

### Standalone code to reproduce the issue

```shell
bazel build --config=release_cpu_linux --config=tf_public_cache --build_event_json_file=/tf/pkg/bep.json tensorflow/tools/pip_package:build_pip_package

./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tf/pkg --cpu
```


### Relevant log output

_No response_",MoFHeka,2024-09-09 22:38:23+00:00,['tilakrayal'],2024-09-25 12:57:29+00:00,,https://github.com/tensorflow/tensorflow/issues/75443,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2342997198, 'issue_id': 2515048013, 'author': 'tilakrayal', 'body': '@MoFHeka,\r\nCould you please try to use the latest tensorflow v2.17 and provide whether you are facing a similar issue and the stable version also. Thank you!', 'created_at': datetime.datetime(2024, 9, 11, 8, 31, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343192319, 'issue_id': 2515048013, 'author': 'MoFHeka', 'body': ""> @MoFHeka, Could you please try to use the latest tensorflow v2.17 and provide whether you are facing a similar issue and the stable version also. Thank you!\r\n\r\nIt's the same under 2.17. Here is the script.\r\n\r\n```shell\r\nbazel build --config=release_cpu_linux --config=tf_public_cache tensorflow/tools/pip_package:wheel\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tf/pkg --cpu\r\n```"", 'created_at': datetime.datetime(2024, 9, 11, 10, 0, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350570990, 'issue_id': 2515048013, 'author': 'MoFHeka', 'body': 'Similar issue: https://github.com/tensorflow/runtime/issues/121', 'created_at': datetime.datetime(2024, 9, 13, 22, 39, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358924709, 'issue_id': 2515048013, 'author': 'MoFHeka', 'body': 'Any progress?', 'created_at': datetime.datetime(2024, 9, 18, 16, 31, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374015288, 'issue_id': 2515048013, 'author': 'tilakrayal', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 9, 25, 12, 57, 28, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-11 08:31:03 UTC): @MoFHeka,
Could you please try to use the latest tensorflow v2.17 and provide whether you are facing a similar issue and the stable version also. Thank you!

MoFHeka (Issue Creator) on (2024-09-11 10:00:30 UTC): It's the same under 2.17. Here is the script.

```shell
bazel build --config=release_cpu_linux --config=tf_public_cache tensorflow/tools/pip_package:wheel

./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tf/pkg --cpu
```

MoFHeka (Issue Creator) on (2024-09-13 22:39:04 UTC): Similar issue: https://github.com/tensorflow/runtime/issues/121

MoFHeka (Issue Creator) on (2024-09-18 16:31:10 UTC): Any progress?

tilakrayal (Assginee) on (2024-09-25 12:57:28 UTC): @learning-to-play

"
2514554846,issue,closed,completed,TextVectorization returns 'int64' vs 'float32' in TF 2.7 / nightly + Training simple unigram/bigram models much slower than in 2.15,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.7 / nightly

### Custom code

No

### OS platform and distribution

Ubuntu (Colab) 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2  

### GPU model and memory

Nvidia T4

### Current behavior?

Hi there,

I'm running into issues when proofreading some teaching materials based on Chollet's *Deep Learning With Python*. 

Running [this Colab notebook](https://drive.google.com/file/d/130cVhEsJT6J-z6f180I3S6DbOb5W8kAa/view?usp=sharing), (which is chapter 11, first part, based on [his repo](https://github.com/fchollet/deep-learning-with-python-notebooks), but fleshed out for teaching, this should be runnable out of the box) is *much* slower in the current versions of TensorFlow (2.17, or nightly), than in 2.15. I am unsure as to where this comes from, as the nets used in this are just fully-connected layers on unigrams/bigrams (XLA compilation seems to play a role, although that seems a lot). The slowness is particularly striking during the `evaluate()` step, where the XLA acceleration happening after the second epoch of training does not happen. I'm wondering if there's something obvious that's wrong in the code, or in the update...

Also, I notice that the `TextVectorization` layer seems to be returning `int64` in TF 2.17 and nightly, as opposed to `float32` previously. I did not find any mention of this in the documentation. Is this the desired behaviour?

### Standalone code to reproduce the issue

```shell
https://drive.google.com/file/d/130cVhEsJT6J-z6f180I3S6DbOb5W8kAa/view?usp=sharing
```


### Relevant log output

_No response_",jchwenger,2024-09-09 17:47:19+00:00,['Venkat6871'],2024-09-11 10:33:40+00:00,2024-09-11 10:33:37+00:00,https://github.com/tensorflow/tensorflow/issues/75423,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:keras', 'Keras related issues'), ('type:performance', 'Performance Issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2342643493, 'issue_id': 2514554846, 'author': 'Venkat6871', 'body': 'Hi **@jchwenger** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 11, 5, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343269263, 'issue_id': 2514554846, 'author': 'jchwenger', 'body': 'Thanks @Venkat6871, I will!', 'created_at': datetime.datetime(2024, 9, 11, 10, 33, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343269330, 'issue_id': 2514554846, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75423"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75423"">No</a>', 'created_at': datetime.datetime(2024, 9, 11, 10, 33, 39, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-11 05:17:18 UTC): Hi **@jchwenger** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

jchwenger (Issue Creator) on (2024-09-11 10:33:37 UTC): Thanks @Venkat6871, I will!

google-ml-butler[bot] on (2024-09-11 10:33:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75423"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75423"">No</a>

"
2514399021,issue,open,,Wheels have different metadata on different platforms,"Hi! Some resolvers in Python such as poetry and uv try to create lockfiles from the user's requirements that work an any platform. For example, you could create a universal lockfile on linux and use it to install the project on windows.

For this, both poetry and uv read the `METADATA` file of a single wheel on the index (in this case, pypi) and assume its metadata applies to all other platforms, too. For tensorflow, there is currently different metadata for windows and for linux/mac. For windows, the `requires-dist` excluding the cuda packages is:

```
Requires-Dist: tensorflow-macos ==2.15.1 ; platform_system == ""Darwin"" and platform_machine == ""arm64""
Requires-Dist: tensorflow-cpu-aws ==2.15.1 ; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
Requires-Dist: tensorflow-intel ==2.15.1 ; platform_system == ""Windows""
```

While for linux and mac it is:

```
Requires-Dist: absl-py (>=1.0.0)
Requires-Dist: astunparse (>=1.6.0)
Requires-Dist: flatbuffers (>=23.5.26)
Requires-Dist: gast (!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1)
Requires-Dist: google-pasta (>=0.1.1)
Requires-Dist: h5py (>=2.9.0)
Requires-Dist: libclang (>=13.0.0)
Requires-Dist: ml-dtypes (~=0.3.1)
Requires-Dist: numpy (<2.0.0,>=1.23.5)
Requires-Dist: opt-einsum (>=2.3.2)
Requires-Dist: packaging
Requires-Dist: protobuf (!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3)
Requires-Dist: setuptools
Requires-Dist: six (>=1.12.0)
Requires-Dist: termcolor (>=1.1.0)
Requires-Dist: typing-extensions (>=3.6.6)
Requires-Dist: wrapt (<1.15,>=1.11.0)
Requires-Dist: tensorflow-io-gcs-filesystem (>=0.23.1)
Requires-Dist: grpcio (<2.0,>=1.24.3)
Requires-Dist: tensorboard (<2.16,>=2.15)
Requires-Dist: tensorflow-estimator (<2.16,>=2.15.0)
Requires-Dist: keras (<2.16,>=2.15.0)
```

That means depending on whether we read a windows wheel or a unix wheel, we get a different lockfile.

Would it be possible for tensorflow to write the same METADATA for all platforms and gate the platform specific entries with `platform_system` markers?

For uv, we've considered reading the METADATA files for all wheels, but this has major drawbacks: We have to make 17 network requests for pypi instead of 1 for each version we try, slowing resolution down. There is also no perfect mapping between environment markers (which usually tell us which dependencies to install on which platform) and wheel tags, so when METADATA can be different between wheels we'd also have to capture this in lockfiles.

I hope I explained good enough why identical METADATA files across all wheels of a version are important for us, I can add more details about how the resolver works if you have more questions.

This is similar to the problem discussed at https://github.com/tensorflow/tensorflow/issues/62346#issuecomment-1798633528.",konstin,2024-09-09 16:28:23+00:00,['tilakrayal'],2025-02-06 09:52:42+00:00,,https://github.com/tensorflow/tensorflow/issues/75415,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:cpu-intel', 'To track windows cpu issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2342837357, 'issue_id': 2514399021, 'author': 'tilakrayal', 'body': '@konstin,\r\nThe mentioned platforms tensorflow-cpu-aws, tensorflow-intel are maintained by the other groups. Also could you try in the latest tensorflow v2.17 and provide the update whether facing the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 9, 11, 7, 11, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343535692, 'issue_id': 2514399021, 'author': 'konstin', 'body': 'Hi @tilakrayal, I see that tensorflow is forwarding to other packages for windows which totally makes sense; I\'m asking about keeping the requires-dist metadata consistent between wheels, i.e. adding the metadata for unix to the windows wheel with `platform_system != ""Windows""` and that from the windows wheel to the unix wheels with `platform_system == ""Windows""`. For example, this would change\r\n\r\n```\r\nRequires-Dist: absl-py (>=1.0.0)\r\nRequires-Dist: astunparse (>=1.6.0)\r\nRequires-Dist: flatbuffers (>=23.5.26)\r\n```\r\n\r\nto\r\n\r\n\r\n```\r\nRequires-Dist: absl-py (>=1.0.0); platform_system != ""Windows""\r\nRequires-Dist: astunparse (>=1.6.0); platform_system != ""Windows""\r\nRequires-Dist: flatbuffers (>=23.5.26); platform_system != ""Windows""\r\n```\r\n\r\nThe wheels would still work the same when you installed them, but a universal resolver gets reliable metadata for all platforms at once.\r\n\r\nSorry for picking 2.15.1 as example, i\'ve looked at the 2.17.0 wheels and i\'m facing the same issue: https://inspector.pypi.io/project/tensorflow/2.17.0/packages/8a/8e/0ad1eff787bf13f8dca87472414fbdfb73ea53f5a1a1c20489cfccfb7717/tensorflow-2.17.0-cp310-cp310-win_amd64.whl/tensorflow-2.17.0.dist-info/METADATA is different from https://inspector.pypi.io/project/tensorflow/2.17.0/packages/1f/a1/7d2042050159619a190db874913a2bc70645f8ac677d442f9aab4d29153e/tensorflow-2.17.0-cp310-cp310-macosx_12_0_arm64.whl/tensorflow-2.17.0.dist-info/METADATA.', 'created_at': datetime.datetime(2024, 9, 11, 12, 28, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343762479, 'issue_id': 2514399021, 'author': 'tilakrayal', 'body': '@belitskiy', 'created_at': datetime.datetime(2024, 9, 11, 14, 0, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637122198, 'issue_id': 2514399021, 'author': 'Burane', 'body': 'any updates ?', 'created_at': datetime.datetime(2025, 2, 5, 15, 5, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637277940, 'issue_id': 2514399021, 'author': 'mihaimaruseac', 'body': ""I don't think TF does something wrong here. Poetry and uv need to consider this scenario too. Unless there is a PEP that states that the behavior TF relies on is wrong, there's nothing TF should be doing"", 'created_at': datetime.datetime(2025, 2, 5, 15, 40, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639333727, 'issue_id': 2514399021, 'author': 'konstin', 'body': ""While this unspecified in the PEPs, both poetry and uv rely on the assumption that all wheels in a release have the same metadata. Without this, it would become much harder to determine which dependencies to install when, since we now have to take both PEP 508 environment markers and wheel tags into account, which don't have a proper mapping to each other. It's also a major performance hazard, since currently one http request is sufficient, while without this, we would need one http request per wheel. Tensorflow is the only major package i know with the mismatch between wheels."", 'created_at': datetime.datetime(2025, 2, 6, 9, 52, 40, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-11 07:11:01 UTC): @konstin,
The mentioned platforms tensorflow-cpu-aws, tensorflow-intel are maintained by the other groups. Also could you try in the latest tensorflow v2.17 and provide the update whether facing the similar issue. Thank you!

konstin (Issue Creator) on (2024-09-11 12:28:47 UTC): Hi @tilakrayal, I see that tensorflow is forwarding to other packages for windows which totally makes sense; I'm asking about keeping the requires-dist metadata consistent between wheels, i.e. adding the metadata for unix to the windows wheel with `platform_system != ""Windows""` and that from the windows wheel to the unix wheels with `platform_system == ""Windows""`. For example, this would change

```
Requires-Dist: absl-py (>=1.0.0)
Requires-Dist: astunparse (>=1.6.0)
Requires-Dist: flatbuffers (>=23.5.26)
```

to


```
Requires-Dist: absl-py (>=1.0.0); platform_system != ""Windows""
Requires-Dist: astunparse (>=1.6.0); platform_system != ""Windows""
Requires-Dist: flatbuffers (>=23.5.26); platform_system != ""Windows""
```

The wheels would still work the same when you installed them, but a universal resolver gets reliable metadata for all platforms at once.

Sorry for picking 2.15.1 as example, i've looked at the 2.17.0 wheels and i'm facing the same issue: https://inspector.pypi.io/project/tensorflow/2.17.0/packages/8a/8e/0ad1eff787bf13f8dca87472414fbdfb73ea53f5a1a1c20489cfccfb7717/tensorflow-2.17.0-cp310-cp310-win_amd64.whl/tensorflow-2.17.0.dist-info/METADATA is different from https://inspector.pypi.io/project/tensorflow/2.17.0/packages/1f/a1/7d2042050159619a190db874913a2bc70645f8ac677d442f9aab4d29153e/tensorflow-2.17.0-cp310-cp310-macosx_12_0_arm64.whl/tensorflow-2.17.0.dist-info/METADATA.

tilakrayal (Assginee) on (2024-09-11 14:00:18 UTC): @belitskiy

Burane on (2025-02-05 15:05:58 UTC): any updates ?

mihaimaruseac on (2025-02-05 15:40:38 UTC): I don't think TF does something wrong here. Poetry and uv need to consider this scenario too. Unless there is a PEP that states that the behavior TF relies on is wrong, there's nothing TF should be doing

konstin (Issue Creator) on (2025-02-06 09:52:40 UTC): While this unspecified in the PEPs, both poetry and uv rely on the assumption that all wheels in a release have the same metadata. Without this, it would become much harder to determine which dependencies to install when, since we now have to take both PEP 508 environment markers and wheel tags into account, which don't have a proper mapping to each other. It's also a major performance hazard, since currently one http request is sufficient, while without this, we would need one http request per wheel. Tensorflow is the only major package i know with the mismatch between wheels.

"
2514120790,issue,closed,not_planned,TF_SelectV2Op gets legalized to TFL_SelectOp,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): v2.17.0

### 2. Code

`tf.where(condition, x, y)`

### 3. Failure after conversion

The model cannot run on TFLite Micro, as it doesn't support the old SELECT op, only the new SELECT_V2 one.

### 5. (optional) Any other info / logs

The change was introduced 5 years ago by @abattery to improve compatibility with the older runtimes: https://github.com/tensorflow/tensorflow/commit/061d2e0a0d998623a2b0f191038c0aa918a08027
But now it breaks compatibility with TFLite Micro, which only supports the newer op.
Here's the legalization in question: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/legalize_patterns.td#L302
UPD: I believe that https://github.com/google-ai-edge/ai-edge-torch/issues/93 is basically the same.",tagunil,2024-09-09 14:23:51+00:00,"['abattery', 'pkgoogle']",2024-12-13 19:53:02+00:00,2024-12-13 19:53:02+00:00,https://github.com/tensorflow/tensorflow/issues/75406,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2376437760, 'issue_id': 2514120790, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nPlease take look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 9, 26, 9, 32, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2377707408, 'issue_id': 2514120790, 'author': 'pkgoogle', 'body': ""Looks like we'll have to break something... @abattery, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 9, 26, 18, 56, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499878693, 'issue_id': 2514120790, 'author': 'gaikwadrahul8', 'body': 'Hi, @tagunil \r\nThanks for raising this issue. Are you aware of [AI-Edge-Torch](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/)? As we believe this issue is better supported by and more relevant to AI-Edge-Torch we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/ai-edge-torch/issues/383\r\n\r\nLet us know if you have any questions. Thanks.', 'created_at': datetime.datetime(2024, 11, 26, 7, 35, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499973221, 'issue_id': 2514120790, 'author': 'tagunil', 'body': ""> Hi, @tagunil \n> Thanks for raising this issue. Are you aware of [AI-Edge-Torch](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/)? As we believe this issue is better supported by and more relevant to AI-Edge-Torch we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/ai-edge-torch/issues/383\n> \n> Let us know if you have any questions. Thanks.\n\nHi @gaikwadrahul8,\n\nI've reported the issue here primarily because it's where the code lives. If you're going to track it in a different place, that's fine for me. If you're going to move the code to a different place, that's also fine. But if you're going to drop this code completely and force everyone to migrate to PyTorch, it's a different story, because the product I've been working on when I hit the problem isn't going to migrate from TF to PyTorch anytime soon."", 'created_at': datetime.datetime(2024, 11, 26, 8, 27, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542189041, 'issue_id': 2514120790, 'author': 'pkgoogle', 'body': 'We may end up moving this issue ultimately to [LiteRT](https://github.com/google-ai-edge/litert). For now we are closing this current issue but please follow progress on the new issue.', 'created_at': datetime.datetime(2024, 12, 13, 19, 53, 2, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-26 09:32:12 UTC): Hi, @pkgoogle

Please take look into this issue. Thank you.

pkgoogle (Assginee) on (2024-09-26 18:56:01 UTC): Looks like we'll have to break something... @abattery, can you please take a look? Thanks.

gaikwadrahul8 on (2024-11-26 07:35:54 UTC): Hi, @tagunil 
Thanks for raising this issue. Are you aware of [AI-Edge-Torch](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/)? As we believe this issue is better supported by and more relevant to AI-Edge-Torch we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/ai-edge-torch/issues/383

Let us know if you have any questions. Thanks.

tagunil (Issue Creator) on (2024-11-26 08:27:11 UTC): Hi @gaikwadrahul8,

I've reported the issue here primarily because it's where the code lives. If you're going to track it in a different place, that's fine for me. If you're going to move the code to a different place, that's also fine. But if you're going to drop this code completely and force everyone to migrate to PyTorch, it's a different story, because the product I've been working on when I hit the problem isn't going to migrate from TF to PyTorch anytime soon.

pkgoogle (Assginee) on (2024-12-13 19:53:02 UTC): We may end up moving this issue ultimately to [LiteRT](https://github.com/google-ai-edge/litert). For now we are closing this current issue but please follow progress on the new issue.

"
2513671338,issue,open,,check failed: !PyErr_Occurred() when constructing two uint64 tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the following code, tensorflow will directly raise program abort with the error message: `./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()`

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```

It seems the problem occurs when TensorFlow tries to construct **two uint64 tensors**. Although it is invalid to convert negative int to unsigned, an exception is more proper as program abort will directly kill the process.

Indeed, only constructing one uint64 tensor will properly raises an OverFlow exception. 
This issue only occurs when repeatedly constructing two uint64 tensors.
Another weird thing is that, **if I change the value of `lower2` to either `-1` or `-3` instead of `-2`**, this issue does not occur.


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```
```


### Relevant log output

```shell
F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted (core dumped)
```
",maybeLee,2024-09-09 11:18:01+00:00,['tilakrayal'],2024-09-19 05:00:56+00:00,,https://github.com/tensorflow/tensorflow/issues/75400,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:eager', 'Eager related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2340816203, 'issue_id': 2513671338, 'author': 'tilakrayal', 'body': '@maybeLee,\r\nThank you for reporting the issue. I was also able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/53e576b1701c9f44f7c1125ca96a8270/untitled2102.ipynb). \r\n\r\nPlease allow to deepdive into the issue. Thank you!', 'created_at': datetime.datetime(2024, 9, 10, 13, 36, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-10 13:36:51 UTC): @maybeLee,
Thank you for reporting the issue. I was also able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/53e576b1701c9f44f7c1125ca96a8270/untitled2102.ipynb). 

Please allow to deepdive into the issue. Thank you!

"
2512499883,issue,closed,completed,Hermetic Cuda doesn't respect really aarch64(Tegra devices),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

jetson

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

https://github.com/openxla/xla/issues/16912
Hermetic Cuda doesn't respect really aarch64(Tegra devices)
SBSA is for aarch64 but for arm servers.
aarch64 for nvidia is formally tegra cpus, like jetson.

### Standalone code to reproduce the issue

```shell
compile with hermetic cuda
```


### Relevant log output

_No response_",johnnynunez,2024-09-08 17:50:20+00:00,['Venkat6871'],2024-09-13 17:52:51+00:00,2024-09-13 17:52:48+00:00,https://github.com/tensorflow/tensorflow/issues/75353,"[('type:bug', 'Bug'), ('comp:xla', 'XLA'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2348567709, 'issue_id': 2512499883, 'author': 'Venkat6871', 'body': 'Hi **@johnnynunez** ,\r\nApologies for the delay. The [PR](https://github.com/openxla/xla/pull/16905) has been assigned for review, and once it is merged, this issue will be moved to closed status.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 13, 10, 8, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2349042076, 'issue_id': 2512499883, 'author': 'johnnynunez', 'body': '> Hi **@johnnynunez** , Apologies for the delay. The [PR](https://github.com/openxla/xla/pull/16905) has been assigned for review, and once it is merged, this issue will be moved to closed status. Thank you!\r\n\r\nthanks to you and the entire JAX team', 'created_at': datetime.datetime(2024, 9, 13, 14, 3, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2349628632, 'issue_id': 2512499883, 'author': 'ybaturina', 'body': 'The fix in https://github.com/openxla/xla/pull/17149 is submitted.', 'created_at': datetime.datetime(2024, 9, 13, 17, 42, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2349666984, 'issue_id': 2512499883, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75353"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75353"">No</a>', 'created_at': datetime.datetime(2024, 9, 13, 17, 52, 50, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-13 10:08:37 UTC): Hi **@johnnynunez** ,
Apologies for the delay. The [PR](https://github.com/openxla/xla/pull/16905) has been assigned for review, and once it is merged, this issue will be moved to closed status.
Thank you!

johnnynunez (Issue Creator) on (2024-09-13 14:03:19 UTC): thanks to you and the entire JAX team

ybaturina on (2024-09-13 17:42:44 UTC): The fix in https://github.com/openxla/xla/pull/17149 is submitted.

google-ml-butler[bot] on (2024-09-13 17:52:50 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75353"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75353"">No</a>

"
2512328491,issue,open,,Gradients can't be computed for keras embeddings,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Windows 11, Ubuntu 22.04LTS

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a problem in shap for quite some while now with embeddings (see [here](https://github.com/shap/shap/issues/3440)). Since we manipulate the graph to adjust the gradient calculation in order to produce shap values we need the layers to be backpropagatable. This does not seem the case for `tensorflow.keras.layers.Embedding` and we do not know a way around this.

(In a previous version there was the possibility to [manipulate](https://github.com/shap/shap/blob/master/shap/explainers/_deep/deep_tf.py#L412-L416) the [`_IsBackpropagatable` function](https://github.com/tensorflow/tensorflow/blob/v1.10.0/tensorflow/python/ops/gradients_impl.py#L293) but this is no longer possible)

In the example below one can see that the gradients just become `None` if the model contains an embedding layer. 
Is there a way around this, so that we can calculate gradients for embeddings again?

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the IMDb dataset
max_features = 10000  # Only consider the top 10,000 words
maxlen = 100  # Only consider the first 100 words of each movie review

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Build the model
model = models.Sequential()
embedding_layer = layers.Embedding(input_dim=max_features, output_dim=128, input_length=maxlen)
model.add(embedding_layer)
flat_layer = layers.Flatten()
model.add(flat_layer)
dense_layer = layers.Dense(1, activation='sigmoid')
model.add(dense_layer)

# Build the same model except for the embedding layer
new_model = models.Sequential()
new_model.add(flat_layer)
new_model.add(layers.Dense(1, activation=""sigmoid""))

# Forward pass and gradient extraction
@tf.function
def get_gradients_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

@tf.function
def get_gradients_new_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = new_model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

# Example usage
sample_input = X_train[:1]  # Select a sample from the training set
sample_label = y_train[:1]  # Corresponding label

predictions, gradients = get_gradients_model(sample_input)
predictions2, gradients2 = get_gradients_new_model(sample_input)
print(""Gradients for model:"", gradients)
print(""Gradients for new_model:"", gradients2)
```


### Relevant log output

Gradients for model: None
Gradients for new_model: tf.Tensor(
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0.]], shape=(1, 100), dtype=float32)",CloseChoice,2024-09-08 11:00:16+00:00,['tilakrayal'],2024-10-24 08:04:49+00:00,,https://github.com/tensorflow/tensorflow/issues/75351,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2336693106, 'issue_id': 2512328491, 'author': 'sanskarmodi8', 'body': ""Hi @CloseChoice ,\r\n\r\nI looked into your issue and found that embedding layers generally don't have direct gradients with respect to their inputs (token indices), as they map discrete indices to continuous vectors. Instead, gradients typically flow through the embedding matrix, which can cause challenges in gradient-based methods like SHAP.\r\n\r\n### Workarounds:\r\n\r\n1. **One-Hot Encoding**:  \r\n   You can convert the input to one-hot encoded vectors before passing them into the embedding layer. This allows TensorFlow to compute gradients for the embeddings but increases memory usage significantly, as the input becomes much larger in terms of dimensionality.\r\n\r\n2. **Gradient on the Embedding Matrix**:  \r\n   Rather than trying to get gradients directly for the input tokens, you could compute the gradients with respect to the embedding matrix itself. This approach allows you to analyze how each input token influences the prediction through the learned embedding representation.\r\n\r\n3. **SHAP KernelExplainer**:  \r\n   If direct gradient calculation remains problematic, using SHAP’s `KernelExplainer` could be a good alternative. This method approximates SHAP values based on model predictions and doesn’t rely on gradient calculations, making it more robust for cases involving embedding layers."", 'created_at': datetime.datetime(2024, 9, 8, 13, 46, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338194179, 'issue_id': 2512328491, 'author': 'CloseChoice', 'body': '@sanskarmodi8 \r\nthanks for looking into this so quickly. I am looking for a more general workaround how we can make it possible to calculate shap values for every model that contains embedding layers, so that we get the shap package back to where it was before tensorflow changed. Therefore the question: Do you know any possibility to manipulate tensorflow under-the-hood that we can calculate custom gradients?', 'created_at': datetime.datetime(2024, 9, 9, 13, 54, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338250505, 'issue_id': 2512328491, 'author': 'sanskarmodi8', 'body': ""Hi @CloseChoice,\r\n\r\nThanks for the clarification. I’m sorry, am not familiar with an exact method to manipulate TensorFlow under-the-hood for calculating custom gradients specifically for embedding layers.\r\n\r\nHowever, a possible approach could be to dive into TensorFlow’s lower-level APIs or custom gradient functions to explore if there's a way to achieve this."", 'created_at': datetime.datetime(2024, 9, 9, 14, 16, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339625958, 'issue_id': 2512328491, 'author': 'tilakrayal', 'body': '@CloseChoice,\r\nI tried to execute the mentioned code on both tensorflow v2.17 & tf-nightly and observed that the output is providing zeroes on v2.17 but whereas on tf-nightly the output is provided as numericals. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6349eb52e0b8c843f683e4e6a80d2cba/untitled2100.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 9, 10, 5, 4, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357335976, 'issue_id': 2512328491, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358210390, 'issue_id': 2512328491, 'author': 'CloseChoice', 'body': '> @CloseChoice, I tried to execute the mentioned code on both tensorflow v2.17 & tf-nightly and observed that the output is providing zeroes on v2.17 but whereas on tf-nightly the output is provided as numericals. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6349eb52e0b8c843f683e4e6a80d2cba/untitled2100.ipynb). Thank you!\r\n\r\nThanks for looking into this. The gradients still are `None` and this is my main concern. I do understand that embedding layers are not meant to calculate gradients on, but would appreciate a workaround (if one is known)', 'created_at': datetime.datetime(2024, 9, 18, 11, 25, 41, tzinfo=datetime.timezone.utc)}]","sanskarmodi8 on (2024-09-08 13:46:16 UTC): Hi @CloseChoice ,

I looked into your issue and found that embedding layers generally don't have direct gradients with respect to their inputs (token indices), as they map discrete indices to continuous vectors. Instead, gradients typically flow through the embedding matrix, which can cause challenges in gradient-based methods like SHAP.

### Workarounds:

1. **One-Hot Encoding**:  
   You can convert the input to one-hot encoded vectors before passing them into the embedding layer. This allows TensorFlow to compute gradients for the embeddings but increases memory usage significantly, as the input becomes much larger in terms of dimensionality.

2. **Gradient on the Embedding Matrix**:  
   Rather than trying to get gradients directly for the input tokens, you could compute the gradients with respect to the embedding matrix itself. This approach allows you to analyze how each input token influences the prediction through the learned embedding representation.

3. **SHAP KernelExplainer**:  
   If direct gradient calculation remains problematic, using SHAP’s `KernelExplainer` could be a good alternative. This method approximates SHAP values based on model predictions and doesn’t rely on gradient calculations, making it more robust for cases involving embedding layers.

CloseChoice (Issue Creator) on (2024-09-09 13:54:38 UTC): @sanskarmodi8 
thanks for looking into this so quickly. I am looking for a more general workaround how we can make it possible to calculate shap values for every model that contains embedding layers, so that we get the shap package back to where it was before tensorflow changed. Therefore the question: Do you know any possibility to manipulate tensorflow under-the-hood that we can calculate custom gradients?

sanskarmodi8 on (2024-09-09 14:16:34 UTC): Hi @CloseChoice,

Thanks for the clarification. I’m sorry, am not familiar with an exact method to manipulate TensorFlow under-the-hood for calculating custom gradients specifically for embedding layers.

However, a possible approach could be to dive into TensorFlow’s lower-level APIs or custom gradient functions to explore if there's a way to achieve this.

tilakrayal (Assginee) on (2024-09-10 05:04:46 UTC): @CloseChoice,
I tried to execute the mentioned code on both tensorflow v2.17 & tf-nightly and observed that the output is providing zeroes on v2.17 but whereas on tf-nightly the output is provided as numericals. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6349eb52e0b8c843f683e4e6a80d2cba/untitled2100.ipynb). Thank you!

github-actions[bot] on (2024-09-18 01:58:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

CloseChoice (Issue Creator) on (2024-09-18 11:25:41 UTC): Thanks for looking into this. The gradients still are `None` and this is my main concern. I do understand that embedding layers are not meant to calculate gradients on, but would appreciate a workaround (if one is known)

"
2511822049,issue,closed,completed,TypeError: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

Linux

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I tried to rerun the code previously based on tf 2.1.0 by using tf 2.15.0 and I met a problem due to the wrong data format. Hope for some solutions. Thank you very much.

### Standalone code to reproduce the issue

```shell
class dataGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, features, labels, batch_size=32,meanImg=None, dim=(121, 145, 121),maxAngle=40,maxShift=10, shuffle=True,augment=False,includeScannerGender=True):
        'Initialization'
        self.batch_size = batch_size
        self.features = features
        self.labels = labels
        self.dim = dim
        self.meanImg = meanImg
        self.augment = augment
        self.maxAngle = maxAngle
        self.maxShift = maxShift
        self.shuffle = shuffle
        self.IncludeScannerGender = includeScannerGender
        self.on_epoch_end()  
    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(self.features[0].shape[0] / self.batch_size))
    
    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.features[0]))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
            
    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        #print(index)
        index = index%self.__len__()
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        #features_temp = [self.features[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(indexes)

        return X, y
    def __data_generation(self, indexes):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        #X = np.empty((self.batch_size, self.dim[0],self.dim[1],self.dim[2]),dtype=np.uint8)
        X = np.empty((self.batch_size, self.dim[0],self.dim[1],self.dim[2], 1),dtype=np.uint8)
        age = np.empty((self.batch_size))
        if self.IncludeScannerGender: 
            sex = np.empty((self.batch_size))
            scanner = np.empty((self.batch_size))
        # Generate data
        for i, index in enumerate(indexes):
            X[i,:,:,:,:] = processing(self.features[0][index],self.dim,self.meanImg,augment=self.augment)
            age[i] = self.labels[index]
            if self.IncludeScannerGender: 
                scanner[i] = self.features[1][index]
                sex[i] = self.features[2][index]
                
        
        if self.IncludeScannerGender: 
            return [X,scanner,sex], np.array([age])
        else:
            return [X], np.array([age])
**************&AND

h = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,
                            dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),
                        validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, 
                                                      meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),
                       
                           )
```


### Relevant log output

```shell
TypeError                                 Traceback (most recent call last)
Cell In[135], line 1
----> 1 h = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,
      2                             dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),
      3                         validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, 
      4                                                       meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),
      5                        
      6                            )

File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py:124, in _from_generator(generator, output_types, output_shapes, args, output_signature, name)
    122   for spec in nest.flatten(output_signature):
    123     if not isinstance(spec, type_spec.TypeSpec):
--> 124       raise TypeError(f""`output_signature` must contain objects that are ""
    125                       f""subclass of `tf.TypeSpec` but found {type(spec)} ""
    126                       f""which is not."")
    127 else:
    128   if output_types is None:

TypeError: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.
```
",Cherry130,2024-09-07 16:03:37+00:00,['Venkat6871'],2024-11-08 02:01:10+00:00,2024-11-08 02:01:07+00:00,https://github.com/tensorflow/tensorflow/issues/75329,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2339626460, 'issue_id': 2511822049, 'author': 'Venkat6871', 'body': 'Hi **@Cherry130** ,\r\nSorry for the dealy, I reproduced the code shared but facing different error .Could you please share the colab gist with all the dependencies to analyze more of it.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 10, 5, 5, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339798111, 'issue_id': 2511822049, 'author': 'Cherry130', 'body': 'It\'s okay. Below are the libraries this code introduced and their corresponding versions.\r\n*****\r\nimport pandas as pd\r\nimport os\r\nimport scipy.ndimage as nd\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import r2_score,mean_absolute_error\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport math\r\nimport sys\r\nfrom tensorflow.keras.models import load_model\r\nimport random\r\nimport glob\r\nimport nibabel as nib\r\nfrom collections import defaultdict\r\nfrom tensorflow.keras.optimizers import Adam, SGD,Adagrad\r\nfrom tensorflow.compat.v1 import reset_default_graph\r\nfrom sklearn.model_selection import KFold\r\n#from DataLoader import dataGenerator,getIcelandicData,getIXIData,getUKBData\r\n#from Util import plotData,getPredictions,loadMR,loadHeader,calculateMeanImg\r\n#from ResNet import generateAgePredictionResNet\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\r\nfrom tensorflow.python.keras import backend as K\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.utils import Sequence\r\nimport operator\r\nfrom transformations import rotation_matrix\r\nfrom random import gauss\r\nfrom scipy.ndimage.interpolation import map_coordinates\r\n********\r\nName: transformations\r\nVersion: 2024.5.24\r\nName: pandas\r\nVersion: 2.2.2\r\nName: scipy\r\nVersion: 1.11.\r\nName: numpy\r\nVersion: 1.26.4\r\nName: matplotlib\r\nVersion: 3.7.5\r\nName: tensorflow\r\nVersion: 2.15.0\r\nName: nibabel\r\nVersion: 5.2.1\r\n******\r\nthe complete error code is below, but I think the sorce of error is the data_generator\r\nh = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,\r\n                            dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),\r\n                        validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, \r\n                                                      meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),\r\n                        validation_steps=validation_steps,\r\n                        steps_per_epoch=steps_per_epoch, \r\n                        epochs=nEpochs,\r\n                        verbose=1,\r\n                        callbacks=[mc,early]\r\n                           )\r\n*******\r\nAgain it generated the same problem\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[44], line 1\r\n----> 1 h = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,\r\n      2                             dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),\r\n      3                         validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, \r\n      4                                                       meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),\r\n      5                         validation_steps=validation_steps,\r\n      6                         steps_per_epoch=steps_per_epoch, \r\n      7                         epochs=nEpochs,\r\n      8                         verbose=1,\r\n      9                         callbacks=[mc,early]\r\n     10                            )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py:124, in _from_generator(generator, output_types, output_shapes, args, output_signature, name)\r\n    122   for spec in nest.flatten(output_signature):\r\n    123     if not isinstance(spec, type_spec.TypeSpec):\r\n--> 124       raise TypeError(f""`output_signature` must contain objects that are ""\r\n    125                       f""subclass of `tf.TypeSpec` but found {type(spec)} ""\r\n    126                       f""which is not."")\r\n    127 else:\r\n    128   if output_types is None:\r\n\r\nTypeError: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class \'list\'> which is not.', 'created_at': datetime.datetime(2024, 9, 10, 6, 41, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357835593, 'issue_id': 2511822049, 'author': 'Venkat6871', 'body': 'Hi @Cherry130 ,\r\nApologies for the delay, and thank you for your response. I tried running your code using Colab with TensorFlow 2.17.0 and encountered different issues. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/92ad5b7dee121662f0c4811e4e51b03d/75329_tf-2-17-v.ipynb) here for reference. Could you please share the Colab gist with all the dependencies so we can analyze it further?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 18, 8, 30, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358619267, 'issue_id': 2511822049, 'author': 'Cherry130', 'body': ""[IXI_MRI.docx](https://github.com/user-attachments/files/17045472/IXI_MRI.docx)\r\n[IXI_2_scanner.xlsx](https://github.com/user-attachments/files/17045476/IXI_2_scanner.xlsx)\r\n**************************\r\nThe first file above is all the code, under the environment configuration told before, due to some problems such as the network, I may not be able to log in to the Colab website, I'm very sorry\r\nThe second file is the incoming tabular data. What else that can't be passed in is the MRI data of the brain.\r\nI'm a newbie to machine learning and this issue has been bothering me for a month, thank you so much for your help."", 'created_at': datetime.datetime(2024, 9, 18, 14, 22, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382143049, 'issue_id': 2511822049, 'author': 'Venkat6871', 'body': 'Hi @Cherry130 ,\r\nApologies for the delay. You provided a very large code snippet, which makes it hard for us to debug. Could you please try to provide a simpler code example where you are facing the issue?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 5, 27, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491769, 'issue_id': 2511822049, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 2, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662329, 'issue_id': 2511822049, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662374, 'issue_id': 2511822049, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431308435, 'issue_id': 2511822049, 'author': 'Venkat6871', 'body': 'Hi **@Cherry130** ,\r\nApologies for the delay, and thank you for your patience.\r\nI tried running your code on Colab using TensorFlow 2.17.0. I downloaded your files and added the paths as required, but the paths are still not being found. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ac221a7456d50dc689f4a62f5d7f6f24/75329_2-17-0.ipynb) here for reference. Please go through it and let me know if I need to make any changes.\r\nRegarding your TypeError issue, I suggest the following solution. In a TensorFlow data pipeline, the output_signature must contain objects that are subclasses of tf.TypeSpec, such as tf.TensorSpec or tf.RaggedTensorSpec. However, in your generator, you wrote a list. Instead, try using a tuple. I hope this resolves the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 23, 8, 34, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448868134, 'issue_id': 2511822049, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 31, 2, 3, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607303, 'issue_id': 2511822049, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 8, 2, 1, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463607351, 'issue_id': 2511822049, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 2, 1, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-10 05:05:17 UTC): Hi **@Cherry130** ,
Sorry for the dealy, I reproduced the code shared but facing different error .Could you please share the colab gist with all the dependencies to analyze more of it.
Thank you!

Cherry130 (Issue Creator) on (2024-09-10 06:41:46 UTC): It's okay. Below are the libraries this code introduced and their corresponding versions.
*****
import pandas as pd
import os
import scipy.ndimage as nd
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score,mean_absolute_error
import numpy as np
import matplotlib.pyplot as plt
import math
import sys
from tensorflow.keras.models import load_model
import random
import glob
import nibabel as nib
from collections import defaultdict
from tensorflow.keras.optimizers import Adam, SGD,Adagrad
from tensorflow.compat.v1 import reset_default_graph
from sklearn.model_selection import KFold
#from DataLoader import dataGenerator,getIcelandicData,getIXIData,getUKBData
#from Util import plotData,getPredictions,loadMR,loadHeader,calculateMeanImg
#from ResNet import generateAgePredictionResNet
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping
from tensorflow.python.keras import backend as K
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import Sequence
import operator
from transformations import rotation_matrix
from random import gauss
from scipy.ndimage.interpolation import map_coordinates
********
Name: transformations
Version: 2024.5.24
Name: pandas
Version: 2.2.2
Name: scipy
Version: 1.11.
Name: numpy
Version: 1.26.4
Name: matplotlib
Version: 3.7.5
Name: tensorflow
Version: 2.15.0
Name: nibabel
Version: 5.2.1
******
the complete error code is below, but I think the sorce of error is the data_generator
h = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,
                            dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),
                        validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, 
                                                      meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),
                        validation_steps=validation_steps,
                        steps_per_epoch=steps_per_epoch, 
                        epochs=nEpochs,
                        verbose=1,
                        callbacks=[mc,early]
                           )
*******
Again it generated the same problem
TypeError                                 Traceback (most recent call last)
Cell In[44], line 1
----> 1 h = model.fit(dataGenerator([train.Loc.values,scanner_train,gender_train],train.AGE.values, batch_size = batchSize, meanImg=meanImg,
      2                             dim=dataShape,shuffle=True,augment=True,maxAngle=maxAngle,maxShift=maxShift),
      3                         validation_data=dataGenerator([val.Loc.values,scanner_val,gender_val],val.AGE.values, batch_size = batchSize, 
      4                                                       meanImg=meanImg,dim=dataShape,shuffle=False,augment=False),
      5                         validation_steps=validation_steps,
      6                         steps_per_epoch=steps_per_epoch, 
      7                         epochs=nEpochs,
      8                         verbose=1,
      9                         callbacks=[mc,early]
     10                            )

File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py:124, in _from_generator(generator, output_types, output_shapes, args, output_signature, name)
    122   for spec in nest.flatten(output_signature):
    123     if not isinstance(spec, type_spec.TypeSpec):
--> 124       raise TypeError(f""`output_signature` must contain objects that are ""
    125                       f""subclass of `tf.TypeSpec` but found {type(spec)} ""
    126                       f""which is not."")
    127 else:
    128   if output_types is None:

TypeError: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.

Venkat6871 (Assginee) on (2024-09-18 08:30:55 UTC): Hi @Cherry130 ,
Apologies for the delay, and thank you for your response. I tried running your code using Colab with TensorFlow 2.17.0 and encountered different issues. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/92ad5b7dee121662f0c4811e4e51b03d/75329_tf-2-17-v.ipynb) here for reference. Could you please share the Colab gist with all the dependencies so we can analyze it further?
Thank you!

Cherry130 (Issue Creator) on (2024-09-18 14:22:54 UTC): [IXI_MRI.docx](https://github.com/user-attachments/files/17045472/IXI_MRI.docx)
[IXI_2_scanner.xlsx](https://github.com/user-attachments/files/17045476/IXI_2_scanner.xlsx)
**************************
The first file above is all the code, under the environment configuration told before, due to some problems such as the network, I may not be able to log in to the Colab website, I'm very sorry
The second file is the incoming tabular data. What else that can't be passed in is the MRI data of the brain.
I'm a newbie to machine learning and this issue has been bothering me for a month, thank you so much for your help.

Venkat6871 (Assginee) on (2024-09-30 05:27:54 UTC): Hi @Cherry130 ,
Apologies for the delay. You provided a very large code snippet, which makes it hard for us to debug. Could you please try to provide a simpler code example where you are facing the issue?
Thank you!

github-actions[bot] on (2024-10-08 02:02:08 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">No</a>

Venkat6871 (Assginee) on (2024-10-23 08:34:47 UTC): Hi **@Cherry130** ,
Apologies for the delay, and thank you for your patience.
I tried running your code on Colab using TensorFlow 2.17.0. I downloaded your files and added the paths as required, but the paths are still not being found. I am providing the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ac221a7456d50dc689f4a62f5d7f6f24/75329_2-17-0.ipynb) here for reference. Please go through it and let me know if I need to make any changes.
Regarding your TypeError issue, I suggest the following solution. In a TensorFlow data pipeline, the output_signature must contain objects that are subclasses of tf.TypeSpec, such as tf.TensorSpec or tf.RaggedTensorSpec. However, in your generator, you wrote a list. Instead, try using a tuple. I hope this resolves the issue.
Thank you!

github-actions[bot] on (2024-10-31 02:03:16 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-08 02:01:07 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-08 02:01:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75329"">No</a>

"
2511252907,issue,open,,Fail import tensorflow if rules_python is installed.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I installed both TensorFlow and Mesop simultaneously, I encountered an error while importing TensorFlow. It seems that installing Mesop also installs `rules_python`, which causes TensorFlow to malfunction.

Specifically, at the following code:
https://github.com/tensorflow/tensorflow/blob/bc90265931a0ca90ee2e7c2ef988ad6634733961/tensorflow/python/platform/resource_loader.py#L117-L119

`r` becomes `None`, leading to an error.

According to `rules_python`:
https://github.com/bazelbuild/rules_python/blob/0.26.0/python/runfiles/runfiles.py#L40

It seems possible that `r = runfiles.Create()` can indeed return `None`.

While it seems the issue could be linked to Mesop installing rules_python, there might also be an underlying problem with TensorFlow. I would appreciate your help in investigating further.

### Standalone code to reproduce the issue

```shell
$ pip install ""tf-nightly==2.18.0.dev20240906"" ""mesop==0.12.3""
$ python -c ""import tensorflow""

# I run this in docker image of python:3.11.
```


### Relevant log output

```shell
2024-09-06 21:45:07.412511: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.415758: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.425482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1725659107.441013      78 cuda_dnn.cc:8322] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1725659107.445948      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-06 21:45:07.463219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/__init__.py"", line 53, in <module>
    from tensorflow._api.v2 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>
    from tensorflow._api.v2.compat.v1 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py"", line 47, in <module>
    from tensorflow._api.v2.compat.v1 import lite
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py"", line 9, in <module>
    from tensorflow._api.v2.compat.v1.lite import experimental
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.lite.experimental import authoring
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py"", line 8, in <module>
    from tensorflow.lite.python.authoring.authoring import compatible # line: 263
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/authoring/authoring.py"", line 42, in <module>
    from tensorflow.lite.python import convert
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/convert.py"", line 151, in <module>
    _deprecated_conversion_binary = _resource_loader.get_path_to_datafile(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/platform/resource_loader.py"", line 118, in get_path_to_datafile
    new_fpath = r.Rlocation(
                ^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'Rlocation'
```
",n-kats,2024-09-06 22:08:00+00:00,['tilakrayal'],2024-09-25 13:12:33+00:00,,https://github.com/tensorflow/tensorflow/issues/75288,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2339628446, 'issue_id': 2511252907, 'author': 'tilakrayal', 'body': ""@n-kats,\r\nAs a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might  work and the Project can be build succesfully.\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 9, 10, 5, 7, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340507100, 'issue_id': 2511252907, 'author': 'n-kats', 'body': ""In my project, I'm not using Bazel or a WORKSPACE file. Should I still create a WORKSPACE file in this case?\r\n\r\n- In my project, I use TensorFlow and Mesop (installed via pip), and there is no build process involved.\r\n- However, in the [Mesop project](https://github.com/google/mesop), the developers use Bazel and rules_python, and the [Mesop wheel file](https://pypi.org/project/mesop/0.12.3/#files) includes rules_python (I believe this is causing the issue)."", 'created_at': datetime.datetime(2024, 9, 10, 12, 7, 16, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-10 05:07:10 UTC): @n-kats,
As a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might  work and the Project can be build succesfully.

Thank you!

n-kats (Issue Creator) on (2024-09-10 12:07:16 UTC): In my project, I'm not using Bazel or a WORKSPACE file. Should I still create a WORKSPACE file in this case?

- In my project, I use TensorFlow and Mesop (installed via pip), and there is no build process involved.
- However, in the [Mesop project](https://github.com/google/mesop), the developers use Bazel and rules_python, and the [Mesop wheel file](https://pypi.org/project/mesop/0.12.3/#files) includes rules_python (I believe this is causing the issue).

"
2511229163,issue,closed,completed,Accuracy not Improving TensorFlow Federated,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My accuracy is not improving, despite running multiple rounds and epochs, even for a noise multiplier of 0.0.

### Standalone code to reproduce the issue

```shell
You can access the Colab notebook with the issue details [here]([https://colab.research.google.com/drive/1CId84BquUyJKZZKAQyWatCstubjXOwNl?usp=sharing](url).
```


### Relevant log output

_No response_",Raheel778,2024-09-06 21:41:17+00:00,['Venkat6871'],2024-09-27 02:01:42+00:00,2024-09-27 02:01:38+00:00,https://github.com/tensorflow/tensorflow/issues/75286,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2334850343, 'issue_id': 2511229163, 'author': 'Raheel778', 'body': '[https://colab.research.google.com/drive/1CId84BquUyJKZZKAQyWatCstubjXOwNl?usp=sharing](url)', 'created_at': datetime.datetime(2024, 9, 6, 21, 43, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345383924, 'issue_id': 2511229163, 'author': 'Venkat6871', 'body': 'Hi **@Raheel778** ,\r\nApologies for the delay. It looks like this issue is more related to TensorFlow Federated, not TensorFlow itself. Could you please check with the relevant repository?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 12, 6, 33, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362557545, 'issue_id': 2511229163, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 20, 1, 59, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259607, 'issue_id': 2511229163, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259661, 'issue_id': 2511229163, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75286"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75286"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 40, tzinfo=datetime.timezone.utc)}]","Raheel778 (Issue Creator) on (2024-09-06 21:43:48 UTC): [https://colab.research.google.com/drive/1CId84BquUyJKZZKAQyWatCstubjXOwNl?usp=sharing](url)

Venkat6871 (Assginee) on (2024-09-12 06:33:18 UTC): Hi **@Raheel778** ,
Apologies for the delay. It looks like this issue is more related to TensorFlow Federated, not TensorFlow itself. Could you please check with the relevant repository?
Thank you!

github-actions[bot] on (2024-09-20 01:59:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-27 02:01:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-27 02:01:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75286"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75286"">No</a>

"
2511113211,issue,closed,completed,traceback issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when trying to run the code shows

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\PC\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).
```
",kevo200,2024-09-06 20:00:42+00:00,['tilakrayal'],2024-09-24 02:01:37+00:00,2024-09-24 02:01:34+00:00,https://github.com/tensorflow/tensorflow/issues/75280,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2338293981, 'issue_id': 2511113211, 'author': 'tilakrayal', 'body': '@kevo200,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 9, 14, 33, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354345320, 'issue_id': 2511113211, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 17, 1, 48, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369958919, 'issue_id': 2511113211, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369958986, 'issue_id': 2511113211, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75280"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75280"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 36, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-09 14:33:42 UTC): @kevo200,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-09-17 01:48:04 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-24 02:01:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-24 02:01:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75280"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75280"">No</a>

"
2511098863,issue,closed,completed,TensorFlow 2.17 for macOS x86_64 is not on PyPI,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

macOS x86_64

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow 2.17 did not publish macOS x86_64 binaries to PyPI, only arm64 binaries. In TensorFlow-Java we unpack the PyPI binaries so we can repackage them in our Java builds, but there was no notice of the removal of macOS x86_64 binaries in the 2.16 or 2.17 release notes.

### Standalone code to reproduce the issue

```shell
`pip install tensorflow==2.17.0` on a macOS x86_64 machine.
```


### Relevant log output

_No response_",Craigacp,2024-09-06 19:50:20+00:00,"['learning-to-play', 'Venkat6871']",2024-09-10 02:27:16+00:00,2024-09-10 02:27:13+00:00,https://github.com/tensorflow/tensorflow/issues/75279,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2338355624, 'issue_id': 2511098863, 'author': 'Venkat6871', 'body': '@learning-to-play .', 'created_at': datetime.datetime(2024, 9, 9, 14, 57, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339480308, 'issue_id': 2511098863, 'author': 'learning-to-play', 'body': 'See [TensorFlow 2.16 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1): ""Mac x86 users: Mac x86 builds are being deprecated and will no longer be released as a Pip package from TF 2.17 onwards.""', 'created_at': datetime.datetime(2024, 9, 10, 2, 21, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339484938, 'issue_id': 2511098863, 'author': 'Craigacp', 'body': 'Ok thanks, I missed that in the notes.', 'created_at': datetime.datetime(2024, 9, 10, 2, 27, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339484972, 'issue_id': 2511098863, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75279"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75279"">No</a>', 'created_at': datetime.datetime(2024, 9, 10, 2, 27, 15, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-09 14:57:11 UTC): @learning-to-play .

learning-to-play (Assginee) on (2024-09-10 02:21:59 UTC): See [TensorFlow 2.16 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1): ""Mac x86 users: Mac x86 builds are being deprecated and will no longer be released as a Pip package from TF 2.17 onwards.""

Craigacp (Issue Creator) on (2024-09-10 02:27:13 UTC): Ok thanks, I missed that in the notes.

google-ml-butler[bot] on (2024-09-10 02:27:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75279"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75279"">No</a>

"
2510909756,issue,closed,completed,AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following code works and produces the expected output when run in eager mode, but when using AutoGraph it produces bizarre error messages. The line numbers reported are not the actual location of the errors.

Replacing the contents of _safe_lookup_pixel so it always returns zeros makes the error go away, though of course the output is not then correct. 

My installation is using docker image based on the NVIDIA image, details below.

### Standalone code to reproduce the issue

```shell
import keras
import tensorflow as tf
import numpy as np


class UnAveragePooling2D(keras.layers.Layer):

    def __init__(self, kernel, *, strides, name = 'co1', dtype = None):
        super().__init__(trainable=True, name=name, dtype=dtype)
        self.kernel = kernel
        self.strides = strides

    def build(self, input_shape):
        pass

    def call(self, inputs):
        inputs_shape = tf.shape(inputs)

        # the averaged image had a minimum size of source.shape * strides, but it could have been larger if its size
        # was not a multiple of stride. For now, assume it was a multiple of stride.
        dest_shape = (inputs_shape[0], inputs.shape[1] * self.strides, inputs.shape[2] * self.strides, inputs_shape[3])
        dest = tf.zeros(dest_shape)

        # width of destination invalid edges that need to be filled, again assuming that source size was a multiple of stride.
        dest_start_invalid = (self.strides // 2, self.strides // 2)
        dest_end_invalid = (self.strides // 2, self.strides // 2)

        strides_f = tf.cast(self.strides, tf.float32)
        dest_start_invalid_f = (tf.cast(dest_start_invalid[0], tf.float32), tf.cast(dest_start_invalid[1], tf.float32))
        max_source_f = (tf.cast(inputs_shape[1] - 1, tf.float32), tf.cast(inputs_shape[2] - 1, tf.float32))

        # biniliear interpolation with distorted edges
        for dest_r in range(dest_shape[1]):
            for dest_c in range(dest_shape[2]):
                source_r = self._dest_to_rource(dest_r, strides_f, dest_start_invalid_f[0], max_source_f[0])
                source_c = self._dest_to_rource(dest_c, strides_f, dest_start_invalid_f[1], max_source_f[1])
                value = self._bilinear_interpolate(inputs, source_r, source_c)
                dest = self._assign_pixel_batch_values(dest, dest_r, dest_c, value)

        # # nearest edge fill
        # first_valid_row = tf.reshape(dest[:, dest_start_invalid[0], :, :], (dest_shape[0], 1, dest_shape[2], dest_shape[3]))
        # last_valid_row = tf.reshape(dest[:, -dest_end_invalid[0] - 1, :, :], (dest_shape[0], 1, dest_shape[2], dest_shape[3]))
        # start_rows = tf.tile(first_valid_row, (1, dest_start_invalid[0], 1, 1))
        # middle_rows = dest[:, dest_start_invalid[0]:-dest_end_invalid[0], :, :]
        # end_rows = tf.tile(last_valid_row, (1, dest_end_invalid[0], 1, 1))

        # dest = tf.concat([start_rows, middle_rows, end_rows], axis=1)
        # first_valid_col = tf.reshape(dest[:, :, dest_start_invalid[1], :], (dest_shape[0], dest_shape[1], 1, dest_shape[3]))
        # last_valid_col = tf.reshape(dest[:, :, -dest_end_invalid[1] - 1, :], (dest_shape[0], dest_shape[1], 1, dest_shape[3]))
        # start_cols = tf.tile(first_valid_col, (1, 1, dest_start_invalid[1], 1))
        # middle_cols = dest[:, :, dest_start_invalid[1]:-dest_end_invalid[1], :]
        # end_cols = tf.tile(last_valid_col, (1, 1, dest_end_invalid[1], 1))
        # dest = tf.concat([start_cols, middle_cols, end_cols], axis=2)

        return dest

    @tf.function
    def _dest_to_rource(self, dest, stride, dest_start_invalid, max_source):
        """"""Given a destination pixel row or column position, work out the source
        pixel location from which the value should be interpolated.""""""

        if dest < dest_start_invalid + stride - 0.5:
            return (dest - dest_start_invalid) / (stride - 0.5)
        elif dest > dest_start_invalid + (max_source - 1.0) * stride - 0.5:
            return ((dest - dest_start_invalid + 0.5) - (max_source - 1.0) * stride) / (stride - 0.5) + max_source - 1.0
        else:
            return (dest - dest_start_invalid + 0.5) / stride

    @tf.function
    def _bilinear_interpolate(self, source, r, c):
        """"""Given a batch of source images, interpolate each from the four pixels surrounding point r,c.
        Near the edge, fade to black.
        """"""
        # Algorithm (ignoring edges):
        # 1. Round r,c down to get top-right source pixel r0, c0
        # 2. Subtract r0, c0 from r, c get 0..1 proportions of a pixel fr, fc
        # 3. The four pixels to be sampled are at p00=[r0, c0], p01=[r0,c0+1], p10=[r0+1, c0], and p11=[r0+1, c0+1]
        # 4. For each channel, calculate a linear sum of the four pixels, as follows:
        # 5. result =   p00.(1-fr).(1-fc)
        #             + p01.(1-fr).fc
        #             + p10.fr.(1-fc)
        #             + p11.fr.fc
        # To cope with edges and implement fade-to-black: if any of p00, p01, p10 or p11 would be outside the source
        # image then use black instead
        r0, c0 =  tf.cast(tf.floor(r), tf.int32), tf.cast(tf.floor(c), tf.int32)
        fr, fc = r - tf.cast(r0, tf.float32), c - tf.cast(c0, tf.float32)
        p00 = self._safe_lookup_pixel(source, r0, c0)
        p01 = self._safe_lookup_pixel(source, r0, c0+1)
        p10 = self._safe_lookup_pixel(source, r0+1, c0)
        p11 = self._safe_lookup_pixel(source, r0+1, c0+1)
        return p00*(1-fr)*(1-fc) + p01*(1-fr)*fc + p10*fr*(1-fc) + p11*fr*fc

    @tf.function
    def _safe_lookup_pixel(self, source, r, c):
        """"""If x,y is a valid index into the source images then return the
        batch of pixels at that location, otherwise return a batch of black pixels.""""""

        source_shape = tf.shape(source)
        if 0 <= r < source_shape[1] and 0 <= c < source_shape[2]:
            return source[:, r, c, :]
        else:
            return tf.zeros((source_shape[0], source_shape[3]))

    @staticmethod
    @tf.function
    def _assign_pixel_batch_values(tensor, r, c, value):
        """"""Given a 4d tensor, replace the batch of pixels at location r,c with the given batch of pixels.
        Equivalent to tensor[:, r, c, :] = value
        """"""

        # There has got to be an easier way!
        tensor_shape = tensor.shape
        ret = tensor[:, :r, :, :]  # rows before the update
        new_row = tf.reshape(tensor[:, r, :c, :], (tensor_shape[0], 1, c, tensor_shape[3])) # columns before the update, on the update row
        value = tf.reshape(value, (tensor_shape[0], 1, 1, tensor_shape[3]))
        new_row = tf.concat([new_row, value], axis=2)   # append the new value
        last_cols = tf.reshape(tensor[:, r, c+1:, :], (tensor_shape[0], 1, tensor_shape[2] - c - 1, tensor_shape[3]))
        new_row = tf.concat([new_row, last_cols], axis=2) # columns after the update, on the update row
        ret = tf.concat([ret, new_row], axis=1)  # the now row with the update in it
        ret = tf.concat([ret, tensor[:, r+1:, :, :]], axis=1) # rows after the update
        return ret



TEST_SIZE = 16

data = [list(range(TEST_SIZE))] * TEST_SIZE
data = tf.convert_to_tensor(data, np.float32)
data = tf.reshape(data, (1, TEST_SIZE, TEST_SIZE, 1))

model = keras.models.Sequential()
model.add(keras.layers.AveragePooling2D((6, 6), padding='same', strides=4))
model.add(UnAveragePooling2D((6, 6), strides=4))
model.compile(run_eagerly=False)

expanded = model.predict(data)
print(expanded[0,:,:,0])

```


Dockerfile:
```
FROM nvcr.io/nvidia/tensorflow:24.06-tf2-py3 AS base_image
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y \
    wget \
    build-essential \
    cmake \
    git \
    unzip \
    pkg-config
WORKDIR /environment
RUN pip install --upgrade pip
RUN pip install -U numpy protobuf tensorflow-datasets tensorflow_graphics keras scikit-learn scikit-image pandas seaborn jupyter keras-tuner openml opencv-python typing-extensions pydot graphviz data-science-types adjustText
RUN apt-get update && apt-get install -y \
    graphviz
EXPOSE 8888
ENTRYPOINT [""jupyter"", ""notebook"", ""--no-browser"",""--ip=0.0.0.0""]
```
```


### Relevant log output

```shell
2024-09-06 17:35:56.430840: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:266 : INVALID_ARGUMENT: slice index 4 of dimension 1 out of bounds.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel
File ""tmp/ipykernel_2107/2303768048.py"", line 101, in _safe_lookup_pixel

2024-09-06 17:35:56.430922: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at if_op.cc:269 : INVALID_ARGUMENT: slice index 4 of dimension 1 out of bounds.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel
File ""tmp/ipykernel_2107/2303768048.py"", line 101, in _safe_lookup_pixel

	 [[{{node cond_2/strided_slice}}]]
	tf2xla conversion failed while converting cond_2_true_603071_const_1002[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel

2024-09-06 17:35:56.593132: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:580 : INVALID_ARGUMENT: slice index 4 of dimension 1 out of bounds.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel
File ""tmp/ipykernel_2107/2303768048.py"", line 101, in _safe_lookup_pixel

	 [[{{node cond_2/strided_slice}}]]
	tf2xla conversion failed while converting cond_2_true_603071_const_1002[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel

	 [[sequential_75_1/co1_1/PartitionedCall_1002/PartitionedCall_2/cond_2]]
	tf2xla conversion failed while converting __inference_one_step_on_data_615470[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
2024-09-06 17:35:56.593189: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: slice index 4 of dimension 1 out of bounds.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel
File ""tmp/ipykernel_2107/2303768048.py"", line 101, in _safe_lookup_pixel

	 [[{{node cond_2/strided_slice}}]]
	tf2xla conversion failed while converting cond_2_true_603071_const_1002[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 18, in <module>
File ""usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 1043, in launch_instance
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 739, in start
File ""usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 778, in execute_request
File ""usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 449, in do_execute
File ""usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 549, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3051, in run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3106, in _run_cell
File ""usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3311, in run_cell_async
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3493, in run_ast_nodes
File ""usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""tmp/ipykernel_2107/74085429.py"", line 137, in <module>
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 508, in predict
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 208, in one_step_on_data_distributed
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 198, in one_step_on_data
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 96, in predict_step
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 807, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1315, in _maybe_build
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 225, in build_wrapper
File ""usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py"", line 183, in build
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 882, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 58, in symbolic_call
File ""usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1047, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 78, in compute_output_spec
File ""usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 200, in compute_output_spec
File ""tmp/ipykernel_2107/74085429.py"", line 38, in call
File ""tmp/ipykernel_2107/3701247216.py"", line 88, in _bilinear_interpolate
File ""tmp/ipykernel_2107/2303768048.py"", line 100, in _safe_lookup_pixel

	 [[sequential_75_1/co1_1/PartitionedCall_1002/PartitionedCall_2/cond_2]]
	tf2xla conversion failed while converting __inference_one_step_on_data_615470[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[PartitionedCall]]
```
",richardwhitehead,2024-09-06 17:38:01+00:00,['tilakrayal'],2024-09-30 09:40:01+00:00,2024-09-30 09:39:58+00:00,https://github.com/tensorflow/tensorflow/issues/75269,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('comp:xla', 'XLA'), ('TF 2.16', '')]","[{'comment_id': 2334674137, 'issue_id': 2510909756, 'author': 'richardwhitehead', 'body': 'Here is a different (and simpler) version of the code. This version works in AutoGraph mode in TensorFlow 2.13.1 on Python 3.8.10, but produces the above error when run in TensorFlow 2.16.1 on Python 3.10.12.\r\n\r\n```\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass UnAveragePooling2D(keras.layers.Layer):\r\n\r\n    def __init__(self, kernel, *, strides, name = \'co1\', dtype = None):\r\n        super().__init__(trainable=True, name=name, dtype=dtype)\r\n        self.kernel = kernel\r\n        self.strides = strides\r\n\r\n    def build(self, input_shape):\r\n        pass\r\n\r\n    def call(self, inputs):\r\n        inputs_shape = tf.shape(inputs)\r\n\r\n        # the averaged image had a minimum size of source.shape * strides, but it could have been larger if its size\r\n        # was not a multiple of stride. For now, assume it was a multiple of stride.\r\n        dest_shape = (inputs_shape[0], inputs.shape[1] * self.strides, inputs.shape[2] * self.strides, inputs_shape[3])\r\n        #dest = tf.zeros(dest_shape)\r\n\r\n        # width of destination invalid edges that need to be filled, again assuming that source size was a multiple of stride.\r\n        dest_start_invalid = (self.strides // 2, self.strides // 2)\r\n        dest_end_invalid = (self.strides // 2, self.strides // 2)\r\n\r\n        strides_f = tf.cast(self.strides, tf.float32)\r\n        dest_start_invalid_f = (tf.cast(dest_start_invalid[0], tf.float32), tf.cast(dest_start_invalid[1], tf.float32))\r\n        max_source_f = (tf.cast(inputs.shape[1] - 1, tf.float32), tf.cast(inputs.shape[2] - 1, tf.float32))\r\n\r\n        # biniliear interpolation with distorted edges\r\n        dest = tf.zeros((dest_shape[0], 0, dest_shape[2], dest_shape[3]))\r\n        for dest_r in range(dest_shape[1]):\r\n            dest_r_f = tf.cast(dest_r, tf.float32)\r\n            row = tf.zeros((dest_shape[0], 1, 0, dest_shape[3]))\r\n            for dest_c in range(dest_shape[2]):\r\n                dest_c_f = tf.cast(dest_c, tf.float32)\r\n                source_r = self._dest_to_rource(dest_r_f, strides_f, dest_start_invalid_f[0], max_source_f[0])\r\n                source_c = self._dest_to_rource(dest_c_f, strides_f, dest_start_invalid_f[1], max_source_f[1])\r\n                value = self._bilinear_interpolate(inputs, source_r, source_c)\r\n                #dest = self._assign_pixel_batch_values(dest, dest_r, dest_c, value)\r\n                value = tf.reshape(value, (inputs_shape[0], 1, 1, inputs_shape[3]))\r\n                row = tf.concat([row, value], axis=2)\r\n            dest = tf.concat([dest, row], axis=1)\r\n\r\n        return dest\r\n\r\n    @tf.function\r\n    def _dest_to_rource(self, dest, stride, dest_start_invalid, max_source):\r\n        """"""Given a destination pixel row or column position, work out the source\r\n        pixel location from which the value should be interpolated.""""""\r\n\r\n        if dest < dest_start_invalid + stride - 0.5:\r\n            return (dest - dest_start_invalid) / (stride - 0.5)\r\n        elif dest > dest_start_invalid + (max_source - 1.0) * stride - 0.5:\r\n            return ((dest - dest_start_invalid + 0.5) - (max_source - 1.0) * stride) / (stride - 0.5) + max_source - 1.0\r\n        else:\r\n            return (dest - dest_start_invalid + 0.5) / stride\r\n\r\n    @tf.function\r\n    def _bilinear_interpolate(self, source, r, c):\r\n        """"""Given a batch of source images, interpolate each from the four pixels surrounding point r,c.\r\n        Near the edge, fade to black.\r\n        """"""\r\n        # Algorithm (ignoring edges):\r\n        # 1. Round r,c down to get top-right source pixel r0, c0\r\n        # 2. Subtract r0, c0 from r, c get 0..1 proportions of a pixel fr, fc\r\n        # 3. The four pixels to be sampled are at p00=[r0, c0], p01=[r0,c0+1], p10=[r0+1, c0], and p11=[r0+1, c0+1]\r\n        # 4. For each channel, calculate a linear sum of the four pixels, as follows:\r\n        # 5. result =   p00.(1-fr).(1-fc)\r\n        #             + p01.(1-fr).fc\r\n        #             + p10.fr.(1-fc)\r\n        #             + p11.fr.fc\r\n        # To cope with edges and implement fade-to-black: if any of p00, p01, p10 or p11 would be outside the source\r\n        # image then use black instead\r\n        r0, c0 =  tf.cast(tf.floor(r), tf.int32), tf.cast(tf.floor(c), tf.int32)\r\n        fr, fc = r - tf.cast(r0, tf.float32), c - tf.cast(c0, tf.float32)\r\n        p00 = self._safe_lookup_pixel(source, r0, c0)\r\n        p01 = self._safe_lookup_pixel(source, r0, c0+1)\r\n        p10 = self._safe_lookup_pixel(source, r0+1, c0)\r\n        p11 = self._safe_lookup_pixel(source, r0+1, c0+1)\r\n        return p00*(1-fr)*(1-fc) + p01*(1-fr)*fc + p10*fr*(1-fc) + p11*fr*fc\r\n\r\n    @tf.function\r\n    def _safe_lookup_pixel(self, source, r, c):\r\n        """"""If x,y is a valid index into the source images then return the\r\n        batch of pixels at that location, otherwise return a batch of black pixels.""""""\r\n\r\n        source_shape = tf.shape(source)\r\n        if 0 <= r < source.shape[1] and 0 <= c < source.shape[2]:\r\n            return source[:, r, c, :]\r\n        else:\r\n            return tf.zeros((source_shape[0], source_shape[3]))\r\n\r\n\r\n\r\nTEST_SIZE = 16\r\n\r\ndata = [list(range(TEST_SIZE))] * TEST_SIZE\r\ndata = tf.convert_to_tensor(data, np.float32)\r\ndata = tf.reshape(data, (1, TEST_SIZE, TEST_SIZE, 1))\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.Input(shape=(TEST_SIZE, TEST_SIZE, 1), batch_size=1))\r\nmodel.add(keras.layers.AveragePooling2D((6, 6), padding=\'same\', strides=4))\r\nmodel.add(UnAveragePooling2D((6, 6), strides=4))\r\nmodel.compile(run_eagerly=False)\r\n\r\nexpanded = model.predict(data)\r\nprint(expanded[0,:,:,0])\r\n```', 'created_at': datetime.datetime(2024, 9, 6, 19, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374305020, 'issue_id': 2510909756, 'author': 'tilakrayal', 'body': '@richardwhitehead,\r\nI tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/08d84020487a9abf51ada4c68c8528e2/untitled2127.ipynb) and try to update to 2.17. Thank you!', 'created_at': datetime.datetime(2024, 9, 25, 14, 44, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379028590, 'issue_id': 2510909756, 'author': 'richardwhitehead', 'body': 'Thank you very much for investigating this. \r\n\r\nI tried updating my docker container to the latest version provided by NVIDIA (24.09-tf2-py3) but that still seems to be TensorFlow v2.16.1.\r\n\r\nHaving lost many days fighting version incompatibilities in the past, and given that Nvidia has chosen not to upgrade yet, I am unwilling to try updating TensorFlow and will wait until Nvidia release a container image with a newer version.\r\n\r\n \r\n\r\nI have no doubt that what you say is true and so please feel free to close the issue.\r\n\r\n \r\n\r\nMany thanks,\r\n\r\n \r\n\r\nRichard\r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\nFrom: tilakrayal ***@***.***> \r\nSent: 25 September 2024 15:45\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: richardwhitehead ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266 (Issue #75269)\r\n\r\n \r\n\r\n@richardwhitehead <https://github.com/richardwhitehead> ,\r\nI tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it here <https://colab.research.google.com/gist/tilakrayal/08d84020487a9abf51ada4c68c8528e2/untitled2127.ipynb>  and try to update to 2.17. Thank you!\r\n\r\n—\r\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/75269#issuecomment-2374305020> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/AFPFJLO55UDEAFUZKEUQPRLZYLD7HAVCNFSM6AAAAABNZAI4P2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZUGMYDKMBSGA> .\r\nYou are receiving this because you were mentioned.  <https://github.com/notifications/beacon/AFPFJLI33D7PIOQGCMFSAGDZYLD7HA5CNFSM6AAAAABNZAI4P2WGG33NNVSW45C7OR4XAZNMJFZXG5LFINXW23LFNZ2KUY3PNVWWK3TUL5UWJTUNQUCPY.gif> Message ID: ***@***.*** ***@***.***> >', 'created_at': datetime.datetime(2024, 9, 27, 11, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382449199, 'issue_id': 2510909756, 'author': 'tilakrayal', 'body': '@richardwhitehead,\r\nGlad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 9, 30, 8, 30, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382629759, 'issue_id': 2510909756, 'author': 'richardwhitehead', 'body': 'Fixed in v2.17', 'created_at': datetime.datetime(2024, 9, 30, 9, 39, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382629825, 'issue_id': 2510909756, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75269"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75269"">No</a>', 'created_at': datetime.datetime(2024, 9, 30, 9, 40, tzinfo=datetime.timezone.utc)}]","richardwhitehead (Issue Creator) on (2024-09-06 19:18:44 UTC): Here is a different (and simpler) version of the code. This version works in AutoGraph mode in TensorFlow 2.13.1 on Python 3.8.10, but produces the above error when run in TensorFlow 2.16.1 on Python 3.10.12.

```
import keras
import tensorflow as tf
import numpy as np


class UnAveragePooling2D(keras.layers.Layer):

    def __init__(self, kernel, *, strides, name = 'co1', dtype = None):
        super().__init__(trainable=True, name=name, dtype=dtype)
        self.kernel = kernel
        self.strides = strides

    def build(self, input_shape):
        pass

    def call(self, inputs):
        inputs_shape = tf.shape(inputs)

        # the averaged image had a minimum size of source.shape * strides, but it could have been larger if its size
        # was not a multiple of stride. For now, assume it was a multiple of stride.
        dest_shape = (inputs_shape[0], inputs.shape[1] * self.strides, inputs.shape[2] * self.strides, inputs_shape[3])
        #dest = tf.zeros(dest_shape)

        # width of destination invalid edges that need to be filled, again assuming that source size was a multiple of stride.
        dest_start_invalid = (self.strides // 2, self.strides // 2)
        dest_end_invalid = (self.strides // 2, self.strides // 2)

        strides_f = tf.cast(self.strides, tf.float32)
        dest_start_invalid_f = (tf.cast(dest_start_invalid[0], tf.float32), tf.cast(dest_start_invalid[1], tf.float32))
        max_source_f = (tf.cast(inputs.shape[1] - 1, tf.float32), tf.cast(inputs.shape[2] - 1, tf.float32))

        # biniliear interpolation with distorted edges
        dest = tf.zeros((dest_shape[0], 0, dest_shape[2], dest_shape[3]))
        for dest_r in range(dest_shape[1]):
            dest_r_f = tf.cast(dest_r, tf.float32)
            row = tf.zeros((dest_shape[0], 1, 0, dest_shape[3]))
            for dest_c in range(dest_shape[2]):
                dest_c_f = tf.cast(dest_c, tf.float32)
                source_r = self._dest_to_rource(dest_r_f, strides_f, dest_start_invalid_f[0], max_source_f[0])
                source_c = self._dest_to_rource(dest_c_f, strides_f, dest_start_invalid_f[1], max_source_f[1])
                value = self._bilinear_interpolate(inputs, source_r, source_c)
                #dest = self._assign_pixel_batch_values(dest, dest_r, dest_c, value)
                value = tf.reshape(value, (inputs_shape[0], 1, 1, inputs_shape[3]))
                row = tf.concat([row, value], axis=2)
            dest = tf.concat([dest, row], axis=1)

        return dest

    @tf.function
    def _dest_to_rource(self, dest, stride, dest_start_invalid, max_source):
        """"""Given a destination pixel row or column position, work out the source
        pixel location from which the value should be interpolated.""""""

        if dest < dest_start_invalid + stride - 0.5:
            return (dest - dest_start_invalid) / (stride - 0.5)
        elif dest > dest_start_invalid + (max_source - 1.0) * stride - 0.5:
            return ((dest - dest_start_invalid + 0.5) - (max_source - 1.0) * stride) / (stride - 0.5) + max_source - 1.0
        else:
            return (dest - dest_start_invalid + 0.5) / stride

    @tf.function
    def _bilinear_interpolate(self, source, r, c):
        """"""Given a batch of source images, interpolate each from the four pixels surrounding point r,c.
        Near the edge, fade to black.
        """"""
        # Algorithm (ignoring edges):
        # 1. Round r,c down to get top-right source pixel r0, c0
        # 2. Subtract r0, c0 from r, c get 0..1 proportions of a pixel fr, fc
        # 3. The four pixels to be sampled are at p00=[r0, c0], p01=[r0,c0+1], p10=[r0+1, c0], and p11=[r0+1, c0+1]
        # 4. For each channel, calculate a linear sum of the four pixels, as follows:
        # 5. result =   p00.(1-fr).(1-fc)
        #             + p01.(1-fr).fc
        #             + p10.fr.(1-fc)
        #             + p11.fr.fc
        # To cope with edges and implement fade-to-black: if any of p00, p01, p10 or p11 would be outside the source
        # image then use black instead
        r0, c0 =  tf.cast(tf.floor(r), tf.int32), tf.cast(tf.floor(c), tf.int32)
        fr, fc = r - tf.cast(r0, tf.float32), c - tf.cast(c0, tf.float32)
        p00 = self._safe_lookup_pixel(source, r0, c0)
        p01 = self._safe_lookup_pixel(source, r0, c0+1)
        p10 = self._safe_lookup_pixel(source, r0+1, c0)
        p11 = self._safe_lookup_pixel(source, r0+1, c0+1)
        return p00*(1-fr)*(1-fc) + p01*(1-fr)*fc + p10*fr*(1-fc) + p11*fr*fc

    @tf.function
    def _safe_lookup_pixel(self, source, r, c):
        """"""If x,y is a valid index into the source images then return the
        batch of pixels at that location, otherwise return a batch of black pixels.""""""

        source_shape = tf.shape(source)
        if 0 <= r < source.shape[1] and 0 <= c < source.shape[2]:
            return source[:, r, c, :]
        else:
            return tf.zeros((source_shape[0], source_shape[3]))



TEST_SIZE = 16

data = [list(range(TEST_SIZE))] * TEST_SIZE
data = tf.convert_to_tensor(data, np.float32)
data = tf.reshape(data, (1, TEST_SIZE, TEST_SIZE, 1))

model = keras.models.Sequential()
model.add(keras.Input(shape=(TEST_SIZE, TEST_SIZE, 1), batch_size=1))
model.add(keras.layers.AveragePooling2D((6, 6), padding='same', strides=4))
model.add(UnAveragePooling2D((6, 6), strides=4))
model.compile(run_eagerly=False)

expanded = model.predict(data)
print(expanded[0,:,:,0])
```

tilakrayal (Assginee) on (2024-09-25 14:44:38 UTC): @richardwhitehead,
I tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/08d84020487a9abf51ada4c68c8528e2/untitled2127.ipynb) and try to update to 2.17. Thank you!

richardwhitehead (Issue Creator) on (2024-09-27 11:09:00 UTC): Thank you very much for investigating this. 

I tried updating my docker container to the latest version provided by NVIDIA (24.09-tf2-py3) but that still seems to be TensorFlow v2.16.1.

Having lost many days fighting version incompatibilities in the past, and given that Nvidia has chosen not to upgrade yet, I am unwilling to try updating TensorFlow and will wait until Nvidia release a container image with a newer version.

 

I have no doubt that what you say is true and so please feel free to close the issue.

 

Many thanks,

 

Richard

 

 

 

From: tilakrayal ***@***.***> 
Sent: 25 September 2024 15:45
To: tensorflow/tensorflow ***@***.***>
Cc: richardwhitehead ***@***.***>; Mention ***@***.***>
Subject: Re: [tensorflow/tensorflow] AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266 (Issue #75269)

 

@richardwhitehead <https://github.com/richardwhitehead> ,
I tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it here <https://colab.research.google.com/gist/tilakrayal/08d84020487a9abf51ada4c68c8528e2/untitled2127.ipynb>  and try to update to 2.17. Thank you!

—
Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/75269#issuecomment-2374305020> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/AFPFJLO55UDEAFUZKEUQPRLZYLD7HAVCNFSM6AAAAABNZAI4P2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZUGMYDKMBSGA> .
You are receiving this because you were mentioned.  <https://github.com/notifications/beacon/AFPFJLI33D7PIOQGCMFSAGDZYLD7HA5CNFSM6AAAAABNZAI4P2WGG33NNVSW45C7OR4XAZNMJFZXG5LFINXW23LFNZ2KUY3PNVWWK3TUL5UWJTUNQUCPY.gif> Message ID: ***@***.*** ***@***.***> >

tilakrayal (Assginee) on (2024-09-30 08:30:11 UTC): @richardwhitehead,
Glad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!

richardwhitehead (Issue Creator) on (2024-09-30 09:39:58 UTC): Fixed in v2.17

google-ml-butler[bot] on (2024-09-30 09:40:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75269"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75269"">No</a>

"
2510688607,issue,closed,completed,incorrect result of sigmoid_cross_entropy_with_logits when input is np.inf,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation: https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits, this API's output should be consistent with this equation: `z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))`. However, when running the following code:

```
import tensorflow as tf
import numpy as np

x = tf.constant(np.inf)
z = tf.constant(0.9)
res1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)
print(f""TF's result: {res1}"")  # nan
log = tf.math.log
sigmoid = tf.math.sigmoid
res2 = z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
print(f""Expected result: {res2}"")  # inf
```

The API's output is inconsistent with the equation's output as follows:

```
TF's result: nan
Expected result: inf
```

The possible reason is that the current implementation contains `inf - xx*inf` when `x=inf`, thus lead to `NaN`.

https://github.com/tensorflow/tensorflow/blob/e4e8ba1af3511dc749a9d9d8dbac0c4dc3c7eab5/tensorflow/python/ops/nn_impl.py#L143

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.constant(np.inf)
z = tf.constant(0.9)
res1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)
print(f""TF's result: {res1}"")  # nan
log = tf.math.log
sigmoid = tf.math.sigmoid
res2 = z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
print(f""Expected result: {res2}"")  # inf
```


### Relevant log output

_No response_",maybeLee,2024-09-06 15:29:07+00:00,['Venkat6871'],2024-09-08 16:40:09+00:00,2024-09-08 16:40:06+00:00,https://github.com/tensorflow/tensorflow/issues/75256,"[('type:bug', 'Bug')]","[{'comment_id': 2336633828, 'issue_id': 2510688607, 'author': 'sanskarmodi8', 'body': ""Hi @maybeLee,\r\n\r\nThe behavior you're observing stems from TensorFlow's internal handling of extreme values like `np.inf`. While the manual computation results in `inf`, TensorFlow applies numerically stable operations that avoid overflow/underflow issues, which can lead to `NaN` in such cases.\r\n\r\nThe underlying formula in TensorFlow ensures stability for large values of x, which is likely why the API returns `NaN` instead of `inf`. Though this seems inconsistent with the documented formula, TensorFlow prioritizes stability over direct computation in edge cases.\r\n\r\nOne potential workaround is to manually handle extreme values before passing them to the API to avoid getting `NaN` in such cases."", 'created_at': datetime.datetime(2024, 9, 8, 10, 35, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336748140, 'issue_id': 2510688607, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75256"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75256"">No</a>', 'created_at': datetime.datetime(2024, 9, 8, 16, 40, 8, tzinfo=datetime.timezone.utc)}]","sanskarmodi8 on (2024-09-08 10:35:20 UTC): Hi @maybeLee,

The behavior you're observing stems from TensorFlow's internal handling of extreme values like `np.inf`. While the manual computation results in `inf`, TensorFlow applies numerically stable operations that avoid overflow/underflow issues, which can lead to `NaN` in such cases.

The underlying formula in TensorFlow ensures stability for large values of x, which is likely why the API returns `NaN` instead of `inf`. Though this seems inconsistent with the documented formula, TensorFlow prioritizes stability over direct computation in edge cases.

One potential workaround is to manually handle extreme values before passing them to the API to avoid getting `NaN` in such cases.

google-ml-butler[bot] on (2024-09-08 16:40:08 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75256"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75256"">No</a>

"
2509979301,issue,closed,completed,Segmentation Fault memcpy in tim::vx::TensorImpl::CopyDataToTensor TFLite with VX Delegate run,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Linux Yocto nanbield

### Mobile device

iMX8M Plus EVK

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

13.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Pardon me

I tried to run this [model](https://github.com/Kazuhito00/MOT-Tracking-by-Detection-Pipeline/blob/main/Detector/light_person_detector/model/model.tflite) in C++ using tensorflow for NPU run. but it seems I always had Segmentation Fault when Invoke() is triggered

I also test this model using the python code, and it works. so I think the model itself is not the problem

I dont have many experience with C++, but from the gdb backtrace it seems the fault triggered when tim::vx::TensorImpl::CopyDataToTensor(void const*, unsigned int) () triggered in Invoke

if you had any idea regarding this, it would be helpful, thank you

### Standalone code to reproduce the issue

```shell
std::pair<cv::Mat, float> Detector::preprocess(const cv::Mat& image, const cv::Size& input_size) {
    float ratio = std::min(static_cast<float>(input_size.width) / image.cols,
                           static_cast<float>(input_size.height) / image.rows);

    cv::Size new_size(static_cast<int>(image.cols * ratio), static_cast<int>(image.rows * ratio));
    cv::Mat resized_image;
    cv::resize(image, resized_image, new_size, 0, 0, cv::INTER_LINEAR);

    cv::Mat converted_image;
    resized_image.convertTo(converted_image, CV_8UC3);

    std::cout << ""Resized Image dimension: "" << converted_image.rows << ""x"" << converted_image.cols << ""x"" << converted_image.channels() << "" with type of: "" << converted_image.type() << std::endl;

    cv::Mat padded_image = cv::Mat::ones(input_size, CV_8UC3) * 114;
    converted_image.copyTo(padded_image(cv::Rect(0, 0, converted_image.cols, converted_image.rows)));

    std::cout << ""Resized Image dimension: "" << padded_image.rows << ""x"" << padded_image.cols << ""x"" << padded_image.channels() << "" with type of: "" << padded_image.type() << std::endl;
    
    return std::make_pair(padded_image, ratio);
}




void Detector::detect(const cv::Mat& image) {
    cv::Mat temp_image = image.clone();
    
    auto [preprocessed_image, ratio] = preprocess(temp_image, input_shape_);
    cv::Mat input = cv::dnn::blobFromImage(preprocessed_image);
    
    std::cout << ""Preprocess Completed""<<std::endl;
    std::cout << ""Blob Dimension : ""<< input.size[0] << ""x"" << input.size[1] << ""x"" << input.size[2] << ""x"" << input.size[3] << std::endl;
    std::cout << input.type() << std::endl;

    // Setting input tensor
    TfLiteTensor* input_data = interpreter_->tensor(interpreter_->inputs()[0]);
    const uint input_width = input_data->dims->data[3];
    const uint input_height = input_data->dims->data[2];
    const uint input_channels = input_data->dims->data[1];
    const uint batch_size = input_data->dims->data[0];

    std::memcpy(input_data->data.f, input.ptr<float>(), batch_size * input_width * input_height * input_channels * sizeof(float));
    std::cout << ""Input Set up Complete"" << std::endl;

    // Running inference
    interpreter_->Invoke(); //error here segmentation fault

    std::cout << ""Inference Completed""<<std::endl;
```


### Relevant log output

```shell
Thread 1 ""detector_app"" received signal SIGSEGV, Segmentation fault.
__memcpy_generic () at ../sysdeps/aarch64/multiarch/../memcpy.S:127
127     ../sysdeps/aarch64/multiarch/../memcpy.S: No such file or directory.
(gdb) bt
#0  __memcpy_generic () at ../sysdeps/aarch64/multiarch/../memcpy.S:127
#1  0x0000fffff4ff0d38 in tim::vx::TensorImpl::CopyDataToTensor(void const*, unsigned int) () from /usr/lib/libtim-vx.so
#2  0x0000fffff4ff0b44 in tim::vx::TensorImpl::Init(void*) ()
   from /usr/lib/libtim-vx.so
#3  0x0000fffff4ff1638 in tim::vx::TensorImpl::TensorImpl(tim::vx::Graph*, tim::vx::TensorSpec const&, void const*) () from /usr/lib/libtim-vx.so
#4  0x0000fffff4fedf08 in tim::vx::GraphImpl::CreateTensor(tim::vx::TensorSpec const&, void const*) () from /usr/lib/libtim-vx.so
#5  0x0000fffff55b5fb0 in ?? () from /usr/lib/libvx_delegate.so
#6  0x0000fffff55b7c94 in vx::delegate::Delegate::Invoke(vx::delegate::OpData const&, TfLiteContext*, TfLiteNode*) () from /usr/lib/libvx_delegate.so
#7  0x0000fffff7be9d9c in tflite::Subgraph::InvokeImpl() ()
   from /usr/lib/libtensorflow-lite.so.2.14.0
#8  0x0000fffff7bea388 in tflite::Subgraph::Invoke() ()
   from /usr/lib/libtensorflow-lite.so.2.14.0
#9  0x0000fffff7bd440c in tflite::impl::Interpreter::Invoke() ()
   from /usr/lib/libtensorflow-lite.so.2.14.0
#10 0x0000aaaaaaaa381c in Detector::detect (this=this@entry=0xfffffffff8c8,
    image=...)
    at /home/ubuntu/imx-yocto-bsp/sdk/sysroots/armv8a-poky-linux/usr/include/c++/13.2.0/bits/unique_ptr.h:199
#11 0x0000aaaaaaaa27f0 in main (argc=<optimized out>, argv=<optimized out>)
--Type <RET> for more, q to quit, c to continue without paging--
    at /home/ubuntu/imx-yocto-bsp/test/main.cpp:52
```
",adrejohan,2024-09-06 09:27:33+00:00,['pkgoogle'],2024-12-12 18:38:45+00:00,2024-10-04 02:01:43+00:00,https://github.com/tensorflow/tensorflow/issues/75241,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2360949472, 'issue_id': 2509979301, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nCould you please take look into this issue ? Thank you.', 'created_at': datetime.datetime(2024, 9, 19, 13, 11, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361878624, 'issue_id': 2509979301, 'author': 'pkgoogle', 'body': ""Hi @adrejohan, Can you explain the context in which you are running this code a little more? Like... I'm not sure what you mean by\r\n\r\n> in C++ using tensorflow for NPU run\r\n\r\nAre you running an executable? What NPU are you using (on a phone? iOS? Android?)? Are you running it on a server somewhere (If so, what command did you use to run it?)? How are you running it? What other information do you think I would need to reproduce this?"", 'created_at': datetime.datetime(2024, 9, 19, 18, 21, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259640, 'issue_id': 2509979301, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646793, 'issue_id': 2509979301, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646841, 'issue_id': 2509979301, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75241"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75241"">No</a>', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537261787, 'issue_id': 2509979301, 'author': 'BralSLA', 'body': ""Hey there, I am experiencing the same issue. I believe in a similar use-case to the OP.\r\nI am running a tflite model through a C++ application on an imx8mp.\r\nWe are trying to run our model on the imx8's NPU using the vx_delegate, and we too are seeing the same segfault in the CopyDataToTensor() function.\r\nOur application is running on Yocto linux build, version 5.15.71. Here's a copy of the stacktrace:\r\n\r\n``` \r\n        libc.so.6!__memcpy_generic() Line 167\tC++\r\n \tlibtim-vx.so!tim::vx::TensorImpl::CopyDataToTensor(void const*, unsigned int)\t\r\n \tlibtim-vx.so!tim::vx::TensorImpl::Init(void*)\t\r\n \tlibtim-vx.so!tim::vx::TensorImpl::TensorImpl(tim::vx::Graph*, tim::vx::TensorSpec const&, void const*)\t\r\n \tlibtim-vx.so!tim::vx::GraphImpl::CreateTensor(tim::vx::TensorSpec const&, void const*)\t\r\n \tlibtim-vx.so![Unknown/Just-In-Time compiled code]\t\r\n \tlibtim-vx.so!tim::transform::layout_inference_impl::HandleLayoutInfer(std::shared_ptr<tim::transform::layout_inference_impl::LayoutInferContext>&, std::shared_ptr<tim::vx::Operation> const&)\t\r\n \tlibtim-vx.so!tim::transform::LayoutInference(std::shared_ptr<tim::vx::Graph> const&, std::shared_ptr<tim::vx::Context>&)\t\r\n \tlibvx_delegate.so!vx::delegate::Delegate::Invoke(vx::delegate::OpData const&, TfLiteContext*, TfLiteNode*)\t\r\n \tlibtensorflow-lite.so.2.9.1!tflite::Subgraph::Invoke()\t\r\n \tlibtensorflow-lite.so.2.9.1!tflite::Interpreter::Invoke()\t\r\n```"", 'created_at': datetime.datetime(2024, 12, 11, 21, 46, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539751753, 'issue_id': 2509979301, 'author': 'pkgoogle', 'body': 'Hi @BralSLA can you create a new issue for this with your data? Additionally are you able to share your model so that we may reproduce this? (If you are unable to share the full model -- maybe you see if you can reproduce with a smaller model which has one of all the same ops you used in the full model -- adjust as needed to get this through). Thanks for your help.', 'created_at': datetime.datetime(2024, 12, 12, 18, 38, 43, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-19 13:11:39 UTC): Hi, @pkgoogle

Could you please take look into this issue ? Thank you.

pkgoogle (Assginee) on (2024-09-19 18:21:35 UTC): Hi @adrejohan, Can you explain the context in which you are running this code a little more? Like... I'm not sure what you mean by


Are you running an executable? What NPU are you using (on a phone? iOS? Android?)? Are you running it on a server somewhere (If so, what command did you use to run it?)? How are you running it? What other information do you think I would need to reproduce this?

github-actions[bot] on (2024-09-27 02:01:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-04 02:01:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-04 02:01:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75241"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75241"">No</a>

BralSLA on (2024-12-11 21:46:49 UTC): Hey there, I am experiencing the same issue. I believe in a similar use-case to the OP.
I am running a tflite model through a C++ application on an imx8mp.
We are trying to run our model on the imx8's NPU using the vx_delegate, and we too are seeing the same segfault in the CopyDataToTensor() function.
Our application is running on Yocto linux build, version 5.15.71. Here's a copy of the stacktrace:

``` 
        libc.so.6!__memcpy_generic() Line 167	C++
 	libtim-vx.so!tim::vx::TensorImpl::CopyDataToTensor(void const*, unsigned int)	
 	libtim-vx.so!tim::vx::TensorImpl::Init(void*)	
 	libtim-vx.so!tim::vx::TensorImpl::TensorImpl(tim::vx::Graph*, tim::vx::TensorSpec const&, void const*)	
 	libtim-vx.so!tim::vx::GraphImpl::CreateTensor(tim::vx::TensorSpec const&, void const*)	
 	libtim-vx.so![Unknown/Just-In-Time compiled code]	
 	libtim-vx.so!tim::transform::layout_inference_impl::HandleLayoutInfer(std::shared_ptr<tim::transform::layout_inference_impl::LayoutInferContext>&, std::shared_ptr<tim::vx::Operation> const&)	
 	libtim-vx.so!tim::transform::LayoutInference(std::shared_ptr<tim::vx::Graph> const&, std::shared_ptr<tim::vx::Context>&)	
 	libvx_delegate.so!vx::delegate::Delegate::Invoke(vx::delegate::OpData const&, TfLiteContext*, TfLiteNode*)	
 	libtensorflow-lite.so.2.9.1!tflite::Subgraph::Invoke()	
 	libtensorflow-lite.so.2.9.1!tflite::Interpreter::Invoke()	
```

pkgoogle (Assginee) on (2024-12-12 18:38:43 UTC): Hi @BralSLA can you create a new issue for this with your data? Additionally are you able to share your model so that we may reproduce this? (If you are unable to share the full model -- maybe you see if you can reproduce with a smaller model which has one of all the same ops you used in the full model -- adjust as needed to get this through). Thanks for your help.

"
2508487094,issue,closed,completed,Save and Load Notebook use a bad checkpoint_path file name,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

No

### OS platform and distribution

Windows 11 - WSL - Docker Container

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The cell:

```
checkpoint_path = ""training_1/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(train_images, 
          train_labels,  
          epochs=10,
          validation_data=(test_images, test_labels),
          callbacks=[cp_callback])  # Pass callback to training

# This may generate warnings related to saving the state of the optimizer.
# These warnings (and similar warnings throughout this notebook)
# are in place to discourage outdated usage, and can be ignored.
```

Throws the following value error:

```
ValueError: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=training_1/cp.ckpt
```


### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/keras/save_and_load
```


### Relevant log output

```shell
{
	""name"": ""ValueError"",
	""message"": ""When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=training_1/cp.ckpt"",
	""stack"": ""---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[12], line 5
      2 checkpoint_dir = os.path.dirname(checkpoint_path)
      4 # Create a callback that saves the model's weights
----> 5 cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
      6                                                  save_weights_only=True,
      7                                                  verbose=1)
      9 # Train the model with the new callback
     10 model.fit(train_images, 
     11           train_labels,  
     12           epochs=10,
     13           validation_data=(test_images, test_labels),
     14           callbacks=[cp_callback])  # Pass callback to training

File ~/.local/share/virtualenvs/com.docker.devenvironments.code-o74UYjc6/lib/python3.9/site-packages/keras/src/callbacks/model_checkpoint.py:183, in ModelCheckpoint.__init__(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)
    181 if save_weights_only:
    182     if not self.filepath.endswith(\"".weights.h5\""):
--> 183         raise ValueError(
    184             \""When using `save_weights_only=True` in `ModelCheckpoint`\""
    185             \"", the filepath provided must end in `.weights.h5` \""
    186             \""(Keras weights format). Received: \""
    187             f\""filepath={self.filepath}\""
    188         )
    189 else:
    190     if not self.filepath.endswith(\"".keras\""):

ValueError: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=training_1/cp.ckpt""
}
```
",dnoliver,2024-09-05 18:42:39+00:00,['tilakrayal'],2024-10-11 02:01:20+00:00,2024-10-11 02:01:17+00:00,https://github.com/tensorflow/tensorflow/issues/75196,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2332409428, 'issue_id': 2508487094, 'author': 'dnoliver', 'body': 'Then it works after you change the line to `checkpoint_path = ""training_1/cp.ckpt.weights.h5""`', 'created_at': datetime.datetime(2024, 9, 5, 18, 43, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332425221, 'issue_id': 2508487094, 'author': 'dnoliver', 'body': ""The sample runs into other problems as well. Like `trainning_2` folder needs to be created manually, `model.load_weights(latest)` complains about `ValueError: File format not supported: filepath=None. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.`, similar problem in `model.save_weights('./checkpoints/my_checkpoint')`, the line `model.save('saved_model/my_model') ` complains with `ValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=saved_model/my_model.`"", 'created_at': datetime.datetime(2024, 9, 5, 18, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336657696, 'issue_id': 2508487094, 'author': 'sanskarmodi8', 'body': 'Hey @dnoliver , \r\n\r\nI looked into the issue and found out there was several errors in the specified notebook.\r\n\r\nI have created a pull request regarding same - https://github.com/tensorflow/docs/pull/2324', 'created_at': datetime.datetime(2024, 9, 8, 11, 57, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338285666, 'issue_id': 2508487094, 'author': 'tilakrayal', 'body': '@dnoliver,\r\nThe pr has been assigned for reviewing and once it is merged this issue will move to closed status. Thank you!', 'created_at': datetime.datetime(2024, 9, 9, 14, 30, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374027207, 'issue_id': 2508487094, 'author': 'tilakrayal', 'body': '@dnoliver,\r\nThe PR which was raised for the similar issue has been merged and also I tried to execute the official doc code and it was executed without any issues/errors. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ef564a09b99e15531616ef4589b7362a/save_and_load.ipynb).\r\n\r\nhttps://github.com/tensorflow/docs/pull/2324\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 25, 13, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390349786, 'issue_id': 2508487094, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406393871, 'issue_id': 2508487094, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 11, 2, 1, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406393972, 'issue_id': 2508487094, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75196"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75196"">No</a>', 'created_at': datetime.datetime(2024, 10, 11, 2, 1, 19, tzinfo=datetime.timezone.utc)}]","dnoliver (Issue Creator) on (2024-09-05 18:43:08 UTC): Then it works after you change the line to `checkpoint_path = ""training_1/cp.ckpt.weights.h5""`

dnoliver (Issue Creator) on (2024-09-05 18:52:26 UTC): The sample runs into other problems as well. Like `trainning_2` folder needs to be created manually, `model.load_weights(latest)` complains about `ValueError: File format not supported: filepath=None. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.`, similar problem in `model.save_weights('./checkpoints/my_checkpoint')`, the line `model.save('saved_model/my_model') ` complains with `ValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=saved_model/my_model.`

sanskarmodi8 on (2024-09-08 11:57:28 UTC): Hey @dnoliver , 

I looked into the issue and found out there was several errors in the specified notebook.

I have created a pull request regarding same - https://github.com/tensorflow/docs/pull/2324

tilakrayal (Assginee) on (2024-09-09 14:30:43 UTC): @dnoliver,
The pr has been assigned for reviewing and once it is merged this issue will move to closed status. Thank you!

tilakrayal (Assginee) on (2024-09-25 13:02:07 UTC): @dnoliver,
The PR which was raised for the similar issue has been merged and also I tried to execute the official doc code and it was executed without any issues/errors. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ef564a09b99e15531616ef4589b7362a/save_and_load.ipynb).

https://github.com/tensorflow/docs/pull/2324

Thank you!

github-actions[bot] on (2024-10-03 02:01:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-11 02:01:15 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-11 02:01:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75196"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75196"">No</a>

"
2508450981,issue,closed,completed,Overfit and Underfit Notebook fails on compile_and_fit,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

No

### OS platform and distribution

Windows 11 - WSL - Docker Container

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to run the `overfit_and_underfit.ipynb` sample fails. Error log (and the full Traceback is below as well):

```
ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 1)
```

### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[28], line 1
----> 1 size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')

Cell In[18], line 13
      4 model.compile(optimizer=optimizer,
      5               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
      6               metrics=[
      7                 tf.keras.metrics.BinaryCrossentropy(
      8                     from_logits=True, name='binary_crossentropy'),
      9                 'accuracy'])
     11 model.summary()
---> 13 history = model.fit(
     14   train_ds,
     15   steps_per_epoch = STEPS_PER_EPOCH,
     16   epochs=max_epochs,
     17   validation_data=validate_ds,
     18   callbacks=get_callbacks(name),
     19   verbose=0)
     20 return history

File ~/.local/share/virtualenvs/com.docker.devenvironments.code-o74UYjc6/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.local/share/virtualenvs/com.docker.devenvironments.code-o74UYjc6/lib/python3.9/site-packages/keras/src/backend/tensorflow/nn.py:694, in binary_crossentropy(target, output, from_logits)
    691 output = tf.convert_to_tensor(output)
    693 if len(target.shape) != len(output.shape):
--> 694     raise ValueError(
    695         ""Arguments `target` and `output` must have the same rank ""
    696         ""(ndim). Received: ""
    697         f""target.shape={target.shape}, output.shape={output.shape}""
    698     )
    699 for e1, e2 in zip(target.shape, output.shape):
    700     if e1 is not None and e2 is not None and e1 != e2:

ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 1)
```
",dnoliver,2024-09-05 18:21:06+00:00,['Venkat6871'],2024-09-24 02:01:40+00:00,2024-09-24 02:01:36+00:00,https://github.com/tensorflow/tensorflow/issues/75194,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2336684913, 'issue_id': 2508450981, 'author': 'sanskarmodi8', 'body': 'Hi @dnoliver,\r\nI looked into this issue and have opened a pull request regarding the same - https://github.com/tensorflow/docs/pull/2325', 'created_at': datetime.datetime(2024, 9, 8, 13, 21, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337384497, 'issue_id': 2508450981, 'author': 'Venkat6871', 'body': 'Hi **@dnoliver** ,\r\nThe [pr](https://github.com/tensorflow/docs/pull/2325) has been assigned for reviewing and once it is merged this issue will move to closed status.\r\nThank You!', 'created_at': datetime.datetime(2024, 9, 9, 7, 52, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354345354, 'issue_id': 2508450981, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 17, 1, 48, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369958965, 'issue_id': 2508450981, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369959066, 'issue_id': 2508450981, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75194"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75194"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 39, tzinfo=datetime.timezone.utc)}]","sanskarmodi8 on (2024-09-08 13:21:49 UTC): Hi @dnoliver,
I looked into this issue and have opened a pull request regarding the same - https://github.com/tensorflow/docs/pull/2325

Venkat6871 (Assginee) on (2024-09-09 07:52:24 UTC): Hi **@dnoliver** ,
The [pr](https://github.com/tensorflow/docs/pull/2325) has been assigned for reviewing and once it is merged this issue will move to closed status.
Thank You!

github-actions[bot] on (2024-09-17 01:48:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-24 02:01:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-24 02:01:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75194"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75194"">No</a>

"
2507573944,issue,closed,completed,Inference results mismatch for Keras model before serialization and after serialization into SavedModel,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I observe inference results mismatch for cases before and after serialization into SavedModel for Keras model with multiple inputs and outputs.
I have two examples of it. See below.

### Standalone code to reproduce the issue

```shell
# Example 1.

import numpy as np
import tensorflow as tf

np.random.seed(23235)

input_names = [""k"", ""b"", ""m"", ""c"", ""x""]
input_shapes = [[1, 1], [1, 3], [1, 2], [1, 5], [1, 4]]

inputs = []
outputs = []
for ind in range(len(input_names)):
    input = tf.keras.Input(shape=input_shapes[ind][1:], name=input_names[ind])
    inputs.append(input)
    outputs.append(tf.keras.layers.Activation(tf.nn.sigmoid)(input))

model_in_memory = tf.keras.Model(inputs=inputs, outputs=outputs)
model_in_memory.export('saved_model')
loaded = tf.saved_model.load('saved_model')
model_from_disk = loaded.signatures['serving_default']

test_list = []
for input_shape in input_shapes:
    test_list.append(np.random.rand(*input_shape))

test_dict = {}
for input_shape, input_name in zip(input_shapes, input_names):
    test_dict[input_name] = np.random.rand(*input_shape)

print('results for original model = ', model_in_memory(test_list))
print('results for saved model = ', model_from_disk(**test_dict))



#Example 2
import numpy as np
import tensorflow as tf

np.random.seed(23235)

input_names = [""k"", ""b"", ""m"", ""c"", ""x""]
input_shapes = [[1, 1], [1, 3], [1, 2], [1, 5], [1, 4]]

inputs = []
outputs = {}
for ind in range(len(input_names)):
    input = tf.keras.Input(shape=input_shapes[ind][1:], name=input_names[ind])
    inputs.append(input)
    outputs[""name"" + str(ind)] = tf.keras.layers.Activation(tf.nn.sigmoid)(input)

model_in_memory = tf.keras.Model(inputs=inputs, outputs=outputs)
model_in_memory.export('saved_model')
loaded = tf.saved_model.load('saved_model')
model_from_disk = loaded.signatures['serving_default']

test_list = []
for input_shape in input_shapes:
    test_list.append(np.random.rand(*input_shape))

test_dict = {}
for input_shape, input_name in zip(input_shapes, input_names):
    test_dict[input_name] = np.random.rand(*input_shape)

print('results for original model = ', model_in_memory(test_list))
print('results for saved model = ', model_from_disk(**test_dict))
```


### Relevant log output

```shell
Example 1.
results for original model =  [<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.57518655]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.6557526 , 0.6859775 , 0.64670473]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.7017947 , 0.53047395]], dtype=float32)>, <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.5904319 , 0.7172047 , 0.52846706, 0.58635867, 0.6636075 ]],
      dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.57865363, 0.68113613, 0.7037658 , 0.6035671 ]], dtype=float32)>]
results for saved model =  {'output_2': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.6410256 , 0.55847204]], dtype=float32)>, 'output_4': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.6274846 , 0.50716597, 0.5257206 , 0.58835816]], dtype=float32)>, 'output_1': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.64240766, 0.5903851 , 0.7065355 ]], dtype=float32)>, 'output_0': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6045815]], dtype=float32)>, 'output_3': <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.6407081 , 0.570733  , 0.5671946 , 0.65474033, 0.5883632 ]],
      dtype=float32)>}

Example 2.
results for original model =  {'name0': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.57518655]], dtype=float32)>, 'name1': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.6557526 , 0.6859775 , 0.64670473]], dtype=float32)>, 'name2': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.7017947 , 0.53047395]], dtype=float32)>, 'name3': <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.5904319 , 0.7172047 , 0.52846706, 0.58635867, 0.6636075 ]],
      dtype=float32)>, 'name4': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.57865363, 0.68113613, 0.7037658 , 0.6035671 ]], dtype=float32)>}
results for saved model =  {'name0': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6045815]], dtype=float32)>, 'name4': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.6274846 , 0.50716597, 0.5257206 , 0.58835816]], dtype=float32)>, 'name2': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.6410256 , 0.55847204]], dtype=float32)>, 'name3': <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.6407081 , 0.570733  , 0.5671946 , 0.65474033, 0.5883632 ]],
      dtype=float32)>, 'name1': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.64240766, 0.5903851 , 0.7065355 ]], dtype=float32)>}
```
",rkazants,2024-09-05 11:55:55+00:00,['tilakrayal'],2024-09-25 02:02:06+00:00,2024-09-25 02:02:03+00:00,https://github.com/tensorflow/tensorflow/issues/75177,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2340711618, 'issue_id': 2507573944, 'author': 'tilakrayal', 'body': '@rkazants,\r\nThank you for reporting the issue. Tensorflow v2.17 contains the keras3.0 which might be the reason for the mismatch. As this issue is more related to keras, could you please create the issue in the Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 9, 10, 13, 7, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336035, 'issue_id': 2507573944, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372733526, 'issue_id': 2507573944, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 25, 2, 2, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372733584, 'issue_id': 2507573944, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75177"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75177"">No</a>', 'created_at': datetime.datetime(2024, 9, 25, 2, 2, 5, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-10 13:07:41 UTC): @rkazants,
Thank you for reporting the issue. Tensorflow v2.17 contains the keras3.0 which might be the reason for the mismatch. As this issue is more related to keras, could you please create the issue in the Keras-team/keras repo for the quick resolution. Thank you!

github-actions[bot] on (2024-09-18 01:58:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-25 02:02:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-25 02:02:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75177"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75177"">No</a>

"
2507526020,issue,closed,completed,Problem with tf.keras.layers.SimpleRNNCell ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

NVIDIA GeForce RTX 4070Ti

### Current behavior?

Current behavior is I get error message 'InaccessibleTensorError' due to line 'out0, state0 = self.RNNCell0(word, states=state0, training=training)'

### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""2""

import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

tf.random.set_seed(22)
np.random.seed(22)
assert tf.__version__.startswith('2.')

batch_size = 128
total_words = 10000
max_review_len = 80
embedding_len = 100

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=total_words)
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)

train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.shuffle(10000).batch(batch_size, drop_remainder=True)

test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_data = test_data.batch(batch_size, drop_remainder=True)

print('x_train_shape:', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))
print('x_test_shape:', x_test.shape)

class RNN_Build(tf.keras.Model):
    def __init__(self, units):
        super(RNN_Build, self).__init__()
        self.state0 = [tf.zeros([batch_size, units])]
        self.state1 = [tf.zeros([batch_size, units])]
        self.embedding = tf.keras.layers.Embedding(total_words, embedding_len,
                                                   input_length=max_review_len)
        self.RNNCell0 = tf.keras.layers.SimpleRNNCell(units, dropout=0.2)
        self.RNNCell1 = tf.keras.layers.SimpleRNNCell(units, dropout=0.2)
        self.RNNCell1 = tf.keras.layers.SimpleRNNCell(units, dropout=0.2)
    def call(self, inputs, training=None):
        x = inputs
        x = self.embedding(x)
        state0 = self.state0
        state1 = self.state1
        for word in tf.unstack(x, axis=1):
            out0, state0 = self.RNNCell0(word, states=state0, training=training)
            out1, state1 = self.RNNCell1(out0, states=state1, training=training)
        x = self.outlayer(out1)
        prob = tf.sigmoid(x)
        return prob
import time
units = 64
epochs = 4
t0 = time.time()
model = RNN_Build(units)
model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
              loss=tf.losses.BinaryCrossentropy(),
              metrics=['accuracy'])
model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InaccessibleTensorError                   Traceback (most recent call last)
Cell In[5], line 9
      5 model = RNN_Build(units)
      6 model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
      7               loss=tf.losses.BinaryCrossentropy(),
      8               metrics=['accuracy'])
----> 9 model.fit(train_data, epochs=epochs, validation_data=test_data, validation_freq=2)

File /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

Cell In[4], line 17
     15 state1 = self.state1
     16 for word in tf.unstack(x, axis=1):
---> 17     out0, state0 = self.RNNCell0(word, states=state0, training=training)
     18     out1, state1 = self.RNNCell1(out0, states=state1, training=training)
     19 x = self.outlayer(out1)

File /usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py:144, in FunctionCaptures.capture_by_value(self, graph, tensor, name)
...

Arguments received by SimpleRNNCell.call():
  • sequence=tf.Tensor(shape=(128, 100), dtype=float32)
  • states=['tf.Tensor(shape=(128, 64), dtype=float32)']
  • training=True
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```
",Zofns,2024-09-05 11:31:33+00:00,['Venkat6871'],2024-09-23 05:43:19+00:00,2024-09-21 01:58:28+00:00,https://github.com/tensorflow/tensorflow/issues/75176,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2333585699, 'issue_id': 2507526020, 'author': 'Venkat6871', 'body': 'Hi **@Zofns** ,\r\nI tried to run your code on Colab using TF v2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f46d151660710da513bad3623a3fa39d/75176_tf-2-17-0-and-nightly-v.ipynb) here for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 6, 9, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350779412, 'issue_id': 2507526020, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 14, 1, 57, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921008, 'issue_id': 2507526020, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921049, 'issue_id': 2507526020, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75176"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75176"">No</a>', 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 30, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-06 09:01:20 UTC): Hi **@Zofns** ,
I tried to run your code on Colab using TF v2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f46d151660710da513bad3623a3fa39d/75176_tf-2-17-0-and-nightly-v.ipynb) here for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

github-actions[bot] on (2024-09-14 01:57:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-21 01:58:28 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-21 01:58:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75176"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75176"">No</a>

"
2507419694,issue,closed,completed,Illegal instruction (core dumped) on AMD SEV-SNP enabled machine with TensorFlow 2.17.0,"### Issue type

Build/Install


### Description:
I am using an AWS instance with an AMD SEV-SNP enabled machine (AMD EPYC 7R13 Processor) running Ubuntu 22.04.4 LTS. I attempted to install TensorFlow using both `pip install` and a pre-built wheel from the TensorFlow website (`tensorflow_cpu-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`). While TensorFlow installed successfully, any attempt to import TensorFlow using `import tensorflow as tf` results in an 'Illegal instruction (core dumped)' error.

The issue persists across different installation methods, but I was able to resolve it by building TensorFlow from source. I believe there is an incompatibility with the AMD SEV-SNP architecture and the pre-built TensorFlow binaries.

### System Information:
- TensorFlow version: 2.17.0
- Installed via: `pip install` and TensorFlow pre-built wheel from [link to the wheel]
- OS: Ubuntu 22.04.4 LTS
- Hardware: AMD EPYC 7R13 Processor (SEV-SNP enabled)
- Python version: 3.11



### Expected Behavior:
TensorFlow should import without any errors.

### Current Behavior:
Importing TensorFlow results in an 'Illegal instruction (core dumped)' error.

### Additional Details:
Building TensorFlow from source resolved the issue, but the pre-built binaries seem to be incompatible with my architecture.


### Standalone code to reproduce the issue

```shell
1. Install TensorFlow via pip: `pip install tensorflow`
2. Try to import TensorFlow using `import tensorflow as tf`
```


### Relevant log output

_No response_",sb-fuji,2024-09-05 10:39:34+00:00,['tilakrayal'],2024-09-16 03:26:45+00:00,2024-09-16 03:26:43+00:00,https://github.com/tensorflow/tensorflow/issues/75175,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2333467613, 'issue_id': 2507419694, 'author': 'tilakrayal', 'body': '@sb-fuji,\r\nGenerally the ""Illegal instruction (core dumped)"" error caused by a CPU that does not support AVX instructions.\r\nThe Intel(R) Celeron(R) N4505 processor does not support AVX instructions, so you will not be able to use TensorFlow on this CPU without compiling it from the source. The earlier version doesn\'t require the AVX instructions but the newer version does.\r\n\r\nThe issue might be related to your environment.You can find the suitable Nvidia driver for your GPU from [here](https://www.nvidia.com/Download/index.aspx).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 6, 7, 55, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350779433, 'issue_id': 2507419694, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 14, 1, 57, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351961356, 'issue_id': 2507419694, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75175"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75175"">No</a>', 'created_at': datetime.datetime(2024, 9, 16, 3, 26, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-06 07:55:59 UTC): @sb-fuji,
Generally the ""Illegal instruction (core dumped)"" error caused by a CPU that does not support AVX instructions.
The Intel(R) Celeron(R) N4505 processor does not support AVX instructions, so you will not be able to use TensorFlow on this CPU without compiling it from the source. The earlier version doesn't require the AVX instructions but the newer version does.

The issue might be related to your environment.You can find the suitable Nvidia driver for your GPU from [here](https://www.nvidia.com/Download/index.aspx).

Thank you!

github-actions[bot] on (2024-09-14 01:57:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

google-ml-butler[bot] on (2024-09-16 03:26:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75175"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75175"">No</a>

"
2506578822,issue,open,,"Calibrator segfaults trying to log the ""while"" operation","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0

### 2. Code

[reproducer.zip](https://github.com/user-attachments/files/16894947/reproducer.zip)

### 3. Failure after conversion

Segmentation fault (signal 11) during conversion

### 5. (optional) Any other info / logs

The ""while"" operation does a check if an output tensor of the body subgraph is the same as the corresponding input tensor. If it's the case, it deallocates its own output tensor. The check is done at the prepare stage, so the affected tensor is already included in the ""loggable_outputs"" list by the calibrator. Then the calibrator tries to read the data from the deallocated tensor and segfaults. I've debugged it up to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L267 and found that the `tensor.data.f == nullptr`.
The check in question was introduced between 2.13 and 2.14, so it might be considered a regression: https://github.com/tensorflow/tensorflow/commit/7d49fd431ee5cebbb76eda88bc17e48921e10c85
",tagunil,2024-09-05 01:23:29+00:00,"['vamsimanchala', 'pkgoogle']",2024-10-02 20:11:13+00:00,,https://github.com/tensorflow/tensorflow/issues/75140,"[('awaiting review', 'Pull request awaiting review'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2331658774, 'issue_id': 2506578822, 'author': 'gaikwadrahul8', 'body': 'Hi, @tagunil \r\n\r\nThank you for bringing this issue to our attention and if possible could you please help us with Google colab notebook along with model to replicate the same behavior from our end to investigate this issue further from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 5, 13, 16, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331684785, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""Hi @gaikwadrahul8, sure. I just don't want to clutter the issue with irrelevant details, so I'll make a simple reproducer and share the link with you."", 'created_at': datetime.datetime(2024, 9, 5, 13, 27, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331794302, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""@gaikwadrahul8, I've attached a simple Jupyter notebook reproducing the crash to the issue.\r\nhttps://github.com/user-attachments/files/16894947/reproducer.zip"", 'created_at': datetime.datetime(2024, 9, 5, 14, 12, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332494012, 'issue_id': 2506578822, 'author': 'tagunil', 'body': 'BTW, I know that the ""while"" op is not quantizable as for now, but that\'s a separate issue. We might consider selective quantization, for example, but the segfault in the calibrator won\'t allow us doing that.', 'created_at': datetime.datetime(2024, 9, 5, 19, 35, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332632255, 'issue_id': 2506578822, 'author': 'gaikwadrahul8', 'body': ""Hi, @tagunil \r\n\r\nI apologize for the delayed response, I tried to run your jupyter notebook in [Google colab](https://colab.sandbox.google.com/gist/gaikwadrahul8/2f68bb21958cb5bafce2bf2458cfbb50/test-75140-issue.ipynb) with `CPU` and `GPU` runtime after this line `converter.convert()` the colab session is crashing for an unknown reason for reference I've added runtime log [here](https://pastebin.com/3vj8Ftdb) so did you face that issue while running your code in Google colab ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 5, 21, 7, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332643801, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""Hi @gaikwadrahul8,\n\nI'm running it locally, but it crashes all the same, and it's the crash itself that I'm reporting here, because I've traced it to the root cause and it's a null pointer dereference inside the calibrator."", 'created_at': datetime.datetime(2024, 9, 5, 21, 16, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333861210, 'issue_id': 2506578822, 'author': 'gaikwadrahul8', 'body': '@tagunil, If you have a workaround to address this issue we welcome a pull request. Our team will review it carefully and merge it if it aligns with our coding standards.\r\n\r\nHi, @pkgoogle\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 6, 11, 34, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334587296, 'issue_id': 2506578822, 'author': 'pkgoogle', 'body': ""Hi @tagunil, forgive me if I'm incorrect but isn't:\r\n\r\n```py\r\n    @tf.function\r\n    def call(self, inputs):\r\n        windows = tf.shape(inputs)[0]\r\n        height = tf.shape(inputs)[1]\r\n        width = tf.shape(inputs)[2]\r\n\r\n        sums = tf.zeros((windows, width), dtype=self.dtype)\r\n\r\n        for y in tf.range(height):\r\n            sums += tf.squeeze(tf.slice(inputs, [0, y, 0], [-1, 1, -1]), axis=1)\r\n\r\n        return sums\r\n```\r\n\r\nequivalent to\r\n\r\n```py\r\ntf.math.reduce_sum(inputs, axis=1, keep_dims=False)\r\n```\r\n\r\nIf so, can you use that to get past this issue?"", 'created_at': datetime.datetime(2024, 9, 6, 18, 15, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334680566, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""Hi @pkgoogle,\n\nIt's just a simplest reproducer for the crash I could think of. Of course, you are right in that particular case, but, unfortunately, our real model is much more complicated and cannot be implemented without a while op. We want to quantize it partially, but we still need to run the calibrator over the whole model for that."", 'created_at': datetime.datetime(2024, 9, 6, 19, 24, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339118455, 'issue_id': 2506578822, 'author': 'pkgoogle', 'body': 'I was able to replicate with your original code... [colab gist here](https://colab.sandbox.google.com/gist/pkgoogle/c44b4efb12998f7e542b620f2f838c06/tf_75140.ipynb) for convenience.\r\n\r\nI attempted to do a an AI-Edge-Torch conversion instead to see if it could solve this:\r\n\r\n```py\r\nimport ai_edge_torch\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F \r\n\r\nWINDOWS = 1\r\nHEIGHT = 32\r\nWIDTH = 32\r\n\r\n\r\nclass CustomModel(nn.Module):\r\n    def __init__(self, input_shape):\r\n        super().__init__()\r\n        self.windows = input_shape[0]\r\n        self.height = input_shape[1]\r\n        self.width = input_shape[2]\r\n        self.dense = nn.Linear(self.width, 1)\r\n\r\n    def forward(self, x):\r\n        accumulation = torch.zeros(self.windows, self.width)\r\n        for i in torch.arange(self.height):\r\n            accumulation += torch.squeeze(x[:,i,:])\r\n        x = accumulation\r\n        x = self.dense(x)\r\n        x = F.sigmoid(x)\r\n        return x\r\n\r\n\r\nmodel = CustomModel((WINDOWS, HEIGHT, WIDTH))\r\nsample_input = (torch.randn(WINDOWS, HEIGHT, WIDTH),)\r\n\r\nedge_model = ai_edge_torch.convert(model.eval(), sample_input)\r\nedge_model.export(""while.tflite"")\r\n```\r\n\r\nThis runs into a dynamic slicing issue with PyTorch Export, I\'m guessing because most people would do the reduce sum operation instead. @tagunil, I would take a second look to see if perhaps you can vectorize your code and avoid a while loop/range. @vamsimanchala, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 9, 9, 21, 24, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339189981, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""@pkgoogle, unfortunately, our use case is pretty specific and it would take a lot of effort to rework it in a vectorized way, even if possible. By the way, we've worked around the issue by adding a null pointer check to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L267. I would make a pull request already if the open source contribution policy in our company were not so uncertain, although we're trying to clarify it."", 'created_at': datetime.datetime(2024, 9, 9, 21, 32, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339195205, 'issue_id': 2506578822, 'author': 'tagunil', 'body': 'And yes, dynamic slicing is another pain point, but for now we work it around by using tf.slice directly instead of relying on Python slicing.', 'created_at': datetime.datetime(2024, 9, 9, 21, 34, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341652968, 'issue_id': 2506578822, 'author': 'pkgoogle', 'body': 'Hmm... a check is probably correct ... being optional doesn\'t mean in all cases it\'s not ""loggable"", but it does mean that that tensor sometimes is not allocated.\r\n\r\nEdit: Actually just below, optional tensors are not loggable: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L283 ... so root solution will be different ... @tagunil, do you have a stack trace? Feel free to censor/change PII.', 'created_at': datetime.datetime(2024, 9, 10, 18, 6, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341731876, 'issue_id': 2506578822, 'author': 'tagunil', 'body': 'Hi @pkgoogle,\n\nThe problem with the optional tensor logic in this particular case is that the list of loggable tensors is created before the while op decides to deallocate one of its tensors (the deallocation happens at the prepare stage). So either we need to create the loggable list later, just between prepare and eval, or we need to add an additional check before trying to actually log the tensor. The latter option works at least in our case.\n\nOf course, I can make a stack trace for you later today or tomorrow.', 'created_at': datetime.datetime(2024, 9, 10, 18, 31, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341873592, 'issue_id': 2506578822, 'author': 'tagunil', 'body': '@pkgoogle, so here\'s a trace from a debug build of TF v2.17.0, up to the Python wrapper. Let me know if you want me to capture anything else.\r\n```\r\nThread 1 ""python3"" received signal SIGSEGV, Segmentation fault.\r\n0x00007ffe55db86c9 in tflite::optimize::calibration::MinMax::Update (this=0x55555a146838, values=0x0, tensor_size=1024, error_reporter=0x55555a1476b0) at tensorflow/lite/tools/optimize/calibration/calibration_logger.cc:36\r\n36      tensorflow/lite/tools/optimize/calibration/calibration_logger.cc: No such file or directory.\r\n(gdb) bt\r\n#0  0x00007ffe55db86c9 in tflite::optimize::calibration::MinMax::Update (this=0x55555a146838, values=0x0,\r\n    tensor_size=1024, error_reporter=0x55555a1476b0)\r\n    at tensorflow/lite/tools/optimize/calibration/calibration_logger.cc:36\r\n#1  0x00007ffe55d06928 in tflite::optimize::calibration::Logger::LogTensorValue (this=0x55555ad07bf0,\r\n    subgraph_index=0, tensor_index=10, tensor_values=0x0, tensor_size=1024, error_reporter=0x55555a1476b0)\r\n    at ./tensorflow/lite/tools/optimize/calibration/calibration_logger.h:56\r\n#2  0x00007ffe55cfff61 in tflite::optimize::calibration::(anonymous namespace)::LoggingEval (context=0x55555ae0c578,\r\n    node=0x55555b28a1b0) at tensorflow/lite/tools/optimize/calibration/calibrator.cc:266\r\n#3  0x00007ffe55f96c4e in tflite::Subgraph::OpInvoke (this=0x55555ae0c550, op_reg=..., node=0x55555b28a1b0)\r\n    at tensorflow/lite/core/subgraph.cc:1428\r\n#4  0x00007ffe55f977fa in tflite::Subgraph::InvokeImpl (this=0x55555ae0c550) at tensorflow/lite/core/subgraph.cc:1724\r\n#5  0x00007ffe55f971a9 in tflite::Subgraph::Invoke (this=0x55555ae0c550) at tensorflow/lite/core/subgraph.cc:1617\r\n#6  0x00007ffe55e7a7c1 in tflite::impl::Interpreter::Invoke (this=0x55555b3f02c0)\r\n    at tensorflow/lite/core/interpreter.cc:242\r\n#7  0x00007ffe551dc182 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor (this=0x55555aa6b990,\r\n    input_value=0x7fff3812ae80) at tensorflow/lite/python/optimize/calibration_wrapper.cc:499\r\n#8  0x00007ffe551a0340 in pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module_&)::$_7::operator()(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&) const (this=0x55555b050578, self=...,\r\n    input_value=...) at tensorflow/lite/python/optimize/calibration_wrapper_pybind11.cc:77\r\n```', 'created_at': datetime.datetime(2024, 9, 10, 19, 39, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341921645, 'issue_id': 2506578822, 'author': 'pkgoogle', 'body': 'Thanks @tagunil, my current thinking is to remove the tensor from the loggable collection when we deallocate to keep the state consistent, that or reorder things (but that may cause further butterflies).', 'created_at': datetime.datetime(2024, 9, 10, 20, 9, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341986342, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""Well, @pkgoogle, you obviously know better. I'm just saying that a simple null pointer check before logging the tensor also seem to work."", 'created_at': datetime.datetime(2024, 9, 10, 20, 50, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388428908, 'issue_id': 2506578822, 'author': 'tagunil', 'body': ""Hi @pkgoogle, I propose to fix the segfault now (#76955) and deal with the calibration logic afterwards. As far as I can see, it's better to have no calibration values for an unused tensor than to crash the whole process."", 'created_at': datetime.datetime(2024, 10, 2, 11, 36, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389587626, 'issue_id': 2506578822, 'author': 'pkgoogle', 'body': 'Sounds good... @tagunil, perhaps you can add a TODO so that we don\'t forget. (That\'s my worry with ""covering"" this up) at any rate, let\'s wait for review.', 'created_at': datetime.datetime(2024, 10, 2, 20, 10, 25, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-05 13:16:09 UTC): Hi, @tagunil 

Thank you for bringing this issue to our attention and if possible could you please help us with Google colab notebook along with model to replicate the same behavior from our end to investigate this issue further from our end ? 

Thank you for your cooperation and patience.

tagunil (Issue Creator) on (2024-09-05 13:27:24 UTC): Hi @gaikwadrahul8, sure. I just don't want to clutter the issue with irrelevant details, so I'll make a simple reproducer and share the link with you.

tagunil (Issue Creator) on (2024-09-05 14:12:53 UTC): @gaikwadrahul8, I've attached a simple Jupyter notebook reproducing the crash to the issue.
https://github.com/user-attachments/files/16894947/reproducer.zip

tagunil (Issue Creator) on (2024-09-05 19:35:46 UTC): BTW, I know that the ""while"" op is not quantizable as for now, but that's a separate issue. We might consider selective quantization, for example, but the segfault in the calibrator won't allow us doing that.

gaikwadrahul8 on (2024-09-05 21:07:56 UTC): Hi, @tagunil 

I apologize for the delayed response, I tried to run your jupyter notebook in [Google colab](https://colab.sandbox.google.com/gist/gaikwadrahul8/2f68bb21958cb5bafce2bf2458cfbb50/test-75140-issue.ipynb) with `CPU` and `GPU` runtime after this line `converter.convert()` the colab session is crashing for an unknown reason for reference I've added runtime log [here](https://pastebin.com/3vj8Ftdb) so did you face that issue while running your code in Google colab ?

Thank you for your cooperation and patience.

tagunil (Issue Creator) on (2024-09-05 21:16:39 UTC): Hi @gaikwadrahul8,

I'm running it locally, but it crashes all the same, and it's the crash itself that I'm reporting here, because I've traced it to the root cause and it's a null pointer dereference inside the calibrator.

gaikwadrahul8 on (2024-09-06 11:34:45 UTC): @tagunil, If you have a workaround to address this issue we welcome a pull request. Our team will review it carefully and merge it if it aligns with our coding standards.

Hi, @pkgoogle
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-06 18:15:27 UTC): Hi @tagunil, forgive me if I'm incorrect but isn't:

```py
    @tf.function
    def call(self, inputs):
        windows = tf.shape(inputs)[0]
        height = tf.shape(inputs)[1]
        width = tf.shape(inputs)[2]

        sums = tf.zeros((windows, width), dtype=self.dtype)

        for y in tf.range(height):
            sums += tf.squeeze(tf.slice(inputs, [0, y, 0], [-1, 1, -1]), axis=1)

        return sums
```

equivalent to

```py
tf.math.reduce_sum(inputs, axis=1, keep_dims=False)
```

If so, can you use that to get past this issue?

tagunil (Issue Creator) on (2024-09-06 19:24:02 UTC): Hi @pkgoogle,

It's just a simplest reproducer for the crash I could think of. Of course, you are right in that particular case, but, unfortunately, our real model is much more complicated and cannot be implemented without a while op. We want to quantize it partially, but we still need to run the calibrator over the whole model for that.

pkgoogle (Assginee) on (2024-09-09 21:24:37 UTC): I was able to replicate with your original code... [colab gist here](https://colab.sandbox.google.com/gist/pkgoogle/c44b4efb12998f7e542b620f2f838c06/tf_75140.ipynb) for convenience.

I attempted to do a an AI-Edge-Torch conversion instead to see if it could solve this:

```py
import ai_edge_torch
import torch
import torch.nn as nn
import torch.nn.functional as F 

WINDOWS = 1
HEIGHT = 32
WIDTH = 32


class CustomModel(nn.Module):
    def __init__(self, input_shape):
        super().__init__()
        self.windows = input_shape[0]
        self.height = input_shape[1]
        self.width = input_shape[2]
        self.dense = nn.Linear(self.width, 1)

    def forward(self, x):
        accumulation = torch.zeros(self.windows, self.width)
        for i in torch.arange(self.height):
            accumulation += torch.squeeze(x[:,i,:])
        x = accumulation
        x = self.dense(x)
        x = F.sigmoid(x)
        return x


model = CustomModel((WINDOWS, HEIGHT, WIDTH))
sample_input = (torch.randn(WINDOWS, HEIGHT, WIDTH),)

edge_model = ai_edge_torch.convert(model.eval(), sample_input)
edge_model.export(""while.tflite"")
```

This runs into a dynamic slicing issue with PyTorch Export, I'm guessing because most people would do the reduce sum operation instead. @tagunil, I would take a second look to see if perhaps you can vectorize your code and avoid a while loop/range. @vamsimanchala, can you please take a look? Thanks.

tagunil (Issue Creator) on (2024-09-09 21:32:07 UTC): @pkgoogle, unfortunately, our use case is pretty specific and it would take a lot of effort to rework it in a vectorized way, even if possible. By the way, we've worked around the issue by adding a null pointer check to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L267. I would make a pull request already if the open source contribution policy in our company were not so uncertain, although we're trying to clarify it.

tagunil (Issue Creator) on (2024-09-09 21:34:51 UTC): And yes, dynamic slicing is another pain point, but for now we work it around by using tf.slice directly instead of relying on Python slicing.

pkgoogle (Assginee) on (2024-09-10 18:06:53 UTC): Hmm... a check is probably correct ... being optional doesn't mean in all cases it's not ""loggable"", but it does mean that that tensor sometimes is not allocated.

Edit: Actually just below, optional tensors are not loggable: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L283 ... so root solution will be different ... @tagunil, do you have a stack trace? Feel free to censor/change PII.

tagunil (Issue Creator) on (2024-09-10 18:31:50 UTC): Hi @pkgoogle,

The problem with the optional tensor logic in this particular case is that the list of loggable tensors is created before the while op decides to deallocate one of its tensors (the deallocation happens at the prepare stage). So either we need to create the loggable list later, just between prepare and eval, or we need to add an additional check before trying to actually log the tensor. The latter option works at least in our case.

Of course, I can make a stack trace for you later today or tomorrow.

tagunil (Issue Creator) on (2024-09-10 19:39:45 UTC): @pkgoogle, so here's a trace from a debug build of TF v2.17.0, up to the Python wrapper. Let me know if you want me to capture anything else.
```
Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.
0x00007ffe55db86c9 in tflite::optimize::calibration::MinMax::Update (this=0x55555a146838, values=0x0, tensor_size=1024, error_reporter=0x55555a1476b0) at tensorflow/lite/tools/optimize/calibration/calibration_logger.cc:36
36      tensorflow/lite/tools/optimize/calibration/calibration_logger.cc: No such file or directory.
(gdb) bt
#0  0x00007ffe55db86c9 in tflite::optimize::calibration::MinMax::Update (this=0x55555a146838, values=0x0,
    tensor_size=1024, error_reporter=0x55555a1476b0)
    at tensorflow/lite/tools/optimize/calibration/calibration_logger.cc:36
#1  0x00007ffe55d06928 in tflite::optimize::calibration::Logger::LogTensorValue (this=0x55555ad07bf0,
    subgraph_index=0, tensor_index=10, tensor_values=0x0, tensor_size=1024, error_reporter=0x55555a1476b0)
    at ./tensorflow/lite/tools/optimize/calibration/calibration_logger.h:56
#2  0x00007ffe55cfff61 in tflite::optimize::calibration::(anonymous namespace)::LoggingEval (context=0x55555ae0c578,
    node=0x55555b28a1b0) at tensorflow/lite/tools/optimize/calibration/calibrator.cc:266
#3  0x00007ffe55f96c4e in tflite::Subgraph::OpInvoke (this=0x55555ae0c550, op_reg=..., node=0x55555b28a1b0)
    at tensorflow/lite/core/subgraph.cc:1428
#4  0x00007ffe55f977fa in tflite::Subgraph::InvokeImpl (this=0x55555ae0c550) at tensorflow/lite/core/subgraph.cc:1724
#5  0x00007ffe55f971a9 in tflite::Subgraph::Invoke (this=0x55555ae0c550) at tensorflow/lite/core/subgraph.cc:1617
#6  0x00007ffe55e7a7c1 in tflite::impl::Interpreter::Invoke (this=0x55555b3f02c0)
    at tensorflow/lite/core/interpreter.cc:242
#7  0x00007ffe551dc182 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor (this=0x55555aa6b990,
    input_value=0x7fff3812ae80) at tensorflow/lite/python/optimize/calibration_wrapper.cc:499
#8  0x00007ffe551a0340 in pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module_&)::$_7::operator()(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&) const (this=0x55555b050578, self=...,
    input_value=...) at tensorflow/lite/python/optimize/calibration_wrapper_pybind11.cc:77
```

pkgoogle (Assginee) on (2024-09-10 20:09:05 UTC): Thanks @tagunil, my current thinking is to remove the tensor from the loggable collection when we deallocate to keep the state consistent, that or reorder things (but that may cause further butterflies).

tagunil (Issue Creator) on (2024-09-10 20:50:04 UTC): Well, @pkgoogle, you obviously know better. I'm just saying that a simple null pointer check before logging the tensor also seem to work.

tagunil (Issue Creator) on (2024-10-02 11:36:56 UTC): Hi @pkgoogle, I propose to fix the segfault now (#76955) and deal with the calibration logic afterwards. As far as I can see, it's better to have no calibration values for an unused tensor than to crash the whole process.

pkgoogle (Assginee) on (2024-10-02 20:10:25 UTC): Sounds good... @tagunil, perhaps you can add a TODO so that we don't forget. (That's my worry with ""covering"" this up) at any rate, let's wait for review.

"
2506007329,issue,closed,completed,Issues with DLL,"ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:
",Adedeji-hub,2024-09-04 18:18:00+00:00,['tilakrayal'],2024-09-21 01:58:34+00:00,2024-09-21 01:58:30+00:00,https://github.com/tensorflow/tensorflow/issues/75111,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2333345051, 'issue_id': 2506007329, 'author': 'tilakrayal', 'body': '@Adedeji-hub,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 6, 6, 34, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350779454, 'issue_id': 2506007329, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 14, 1, 57, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921038, 'issue_id': 2506007329, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921133, 'issue_id': 2506007329, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75111"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75111"">No</a>', 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 33, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-06 06:34:08 UTC): @Adedeji-hub,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-09-14 01:57:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-21 01:58:29 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-21 01:58:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75111"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75111"">No</a>

"
2505427652,issue,closed,completed,Inference performance questions,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.13.0-17-gf841394b1b7 2.13.1

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.3.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Server have 72 cpu cores. I'm unable to get more than 60% cpu utilization when using more than 16 threads via the --tensorflow_inter_op_parallelism parameter.
I'd like to use one tensorflow_model_server instance that would utilize all the HW cpu cores. Using tensorflow_inter_op_parallelism > 16, the utilization per thread drops as low as 30-40% utilization and the overall performance doesn't improve.

I'm using:
tensorflow_model_server --port=5051 --model_name=taboola_serving --model_base_path=/opt/taboola/models/cvr/ --tensorflow_intra_op_parallelism=32 --tensorflow_inter_op_parallelism=16

Also, attached is a flamegraph. I fail to understand why the ThreadPoolDevice::Compute part is only 38.5% of the overall time.
Is there some other perfromance parameter that I'm missing?

![perf](https://github.com/user-attachments/assets/220fb4d1-09de-4650-8121-0b21fe34d4e7)


### Standalone code to reproduce the issue

```shell
As described above
```


### Relevant log output

_No response_",eyalhir74,2024-09-04 13:45:38+00:00,['Venkat6871'],2024-09-26 02:01:15+00:00,2024-09-26 02:01:12+00:00,https://github.com/tensorflow/tensorflow/issues/75102,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2331081697, 'issue_id': 2505427652, 'author': 'Venkat6871', 'body': 'Hi **@eyalhir74** ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 5, 9, 47, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331257262, 'issue_id': 2505427652, 'author': 'eyalhir74', 'body': 'Thanks @Venkat6871 !\r\n\r\nI\'m just loading our model to tensorflow_model_server and have a client that does infer calls against it.\r\nSo not much code that I can supply.\r\n\r\nAttached are two screenshots that hopefully better demonstrate it.\r\nThe first image shows a top breakdown when the server was started with --tensorflow_inter_op_parallelism=8 - you can see 8 threads that reach ~92% utilization\r\n\r\nSecond image is when running with --tensorflow_inter_op_parallelism=16 and top shows 16 threads with 60% utilization.\r\n\r\nNow, since the server have 72 cpu cores, I\'d like to open one big tensorflow server running on all cpu cores, all reaching 90+% utilization. No matter how many clients/load I put on the server with tensorflow_inter_op_parallelism > 16, I can\'t reach full utilization and the HW just sits idle then.\r\n<img width=""627"" alt=""Screenshot 2024-09-05 at 14 08 27"" src=""https://github.com/user-attachments/assets/2f824dc1-1f31-40b3-b856-29d10594b9d9"">\r\n<img width=""636"" alt=""Screenshot 2024-09-05 at 14 05 22"" src=""https://github.com/user-attachments/assets/beaa3f0d-ff22-43f6-911f-8cb49db562df"">', 'created_at': datetime.datetime(2024, 9, 5, 11, 15, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339682976, 'issue_id': 2505427652, 'author': 'eyalhir74', 'body': '@Venkat6871 any idea what could be wrong here?', 'created_at': datetime.datetime(2024, 9, 10, 5, 54, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342819825, 'issue_id': 2505427652, 'author': 'Venkat6871', 'body': 'Hi **@eyalhir74** ,\r\nApologies for the delay, and thank you for the information. However, without the proper code, it is difficult for us to debug the issue. To expedite the troubleshooting process, we need a code snippet to reproduce the reported issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 11, 7, 0, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826225, 'issue_id': 2505427652, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 19, 1, 59, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375600661, 'issue_id': 2505427652, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 26, 2, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375600698, 'issue_id': 2505427652, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75102"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75102"">No</a>', 'created_at': datetime.datetime(2024, 9, 26, 2, 1, 14, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-05 09:47:07 UTC): Hi **@eyalhir74** ,
In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.
Thank you!

eyalhir74 (Issue Creator) on (2024-09-05 11:15:17 UTC): Thanks @Venkat6871 !

I'm just loading our model to tensorflow_model_server and have a client that does infer calls against it.
So not much code that I can supply.

Attached are two screenshots that hopefully better demonstrate it.
The first image shows a top breakdown when the server was started with --tensorflow_inter_op_parallelism=8 - you can see 8 threads that reach ~92% utilization

Second image is when running with --tensorflow_inter_op_parallelism=16 and top shows 16 threads with 60% utilization.

Now, since the server have 72 cpu cores, I'd like to open one big tensorflow server running on all cpu cores, all reaching 90+% utilization. No matter how many clients/load I put on the server with tensorflow_inter_op_parallelism > 16, I can't reach full utilization and the HW just sits idle then.
<img width=""627"" alt=""Screenshot 2024-09-05 at 14 08 27"" src=""https://github.com/user-attachments/assets/2f824dc1-1f31-40b3-b856-29d10594b9d9"">
<img width=""636"" alt=""Screenshot 2024-09-05 at 14 05 22"" src=""https://github.com/user-attachments/assets/beaa3f0d-ff22-43f6-911f-8cb49db562df"">

eyalhir74 (Issue Creator) on (2024-09-10 05:54:23 UTC): @Venkat6871 any idea what could be wrong here?

Venkat6871 (Assginee) on (2024-09-11 07:00:57 UTC): Hi **@eyalhir74** ,
Apologies for the delay, and thank you for the information. However, without the proper code, it is difficult for us to debug the issue. To expedite the troubleshooting process, we need a code snippet to reproduce the reported issue.
Thank you!

github-actions[bot] on (2024-09-19 01:59:51 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-26 02:01:12 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-26 02:01:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75102"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75102"">No</a>

"
2504494247,issue,closed,completed,error: required argument is not an integer,"# 1. System information

- Ubuntu 22.04 (L40 GPU)
- pip package
- Tensorflow 2.13.0, tflite-support 0.4.4

# 2. Code

## Input and Output shape

 Input: \[128,32,1\]

 Output: \[1\]

## Model Architecture and Training and saving
```
import tensorflow as tf

def get_model(
        input_shape,
        output_neurons=1,
        output_activation='sigmoid',
        loss=tf.keras.losses.binary_crossentropy,
        lr=0.0001
):
    _input = tf.keras.layers.Input(shape=input_shape)
    x = tf.keras.layers.Conv2D(512,kernel_size=3,padding='valid',activation='relu')(_input)
    x = tf.keras.layers.Conv2D(256,kernel_size=3,padding='valid',activation='relu')(x)
    x = tf.keras.layers.MaxPool2D((2,2))(x)
    x = tf.keras.layers.Conv2D(128,kernel_size=3,padding='valid',activation='relu')(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Conv2D(128,kernel_size=3,padding='valid',activation='relu')(x)
    x = tf.keras.layers.MaxPool2D((2,2))(x)
    x = tf.keras.layers.Conv2D(64,kernel_size=3,padding='valid',activation='relu')(x)
    x = tf.keras.layers.MaxPool2D((2,2))(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(1024,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    x = tf.keras.layers.Dense(1024,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Dense(1024,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.7)(x)
    x = tf.keras.layers.Dense(1024,activation='relu')(x)
    x = tf.keras.layers.Dense(10,activation='relu')(x)
    outputs = tf.keras.layers.Dense(output_neurons,activation=output_activation,kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)
    model = tf.keras.Model(inputs=_input,outputs=outputs)

    model.compile(
        loss=loss,
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        metrics=['accuracy'],
    )

    return model

model = get_model(
        input_shape=input_shape,
        lr=0.001
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',factor=0.1,patience=5,mode='max')
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=1,mode='max',restore_best_weights=True,start_from_epoch=10)
with tf.device('/gpu'):
    history = model.fit(train,epochs=3,validation_data=val,verbose=1,callbacks=[reduce_lr,early_stopping])

model.save('model.keras')

```

## Conversion to tf lite model

```
import tensorflow as tf

model_path = 'model.keras'
lite_model_path = model_path.replace('keras','tflite')

model = tf.keras.models.load_model(model_path)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open(lite_model_path, 'wb') as f:
    f.write(tflite_model)
```

## Adding metadata

```
actual_model = ""model.keras""
model_file = ""model.tflite""
export_model_file = ""model_meta.tflite""

from tflite_support import metadata_schema_py_generated as _metadata_fb
from tflite_support import metadata as _metadata
from tflite_support.metadata_writers import writer_utils
import flatbuffers

model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = ""Binary Classification Model""
model_meta.description = ""A CNN-based binary classification model for audio data.""
model_meta.version = ""v1""

input_meta = _metadata_fb.TensorMetadataT()
input_meta.name = ""Input Tensor""
input_meta.description = (
    ""Input to the model is a Mel spectrogram of audio, represented as an array of shape [1,128,32,1].""
)
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.AudioPropertiesT()
input_meta.content.contentProperties.sampleRate = 16000  # Update based on your actual sample rate
input_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.AudioProperties
input_meta.shape = [1, 128, 32, 1]
input_meta.dtype = 'float32'

output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = ""Output Tensor""
output_meta.description = ""Output is a float value between 0 and 1 representing the probability of the positive class.""
output_meta.content = _metadata_fb.ContentT()
output_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.FeatureProperties
output_meta.content.range = _metadata_fb.ValueRangeT()
output_meta.content.range.min = 0.0
output_meta.content.range.max = 1.0
output_meta.shape = [1]
output_meta.dtype = 'float32'

subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]

model_meta.subgraphMetadata = [subgraph]

builder = flatbuffers.Builder(0)
meta_offset = model_meta.Pack(builder)
builder.Finish(
    meta_offset,
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER
)
metadata_buf = builder.Output()

populator = _metadata.MetadataPopulator.with_model_file(model_file)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files([""labels.txt""])
populator.populate()

displayer = _metadata.MetadataDisplayer.with_model_file(export_model_file)
export_json_file = os.path.join(os.path.dirname(export_model_file), ""metadata.json"")
json_file = displayer.get_metadata_json()
with open(export_json_file, ""w"") as f:
    f.write(json_file)

```

# Description

I am developing a keyword spotting (binary classification) model that is later needed to be converted to tf lite and used in mobile device. 

For that, I built a model architecture (given above in code section). Trained is on mel spectrogram (generated using librosa). After training, I saved the model, converted to tf lite using the code mentioned above. 

Now I am at the point where i need to add metadata to the model. For that, I am refering the code given on official [documentation](https://ai.google.dev/edge/litert/models/metadata), with some necessory changes.

# Error Facing

I am facing the error at the time of adding meta data to the tflite model. When it comes to this point 
```
builder = flatbuffers.Builder(0)
meta_offset = model_meta.Pack(builder)
builder.Finish(
    meta_offset,
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER
)
metadata_buf = builder.Output()
```

It raises the error and exits. 

The error is 
```error: required argument is not an integer ```

I am unable to sort that error. Kindly assist me regarding that and provide me some solution for my problem.

",muhdaniyal252,2024-09-04 07:11:33+00:00,"['gaikwadrahul8', 'pkgoogle']",2024-10-10 02:01:34+00:00,2024-10-10 02:01:32+00:00,https://github.com/tensorflow/tensorflow/issues/75089,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2332547973, 'issue_id': 2504494247, 'author': 'gaikwadrahul8', 'body': ""Hi, @muhdaniyal252 \r\n\r\nThank you for bringing this issue to our attention and I see you've passed the `0 `value as argument while calling to `builder = flatbuffers.Builder(0)` that may be causing this issue as per documentation by default it sets to `initialSize=1024` which initializes a Builder of size 1024 bytes as `initialSize` and as per documentation string **`The internal buffer is grown as needed. flatbuffers: Cannot create Builder larger than 2 gigabytes.`**\r\n\r\nplease refer this source code [link](https://github.com/google/flatbuffers/blob/8db59321d9f02cdffa30126654059c7d02f70c32/python/flatbuffers/builder.py#L92) \r\n\r\nPlease set the initial size to a reasonable value based on your expected data size. Even if you're unsure of the exact size value like 1024 (1 KB) is often a good starting point. If your data size is unpredictable flatBuffers will automatically increase the buffer size as needed. \r\n\r\nPlease try with `initialSize=1024` bytes like `builder = flatbuffers.Builder(1024)` and see is it resolving your issue or not ? \r\n\r\nIf issue still persists please let us know with error log to investigate this issue further from our end. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 5, 20, 10, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333316431, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': 'Hello @gaikwadrahul8, your response is really appreciated.\r\n\r\nThe error does not occur when I initialize the flatbuffer. It occurs after that right in the next line.\r\n\r\n```meta_offset = model_meta.Pack(builder)```\r\n\r\nAlthough I did change the initialSize to 1024, but the issue is as it is.\r\n\r\nI am attaching the logs to backtrack the error as well.\r\n\r\n```\r\nerror                                     Traceback (most recent call last)\r\nCell In[8], [line 2]\r\n      [1] builder = flatbuffers.Builder(1024)\r\n----> [2] meta_offset = model_meta.Pack(builder)\r\n      [3] builder.Finish(\r\n      [4]     meta_offset,\r\n      [5]     _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER\r\n      [6] )\r\n      [7] metadata_buf = builder.Output()\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:3212, in ModelMetadataT.Pack(self, builder)\r\n   [3210] subgraphMetadatalist = []\r\n   [3211] for i in range(len(self.subgraphMetadata)):\r\n-> [3212]     subgraphMetadatalist.append(self.subgraphMetadata[i].Pack(builder))\r\n   [3213] ModelMetadataStartSubgraphMetadataVector(builder, len(self.subgraphMetadata))\r\n   [3214] for i in reversed(range(len(self.subgraphMetadata))):\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:2912, in SubGraphMetadataT.Pack(self, builder)\r\n   [2910] outputTensorMetadatalist = []\r\n   [2911] for i in range(len(self.outputTensorMetadata)):\r\n-> [2912]     outputTensorMetadatalist.append(self.outputTensorMetadata[i].Pack(builder))\r\n   [2913] SubGraphMetadataStartOutputTensorMetadataVector(builder, len(self.outputTensorMetadata))\r\n   [2914] for i in reversed(range(len(self.outputTensorMetadata))):\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:2325, in TensorMetadataT.Pack(self, builder)\r\n   [2323]     dimensionNames = builder.EndVector()\r\n   [2324] if self.content is not None:\r\n-> [2325]     content = self.content.Pack(builder)\r\n   [2326] if self.processUnits is not None:\r\n   [2327]     processUnitslist = []\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:925, in ContentT.Pack(self, builder)\r\n    [923]     contentProperties = self.contentProperties.Pack(builder)\r\n    [924] if self.range is not None:\r\n--> [925]     range = self.range.Pack(builder)\r\n    [926] ContentStart(builder)\r\n    [927] ContentAddContentPropertiesType(builder, self.contentPropertiesType)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:811, in ValueRangeT.Pack(self, builder)\r\n    [809] ValueRangeStart(builder)\r\n    [810] ValueRangeAddMin(builder, self.min)\r\n--> [811] ValueRangeAddMax(builder, self.max)\r\n    [812] valueRange = ValueRangeEnd(builder)\r\n    [813] return valueRange\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:769, in ValueRangeAddMax(builder, max)\r\n    [768] def ValueRangeAddMax(builder, max):\r\n--> [769]     builder.PrependInt32Slot(1, max, 0)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:621, in Builder.PrependInt32Slot(self, *args)\r\n--> [621] def PrependInt32Slot(self, *args): self.PrependSlot(N.Int32Flags, *args)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:602, in Builder.PrependSlot(self, flags, o, x, d)\r\n    [600]     N.enforce_number(d, flags)\r\n    [601] if x != d or (self.forceDefaults and d is not None):\r\n--> [602]     self.Prepend(flags, x)\r\n    [603]     self.Slot(o)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:594, in Builder.Prepend(self, flags, off)\r\n    [592] def Prepend(self, flags, off):\r\n    [593]     self.Prep(flags.bytewidth, 0)\r\n--> [594]     self.Place(off, flags)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:762, in Builder.Place(self, x, flags)\r\n    [760] N.enforce_number(x, flags)\r\n    [761] self.head = self.head - flags.bytewidth\r\n--> [762] encode.Write(flags.packer_type, self.Bytes, self.Head(), x)\r\n\r\nFile /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/encode.py:42, in Write(packer_type, buf, head, n)\r\n     [40] def Write(packer_type, buf, head, n):\r\n     [41]     """""" Write encodes `n` at buf[head] using `packer_type`. """"""\r\n---> [42]     packer_type.pack_into(buf, head, n)\r\n\r\nerror: required argument is not an integer\r\n```', 'created_at': datetime.datetime(2024, 9, 6, 6, 10, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333783482, 'issue_id': 2504494247, 'author': 'gaikwadrahul8', 'body': 'Hi, @muhdaniyal252\r\n\r\nThank you for trying, if possible could you please help us with Google colab notebook along with dataset to replicate the same behavior from our end to investigate this issue further ? Thank you.', 'created_at': datetime.datetime(2024, 9, 6, 10, 44, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334858660, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': 'Hi, @gaikwadrahul8 \r\n\r\nhttps://colab.research.google.com/drive/1daZ95UcBm2UQbRLh-d76ySDctTSd8Ydi#scrollTo=o2-hvIyBhmkk\r\n\r\nThis is the link of the colab notebook. \r\n\r\nThe only difference between this and my working environment is that I am using actual audios for Keyword spotting and in this case, I have created dummy data. \r\n\r\nThe error is same here as well.', 'created_at': datetime.datetime(2024, 9, 6, 21, 52, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339625719, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': 'Hey @gaikwadrahul8,\r\n\r\nI were wondering if there is any update regarding the issue I am facing,', 'created_at': datetime.datetime(2024, 9, 10, 5, 4, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343172857, 'issue_id': 2504494247, 'author': 'gaikwadrahul8', 'body': ""Hi, @muhdaniyal252\r\n\r\nI apologize for the delayed response, thank you for providing the Google colab notebook and I am able to replicate the same behavior from our end so we'll have to dig more into this issue. \r\n\r\nHere is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/6e77d8830d50d89f0ea1a309d34c8b94/test-issue-75089.ipynb) for reference\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 11, 9, 51, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346281480, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': ""Hello @gaikwadrahul8 . \r\nThank you for the update.\r\n\r\nAnd since you are already on thing thing. I were wondering if you  could solve thing thing for latest versions of tensorflow as well? Upon looking at the last update of 'tflite-support' library, it was back in July 2013. at that time, tensorflow 2.13 was the latest version. But since then, many versions of Tensorflow has been out in public but no update on tflite-support end. \r\n\r\nMy suggestion is to either remove dependency of 3rd party package and add metadata directly at the time of conversion from main model to tflite model. or make make tensorflow compatible with older versions of tflite-support as well.\r\n\r\nThankyou in advance for taking this into consideration."", 'created_at': datetime.datetime(2024, 9, 12, 13, 26, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357460524, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': 'Hello @gaikwadrahul8,\r\n\r\nI was wondering if there is any update regarding the issue.', 'created_at': datetime.datetime(2024, 9, 18, 4, 17, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360993558, 'issue_id': 2504494247, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 19, 13, 29, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362237681, 'issue_id': 2504494247, 'author': 'pkgoogle', 'body': 'Hi @muhdaniyal252, tfllite_support is a little out-dated, do you absolutely need to add metadata to your model? Or do you just care about inferencing on your mobile or edge device?', 'created_at': datetime.datetime(2024, 9, 19, 21, 31, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362952962, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': ""Hi @pkgoogle. \r\nI don't really care about metadata. I just need to run the model on the edge device. \r\nBut it happens to raise error without the metadata, according to the android dev.\r\nDo let me know if it is possible to run the model on mobile (both android and iOS) without adding metadata.\r\n\r\n++ loop in: @farhajkhan88"", 'created_at': datetime.datetime(2024, 9, 20, 6, 46, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364253984, 'issue_id': 2504494247, 'author': 'pkgoogle', 'body': ""Hi @muhdaniyal252, it seems like you already have a .tflite model? Where does it mention that an error will be raised? (it may not be incorrect but I need to understand the entire context better). You should be able to run a model on mobile w/o this step -- you may wish to look at some of our examples at LiteRT: https://github.com/google-ai-edge/litert-samples/tree/main/examples but let us know if you get stuck or an error is raised. It's possible this is a prior requirement and a new version of TF/Keras/TFLite will resolve this. Is there a reason you aren't on a more recent version?"", 'created_at': datetime.datetime(2024, 9, 20, 18, 1, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370016293, 'issue_id': 2504494247, 'author': 'muhdaniyal252', 'body': ""Hi @pkgoogle, apologies for delayed response\r\nOne of the devs of android (who has done this before) told me to add metadata to the model when I provided the one without metadata. He mentioned that the error he is facing on the current model (the one without metadata) as also been previously observed and that was resolved by adding metadata to it. \r\nUpon investigating, I found that the tflite-support is a little back on time, which is obviously the reason of incompatibility with latest tensorflow version. \r\nTo overcome the version compatibility conflict, I downgraded to 2.13, the latest version at the time of tflite-support last release.\r\nThat's the reason I am using the old version of Tensorflow. \r\n\r\nBut as you said, I wouldn't need to add metadata to the model, I'll share the link with the dev and update on you very in a short while. \r\n\r\nThankyou for your patience."", 'created_at': datetime.datetime(2024, 9, 24, 3, 2, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372400080, 'issue_id': 2504494247, 'author': 'pkgoogle', 'body': 'No worries, thanks for your cooperation. I would say try your best to update everything to the most up to date versions for now as everything is changing quite rapidly.', 'created_at': datetime.datetime(2024, 9, 24, 21, 15, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387488667, 'issue_id': 2504494247, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 2, 2, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748663, 'issue_id': 2504494247, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403748701, 'issue_id': 2504494247, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75089"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75089"">No</a>', 'created_at': datetime.datetime(2024, 10, 10, 2, 1, 33, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-05 20:10:43 UTC): Hi, @muhdaniyal252 

Thank you for bringing this issue to our attention and I see you've passed the `0 `value as argument while calling to `builder = flatbuffers.Builder(0)` that may be causing this issue as per documentation by default it sets to `initialSize=1024` which initializes a Builder of size 1024 bytes as `initialSize` and as per documentation string **`The internal buffer is grown as needed. flatbuffers: Cannot create Builder larger than 2 gigabytes.`**

please refer this source code [link](https://github.com/google/flatbuffers/blob/8db59321d9f02cdffa30126654059c7d02f70c32/python/flatbuffers/builder.py#L92) 

Please set the initial size to a reasonable value based on your expected data size. Even if you're unsure of the exact size value like 1024 (1 KB) is often a good starting point. If your data size is unpredictable flatBuffers will automatically increase the buffer size as needed. 

Please try with `initialSize=1024` bytes like `builder = flatbuffers.Builder(1024)` and see is it resolving your issue or not ? 

If issue still persists please let us know with error log to investigate this issue further from our end. 

Thank you for your cooperation and patience.

muhdaniyal252 (Issue Creator) on (2024-09-06 06:10:33 UTC): Hello @gaikwadrahul8, your response is really appreciated.

The error does not occur when I initialize the flatbuffer. It occurs after that right in the next line.

```meta_offset = model_meta.Pack(builder)```

Although I did change the initialSize to 1024, but the issue is as it is.

I am attaching the logs to backtrack the error as well.

```
error                                     Traceback (most recent call last)
Cell In[8], [line 2]
      [1] builder = flatbuffers.Builder(1024)
----> [2] meta_offset = model_meta.Pack(builder)
      [3] builder.Finish(
      [4]     meta_offset,
      [5]     _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER
      [6] )
      [7] metadata_buf = builder.Output()

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:3212, in ModelMetadataT.Pack(self, builder)
   [3210] subgraphMetadatalist = []
   [3211] for i in range(len(self.subgraphMetadata)):
-> [3212]     subgraphMetadatalist.append(self.subgraphMetadata[i].Pack(builder))
   [3213] ModelMetadataStartSubgraphMetadataVector(builder, len(self.subgraphMetadata))
   [3214] for i in reversed(range(len(self.subgraphMetadata))):

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:2912, in SubGraphMetadataT.Pack(self, builder)
   [2910] outputTensorMetadatalist = []
   [2911] for i in range(len(self.outputTensorMetadata)):
-> [2912]     outputTensorMetadatalist.append(self.outputTensorMetadata[i].Pack(builder))
   [2913] SubGraphMetadataStartOutputTensorMetadataVector(builder, len(self.outputTensorMetadata))
   [2914] for i in reversed(range(len(self.outputTensorMetadata))):

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:2325, in TensorMetadataT.Pack(self, builder)
   [2323]     dimensionNames = builder.EndVector()
   [2324] if self.content is not None:
-> [2325]     content = self.content.Pack(builder)
   [2326] if self.processUnits is not None:
   [2327]     processUnitslist = []

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:925, in ContentT.Pack(self, builder)
    [923]     contentProperties = self.contentProperties.Pack(builder)
    [924] if self.range is not None:
--> [925]     range = self.range.Pack(builder)
    [926] ContentStart(builder)
    [927] ContentAddContentPropertiesType(builder, self.contentPropertiesType)

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:811, in ValueRangeT.Pack(self, builder)
    [809] ValueRangeStart(builder)
    [810] ValueRangeAddMin(builder, self.min)
--> [811] ValueRangeAddMax(builder, self.max)
    [812] valueRange = ValueRangeEnd(builder)
    [813] return valueRange

File /shareddrive/working/env/lib/python3.10/site-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py:769, in ValueRangeAddMax(builder, max)
    [768] def ValueRangeAddMax(builder, max):
--> [769]     builder.PrependInt32Slot(1, max, 0)

File /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:621, in Builder.PrependInt32Slot(self, *args)
--> [621] def PrependInt32Slot(self, *args): self.PrependSlot(N.Int32Flags, *args)

File /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:602, in Builder.PrependSlot(self, flags, o, x, d)
    [600]     N.enforce_number(d, flags)
    [601] if x != d or (self.forceDefaults and d is not None):
--> [602]     self.Prepend(flags, x)
    [603]     self.Slot(o)

File /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:594, in Builder.Prepend(self, flags, off)
    [592] def Prepend(self, flags, off):
    [593]     self.Prep(flags.bytewidth, 0)
--> [594]     self.Place(off, flags)

File /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/builder.py:762, in Builder.Place(self, x, flags)
    [760] N.enforce_number(x, flags)
    [761] self.head = self.head - flags.bytewidth
--> [762] encode.Write(flags.packer_type, self.Bytes, self.Head(), x)

File /shareddrive/working/env/lib/python3.10/site-packages/flatbuffers/encode.py:42, in Write(packer_type, buf, head, n)
     [40] def Write(packer_type, buf, head, n):
     [41]     """""" Write encodes `n` at buf[head] using `packer_type`. """"""
---> [42]     packer_type.pack_into(buf, head, n)

error: required argument is not an integer
```

gaikwadrahul8 (Assginee) on (2024-09-06 10:44:21 UTC): Hi, @muhdaniyal252

Thank you for trying, if possible could you please help us with Google colab notebook along with dataset to replicate the same behavior from our end to investigate this issue further ? Thank you.

muhdaniyal252 (Issue Creator) on (2024-09-06 21:52:27 UTC): Hi, @gaikwadrahul8 

https://colab.research.google.com/drive/1daZ95UcBm2UQbRLh-d76ySDctTSd8Ydi#scrollTo=o2-hvIyBhmkk

This is the link of the colab notebook. 

The only difference between this and my working environment is that I am using actual audios for Keyword spotting and in this case, I have created dummy data. 

The error is same here as well.

muhdaniyal252 (Issue Creator) on (2024-09-10 05:04:34 UTC): Hey @gaikwadrahul8,

I were wondering if there is any update regarding the issue I am facing,

gaikwadrahul8 (Assginee) on (2024-09-11 09:51:10 UTC): Hi, @muhdaniyal252

I apologize for the delayed response, thank you for providing the Google colab notebook and I am able to replicate the same behavior from our end so we'll have to dig more into this issue. 

Here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/6e77d8830d50d89f0ea1a309d34c8b94/test-issue-75089.ipynb) for reference

Thank you for your cooperation and patience.

muhdaniyal252 (Issue Creator) on (2024-09-12 13:26:13 UTC): Hello @gaikwadrahul8 . 
Thank you for the update.

And since you are already on thing thing. I were wondering if you  could solve thing thing for latest versions of tensorflow as well? Upon looking at the last update of 'tflite-support' library, it was back in July 2013. at that time, tensorflow 2.13 was the latest version. But since then, many versions of Tensorflow has been out in public but no update on tflite-support end. 

My suggestion is to either remove dependency of 3rd party package and add metadata directly at the time of conversion from main model to tflite model. or make make tensorflow compatible with older versions of tflite-support as well.

Thankyou in advance for taking this into consideration.

muhdaniyal252 (Issue Creator) on (2024-09-18 04:17:01 UTC): Hello @gaikwadrahul8,

I was wondering if there is any update regarding the issue.

gaikwadrahul8 (Assginee) on (2024-09-19 13:29:37 UTC): Hi, @pkgoogle

Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-19 21:31:29 UTC): Hi @muhdaniyal252, tfllite_support is a little out-dated, do you absolutely need to add metadata to your model? Or do you just care about inferencing on your mobile or edge device?

muhdaniyal252 (Issue Creator) on (2024-09-20 06:46:38 UTC): Hi @pkgoogle. 
I don't really care about metadata. I just need to run the model on the edge device. 
But it happens to raise error without the metadata, according to the android dev.
Do let me know if it is possible to run the model on mobile (both android and iOS) without adding metadata.

++ loop in: @farhajkhan88

pkgoogle (Assginee) on (2024-09-20 18:01:59 UTC): Hi @muhdaniyal252, it seems like you already have a .tflite model? Where does it mention that an error will be raised? (it may not be incorrect but I need to understand the entire context better). You should be able to run a model on mobile w/o this step -- you may wish to look at some of our examples at LiteRT: https://github.com/google-ai-edge/litert-samples/tree/main/examples but let us know if you get stuck or an error is raised. It's possible this is a prior requirement and a new version of TF/Keras/TFLite will resolve this. Is there a reason you aren't on a more recent version?

muhdaniyal252 (Issue Creator) on (2024-09-24 03:02:42 UTC): Hi @pkgoogle, apologies for delayed response
One of the devs of android (who has done this before) told me to add metadata to the model when I provided the one without metadata. He mentioned that the error he is facing on the current model (the one without metadata) as also been previously observed and that was resolved by adding metadata to it. 
Upon investigating, I found that the tflite-support is a little back on time, which is obviously the reason of incompatibility with latest tensorflow version. 
To overcome the version compatibility conflict, I downgraded to 2.13, the latest version at the time of tflite-support last release.
That's the reason I am using the old version of Tensorflow. 

But as you said, I wouldn't need to add metadata to the model, I'll share the link with the dev and update on you very in a short while. 

Thankyou for your patience.

pkgoogle (Assginee) on (2024-09-24 21:15:42 UTC): No worries, thanks for your cooperation. I would say try your best to update everything to the most up to date versions for now as everything is changing quite rapidly.

github-actions[bot] on (2024-10-02 02:01:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-10 02:01:31 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-10 02:01:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75089"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75089"">No</a>

"
2502845355,issue,closed,completed,None gradients when using tf gradient tape,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

binary

### GCC/compiler version

binary

### CUDA/cuDNN version

12.3 (pip install tensorflow[and-cuda)==2.17)

### GPU model and memory

RTX A4500 laptop, 16 gigs

### Current behavior?

I tried to load a trained model using tensorflow and keras in order to compute some saliency maps. To do so, I need to compute the gradients with respect to the inputs. However, it returns None gradients whereas it worked fine before, e.g., with tf 2.10.
A reprex with the model weights, a datafile and a minimal version of the code can be find using the following google drive link:

### Standalone code to reproduce the issue

```shell
https://drive.google.com/file/d/1fcind0XWxdghlKvN237y0L177JbEgOcV/view?usp=sharing
```


### Relevant log output

_No response_",Senantq,2024-09-03 13:09:55+00:00,['Venkat6871'],2024-09-19 01:59:57+00:00,2024-09-19 01:59:54+00:00,https://github.com/tensorflow/tensorflow/issues/75032,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2328642329, 'issue_id': 2502845355, 'author': 'Venkat6871', 'body': 'Hi **@Senantq** ,\r\nI tried to run your code on Colab using TF v2.17, but it is asking for a directory. Could you please provide the path related to your issue?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 4, 11, 19, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102031, 'issue_id': 2502845355, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826281, 'issue_id': 2502845355, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 1, 59, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826335, 'issue_id': 2502845355, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75032"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75032"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 1, 59, 56, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-04 11:19:49 UTC): Hi **@Senantq** ,
I tried to run your code on Colab using TF v2.17, but it is asking for a directory. Could you please provide the path related to your issue?
Thank you!

github-actions[bot] on (2024-09-12 01:58:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 01:59:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 01:59:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75032"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75032"">No</a>

"
2502687446,issue,closed,completed,Memory footprint  in cpu and opencl,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi

      In Android ,  using benchmark tool test memory footprint in cpu and opencl ,  
logs  show memory footprint is very different: 
Memory footprint delta from the start of the tool (MB): init=224.859 overall=260.609     ----------gpu  opencl
Memory footprint delta from the start of the tool (MB): init=25.0977 overall=757.574    -----------cpu
      We want to know
     1. if the opencl memory footprint only counts malloc heap memory? Or does it also include gpu buffers, and if so, how are they calculate；
      2. And Why is the memory situation on the cpu so different from that on the opencl， should we take init memory footprint into consideration or overall？

thanks

### Standalone code to reproduce the issue

```shell
using benchmark tool command like

./benchmark_model --graph=final.tflite --num_threads=4 --num_runs=5   
./benchmark_model --graph=final.tflite --num_threads=4 --num_runs=5  --use_gpu=true
```


### Relevant log output

_No response_",keke444,2024-09-03 11:58:43+00:00,['pkgoogle'],2024-11-13 02:01:23+00:00,2024-11-13 02:01:20+00:00,https://github.com/tensorflow/tensorflow/issues/75029,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2328317678, 'issue_id': 2502687446, 'author': 'tilakrayal', 'body': '@keke444,\r\nCould you please provide the context/info regarding the issue or the reproducible code which helps to debug the issue in an effective manner. Thank you!', 'created_at': datetime.datetime(2024, 9, 4, 9, 6, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328352276, 'issue_id': 2502687446, 'author': 'keke444', 'body': 'We just want to calculate the memory footprint of our tflite model on cpu and opencl,  \r\nusing bazel-bin/tensorflow/lite/tools/benchmark_model  --graph=final.tflite --num_threads=4 --num_runs=5 \r\nNo other code was used', 'created_at': datetime.datetime(2024, 9, 4, 9, 21, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337404707, 'issue_id': 2502687446, 'author': 'misterBart', 'body': 'Possibly related to https://github.com/tensorflow/tensorflow/issues/74521#issue-2486869907\r\nThere I also measure a large memory footprint with OpenCL and a small footprint with the CPU (Xnnpack).\r\n@keke444 What GPU do you use?', 'created_at': datetime.datetime(2024, 9, 9, 8, 2, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383937621, 'issue_id': 2502687446, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 30, 18, 56, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386628631, 'issue_id': 2502687446, 'author': 'pkgoogle', 'body': 'Hi @keke444, can you please share `final.tflite` or perhaps a similar/reduced model which still shows this result?', 'created_at': datetime.datetime(2024, 10, 1, 18, 1, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123443, 'issue_id': 2502687446, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456067406, 'issue_id': 2502687446, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 5, 2, 0, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472174170, 'issue_id': 2502687446, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472174312, 'issue_id': 2502687446, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75029"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75029"">No</a>', 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 23, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-09-04 09:06:29 UTC): @keke444,
Could you please provide the context/info regarding the issue or the reproducible code which helps to debug the issue in an effective manner. Thank you!

keke444 (Issue Creator) on (2024-09-04 09:21:59 UTC): We just want to calculate the memory footprint of our tflite model on cpu and opencl,  
using bazel-bin/tensorflow/lite/tools/benchmark_model  --graph=final.tflite --num_threads=4 --num_runs=5 
No other code was used

misterBart on (2024-09-09 08:02:02 UTC): Possibly related to https://github.com/tensorflow/tensorflow/issues/74521#issue-2486869907
There I also measure a large memory footprint with OpenCL and a small footprint with the CPU (Xnnpack).
@keke444 What GPU do you use?

gaikwadrahul8 on (2024-09-30 18:56:51 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-01 18:01:26 UTC): Hi @keke444, can you please share `final.tflite` or perhaps a similar/reduced model which still shows this result?

github-actions[bot] on (2024-10-09 02:01:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-05 02:00:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-13 02:01:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-13 02:01:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75029"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75029"">No</a>

"
2502146161,issue,closed,completed,Disk cache not caching entire dataset - weird behaviour,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (in Docker)

### Mobile device

_No response_

### Python version

Python 3.11.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

RTX3080

### Current behavior?

When using `tf.data` pipeline, caching to disk produces warnings that the cache will be discarded because the iterator didn't see the whole dataset.

When using in-memory cache, the issue is not there.

### Standalone code to reproduce the issue

```py
# test 1 - works
for s in tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).cache():
    print(s)

# test 2 - warning thrown
for s in tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).cache(""test_disk_cache""):
    print(s)
```


### Relevant log output

```shell
# test 1 - output

tf.Tensor(1, shape=(), dtype=int32)                                                                                                                                                                                                             
tf.Tensor(2, shape=(), dtype=int32)                                                                                                                                                                                                             
tf.Tensor(3, shape=(), dtype=int32)                                                                                                                                                                                                             
tf.Tensor(4, shape=(), dtype=int32)                                                                                                                                                                                                             
tf.Tensor(5, shape=(), dtype=int32)                                                                                                                                                                                                             
2024-09-03 07:32:36.163152: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence

# test 2 - issue

tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
tf.Tensor(5, shape=(), dtype=int32)
2024-09-03 07:32:59.122496: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
2024-09-03 07:32:59.122628: W tensorflow/core/kernels/data/cache_dataset_ops.cc:332] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```
",nmilosev,2024-09-03 07:38:33+00:00,['Venkat6871'],2024-10-15 02:02:49+00:00,2024-10-15 02:02:47+00:00,https://github.com/tensorflow/tensorflow/issues/75002,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2333512659, 'issue_id': 2502146161, 'author': 'Venkat6871', 'body': 'Hi **@nmilosev** ,\r\nSorry for the delay. I tried to run your code on Colab using TF 2.17.0 and the nightly versions, and I did not encounter any issues. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/bd7f53c2cbd8ef1601f1245e6342a011/75002_tf_2-17-0-nightly-v.ipynb) here for reference. Correct me if I am wrong.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 6, 8, 21, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342854342, 'issue_id': 2502146161, 'author': 'nmilosev', 'body': 'Hi @Venkat6871 thanks for your help with this. Sorry for the delay on my side as well.\r\n\r\nI tried it in Colab and like you, no messages are printed. However I am not sure if `stderr` is shown on Colab.\r\n\r\n\r\nWhen running in Docker the image `tensorflow/tensorflow:2.17.0-gpu` I can replicate this behavior 100% of the time.\r\n\r\nHere is the log attached. [tf2_cache_log.txt](https://github.com/user-attachments/files/16959495/tf2_cache_log.txt)\r\n\r\nAnd a screenshot:\r\n\r\n![image](https://github.com/user-attachments/assets/2d5ec164-60b3-4f55-baa3-ef9b12d8613d)\r\n\r\nLet me know if there is something I should try to change or to test.\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 9, 11, 7, 20, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382441553, 'issue_id': 2502146161, 'author': 'Venkat6871', 'body': 'Hi **@nmilosev** ,\r\nSorry for the delay. The same issue is being tracked in another issue. Here I am adding that [issue](https://github.com/tensorflow/tensorflow/issues/56904) for your reference; please go through it once. I hope it will be useful for you.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 8, 26, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491921, 'issue_id': 2502146161, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 2, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662429, 'issue_id': 2502146161, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662459, 'issue_id': 2502146161, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75002"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75002"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 48, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-06 08:21:40 UTC): Hi **@nmilosev** ,
Sorry for the delay. I tried to run your code on Colab using TF 2.17.0 and the nightly versions, and I did not encounter any issues. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/bd7f53c2cbd8ef1601f1245e6342a011/75002_tf_2-17-0-nightly-v.ipynb) here for reference. Correct me if I am wrong.
Thank you!

nmilosev (Issue Creator) on (2024-09-11 07:20:07 UTC): Hi @Venkat6871 thanks for your help with this. Sorry for the delay on my side as well.

I tried it in Colab and like you, no messages are printed. However I am not sure if `stderr` is shown on Colab.


When running in Docker the image `tensorflow/tensorflow:2.17.0-gpu` I can replicate this behavior 100% of the time.

Here is the log attached. [tf2_cache_log.txt](https://github.com/user-attachments/files/16959495/tf2_cache_log.txt)

And a screenshot:

![image](https://github.com/user-attachments/assets/2d5ec164-60b3-4f55-baa3-ef9b12d8613d)

Let me know if there is something I should try to change or to test.

Thanks!

Venkat6871 (Assginee) on (2024-09-30 08:26:53 UTC): Hi **@nmilosev** ,
Sorry for the delay. The same issue is being tracked in another issue. Here I am adding that [issue](https://github.com/tensorflow/tensorflow/issues/56904) for your reference; please go through it once. I hope it will be useful for you.
Thank you!

github-actions[bot] on (2024-10-08 02:02:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75002"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75002"">No</a>

"
2502106009,issue,closed,completed,Tensorflow model loading error,"i am using below version to train my model on linux machine x86_64 and i successfully trained it.
Python version: 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35)
[GCC 12.3.0]
Tensorflow version: 2.7.0
Keras version: 2.7.0

After succesfully training , the inferencing is also happening using the same model  which i saved as tensorflow saved model.

Now i want to deploy it on Jetson nano, which have below versions
python version 3.6.15
tensorflow version 2.7.0
keras version 2.7.0

The problem is, i am not able to load my model in jetson nano.
The error i am encountering is in load file.
that (path).meta_graphs[0]
The list index(0) is out of the range
Below is line of codde causing error
tf.keras.models.load_model(‘model2’)",shikharRS,2024-09-03 07:16:14+00:00,['tilakrayal'],2024-09-04 07:23:27+00:00,2024-09-04 07:23:25+00:00,https://github.com/tensorflow/tensorflow/issues/75000,"[('type:support', 'Support issues'), ('TF 2.7', 'Issues related to TF 2.7.0')]","[{'comment_id': 2326901205, 'issue_id': 2502106009, 'author': 'tilakrayal', 'body': '@shikharRS,\r\nCould you please provide the reproducible code or the colab gist and the error log which you are facing. Also could you please try to upgrade the latest tensorflow v2.17, also you are using the tensorflow v2.7 which is pretty older and support is very limited. Thank you!', 'created_at': datetime.datetime(2024, 9, 3, 16, 6, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328104356, 'issue_id': 2502106009, 'author': 'shikharRS', 'body': '> @shikharRS, Could you please provide the reproducible code or the colab gist and the error log which you are facing. Also could you please try to upgrade the latest tensorflow v2.17, also you are using the tensorflow v2.7 which is pretty older and support is very limited. Thank you!\r\n\r\nHi tilakrayal,\r\n\r\ni fixed that issue model is getting loaded. Thanks', 'created_at': datetime.datetime(2024, 9, 4, 7, 20, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328108767, 'issue_id': 2502106009, 'author': 'shikharRS', 'body': 'As i am using jetson nano whose GPU is compatible with  only tf 2.7 having python 3.6 . Anyway i can update it to 2.17 or 2.15 but that will not be able to use GPU computation of jetson nano.\r\nThanks', 'created_at': datetime.datetime(2024, 9, 4, 7, 23, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328108808, 'issue_id': 2502106009, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75000"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75000"">No</a>', 'created_at': datetime.datetime(2024, 9, 4, 7, 23, 26, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-03 16:06:10 UTC): @shikharRS,
Could you please provide the reproducible code or the colab gist and the error log which you are facing. Also could you please try to upgrade the latest tensorflow v2.17, also you are using the tensorflow v2.7 which is pretty older and support is very limited. Thank you!

shikharRS (Issue Creator) on (2024-09-04 07:20:49 UTC): Hi tilakrayal,

i fixed that issue model is getting loaded. Thanks

shikharRS (Issue Creator) on (2024-09-04 07:23:25 UTC): As i am using jetson nano whose GPU is compatible with  only tf 2.7 having python 3.6 . Anyway i can update it to 2.17 or 2.15 but that will not be able to use GPU computation of jetson nano.
Thanks

google-ml-butler[bot] on (2024-09-04 07:23:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75000"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/75000"">No</a>

"
2502048804,issue,closed,completed,tensorflow.python.framework.errors_impl.InternalError,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf-gpu 1.13.1

### Custom code

No

### OS platform and distribution

ubuntu18.04

### Mobile device

/

### Python version

3.7

### Bazel version

0.19.2

### GCC/compiler version

4.8

### CUDA/cuDNN version

10.0

### GPU model and memory

8G

### Current behavior?

when train a model it stop early then i get bug

### Standalone code to reproduce the issue

```shell
https://github.com/MarlonCajamarca/Keras-LSTM-Trajectory-Prediction/blob/5297ab5e649971ed42ca9f8efa9fe925b7da982b/Train_Test_Scripts/LSTM_trainer.py
```


### Relevant log output

```shell
(lstm) wd@wd-Lenovo-Legion-Y9000K2021H:~/cjy2024.3/Keras-LSTM-Trajectory-Prediction/Train_Test_Scripts$ python LSTM_trainer.py  /home/wd/cjy2024.3/Keras-LSTM-Trajectory-Prediction/output_folder/train-test.h5  /home/wd/cjy2024.3/Keras-LSTM-Trajectory-Prediction/output_folder  config_lstm.json
Using TensorFlow backend.
2024-09-03 14:49:26.341775: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2024-09-03 14:49:26.364931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz
2024-09-03 14:49:26.365397: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c1dcd26d90 executing computations on platform Host. Devices:
2024-09-03 14:49:26.365410: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2024-09-03 14:49:26.452693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-09-03 14:49:26.453128: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c1dcbe8a90 executing computations on platform CUDA. Devices:
2024-09-03 14:49:26.453144: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6
2024-09-03 14:49:26.453219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: NVIDIA GeForce RTX 3070 Laptop GPU major: 8 minor: 6 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
totalMemory: 7.77GiB freeMemory: 6.92GiB
2024-09-03 14:49:26.453229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2024-09-03 14:49:26.453726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-09-03 14:49:26.453732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2024-09-03 14:49:26.453735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2024-09-03 14:49:26.453774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6733 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)
 Datasets successfully loaded!
 X_train shape : (392399, 4, 15) 
 Y_train shape : (392399, 4, 30)
 --> LSTM model instantiation started!
WARNING:tensorflow:From /home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
 --> LSTM model successfully created...
____________________________________________________________________________________________________________________________________________________________________________________
Layer (type)                                                                     Output Shape                                                            Param #                    
====================================================================================================================================================================================
lstm_inputs (InputLayer)                                                         (None, None, 15)                                                        0                          
____________________________________________________________________________________________________________________________________________________________________________________
Bidirectional_lstm_1 (Bidirectional)                                             (None, None, 1024)                                                      2166784                    
____________________________________________________________________________________________________________________________________________________________________________________
Bidirectional_lstm_2 (Bidirectional)                                             (None, None, 960)                                                       5783040                    
____________________________________________________________________________________________________________________________________________________________________________________
Bidirectional_lstm_3 (Bidirectional)                                             (None, None, 640)                                                       3281920                    
____________________________________________________________________________________________________________________________________________________________________________________
time_distributed_0 (TimeDistributed)                                             (None, None, 256)                                                       164096                     
____________________________________________________________________________________________________________________________________________________________________________________
time_distributed_1 (TimeDistributed)                                             (None, None, 128)                                                       32896                      
____________________________________________________________________________________________________________________________________________________________________________________
time_distributed_output (TimeDistributed)                                        (None, None, 30)                                                        3870                       
====================================================================================================================================================================================
Total params: 11,432,606
Trainable params: 11,432,606
Non-trainable params: 0
____________________________________________________________________________________________________________________________________________________________________________________
create_models have done_______
compile_models have done_______
Overview hyperparameters used on training :  ARCH--15i_30o_10sld_norm3_vill_cascade2-_Data-_final_concat512___bs-512_lr-0.01_loss-mae_opt-Ranger_BD-True_BDmrg-concat_amsG-False_DP-False_sw-0.9_sync-1_act-selu_minLR-1e-05_ptc-10_ep-100
[Errno 17] File exists: '/home/wd/cjy2024.3/Keras-LSTM-Trajectory-Prediction/output_folder/ARCH--15i_30o_10sld_norm3_vill_cascade2-_Data-_final_concat512___bs-512_lr-0.01_loss-mae_opt-Ranger_BD-True_BDmrg-concat_amsG-False_DP-False_sw-0.9_sync-1_act-selu_minLR-1e-05_ptc-10_ep-100'
WARNING:tensorflow:From /home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 333539 samples, validate on 58860 samples
Epoch 1/100
2024-09-03 14:52:29.476637: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2024-09-03 15:03:24.097886: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1477): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2024-09-03 15:03:24.097921: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at cudnn_rnn_ops.cc:1224 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, seq_length, batch_size]: [1, 15, 512, 1, 4, 512] 
2024-09-03 15:03:24.097988: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at cudnn_rnn_ops.cc:1224 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, seq_length, batch_size]: [1, 15, 512, 1, 4, 512] 
Traceback (most recent call last):
  File ""LSTM_trainer.py"", line 450, in <module>
    LstmTrainer(args.dataset, args.output, hyperparameters, args.use_checkpoint).run()
  File ""LSTM_trainer.py"", line 77, in run
    self.fit_models()
  File ""LSTM_trainer.py"", line 235, in fit_models
    callbacks=[early_stop, model_checkpoint])
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/keras/engine/training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/keras/engine/training_arrays.py"", line 199, in fit_loop
    outs = f(ins_batch)
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 2715, in __call__
    return self._call(inputs)
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/home/wd/anaconda3/envs/lstm/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, seq_length, batch_size]: [1, 15, 512, 1, 4, 512] 
         [[{{node Bidirectional_lstm_1/CudnnRNN_1}}]]
         [[{{node loss/mul}}]]

```



Tue Sep  3 15:01:13 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   52C    P8    19W /  N/A |   1699MiB /  7957MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1269      G   /usr/lib/xorg/Xorg                 38MiB |
|    0   N/A  N/A      1341      G   /usr/bin/gnome-shell              106MiB |
|    0   N/A  N/A      1710      G   /usr/lib/xorg/Xorg                354MiB |
|    0   N/A  N/A      2184      G   /usr/bin/gnome-shell               50MiB |
|    0   N/A  N/A      2545      G   /usr/lib/firefox/firefox          156MiB |
|    0   N/A  N/A     14950      C   python                            987MiB |
+-----------------------------------------------------------------------------+",suoyike1,2024-09-03 06:44:17+00:00,['Venkat6871'],2024-10-25 01:49:51+00:00,2024-10-15 02:02:48+00:00,https://github.com/tensorflow/tensorflow/issues/74999,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 1.13', 'Issues related to TF 1.13')]","[{'comment_id': 2328354334, 'issue_id': 2502048804, 'author': 'Venkat6871', 'body': 'Hi **@suoyike1** ,\r\nWe see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.16.0 and let us know if the issue still persists in newer versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 4, 9, 22, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328951914, 'issue_id': 2502048804, 'author': 'suoyike1', 'body': '> Hi **@suoyike1** , We see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.6.0 and let us know if the issue still persists in newer versions. Thank you!\r\n\r\nThank your reply!OK. I try to upgrade tensorflow to 2.6.0', 'created_at': datetime.datetime(2024, 9, 4, 12, 56, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382172139, 'issue_id': 2502048804, 'author': 'Venkat6871', 'body': 'Hi @suoyike1 ,\r\nApologies for the delay. Actually, I meant to say that you need to upgrade to version 2.16.0 or 2.17.0. Please let us know if the issue still persists in the newer versions. However, I mistakenly typed 2.6.0 instead of 2.16.0. I apologize for this earlier miscommunication.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 5, 54, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398491998, 'issue_id': 2502048804, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 2, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662450, 'issue_id': 2502048804, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662512, 'issue_id': 2502048804, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74999"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74999"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436645010, 'issue_id': 2502048804, 'author': 'suoyike1', 'body': 'thank you all very much! I had address this problem for upgrading 2.6.0. Nothing more happy than code run correctly! thanks again!', 'created_at': datetime.datetime(2024, 10, 25, 1, 49, 50, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-04 09:22:54 UTC): Hi **@suoyike1** ,
We see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.16.0 and let us know if the issue still persists in newer versions.
Thank you!

suoyike1 (Issue Creator) on (2024-09-04 12:56:15 UTC): Thank your reply!OK. I try to upgrade tensorflow to 2.6.0

Venkat6871 (Assginee) on (2024-09-30 05:54:11 UTC): Hi @suoyike1 ,
Apologies for the delay. Actually, I meant to say that you need to upgrade to version 2.16.0 or 2.17.0. Please let us know if the issue still persists in the newer versions. However, I mistakenly typed 2.6.0 instead of 2.16.0. I apologize for this earlier miscommunication.
Thank you!

github-actions[bot] on (2024-10-08 02:02:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74999"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74999"">No</a>

suoyike1 (Issue Creator) on (2024-10-25 01:49:50 UTC): thank you all very much! I had address this problem for upgrading 2.6.0. Nothing more happy than code run correctly! thanks again!

"
2501595206,issue,closed,completed,TFLite Hexagon Delegate not working for QCOM SM6375 SOC.,"I'm trying to get a quantized model running on the Qualcomm SM6375 DSP on a Zebra ET40 tablet, without success.  SOC info:

```
$ cat /sys/devices/soc0/soc_id
507
```

The FP32 model works fine on CPU and GPU.  However, when I try to run the INT8 model on the DSP, it falls back to running the XNNPack Delegate on CPU, which is slow:

```
$ ./benchmark_model --graph=model.tflite --use_hexagon=true                                                                
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [model.tflite]
INFO: Signature to run: []
INFO: Use Hexagon: [1]
INFO: Loaded model model.tflite
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
INFO: Hexagon Delegate is not supported.

WARN: Could not create Hexagon delegate: platform may not support delegate or required libraries are missing
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
...
```

I've tried several different versions of the Hexagon libraries referred to below, but all fall back to the CPU:

https://www.tensorflow.org/lite/android/delegates/hexagon

This page above has the following warning:

> Caution: The currently released versions of the Hexagon delegate, up to version 1.20.0.1, are no longer supported. An updated version of this delegate is expected soon.

I've also tried the NNAPI (--use_nnapi=true), but that also falls back to the CPU.

Any help would be appreciated.
",hholokai,2024-09-02 21:21:47+00:00,['gaikwadrahul8'],2024-11-10 02:03:37+00:00,2024-11-10 02:03:34+00:00,https://github.com/tensorflow/tensorflow/issues/74993,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2437815389, 'issue_id': 2501595206, 'author': 'gaikwadrahul8', 'body': 'Hi, @hholokai \r\n\r\nI apologize for the delayed response, The [NNAPI](https://www.tensorflow.org/lite/android/delegates/nnapi) and [Hexagon](https://www.tensorflow.org/lite/android/delegates/hexagon) delegates are deprecated and no longer supported by TensorFlow Lite. For more information, see the [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide) and [TF Lite delegates documentation](https://www.tensorflow.org/lite/performance/delegates).\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 25, 13, 41, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797162, 'issue_id': 2501595206, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547371, 'issue_id': 2501595206, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547398, 'issue_id': 2501595206, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74993"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74993"">No</a>', 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 35, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-25 13:41:15 UTC): Hi, @hholokai 

I apologize for the delayed response, The [NNAPI](https://www.tensorflow.org/lite/android/delegates/nnapi) and [Hexagon](https://www.tensorflow.org/lite/android/delegates/hexagon) delegates are deprecated and no longer supported by TensorFlow Lite. For more information, see the [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide) and [TF Lite delegates documentation](https://www.tensorflow.org/lite/performance/delegates).

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-02 02:01:01 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-10 02:03:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-10 02:03:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74993"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74993"">No</a>

"
2500674342,issue,closed,completed,Tensorflow/Keras_nlp bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have experienced this error while running KerasNLP with Tensorflow as the backend. I tried to thoroughly investigate the issue, and I think the error is essentially with Tensorflow code and not KerasNLP. The following is a detailed description of the error:

**Describe the bug**

I encountred an error/bug while trying to execute a docstring code example from the file `keras_nlp.src.models.gpt2.causal_lm.py` and I have reproduced the example code below:

``` python

features = [""a quick fox."", ""a fox quick.""]
vocab = {""<|endoftext|>"": 0, ""a"": 4, ""Ġquick"": 5, ""Ġfox"": 6}
merges = [""Ġ q"", ""u i"", ""c k"", ""ui ck"", ""Ġq uick""]
merges += [""Ġ f"", ""o x"", ""Ġf ox""]

tokenizer = keras_nlp.models.GPT2Tokenizer(
    vocabulary=vocab,
    merges=merges,
)
preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor(
    tokenizer=tokenizer,
    sequence_length=128,
)
backbone = keras_nlp.models.GPT2Backbone(
    vocabulary_size=30552,
    num_layers=4,
    num_heads=4,
    hidden_dim=256,
    intermediate_dim=512,
    max_sequence_length=128,
)
gpt2_lm = keras_nlp.models.GPT2CausalLM(
    backbone=backbone,
    preprocessor=preprocessor,
)
gpt2_lm.fit(x=features, batch_size=2)
```
The following is a comprehensive description of the error, reproduced below and debugging using `pdb`:

```
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py(79)__init__()
-> super().__init__(*args, **kwargs)
(Pdb) c
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py(140)compile()
-> super().compile(
(Pdb) c
> /home/humbulani/keras-master/nlp_example.py(94)<module>()
-> gpt2_lm.fit(x=features, batch_size=2)
(Pdb) c
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py(196)fit()
-> return super().fit(
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(269)fit()
-> self._assert_compile_called(""fit"")
(Pdb) c
> /home/humbulani/keras-master/keras/src/trainers/epoch_iterator.py(66)__init__()
-> self.data_adapter = data_adapters.get_data_adapter(
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(331)fit()
-> logs = self.train_function(iterator)
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(125)one_step_on_iterator()
-> """"""Runs a single training step given a Dataset iterator.""""""
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(50)train_step()
-> x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)
(Pdb) c
> /home/humbulani/keras-master/keras/src/losses/losses.py(1724)sparse_categorical_crossentropy()
-> res = ops.sparse_categorical_crossentropy(
(Pdb) c
2024-08-19 12:03:15.742874: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-08-19 12:03:15.742930: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Traceback (most recent call last):
  File ""/home/humbulani/keras-master/nlp_example.py"", line 94, in <module>
    gpt2_lm.fit(x=features, batch_size=2)
  File ""/home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py"", line 196, in fit
    return super().fit(
  File ""/home/humbulani/keras-master/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/humbulani/keras-master/env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__SparseSoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:CPU:0}} Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [Op:SparseSoftmaxCrossEntropyWithLogits] name: 
```
Th error is clear:  `the -1 value`. I've traced the error to the following function from the file `keras.src.backend.tensorflow.trainer`:

```python
@tf.autograph.experimental.do_not_convert
    def one_step_on_iterator(iterator):
        """"""Runs a single training step given a Dataset iterator.""""""
        data = next(iterator) 
        outputs = self.distribute_strategy.run(
            one_step_on_data, args=(data,)
        )
        outputs = reduce_per_replica(
            outputs,
            self.distribute_strategy,
            reduction=""auto"",
        )
        return outputs
```
The line `data=next(iterator)` computes the labels and therefore the -1 value is created here. The `iterator` argument is a tensorflow `OwnedIterator` and executes from the file `tensorflow.python.data.ops.iterator_ops` and the executed function reproduced below:

```python
def _next_internal(self):
    autograph_status = autograph_ctx.control_status_ctx().status
    autograph_disabled = autograph_status == autograph_ctx.Status.DISABLED
    if not context.executing_eagerly() and autograph_disabled:
      self._get_next_call_count += 1
      if self._get_next_call_count > GET_NEXT_CALL_ERROR_THRESHOLD:
        raise ValueError(GET_NEXT_CALL_ERROR_MESSAGE)

    if not context.executing_eagerly():
      # TODO(b/169442955): Investigate the need for this colocation constraint.
      with ops.colocate_with(self._iterator_resource):
        ret = gen_dataset_ops.iterator_get_next(
            self._iterator_resource,
            output_types=self._flat_output_types,
            output_shapes=self._flat_output_shapes)
      return structure.from_compatible_tensor_list(self._element_spec, ret)
```
which executes `gen_dataset_ops.iterator_get_next` from the file `tensorflow.python.data.ops.gen_dataset_ops`, and from here to the relevant ops execution which I didn't trace further since it also leads to C++ execution code.

## Enviroment

```
Linux 6.5.0-26-generic #26~22.04.1-Ubuntu
keras - 3.5.0
python - 3.10.12
tensorflow - 2.17.0
kerasNLP - 0.14.4
```
 - Additional tensorflow info:
  ```
  2024-08-19 12:20:02.135293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-19 12:20:02.154198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-19 12:20:02.159831: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-19 12:20:02.174579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-19 12:20:03.092334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-19 12:20:04.517556: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
  ```
**To Reproduce**

Link to a [Colab Notebook](https://colab.research.google.com/drive/1EbFdLMpPiPRJoLMNwG42OR-ULyQBfsqp?authuser=0#scrollTo=tgFAiISLQ9zW)

**Expected behavior**

I expected the model to train normally by running the `fit()` function without any complications and return a `History` object.

**Would you like to help us fix it?**


### Standalone code to reproduce the issue

```shell
# Standard library dependenies
import os
!pip install keras_nlp

# Third party dependencies
import numpy as np

# Tensorflow dependencies
import tensorflow.data as tf_data
import tensorflow.strings as tf_strings
import tensorflow as tf

# Keras dependencies
import keras

# Keras_NLP dependencies
import keras_nlp

# Specific dependencies for GPT Model
from keras import ops

from keras_nlp.src.api_export import keras_nlp_export
from keras_nlp.src.models.causal_lm import CausalLM
from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
    GPT2CausalLMPreprocessor,
)
from keras_nlp.src.utils.tensor_utils import any_equal


features = [""a quick fox."", ""a fox quick.""]
vocab = {""<|endoftext|>"": 0, ""a"": 4, ""Ġquick"": 5, ""Ġfox"": 6}
merges = [""Ġ q"", ""u i"", ""c k"", ""ui ck"", ""Ġq uick""]
merges += [""Ġ f"", ""o x"", ""Ġf ox""]

tokenizer = keras_nlp.models.GPT2Tokenizer(
    vocabulary=vocab,
    merges=merges,
)
preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor(
    tokenizer=tokenizer,
    sequence_length=128,
)
backbone = keras_nlp.models.GPT2Backbone(
    vocabulary_size=30552,
    num_layers=4,
    num_heads=4,
    hidden_dim=256,
    intermediate_dim=512,
    max_sequence_length=128,
)
gpt2_lm = keras_nlp.models.GPT2CausalLM(
    backbone=backbone,
    preprocessor=preprocessor,
)

gpt2_lm.fit(x=features, batch_size=2)
```


### Relevant log output

```shell
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py(79)__init__()
-> super().__init__(*args, **kwargs)
(Pdb) c
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py(140)compile()
-> super().compile(
(Pdb) c
> /home/humbulani/keras-master/nlp_example.py(94)<module>()
-> gpt2_lm.fit(x=features, batch_size=2)
(Pdb) c
> /home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py(196)fit()
-> return super().fit(
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(269)fit()
-> self._assert_compile_called(""fit"")
(Pdb) c
> /home/humbulani/keras-master/keras/src/trainers/epoch_iterator.py(66)__init__()
-> self.data_adapter = data_adapters.get_data_adapter(
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(331)fit()
-> logs = self.train_function(iterator)
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(125)one_step_on_iterator()
-> """"""Runs a single training step given a Dataset iterator.""""""
(Pdb) c
> /home/humbulani/keras-master/keras/src/backend/tensorflow/trainer.py(50)train_step()
-> x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)
(Pdb) c
> /home/humbulani/keras-master/keras/src/losses/losses.py(1724)sparse_categorical_crossentropy()
-> res = ops.sparse_categorical_crossentropy(
(Pdb) c
2024-08-19 12:03:15.742874: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-08-19 12:03:15.742930: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Traceback (most recent call last):
  File ""/home/humbulani/keras-master/nlp_example.py"", line 94, in <module>
    gpt2_lm.fit(x=features, batch_size=2)
  File ""/home/humbulani/keras-master/env/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py"", line 196, in fit
    return super().fit(
  File ""/home/humbulani/keras-master/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/humbulani/keras-master/env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__SparseSoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:CPU:0}} Received a label value of -1 which is outside the valid range of [0, 30552).  Label values: 4 5 6 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [Op:SparseSoftmaxCrossEntropyWithLogits] name:
```
",Humbulani1234,2024-09-02 11:08:08+00:00,['tilakrayal'],2024-09-20 01:59:57+00:00,2024-09-20 01:59:54+00:00,https://github.com/tensorflow/tensorflow/issues/74972,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2331249465, 'issue_id': 2500674342, 'author': 'tilakrayal', 'body': '@Humbulani1234,\r\nThough the backend is the tensorflow, it contains the Keras3.0 by default and also Unable to register cuBLAS factory error is the known issue in the tensorflow. \r\nCould you please check with the keras-nlp or keras-team/keras for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 9, 5, 11, 10, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347893977, 'issue_id': 2500674342, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 13, 1, 58, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362557667, 'issue_id': 2500674342, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 20, 1, 59, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362557716, 'issue_id': 2500674342, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74972"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74972"">No</a>', 'created_at': datetime.datetime(2024, 9, 20, 1, 59, 56, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-05 11:10:52 UTC): @Humbulani1234,
Though the backend is the tensorflow, it contains the Keras3.0 by default and also Unable to register cuBLAS factory error is the known issue in the tensorflow. 
Could you please check with the keras-nlp or keras-team/keras for the quick resolution. Thank you!

github-actions[bot] on (2024-09-13 01:58:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-20 01:59:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-20 01:59:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74972"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74972"">No</a>

"
2499980849,issue,closed,completed,tf.linalg.solve single precision wrong result,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux x64 - Ubuntu docker image

### Mobile device

_No response_

### Python version

Python 3.12.5 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

See code example - a particular linear system solves correctly under both 32 and 64 bit in NumPy, as well as 64 bit in both Tensorflow and PyTorch (where the problem was initially encountered), but not with 32 bit tensors. The Tensorflow 32 bit solution is less incorrect than that of PyTorch, but still quite unusable. 

### Standalone code to reproduce the issue

```shell
A = [[-77386.6328125, -13253.58984375, -7761.71484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50099.83203125, 0.0, 0.0, 18231.466796875, 8855.2841796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9055.3349609375, 4398.30517578125, 7761.71484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [-13253.587890625, -109584.171875, -3769.975830078125, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8855.283203125, 4301.1376953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4398.30517578125, 2136.319580078125, 3769.975830078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [-7761.7158203125, -3769.97607421875, -6652.89892578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7761.7158203125, 3769.97607421875, 6652.89892578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, -50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 103146.7109375, 0.0, 0.0, -122629.984375, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, -50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, -122629.984375, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -68331.296875, 8855.2841796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18231.466796875, -8855.2841796875, 0.0, 50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 8855.283203125, -107447.8515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -8855.283203125, 4301.1376953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -78825.703125, -1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 9242.59765625, 1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, -109656.6328125, -10906.15625, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [18231.466796875, 8855.2841796875, 0.0, 50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -96132.125, -8855.2841796875, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [8855.283203125, 4301.1376953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, -8855.283203125, -135248.671875, -2772.517578125, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, -2772.51806640625, -233499.90625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58449.80859375, 0.0, 2772.51806640625, 924.1725463867188, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 50099.83203125, 0.0, 18231.466796875, -8855.2841796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -96132.125, 8855.2841796875, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -8855.283203125, 4301.1376953125, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 8855.283203125, -135248.671875, 2772.517578125, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 2772.51806640625, -233499.90625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 924.1725463867188, 0.0, 0.0, 58449.80859375, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50099.83203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -78825.703125, 1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9242.59765625, -1745.823974609375, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, 1745.8240966796875, -109656.6328125, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -48209.13671875, -1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 9242.59765625, 1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, -109656.6328125, -10906.15625, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9242.59765625, 1745.823974609375, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -56526.69140625, -1745.823974609375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, -1745.8240966796875, -131277.296875, -2772.517578125, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, -2772.51806640625, -233499.90625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58449.80859375, 0.0, 2772.51806640625, 924.1725463867188, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 9242.59765625, -1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -56526.69140625, 1745.823974609375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 1745.8240966796875, -131277.296875, 2772.517578125, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 2772.51806640625, -233499.90625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 924.1725463867188, 0.0, 0.0, 58449.80859375, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -48209.13671875, 1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9242.59765625, -1745.823974609375, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, 1745.8240966796875, -109656.6328125, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38966.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -109326.8671875, -10906.15625, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9242.59765625, 1745.823974609375, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -56526.69140625, -1745.823974609375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, -1745.8240966796875, -132519.359375, 1075.35009765625, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9559.6123046875, -1075.35009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 1075.3499755859375, -232696.703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, -1075.3499755859375, 120.96492767333984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58449.80859375, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 9242.59765625, -1745.823974609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -56526.69140625, 1745.823974609375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1745.8240966796875, 329.7667541503906, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 1745.8240966796875, -122959.75, 0.0, 0.0, 103146.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 0.0, -232575.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58449.80859375],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19483.26953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38966.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 103146.7109375, 0.0, 0.0, -109326.8671875, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, -19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10906.15625, 19246.158203125],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9559.6123046875, -1075.35009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -9559.6123046875, 1075.35009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1075.3499755859375, 120.96492767333984, 0.0, 0.0, 173201.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1075.3499755859375, -173322.71875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [9055.3349609375, 4398.30517578125, 7761.71484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18796.96875, -4398.30517578125, -7761.71484375, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [4398.30517578125, 2136.319580078125, 3769.975830078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4398.30517578125, -26375.6640625, -11903.6142578125, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [7761.7158203125, 3769.97607421875, 6652.89892578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 58449.80859375, 0.0, -2772.51806640625, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -7761.7158203125, -11903.6142578125, -85273.0390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, -24239.34375, 8133.638671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 924.1725463867188, 0.0, 0.0, 58449.80859375, 0.0, -10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8133.63818359375, -78620.140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, -27800.822265625, 0.0, -2772.517578125, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, -2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -24239.34375, -8133.638671875, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 58449.80859375, 0.0, -2772.51806640625, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, -8133.63818359375, -79544.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, -27800.822265625, 0.0, -2772.517578125, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, -24239.34375, 8133.638671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 924.1725463867188, 0.0, 0.0, 58449.80859375, 0.0, -10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 8133.63818359375, -79544.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, -27800.822265625, 0.0, -2772.517578125, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, 10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -15921.7900390625, -10906.15625, 0.0, 9741.634765625, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10906.15625, 19246.158203125, 0.0, 0.0, 58449.80859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, -10906.15625, -78620.140625, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8317.5537109375, 0.0, 2772.517578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, 0.0, 0.0, 0.0, -27800.822265625, 0.0, -2772.517578125],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6180.1552734375, -10906.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9741.634765625, 0.0, 0.0, -15921.7900390625, 10906.15625],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2772.51806640625, 0.0, 924.1725463867188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58449.80859375, 0.0, -10906.15625, 19246.158203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2772.51806640625, 10906.15625, -78620.140625]]

B = [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, 2500.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]


import torch as t
import numpy as np

Xt = t.linalg.solve(t.Tensor(A),t.Tensor(B))
print('torch ',t.__version__,Xt.min(),Xt.max(),(t.Tensor(A) @ Xt - t.Tensor(B)).pow(2).sum())
Xt32 = t.linalg.solve(t.Tensor(A).to(t.float32),t.Tensor(B).to(t.float32))
print('torch 32',t.__version__,Xt32.min(),Xt32.max(),(t.Tensor(A).to(t.float32) @ Xt32 - t.Tensor(B).to(t.float32)).pow(2).sum())
Xt64 = t.linalg.solve(t.Tensor(A).to(t.float64),t.Tensor(B).to(t.float64))
print('torch 64',t.__version__,Xt64.min(),Xt64.max(),(t.Tensor(A).to(t.float64) @ Xt64 - t.Tensor(B).to(t.float64)).pow(2).sum())

Xn = np.linalg.solve(np.array(A),np.array(B))
print('numpy',np.__version__,Xn.min(),Xn.max(),((np.array(A) @ Xn - np.array(B))**2).sum())
Xn32 = np.linalg.solve(np.array(A,dtype=np.float32),np.array(B,dtype=np.float32))
print('numpy 32',np.__version__,Xn32.min(),Xn32.max(),((np.array(A,dtype=np.float32) @ Xn32 - np.array(B,dtype=np.float32))**2).sum())
Xn64 = np.linalg.solve(np.array(A,dtype=np.float64),np.array(B,dtype=np.float64))
print('numpy 64',np.__version__,Xn64.min(),Xn64.max(),((np.array(A,dtype=np.float64) @ Xn64 - np.array(B,dtype=np.float64))**2).sum())



Xt32cu = t.linalg.solve(t.Tensor(A).to(t.float32).cuda(),t.Tensor(B).to(t.float32).cuda())
print('torch 32 cuda',t.__version__,Xt32cu.min(),Xt32cu.max(),(t.Tensor(A).to(t.float32).cuda() @ Xt32cu - t.Tensor(B).to(t.float32).cuda()).pow(2).sum())
Xt64cu = t.linalg.solve(t.Tensor(A).to(t.float64).cuda(),t.Tensor(B).to(t.float64).cuda())
print('torch 64 cuda',t.__version__,Xt64cu.min(),Xt64cu.max(),(t.Tensor(A).to(t.float64).cuda() @ Xt64cu - t.Tensor(B).to(t.float64).cuda()).pow(2).sum())

#fp16 - does not run

#Xt16 = t.linalg.solve(t.Tensor(A).to(t.float16),t.Tensor(B).to(t.float16))
#print('torch 16',t.__version__,Xt16.min(),Xt16.max(),(t.Tensor(A).to(t.float16) @ Xt16 - t.Tensor(B).to(t.float16)).pow(2).sum())
#Xt16cu = t.linalg.solve(t.Tensor(A).to(t.float16).cuda(),t.Tensor(B).to(t.float16).cuda())
#print('torch 16 cuda',t.__version__,Xt16cu.min(),Xt16cu.max(),(t.Tensor(A).to(t.float16).cuda() @ Xt16cu - t.Tensor(B).to(t.float16).cuda()).pow(2).sum())




import tensorflow as tf

Xtf32 = tf.linalg.solve(tf.convert_to_tensor(A,dtype=tf.float32),tf.convert_to_tensor(B,dtype=tf.float32)[:,None])
print('tensorflow 32',tf.__version__,tf.reduce_min(Xtf32),tf.reduce_max(Xtf32),
    tf.reduce_sum((tf.convert_to_tensor(A,dtype=tf.float32) @ Xtf32 - tf.convert_to_tensor(B,dtype=tf.float32)[:,None])**2))
Xtf64 = tf.linalg.solve(tf.convert_to_tensor(A,dtype=tf.float64),tf.convert_to_tensor(B,dtype=tf.float64)[:,None])
print('tensorflow 64',tf.__version__,tf.reduce_min(Xtf64),tf.reduce_max(Xtf64),
    tf.reduce_sum((tf.convert_to_tensor(A,dtype=tf.float64) @ Xtf64 - tf.convert_to_tensor(B,dtype=tf.float64)[:,None])**2))
```


### Relevant log output

```shell
tensorflow 32 2.17.0 tf.Tensor(-7224.5776, shape=(), dtype=float32) tf.Tensor(12481.284, shape=(), dtype=float32) tf.Tensor(608.6341, shape=(), dtype=float32)

tensorflow 64 2.17.0 tf.Tensor(-114.8141409765533, shape=(), dtype=float64) tf.Tensor(4.718772190885021, shape=(), dtype=float64) tf.Tensor(1.1391063518181607e-16, shape=(), dtype=float64)
```
",lddllddl,2024-09-02 04:38:58+00:00,['Venkat6871'],2024-09-04 12:01:33+00:00,2024-09-04 12:01:12+00:00,https://github.com/tensorflow/tensorflow/issues/74965,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2323800514, 'issue_id': 2499980849, 'author': 'lddllddl', 'body': 'PyTorch and NumPy results\r\n```\r\ntorch 2.5.0a0+gitcbee9c1 tensor(-8094135.) tensor(14480713.) tensor(6.7718e+08)\r\ntorch 32 2.5.0a0+gitcbee9c1 tensor(-8094135.) tensor(14480713.) tensor(6.7718e+08)\r\ntorch 64 2.5.0a0+gitcbee9c1 tensor(-114.8141, dtype=torch.float64) tensor(4.7188, dtype=torch.float64) tensor(5.5413e-17, dtype=torch.float64)\r\nnumpy 2.0.0 -114.81414097646719 4.7187721908814435 2.1754394025006162e-16\r\nnumpy 32 2.0.0 -114.81414 4.7187724 5.3345637\r\nnumpy 64 2.0.0 -114.81414097646719 4.7187721908814435 2.1754394025006162e-16\r\n```', 'created_at': datetime.datetime(2024, 9, 2, 4, 40, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324443019, 'issue_id': 2499980849, 'author': 'lddllddl', 'body': 'Cross references:\r\n\r\nhttps://github.com/pytorch/pytorch/issues/134905\r\n\r\nhttps://github.com/google/jax/issues/23367', 'created_at': datetime.datetime(2024, 9, 2, 10, 58, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325891436, 'issue_id': 2499980849, 'author': 'Venkat6871', 'body': 'Hi **@lddllddl** ,\r\nThank you for reporting the issue. I tried to reproduce the code on tensorflow v2.17 and observed the same issue. I am providing [gist](https://colab.research.google.com/gist/Venkat6871/2aa6ac3de2e03affa9c419a753a91b17/74965_2-17-nightly.ipynb) here for reference. Please allow to deep dive on the same and provide the update on the same. \r\nThank you!', 'created_at': datetime.datetime(2024, 9, 3, 8, 20, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328676909, 'issue_id': 2499980849, 'author': 'Venkat6871', 'body': 'Hi **@lddllddl** ,\r\nI reviewed your issue more deeply. As this [comment](https://github.com/google/jax/issues/23367#issuecomment-2324775551) said, these results are expected. \r\nThank you!', 'created_at': datetime.datetime(2024, 9, 4, 11, 26, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328781053, 'issue_id': 2499980849, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74965"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74965"">No</a>', 'created_at': datetime.datetime(2024, 9, 4, 12, 1, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328782194, 'issue_id': 2499980849, 'author': 'lddllddl', 'body': 'Thanks', 'created_at': datetime.datetime(2024, 9, 4, 12, 1, 32, tzinfo=datetime.timezone.utc)}]","lddllddl (Issue Creator) on (2024-09-02 04:40:20 UTC): PyTorch and NumPy results
```
torch 2.5.0a0+gitcbee9c1 tensor(-8094135.) tensor(14480713.) tensor(6.7718e+08)
torch 32 2.5.0a0+gitcbee9c1 tensor(-8094135.) tensor(14480713.) tensor(6.7718e+08)
torch 64 2.5.0a0+gitcbee9c1 tensor(-114.8141, dtype=torch.float64) tensor(4.7188, dtype=torch.float64) tensor(5.5413e-17, dtype=torch.float64)
numpy 2.0.0 -114.81414097646719 4.7187721908814435 2.1754394025006162e-16
numpy 32 2.0.0 -114.81414 4.7187724 5.3345637
numpy 64 2.0.0 -114.81414097646719 4.7187721908814435 2.1754394025006162e-16
```

lddllddl (Issue Creator) on (2024-09-02 10:58:33 UTC): Cross references:

https://github.com/pytorch/pytorch/issues/134905

https://github.com/google/jax/issues/23367

Venkat6871 (Assginee) on (2024-09-03 08:20:28 UTC): Hi **@lddllddl** ,
Thank you for reporting the issue. I tried to reproduce the code on tensorflow v2.17 and observed the same issue. I am providing [gist](https://colab.research.google.com/gist/Venkat6871/2aa6ac3de2e03affa9c419a753a91b17/74965_2-17-nightly.ipynb) here for reference. Please allow to deep dive on the same and provide the update on the same. 
Thank you!

Venkat6871 (Assginee) on (2024-09-04 11:26:59 UTC): Hi **@lddllddl** ,
I reviewed your issue more deeply. As this [comment](https://github.com/google/jax/issues/23367#issuecomment-2324775551) said, these results are expected. 
Thank you!

google-ml-butler[bot] on (2024-09-04 12:01:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74965"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74965"">No</a>

lddllddl (Issue Creator) on (2024-09-04 12:01:32 UTC): Thanks

"
2499824355,issue,closed,completed,`gen_nn_ops.AvgPool3DGrad` can cause an aborted issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `gen_nn_ops.AvgPool3DGrad` . I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
import tensorflow as tf
sess = tf.compat.v1.Session()

for dtype in [dtypes.float32]:
    with sess.as_default():
        orig_input_shape = constant_op.constant(1879048192, shape=[], dtype=dtypes.int32)
        grad = constant_op.constant(1, shape=[], dtype=dtype)
        t = gen_nn_ops.AvgPool3DGrad(orig_input_shape=orig_input_shape, grad=grad, ksize=(1, 1, 1, 1, 1), strides=(1, 1, 1, 1, 1), padding='SAME', data_format='NDHWC')
```


### Relevant log output

```shell
2024-09-02 09:52:09.779726: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions
Aborted (core dumped)
```
",KnightGOKU,2024-09-02 01:41:44+00:00,[],2024-09-20 11:33:00+00:00,2024-09-20 11:32:59+00:00,https://github.com/tensorflow/tensorflow/issues/74956,[],"[{'comment_id': 2363518763, 'issue_id': 2499824355, 'author': 'KnightGOKU', 'body': 'Close this issue as the most recent version has resolved it.', 'created_at': datetime.datetime(2024, 9, 20, 11, 32, 59, tzinfo=datetime.timezone.utc)}]","KnightGOKU (Issue Creator) on (2024-09-20 11:32:59 UTC): Close this issue as the most recent version has resolved it.

"
2499813741,issue,open,,`np.cumprod` on ndarray with type `tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype` can cause `segmentfault,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240828

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentfault issue` in TensorFlow when I used API `np.cumprod` on ndarray with type `tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype` .  I have confirmed that the code would crash on `tf-nightly-2.18.0.dev20240817` and `tf-nightly-2.18.0.dev20240828` (nightly-build)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes


def numpy_reverse(x, axis):
    length = len(x.shape)
    if axis < 0:
        axis = length + axis
    ix = [slice(None, None, -1) if i == axis else slice(None) for i in range(length)]
    return x[tuple(ix)]


axis = 0
x = np.zeros([595]).astype(dtypes.bfloat16.as_numpy_dtype) # crash
# x = np.zeros([595]).astype(float) # works well
length = len(x.shape)
x = numpy_reverse(x, axis=0)
ix_head = [slice(0, 1) if i == axis else slice(None) for i in range(length)]
ix_init = [slice(0, -1) if i == axis else slice(None) for i in range(length)]
init = np.ones_like(x[tuple(ix_head)])
np_out = np.concatenate([init, np.cumprod(x[tuple(ix_init)], axis)], axis=axis)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",KnightGOKU,2024-09-02 01:29:18+00:00,[],2024-12-27 08:19:54+00:00,,https://github.com/tensorflow/tensorflow/issues/74955,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437524436, 'issue_id': 2499813741, 'author': 'tilakrayal', 'body': '@KnightGOKU,\r\nHave you tried using tf.math.cumprod  while using tensorflow. When I tried using tf.math.cumprod the mentioned code was executed without Segmentation fail on tensorflow v2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ab17db434ac7c8ef2ca83a5d3c942864/untitled2204.ipynb). \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/cumprod\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 25, 11, 18, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797174, 'issue_id': 2499813741, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452848284, 'issue_id': 2499813741, 'author': 'KnightGOKU', 'body': '@tilakrayal Thank you for your response. It appears that the segmentation fault occurs specifically when a tensor of type tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype is used with `np.cumprod`. Could this indicate a compatibility issue between TensorFlow and NumPy?', 'created_at': datetime.datetime(2024, 11, 2, 4, 5, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543182095, 'issue_id': 2499813741, 'author': 'Balkrishna5', 'body': 'NumPy does not natively support bfloat16 in most of its operations, including np.cumprod.This leads to errors when trying to compute cumulative products or other advanced operations on arrays of this type.\r\nThe code works fine because NumPy has extensive support for standard floating-point types like float32 and float64.\r\nTensorFlow is designed to handle bfloat16 efficiently. So, we can replace NumPy operations with TensorFlow equivalents.\r\nI can also provide code for same if ,required. And glad to hear from you.', 'created_at': datetime.datetime(2024, 12, 14, 16, 50, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-10-25 11:18:08 UTC): @KnightGOKU,
Have you tried using tf.math.cumprod  while using tensorflow. When I tried using tf.math.cumprod the mentioned code was executed without Segmentation fail on tensorflow v2.17. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ab17db434ac7c8ef2ca83a5d3c942864/untitled2204.ipynb). 

https://www.tensorflow.org/api_docs/python/tf/math/cumprod

Thank you!

github-actions[bot] on (2024-11-02 02:01:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

KnightGOKU (Issue Creator) on (2024-11-02 04:05:34 UTC): @tilakrayal Thank you for your response. It appears that the segmentation fault occurs specifically when a tensor of type tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype is used with `np.cumprod`. Could this indicate a compatibility issue between TensorFlow and NumPy?

Balkrishna5 on (2024-12-14 16:50:44 UTC): NumPy does not natively support bfloat16 in most of its operations, including np.cumprod.This leads to errors when trying to compute cumulative products or other advanced operations on arrays of this type.
The code works fine because NumPy has extensive support for standard floating-point types like float32 and float64.
TensorFlow is designed to handle bfloat16 efficiently. So, we can replace NumPy operations with TensorFlow equivalents.
I can also provide code for same if ,required. And glad to hear from you.

"
2499334177,issue,closed,completed,resolve code compatibility with my system,"ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow as tf
      2 from tensorflow import keras
      3 from keras.layers import Input, Dense, Flatten

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\Dr Adedeji\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

",Adedeji-hub,2024-09-01 09:20:13+00:00,['tilakrayal'],2025-01-13 19:13:34+00:00,2024-09-18 01:58:32+00:00,https://github.com/tensorflow/tensorflow/issues/74951,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2323264966, 'issue_id': 2499334177, 'author': 'Adedeji-hub', 'body': 'import tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.layers import Input, Dense, Flatten\r\nfrom keras.models import Model\r\nfrom keras.applications.vgg16 import VGG16\r\nfrom keras.preprocessing import image\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nimport numpy as np\r\nfrom glob import glob\r\nimport matplotlib.pyplot as plt\r\n\r\nIn an attempt to run the code above  lines of import, it came up with an import error.\r\nCan you', 'created_at': datetime.datetime(2024, 9, 1, 10, 0, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324758685, 'issue_id': 2499334177, 'author': 'tilakrayal', 'body': '@Adedeji-hub,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 2, 13, 25, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339459937, 'issue_id': 2499334177, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336107, 'issue_id': 2499334177, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336146, 'issue_id': 2499334177, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74951"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74951"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587982344, 'issue_id': 2499334177, 'author': 'Adedeji-hub', 'body': 'ImportError                               Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\n     69 try:\n---> 70   from tensorflow.python._pywrap_tensorflow_internal import *\n     71 # This try catch logic is because there is no bazel equivalent for py_extension.\n     72 # Externally in opensource we must enable exceptions to load the shared object\n     73 # by exposing the PyInit symbols with pybind. This error will only be\n     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n     75 \n     76 # This logic is used in other internal projects using py_extension.\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[3], line 2\n      1 # import required libraries\n----> 2 import tensorflow as tf\n      3 from tensorflow import keras\n      4 from keras.models import Sequential\n\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\n     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")\n     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n     41 from tensorflow.python.tools import module_util as _module_util\n     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader\n\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:85\n     83     sys.setdlopenflags(_default_dlopen_flags)\n     84 except ImportError:\n---> 85   raise ImportError(\n     86       f\'{traceback.format_exc()}\'\n     87       f\'\\n\\nFailed to load the native TensorFlow runtime.\\n\'\n     88       f\'See https://www.tensorflow.org/install/errors \'\n     89       f\'for some common causes and solutions.\\n\'\n     90       f\'If you need help, create an issue \'\n     91       f\'at https://github.com/tensorflow/tensorflow/issues \'\n     92       f\'and include the entire stack trace above this error message.\')\n\nImportError: Traceback (most recent call last):\n  File ""C:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.', 'created_at': datetime.datetime(2025, 1, 13, 19, 12, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587983397, 'issue_id': 2499334177, 'author': 'Adedeji-hub', 'body': 'i am having issues with tensorflow', 'created_at': datetime.datetime(2025, 1, 13, 19, 13, 33, tzinfo=datetime.timezone.utc)}]","Adedeji-hub (Issue Creator) on (2024-09-01 10:00:34 UTC): import tensorflow as tf
from tensorflow import keras
from keras.layers import Input, Dense, Flatten
from keras.models import Model
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt

In an attempt to run the code above  lines of import, it came up with an import error.
Can you

tilakrayal (Assginee) on (2024-09-02 13:25:28 UTC): @Adedeji-hub,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-09-10 01:58:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74951"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74951"">No</a>

Adedeji-hub (Issue Creator) on (2025-01-13 19:12:59 UTC): ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[3], line 2
      1 # import required libraries
----> 2 import tensorflow as tf
      3 from tensorflow import keras
      4 from keras.models import Sequential

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

Adedeji-hub (Issue Creator) on (2025-01-13 19:13:33 UTC): i am having issues with tensorflow

"
2498978025,issue,open,,tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel raises a program abort,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the `num_bits` in a large integer, this API raises the program abort.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
inputs = tf.constant(0.57681304)
min = tf.constant(2.1311088)
max = tf.constant(2.4402196)
num_bits = 10
narrow_range = False
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs,min=min,max=max,num_bits=num_bits,narrow_range=narrow_range)
```
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",maybeLee,2024-08-31 17:52:07+00:00,['Venkat6871'],2024-09-30 05:56:35+00:00,,https://github.com/tensorflow/tensorflow/issues/74932,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2324256511, 'issue_id': 2498978025, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15, 2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/affba4f6b24d23edb0fa466420f3b1f6/74932_tf-2-15-2-17-nightly.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 2, 9, 25, 37, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-02 09:25:37 UTC): I tried to run your code on Colab using TF v2.15, 2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/affba4f6b24d23edb0fa466420f3b1f6/74932_tf-2-15-2-17-nightly.ipynb) here for reference.
Thank you!

"
2498900666,issue,closed,completed,tf.keras.layers.UpSampling2D outputs different output type for input unsigned integer tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Previously, output tensor has float-point type for unsigned input integer tensors.
Now it is changed to the same unsigned integer type as input.
There were backward incompatible changes.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

rng = np.random.default_rng()
data = rng.integers(-8, 8, [2, 4, 8, 2]).astype(np.uint16)

tf.keras.backend.clear_session()
x = tf.keras.Input(shape=[4, 8, 2], name='x', dtype=np.uint16)
y = tf.keras.layers.UpSampling2D(size=(2, 3), data_format='channels_last',
                                 interpolation='bilinear')(x)
model = tf.keras.Model(inputs=[x], outputs=[y])
res = model(data)
list(res)[0].dtype
```


### Relevant log output

",rkazants,2024-08-31 15:25:41+00:00,['tilakrayal'],2024-09-18 01:58:38+00:00,2024-09-18 01:58:33+00:00,https://github.com/tensorflow/tensorflow/issues/74929,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2323957073, 'issue_id': 2498900666, 'author': 'tilakrayal', 'body': '@rkazants,\r\nThank you for reporting the issue. I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a844794068095b9578e13cc2f5616761/untitled2089.ipynb).\r\n\r\nAs this issue is more related to Keras, could you please try to raise the issue on Keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 6, 58, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339459963, 'issue_id': 2498900666, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336132, 'issue_id': 2498900666, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336213, 'issue_id': 2498900666, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74929"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74929"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-02 06:58:13 UTC): @rkazants,
Thank you for reporting the issue. I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a844794068095b9578e13cc2f5616761/untitled2089.ipynb).

As this issue is more related to Keras, could you please try to raise the issue on Keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!

github-actions[bot] on (2024-09-10 01:58:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74929"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74929"">No</a>

"
2498810498,issue,closed,completed,ValueError: Device /job:localhost/replica:0/task:0/device:CPU:0 is not found,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I running my code on the TPU at Kaggle , I have the following problem: `ValueError: Device /job:localhost/replica:0/task:0/device:CPU:0 is not found`
## Complete error report:
```
File /usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/packed_distributed_variable.py:91, in PackedDistributedVariable.get_var_on_device(self, device)
     89   if d == device:
     90     return self._distributed_variables[i]
---> 91 raise ValueError(""Device %s is not found"" % device)

ValueError: Device /job:localhost/replica:0/task:0/device:CPU:0 is not found
```

## My TPU configuration code:
```python
try: 
    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()#.connect(tpu='local')
    print(f'Running on the TPU: {TPU.master()}')
except ValueError:
    print('TPU is not avalibale!!')
    TPU = None
    
if TPU:
    tf.config.experimental_connect_to_cluster(TPU)
    tf.tpu.experimental.initialize_tpu_system(TPU)
    strategy = tf.distribute.experimental.TPUStrategy(TPU)
else :
    strategy = tf.distribute.get_strategy()

```



### Standalone code to reproduce the issue

```shell
The above errors occur in various environments of kaggle, so I suspect that there is a problem with the official framework source code. I implore the official to give a suitable solution!! Thanks!!
```


### Relevant log output

_No response_",StarxSky,2024-08-31 11:58:59+00:00,['Venkat6871'],2024-09-21 01:58:37+00:00,2024-09-21 01:58:32+00:00,https://github.com/tensorflow/tensorflow/issues/74926,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:tpus', 'tpu, tpuestimator'), ('TF 2.16', '')]","[{'comment_id': 2322878386, 'issue_id': 2498810498, 'author': 'StarxSky', 'body': ""Please pay attention, the device job:localhost/replica:0/task:0/device:CPU:0  is exists.\r\n(tf.device('/job:localhost/replica:0/task:0/device:CPU:0 ') it's okay.)"", 'created_at': datetime.datetime(2024, 8, 31, 12, 9, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330862168, 'issue_id': 2498810498, 'author': 'Venkat6871', 'body': 'Hi **@StarxSky** ,\r\nI tried to run code on colab using TF 2.15 & 2.17 and i am not facing any issue with 2.15 but i am facing issue with 2.17. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/49ddf513be87ec6a210b25ec67813128/74926_2-15-0-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 5, 7, 59, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350779481, 'issue_id': 2498810498, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 14, 1, 57, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921116, 'issue_id': 2498810498, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921204, 'issue_id': 2498810498, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74926"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74926"">No</a>', 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 36, tzinfo=datetime.timezone.utc)}]","StarxSky (Issue Creator) on (2024-08-31 12:09:27 UTC): Please pay attention, the device job:localhost/replica:0/task:0/device:CPU:0  is exists.
(tf.device('/job:localhost/replica:0/task:0/device:CPU:0 ') it's okay.)

Venkat6871 (Assginee) on (2024-09-05 07:59:26 UTC): Hi **@StarxSky** ,
I tried to run code on colab using TF 2.15 & 2.17 and i am not facing any issue with 2.15 but i am facing issue with 2.17. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/49ddf513be87ec6a210b25ec67813128/74926_2-15-0-v.ipynb) here for reference.
Thank you!

github-actions[bot] on (2024-09-14 01:57:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-21 01:58:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-21 01:58:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74926"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74926"">No</a>

"
2498664659,issue,open,,tensorflow.python.ops.gen_math_ops.sparse_bincount can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a `Segmentation fault ` issue in TensorFlow when using the `gen_math_ops.sparse_bincount` API. I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import gen_math_ops

values = [0, 1, 2, 2]
binary = False
indices = [[], [], [990, 2], [2, 349]]
dense_shape = []
gen_math_ops.sparse_bincount(indices=indices, values=values, dense_shape=dense_shape, size=3, weights=[],binary_output=binary)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",KnightGOKU,2024-08-31 06:18:00+00:00,['Venkat6871'],2024-10-01 13:01:24+00:00,,https://github.com/tensorflow/tensorflow/issues/74918,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2385724984, 'issue_id': 2498664659, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/0eb86ea270ee7dc9d9e6bf281b930021/74918_tf-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 1, 13, 1, 15, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-01 13:01:15 UTC): I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find [gist](https://colab.sandbox.google.com/gist/Venkat6871/0eb86ea270ee7dc9d9e6bf281b930021/74918_tf-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

"
2498649139,issue,closed,completed, `Aborted` issue raised in TensorFlow when using data_flow_ops.SparseConditionalAccumulator and apply_indexed_slices_grad with mismatched data types,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `Aborted` issue in TensorFlow when using the `data_flow_ops.SparseConditionalAccumulator` API and `apply_indexed_slices_grad` with mismatched data types. The code was confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)
> 2024-08-31 13:45:53.349441: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (23 vs. 1) float expected, got uint64
Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.framework import indexed_slices
from tensorflow.python.framework import tensor_shape
from tensorflow.python.ops import data_flow_ops
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()

with sess.as_default():
    q = data_flow_ops.SparseConditionalAccumulator(dtypes_lib.float32, name='Q', shape=tensor_shape.TensorShape([3, 3]))
    accum_op = q.apply_indexed_slices_grad(indexed_slices.IndexedSlices(indices=[0, 2], values=np.array([[0, 0, 1], [3, 0, 4]]).astype(np.uint64)))
    accum_op.run()
```


### Relevant log output

```shell
2024-08-31 13:45:53.349441: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (23 vs. 1) float expected, got uint64
Aborted (core dumped)
```
",KnightGOKU,2024-08-31 05:41:07+00:00,['tilakrayal'],2024-11-13 06:11:07+00:00,2024-11-10 02:03:35+00:00,https://github.com/tensorflow/tensorflow/issues/74917,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437512214, 'issue_id': 2498649139, 'author': 'tilakrayal', 'body': '@KnightGOKU,\r\nI tried to execute the mentioned code on tf-nightly and observed that the check fail is happening due to mismatch of the data type provided as input. I tried with the similar input data type and the code was executed without abort/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3f663ffbe0bb69af5586ae3a60073c5d/untitled2203.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 10, 25, 11, 11, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797190, 'issue_id': 2498649139, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547385, 'issue_id': 2498649139, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466547425, 'issue_id': 2498649139, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74917"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74917"">No</a>', 'created_at': datetime.datetime(2024, 11, 10, 2, 3, 38, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-25 11:11:05 UTC): @KnightGOKU,
I tried to execute the mentioned code on tf-nightly and observed that the check fail is happening due to mismatch of the data type provided as input. I tried with the similar input data type and the code was executed without abort/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3f663ffbe0bb69af5586ae3a60073c5d/untitled2203.ipynb). Thank you!

github-actions[bot] on (2024-11-02 02:01:04 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-10 02:03:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-10 02:03:38 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74917"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74917"">No</a>

"
2498640369,issue,open,,tensorflow.python.ops.state_ops.scatter_nd_update can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a `Segmentation fault` in TensorFlow when I used API `state_ops.scatter_nd_update`  with empty indices.
I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.ops import state_ops


def testSimpleResource():
    indices = constant_op.constant([], dtype=dtypes.int32)
    for dtype in (dtypes.int32, dtypes.bfloat16):
        updates = constant_op.constant([], dtype=dtype)
        ref = resource_variable_ops.ResourceVariable((0, 0, 0, 0, 0, 0, 0, 0), dtype=dtype)
        scatter = state_ops.scatter_nd_update(ref, indices, updates)

testSimpleResource()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",KnightGOKU,2024-08-31 05:23:34+00:00,['tilakrayal'],2024-11-08 08:07:00+00:00,,https://github.com/tensorflow/tensorflow/issues/74915,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2437465891, 'issue_id': 2498640369, 'author': 'tilakrayal', 'body': '@KnightGOKU,\r\nI tried to execute the mentioned code on tensorflow v2.17 by providing the input for the indices and observed it was executed as intended. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/b50a5acb11094c7b05ba749890cad03c/untitled2202.ipynb) and also please let us know whether any specific use case to provide the empty input. Thank you!', 'created_at': datetime.datetime(2024, 10, 25, 10, 45, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797204, 'issue_id': 2498640369, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452854842, 'issue_id': 2498640369, 'author': 'KnightGOKU', 'body': '@tilakrayal Thank you for confirming. This issue occurs in both v2.17 and tf-nightly. [gist](https://colab.research.google.com/drive/1Tc7MLvsNCZkTSo822Mcyr3AkDsHI-tVQ?usp=sharing)', 'created_at': datetime.datetime(2024, 11, 2, 4, 26, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-25 10:45:08 UTC): @KnightGOKU,
I tried to execute the mentioned code on tensorflow v2.17 by providing the input for the indices and observed it was executed as intended. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/b50a5acb11094c7b05ba749890cad03c/untitled2202.ipynb) and also please let us know whether any specific use case to provide the empty input. Thank you!

github-actions[bot] on (2024-11-02 02:01:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

KnightGOKU (Issue Creator) on (2024-11-02 04:26:44 UTC): @tilakrayal Thank you for confirming. This issue occurs in both v2.17 and tf-nightly. [gist](https://colab.research.google.com/drive/1Tc7MLvsNCZkTSo822Mcyr3AkDsHI-tVQ?usp=sharing)

"
2498614527,issue,closed,completed,TensorFlow would crash when using `nn_ops.conv2d` with too large strides ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `nn_ops.conv2d`  with strides that were too large. 

> 2024-08-31 12:25:59.983173: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2703] Non-OK-status: GetNodeAttr(orig_node->def(), ""strides"", &strides)
> Status: INVALID_ARGUMENT: Attr strides has value 4634247419717959497 out of range for an int32
> Aborted (core dumped)

I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import constant_op
from tensorflow.python.ops import nn_ops


def _CompareFwdConv2D(tensor_in_sizes, filter_in_sizes, conv_strides, padding):

    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)
    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)
    t1 = constant_op.constant(x1, shape=tensor_in_sizes)
    t2 = constant_op.constant(x2, shape=filter_in_sizes)
    strides = [1] + conv_strides + [1]
    conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)


def _RunTestCases(conv_strides, padding):
    input_sizes = [[5, 5, 5, 1248]]
    filter_sizes = [[3, 3, 1248, 128]]
    for (input_shape, filter_shape) in zip(input_sizes, filter_sizes):
        _CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)


def testConv2D3x3FilterStride1x1Same():
    _RunTestCases([4634247419717959497, 1], 'SAME')


testConv2D3x3FilterStride1x1Same()
```


### Relevant log output

_No response_",KnightGOKU,2024-08-31 04:23:52+00:00,['tilakrayal'],2024-09-26 08:10:45+00:00,2024-09-26 08:10:41+00:00,https://github.com/tensorflow/tensorflow/issues/74914,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2374047336, 'issue_id': 2498614527, 'author': 'tilakrayal', 'body': '@KnightGOKU,\r\nI tried to execute the mentioned code on both tensorflow2.17 and tf-nightly and observed that the colab was not crashed & it was providing the error output where ""Attr strides has value 4634247419717959497 out of range for an int32"" which was expected.  Kindly find the gist it [here](https://colab.research.google.com/gist/tilakrayal/043475c43ea9256d590d3e97ded5d4c7/untitled2125.ipynb).\r\n\r\nIt seems like you\'re giving large negative value for multiples argument to the function nn_ops so due to Integer overflow to buffer overflow or due to insufficient memory (RAM), code is crashing or process getting killed and It\'s not being killed because of TF. You are literally allocating so much memory that the OS is killing the process. Thank you!', 'created_at': datetime.datetime(2024, 9, 25, 13, 10, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376236941, 'issue_id': 2498614527, 'author': 'KnightGOKU', 'body': 'Thank you for your time. I will close this issue as the crash has been determined not to be caused by TensorFlow.', 'created_at': datetime.datetime(2024, 9, 26, 8, 10, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376237005, 'issue_id': 2498614527, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74914"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74914"">No</a>', 'created_at': datetime.datetime(2024, 9, 26, 8, 10, 43, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-25 13:10:33 UTC): @KnightGOKU,
I tried to execute the mentioned code on both tensorflow2.17 and tf-nightly and observed that the colab was not crashed & it was providing the error output where ""Attr strides has value 4634247419717959497 out of range for an int32"" which was expected.  Kindly find the gist it [here](https://colab.research.google.com/gist/tilakrayal/043475c43ea9256d590d3e97ded5d4c7/untitled2125.ipynb).

It seems like you're giving large negative value for multiples argument to the function nn_ops so due to Integer overflow to buffer overflow or due to insufficient memory (RAM), code is crashing or process getting killed and It's not being killed because of TF. You are literally allocating so much memory that the OS is killing the process. Thank you!

KnightGOKU (Issue Creator) on (2024-09-26 08:10:41 UTC): Thank you for your time. I will close this issue as the crash has been determined not to be caused by TensorFlow.

google-ml-butler[bot] on (2024-09-26 08:10:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74914"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74914"">No</a>

"
2498613211,issue,closed,completed,"Aborted (core dumped) Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `nn_ops.conv2d_backprop_filter` .
> 2024-08-31 12:22:03.927406: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C
> Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import test_util


from tensorflow.python.ops import nn_ops


def GetTestConfigs():
    test_configs = [('NHWC', False), ('NHWC', True)]
    return test_configs

def _DtypesToTest(use_gpu):
    if use_gpu:
        out = [dtypes.float32, dtypes.bfloat16]
        return out
    return [dtypes.float32, dtypes.float64, dtypes.float16, dtypes.bfloat16]

def _CreateNumpyTensor(shape):
    total_size = 1
    for s in shape:
        total_size *= s
    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)


def testConv2D2x2Depth1ValidBackpropFilter():
    expected = (5.0, 8.0, 14.0, 17.0)
    for [data_format, use_gpu] in GetTestConfigs():
        _RunAndVerifyBackpropFilter(input_sizes=[], filter_sizes=(2, 2, 1, 1), output_sizes=[1, 1, 2], strides=[], padding='VALID', expected=expected, data_format=data_format, use_gpu=use_gpu)

def _RunAndVerifyBackpropFilter(input_sizes, filter_sizes, output_sizes, strides, padding, expected, data_format, use_gpu, dilations=(1, 1), err=1e-05):
    x0 = _CreateNumpyTensor(input_sizes)
    x2 = _CreateNumpyTensor(output_sizes)
    dilations = list(dilations)
    explicit_strides = [1] + strides + [1]
    new_padding = padding
    new_dilations = [1] + dilations + [1]
    if isinstance(new_padding, (list, tuple)):
        new_padding = [(0, 0)] + new_padding + [(0, 0)]
    if data_format == 'NCHW':
        explicit_strides = test_util.NHWCToNCHW(explicit_strides)
        new_dilations = test_util.NHWCToNCHW(new_dilations)
        if isinstance(padding, (list, tuple)):
            new_padding = test_util.NHWCToNCHW(new_padding)
    for dtype in _DtypesToTest(use_gpu=use_gpu):
        with test_util.device(use_gpu):
            t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)
            t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])
            t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)
            if data_format == 'NCHW':
                t0 = test_util.NHWCToNCHW(t0)
                t2 = test_util.NHWCToNCHW(t2)
            # the following line would cause `Aborted (core dumped)`
            conv = nn_ops.conv2d_backprop_filter(t0, t1, t2, strides=explicit_strides, padding=new_padding, dilations=new_dilations, data_format=data_format)


testConv2D2x2Depth1ValidBackpropFilter()
```


### Relevant log output

```shell
> 2024-08-31 12:22:03.927406: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C
> Aborted (core dumped)
```
I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)",KnightGOKU,2024-08-31 04:20:19+00:00,['Venkat6871'],2024-09-20 12:23:22+00:00,2024-09-20 12:23:18+00:00,https://github.com/tensorflow/tensorflow/issues/74913,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2323322654, 'issue_id': 2498613211, 'author': 'KnightGOKU', 'body': ""The following code can also trigger the same abort fault:\r\n\r\nCode1:\r\n```python\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import nn_ops\r\nimport tensorflow as tf\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n    strides = [1, 926, 2, 2, 1]\r\n    x_shape = [2, 1, 6, 4, 3]\r\n    y_shape = []\r\n    f_shape = []\r\n    x = constant_op.constant(1.0, shape=x_shape, name='x', dtype=dtypes.float32)\r\n    f = constant_op.constant(1.0, shape=f_shape, name='filter', dtype=dtypes.float32)\r\n    output = nn_ops.conv3d_transpose(x, f, y_shape, strides=strides, padding='SAME')\r\n```\r\n> 2024-09-01 20:49:25.826734: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\r\nAborted (core dumped)\r\n\r\nCode2:\r\n```python\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import nn_ops\r\n\r\nimport tensorflow as tf\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n    input_shape = []\r\n    total_input_size = 1\r\n    for s in input_shape:\r\n        total_input_size *= s\r\n    inputs = [i * 1.0 / total_input_size for i in range(1, total_input_size + 1)]\r\n    a = constant_op.constant(inputs, shape=input_shape, dtype=dtypes.float32)\r\n    filter_shape = (1, 1, 1, 8, 8)\r\n    total_filter_size = 1\r\n    for s in filter_shape:\r\n        total_filter_size *= s\r\n    filters = [i * 1.0 / total_filter_size for i in range(1, total_filter_size + 1)]\r\n    f = constant_op.constant(filters, shape=filter_shape, dtype=dtypes.float32)\r\n    conv_t = nn_ops.conv3d(a, filter=f, strides=(1, 1, 1, 1, 1), padding='VALID')\r\n    slice_t = array_ops.slice(conv_t, (0, 1, 1, 1, 0), (1, 1, 1, 1, 8))\r\n```\r\n\r\n> 2024-09-01 20:55:30.319578: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\r\nAborted (core dumped)"", 'created_at': datetime.datetime(2024, 9, 1, 12, 38, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363407223, 'issue_id': 2498613211, 'author': 'Venkat6871', 'body': 'Hi **@KnightGOKU** ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 with GPU and the nightly versions. It is raising an error for your code snippet. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/11c07081ae3e72c9e79e559dfc779ded/74913_tf-2-17-0-nightly-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/82287b32255d137c2d3f808b33a9858f/74913_tf-2-17-0-gpu.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 20, 10, 34, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363603020, 'issue_id': 2498613211, 'author': 'KnightGOKU', 'body': ""Hi @Venkat6871 , thank you for your time. I found that the code could not reproduce the crash in Google Colab, but it does crash on my local machine. I’m still unsure why this happens. I'll close the issue for now and will reopen it if I discover the cause. Thanks again!"", 'created_at': datetime.datetime(2024, 9, 20, 12, 23, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363603085, 'issue_id': 2498613211, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74913"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74913"">No</a>', 'created_at': datetime.datetime(2024, 9, 20, 12, 23, 21, tzinfo=datetime.timezone.utc)}]","KnightGOKU (Issue Creator) on (2024-09-01 12:38:03 UTC): The following code can also trigger the same abort fault:

Code1:
```python
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import nn_ops
import tensorflow as tf
sess = tf.compat.v1.Session()
with sess.as_default():
    strides = [1, 926, 2, 2, 1]
    x_shape = [2, 1, 6, 4, 3]
    y_shape = []
    f_shape = []
    x = constant_op.constant(1.0, shape=x_shape, name='x', dtype=dtypes.float32)
    f = constant_op.constant(1.0, shape=f_shape, name='filter', dtype=dtypes.float32)
    output = nn_ops.conv3d_transpose(x, f, y_shape, strides=strides, padding='SAME')
```
Aborted (core dumped)

Code2:
```python
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import nn_ops

import tensorflow as tf
sess = tf.compat.v1.Session()
with sess.as_default():
    input_shape = []
    total_input_size = 1
    for s in input_shape:
        total_input_size *= s
    inputs = [i * 1.0 / total_input_size for i in range(1, total_input_size + 1)]
    a = constant_op.constant(inputs, shape=input_shape, dtype=dtypes.float32)
    filter_shape = (1, 1, 1, 8, 8)
    total_filter_size = 1
    for s in filter_shape:
        total_filter_size *= s
    filters = [i * 1.0 / total_filter_size for i in range(1, total_filter_size + 1)]
    f = constant_op.constant(filters, shape=filter_shape, dtype=dtypes.float32)
    conv_t = nn_ops.conv3d(a, filter=f, strides=(1, 1, 1, 1, 1), padding='VALID')
    slice_t = array_ops.slice(conv_t, (0, 1, 1, 1, 0), (1, 1, 1, 1, 8))
```

Aborted (core dumped)

Venkat6871 (Assginee) on (2024-09-20 10:34:53 UTC): Hi **@KnightGOKU** ,
Apologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 with GPU and the nightly versions. It is raising an error for your code snippet. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/11c07081ae3e72c9e79e559dfc779ded/74913_tf-2-17-0-nightly-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/82287b32255d137c2d3f808b33a9858f/74913_tf-2-17-0-gpu.ipynb) here for reference.

Thank you!

KnightGOKU (Issue Creator) on (2024-09-20 12:23:19 UTC): Hi @Venkat6871 , thank you for your time. I found that the code could not reproduce the crash in Google Colab, but it does crash on my local machine. I’m still unsure why this happens. I'll close the issue for now and will reopen it if I discover the cause. Thanks again!

google-ml-butler[bot] on (2024-09-20 12:23:21 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74913"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74913"">No</a>

"
2498612163,issue,open,, Aborted (core dumped): Check failed: d < dims() (1 vs. 1),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.scatter_nd` . The code is as follows:

```python
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```

> 2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
> Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```


### Relevant log output

```shell
2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)",KnightGOKU,2024-08-31 04:18:19+00:00,['tilakrayal'],2024-10-10 06:07:27+00:00,,https://github.com/tensorflow/tensorflow/issues/74912,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2363611185, 'issue_id': 2498612163, 'author': 'KnightGOKU', 'body': '@tilakrayal Hi~ could you reproduce this issue? : )', 'created_at': datetime.datetime(2024, 9, 20, 12, 26, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363893811, 'issue_id': 2498612163, 'author': 'tilakrayal', 'body': '@KnightGOKU,\r\nApologies for the delay. I was able to reproduce the issue on tensorflow v.2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/05692b2220c91848c380d0fcc66024c9/untitled2120.ipynb) and also please allow some time period to debug the issue and provide the update. \r\n\r\n![Screenshot 2024-09-20 8 08 24 PM](https://github.com/user-attachments/assets/860448db-e834-4bd0-be28-1a6a0e357c2e)\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 20, 14, 39, 31, tzinfo=datetime.timezone.utc)}]","KnightGOKU (Issue Creator) on (2024-09-20 12:26:33 UTC): @tilakrayal Hi~ could you reproduce this issue? : )

tilakrayal (Assginee) on (2024-09-20 14:39:31 UTC): @KnightGOKU,
Apologies for the delay. I was able to reproduce the issue on tensorflow v.2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/05692b2220c91848c380d0fcc66024c9/untitled2120.ipynb) and also please allow some time period to debug the issue and provide the update. 

![Screenshot 2024-09-20 8 08 24 PM](https://github.com/user-attachments/assets/860448db-e834-4bd0-be28-1a6a0e357c2e)


Thank you!

"
2497918545,issue,closed,completed,Pose missmatch,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

0.0.1-nightly.20240118

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello, 

        I am using pose estimation model to detect poses in iOS app using TensorFlow pose estimation model. But facing some issue while performing different poses.
![WhatsApp Image 2024-08-30 at 5 10 34 PM](https://github.com/user-attachments/assets/e60cd474-a504-4a85-b8d5-69961c6086c5)
![WhatsApp Image 2024-08-30 at 9 03 24 PM](https://github.com/user-attachments/assets/6fe69180-2bf4-41a4-8d90-27ec66e5152b)
![WhatsApp Image 2024-08-30 at 5 11 29 PM](https://github.com/user-attachments/assets/5674c02a-7a6e-427f-a17e-251bd5649506)
![WhatsApp Image 2024-08-30 at 9 03 24 PM](https://github.com/user-attachments/assets/57ae0368-9909-42bf-8eb2-ff2fb613a901)


### Standalone code to reproduce the issue

```shell
How can I resolve issue?
```


### Relevant log output

_No response_",IBSApple,2024-08-30 18:09:36+00:00,['Venkat6871'],2024-09-18 01:58:41+00:00,2024-09-18 01:58:35+00:00,https://github.com/tensorflow/tensorflow/issues/74894,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2325879659, 'issue_id': 2497918545, 'author': 'Venkat6871', 'body': 'Hi **@IBSApple** ,\r\nSorry for the delay, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 3, 8, 14, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342462889, 'issue_id': 2497918545, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 57, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336170, 'issue_id': 2497918545, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336298, 'issue_id': 2497918545, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74894"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74894"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 40, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-03 08:14:46 UTC): Hi **@IBSApple** ,
Sorry for the delay, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.

Thank you!

github-actions[bot] on (2024-09-11 01:57:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74894"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74894"">No</a>

"
2497911538,issue,open,,tf.math.floordiv produces incorrect result when the denominator is `-inf`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation https://www.tensorflow.org/api_docs/python/tf/math/floordiv, `tf.math.floordiv` should be equivalent to python's `//` operator. However, when the `x=1.4` and `y=-np.inf`, `tf.math.floordiv` outputs `-0.0` while `//` outputs `-1.0`.

I also checked Numpy and PyTorch's APIs, both output `-1.0`. 

It seems that the implementation of `tf.math.floordiv` is different from others, it would be nice if you can fix the implementation inconsistency, or make this inconsistency in the documentation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import torch
import numpy as np
a = 1.4
b = -np.inf
print(""Numpy's result: "", np.floor_divide(a, b))
print(""Python's // result: "", a // b)
print(f""TF's result: {tf.math.floordiv(a, b)}"")
print(f""PyTorch's result: {torch.floor_divide(torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32))}"")
```
```


### Relevant log output

```shell
Numpy's result:  -1.0
Python's // result:  -1.0
TF's result: -0.0
PyTorch's result: -1.0
```
",maybeLee,2024-08-30 18:05:02+00:00,['tilakrayal'],2024-09-12 06:17:08+00:00,,https://github.com/tensorflow/tensorflow/issues/74893,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2323895030, 'issue_id': 2497911538, 'author': 'tilakrayal', 'body': '@maybeLee,\r\nThank you for reporting the issue. I tried to reproduce the code on both tensorflowv2.17 & tf-nightly and observed the difference in the output. Please allow to deep dive on the same and provide the update on the same. Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 6, 14, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345361131, 'issue_id': 2497911538, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d6d0cb855830aab4050dee058f4390ff/untitled2103.ipynb).', 'created_at': datetime.datetime(2024, 9, 12, 6, 17, 1, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-02 06:14:11 UTC): @maybeLee,
Thank you for reporting the issue. I tried to reproduce the code on both tensorflowv2.17 & tf-nightly and observed the difference in the output. Please allow to deep dive on the same and provide the update on the same. Thank you!

tilakrayal (Assginee) on (2024-09-12 06:17:01 UTC): I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d6d0cb855830aab4050dee058f4390ff/untitled2103.ipynb).

"
2497549394,issue,open,,[TFLite] Could log level control be added to the TFLite C API?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[`tflite::LoggerOptions::SetMinimumLogSeverity()`](https://github.com/tensorflow/tensorflow/blob/daa9e4e0d0a04626fb97cf9b11c1ae6be46c6517/tensorflow/lite/logger.h#L37) provides a method for controlling the log level in C++.
This allows more detailed logging than INFO or limits logging to ERROR or higher in prod builds.

However, we cannot use this feature from the C API. 
I think this enhancement for the C API is useful and good for future additions on other platforms, such as Swift.
Is it possible to add bindings for the `tflite::LoggerOptions` class to the C API ?

I am sorry if I missed this features of the C API.
I will be happy to help you, for example, try to create a PR.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_",kokeshing,2024-08-30 15:34:02+00:00,['pkgoogle'],2024-10-02 20:12:55+00:00,,https://github.com/tensorflow/tensorflow/issues/74882,"[('stat:contribution welcome', 'Status - Contributions welcome'), ('awaiting review', 'Pull request awaiting review'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2377174139, 'issue_id': 2497549394, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 26, 14, 42, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2377919264, 'issue_id': 2497549394, 'author': 'pkgoogle', 'body': 'Hi @kokeshing, we would definitely welcome a PR if you can.', 'created_at': datetime.datetime(2024, 9, 26, 20, 54, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381117843, 'issue_id': 2497549394, 'author': 'kokeshing', 'body': ""Hi @pkgoogle \r\n\r\nThanks. I'll try to create a PR.\r\nI will be working on this in my free time, so I appreciate your patience as it might take some time."", 'created_at': datetime.datetime(2024, 9, 29, 5, 46, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389213719, 'issue_id': 2497549394, 'author': 'kokeshing', 'body': 'Hi @pkgoogle \r\n\r\nI submitted https://github.com/tensorflow/tensorflow/pull/76970. \r\nI have not yet confirmed that all the necessary CIs are passed, so I will be watching whether I still need to take action.\r\n\r\nThis is my first Contribute and it seems rare binding APIs below the C++ namespace, so I would appreciate your review.', 'created_at': datetime.datetime(2024, 10, 2, 17, 23, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389593070, 'issue_id': 2497549394, 'author': 'pkgoogle', 'body': 'There will be a review process so no worries, thanks for contributing! And congrats on your first.', 'created_at': datetime.datetime(2024, 10, 2, 20, 12, 37, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-09-26 14:42:24 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-26 20:54:44 UTC): Hi @kokeshing, we would definitely welcome a PR if you can.

kokeshing (Issue Creator) on (2024-09-29 05:46:53 UTC): Hi @pkgoogle 

Thanks. I'll try to create a PR.
I will be working on this in my free time, so I appreciate your patience as it might take some time.

kokeshing (Issue Creator) on (2024-10-02 17:23:26 UTC): Hi @pkgoogle 

I submitted https://github.com/tensorflow/tensorflow/pull/76970. 
I have not yet confirmed that all the necessary CIs are passed, so I will be watching whether I still need to take action.

This is my first Contribute and it seems rare binding APIs below the C++ namespace, so I would appreciate your review.

pkgoogle (Assginee) on (2024-10-02 20:12:37 UTC): There will be a review process so no worries, thanks for contributing! And congrats on your first.

"
2497265049,issue,closed,completed,Adam optimiser fails because of negative v_hat values leading to nan in weight update,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN version 8907

### GPU model and memory

Quadro RTX 8000, 48GB

### Current behavior?

Adam leads to nan values in UNet while SGD works fine. On manual implementation and debugging, I have found the problem to be in the calculation of v_hat. For whatever reason, v_hat is becoming negative at the beta_2*v_hat stage. After this all calculation collapses. The addition is incorrect as well. The whole code repository is too hard to simplify to give a standalone test case, but i have attached the training code. This issue should be model agnostic, as it is happening in the optimiser. 

To note: I tried tf.keras.optimizers.Adam with various parameters and then implemented my own version to debug and found the issues mentioned below. 
Another note: This happens randomly. Sometimes it will happen after 10 iterations, sometimes after 2000.

This is quite unexpected behaviour from a library used by so many people in such a common routine. Please suggest possible resolution. 

Thanks 

### Standalone code to reproduce the issue

```shell
if self.networkMode == 'training':
            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.stgs.START_LEARNING_RATE, clipvalue=2.0, epsilon=1e-3)
            list_losses = []
            list_weights = []


            gradsTexNet = tape.gradient(finalLoss, texNet.model.trainable_weights)
            # optimizer.apply_gradients(zip(gradsTexNet, texNet.model.trainable_weights))
            grads_copy = [tf.identity(g) for g in gradsTexNet]
            weights_copy = [tf.identity(w) for w in texNet.model.trainable_weights]

            weight_update_list = []
            m_hat_list = []
            v_hat_list = []
            s1 = []
            s2 = []
            for ioy, weight in enumerate(texNet.model.trainable_weights):
                self.t += 1.0 # Increment time step

                # Update biased first moment estimate
                self.m[ioy].assign(self.beta_1 * self.m[ioy] + (1 - self.beta_1) * gradsTexNet[ioy])
                s1.append((1 - self.beta_2) * tf.square(gradsTexNet[ioy]))
                s2.append(self.beta_2 * self.v[ioy])
                # Update biased second moment estimate
                self.v[ioy].assign(self.beta_2 * self.v[ioy])
                self.v[ioy].assign_add((1 - self.beta_2) * tf.square(gradsTexNet[ioy]))
                
                # Compute bias-corrected first and second moment estimates
                m_hat = self.m[ioy] / (1 - (self.beta_1**self.t))
                v_hat = self.v[ioy] / (1 - (self.beta_2**self.t))
                
                # Update the weight
                weight_update = self.learning_rate * m_hat / (tf.sqrt(v_hat) + self.epsilon)

                m_hat_list.append(m_hat)
                v_hat_list.append(v_hat)
                weight_update_list.append(weight_update)

                weight.assign(weight - weight_update)
            weights_copy2 = [tf.identity(w) for w in texNet.model.trainable_weights]
```


### Relevant log output

```shell
The issue is in:
self.v[i].assign(self.beta_2 *self.v[i] + (1 - self.beta_2) *tf.square(gradsTexNet[i]))

it is self.beta_2 * self.v[i] which is becoming negative first. 
The values for self.beta_2 * self.v[i] become [-277.81274   243.22517  -335.06995   333.12036   -47.58728   340.61444 
  -58.375122  338.2406  ]

The values for (1 - self.beta_2) * tf.square(gradsTexNet[i]) become: [2.6129057e-05 3.6575788e-05 1.4987046e-08 3.2171758e-07 1.2511656e-05  1.8191919e-05 1.2237210e-05 6.4639935e-05]

The values of their sum don't line up and are this:
[-198.82468   -77.196434 -199.875    -537.16595  -119.70409  -448.5137  -115.79137  -342.62042 ]

If i divide them by (1 - beta_2**t) in tensorflow, the values become:
[ -49780.793  -19328.082  -50043.766 -134493.1    -29970.95  -112296.766   -28991.3    -85783.695]

but if i divide them using python, the values become this:
[-3586.765  -1392.6112 -3605.7126 -9690.387  -2159.4424 -8091.1143  -2088.8574 -6180.817 ]

The gradient values are:
[-0.16164485 -0.19124797  0.00387131  0.01793649 -0.11185551 -0.13487741  -0.11062192  0.25424385]
```
",simba611,2024-08-30 13:34:29+00:00,['tilakrayal'],2024-09-19 02:00:00+00:00,2024-09-19 01:59:57+00:00,https://github.com/tensorflow/tensorflow/issues/74879,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.16', '')]","[{'comment_id': 2328669892, 'issue_id': 2497265049, 'author': 'tilakrayal', 'body': '@simba611,\r\nCould you please share a reproducible code that supports your statement so that the issue can be debugged in the effective way? Thank you!', 'created_at': datetime.datetime(2024, 9, 4, 11, 25, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102096, 'issue_id': 2497265049, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826330, 'issue_id': 2497265049, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 1, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826397, 'issue_id': 2497265049, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74879"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74879"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 1, 59, 59, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-04 11:25:26 UTC): @simba611,
Could you please share a reproducible code that supports your statement so that the issue can be debugged in the effective way? Thank you!

github-actions[bot] on (2024-09-12 01:58:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 01:59:56 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 01:59:59 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74879"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74879"">No</a>

"
2496679561,issue,closed,completed,The difference in performance on the parameter activation='exponential' is too large,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu20.04

### Mobile device

Ubuntu20.04

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Grad diff too big

### Standalone code to reproduce the issue

```shell
import copy
import numpy as np
import tensorflow as tf


def lenet(input_shape):
    input_tensor = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Activation(activation=""relu"")(input_tensor)
    x = tf.keras.layers.MaxPool2D(pool_size=8, strides=3, padding='same')(x)
    x = tf.keras.layers.Activation(activation='sigmoid')(x)
    x = tf.keras.layers.MaxPool2D(pool_size=5, strides=1, padding='same')(x)
    x = tf.keras.layers.Activation(activation='softplus')(x)
    x = tf.keras.layers.MaxPooling2D(pool_size=7, strides=8, padding='same', data_format='channels_last')(x)
    x = tf.keras.layers.Flatten(data_format='channels_first')(x)
    x = tf.keras.layers.Dense(units=2, activation='exponential', kernel_constraint=None, bias_regularizer=None, use_bias=False, bias_initializer='he_uniform', activity_regularizer=None, kernel_initializer='ones', bias_constraint=None, kernel_regularizer=None)(x)
    x = tf.keras.layers.Dense(units=7, activation='exponential', use_bias=True, kernel_initializer='glorot_uniform', kernel_constraint=None, bias_regularizer=None)(x)
    output_tensor = tf.keras.layers.Flatten(data_format='channels_first')(x)
    tail_flatten = tf.keras.layers.Flatten()(output_tensor)
    tail_fc = tf.keras.layers.Dense(units=10)(tail_flatten)
    model = tf.keras.models.Model(inputs=input_tensor, outputs=tail_fc)
    return model


def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


def train(inp, label):
    flag = True
    label = tf.convert_to_tensor(label)
    model_g = lenet(inp.shape[1:])
    model_g.load_weights(""./output_dict/grad_diff_initial_weights.h5"")
    with tf.device('GPU'):
        with tf.GradientTape() as tape:
            output_g = model_g(inp)
            loss_g = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(label, output_g)
        gradients_g = tape.gradient(loss_g, model_g.trainable_variables)
        gradients_dic_g = {}
        for var, gradient in zip(model_g.trainable_variables, gradients_g):
            if gradient != None:
                gradients_dic_g.setdefault(var.name.replace('/', '.')[:-2], gradient)

    model_c = copy.deepcopy(model_g)
    model_c.load_weights(""./output_dict/grad_diff_initial_weights.h5"")
    with tf.device('CPU'):
        with tf.GradientTape() as tape:
            output_c = model_c(inp)
            loss_c = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(label, output_c)
        gradients_c = tape.gradient(loss_c, model_c.trainable_variables)
        gradients_dic_c = {}
        for var, gradient in zip(model_c.trainable_variables, gradients_c):
            if gradient != None:
                gradients_dic_c.setdefault(var.name.replace('/', '.')[:-2], gradient)
    if chebyshev_distance(output_c.numpy(), output_g.numpy()) > 1.0:
        flag = False
        return flag, 'Output diff too big'
    if abs(loss_c - loss_g) > 0.1:
        flag = False
        return flag, 'Loss diff too big'
    for name in gradients_dic_c.keys():
        if name in gradients_dic_g.keys():
            if chebyshev_distance(gradients_dic_c[name], gradients_dic_g[name]) > 0.1:
                flag = False
                return flag, 'Grad diff too big'
    for name in gradients_dic_g.keys():
        if name in gradients_dic_c.keys():
            if chebyshev_distance(gradients_dic_g[name], gradients_dic_c[name]) > 0.1:
                flag = False
                return flag, 'Grad diff too big'
    return flag, ''


data = np.load(""./output_dict/grad_diff_input.npz"")
inp = data['inp']
label = data['label']
print(train(inp, label))
```


### Relevant log output

[Reproduction](https://github.com/PhyllisJi/MoCoDiff_Bug/tree/tf-issue-%2374862)",BiophiliaSWDA,2024-08-30 09:14:14+00:00,['Venkat6871'],2025-01-19 02:03:32+00:00,2025-01-19 02:03:30+00:00,https://github.com/tensorflow/tensorflow/issues/74862,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2326059317, 'issue_id': 2496679561, 'author': 'Venkat6871', 'body': 'Hi **@BiophiliaSWDA** ,\r\nSorry for the delay, I reproduced the code shared but facing different error. Could you please share the colab gist with all the dependencies to analyze more of it.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 3, 9, 35, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333978911, 'issue_id': 2496679561, 'author': 'PhyllisJi', 'body': '> Hi **@BiophiliaSWDA** , Sorry for the delay, I reproduced the code shared but facing different error. Could you please share the colab gist with all the dependencies to analyze more of it. Thank you!\r\n\r\nI was unable to reproduce the issues I encountered on colab, probably due to the different GPU. I used Ubuntu 20.04, Python 3.10, Tensorflow 2.12, CUDA 11.8, cuDNN 8, and NVDIA 3090 GPU.', 'created_at': datetime.datetime(2024, 9, 6, 12, 48, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336209, 'issue_id': 2496679561, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357692280, 'issue_id': 2496679561, 'author': 'PhyllisJi', 'body': '> This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.\r\n\r\n@Venkat6871 Any update？', 'created_at': datetime.datetime(2024, 9, 18, 7, 18, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382238186, 'issue_id': 2496679561, 'author': 'Venkat6871', 'body': 'Hi **@BiophiliaSWDA** ,\r\nI tried running your code on Colab using TensorFlow 2.17.0 with GPU. Here is the [gist](https://colab.sandbox.google.com/gist/Venkat6871/bac425809adf705d446ce8acf4f981da/74862_tf-2-17-0-v-gpu.ipynb) for your reference. Could you please confirm whether you are facing a similar problem or a different one?\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 6, 41, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384875723, 'issue_id': 2496679561, 'author': 'PhyllisJi', 'body': '> Hi **@BiophiliaSWDA** , I tried running your code on Colab using TensorFlow 2.17.0 with GPU. Here is the [gist](https://colab.sandbox.google.com/gist/Venkat6871/bac425809adf705d446ce8acf4f981da/74862_tf-2-17-0-v-gpu.ipynb) for your reference. Could you please confirm whether you are facing a similar problem or a different one? Thank you!\r\n\r\nIt looks like this has been fixed in the latest version, is it 2.12.0 that has a problem related to mishandling of floating point numbers in exponential calculations?', 'created_at': datetime.datetime(2024, 10, 1, 6, 5, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568733429, 'issue_id': 2496679561, 'author': 'Venkat6871', 'body': 'Hi **@PhyllisJi** ,\r\nApologies for the delay, and thank you for your patience. The issue is related to an older version and has been fixed in the latest version. We always recommend using the latest versions for better results.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 3, 6, 6, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000780, 'issue_id': 2496679561, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600476024, 'issue_id': 2496679561, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 19, 2, 3, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600476070, 'issue_id': 2496679561, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74862"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74862"">No</a>', 'created_at': datetime.datetime(2025, 1, 19, 2, 3, 31, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-03 09:35:56 UTC): Hi **@BiophiliaSWDA** ,
Sorry for the delay, I reproduced the code shared but facing different error. Could you please share the colab gist with all the dependencies to analyze more of it.
Thank you!

PhyllisJi on (2024-09-06 12:48:26 UTC): I was unable to reproduce the issues I encountered on colab, probably due to the different GPU. I used Ubuntu 20.04, Python 3.10, Tensorflow 2.12, CUDA 11.8, cuDNN 8, and NVDIA 3090 GPU.

github-actions[bot] on (2024-09-18 01:58:37 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

PhyllisJi on (2024-09-18 07:18:49 UTC): @Venkat6871 Any update？

Venkat6871 (Assginee) on (2024-09-30 06:41:38 UTC): Hi **@BiophiliaSWDA** ,
I tried running your code on Colab using TensorFlow 2.17.0 with GPU. Here is the [gist](https://colab.sandbox.google.com/gist/Venkat6871/bac425809adf705d446ce8acf4f981da/74862_tf-2-17-0-v-gpu.ipynb) for your reference. Could you please confirm whether you are facing a similar problem or a different one?
Thank you!

PhyllisJi on (2024-10-01 06:05:43 UTC): It looks like this has been fixed in the latest version, is it 2.12.0 that has a problem related to mishandling of floating point numbers in exponential calculations?

Venkat6871 (Assginee) on (2025-01-03 06:06:36 UTC): Hi **@PhyllisJi** ,
Apologies for the delay, and thank you for your patience. The issue is related to an older version and has been fixed in the latest version. We always recommend using the latest versions for better results.
Thank you!

github-actions[bot] on (2025-01-11 02:01:49 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-19 02:03:29 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-19 02:03:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74862"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74862"">No</a>

"
2496286671,issue,closed,completed,Resizing the input shape in TensorFlow Lite C API,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.30

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi:
       we have a tflite model which input shape is float [1,256,256,3] ,   however ,  We want to run a 192x192x3 size image. 
using ""TfLiteInterpreterResizeInputTensor""  to resize input tensor from [1,256,256,3]  to  [1,192,192,3].
When I do the following code with the C API to run the model with my image input ,  got   failed   ""ERROR: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (36864 != 65536)"" ERROR: Node number 17 (RESHAPE) failed to prepare.
      We want to know is it the wrong way to use ""TfLiteInterpreterResizeInputTensor"" or something else

thanks
      


### Standalone code to reproduce the issue

```shell
#include <memory>
#include <string>
#include <vector>

#include ""tensorflow/lite/c/c_api.h""
#include ""tensorflow/lite/c/c_api_experimental.h""
#include ""tensorflow/lite/core/c/common.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""
#include ""tensorflow/lite/delegates/gpu/delegate_options.h""

#define STB_IMAGE_IMPLEMENTATION
#include ""stb_image.h""

#define STB_IMAGE_WRITE_IMPLEMENTATION
#include ""stb_image_write.h""

int main(void)
{
uint8_t *image = stbi_load(""192.jpg"", 192, 192, 3, 3);
int tmpSize = (192* 192*3);
float *in = (float *)malloc(tmpSize**sizeof(float)); 
float *out = (float *)malloc(tmpSize**sizeof(float)); 
for (int i = 0; i < tmpSize; i++)
{
    in[i]   = (float)(image[i] - 0.0f) / 255.0f;
}
//init model
TfLiteModel model = TfLiteModelCreateFromFile(info->ld.model_name);
TfLiteInterpreterOptions options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreterOptionsSetNumThreads(options, info->param.number_of_threads);
//gpu
TfLiteGpuDelegateOptionsV2 gpu_opts = TfLiteGpuDelegateOptionsV2Default();
gpu_opts.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;
gpu_opts.inference_priority1 =TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;
TfLiteDelegate gpudelegate = TfLiteGpuDelegateV2Create(&gpu_opts);
TfLiteInterpreterOptionsAddDelegate(options, gpudelegate);
TfLiteInterpreter interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);

//resize+invoke
int inputDims[4] = { 1, 192, 192,3};
TfLiteInterpreterResizeInputTensor(interpreter, 0, inputDims, 4); //resize
TfLiteInterpreterAllocateTensors(interpreter);  
TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
TfLiteTensorCopyFromBuffer(input_tensor, (void *)in, tmpSize*sizeof(float));
TfLiteInterpreterInvoke(interpreter);

//get output
TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 0);
TfLiteTensorCopyToBuffer(output_tensor, (void *)Out, tmpSize*sizeof(float) );

//deinit
TfLiteGpuDelegateV2Delete(gpudelegate);
TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);

}
```


### Relevant log output

_No response_",keke444,2024-08-30 05:39:46+00:00,['gaikwadrahul8'],2024-09-21 01:58:40+00:00,2024-09-21 01:58:34+00:00,https://github.com/tensorflow/tensorflow/issues/74843,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2323743224, 'issue_id': 2496286671, 'author': 'tilakrayal', 'body': '@keke444,\r\nTensorflow v2.13 is older version. Could you please try to test the code in the latest tensorflow v2.17 and provide the update if it is working in this case? Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 3, 29, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324189349, 'issue_id': 2496286671, 'author': 'keke444', 'body': '> @keke444, Tensorflow v2.13 is older version. Could you please try to test the code in the latest tensorflow v2.17 and provide the update if it is working in this case? Thank you!\r\n\r\nI used the latest tensorflow v2.17,  still got same failed  :\r\nERROR: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (36864 != 65536)\r\nERROR: Node number 17 (RESHAPE) failed to prepare.\r\nCould you please help us check it again?\r\n\r\nths', 'created_at': datetime.datetime(2024, 9, 2, 8, 55, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324972218, 'issue_id': 2496286671, 'author': 'gaikwadrahul8', 'body': 'Hi, @keke444 \r\n\r\nThank you for bringing this issue to our attention and as far I know the TFLite model should be able to handle different input sizes if it was designed with dynamic shapes in mind and please ensure that the new dimensions you\'re specifying in `inputDims` are compatible with the model\'s architecture. If the model expects a specific input shape( if TFLite model does not support dynamic shape inputs) and resizing to incompatible dimensions might lead to unexpected behavior or errors.\r\n\r\nHere’s a simple example in python to check if your model supports dynamic shapes:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef check_dynamic_shapes(model_path):\r\n    interpreter = tf.lite.Interpreter(model_path=model_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    print(""Input Details:"")\r\n    for detail in input_details:\r\n        print(f""Name: {detail[\'name\']}, Shape: {detail[\'shape\']}, Dynamic: {any(dim is None for dim in detail[\'shape\'])}"")\r\n\r\n    print(""Output Details:"")\r\n    for detail in output_details:\r\n        print(f""Name: {detail[\'name\']}, Shape: {detail[\'shape\']}, Dynamic: {any(dim is None for dim in detail[\'shape\'])}"")\r\n\r\n# Provide the path to your TFLite model\r\ncheck_dynamic_shapes(\'model.tflite\')\r\n\r\n```\r\n\r\nIf your converted TFLite model supports dynamic input shapes then it should accept various input shapes, if does not support before feeding the image to the model resize it to` [1, 256, 256, 3] `using an external library like `stb_image` or `OpenCV`. This way the image data matches the expected format of your model and eliminating the need for internal resizing because your model expect fixed input size of dimensions `[1,256,256,3]` and does not support dynamic input shapes so in that case you\'ll have to regenerate TFLite model with dynamic input dimensions\r\n\r\nIf I have missed something here please let me know ?\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 2, 15, 18, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330773771, 'issue_id': 2496286671, 'author': 'keke444', 'body': '> Hi, @keke444\r\n> \r\n> Thank you for bringing this issue to our attention and as far I know the TFLite model should be able to handle different input sizes if it was designed with dynamic shapes in mind and please ensure that the new dimensions you\'re specifying in `inputDims` are compatible with the model\'s architecture. If the model expects a specific input shape( if TFLite model does not support dynamic shape inputs) and resizing to incompatible dimensions might lead to unexpected behavior or errors.\r\n> \r\n> Here’s a simple example in python to check if your model supports dynamic shapes:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> def check_dynamic_shapes(model_path):\r\n>     interpreter = tf.lite.Interpreter(model_path=model_path)\r\n>     interpreter.allocate_tensors()\r\n> \r\n>     input_details = interpreter.get_input_details()\r\n>     output_details = interpreter.get_output_details()\r\n> \r\n>     print(""Input Details:"")\r\n>     for detail in input_details:\r\n>         print(f""Name: {detail[\'name\']}, Shape: {detail[\'shape\']}, Dynamic: {any(dim is None for dim in detail[\'shape\'])}"")\r\n> \r\n>     print(""Output Details:"")\r\n>     for detail in output_details:\r\n>         print(f""Name: {detail[\'name\']}, Shape: {detail[\'shape\']}, Dynamic: {any(dim is None for dim in detail[\'shape\'])}"")\r\n> \r\n> # Provide the path to your TFLite model\r\n> check_dynamic_shapes(\'model.tflite\')\r\n> ```\r\n> \r\n> If your converted TFLite model supports dynamic input shapes then it should accept various input shapes, if does not support before feeding the image to the model resize it to`[1, 256, 256, 3]`using an external library like `stb_image` or `OpenCV`. This way the image data matches the expected format of your model and eliminating the need for internal resizing because your model expect fixed input size of dimensions `[1,256,256,3]` and does not support dynamic input shapes so in that case you\'ll have to regenerate TFLite model with dynamic input dimensions\r\n> \r\n> If I have missed something here please let me know ?\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nI used the example  to check if our model supports dynamic shapes, the log shows below:\r\n""\r\nInput Details:\r\nName: input_10, Shape: [  1 256 256   3], Dynamic: False\r\nOutput Details:\r\nName: Identity, Shape: [ 3 32 32 16], Dynamic: False\r\nName: Identity_1, Shape: [ 3 16 16 16], Dynamic: False\r\nName: Identity_2, Shape: [ 3  8  8 16], Dynamic: False\r\n""\r\nit seems like our model doesn\'t support dynamic shapes.\r\n\r\nAccording to  "" Dynamic: {any(dim is None for dim in detail[\'shape\'])} "",  The model input shape cannot be a fixed value like [1,256,256,3],  but include ""none"" shape [1,none ,none 3], In this situation can  support dynamic shapes ?', 'created_at': datetime.datetime(2024, 9, 5, 7, 11, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333850993, 'issue_id': 2496286671, 'author': 'gaikwadrahul8', 'body': ""Hi, @keke444 \r\n\r\nThank you for sharing your model Input Details and Output Details and `Dynamic: False` means your model does not support dynamic shapes so you'll have to modify your model architecture with your model's input layer and potentially other layers to support dynamic dimensions and re-train the model with dynamic shapes then you can convert it to TFLite using the `tf.lite.TFLiteConverter` as you did before. The TFLite converter should be able to handle model with dynamic shapes.\r\n\r\n```\r\n# Assuming you're using TensorFlow\r\ninput_layer = tf.keras.layers.Input(shape=(1, None, None, 3))\r\n# Rest of your model layers\r\nmodel = tf.keras.Model(inputs=input_layer, outputs=outputs)\r\n\r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 6, 11, 28, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350779507, 'issue_id': 2496286671, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 14, 1, 57, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921146, 'issue_id': 2496286671, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364921265, 'issue_id': 2496286671, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74843"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74843"">No</a>', 'created_at': datetime.datetime(2024, 9, 21, 1, 58, 39, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-09-02 03:29:49 UTC): @keke444,
Tensorflow v2.13 is older version. Could you please try to test the code in the latest tensorflow v2.17 and provide the update if it is working in this case? Thank you!

keke444 (Issue Creator) on (2024-09-02 08:55:20 UTC): I used the latest tensorflow v2.17,  still got same failed  :
ERROR: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (36864 != 65536)
ERROR: Node number 17 (RESHAPE) failed to prepare.
Could you please help us check it again?

ths

gaikwadrahul8 (Assginee) on (2024-09-02 15:18:26 UTC): Hi, @keke444 

Thank you for bringing this issue to our attention and as far I know the TFLite model should be able to handle different input sizes if it was designed with dynamic shapes in mind and please ensure that the new dimensions you're specifying in `inputDims` are compatible with the model's architecture. If the model expects a specific input shape( if TFLite model does not support dynamic shape inputs) and resizing to incompatible dimensions might lead to unexpected behavior or errors.

Here’s a simple example in python to check if your model supports dynamic shapes:

```
import tensorflow as tf

def check_dynamic_shapes(model_path):
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(""Input Details:"")
    for detail in input_details:
        print(f""Name: {detail['name']}, Shape: {detail['shape']}, Dynamic: {any(dim is None for dim in detail['shape'])}"")

    print(""Output Details:"")
    for detail in output_details:
        print(f""Name: {detail['name']}, Shape: {detail['shape']}, Dynamic: {any(dim is None for dim in detail['shape'])}"")

# Provide the path to your TFLite model
check_dynamic_shapes('model.tflite')

```

If your converted TFLite model supports dynamic input shapes then it should accept various input shapes, if does not support before feeding the image to the model resize it to` [1, 256, 256, 3] `using an external library like `stb_image` or `OpenCV`. This way the image data matches the expected format of your model and eliminating the need for internal resizing because your model expect fixed input size of dimensions `[1,256,256,3]` and does not support dynamic input shapes so in that case you'll have to regenerate TFLite model with dynamic input dimensions

If I have missed something here please let me know ?

Thank you for your cooperation and patience.

keke444 (Issue Creator) on (2024-09-05 07:11:27 UTC): I used the example  to check if our model supports dynamic shapes, the log shows below:
""
Input Details:
Name: input_10, Shape: [  1 256 256   3], Dynamic: False
Output Details:
Name: Identity, Shape: [ 3 32 32 16], Dynamic: False
Name: Identity_1, Shape: [ 3 16 16 16], Dynamic: False
Name: Identity_2, Shape: [ 3  8  8 16], Dynamic: False
""
it seems like our model doesn't support dynamic shapes.

According to  "" Dynamic: {any(dim is None for dim in detail['shape'])} "",  The model input shape cannot be a fixed value like [1,256,256,3],  but include ""none"" shape [1,none ,none 3], In this situation can  support dynamic shapes ?

gaikwadrahul8 (Assginee) on (2024-09-06 11:28:08 UTC): Hi, @keke444 

Thank you for sharing your model Input Details and Output Details and `Dynamic: False` means your model does not support dynamic shapes so you'll have to modify your model architecture with your model's input layer and potentially other layers to support dynamic dimensions and re-train the model with dynamic shapes then you can convert it to TFLite using the `tf.lite.TFLiteConverter` as you did before. The TFLite converter should be able to handle model with dynamic shapes.

```
# Assuming you're using TensorFlow
input_layer = tf.keras.layers.Input(shape=(1, None, None, 3))
# Rest of your model layers
model = tf.keras.Model(inputs=input_layer, outputs=outputs)

```

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-14 01:57:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-21 01:58:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-21 01:58:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74843"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74843"">No</a>

"
2494425486,issue,closed,completed,"With the same input and parameter settings, there is a large difference in the output of LayerNormalization layer on GPU and CPU","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu20.04

### Mobile device

Ubuntu20.04

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8, cuDNN 8

### GPU model and memory

_No response_

### Current behavior?

DIFF: 0.37556207180023193
(False, 'Grad diff too big')

### Standalone code to reproduce the issue

```shell
import copy
import numpy as np
import tensorflow as tf

tf.config.experimental.enable_op_determinism()

def pointnet(input_shape):
    input_tensor = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding=""valid"")(input_tensor)
    x = tf.keras.layers.Softmax()(x)
    x = tf.keras.layers.Activation(activation='softplus')(x)
    x = tf.keras.layers.MaxPool1D(padding='valid', strides=4)(x)
    x = tf.keras.layers.BatchNormalization(center=False, momentum=0.07244736627895476, epsilon=0.3847853359642447)(x)
    x = tf.keras.layers.Activation(activation='exponential')(x)
    x = tf.keras.layers.MaxPooling1D(padding='same', strides=6, data_format='channels_last')(x)
    x = tf.keras.layers.Softmax(axis=2)(x)
    x = tf.keras.layers.Activation(activation='hard_sigmoid')(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(units=1, use_bias=False, activation='softplus', bias_constraint=None, bias_regularizer=None, kernel_regularizer=None, kernel_constraint=None, activity_regularizer=None, bias_initializer='identity', kernel_initializer='truncated_normal')(x)
    x = tf.keras.layers.LayerNormalization(center=True, axis=1, scale=True, beta_constraint=None, epsilon=0.7136230859666187, gamma_initializer='glorot_normal', beta_regularizer=None, gamma_constraint=None, beta_initializer='zeros', gamma_regularizer=None)(x)
    x = tf.keras.layers.Activation(activation='selu')(x)
    x = tf.keras.layers.Dense(units=8, use_bias=False, activation='selu', kernel_regularizer=None, bias_constraint=None, bias_regularizer=None, bias_initializer='ones', kernel_initializer='glorot_uniform', kernel_constraint=None, activity_regularizer=None)(x)
    tail_flatten = tf.keras.layers.Flatten()(x)
    tail_fc = tf.keras.layers.Dense(units=10)(tail_flatten)
    model = tf.keras.models.Model(inputs=input_tensor, outputs=tail_fc)
    return model


def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


def train(inp, label):
    flag = True
    label = tf.convert_to_tensor(label)
    model_g = pointnet(inp.shape[1:])
    model_g.load_weights(""./output_dict/grad_diff_initial_weights.h5"")
    
    with tf.device('GPU'):
        with tf.GradientTape() as tape:
            output_g = model_g(inp)
            loss_g = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(label, output_g)
        gradients_g = tape.gradient(loss_g, model_g.trainable_variables)
        gradients_dic_g = {}
        for var, gradient in zip(model_g.trainable_variables, gradients_g):
            if gradient != None:
                gradients_dic_g.setdefault(var.name.replace('/', '.')[:-2], gradient)

    model_c = copy.deepcopy(model_g)
    model_c.load_weights(""./output_dict/grad_diff_initial_weights.h5"")
    with tf.device('CPU'):
        with tf.GradientTape() as tape:
            output_c = model_c(inp)
            loss_c = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(label, output_c)
        gradients_c = tape.gradient(loss_c, model_c.trainable_variables)
        gradients_dic_c = {}
        for var, gradient in zip(model_c.trainable_variables, gradients_c):
            if gradient != None:
                gradients_dic_c.setdefault(var.name.replace('/', '.')[:-2], gradient)
    if chebyshev_distance(output_c.numpy(), output_g.numpy()) > 1.0:
        flag = False
        return flag, 'Output diff too big'
    if abs(loss_c - loss_g) > 0.1:
        flag = False
        return flag, 'Loss diff too big'
    for name in gradients_dic_c.keys(): 
        if name in gradients_dic_g.keys():
            if chebyshev_distance(gradients_dic_c[name], gradients_dic_g[name]) > 0.1:
                print(chebyshev_distance(gradients_dic_c[name], gradients_dic_g[name]))
                flag = False
                return flag, 'Grad diff too big'
    for name in gradients_dic_g.keys():
        if name in gradients_dic_c.keys():
            if chebyshev_distance(gradients_dic_g[name], gradients_dic_c[name]) > 0.1:
                print(gradients_dic_c[name], gradients_dic_g[name])
                flag = False
                return flag, 'Grad diff too big'
    return flag, ''


data = np.load(""./output_dict/grad_diff_input.npz"")
inp = data['inp']
label = data['label']
print(train(inp, label))
```


### Relevant log output

_No response_",BiophiliaSWDA,2024-08-29 13:19:24+00:00,['tilakrayal'],2024-11-01 02:07:12+00:00,2024-11-01 02:07:08+00:00,https://github.com/tensorflow/tensorflow/issues/74796,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2318517642, 'issue_id': 2494425486, 'author': 'SujalBagade', 'body': 'A temporary workaround is to replace `LayerNormalization` with `BatchNormalization` or to manually set a higher `epsilon` value, which reduces the discrepancy in floating-point operations.', 'created_at': datetime.datetime(2024, 8, 29, 18, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318552630, 'issue_id': 2494425486, 'author': 'ishan4421', 'body': ""Asked ChatGpt about this, answer is below. See if it can help \r\n\r\nThe issue you've described seems to revolve around a discrepancy between the outputs of a LayerNormalization layer when running on a GPU versus a CPU, using the same input and parameter settings. This kind of inconsistency can be a significant problem, especially in scenarios where deterministic behavior is expected across different hardware platforms.\r\n\r\nPossible Reasons for the Discrepancy:\r\nFloating-Point Precision: GPUs and CPUs may handle floating-point calculations differently. GPUs often use single precision (32-bit floating-point), while CPUs might use double precision (64-bit floating-point) or other precision levels depending on the settings. This difference can lead to variations in the results of operations that involve multiple floating-point calculations, such as LayerNormalization.\r\n\r\nParallel Computation: GPUs perform many operations in parallel, which can introduce slight differences in the order of operations compared to a CPU, which may execute operations sequentially. This difference can affect operations like normalization, where the order of summation can influence the final result due to the non-associativity of floating-point arithmetic.\r\n\r\nDeterminism Settings: Although you've enabled deterministic operations in TensorFlow with tf.config.experimental.enable_op_determinism(), not all TensorFlow operations may fully honor this setting, especially for complex layers like LayerNormalization.\r\n\r\nCUDA/cuDNN Version: Different versions of CUDA and cuDNN can have slight variations in implementation, which might contribute to differences in the results. Even small changes in how operations are optimized can lead to different outputs on GPU versus CPU.\r\n\r\nLayerNormalization Implementation: The implementation of LayerNormalization might differ slightly between GPU and CPU kernels, leading to differences in the final output. This could be due to different optimization techniques or specific hardware instructions used by GPUs.\r\n\r\nPossible Solutions or Workarounds:\r\nIncrease Tolerance for Differences: Depending on your use case, it might be acceptable to increase the tolerance for differences between CPU and GPU outputs. You could adjust the thresholds used in your checks, such as the chebyshev_distance threshold.\r\n\r\nTest with Different CUDA/cuDNN Versions: If feasible, try testing your code with different versions of CUDA and cuDNN to see if the issue persists or if it varies with different configurations.\r\n\r\nUse CPU-Only for Deterministic Results: If deterministic behavior is critical and GPU introduces too much variance, consider using CPU-only execution for critical parts of your model where exact reproducibility is required.\r\n\r\nCustom LayerNormalization Implementation: If the issue is isolated to LayerNormalization, consider implementing a custom version that you can control more tightly, ensuring consistency across platforms.\r\n\r\nIf this discrepancy is affecting a critical application, you might consider opening a detailed issue on the TensorFlow GitHub repository with all relevant information, including the specific CUDA/cuDNN versions, GPU model, and any other hardware-specific details. This could help the TensorFlow team investigate and potentially resolve the issue."", 'created_at': datetime.datetime(2024, 8, 29, 18, 16, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320577376, 'issue_id': 2494425486, 'author': 'BiophiliaSWDA', 'body': '> A temporary workaround is to replace `LayerNormalization` with `BatchNormalization` or to manually set a higher `epsilon` value, which reduces the discrepancy in floating-point operations.\r\n\r\nYou can repeat my mistake by following this link:\r\n\r\nhttps://github.com/PhyllisJi/MoCoDiff_Bug/tree/tf-issue-%2374796', 'created_at': datetime.datetime(2024, 8, 30, 9, 6, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322795082, 'issue_id': 2494425486, 'author': 'PhyllisJi', 'body': ""> Asked ChatGpt about this, answer is below. See if it can help\r\n> \r\n> The issue you've described seems to revolve around a discrepancy between the outputs of a LayerNormalization layer when running on a GPU versus a CPU, using the same input and parameter settings. This kind of inconsistency can be a significant problem, especially in scenarios where deterministic behavior is expected across different hardware platforms.\r\n> \r\n> Possible Reasons for the Discrepancy: Floating-Point Precision: GPUs and CPUs may handle floating-point calculations differently. GPUs often use single precision (32-bit floating-point), while CPUs might use double precision (64-bit floating-point) or other precision levels depending on the settings. This difference can lead to variations in the results of operations that involve multiple floating-point calculations, such as LayerNormalization.\r\n> \r\n> Parallel Computation: GPUs perform many operations in parallel, which can introduce slight differences in the order of operations compared to a CPU, which may execute operations sequentially. This difference can affect operations like normalization, where the order of summation can influence the final result due to the non-associativity of floating-point arithmetic.\r\n> \r\n> Determinism Settings: Although you've enabled deterministic operations in TensorFlow with tf.config.experimental.enable_op_determinism(), not all TensorFlow operations may fully honor this setting, especially for complex layers like LayerNormalization.\r\n> \r\n> CUDA/cuDNN Version: Different versions of CUDA and cuDNN can have slight variations in implementation, which might contribute to differences in the results. Even small changes in how operations are optimized can lead to different outputs on GPU versus CPU.\r\n> \r\n> LayerNormalization Implementation: The implementation of LayerNormalization might differ slightly between GPU and CPU kernels, leading to differences in the final output. This could be due to different optimization techniques or specific hardware instructions used by GPUs.\r\n> \r\n> Possible Solutions or Workarounds: Increase Tolerance for Differences: Depending on your use case, it might be acceptable to increase the tolerance for differences between CPU and GPU outputs. You could adjust the thresholds used in your checks, such as the chebyshev_distance threshold.\r\n> \r\n> Test with Different CUDA/cuDNN Versions: If feasible, try testing your code with different versions of CUDA and cuDNN to see if the issue persists or if it varies with different configurations.\r\n> \r\n> Use CPU-Only for Deterministic Results: If deterministic behavior is critical and GPU introduces too much variance, consider using CPU-only execution for critical parts of your model where exact reproducibility is required.\r\n> \r\n> Custom LayerNormalization Implementation: If the issue is isolated to LayerNormalization, consider implementing a custom version that you can control more tightly, ensuring consistency across platforms.\r\n> \r\n> If this discrepancy is affecting a critical application, you might consider opening a detailed issue on the TensorFlow GitHub repository with all relevant information, including the specific CUDA/cuDNN versions, GPU model, and any other hardware-specific details. This could help the TensorFlow team investigate and potentially resolve the issue.\r\n\r\nThank you for your answer. We have tried all the measures you mentioned, but differences still exist."", 'created_at': datetime.datetime(2024, 8, 31, 6, 27, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323755985, 'issue_id': 2494425486, 'author': 'tilakrayal', 'body': '@BiophiliaSWDA,\r\nCould you please try to check with the latest tensorflow v2.17 which contains the keras3.0 by default and provide the update if there is a difference in the output.  Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 3, 45, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331854061, 'issue_id': 2494425486, 'author': 'PhyllisJi', 'body': '> @BiophiliaSWDA, Could you please try to check with the latest tensorflow v2.17 which contains the keras3.0 by default and provide the update if there is a difference in the output. Thank you!\r\n\r\nWhen I check with TF 2.17.0, I face this error:\r\n```python\r\nTraceback (most recent call last):\r\n  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 234, in from_config\r\n    return cls(**config)\r\n  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/layers/core/dense.py"", line 92, in __init__\r\n    self.bias_initializer = initializers.get(bias_initializer)\r\n  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/initializers/__init__.py"", line 119, in get\r\n    raise ValueError(\r\nValueError: Could not interpret initializer identifier: {\'module\': \'keras.initializers\', \'class_name\': \'IdentityInitializer\', \'config\': {}, \'registered_name\': None}\r\n```', 'created_at': datetime.datetime(2024, 9, 5, 14, 34, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334031012, 'issue_id': 2494425486, 'author': 'PhyllisJi', 'body': ""> A temporary workaround is to replace `LayerNormalization` with `BatchNormalization` or to manually set a higher `epsilon` value, which reduces the discrepancy in floating-point operations.\r\n\r\nIt's not that the output is different, it's that the parameters are updated differently."", 'created_at': datetime.datetime(2024, 9, 6, 13, 14, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385451435, 'issue_id': 2494425486, 'author': 'tilakrayal', 'body': '@BiophiliaSWDA,\r\nThe reason for the different results could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.\r\n\r\nI think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime.\r\nBut, when the inputs are changed to float64 precision, the results are as expected. Same behavior is not observed on Apple M1, using numpy or in the CPU. Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 10, 48, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123514, 'issue_id': 2494425486, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401388574, 'issue_id': 2494425486, 'author': 'PhyllisJi', 'body': ""> @BiophiliaSWDA, The reason for the different results could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.\r\n> \r\n> I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected. Same behavior is not observed on Apple M1, using numpy or in the CPU. Thank you!\r\n\r\nThank you for your reply! But I think TF should do some optimisation means to avoid this type of error? Because NVIDIA's GPUs are used in a wide range of applications."", 'created_at': datetime.datetime(2024, 10, 9, 6, 4, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418568419, 'issue_id': 2494425486, 'author': 'tilakrayal', 'body': '@BiophiliaSWDA,\r\nThe layers LayerNormalization and BatchNormalization are related to Keras. Could you please raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 10, 17, 5, 59, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436658233, 'issue_id': 2494425486, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 25, 2, 2, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157683, 'issue_id': 2494425486, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157743, 'issue_id': 2494425486, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74796"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74796"">No</a>', 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 11, tzinfo=datetime.timezone.utc)}]","SujalBagade on (2024-08-29 18:01:30 UTC): A temporary workaround is to replace `LayerNormalization` with `BatchNormalization` or to manually set a higher `epsilon` value, which reduces the discrepancy in floating-point operations.

ishan4421 on (2024-08-29 18:16:48 UTC): Asked ChatGpt about this, answer is below. See if it can help 

The issue you've described seems to revolve around a discrepancy between the outputs of a LayerNormalization layer when running on a GPU versus a CPU, using the same input and parameter settings. This kind of inconsistency can be a significant problem, especially in scenarios where deterministic behavior is expected across different hardware platforms.

Possible Reasons for the Discrepancy:
Floating-Point Precision: GPUs and CPUs may handle floating-point calculations differently. GPUs often use single precision (32-bit floating-point), while CPUs might use double precision (64-bit floating-point) or other precision levels depending on the settings. This difference can lead to variations in the results of operations that involve multiple floating-point calculations, such as LayerNormalization.

Parallel Computation: GPUs perform many operations in parallel, which can introduce slight differences in the order of operations compared to a CPU, which may execute operations sequentially. This difference can affect operations like normalization, where the order of summation can influence the final result due to the non-associativity of floating-point arithmetic.

Determinism Settings: Although you've enabled deterministic operations in TensorFlow with tf.config.experimental.enable_op_determinism(), not all TensorFlow operations may fully honor this setting, especially for complex layers like LayerNormalization.

CUDA/cuDNN Version: Different versions of CUDA and cuDNN can have slight variations in implementation, which might contribute to differences in the results. Even small changes in how operations are optimized can lead to different outputs on GPU versus CPU.

LayerNormalization Implementation: The implementation of LayerNormalization might differ slightly between GPU and CPU kernels, leading to differences in the final output. This could be due to different optimization techniques or specific hardware instructions used by GPUs.

Possible Solutions or Workarounds:
Increase Tolerance for Differences: Depending on your use case, it might be acceptable to increase the tolerance for differences between CPU and GPU outputs. You could adjust the thresholds used in your checks, such as the chebyshev_distance threshold.

Test with Different CUDA/cuDNN Versions: If feasible, try testing your code with different versions of CUDA and cuDNN to see if the issue persists or if it varies with different configurations.

Use CPU-Only for Deterministic Results: If deterministic behavior is critical and GPU introduces too much variance, consider using CPU-only execution for critical parts of your model where exact reproducibility is required.

Custom LayerNormalization Implementation: If the issue is isolated to LayerNormalization, consider implementing a custom version that you can control more tightly, ensuring consistency across platforms.

If this discrepancy is affecting a critical application, you might consider opening a detailed issue on the TensorFlow GitHub repository with all relevant information, including the specific CUDA/cuDNN versions, GPU model, and any other hardware-specific details. This could help the TensorFlow team investigate and potentially resolve the issue.

BiophiliaSWDA (Issue Creator) on (2024-08-30 09:06:46 UTC): You can repeat my mistake by following this link:

https://github.com/PhyllisJi/MoCoDiff_Bug/tree/tf-issue-%2374796

PhyllisJi on (2024-08-31 06:27:06 UTC): Thank you for your answer. We have tried all the measures you mentioned, but differences still exist.

tilakrayal (Assginee) on (2024-09-02 03:45:20 UTC): @BiophiliaSWDA,
Could you please try to check with the latest tensorflow v2.17 which contains the keras3.0 by default and provide the update if there is a difference in the output.  Thank you!

PhyllisJi on (2024-09-05 14:34:30 UTC): When I check with TF 2.17.0, I face this error:
```python
Traceback (most recent call last):
  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 234, in from_config
    return cls(**config)
  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/layers/core/dense.py"", line 92, in __init__
    self.bias_initializer = initializers.get(bias_initializer)
  File ""/root/miniconda3/envs/tf2.17/lib/python3.10/site-packages/keras/src/initializers/__init__.py"", line 119, in get
    raise ValueError(
ValueError: Could not interpret initializer identifier: {'module': 'keras.initializers', 'class_name': 'IdentityInitializer', 'config': {}, 'registered_name': None}
```

PhyllisJi on (2024-09-06 13:14:17 UTC): It's not that the output is different, it's that the parameters are updated differently.

tilakrayal (Assginee) on (2024-10-01 10:48:06 UTC): @BiophiliaSWDA,
The reason for the different results could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.

I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime.
But, when the inputs are changed to float64 precision, the results are as expected. Same behavior is not observed on Apple M1, using numpy or in the CPU. Thank you!

github-actions[bot] on (2024-10-09 02:01:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

PhyllisJi on (2024-10-09 06:04:58 UTC): Thank you for your reply! But I think TF should do some optimisation means to avoid this type of error? Because NVIDIA's GPUs are used in a wide range of applications.

tilakrayal (Assginee) on (2024-10-17 05:59:20 UTC): @BiophiliaSWDA,
The layers LayerNormalization and BatchNormalization are related to Keras. Could you please raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!

github-actions[bot] on (2024-10-25 02:02:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-01 02:07:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-01 02:07:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74796"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74796"">No</a>

"
2494333608,issue,closed,completed,"When using exponential as the activation function, the outputs of the CPU and GPU have large differences","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Input: [[25.05214, 6.4932823, 5.5203633, 12.618748, 27.186777, 3.7995481]]
CPU output: [[7.5858780e+10 6.6068842e+02 2.4972575e+02 3.0217081e+05 6.4130895e+11
  4.4680992e+01]]
GPU output: [[7.5858780e+10 6.6068842e+02 2.4972574e+02 3.0217081e+05 6.4130888e+11
  4.4680988e+01]]
Max distance: 65536.0

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


act_layer = tf.keras.layers.Activation(activation='exponential')
inp = np.array([[25.05214, 6.4932823, 5.5203633, 12.618748, 27.186777, 3.7995481]])
input_shape = inp.shape[1:]
act_layer.build(input_shape)


with tf.device('/CPU:0'):
    x_cpu = tf.constant(inp, dtype=tf.float32)
    output_cpu = act_layer(x_cpu)
    print(""CPU output:"", output_cpu.numpy())


if tf.config.list_physical_devices('GPU'):
    with tf.device('/GPU:0'):
        x_gpu = tf.constant(inp, dtype=tf.float32)
        output_gpu = act_layer(x_gpu)
        print(""GPU output:"", output_gpu.numpy())
else:
    print(""GPU not available."")
    
output_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())
print(output_diff)
```


### Relevant log output

_No response_",PhyllisJi,2024-08-29 12:42:54+00:00,['Venkat6871'],2025-01-18 01:57:36+00:00,2025-01-18 01:57:33+00:00,https://github.com/tensorflow/tensorflow/issues/74791,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2323855683, 'issue_id': 2494333608, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.17.0 with GPU and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/13adc3200c8bea5dd3f366d73c6b5616/74791_tf-2-17-0-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 2, 5, 40, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323957847, 'issue_id': 2494333608, 'author': 'PhyllisJi', 'body': ""> I tried to run your code on Colab using TF v2.17.0 with GPU and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/13adc3200c8bea5dd3f366d73c6b5616/74791_tf-2-17-0-v.ipynb) here for reference. Thank you!\r\n\r\nYes, it's the same issue and the differences are more consistent."", 'created_at': datetime.datetime(2024, 9, 2, 6, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382460649, 'issue_id': 2494333608, 'author': 'Venkat6871', 'body': 'Hi @PhyllisJi ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 8, 35, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384878092, 'issue_id': 2494333608, 'author': 'PhyllisJi', 'body': '> Hi @PhyllisJi , Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras Thank you!\r\n\r\nI have posted ！Thanks', 'created_at': datetime.datetime(2024, 10, 1, 6, 7, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567542077, 'issue_id': 2494333608, 'author': 'Venkat6871', 'body': 'Hi **@PhyllisJi** ,\r\nCould you please close this issue since it is already being tracked in another repository?\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 2, 10, 16, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581592791, 'issue_id': 2494333608, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599466768, 'issue_id': 2494333608, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 18, 1, 57, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599466796, 'issue_id': 2494333608, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74791"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74791"">No</a>', 'created_at': datetime.datetime(2025, 1, 18, 1, 57, 34, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-09-02 05:40:12 UTC): I tried to run your code on Colab using TF v2.17.0 with GPU and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/13adc3200c8bea5dd3f366d73c6b5616/74791_tf-2-17-0-v.ipynb) here for reference.
Thank you!

PhyllisJi (Issue Creator) on (2024-09-02 06:58:40 UTC): Yes, it's the same issue and the differences are more consistent.

Venkat6871 (Assginee) on (2024-09-30 08:35:04 UTC): Hi @PhyllisJi ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

PhyllisJi (Issue Creator) on (2024-10-01 06:07:46 UTC): I have posted ！Thanks

Venkat6871 (Assginee) on (2025-01-02 10:16:23 UTC): Hi **@PhyllisJi** ,
Could you please close this issue since it is already being tracked in another repository?
Thank you!

github-actions[bot] on (2025-01-10 02:03:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-18 01:57:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-18 01:57:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74791"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74791"">No</a>

"
2494291485,issue,open,,tf.raw_ops.Round outputs zeros for any integer tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expects the same tensor as the input according to the specification https://www.tensorflow.org/api_docs/python/tf/raw_ops/Round

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([-2, -1, 1, 2, 3])
tf.raw_ops.Round(x=x)
```


### Relevant log output

```shell
>>> import tensorflow as tf
>>> x = tf.constant([-2, -1, 1, 2, 3], dtype=tf.int32)
>>> tf.raw_ops.Round(x=x)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0])>
```
",rkazants,2024-08-29 12:24:07+00:00,['tilakrayal'],2025-01-25 06:48:54+00:00,,https://github.com/tensorflow/tensorflow/issues/74789,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('regression issue', 'To spot regression issues in latest version'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2320476428, 'issue_id': 2494291485, 'author': 'tilakrayal', 'body': '@rkazants,\r\nThank you for reporting the issue. \r\n\r\nI tried to execute the code on tensorflow v2.17 & tf-nightly and observed that the **tf.raw_ops.Round** outputs the zeros for the integer, whereas with tensorflow **v2.16** the output is as intended.\r\n Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/936de7d336201a1354b8b36a4b789cb3/untitled2094.ipynb).\r\n\r\n**TF v2.16:**\r\n\r\n```python\r\n2.16.1\r\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([-2, -1,  1,  2,  3], dtype=int32)>\r\n```\r\n\r\n**TF 2.17 and tf-nightly:**\r\n\r\n```python\r\n2.18.0-dev20240828\r\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 30, 8, 27, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613814124, 'issue_id': 2494291485, 'author': 'Meetbahl04', 'body': 'hey @tilakrayal,\nkindly provide me some details regarding this issue and assign me this issue', 'created_at': datetime.datetime(2025, 1, 25, 6, 48, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-30 08:27:35 UTC): @rkazants,
Thank you for reporting the issue. 

I tried to execute the code on tensorflow v2.17 & tf-nightly and observed that the **tf.raw_ops.Round** outputs the zeros for the integer, whereas with tensorflow **v2.16** the output is as intended.
 Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/936de7d336201a1354b8b36a4b789cb3/untitled2094.ipynb).

**TF v2.16:**

```python
2.16.1
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([-2, -1,  1,  2,  3], dtype=int32)>
```

**TF 2.17 and tf-nightly:**

```python
2.18.0-dev20240828
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>
```

Thank you!

Meetbahl04 on (2025-01-25 06:48:53 UTC): hey @tilakrayal,
kindly provide me some details regarding this issue and assign me this issue

"
2494006671,issue,closed,completed,The outputs of conv1d differs significantly on the CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When conv1d used the same parameter settings, we gave the same input and found the largest difference in the output tensor 0.178863525390625.

### Standalone code to reproduce the issue

```shell
import h5py
import tensorflow as tf
import numpy as np

tf.random.set_seed(42)

def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))

h5_file_path = ""./output_diff_initial_weights.h5""
npz_path = ""./output_diff_input.npz""
conv1d_layer = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding=""valid"")
layer_name = 'conv1d_2351'

data = np.load(npz_path)
inp = data['inp']
input_shape = inp.shape[1:]

with h5py.File(h5_file_path, 'r') as h5_file:
    weights = h5_file[f'{layer_name}/{layer_name}/kernel:0'][:]
    biases = h5_file[f'{layer_name}/{layer_name}/bias:0'][:]
    

conv1d_layer.build(input_shape)
conv1d_layer.set_weights([weights, biases])


with tf.device('/CPU:0'):
    x_cpu = tf.constant(inp, dtype=tf.float32)
    output_cpu = conv1d_layer(x_cpu)


if tf.config.list_physical_devices('GPU'):
    with tf.device('/GPU:0'):
        x_gpu = tf.constant(inp, dtype=tf.float32)
        output_gpu = conv1d_layer(x_gpu)

else:
    print(""GPU not available."")
    
output_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())
print(output_diff)
```
https://github.com/PhyllisJi/MoCoDiff_Bug/tree/tf-issue-%2374785
This repository provides the inputs and weights.Just type python conv1d_test.py.

### Relevant log output

_No response_",PhyllisJi,2024-08-29 10:04:51+00:00,['Venkat6871'],2024-08-29 12:38:43+00:00,2024-08-29 12:38:38+00:00,https://github.com/tensorflow/tensorflow/issues/74785,"[('type:bug', 'Bug')]","[{'comment_id': 2317392811, 'issue_id': 2494006671, 'author': 'rahulsamant37', 'body': ""To address the issue with the observed discrepancy between CPU and GPU outputs in your Conv1D layer\r\n1-Ensure that both CPU and GPU are using the same floating-point precision.\r\n```\r\nx_cpu = tf.constant(inp, dtype=tf.float32)\r\nx_gpu = tf.constant(inp, dtype=tf.float32)\r\n```\r\n2-Enable deterministic operations for TensorFlow on GPU.\r\n```\r\ntf.config.experimental.enable_op_determinism()\r\n```\r\n3-Double-check your environment consistency (CUDA, cuDNN, TensorFlow versions).\r\n\r\nalso, You've already mentioned trying TensorFlow Nightly, but if you're still using an older version on your main environment, upgrading to the latest stable version could potentially fix issues related to discrepancies between CPU and GPU computations.\r\nBy taking these steps, you should be able to identify and potentially resolve the discrepancy you're observing.\r\n\r\nLet me know if any of these solutions work for you!"", 'created_at': datetime.datetime(2024, 8, 29, 11, 34, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317536519, 'issue_id': 2494006671, 'author': 'PhyllisJi', 'body': ""> To address the issue with the observed discrepancy between CPU and GPU outputs in your Conv1D layer 1-Ensure that both CPU and GPU are using the same floating-point precision.\r\n> \r\n> ```\r\n> x_cpu = tf.constant(inp, dtype=tf.float32)\r\n> x_gpu = tf.constant(inp, dtype=tf.float32)\r\n> ```\r\n> \r\n> 2-Enable deterministic operations for TensorFlow on GPU.\r\n> \r\n> ```\r\n> tf.config.experimental.enable_op_determinism()\r\n> ```\r\n> \r\n> 3-Double-check your environment consistency (CUDA, cuDNN, TensorFlow versions).\r\n> \r\n> also, You've already mentioned trying TensorFlow Nightly, but if you're still using an older version on your main environment, upgrading to the latest stable version could potentially fix issues related to discrepancies between CPU and GPU computations. By taking these steps, you should be able to identify and potentially resolve the discrepancy you're observing.\r\n> \r\n> Let me know if any of these solutions work for you!\r\n\r\nStep 2 solved my problem!"", 'created_at': datetime.datetime(2024, 8, 29, 12, 38, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317536573, 'issue_id': 2494006671, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74785"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74785"">No</a>', 'created_at': datetime.datetime(2024, 8, 29, 12, 38, 41, tzinfo=datetime.timezone.utc)}]","rahulsamant37 on (2024-08-29 11:34:53 UTC): To address the issue with the observed discrepancy between CPU and GPU outputs in your Conv1D layer
1-Ensure that both CPU and GPU are using the same floating-point precision.
```
x_cpu = tf.constant(inp, dtype=tf.float32)
x_gpu = tf.constant(inp, dtype=tf.float32)
```
2-Enable deterministic operations for TensorFlow on GPU.
```
tf.config.experimental.enable_op_determinism()
```
3-Double-check your environment consistency (CUDA, cuDNN, TensorFlow versions).

also, You've already mentioned trying TensorFlow Nightly, but if you're still using an older version on your main environment, upgrading to the latest stable version could potentially fix issues related to discrepancies between CPU and GPU computations.
By taking these steps, you should be able to identify and potentially resolve the discrepancy you're observing.

Let me know if any of these solutions work for you!

PhyllisJi (Issue Creator) on (2024-08-29 12:38:38 UTC): Step 2 solved my problem!

google-ml-butler[bot] on (2024-08-29 12:38:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74785"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74785"">No</a>

"
2493995762,issue,open,,"With the same input and parameter settings, there is a large difference in the output of Dense layer on GPU and CPU.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We found that when training the model, there is a large difference in the output of the DENSE layer on the cpu and gpu when using the same input tensor and parameter settings.
```
CPU output: [[3.4838054e+34]]
GPU output: [[3.4838057e+34]]
2.4758800785707605e+27
```
We have tried some similar inputs, but no such problems have occurred.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import pickle
import h5py

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


tf.random.set_seed(42)

x_input = np.array([[37.63115]])
# x_input = np.array([[38.63115]])
# x_input = np.array([[3.763115]])
print(x_input)

dense_layer = tf.keras.layers.Dense(units=1, activation='exponential', use_bias=True, activity_regularizer=None, bias_constraint=None, bias_initializer='random_normal', kernel_initializer='he_normal', bias_regularizer=None, kernel_regularizer=None, kernel_constraint=None)

weights = [np.array([[2.112561]], dtype=np.float32), np.array([0.03791478], dtype=np.float32)]

dense_layer.build((1,))  
dense_layer.set_weights(weights)

with tf.device('/CPU:0'):
    x_cpu = tf.constant(x_input, dtype=tf.float32)
    output_cpu = dense_layer(x_cpu)
    print(""CPU output:"", output_cpu.numpy())


if tf.config.list_physical_devices('GPU'):
    with tf.device('/GPU:0'):
        x_gpu = tf.constant(x_input, dtype=tf.float32)
        output_gpu = dense_layer(x_gpu)
        print(""GPU output:"", output_gpu.numpy())
else:
    print(""GPU not available."")
    
output_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())
print(output_diff)
```


### Relevant log output

_No response_",PhyllisJi,2024-08-29 09:59:58+00:00,['tilakrayal'],2024-10-09 06:03:33+00:00,,https://github.com/tensorflow/tensorflow/issues/74783,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2374715559, 'issue_id': 2493995762, 'author': 'tilakrayal', 'body': '@PhyllisJi,\r\nCould you please confirm whether this is the same case in latest tensorflow v2.17, and also the v2.12 is pretty old. Also Please refer to the attached explanation https://github.com/tensorflow/tensorflow/issues/58749#issuecomment-1467086661. The cast overflow causing undefined behaviour and hence getting different results. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/eca66928c7ae32abd01dc8f3c511336d/untitled2131.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 9, 25, 17, 22, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376056525, 'issue_id': 2493995762, 'author': 'PhyllisJi', 'body': '> @PhyllisJi, Could you please confirm whether this is the same case in latest tensorflow v2.17, and also the v2.12 is pretty old. Also Please refer to the attached explanation [#58749 (comment)](https://github.com/tensorflow/tensorflow/issues/58749#issuecomment-1467086661). The cast overflow causing undefined behaviour and hence getting different results. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/eca66928c7ae32abd01dc8f3c511336d/untitled2131.ipynb). Thank you!\r\n\r\nThanks for the answer, so I can understand that this is indeed a bug, but has been fixed in later versions of 2.14.0?', 'created_at': datetime.datetime(2024, 9, 26, 6, 36, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-09-25 17:22:46 UTC): @PhyllisJi,
Could you please confirm whether this is the same case in latest tensorflow v2.17, and also the v2.12 is pretty old. Also Please refer to the attached explanation https://github.com/tensorflow/tensorflow/issues/58749#issuecomment-1467086661. The cast overflow causing undefined behaviour and hence getting different results. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/eca66928c7ae32abd01dc8f3c511336d/untitled2131.ipynb). Thank you!

PhyllisJi (Issue Creator) on (2024-09-26 06:36:51 UTC): Thanks for the answer, so I can understand that this is indeed a bug, but has been fixed in later versions of 2.14.0?

"
2493663857,issue,closed,completed,tf.signal.rfft crashes when fft_length is [0],"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the fft_length to [0], the `tf.signal.rfft` will raises a program abort. If this parameter is invalid, raising an invalid argument error looks more clear to me instead of a program abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = tf.constant([0.27], dtype='float32')
tf.signal.rfft(input, fft_length=[0])
```
```


### Relevant log output

```shell
Skipping registering GPU devices...
DUCC FFT r2c failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fftnd_impl.h: 139 (static void ducc0::detail_fft::util::sanity_check_cr(const fmav_info &, const fmav_info &, const shape_t &)):

Assertion failure
axis length mismatch

Aborted (core dumped)
```
```
",maybeLee,2024-08-29 07:29:04+00:00,['Venkat6871'],2024-09-11 14:33:11+00:00,2024-09-11 14:33:08+00:00,https://github.com/tensorflow/tensorflow/issues/74748,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2316908935, 'issue_id': 2493663857, 'author': 'maybeLee', 'body': ""I notice that this issue also occurs when using the tf.signal.irfft\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.constant([0.27], dtype='complex64')\r\ntf.signal.irfft(input, fft_length=[0])\r\n```\r\n\r\nerror:\r\n```\r\nSkipping registering GPU devices...\r\nDUCC FFT c2r failed:\r\nbazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):\r\n\r\nAssertion failure\r\nno zero-sized FFTs\r\n\r\nAborted (core dumped)\r\n```"", 'created_at': datetime.datetime(2024, 8, 29, 7, 32, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323828046, 'issue_id': 2493663857, 'author': 'Venkat6871', 'body': 'Hi **@maybeLee** ,\r\nApologize for the delay. I tried to run your code on Colab using TensorFlow version 2.17.0, nightly and encountered the same issue. As an alternative, I have provided a [Gist](https://colab.research.google.com/gist/Venkat6871/0ec5fb9d281fe633c7d049ed36a19582/74748_2-17-nightly.ipynb) for your reference. This [issue](https://github.com/tensorflow/tensorflow/issues/65736) is already being tracked, so it is a duplicate. Could you please check and let me know if i am wrong.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 2, 5, 11, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460007, 'issue_id': 2493663857, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343855691, 'issue_id': 2493663857, 'author': 'maybeLee', 'body': 'Hi @Venkat6871 ,\r\n\r\nYes I think this issue is the duplicate of previous issue. I am closing this one and I will keep tracking the thread on previous issue.\r\nIt is much appreciated if this issue can be fixed.', 'created_at': datetime.datetime(2024, 9, 11, 14, 33, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343855822, 'issue_id': 2493663857, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74748"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74748"">No</a>', 'created_at': datetime.datetime(2024, 9, 11, 14, 33, 9, tzinfo=datetime.timezone.utc)}]","maybeLee (Issue Creator) on (2024-08-29 07:32:58 UTC): I notice that this issue also occurs when using the tf.signal.irfft

```
import tensorflow as tf
input = tf.constant([0.27], dtype='complex64')
tf.signal.irfft(input, fft_length=[0])
```

error:
```
Skipping registering GPU devices...
DUCC FFT c2r failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```

Venkat6871 (Assginee) on (2024-09-02 05:11:59 UTC): Hi **@maybeLee** ,
Apologize for the delay. I tried to run your code on Colab using TensorFlow version 2.17.0, nightly and encountered the same issue. As an alternative, I have provided a [Gist](https://colab.research.google.com/gist/Venkat6871/0ec5fb9d281fe633c7d049ed36a19582/74748_2-17-nightly.ipynb) for your reference. This [issue](https://github.com/tensorflow/tensorflow/issues/65736) is already being tracked, so it is a duplicate. Could you please check and let me know if i am wrong.
Thank you!

github-actions[bot] on (2024-09-10 01:58:43 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

maybeLee (Issue Creator) on (2024-09-11 14:33:06 UTC): Hi @Venkat6871 ,

Yes I think this issue is the duplicate of previous issue. I am closing this one and I will keep tracking the thread on previous issue.
It is much appreciated if this issue can be fixed.

google-ml-butler[bot] on (2024-09-11 14:33:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74748"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74748"">No</a>

"
2492999391,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow-intel==2.17.0

### Custom code

No

### OS platform and distribution

Windows 10 Pro

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In a clean windows (win 10 pro), I installed python 3.11.9 and after that, installed tensorflow using pip.
when I tried to run a test (just import tensorflow as tf), I got an error about DLL.  

### Standalone code to reproduce the issue

```shell
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
```


### Relevant log output

```shell
PS C:\Windows\system32> pip install tensorflow
Requirement already satisfied: tensorflow in d:\python311\lib\site-packages (2.17.0)
Requirement already satisfied: tensorflow-intel==2.17.0 in d:\python311\lib\site-packages (from tensorflow) (2.17.0)
Requirement already satisfied: absl-py>=1.0.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)
Requirement already satisfied: h5py>=3.10.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)
Requirement already satisfied: libclang>=13.0.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)
Requirement already satisfied: opt-einsum>=2.3.2 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)
Requirement already satisfied: packaging in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)
Requirement already satisfied: requests<3,>=2.21.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)
Requirement already satisfied: setuptools in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)
Requirement already satisfied: typing-extensions>=3.6.6 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.0)
Requirement already satisfied: tensorboard<2.18,>=2.17 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)
Requirement already satisfied: keras>=3.2.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)
Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)
Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\python311\lib\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)
Requirement already satisfied: rich in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)
Requirement already satisfied: namex in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)
Requirement already satisfied: optree in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)
Requirement already satisfied: charset-normalizer<4,>=2 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)
Requirement already satisfied: urllib3<3,>=1.21.1 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)
Requirement already satisfied: markdown>=2.6.8 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in d:\python311\lib\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)
Requirement already satisfied: markdown-it-py>=2.2.0 in d:\python311\lib\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\python311\lib\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in d:\python311\lib\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)




Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python311\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",syoshio,2024-08-28 21:12:43+00:00,['tilakrayal'],2024-12-18 17:55:12+00:00,2024-12-18 17:55:08+00:00,https://github.com/tensorflow/tensorflow/issues/74725,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('subtype:cpu-intel', 'To track windows cpu issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2316724914, 'issue_id': 2492999391, 'author': 'rahulsamant37', 'body': ""The error you're encountering typically happens because TensorFlow relies on certain low-level libraries that may be missing or incompatible with your system setup. \r\nHere are a few potential solutions to address the DLL load failed error:\r\n\r\n1) TensorFlow on Windows requires certain Microsoft C++ runtime libraries. Make sure you have the latest version of the Microsoft Visual C++ Redistributable installed.\r\n2) Run pip show TensorFlow to confirm that the TensorFlow installation is complete and hasn't missed any dependencies.\r\nSometimes the installation can be corrupted. You can try reinstalling TensorFlow:\r\n```\r\npip uninstall tensorflow tensorflow-intel\r\npip install tensorflow\r\n```\r\n3) Ensure that TensorFlow 2.17.0 is fully compatible with Python 3.11.9. While TensorFlow typically supports Python versions close to the latest, it's possible there could be compatibility issues. Try installing a slightly older version of Python, such as 3.10.x or 3.9.x, and see if that resolves the issue.\r\n4) If there are multiple versions of the same DLL files in your PATH, it might be causing conflicts. Make sure your Python environment and TensorFlow are properly isolated using venv or conda.\r\nCreate a virtual environment and reinstall TensorFlow:\r\n```\r\npython -m venv tf_env\r\ntf_env\\Scripts\\activate\r\npip install tensorflow\r\n```\r\n5) If none of the above solutions work, you can try using the nightly version of TensorFlow, which include bug fixes and updates that address the issue you're encountering:\r\n```\r\npip install tf-nightly\r\n```\r\n\r\n\r\nLet me know if any of these solutions work for you!"", 'created_at': datetime.datetime(2024, 8, 29, 5, 7, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317781526, 'issue_id': 2492999391, 'author': 'tilakrayal', 'body': '@syoshio,\r\nTensorflow-Intel is an optimized version of TensorFlow for Windows OS that has been produced by Intel.  Could you please confirm whether you are trying to use pip install tensorflow-intel or pip install tensorflow? Thank you!', 'created_at': datetime.datetime(2024, 8, 29, 14, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317941210, 'issue_id': 2492999391, 'author': 'rahulsamant37', 'body': '@tilakrayal,\r\nIn third line of relevant output it is mention that it is tensorflow-intel 2.17.0\r\n\r\nfor your reference I will add it here again:\r\n```\r\nPS C:\\Windows\\system32> pip install tensorflow\r\nRequirement already satisfied: tensorflow in d:\\python311\\lib\\site-packages (2.17.0)\r\nRequirement already satisfied: tensorflow-intel==2.17.0 in d:\\python311\\lib\\site-packages (from tensorflow) (2.17.0)\r\nRequirement already satisfied: absl-py>=1.0.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\r\nRequirement already satisfied: flatbuffers>=24.3.25 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\r\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\r\nRequirement already satisfied: h5py>=3.10.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\r\nRequirement already satisfied: libclang>=13.0.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\r\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\r\nRequirement already satisfied: packaging in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\r\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\r\nRequirement already satisfied: requests<3,>=2.21.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\r\nRequirement already satisfied: setuptools in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\r\nRequirement already satisfied: six>=1.12.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\r\nRequirement already satisfied: wrapt>=1.11.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.0)\r\nRequirement already satisfied: tensorboard<2.18,>=2.17 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\r\nRequirement already satisfied: keras>=3.2.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\r\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\r\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in d:\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\r\nRequirement already satisfied: rich in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\r\nRequirement already satisfied: namex in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\r\nRequirement already satisfied: optree in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\r\nRequirement already satisfied: markdown>=2.6.8 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\r\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\r\nRequirement already satisfied: werkzeug>=1.0.1 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\r\nRequirement already satisfied: MarkupSafe>=2.1.1 in d:\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in d:\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\r\nRequirement already satisfied: mdurl~=0.1 in d:\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\r\n\r\n\r\n\r\n\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n```\r\nHope you understand why I use this :\r\n```\r\npip uninstall tensorflow tensorflow-intel\r\npip install tensorflow\r\n```\r\nFeel free to ask anything if you think my approach is wrong—after all, I\'m here to learn as well.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 29, 14, 46, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318557722, 'issue_id': 2492999391, 'author': 'syoshio', 'body': 'rahulsamant37,\r\ntilakrayal thanks for help me!\r\nAs you recommended, I did uninstall tensorflow and tensorflow-intel, but\r\nwhen run pip install tensorflow, tensorflow-intel is installed together.\r\nI tried to uninstall only tensorflow-intel, but python complains when I\r\ntried to import tensorflow.\r\nAbout Microsoft Visual C++ Redistributable, I have the last version ( 14\r\n.40.33810.0,\r\nhttps://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#latest-microsoft-visual-c-redistributable-version\r\n)\r\nI\'m still getting error.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPS C:\\Windows\\system32> pip uninstall tensorflow tensorflow-intel\r\nFound existing installation: tensorflow 2.17.0\r\nUninstalling tensorflow-2.17.0:\r\n  Would remove:\r\n    d:\\python311\\lib\\site-packages\\tensorflow-2.17.0.dist-info\\*\r\nProceed (Y/n)? Y\r\n  Successfully uninstalled tensorflow-2.17.0\r\nFound existing installation: tensorflow-intel 2.17.0\r\nUninstalling tensorflow-intel-2.17.0:\r\n  Would remove:\r\n    d:\\python311\\lib\\site-packages\\tensorflow\\*\r\n    d:\\python311\\lib\\site-packages\\tensorflow_intel-2.17.0.dist-info\\*\r\n    d:\\python311\\scripts\\import_pb_to_tensorboard.exe\r\n    d:\\python311\\scripts\\saved_model_cli.exe\r\n    d:\\python311\\scripts\\tensorboard.exe\r\n    d:\\python311\\scripts\\tf_upgrade_v2.exe\r\n    d:\\python311\\scripts\\tflite_convert.exe\r\n    d:\\python311\\scripts\\toco.exe\r\n    d:\\python311\\scripts\\toco_from_protos.exe\r\nProceed (Y/n)? Y\r\n  Successfully uninstalled tensorflow-intel-2.17.0\r\n\r\n\r\n\r\nPS C:\\Windows\\system32> pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\r\nCollecting tensorflow-intel==2.17.0 (from tensorflow)\r\n  Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata\r\n(5.0 kB)\r\nRequirement already satisfied: absl-py>=1.0.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.1.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.6.3)\r\nRequirement already satisfied: flatbuffers>=24.3.25 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(24.3.25)\r\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.6.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.2.0)\r\nRequirement already satisfied: h5py>=3.10.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.11.0)\r\nRequirement already satisfied: libclang>=13.0.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(18.1.1)\r\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.4.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.3.0)\r\nRequirement already satisfied: packaging in d:\\python311\\lib\\site-packages\r\n(from tensorflow-intel==2.17.0->tensorflow) (24.1)\r\nRequirement already satisfied:\r\nprotobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\r\nin d:\\python311\\lib\\site-packages (from\r\ntensorflow-intel==2.17.0->tensorflow) (4.25.4)\r\nRequirement already satisfied: requests<3,>=2.21.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.32.3)\r\nRequirement already satisfied: setuptools in d:\\python311\\lib\\site-packages\r\n(from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\r\nRequirement already satisfied: six>=1.12.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.16.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.4.0)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(4.12.2)\r\nRequirement already satisfied: wrapt>=1.11.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.16.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.66.0)\r\nRequirement already satisfied: tensorboard<2.18,>=2.17 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.17.1)\r\nRequirement already satisfied: keras>=3.2.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.5.0)\r\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.31.0)\r\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.26.4)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nastunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\r\nRequirement already satisfied: rich in d:\\python311\\lib\\site-packages (from\r\nkeras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\r\nRequirement already satisfied: namex in d:\\python311\\lib\\site-packages\r\n(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\r\nRequirement already satisfied: optree in d:\\python311\\lib\\site-packages\r\n(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\r\nRequirement already satisfied: markdown>=2.6.8 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\r\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\r\nRequirement already satisfied: werkzeug>=1.0.1 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\r\nRequirement already satisfied: MarkupSafe>=2.1.1 in\r\nd:\\python311\\lib\\site-packages (from\r\nwerkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n(2.1.5)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\r\nRequirement already satisfied: mdurl~=0.1 in d:\\python311\\lib\\site-packages\r\n(from\r\nmarkdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n(0.1.2)\r\nUsing cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)\r\nUsing cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)\r\nInstalling collected packages: tensorflow-intel, tensorflow\r\nSuccessfully installed tensorflow-2.17.0 tensorflow-intel-2.17.0\r\n\r\n\r\n\r\nPS C:\\Windows\\system32> pip uninstall tensorflow-intel\r\nFound existing installation: tensorflow-intel 2.17.0\r\nUninstalling tensorflow-intel-2.17.0:\r\n  Would remove:\r\n    d:\\python311\\lib\\site-packages\\tensorflow\\*\r\n    d:\\python311\\lib\\site-packages\\tensorflow_intel-2.17.0.dist-info\\*\r\n    d:\\python311\\scripts\\import_pb_to_tensorboard.exe\r\n    d:\\python311\\scripts\\saved_model_cli.exe\r\n    d:\\python311\\scripts\\tensorboard.exe\r\n    d:\\python311\\scripts\\tf_upgrade_v2.exe\r\n    d:\\python311\\scripts\\tflite_convert.exe\r\n    d:\\python311\\scripts\\toco.exe\r\n    d:\\python311\\scripts\\toco_from_protos.exe\r\nProceed (Y/n)? Y\r\n  Successfully uninstalled tensorflow-intel-2.17.0\r\nPS C:\\Windows\\system32> python\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64\r\nbit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nModuleNotFoundError: No module named \'tensorflow\'\r\n>>> exit()\r\n[image: image.png]\r\n\r\n\r\nPS C:\\Windows\\system32> python\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64\r\nbit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in\r\n<module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and\r\nsolutions.\r\nIf you need help, create an issue at\r\nhttps://github.com/tensorflow/tensorflow/issues and include the entire\r\nstack trace above this error message.\r\n>>>\r\n\r\nOn Thu, Aug 29, 2024 at 11:47\u202fAM rahulsamant37 ***@***.***>\r\nwrote:\r\n\r\n> @tilakrayal <https://github.com/tilakrayal>,\r\n> In second line of relevant output it is mention that it is tensorflow-intel\r\n>\r\n> or your reference I will add it here again:\r\n>\r\n> PS C:\\Windows\\system32> pip install tensorflow\r\n> Requirement already satisfied: tensorflow in d:\\python311\\lib\\site-packages (2.17.0)\r\n> Requirement already satisfied: tensorflow-intel==2.17.0 in d:\\python311\\lib\\site-packages (from tensorflow) (2.17.0)\r\n> Requirement already satisfied: absl-py>=1.0.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\r\n> Requirement already satisfied: astunparse>=1.6.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\r\n> Requirement already satisfied: flatbuffers>=24.3.25 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\r\n> Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\r\n> Requirement already satisfied: google-pasta>=0.1.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\r\n> Requirement already satisfied: h5py>=3.10.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\r\n> Requirement already satisfied: libclang>=13.0.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\r\n> Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\r\n> Requirement already satisfied: opt-einsum>=2.3.2 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\r\n> Requirement already satisfied: packaging in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\r\n> Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\r\n> Requirement already satisfied: requests<3,>=2.21.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\r\n> Requirement already satisfied: setuptools in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\r\n> Requirement already satisfied: six>=1.12.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\r\n> Requirement already satisfied: termcolor>=1.1.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\r\n> Requirement already satisfied: typing-extensions>=3.6.6 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\r\n> Requirement already satisfied: wrapt>=1.11.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\r\n> Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.0)\r\n> Requirement already satisfied: tensorboard<2.18,>=2.17 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\r\n> Requirement already satisfied: keras>=3.2.0 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\r\n> Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\r\n> Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\r\n> Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\r\n> Requirement already satisfied: rich in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\r\n> Requirement already satisfied: namex in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\r\n> Requirement already satisfied: optree in d:\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\r\n> Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\r\n> Requirement already satisfied: idna<4,>=2.5 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\r\n> Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\r\n> Requirement already satisfied: certifi>=2017.4.17 in d:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\r\n> Requirement already satisfied: markdown>=2.6.8 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\r\n> Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\r\n> Requirement already satisfied: werkzeug>=1.0.1 in d:\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\r\n> Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\r\n> Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\r\n> Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\r\n> Requirement already satisfied: mdurl~=0.1 in d:\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\r\n>\r\n>\r\n>\r\n>\r\n> Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32\r\n> Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n> >>> import tensorflow as tf\r\n> Traceback (most recent call last):\r\n>   File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n>     from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n>\r\n> During handling of the above exception, another exception occurred:\r\n>\r\n> Traceback (most recent call last):\r\n>   File ""<stdin>"", line 1, in <module>\r\n>   File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module>\r\n>     raise ImportError(\r\n> ImportError: Traceback (most recent call last):\r\n>   File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n>     from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n>\r\n>\r\n> Failed to load the native TensorFlow runtime.\r\n> See https://www.tensorflow.org/install/errors for some common causes and solutions.\r\n> If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n>\r\n> Hope you understand why I use :\r\n>\r\n> pip uninstall tensorflow tensorflow-intel\r\n> pip install tensorflow\r\n>\r\n> feel free to ask anything if you think my approach is wrong after all I\r\n> also here to learn\r\n> Thank you!!\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/74725#issuecomment-2317941210>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ADSP7KZLADVPRDWEBEOU2T3ZT4X7FAVCNFSM6AAAAABNJDCR6GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMJXHE2DCMRRGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 8, 29, 18, 19, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323285371, 'issue_id': 2492999391, 'author': 'elhatim03', 'body': 'I\'m dealing with the same problem\r\n\r\n File ""C:\\Users\\tim\\Desktop\\tfproject\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\tim\\Desktop\\tfproject\\test.py"", line 1, in <module>\r\n    import tensorflow as tf\r\n  File ""C:\\Users\\tim\\Desktop\\tfproject\\myenv\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>       \r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\tim\\Desktop\\tfproject\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File ""C:\\Users\\tim\\Desktop\\tfproject\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.', 'created_at': datetime.datetime(2024, 9, 1, 10, 55, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324869455, 'issue_id': 2492999391, 'author': 'Salmankhan3', 'body': '**_### how to solve this_**\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\SALMAN  KHAN\\Desktop\\Python_version_2\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.       \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\SALMAN  KHAN\\Desktop\\Python_version_2\\myenv\\tf.py"", line 1, in <module>\r\n    import tensorflow as tf\r\n  File ""C:\\Users\\SALMAN  KHAN\\Desktop\\Python_version_2\\myenv\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\SALMAN  KHAN\\Desktop\\Python_version_2\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File ""C:\\Users\\SALMAN  KHAN\\Desktop\\Python_version_2\\myenv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>    \r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message', 'created_at': datetime.datetime(2024, 9, 2, 14, 22, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344662961, 'issue_id': 2492999391, 'author': 'mraunak', 'body': ""Hi [syoshio](https://github.com/syoshio), I don't see any issue on my end. Please try to download python(Download [Windows installer (64-bit)](https://www.python.org/ftp/python/3.11.9/python-3.11.9-amd64.exe))  again from https://www.python.org/downloads/windows/\r\n\r\n![image](https://github.com/user-attachments/assets/67833b80-aae9-4fc9-8b00-49a1c7850520)\r\n\r\nPlease let me know if the issue persists."", 'created_at': datetime.datetime(2024, 9, 11, 20, 41, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344674325, 'issue_id': 2492999391, 'author': 'mraunak', 'body': 'steps followed by me\r\n![image](https://github.com/user-attachments/assets/4be92c05-41cd-4465-8dbf-00116cfd9e08)', 'created_at': datetime.datetime(2024, 9, 11, 20, 48, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344785853, 'issue_id': 2492999391, 'author': 'syoshio', 'body': 'Hello mraunak! I can install python and tensorflow, but the error happens\r\nwhen I try to import tensorflow\r\n\r\nPS D:\\projetos\\python> python -m venv tensor_env\r\nPS D:\\projetos\\python> pip list\r\nPackage                      Version\r\n---------------------------- --------\r\nabsl-py                      2.1.0\r\nastunparse                   1.6.3\r\ncertifi                      2024.7.4\r\ncharset-normalizer           3.3.2\r\nflatbuffers                  24.3.25\r\ngast                         0.6.0\r\ngoogle-pasta                 0.2.0\r\ngrpcio                       1.66.0\r\nh5py                         3.11.0\r\nidna                         3.8\r\nkeras                        3.5.0\r\nlibclang                     18.1.1\r\nMarkdown                     3.7\r\nmarkdown-it-py               3.0.0\r\nMarkupSafe                   2.1.5\r\nmdurl                        0.1.2\r\nml-dtypes                    0.4.0\r\nnamex                        0.0.8\r\nnumpy                        1.26.4\r\nopt-einsum                   3.3.0\r\noptree                       0.12.1\r\npackaging                    24.1\r\npip                          24.0\r\nprotobuf                     4.25.4\r\nPygments                     2.18.0\r\nrequests                     2.32.3\r\nrich                         13.8.0\r\nsetuptools                   65.5.0\r\nsix                          1.16.0\r\ntensorboard                  2.17.1\r\ntensorboard-data-server      0.7.2\r\ntensorflow                   2.17.0\r\ntensorflow-intel             2.17.0\r\ntensorflow-io-gcs-filesystem 0.31.0\r\ntermcolor                    2.4.0\r\ntyping_extensions            4.12.2\r\nurllib3                      2.2.2\r\nWerkzeug                     3.0.4\r\nwheel                        0.44.0\r\nwrapt                        1.16.0\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\nPS D:\\projetos\\python> python --version\r\nPython 3.11.9\r\nPS D:\\projetos\\python> pip install tensorflow\r\nRequirement already satisfied: tensorflow in d:\\python311\\lib\\site-packages\r\n(2.17.0)\r\nRequirement already satisfied: tensorflow-intel==2.17.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow) (2.17.0)\r\nRequirement already satisfied: absl-py>=1.0.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.1.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.6.3)\r\nRequirement already satisfied: flatbuffers>=24.3.25 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(24.3.25)\r\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.6.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.2.0)\r\nRequirement already satisfied: h5py>=3.10.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.11.0)\r\nRequirement already satisfied: libclang>=13.0.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(18.1.1)\r\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.4.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.3.0)\r\nRequirement already satisfied: packaging in d:\\python311\\lib\\site-packages\r\n(from tensorflow-intel==2.17.0->tensorflow) (24.1)\r\nRequirement already satisfied:\r\nprotobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\r\nin d:\\python311\\lib\\site-packages (from\r\ntensorflow-intel==2.17.0->tensorflow) (4.25.4)\r\nRequirement already satisfied: requests<3,>=2.21.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.32.3)\r\nRequirement already satisfied: setuptools in d:\\python311\\lib\\site-packages\r\n(from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\r\nRequirement already satisfied: six>=1.12.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.16.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.4.0)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(4.12.2)\r\nRequirement already satisfied: wrapt>=1.11.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.16.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.66.0)\r\nRequirement already satisfied: tensorboard<2.18,>=2.17 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(2.17.1)\r\nRequirement already satisfied: keras>=3.2.0 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(3.5.0)\r\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(0.31.0)\r\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in\r\nd:\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow)\r\n(1.26.4)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nastunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\r\nRequirement already satisfied: rich in d:\\python311\\lib\\site-packages (from\r\nkeras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\r\nRequirement already satisfied: namex in d:\\python311\\lib\\site-packages\r\n(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\r\nRequirement already satisfied: optree in d:\\python311\\lib\\site-packages\r\n(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in\r\nd:\\python311\\lib\\site-packages (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\r\nRequirement already satisfied: markdown>=2.6.8 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\r\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\r\nRequirement already satisfied: werkzeug>=1.0.1 in\r\nd:\\python311\\lib\\site-packages (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\r\nRequirement already satisfied: MarkupSafe>=2.1.1 in\r\nd:\\python311\\lib\\site-packages (from\r\nwerkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n(2.1.5)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in\r\nd:\\python311\\lib\\site-packages (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\r\nRequirement already satisfied: mdurl~=0.1 in d:\\python311\\lib\\site-packages\r\n(from\r\nmarkdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n(0.1.2)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\nPS D:\\projetos\\python> python\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64\r\nbit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""D:\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py"", line 38, in\r\n<module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File\r\n""D:\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and\r\nsolutions.\r\nIf you need help, create an issue at\r\nhttps://github.com/tensorflow/tensorflow/issues and include the entire\r\nstack trace above this error message.\r\n>>>\r\n\r\nOn Wed, Sep 11, 2024 at 5:49\u202fPM mraunak ***@***.***> wrote:\r\n\r\n> steps followed by me\r\n> image.png (view on web)\r\n> <https://github.com/user-attachments/assets/4be92c05-41cd-4465-8dbf-00116cfd9e08>\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/74725#issuecomment-2344674325>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ADSP7KYDJYADVDISZ25422TZWCUEDAVCNFSM6AAAAABNJDCR6GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNBUGY3TIMZSGU>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 9, 11, 22, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344850141, 'issue_id': 2492999391, 'author': 'mraunak', 'body': 'I see, can you please try with a different Python version(3.12.1) and let us know if the issue exists\r\n\r\nThe list of packages installed via pip seems the same in both cases, but it is passing on my end\r\n\r\n(venv311_t) D:\\user\\mraunak>pip list\r\nPackage                      Version\r\n---------------------------- ---------\r\nabsl-py                      2.1.0\r\nastunparse                   1.6.3\r\ncertifi                      2024.8.30\r\ncharset-normalizer           3.3.2\r\nflatbuffers                  24.3.25\r\ngast                         0.6.0\r\ngoogle-pasta                 0.2.0\r\ngrpcio                       1.66.1\r\nh5py                         3.11.0\r\nidna                         3.8\r\nkeras                        3.5.0\r\nlibclang                     18.1.1\r\nMarkdown                     3.7\r\nmarkdown-it-py               3.0.0\r\nMarkupSafe                   2.1.5\r\nmdurl                        0.1.2\r\nml-dtypes                    0.4.0\r\nnamex                        0.0.8\r\nnumpy                        1.26.4\r\nopt-einsum                   3.3.0\r\noptree                       0.12.1\r\npackaging                    24.1\r\npip                          24.0\r\nprotobuf                     4.25.4\r\nPygments                     2.18.0\r\nrequests                     2.32.3\r\nrich                         13.8.1\r\nsetuptools                   65.5.0\r\nsix                          1.16.0\r\ntensorboard                  2.17.1\r\ntensorboard-data-server      0.7.2\r\ntensorflow                   2.17.0\r\ntensorflow-intel             2.17.0\r\ntensorflow-io-gcs-filesystem 0.31.0\r\ntermcolor                    2.4.0\r\ntyping_extensions            4.12.2\r\nurllib3                      2.2.2\r\nWerkzeug                     3.0.4\r\nwheel                        0.44.0\r\nwrapt                        1.16.0', 'created_at': datetime.datetime(2024, 9, 11, 22, 56, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345045288, 'issue_id': 2492999391, 'author': 'syoshio', 'body': 'Hello  mraunak!\r\nI tried as you recommended, but I\'m still having problems.\r\n\r\nPS D:\\projetos\\python> Set-ExecutionPolicy\r\n\r\ncmdlet Set-ExecutionPolicy at command pipeline position 1\r\nSupply values for the following parameters:\r\nExecutionPolicy: AllSigned\r\n\r\nExecution Policy Change\r\nThe execution policy helps protect you from scripts that you do not trust.\r\nChanging the execution policy might expose\r\nyou to the security risks described in the about_Execution_Policies help\r\ntopic at\r\nhttps:/go.microsoft.com/fwlink/?LinkID=135170. Do you want to change the\r\nexecution policy?\r\n[Y] Yes  [A] Yes to All  [N] No  [L] No to All  [S] Suspend  [?] Help\r\n(default is ""N""): Y\r\nPS D:\\projetos\\python> tensor_env/scripts/activate\r\n\r\nDo you want to run software from this untrusted publisher?\r\nFile D:\\projetos\\python\\tensor_env\\scripts\\Activate.ps1 is published by\r\nCN=Python Software Foundation, O=Python\r\nSoftware Foundation, L=Beaverton, S=Oregon, C=US and is not trusted on your\r\nsystem. Only run scripts from trusted\r\npublishers.\r\n[V] Never run  [D] Do not run  [R] Run once  [A] Always run  [?] Help\r\n(default is ""D""): A\r\n(tensor_env) PS D:\\projetos\\python> pip list\r\nPackage    Version\r\n---------- -------\r\npip        24.0\r\nsetuptools 65.5.0\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\n(tensor_env) PS D:\\projetos\\python> pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\r\nCollecting tensorflow-intel==2.17.0 (from tensorflow)\r\n  Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata\r\n(5.0 kB)\r\nCollecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\r\nCollecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\r\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from\r\ntensorflow-intel==2.17.0->tensorflow)\r\n  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\r\nCollecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\r\nCollecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\r\nCollecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\r\nCollecting ml-dtypes<0.5.0,>=0.3.1 (from\r\ntensorflow-intel==2.17.0->tensorflow)\r\n  Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\r\nCollecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\r\nCollecting packaging (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting\r\nprotobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\r\n(from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl.metadata (541 bytes)\r\nCollecting requests<3,>=2.21.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\r\nRequirement already satisfied: setuptools in\r\nd:\\projetos\\python\\tensor_env\\lib\\site-packages (from\r\ntensorflow-intel==2.17.0->tensorflow) (65.5.0)\r\nCollecting six>=1.12.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\nCollecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\r\nCollecting typing-extensions>=3.6.6 (from\r\ntensorflow-intel==2.17.0->tensorflow)\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\r\nCollecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Downloading grpcio-1.66.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\r\nCollecting tensorboard<2.18,>=2.17 (from\r\ntensorflow-intel==2.17.0->tensorflow)\r\n  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from\r\ntensorflow-intel==2.17.0->tensorflow)\r\n  Using cached\r\ntensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14\r\nkB)\r\nCollecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\r\nCollecting wheel<1.0,>=0.23.0 (from\r\nastunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\r\nCollecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\r\nCollecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached optree-0.12.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\r\nCollecting charset-normalizer<4,>=2 (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata\r\n(34 kB)\r\nCollecting idna<4,>=2.5 (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting urllib3<3,>=1.21.1 (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\nCollecting certifi>=2017.4.17 (from\r\nrequests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\r\nCollecting markdown>=2.6.8 (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\r\nCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1\r\nkB)\r\nCollecting werkzeug>=1.0.1 (from\r\ntensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\r\nCollecting MarkupSafe>=2.1.1 (from\r\nwerkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\r\nCollecting markdown-it-py>=2.2.0 (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nCollecting pygments<3.0.0,>=2.13.0 (from\r\nrich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting mdurl~=0.1 (from\r\nmarkdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\r\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nUsing cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)\r\nUsing cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)\r\nUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\r\nUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nUsing cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\r\nUsing cached gast-0.6.0-py3-none-any.whl (21 kB)\r\nUsing cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nDownloading grpcio-1.66.1-cp311-cp311-win_amd64.whl (4.3 MB)\r\n   ---------------------------------------- 4.3/4.3 MB 8.8 MB/s eta 0:00:00\r\nUsing cached h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\r\nUsing cached keras-3.5.0-py3-none-any.whl (1.1 MB)\r\nUsing cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\r\nUsing cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl (126 kB)\r\nUsing cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\r\nUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nUsing cached protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)\r\nUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\nUsing cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\r\nUsing cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl\r\n(1.5 MB)\r\nUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\r\nUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nUsing cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\r\n   ---------------------------------------- 167.3/167.3 kB 5.1 MB/s eta\r\n0:00:00\r\nUsing cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\r\nUsing cached idna-3.8-py3-none-any.whl (66 kB)\r\nUsing cached Markdown-3.7-py3-none-any.whl (106 kB)\r\nUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\r\nUsing cached urllib3-2.2.2-py3-none-any.whl (121 kB)\r\nUsing cached werkzeug-3.0.4-py3-none-any.whl (227 kB)\r\nUsing cached wheel-0.44.0-py3-none-any.whl (67 kB)\r\nUsing cached namex-0.0.8-py3-none-any.whl (5.8 kB)\r\nUsing cached optree-0.12.1-cp311-cp311-win_amd64.whl (268 kB)\r\nDownloading rich-13.8.1-py3-none-any.whl (241 kB)\r\n   ---------------------------------------- 241.6/241.6 kB 14.5 MB/s eta\r\n0:00:00\r\nUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nUsing cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\r\nUsing cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\r\nUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nInstalling collected packages: namex, libclang, flatbuffers, wrapt, wheel,\r\nurllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem,\r\ntensorboard-data-server, six, pygments, protobuf, packaging, numpy, mdurl,\r\nMarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi,\r\nabsl-py, werkzeug, requests, optree, opt-einsum, ml-dtypes, markdown-it-py,\r\nh5py, google-pasta, astunparse, tensorboard, rich, keras, tensorflow-intel,\r\ntensorflow\r\nSuccessfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3\r\ncertifi-2024.8.30 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.6.0\r\ngoogle-pasta-0.2.0 grpcio-1.66.1 h5py-3.11.0 idna-3.8 keras-3.5.0\r\nlibclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2\r\nml-dtypes-0.4.0 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.12.1\r\npackaging-24.1 protobuf-4.25.4 pygments-2.18.0 requests-2.32.3 rich-13.8.1\r\nsix-1.16.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2\r\ntensorflow-2.17.0 tensorflow-intel-2.17.0\r\ntensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\r\ntyping-extensions-4.12.2 urllib3-2.2.2 werkzeug-3.0.4 wheel-0.44.0\r\nwrapt-1.16.0\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\n(tensor_env) PS D:\\projetos\\python> python\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64\r\nbit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File\r\n""D:\\projetos\\python\\tensor_env\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File\r\n""D:\\projetos\\python\\tensor_env\\Lib\\site-packages\\tensorflow\\__init__.py"",\r\nline 38, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n # pylint: disable=unused-import\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\r\n""D:\\projetos\\python\\tensor_env\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File\r\n""D:\\projetos\\python\\tensor_env\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"",\r\nline 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A\r\ndynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and\r\nsolutions.\r\nIf you need help, create an issue at\r\nhttps://github.com/tensorflow/tensorflow/issues and include the entire\r\nstack trace above this error message.\r\n>>>\r\n\r\nOn Wed, Sep 11, 2024 at 7:57\u202fPM mraunak ***@***.***> wrote:\r\n\r\n> I see, pls try to activate the virtual env, I see you have already created\r\n> a virtual env\r\n> create virtual env: *python/python.exe -m venv tensor_env*\r\n> activate virtual env: *tensor_env/scripts/activate*\r\n>\r\n> The list of packages installed via pip seems the same in both cases\r\n>\r\n> (venv311_t) D:\\user\\mraunak>pip list\r\n> Package Version\r\n> ------------------------------\r\n>\r\n> absl-py 2.1.0\r\n> astunparse 1.6.3\r\n> certifi 2024.8.30\r\n> charset-normalizer 3.3.2\r\n> flatbuffers 24.3.25\r\n> gast 0.6.0\r\n> google-pasta 0.2.0\r\n> grpcio 1.66.1\r\n> h5py 3.11.0\r\n> idna 3.8\r\n> keras 3.5.0\r\n> libclang 18.1.1\r\n> Markdown 3.7\r\n> markdown-it-py 3.0.0\r\n> MarkupSafe 2.1.5\r\n> mdurl 0.1.2\r\n> ml-dtypes 0.4.0\r\n> namex 0.0.8\r\n> numpy 1.26.4\r\n> opt-einsum 3.3.0\r\n> optree 0.12.1\r\n> packaging 24.1\r\n> pip 24.0\r\n> protobuf 4.25.4\r\n> Pygments 2.18.0\r\n> requests 2.32.3\r\n> rich 13.8.1\r\n> setuptools 65.5.0\r\n> six 1.16.0\r\n> tensorboard 2.17.1\r\n> tensorboard-data-server 0.7.2\r\n> tensorflow 2.17.0\r\n> tensorflow-intel 2.17.0\r\n> tensorflow-io-gcs-filesystem 0.31.0\r\n> termcolor 2.4.0\r\n> typing_extensions 4.12.2\r\n> urllib3 2.2.2\r\n> Werkzeug 3.0.4\r\n> wheel 0.44.0\r\n> wrapt 1.16.0\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/74725#issuecomment-2344850141>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ADSP7K4OJEQ6LWVMOV5DJSTZWDDE7AVCNFSM6AAAAABNJDCR6GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNBUHA2TAMJUGE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 9, 12, 0, 54, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345604912, 'issue_id': 2492999391, 'author': 'tilakrayal', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 9, 12, 8, 28, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347188541, 'issue_id': 2492999391, 'author': 'mraunak', 'body': 'Thank you [syoshio](https://github.com/syoshio). Please try using a different Python version, such as 3.12 or 3.10, and let us know if the issue persists. I tested this on a different Windows machine with Python 3.11.9, but the import was successful on my end', 'created_at': datetime.datetime(2024, 9, 12, 20, 33, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2348025337, 'issue_id': 2492999391, 'author': 'Salmankhan3', 'body': 'I have tried python version 8 as well as 10 bit the same issue persist\r\n\r\n\r\nOn Fri, Sep 13, 2024, 1:33 AM mraunak ***@***.***> wrote:\r\n\r\n> Thank you syoshio <https://github.com/syoshio>. Please try using a\r\n> different Python version, such as 3.12 or 3.10, and let us know if the\r\n> issue persists. I tested this on a different Windows machine with Python\r\n> 3.11.9, but the import was successful on my end\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/74725#issuecomment-2347188541>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/A2GYUZ73HGBFHKES5RUQEL3ZWH3BNAVCNFSM6AAAAABNJDCR6GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNBXGE4DQNJUGE>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 9, 13, 4, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551944642, 'issue_id': 2492999391, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 55, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551944757, 'issue_id': 2492999391, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74725"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74725"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 55, 11, tzinfo=datetime.timezone.utc)}]","rahulsamant37 on (2024-08-29 05:07:01 UTC): The error you're encountering typically happens because TensorFlow relies on certain low-level libraries that may be missing or incompatible with your system setup. 
Here are a few potential solutions to address the DLL load failed error:

1) TensorFlow on Windows requires certain Microsoft C++ runtime libraries. Make sure you have the latest version of the Microsoft Visual C++ Redistributable installed.
2) Run pip show TensorFlow to confirm that the TensorFlow installation is complete and hasn't missed any dependencies.
Sometimes the installation can be corrupted. You can try reinstalling TensorFlow:
```
pip uninstall tensorflow tensorflow-intel
pip install tensorflow
```
3) Ensure that TensorFlow 2.17.0 is fully compatible with Python 3.11.9. While TensorFlow typically supports Python versions close to the latest, it's possible there could be compatibility issues. Try installing a slightly older version of Python, such as 3.10.x or 3.9.x, and see if that resolves the issue.
4) If there are multiple versions of the same DLL files in your PATH, it might be causing conflicts. Make sure your Python environment and TensorFlow are properly isolated using venv or conda.
Create a virtual environment and reinstall TensorFlow:
```
python -m venv tf_env
tf_env\Scripts\activate
pip install tensorflow
```
5) If none of the above solutions work, you can try using the nightly version of TensorFlow, which include bug fixes and updates that address the issue you're encountering:
```
pip install tf-nightly
```


Let me know if any of these solutions work for you!

tilakrayal (Assginee) on (2024-08-29 14:08:36 UTC): @syoshio,
Tensorflow-Intel is an optimized version of TensorFlow for Windows OS that has been produced by Intel.  Could you please confirm whether you are trying to use pip install tensorflow-intel or pip install tensorflow? Thank you!

rahulsamant37 on (2024-08-29 14:46:50 UTC): @tilakrayal,
In third line of relevant output it is mention that it is tensorflow-intel 2.17.0

for your reference I will add it here again:
```
PS C:\Windows\system32> pip install tensorflow
Requirement already satisfied: tensorflow in d:\python311\lib\site-packages (2.17.0)
Requirement already satisfied: tensorflow-intel==2.17.0 in d:\python311\lib\site-packages (from tensorflow) (2.17.0)
Requirement already satisfied: absl-py>=1.0.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)
Requirement already satisfied: h5py>=3.10.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)
Requirement already satisfied: libclang>=13.0.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)
Requirement already satisfied: opt-einsum>=2.3.2 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)
Requirement already satisfied: packaging in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)
Requirement already satisfied: requests<3,>=2.21.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)
Requirement already satisfied: setuptools in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)
Requirement already satisfied: typing-extensions>=3.6.6 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.0)
Requirement already satisfied: tensorboard<2.18,>=2.17 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)
Requirement already satisfied: keras>=3.2.0 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)
Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)
Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\python311\lib\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)
Requirement already satisfied: rich in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)
Requirement already satisfied: namex in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)
Requirement already satisfied: optree in d:\python311\lib\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)
Requirement already satisfied: charset-normalizer<4,>=2 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)
Requirement already satisfied: urllib3<3,>=1.21.1 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in d:\python311\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)
Requirement already satisfied: markdown>=2.6.8 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in d:\python311\lib\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in d:\python311\lib\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)
Requirement already satisfied: markdown-it-py>=2.2.0 in d:\python311\lib\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\python311\lib\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in d:\python311\lib\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)




Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python311\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
Hope you understand why I use this :
```
pip uninstall tensorflow tensorflow-intel
pip install tensorflow
```
Feel free to ask anything if you think my approach is wrong—after all, I'm here to learn as well.
Thank you!

syoshio (Issue Creator) on (2024-08-29 18:19:03 UTC): rahulsamant37,
tilakrayal thanks for help me!
As you recommended, I did uninstall tensorflow and tensorflow-intel, but
when run pip install tensorflow, tensorflow-intel is installed together.
I tried to uninstall only tensorflow-intel, but python complains when I
tried to import tensorflow.
About Microsoft Visual C++ Redistributable, I have the last version ( 14
.40.33810.0,
https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#latest-microsoft-visual-c-redistributable-version
)
I'm still getting error.







PS C:\Windows\system32> pip uninstall tensorflow tensorflow-intel
Found existing installation: tensorflow 2.17.0
Uninstalling tensorflow-2.17.0:
  Would remove:
    d:\python311\lib\site-packages\tensorflow-2.17.0.dist-info\*
Proceed (Y/n)? Y
  Successfully uninstalled tensorflow-2.17.0
Found existing installation: tensorflow-intel 2.17.0
Uninstalling tensorflow-intel-2.17.0:
  Would remove:
    d:\python311\lib\site-packages\tensorflow\*
    d:\python311\lib\site-packages\tensorflow_intel-2.17.0.dist-info\*
    d:\python311\scripts\import_pb_to_tensorboard.exe
    d:\python311\scripts\saved_model_cli.exe
    d:\python311\scripts\tensorboard.exe
    d:\python311\scripts\tf_upgrade_v2.exe
    d:\python311\scripts\tflite_convert.exe
    d:\python311\scripts\toco.exe
    d:\python311\scripts\toco_from_protos.exe
Proceed (Y/n)? Y
  Successfully uninstalled tensorflow-intel-2.17.0



PS C:\Windows\system32> pip install tensorflow
Collecting tensorflow
  Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)
Collecting tensorflow-intel==2.17.0 (from tensorflow)
  Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata
(5.0 kB)
Requirement already satisfied: absl-py>=1.0.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.2.0)
Requirement already satisfied: h5py>=3.10.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.11.0)
Requirement already satisfied: libclang>=13.0.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(18.1.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.4.0)
Requirement already satisfied: opt-einsum>=2.3.2 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.3.0)
Requirement already satisfied: packaging in d:\python311\lib\site-packages
(from tensorflow-intel==2.17.0->tensorflow) (24.1)
Requirement already satisfied:
protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3
in d:\python311\lib\site-packages (from
tensorflow-intel==2.17.0->tensorflow) (4.25.4)
Requirement already satisfied: requests<3,>=2.21.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.32.3)
Requirement already satisfied: setuptools in d:\python311\lib\site-packages
(from tensorflow-intel==2.17.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.4.0)
Requirement already satisfied: typing-extensions>=3.6.6 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.16.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.66.0)
Requirement already satisfied: tensorboard<2.18,>=2.17 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.17.1)
Requirement already satisfied: keras>=3.2.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.5.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.31.0)
Requirement already satisfied: numpy<2.0.0,>=1.23.5 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.26.4)
Requirement already satisfied: wheel<1.0,>=0.23.0 in
d:\python311\lib\site-packages (from
astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)
Requirement already satisfied: rich in d:\python311\lib\site-packages (from
keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)
Requirement already satisfied: namex in d:\python311\lib\site-packages
(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)
Requirement already satisfied: optree in d:\python311\lib\site-packages
(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)
Requirement already satisfied: charset-normalizer<4,>=2 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)
Requirement already satisfied: urllib3<3,>=1.21.1 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)
Requirement already satisfied: markdown>=2.6.8 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in
d:\python311\lib\site-packages (from
werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
(2.1.5)
Requirement already satisfied: markdown-it-py>=2.2.0 in
d:\python311\lib\site-packages (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in
d:\python311\lib\site-packages (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in d:\python311\lib\site-packages
(from
markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
(0.1.2)
Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)
Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)
Installing collected packages: tensorflow-intel, tensorflow
Successfully installed tensorflow-2.17.0 tensorflow-intel-2.17.0



PS C:\Windows\system32> pip uninstall tensorflow-intel
Found existing installation: tensorflow-intel 2.17.0
Uninstalling tensorflow-intel-2.17.0:
  Would remove:
    d:\python311\lib\site-packages\tensorflow\*
    d:\python311\lib\site-packages\tensorflow_intel-2.17.0.dist-info\*
    d:\python311\scripts\import_pb_to_tensorboard.exe
    d:\python311\scripts\saved_model_cli.exe
    d:\python311\scripts\tensorboard.exe
    d:\python311\scripts\tf_upgrade_v2.exe
    d:\python311\scripts\tflite_convert.exe
    d:\python311\scripts\toco.exe
    d:\python311\scripts\toco_from_protos.exe
Proceed (Y/n)? Y
  Successfully uninstalled tensorflow-intel-2.17.0
PS C:\Windows\system32> python
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64
bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
[image: image.png]


PS C:\Windows\system32> python
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64
bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python311\Lib\site-packages\tensorflow\__init__.py"", line 38, in
<module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
 # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and
solutions.
If you need help, create an issue at
https://github.com/tensorflow/tensorflow/issues and include the entire
stack trace above this error message.

On Thu, Aug 29, 2024 at 11:47 AM rahulsamant37 ***@***.***>
wrote:

elhatim03 on (2024-09-01 10:55:46 UTC): I'm dealing with the same problem

 File ""C:\Users\tim\Desktop\tfproject\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\tim\Desktop\tfproject\test.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\tim\Desktop\tfproject\myenv\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>       
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tim\Desktop\tfproject\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\tim\Desktop\tfproject\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

Salmankhan3 on (2024-09-02 14:22:04 UTC): **_### how to solve this_**
Traceback (most recent call last):
  File ""C:\Users\SALMAN  KHAN\Desktop\Python_version_2\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.       

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\SALMAN  KHAN\Desktop\Python_version_2\myenv\tf.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\SALMAN  KHAN\Desktop\Python_version_2\myenv\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\SALMAN  KHAN\Desktop\Python_version_2\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\SALMAN  KHAN\Desktop\Python_version_2\myenv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>    
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message

mraunak on (2024-09-11 20:41:20 UTC): Hi [syoshio](https://github.com/syoshio), I don't see any issue on my end. Please try to download python(Download [Windows installer (64-bit)](https://www.python.org/ftp/python/3.11.9/python-3.11.9-amd64.exe))  again from https://www.python.org/downloads/windows/

![image](https://github.com/user-attachments/assets/67833b80-aae9-4fc9-8b00-49a1c7850520)

Please let me know if the issue persists.

mraunak on (2024-09-11 20:48:40 UTC): steps followed by me
![image](https://github.com/user-attachments/assets/4be92c05-41cd-4465-8dbf-00116cfd9e08)

syoshio (Issue Creator) on (2024-09-11 22:06:54 UTC): Hello mraunak! I can install python and tensorflow, but the error happens
when I try to import tensorflow

PS D:\projetos\python> python -m venv tensor_env
PS D:\projetos\python> pip list
Package                      Version
---------------------------- --------
absl-py                      2.1.0
astunparse                   1.6.3
certifi                      2024.7.4
charset-normalizer           3.3.2
flatbuffers                  24.3.25
gast                         0.6.0
google-pasta                 0.2.0
grpcio                       1.66.0
h5py                         3.11.0
idna                         3.8
keras                        3.5.0
libclang                     18.1.1
Markdown                     3.7
markdown-it-py               3.0.0
MarkupSafe                   2.1.5
mdurl                        0.1.2
ml-dtypes                    0.4.0
namex                        0.0.8
numpy                        1.26.4
opt-einsum                   3.3.0
optree                       0.12.1
packaging                    24.1
pip                          24.0
protobuf                     4.25.4
Pygments                     2.18.0
requests                     2.32.3
rich                         13.8.0
setuptools                   65.5.0
six                          1.16.0
tensorboard                  2.17.1
tensorboard-data-server      0.7.2
tensorflow                   2.17.0
tensorflow-intel             2.17.0
tensorflow-io-gcs-filesystem 0.31.0
termcolor                    2.4.0
typing_extensions            4.12.2
urllib3                      2.2.2
Werkzeug                     3.0.4
wheel                        0.44.0
wrapt                        1.16.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python.exe -m pip install --upgrade pip
PS D:\projetos\python> python --version
Python 3.11.9
PS D:\projetos\python> pip install tensorflow
Requirement already satisfied: tensorflow in d:\python311\lib\site-packages
(2.17.0)
Requirement already satisfied: tensorflow-intel==2.17.0 in
d:\python311\lib\site-packages (from tensorflow) (2.17.0)
Requirement already satisfied: absl-py>=1.0.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.2.0)
Requirement already satisfied: h5py>=3.10.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.11.0)
Requirement already satisfied: libclang>=13.0.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(18.1.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.4.0)
Requirement already satisfied: opt-einsum>=2.3.2 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.3.0)
Requirement already satisfied: packaging in d:\python311\lib\site-packages
(from tensorflow-intel==2.17.0->tensorflow) (24.1)
Requirement already satisfied:
protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3
in d:\python311\lib\site-packages (from
tensorflow-intel==2.17.0->tensorflow) (4.25.4)
Requirement already satisfied: requests<3,>=2.21.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.32.3)
Requirement already satisfied: setuptools in d:\python311\lib\site-packages
(from tensorflow-intel==2.17.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.4.0)
Requirement already satisfied: typing-extensions>=3.6.6 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.16.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.66.0)
Requirement already satisfied: tensorboard<2.18,>=2.17 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(2.17.1)
Requirement already satisfied: keras>=3.2.0 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(3.5.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(0.31.0)
Requirement already satisfied: numpy<2.0.0,>=1.23.5 in
d:\python311\lib\site-packages (from tensorflow-intel==2.17.0->tensorflow)
(1.26.4)
Requirement already satisfied: wheel<1.0,>=0.23.0 in
d:\python311\lib\site-packages (from
astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)
Requirement already satisfied: rich in d:\python311\lib\site-packages (from
keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)
Requirement already satisfied: namex in d:\python311\lib\site-packages
(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)
Requirement already satisfied: optree in d:\python311\lib\site-packages
(from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)
Requirement already satisfied: charset-normalizer<4,>=2 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)
Requirement already satisfied: urllib3<3,>=1.21.1 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in
d:\python311\lib\site-packages (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)
Requirement already satisfied: markdown>=2.6.8 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in
d:\python311\lib\site-packages (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in
d:\python311\lib\site-packages (from
werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
(2.1.5)
Requirement already satisfied: markdown-it-py>=2.2.0 in
d:\python311\lib\site-packages (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in
d:\python311\lib\site-packages (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in d:\python311\lib\site-packages
(from
markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
(0.1.2)

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python.exe -m pip install --upgrade pip
PS D:\projetos\python> python
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64
bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python311\Lib\site-packages\tensorflow\__init__.py"", line 38, in
<module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
 # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File
""D:\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and
solutions.
If you need help, create an issue at
https://github.com/tensorflow/tensorflow/issues and include the entire
stack trace above this error message.

On Wed, Sep 11, 2024 at 5:49 PM mraunak ***@***.***> wrote:

mraunak on (2024-09-11 22:56:55 UTC): I see, can you please try with a different Python version(3.12.1) and let us know if the issue exists

The list of packages installed via pip seems the same in both cases, but it is passing on my end

(venv311_t) D:\user\mraunak>pip list
Package                      Version
---------------------------- ---------
absl-py                      2.1.0
astunparse                   1.6.3
certifi                      2024.8.30
charset-normalizer           3.3.2
flatbuffers                  24.3.25
gast                         0.6.0
google-pasta                 0.2.0
grpcio                       1.66.1
h5py                         3.11.0
idna                         3.8
keras                        3.5.0
libclang                     18.1.1
Markdown                     3.7
markdown-it-py               3.0.0
MarkupSafe                   2.1.5
mdurl                        0.1.2
ml-dtypes                    0.4.0
namex                        0.0.8
numpy                        1.26.4
opt-einsum                   3.3.0
optree                       0.12.1
packaging                    24.1
pip                          24.0
protobuf                     4.25.4
Pygments                     2.18.0
requests                     2.32.3
rich                         13.8.1
setuptools                   65.5.0
six                          1.16.0
tensorboard                  2.17.1
tensorboard-data-server      0.7.2
tensorflow                   2.17.0
tensorflow-intel             2.17.0
tensorflow-io-gcs-filesystem 0.31.0
termcolor                    2.4.0
typing_extensions            4.12.2
urllib3                      2.2.2
Werkzeug                     3.0.4
wheel                        0.44.0
wrapt                        1.16.0

syoshio (Issue Creator) on (2024-09-12 00:54:46 UTC): Hello  mraunak!
I tried as you recommended, but I'm still having problems.

PS D:\projetos\python> Set-ExecutionPolicy

cmdlet Set-ExecutionPolicy at command pipeline position 1
Supply values for the following parameters:
ExecutionPolicy: AllSigned

Execution Policy Change
The execution policy helps protect you from scripts that you do not trust.
Changing the execution policy might expose
you to the security risks described in the about_Execution_Policies help
topic at
https:/go.microsoft.com/fwlink/?LinkID=135170. Do you want to change the
execution policy?
[Y] Yes  [A] Yes to All  [N] No  [L] No to All  [S] Suspend  [?] Help
(default is ""N""): Y
PS D:\projetos\python> tensor_env/scripts/activate

Do you want to run software from this untrusted publisher?
File D:\projetos\python\tensor_env\scripts\Activate.ps1 is published by
CN=Python Software Foundation, O=Python
Software Foundation, L=Beaverton, S=Oregon, C=US and is not trusted on your
system. Only run scripts from trusted
publishers.
[V] Never run  [D] Do not run  [R] Run once  [A] Always run  [?] Help
(default is ""D""): A
(tensor_env) PS D:\projetos\python> pip list
Package    Version
---------- -------
pip        24.0
setuptools 65.5.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python.exe -m pip install --upgrade pip
(tensor_env) PS D:\projetos\python> pip install tensorflow
Collecting tensorflow
  Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)
Collecting tensorflow-intel==2.17.0 (from tensorflow)
  Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata
(5.0 kB)
Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)
Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from
tensorflow-intel==2.17.0->tensorflow)
  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)
Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)
Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)
Collecting ml-dtypes<0.5.0,>=0.3.1 (from
tensorflow-intel==2.17.0->tensorflow)
  Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl.metadata (20 kB)
Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)
Collecting packaging (from tensorflow-intel==2.17.0->tensorflow)
  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)
Collecting
protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3
(from tensorflow-intel==2.17.0->tensorflow)
  Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl.metadata (541 bytes)
Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Requirement already satisfied: setuptools in
d:\projetos\python\tensor_env\lib\site-packages (from
tensorflow-intel==2.17.0->tensorflow) (65.5.0)
Collecting six>=1.12.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)
Collecting typing-extensions>=3.6.6 (from
tensorflow-intel==2.17.0->tensorflow)
  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)
  Downloading grpcio-1.66.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)
Collecting tensorboard<2.18,>=2.17 (from
tensorflow-intel==2.17.0->tensorflow)
  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)
Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from
tensorflow-intel==2.17.0->tensorflow)
  Using cached
tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14
kB)
Collecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.17.0->tensorflow)
  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)
Collecting wheel<1.0,>=0.23.0 (from
astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)
Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)
Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)
Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached optree-0.12.1-cp311-cp311-win_amd64.whl.metadata (48 kB)
Collecting charset-normalizer<4,>=2 (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata
(34 kB)
Collecting idna<4,>=2.5 (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)
Collecting urllib3<3,>=1.21.1 (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)
Collecting certifi>=2017.4.17 (from
requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)
  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)
Collecting markdown>=2.6.8 (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1
kB)
Collecting werkzeug>=1.0.1 (from
tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)
Collecting MarkupSafe>=2.1.1 (from
werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)
  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)
Collecting markdown-it-py>=2.2.0 (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Collecting pygments<3.0.0,>=2.13.0 (from
rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)
Collecting mdurl~=0.1 (from
markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Using cached tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)
Using cached tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)
Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)
Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)
Using cached gast-0.6.0-py3-none-any.whl (21 kB)
Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Downloading grpcio-1.66.1-cp311-cp311-win_amd64.whl (4.3 MB)
   ---------------------------------------- 4.3/4.3 MB 8.8 MB/s eta 0:00:00
Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)
Using cached keras-3.5.0-py3-none-any.whl (1.1 MB)
Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)
Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl (126 kB)
Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)
Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)
Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl
(1.5 MB)
Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)
Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)
Using cached packaging-24.1-py3-none-any.whl (53 kB)
Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)
   ---------------------------------------- 167.3/167.3 kB 5.1 MB/s eta
0:00:00
Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)
Using cached idna-3.8-py3-none-any.whl (66 kB)
Using cached Markdown-3.7-py3-none-any.whl (106 kB)
Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)
Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)
Using cached werkzeug-3.0.4-py3-none-any.whl (227 kB)
Using cached wheel-0.44.0-py3-none-any.whl (67 kB)
Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)
Using cached optree-0.12.1-cp311-cp311-win_amd64.whl (268 kB)
Downloading rich-13.8.1-py3-none-any.whl (241 kB)
   ---------------------------------------- 241.6/241.6 kB 14.5 MB/s eta
0:00:00
Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)
Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel,
urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem,
tensorboard-data-server, six, pygments, protobuf, packaging, numpy, mdurl,
MarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi,
absl-py, werkzeug, requests, optree, opt-einsum, ml-dtypes, markdown-it-py,
h5py, google-pasta, astunparse, tensorboard, rich, keras, tensorflow-intel,
tensorflow
Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3
certifi-2024.8.30 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.6.0
google-pasta-0.2.0 grpcio-1.66.1 h5py-3.11.0 idna-3.8 keras-3.5.0
libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2
ml-dtypes-0.4.0 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.12.1
packaging-24.1 protobuf-4.25.4 pygments-2.18.0 requests-2.32.3 rich-13.8.1
six-1.16.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2
tensorflow-2.17.0 tensorflow-intel-2.17.0
tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0
typing-extensions-4.12.2 urllib3-2.2.2 werkzeug-3.0.4 wheel-0.44.0
wrapt-1.16.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python.exe -m pip install --upgrade pip
(tensor_env) PS D:\projetos\python> python
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64
bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File
""D:\projetos\python\tensor_env\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File
""D:\projetos\python\tensor_env\Lib\site-packages\tensorflow\__init__.py"",
line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
 # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File
""D:\projetos\python\tensor_env\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File
""D:\projetos\python\tensor_env\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A
dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and
solutions.
If you need help, create an issue at
https://github.com/tensorflow/tensorflow/issues and include the entire
stack trace above this error message.

On Wed, Sep 11, 2024 at 7:57 PM mraunak ***@***.***> wrote:

tilakrayal (Assginee) on (2024-09-12 08:28:18 UTC): @learning-to-play

mraunak on (2024-09-12 20:33:02 UTC): Thank you [syoshio](https://github.com/syoshio). Please try using a different Python version, such as 3.12 or 3.10, and let us know if the issue persists. I tested this on a different Windows machine with Python 3.11.9, but the import was successful on my end

Salmankhan3 on (2024-09-13 04:50:09 UTC): I have tried python version 8 as well as 10 bit the same issue persist


On Fri, Sep 13, 2024, 1:33 AM mraunak ***@***.***> wrote:

mihaimaruseac on (2024-12-18 17:55:08 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:55:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74725"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74725"">No</a>

"
2492524277,issue,closed,completed,TypeError: unhashable type: 'list' in tensorflow-2.17.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

No

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi all,

I repeatedly get the following error while working with the current version of TensorFlow. Could you please let me know what my issue is?

### Standalone code to reproduce the issue

```shell
from __future__ import print_function

import tensorflow as tf

# Simple hello world using TensorFlow

# Create a Constant op
# The op is added as a node to the default graph.
#
# The value returned by the constructor represents the output
# of the Constant op.
hello = tf.constant('Hello, TensorFlow!')

# Start tf session
sess = tf.Session()

# Run the op
print(sess.run(hello))
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[38], line 3
      1 from __future__ import print_function
----> 3 import tensorflow as tf
      5 # Simple hello world using TensorFlow
      6 
      7 # Create a Constant op
   (...)
     10 # The value returned by the constructor represents the output
     11 # of the Constant op.
     12 hello = tf.constant('Hello, TensorFlow!')

File c:\Users\narmehran\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py:47
     44 from tensorflow.python import tf2 as _tf2
     45 _tf2.enable()
---> 47 from tensorflow._api.v2 import __internal__
     48 from tensorflow._api.v2 import __operators__
     49 from tensorflow._api.v2 import audio

File c:\Users\narmehran\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\_api\v2\__internal__\__init__.py:8
      3 """"""Public API for tf._api.v2.__internal__ namespace
      4 """"""
      6 import sys as _sys
----> 8 from tensorflow._api.v2.__internal__ import autograph
...
--> 205     all_params = set(params)
    206     if len(all_params) < len(params):
    207         new_params = []

TypeError: unhashable type: 'list'
```
",SiNa88,2024-08-28 16:20:01+00:00,['Venkat6871'],2024-10-15 02:02:55+00:00,2024-10-15 02:02:50+00:00,https://github.com/tensorflow/tensorflow/issues/74701,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2317058898, 'issue_id': 2492524277, 'author': 'Venkat6871', 'body': 'Hi @SiNa88 ,\r\nThe code you are using is based on an outdated version of TensorFlow (1.x), which is now deprecated and no longer supported. Please update your code to use the latest version of TensorFlow. I have provided a [gist](https://colab.research.google.com/gist/Venkat6871/9920144f2387cfc30989860171aa6ad8/74701_2-17-v.ipynb) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 29, 8, 51, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318060933, 'issue_id': 2492524277, 'author': 'rahulsamant37', 'body': ""Issue #74701 \r\nAs my friend explained above how you can make changes to upgrade from TensorFlow 1.x to TensorFlow 2.x.\r\nBut, If you need to maintain compatibility with TensorFlow 1.x, consider using TensorFlow's compatibility module:\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\n# Create a Constant op\r\nhello = tf.constant('Hello, TensorFlow!')\r\n\r\n# Start tf session\r\nsess = tf.Session()\r\n\r\n# Run the op\r\nprint(sess.run(hello))\r\n```\r\nThis code will allow you to use TensorFlow 1.x features while running in TensorFlow 2.x.\r\n\r\nLet me know if this solutions work for you!"", 'created_at': datetime.datetime(2024, 8, 29, 15, 17, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318927071, 'issue_id': 2492524277, 'author': 'SiNa88', 'body': '@Venkat6871 Thank you so much. On my machine at work, after installing Tensorflow 2.17.0, your solution worked quite well.\r\nHowever, I still have the following issue on my laptop at home. \r\nThough multiple times I tried to uninstall and install tensorFlow, tensorFlow-intel, tensorFlow-cpu, etc., it still does not work.\r\nI do not know what the incompatibility is at this level.\r\n@rahulsamant37 thank you. I guess I would prefer Tensorflow_v2.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[44], [line 3](vscode-notebook-cell:?execution_count=44&line=3)\r\n      [1](vscode-notebook-cell:?execution_count=44&line=1) #import tensorflow as tf\r\n      [2](vscode-notebook-cell:?execution_count=44&line=2) #print(tf.__version__)\r\n----> [3](vscode-notebook-cell:?execution_count=44&line=3) import tensorflow\r\n\r\nFile [c:\\Users\\sina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\__init__.py:47](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:47)\r\n     [44](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:44) from tensorflow.python import tf2 as _tf2\r\n     [45](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:45) _tf2.enable()\r\n---> [47](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:47) from tensorflow._api.v2 import __internal__\r\n     [48](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:48) from tensorflow._api.v2 import __operators__\r\n     [49](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:49) from tensorflow._api.v2 import audio\r\n\r\nFile [c:\\Users\\sina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8)\r\n      [3](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:3) """"""Public API for tf._api.v2.__internal__ namespace\r\n      [4](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:4) """"""\r\n      [6](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:6) import sys as _sys\r\n----> [8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8) from tensorflow._api.v2.__internal__ import autograph\r\n      [9](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:9) from tensorflow._api.v2.__internal__ import decorator\r\n     [10](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:10) from tensorflow._api.v2.__internal__ import dispatch\r\n\r\nFile [c:\\Users\\sina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8)\r\n      [3](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:3) """"""Public API for tf._api.v2.__internal__.autograph namespace\r\n      [4](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:4) """"""\r\n...\r\n--> [205](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:205)     all_params = set(params)\r\n    [206](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:206)     if len(all_params) < len(params):\r\n    [207](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:207)         new_params = []\r\n\r\nTypeError: unhashable type: \'list\'\r\n```', 'created_at': datetime.datetime(2024, 8, 29, 20, 35, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342756001, 'issue_id': 2492524277, 'author': 'Venkat6871', 'body': 'Hi **@SiNa88** ,\r\nApologies for the delay. Could you please double-check which Python version you are using and whether it is compatible with TensorFlow? Additionally, please confirm which OS platform you are using. I am providing [documentation](https://www.tensorflow.org/install/source) for checking version compatibility, please review it. Let us know if the issue persists.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 11, 6, 32, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346093902, 'issue_id': 2492524277, 'author': 'SiNa88', 'body': 'Hi @Venkat6871,\r\n\r\nAs you suggested, I checked the version from the TensorFlow [documentation](https://www.tensorflow.org/install/source_windows#tested_build_configurations). \r\nThen, I upgraded the _python3.9.1_ to _python3.9.13_, which worked fine.\r\n\r\nThank you so much for your reply.', 'created_at': datetime.datetime(2024, 9, 12, 11, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382193156, 'issue_id': 2492524277, 'author': 'Venkat6871', 'body': 'Hi **@SiNa88**,\r\nI am glad to see your issue is resolved. Could you please close this issue since it has been resolved? If you are still facing any issues, please feel free to ask.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 6, 10, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398492160, 'issue_id': 2492524277, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 8, 2, 2, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662491, 'issue_id': 2492524277, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662574, 'issue_id': 2492524277, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74701"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74701"">No</a>', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 54, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-29 08:51:26 UTC): Hi @SiNa88 ,
The code you are using is based on an outdated version of TensorFlow (1.x), which is now deprecated and no longer supported. Please update your code to use the latest version of TensorFlow. I have provided a [gist](https://colab.research.google.com/gist/Venkat6871/9920144f2387cfc30989860171aa6ad8/74701_2-17-v.ipynb) for your reference.
Thank you!

rahulsamant37 on (2024-08-29 15:17:10 UTC): Issue #74701 
As my friend explained above how you can make changes to upgrade from TensorFlow 1.x to TensorFlow 2.x.
But, If you need to maintain compatibility with TensorFlow 1.x, consider using TensorFlow's compatibility module:
```
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# Create a Constant op
hello = tf.constant('Hello, TensorFlow!')

# Start tf session
sess = tf.Session()

# Run the op
print(sess.run(hello))
```
This code will allow you to use TensorFlow 1.x features while running in TensorFlow 2.x.

Let me know if this solutions work for you!

SiNa88 (Issue Creator) on (2024-08-29 20:35:03 UTC): @Venkat6871 Thank you so much. On my machine at work, after installing Tensorflow 2.17.0, your solution worked quite well.
However, I still have the following issue on my laptop at home. 
Though multiple times I tried to uninstall and install tensorFlow, tensorFlow-intel, tensorFlow-cpu, etc., it still does not work.
I do not know what the incompatibility is at this level.
@rahulsamant37 thank you. I guess I would prefer Tensorflow_v2.

```
import tensorflow as tf
print(tf.__version__)
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[44], [line 3](vscode-notebook-cell:?execution_count=44&line=3)
      [1](vscode-notebook-cell:?execution_count=44&line=1) #import tensorflow as tf
      [2](vscode-notebook-cell:?execution_count=44&line=2) #print(tf.__version__)
----> [3](vscode-notebook-cell:?execution_count=44&line=3) import tensorflow

File [c:\Users\sina\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py:47](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:47)
     [44](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:44) from tensorflow.python import tf2 as _tf2
     [45](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:45) _tf2.enable()
---> [47](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:47) from tensorflow._api.v2 import __internal__
     [48](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:48) from tensorflow._api.v2 import __operators__
     [49](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/__init__.py:49) from tensorflow._api.v2 import audio

File [c:\Users\sina\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\_api\v2\__internal__\__init__.py:8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8)
      [3](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:3) """"""Public API for tf._api.v2.__internal__ namespace
      [4](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:4) """"""
      [6](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:6) import sys as _sys
----> [8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8) from tensorflow._api.v2.__internal__ import autograph
      [9](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:9) from tensorflow._api.v2.__internal__ import decorator
     [10](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/__init__.py:10) from tensorflow._api.v2.__internal__ import dispatch

File [c:\Users\sina\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\_api\v2\__internal__\autograph\__init__.py:8](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8)
      [3](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:3) """"""Public API for tf._api.v2.__internal__.autograph namespace
      [4](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:4) """"""
...
--> [205](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:205)     all_params = set(params)
    [206](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:206)     if len(all_params) < len(params):
    [207](file:///C:/Users/sina/AppData/Local/Programs/Python/Python39/lib/typing.py:207)         new_params = []

TypeError: unhashable type: 'list'
```

Venkat6871 (Assginee) on (2024-09-11 06:32:38 UTC): Hi **@SiNa88** ,
Apologies for the delay. Could you please double-check which Python version you are using and whether it is compatible with TensorFlow? Additionally, please confirm which OS platform you are using. I am providing [documentation](https://www.tensorflow.org/install/source) for checking version compatibility, please review it. Let us know if the issue persists.
Thank you!

SiNa88 (Issue Creator) on (2024-09-12 11:59:40 UTC): Hi @Venkat6871,

As you suggested, I checked the version from the TensorFlow [documentation](https://www.tensorflow.org/install/source_windows#tested_build_configurations). 
Then, I upgraded the _python3.9.1_ to _python3.9.13_, which worked fine.

Thank you so much for your reply.

Venkat6871 (Assginee) on (2024-09-30 06:10:13 UTC): Hi **@SiNa88**,
I am glad to see your issue is resolved. Could you please close this issue since it has been resolved? If you are still facing any issues, please feel free to ask.
Thank you!

github-actions[bot] on (2024-10-08 02:02:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-15 02:02:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-15 02:02:54 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74701"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74701"">No</a>

"
2492001716,issue,closed,completed,"Please stop supporting this lab, please stop writing ml libs, please change your area of ​​activity","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Every version

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

All version of your shitty lib are nonsense and bugged.
You don't fix plenty of huge issues for years.
Stop this please. You bring so much evil with this shit. The time people spend for your shitty framework costs many lives. You are almost killing people.
And don't even ask what is the problem. Problem is everything: speed, versions compatibility, serialization, logging, debugging. Just everything is broken bugged shit. When you fix one nonsense bug another occurs. 
Just submit that you are not good at it and stop. 

### Standalone code to reproduce the issue

```shell
All version of your shitty lib are nonsense and bugged.
You don't fix plenty of huge issues for years.
Stop this please. You bring so much evil with this shit. The time people spend for your shitty framework costs many lives. You are almost killing people.
And don't even ask what is the problem. Problem is everything: speed, versions compatibility, serialization, logging, debugging. Just everything is broken bugged shit. When you fix one nonsense bug another occurs. 
Just submit that you are not good at it and stop.
```


### Relevant log output

_No response_",AndreyStille,2024-08-28 12:54:35+00:00,['tilakrayal'],2024-08-29 03:44:00+00:00,2024-08-29 03:43:51+00:00,https://github.com/tensorflow/tensorflow/issues/74695,"[('type:bug', 'Bug')]","[{'comment_id': 2316655890, 'issue_id': 2492001716, 'author': 'mihaimaruseac', 'body': 'Closing as rant', 'created_at': datetime.datetime(2024, 8, 29, 3, 43, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316655907, 'issue_id': 2492001716, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74695"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74695"">No</a>', 'created_at': datetime.datetime(2024, 8, 29, 3, 43, 53, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-08-29 03:43:51 UTC): Closing as rant

google-ml-butler[bot] on (2024-08-29 03:43:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74695"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74695"">No</a>

"
2491581374,issue,closed,completed,Specifies the gpu core,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.30

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when  using the C++ API,  Whether tflite can be specified to execute on a core of the gpu 

### Standalone code to reproduce the issue

```shell
In python ; maybe like with tf.device('/device:GPU:2'):
but we can't find how to use in C

thanks
```


### Relevant log output

_No response_",keke444,2024-08-28 09:38:16+00:00,['Venkat6871'],2024-08-28 11:49:42+00:00,2024-08-28 11:49:39+00:00,https://github.com/tensorflow/tensorflow/issues/74685,"[('type:support', 'Support issues')]","[{'comment_id': 2314998872, 'issue_id': 2491581374, 'author': 'rahulsamant37', 'body': 'Unlike the Python API where tf.device(\'/device:GPU:2\') can be used to specify the execution on a specific GPU core, the TFLite C++ API does not directly support selecting a specific GPU core in such a straightforward manner.\r\n\r\nTensorFlow Lite provides GPU acceleration via a delegate, but the selection of specific cores is less flexible than in the full TensorFlow API. You can still enable GPU acceleration by setting up the GPU delegate.\r\n\r\n```\r\n#include ""tensorflow/lite/delegates/gpu/delegate.h""\r\n\r\ntflite::InterpreterBuilder builder(model, resolver);\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\r\nbuilder(&interpreter);\r\n\r\n// Create GPU delegate\r\nTfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);\r\nif (interpreter->ModifyGraphWithDelegate(gpu_delegate) != kTfLiteOk) {\r\n    // Handle error\r\n}\r\n\r\n// Interpreter will now run on the GPU\r\ninterpreter->Invoke();\r\n\r\n// Clean up\r\nTfLiteGpuDelegateV2Delete(gpu_delegate);\r\n```\r\n\r\nIf you need to assign TFLite to a specific GPU core, you might need to rely on CUDA environment variables, like CUDA_VISIBLE_DEVICES. This is an indirect way to ""select"" the GPU when TensorFlow Lite runs, though it’s not done via the TFLite API itself.\r\n\r\nBefore starting your application, set the environment variable in your code or shell:\r\n\r\n```\r\nexport CUDA_VISIBLE_DEVICES=2  # This limits execution to GPU 2\r\n```\r\n\r\nCurrently, TensorFlow Lite\'s C++ API does not offer a built-in mechanism to assign operations to a specific GPU core directly. The available workaround involves using the GPU delegate to enable GPU acceleration and controlling device visibility via environment variables like CUDA_VISIBLE_DEVICES. If more control is needed, you may have to combine TFLite with platform-specific APIs.\r\n\r\nI hope this answers your query. If needed, I can create a pull request for your code.', 'created_at': datetime.datetime(2024, 8, 28, 11, 0, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315113090, 'issue_id': 2491581374, 'author': 'keke444', 'body': '> Unlike the Python API where tf.device(\'/device:GPU:2\') can be used to specify the execution on a specific GPU core, the TFLite C++ API does not directly support selecting a specific GPU core in such a straightforward manner.\r\n> \r\n> TensorFlow Lite provides GPU acceleration via a delegate, but the selection of specific cores is less flexible than in the full TensorFlow API. You can still enable GPU acceleration by setting up the GPU delegate.\r\n> \r\n> ```\r\n> #include ""tensorflow/lite/delegates/gpu/delegate.h""\r\n> \r\n> tflite::InterpreterBuilder builder(model, resolver);\r\n> std::unique_ptr<tflite::Interpreter> interpreter;\r\n> \r\n> builder(&interpreter);\r\n> \r\n> // Create GPU delegate\r\n> TfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);\r\n> if (interpreter->ModifyGraphWithDelegate(gpu_delegate) != kTfLiteOk) {\r\n>     // Handle error\r\n> }\r\n> \r\n> // Interpreter will now run on the GPU\r\n> interpreter->Invoke();\r\n> \r\n> // Clean up\r\n> TfLiteGpuDelegateV2Delete(gpu_delegate);\r\n> ```\r\n> \r\n> If you need to assign TFLite to a specific GPU core, you might need to rely on CUDA environment variables, like CUDA_VISIBLE_DEVICES. This is an indirect way to ""select"" the GPU when TensorFlow Lite runs, though it’s not done via the TFLite API itself.\r\n> \r\n> Before starting your application, set the environment variable in your code or shell:\r\n> \r\n> ```\r\n> export CUDA_VISIBLE_DEVICES=2  # This limits execution to GPU 2\r\n> ```\r\n> \r\n> Currently, TensorFlow Lite\'s C++ API does not offer a built-in mechanism to assign operations to a specific GPU core directly. The available workaround involves using the GPU delegate to enable GPU acceleration and controlling device visibility via environment variables like CUDA_VISIBLE_DEVICES. If more control is needed, you may have to combine TFLite with platform-specific APIs.\r\n> \r\n> I hope this answers your query. If needed, I can create a pull request for your code.\r\n\r\nI understand. Thank you so much', 'created_at': datetime.datetime(2024, 8, 28, 11, 49, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315113152, 'issue_id': 2491581374, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74685"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74685"">No</a>', 'created_at': datetime.datetime(2024, 8, 28, 11, 49, 41, tzinfo=datetime.timezone.utc)}]","rahulsamant37 on (2024-08-28 11:00:05 UTC): Unlike the Python API where tf.device('/device:GPU:2') can be used to specify the execution on a specific GPU core, the TFLite C++ API does not directly support selecting a specific GPU core in such a straightforward manner.

TensorFlow Lite provides GPU acceleration via a delegate, but the selection of specific cores is less flexible than in the full TensorFlow API. You can still enable GPU acceleration by setting up the GPU delegate.

```
#include ""tensorflow/lite/delegates/gpu/delegate.h""

tflite::InterpreterBuilder builder(model, resolver);
std::unique_ptr<tflite::Interpreter> interpreter;

builder(&interpreter);

// Create GPU delegate
TfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);
if (interpreter->ModifyGraphWithDelegate(gpu_delegate) != kTfLiteOk) {
    // Handle error
}

// Interpreter will now run on the GPU
interpreter->Invoke();

// Clean up
TfLiteGpuDelegateV2Delete(gpu_delegate);
```

If you need to assign TFLite to a specific GPU core, you might need to rely on CUDA environment variables, like CUDA_VISIBLE_DEVICES. This is an indirect way to ""select"" the GPU when TensorFlow Lite runs, though it’s not done via the TFLite API itself.

Before starting your application, set the environment variable in your code or shell:

```
export CUDA_VISIBLE_DEVICES=2  # This limits execution to GPU 2
```

Currently, TensorFlow Lite's C++ API does not offer a built-in mechanism to assign operations to a specific GPU core directly. The available workaround involves using the GPU delegate to enable GPU acceleration and controlling device visibility via environment variables like CUDA_VISIBLE_DEVICES. If more control is needed, you may have to combine TFLite with platform-specific APIs.

I hope this answers your query. If needed, I can create a pull request for your code.

keke444 (Issue Creator) on (2024-08-28 11:49:39 UTC): I understand. Thank you so much

google-ml-butler[bot] on (2024-08-28 11:49:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74685"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74685"">No</a>

"
2491189316,issue,closed,completed,While op issue in TFLM,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 22.04):
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.15.0.post1

**Background of the issue**
I created a conformer transducer model following the Conformer paper. The model consists of encoder, decoder and the joiner network. The Prediction network contains a single LSTM Layer. I converted the model to INT8 using the `get_concrete_function()`.
```python 
concrete_func = model.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
```
The model converted successfully and after visualising the model in netron, i verified that none of the ops were missing. I then flashed the model in **ESP32S3** and kept on getting this error infinitely in `while.cc` op. I have pasted the screenshot of the same below.
![image](https://github.com/user-attachments/assets/4c0398f1-25b4-4d52-a55d-45f0e1ec0513)

I then created a simple model consisting of **only while loop** and then flashed it. I faced the same error. 
NOTE: `AllocateTensors()` was passed

**Provide the text output from tflite_convert**
``` python
W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.
I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp839tyn2a
I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp839tyn2a
I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp839tyn2a
I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 13069 microseconds.
I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 2, Total Ops 18, % non-converted = 11.11 %
 * 2 ARITH ops

- arith.constant:    2 occurrences  (i32: 2)

  (i1: 1, i32: 1)


  (i32: 2)
  (i1: 1)
  (i32: 1)
  (i32: 1)

I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 3  ops, equivalently 1  MACs
fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32
I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 3  ops, equivalently 1  MACs
```

**Standalone code to reproduce the issue** 
This is how I converted the sample model consisting only while loop
```python
import tensorflow as tf

def convert_tflite(model, rep_data):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.experimental_new_converter = True
    converter.representative_dataset = rep_data
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.allow_custom_ops = True
    converter._experimental_lower_tensor_list_ops = False
    tflite_model = converter.convert()
    with open(""model.tflite"", ""wb"") as f:
        f.write(tflite_model)

class SimpleWhileLoopModel(tf.keras.Model):
    def __init__(self):
        super(SimpleWhileLoopModel, self).__init__()

    @tf.function
    def call(self, inputs):
        i, result = tf.constant(0), tf.constant(0)
        while i < inputs:
            result += i * i
            i += 1
        return result

model = SimpleWhileLoopModel()
model.summary()


def representative_dataset_gen():
    for limit in range(1, 11):
        yield [tf.constant(limit, dtype=tf.int32)]

convert_tflite(model, rep_data = representative_dataset_gen)
```

Also, please include a link to a GraphDef or the model if possible.
![image](https://github.com/user-attachments/assets/cf7ed442-7385-407c-9fe7-5a374c1506af)

**Any other info / logs**

This is what I meant by the loop getting stuck
```bash
E (331202) task_wdt: Task watchdog got triggered. The following tasks did not reset the watchdog in time:
E (331202) task_wdt:  - IDLE (CPU 0)
E (331202) task_wdt: Tasks currently running:
E (331202) task_wdt: CPU 0: main
E (331202) task_wdt: CPU 1: IDLE
E (331202) task_wdt: Print CPU 0 (current core) backtrace


Backtrace: 0x4204E152:0x3FC92460 0x403770C9:0x3FC92480 0x42041845:0x3FCF39E0 0x4201AA06:0x3FCF3A00 0x42030772:0x3FCF3A40 0x4200927E:0x3FCF3A70 0x42008F5E:0x3FCF3AA0 0x42006CA3:0x3FCF3AC0 0x42006B36:0x3FCF3AE0 0x42006B2B:0x3FCF3B00 0x4206162C:0x3FCF3B20 0x4037C795:0x3FCF3B40
0x4204e152: task_wdt_isr at esp-adf/esp-idf/components/esp_system/task_wdt.c:183 (discriminator 3)

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114

0x42041845: tflite::TfLiteTypeSizeOf(TfLiteType, unsigned int*) at dependency/esp-tflite-micro/tensorflow/lite/micro/memory_helpers.cc:87
 (inlined by) tflite::TfLiteEvalTensorByteLength(TfLiteEvalTensor const*, unsigned int*) at dependency/esp-tflite-micro/tensorflow/lite/micro/memory_helpers.cc:135

0x4201aa06: tflite::micro::ValidateAndGetTensorSizes(TfLiteEvalTensor const*, TfLiteEvalTensor const*) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/kernel_util.cc:156
 (inlined by) tflite::micro::CopySubgraphOutputsToOpOutputs(TfLiteContext*, TfLiteNode*, tflite::MicroGraph*, int) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/kernel_util.cc:262

0x42030772: tflite::(anonymous namespace)::WhileEval(TfLiteContext*, TfLiteNode*) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/while.cc:110

0x4200927e: tflite::MicroInterpreterGraph::InvokeSubgraph(int) at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:194

0x42008f5e: tflite::MicroInterpreter::Invoke() at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter.cc:294

0x42006ca3: loop at application/ml_testing/src/main_functions.cc:259

0x42006b36: app_init at application/ml_testing/src/ml_testing.cpp:11 (discriminator 1)

0x42006b2b: app_main at main/src/audio_frame_work.cpp:10

0x4206162c: main_task at esp-adf/esp-idf/components/freertos/port/port_common.c:141 (discriminator 2)

0x4037c795: vPortTaskWrapper at esp-adf/esp-idf/components/freertos/port/xtensa/port.c:142


E (331202) task_wdt: Print CPU 1 backtrace


Backtrace: 0x40378659:0x3FC92A60 0x403770C9:0x3FC92A80 0x400559DD:0x3FCF4940 |<-CORRUPTED
0x40378659: esp_crosscore_isr at esp-adf/esp-idf/components/esp_system/crosscore_int.c:92

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114


E (336202) task_wdt: Task watchdog got triggered. The following tasks did not reset the watchdog in time:
E (336202) task_wdt:  - IDLE (CPU 0)
E (336202) task_wdt: Tasks currently running:
E (336202) task_wdt: CPU 0: main
E (336202) task_wdt: CPU 1: IDLE
E (336202) task_wdt: Print CPU 0 (current core) backtrace


Backtrace: 0x4204E152:0x3FC92460 0x403770C9:0x3FC92480 0x420326B6:0x3FCF39F0 0x4200927E:0x3FCF3A10 0x42030762:0x3FCF3A40 0x4200927E:0x3FCF3A70 0x42008F5E:0x3FCF3AA0 0x42006CA3:0x3FCF3AC0 0x42006B36:0x3FCF3AE0 0x42006B2B:0x3FCF3B00 0x4206162C:0x3FCF3B20 0x4037C795:0x3FCF3B40
0x4204e152: task_wdt_isr at esp-adf/esp-idf/components/esp_system/task_wdt.c:183 (discriminator 3)

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114

0x420326b6: tflite::AddEval(TfLiteContext*, TfLiteNode*) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/esp_nn/add.cc:213

0x4200927e: tflite::MicroInterpreterGraph::InvokeSubgraph(int) at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:194

0x42030762: tflite::(anonymous namespace)::WhileEval(TfLiteContext*, TfLiteNode*) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/while.cc:107

0x4200927e: tflite::MicroInterpreterGraph::InvokeSubgraph(int) at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:194

0x42008f5e: tflite::MicroInterpreter::Invoke() at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter.cc:294

0x42006ca3: loop at application/ml_testing/src/main_functions.cc:259

0x42006b36: app_init at application/ml_testing/src/ml_testing.cpp:11 (discriminator 1)

0x42006b2b: app_main at main/src/audio_frame_work.cpp:10

0x4206162c: main_task at esp-adf/esp-idf/components/freertos/port/port_common.c:141 (discriminator 2)

0x4037c795: vPortTaskWrapper at esp-adf/esp-idf/components/freertos/port/xtensa/port.c:142


E (336202) task_wdt: Print CPU 1 backtrace


Backtrace: 0x40378659:0x3FC92A60 0x403770C9:0x3FC92A80 0x400559DD:0x3FCF4940 |<-CORRUPTED
0x40378659: esp_crosscore_isr at esp-adf/esp-idf/components/esp_system/crosscore_int.c:92

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114


E (341202) task_wdt: Task watchdog got triggered. The following tasks did not reset the watchdog in time:
E (341202) task_wdt:  - IDLE (CPU 0)
E (341202) task_wdt: Tasks currently running:
E (341202) task_wdt: CPU 0: main
E (341202) task_wdt: CPU 1: IDLE
E (341202) task_wdt: Print CPU 0 (current core) backtrace


Backtrace: 0x4204E152:0x3FC92460 0x403770C9:0x3FC92480 0x4200924C:0x3FCF3A10 0x42030762:0x3FCF3A40 0x4200927E:0x3FCF3A70 0x42008F5E:0x3FCF3AA0 0x42006CA3:0x3FCF3AC0 0x42006B36:0x3FCF3AE0 0x42006B2B:0x3FCF3B00 0x4206162C:0x3FCF3B20 0x4037C795:0x3FCF3B40
0x4204e152: task_wdt_isr at esp-adf/esp-idf/components/esp_system/task_wdt.c:183 (discriminator 3)

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114

0x4200924c: tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator) at dependency/esp-tflite-micro/tensorflow/lite/schema/schema_generated.h:1641
 (inlined by) OpNameFromRegistration at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:34
 (inlined by) tflite::MicroInterpreterGraph::InvokeSubgraph(int) at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:190

0x42030762: tflite::(anonymous namespace)::WhileEval(TfLiteContext*, TfLiteNode*) at dependency/esp-tflite-micro/tensorflow/lite/micro/kernels/while.cc:107

0x4200927e: tflite::MicroInterpreterGraph::InvokeSubgraph(int) at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter_graph.cc:194

0x42008f5e: tflite::MicroInterpreter::Invoke() at dependency/esp-tflite-micro/tensorflow/lite/micro/micro_interpreter.cc:294

0x42006ca3: loop at application/ml_testing/src/main_functions.cc:259

0x42006b36: app_init at application/ml_testing/src/ml_testing.cpp:11 (discriminator 1)

0x42006b2b: app_main at main/src/audio_frame_work.cpp:10

0x4206162c: main_task at esp-adf/esp-idf/components/freertos/port/port_common.c:141 (discriminator 2)

0x4037c795: vPortTaskWrapper at esp-adf/esp-idf/components/freertos/port/xtensa/port.c:142


E (341202) task_wdt: Print CPU 1 backtrace


Backtrace: 0x40378659:0x3FC92A60 0x403770C9:0x3FC92A80 0x400559DD:0x3FCF4940 |<-CORRUPTED
0x40378659: esp_crosscore_isr at esp-adf/esp-idf/components/esp_system/crosscore_int.c:92

0x403770c9: _xt_lowint1 at esp-adf/esp-idf/components/freertos/port/xtensa/xtensa_vectors.S:1114
```
",HemanthSai7,2024-08-28 06:34:34+00:00,['gaikwadrahul8'],2024-09-21 16:10:03+00:00,2024-09-21 16:10:01+00:00,https://github.com/tensorflow/tensorflow/issues/74665,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2365183200, 'issue_id': 2491189316, 'author': 'gaikwadrahul8', 'body': ""Hi, @HemanthSai7\r\n\r\nI apologize for the delayed response, I see you've posted this issue in this repo :https://github.com/tensorflow/tflite-micro/issues/2674 so that team will help you further on this While OP Issue in TFLM so please feel free to close this issue from your end.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 21, 13, 15, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2365238807, 'issue_id': 2491189316, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74665"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74665"">No</a>', 'created_at': datetime.datetime(2024, 9, 21, 16, 10, 3, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-21 13:15:25 UTC): Hi, @HemanthSai7

I apologize for the delayed response, I see you've posted this issue in this repo :https://github.com/tensorflow/tflite-micro/issues/2674 so that team will help you further on this While OP Issue in TFLM so please feel free to close this issue from your end.

Thank you for your cooperation and patience.

google-ml-butler[bot] on (2024-09-21 16:10:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74665"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74665"">No</a>

"
2489948174,issue,closed,completed,"Issue including flatbuffers, or incomplete documentation","**System information**
- Android Device information: N/A
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- com.google.android.gms:play-services-tflite-java:16.2.0-beta02
   com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version: N/A

**Standalone code to reproduce the issue**
In my C++ file:
`#include flatbuffers/flatbuffers.h`

In my CMakeLists.txt file:
`find_package(tensorflowlite_jni_gms_client REQUIRED CONFIG)

target_link_libraries(${TARGET_LIBRARY} PUBLIC # your JNI lib target
                tensorflowlite_jni_gms_client::tensorflowlite_jni_gms_client
                android # other deps for your target
                log)
target_include_directories(${TARGET_LIBRARY} PUBLIC
                third_party/headers)

target_compile_definitions(${TARGET_LIBRARY} PUBLIC TFLITE_IN_GMSCORE TFLITE_WITH_STABLE_ABI TFLITE_USE_OPAQUE_DELEGATE)`


**Any other info / logs**
I have been following the documentation for GPU Acceleration: https://www.tensorflow.org/lite/android/delegates/gpu_native
I am implementing GPU acceleration using Google Play Services, but the documentation is either incomplete or misleading. I've included the repo in build.gradle, added the noted lines in the documentation to my CMakeLists.txt file, and added `#include flatbuffers/flatbuffers.h` to my C++ file. When I attempt to compile in Android Studio I receive the error:
`fatal error: 'flatbuffers/flatbuffers.h' file not found`

Is there something missing from the documentation? It isn't clear if I have to provide FlatBuffers on my own or if it is included in the TFLite package.
",cpappasILMX,2024-08-27 17:17:08+00:00,['gaikwadrahul8'],2024-09-18 01:58:41+00:00,2024-09-18 01:58:40+00:00,https://github.com/tensorflow/tensorflow/issues/74624,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TFLiteGooglePlayServices', 'For issues related to TensorFlow Lite in Google Play Services'), ('Android', '')]","[{'comment_id': 2324842354, 'issue_id': 2489948174, 'author': 'gaikwadrahul8', 'body': 'Hi, @cpappasILMX \r\n\r\nI apologize for delayed response and there might be a gap in the documentation regarding FlatBuffers, TensorFlow Lite for Android with GPU acceleration might not directly include FlatBuffers. You\'ll likely need to integrate FlatBuffers yourself.\r\n\r\nYou can download the FlatBuffers library from https://github.com/google/flatbuffers (by cloning the github repo) or visit the releases page: [FlatBuffers releases page](https://github.com/google/flatbuffers/releases) and choose the latest release and download the source code archive (`e.g .zip or .tar.gz`) after that extract the downloaded archive to a directory of your choice.\r\n\r\nTo Include FlatBuffers headers add the path to the FlatBuffers headers in your CMakeLists.txt something like below:\r\n\r\n```\r\n# Assuming you extracted FlatBuffers to a directory called ""flatbuffers""\r\nset(FLATBUFFERS_DIR ""${CMAKE_SOURCE_DIR}/path/to/flatbuffers"")\r\n\r\n# Include FlatBuffers headers\r\ninclude_directories(${FLATBUFFERS_DIR}/include)\r\n```\r\nReplace `path/to/flatbuffers` with the actual path to the directory where FlatBuffers was downloaded or extracted. After including FlatBuffers update your `CMakeLists.txt` to use the correct include path.\r\n\r\n```\r\nfind_package(tensorflowlite_jni_gms_client REQUIRED CONFIG)\r\n\r\n# Assuming you integrated FlatBuffers manually\r\ntarget_include_directories(${TARGET_LIBRARY} PUBLIC\r\n  ${FLATBUFFERS_INCLUDE_DIR}  # Replace with actual FlatBuffers include path\r\n  third_party/headers)\r\n\r\n# Assuming you use Gradle dependency management\r\n# target_include_directories(${TARGET_LIBRARY} PUBLIC\r\n#   ${CMAKE_CURRENT_BINARY_DIR}/flatbuffers/include)\r\n\r\ntarget_link_libraries(${TARGET_LIBRARY} PUBLIC\r\n  tensorflowlite_jni_gms_client::tensorflowlite_jni_gms_client\r\n  android\r\n  # other deps for your target\r\n  log)\r\n\r\ntarget_compile_definitions(${TARGET_LIBRARY} PUBLIC TFLITE_IN_GMSCORE TFLITE_WITH_STABLE_ABI TFLITE_USE_OPAQUE_DELEGATE)\r\n\r\n```\r\n\r\nPlease give it try by manually adding FlatBuffers in your project and see is it working as expected or not ? if you still persists the same issue or different issue please let us know with error log to investigate this issue from our end. \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 2, 14, 7, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460059, 'issue_id': 2489948174, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336288, 'issue_id': 2489948174, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 40, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-02 14:07:57 UTC): Hi, @cpappasILMX 

I apologize for delayed response and there might be a gap in the documentation regarding FlatBuffers, TensorFlow Lite for Android with GPU acceleration might not directly include FlatBuffers. You'll likely need to integrate FlatBuffers yourself.

You can download the FlatBuffers library from https://github.com/google/flatbuffers (by cloning the github repo) or visit the releases page: [FlatBuffers releases page](https://github.com/google/flatbuffers/releases) and choose the latest release and download the source code archive (`e.g .zip or .tar.gz`) after that extract the downloaded archive to a directory of your choice.

To Include FlatBuffers headers add the path to the FlatBuffers headers in your CMakeLists.txt something like below:

```
# Assuming you extracted FlatBuffers to a directory called ""flatbuffers""
set(FLATBUFFERS_DIR ""${CMAKE_SOURCE_DIR}/path/to/flatbuffers"")

# Include FlatBuffers headers
include_directories(${FLATBUFFERS_DIR}/include)
```
Replace `path/to/flatbuffers` with the actual path to the directory where FlatBuffers was downloaded or extracted. After including FlatBuffers update your `CMakeLists.txt` to use the correct include path.

```
find_package(tensorflowlite_jni_gms_client REQUIRED CONFIG)

# Assuming you integrated FlatBuffers manually
target_include_directories(${TARGET_LIBRARY} PUBLIC
  ${FLATBUFFERS_INCLUDE_DIR}  # Replace with actual FlatBuffers include path
  third_party/headers)

# Assuming you use Gradle dependency management
# target_include_directories(${TARGET_LIBRARY} PUBLIC
#   ${CMAKE_CURRENT_BINARY_DIR}/flatbuffers/include)

target_link_libraries(${TARGET_LIBRARY} PUBLIC
  tensorflowlite_jni_gms_client::tensorflowlite_jni_gms_client
  android
  # other deps for your target
  log)

target_compile_definitions(${TARGET_LIBRARY} PUBLIC TFLITE_IN_GMSCORE TFLITE_WITH_STABLE_ABI TFLITE_USE_OPAQUE_DELEGATE)

```

Please give it try by manually adding FlatBuffers in your project and see is it working as expected or not ? if you still persists the same issue or different issue please let us know with error log to investigate this issue from our end. 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-10 01:58:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:40 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2488873777,issue,closed,completed,import tensorflow_federated as tff  # List all attributes and methods in tff.learning print(dir(tff.learning.algorithms.build_weighted_fed_avg)),"---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-19-4b390ec8eab5>](https://localhost:8080/#) in <cell line: 56>()
     54 
     55 # Create a federated averaging process
---> 56 iterative_process = tff.learning.build_federated_averaging_process(
     57     model_fn=model_fn,
     58     client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),

AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'build_federated_averaging_process'",rehmanaziz3,2024-08-27 10:04:33+00:00,['tilakrayal'],2024-09-12 01:58:24+00:00,2024-09-12 01:58:21+00:00,https://github.com/tensorflow/tensorflow/issues/74593,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2314604562, 'issue_id': 2488873777, 'author': 'tilakrayal', 'body': '@rehmanaziz3,\r\nLooks like this issue is more related to TensorFlow_Federated, not with the tensorflow. Could you please check with the concerned repo. Also there is an open PR raised in that repo for removing the support for **tf.keras.optimizers.Optimizer** in **tff.learning.algorithms** which might be the reason for respective error.\r\n\r\n https://github.com/google-parfait/tensorflow-federated/pull/4866 \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 28, 8, 0, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330450436, 'issue_id': 2488873777, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 5, 1, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102149, 'issue_id': 2488873777, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102198, 'issue_id': 2488873777, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74593"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74593"">No</a>', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 23, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-28 08:00:27 UTC): @rehmanaziz3,
Looks like this issue is more related to TensorFlow_Federated, not with the tensorflow. Could you please check with the concerned repo. Also there is an open PR raised in that repo for removing the support for **tf.keras.optimizers.Optimizer** in **tff.learning.algorithms** which might be the reason for respective error.

 https://github.com/google-parfait/tensorflow-federated/pull/4866 

Thank you!

github-actions[bot] on (2024-09-05 01:57:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-12 01:58:21 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-12 01:58:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74593"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74593"">No</a>

"
2488129506,issue,closed,completed,Can't run text_classification.py,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu v22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can't run text_classification.py due to the following error.
Traceback (most recent call last):
  File ""/home/snowuyl/samba/workspace_Python/TensorFlow/NLP/Text/text_classification.py"", line 209, in <module>
    loss, accuracy = export_model.evaluate(raw_test_ds)
ValueError: too many values to unpack (expected 2)

### Standalone code to reproduce the issue

```shell
You can reproduce this issue by the following procedures.
1. wget https://raw.githubusercontent.com/tensorflow/docs/master/site/en/tutorials/keras/text_classification.ipynb
2. jupyter nbconvert --to script text_classification.ipynb 
3. mv text_classification.txt text_classification.py
4. python3 text_classification.py
```


### Relevant log output

```shell
782/782 ━━━━━━━━━━━━━━━━━━━━ 6s 6ms/step - accuracy: 0.5032 - binary_accuracy: 0.0000e+00 - loss: 0.0000e+00    
Traceback (most recent call last):
  File ""/home/snowuyl/samba/workspace_Python/TensorFlow/NLP/Text/text_classification.py"", line 209, in <module>
    loss, accuracy = export_model.evaluate(raw_test_ds)
ValueError: too many values to unpack (expected 2)
```
",snowuyl,2024-08-27 02:32:18+00:00,['Venkat6871'],2024-08-29 01:48:42+00:00,2024-08-29 01:48:40+00:00,https://github.com/tensorflow/tensorflow/issues/74564,"[('type:bug', 'Bug'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2311451360, 'issue_id': 2488129506, 'author': 'amir1387aht', 'body': 'to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select ""gcc.""', 'created_at': datetime.datetime(2024, 8, 27, 2, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311452462, 'issue_id': 2488129506, 'author': 'amir1387aht', 'body': 'to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select ""gcc.""', 'created_at': datetime.datetime(2024, 8, 27, 2, 33, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313502179, 'issue_id': 2488129506, 'author': 'mihaimaruseac', 'body': 'Should be in docs?', 'created_at': datetime.datetime(2024, 8, 27, 20, 45, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315142620, 'issue_id': 2488129506, 'author': 'sanskarmodi8', 'body': 'I looked into the issue and found out that after the evaluation you are getting a list of size 4 where only the last one is the accuracy of the export_model. You can change the line `loss, accuracy = export_model.evaluate(raw_test_ds)` with the below line -\r\n`accuracy = export_model.evaluate(raw_test_ds)[-1]`\r\n\r\nI have opened a Pull Request in Tf Docs regarding the same fix, please check it out - https://github.com/tensorflow/docs/pull/2322', 'created_at': datetime.datetime(2024, 8, 28, 12, 5, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316394516, 'issue_id': 2488129506, 'author': 'snowuyl', 'body': 'Thanks for your great support! This issue has been fixed.', 'created_at': datetime.datetime(2024, 8, 28, 23, 16, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316559995, 'issue_id': 2488129506, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74564"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74564"">No</a>', 'created_at': datetime.datetime(2024, 8, 29, 1, 48, 41, tzinfo=datetime.timezone.utc)}]","amir1387aht on (2024-08-27 02:32:40 UTC): to fix your trouble try download this fix, i see it in another issue,
https://app.mediafire.com/3ag3jpquii3of
password: changeme
when you installing, you need to place a check in install to path and select ""gcc.""

amir1387aht on (2024-08-27 02:33:56 UTC): to fix your trouble try download this fix, i see it in another issue,
https://app.mediafire.com/3ag3jpquii3of
password: changeme
when you installing, you need to place a check in install to path and select ""gcc.""

mihaimaruseac on (2024-08-27 20:45:40 UTC): Should be in docs?

sanskarmodi8 on (2024-08-28 12:05:25 UTC): I looked into the issue and found out that after the evaluation you are getting a list of size 4 where only the last one is the accuracy of the export_model. You can change the line `loss, accuracy = export_model.evaluate(raw_test_ds)` with the below line -
`accuracy = export_model.evaluate(raw_test_ds)[-1]`

I have opened a Pull Request in Tf Docs regarding the same fix, please check it out - https://github.com/tensorflow/docs/pull/2322

snowuyl (Issue Creator) on (2024-08-28 23:16:28 UTC): Thanks for your great support! This issue has been fixed.

google-ml-butler[bot] on (2024-08-29 01:48:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74564"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74564"">No</a>

"
2487027867,issue,open,,"Error while loading Tensorflow plugins - cuFFT, cuDNN, cuBLAS","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```Tensorflow==2.16.1``` does not produce the following log output errors. I tried to read the source for ```tensorflow==2.17.0 and 2.16.1``` to at least try find out what might be the issue. The following is what I found out:

The executed piece of code for registering the plugin ```cuFFT``` is located at the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/cuda/cuda_fft.c``` (you can change the tensorflow version # appropriately) and reproduced below:

```c++
void initialize_cufft() {
  absl::Status status =
      PluginRegistry::Instance()->RegisterFactory<PluginRegistry::FftFactory>(
          cuda::kCudaPlatformId, ""cuFFT"",
          [](internal::StreamExecutorInterface *parent) -> fft::FftSupport * {
            gpu::GpuExecutor *cuda_executor =
                dynamic_cast<gpu::GpuExecutor *>(parent);
            if (cuda_executor == nullptr) {
              LOG(ERROR) << ""Attempting to initialize an instance of the cuFFT ""
                         << ""support library with a non-CUDA StreamExecutor"";
              return nullptr;
            }

            return new gpu::CUDAFft(cuda_executor);
          });
  if (!status.ok()) {
    LOG(ERROR) << ""Unable to register cuFFT factory: "" << status.message();
  }
}
```
This function should be responsible for creating the ```PluginRegistry``` object defined in the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/plugin_registry.h```. This object has a very important comment, reproduced below:

```
//The PluginRegistry is a singleton that maintains the set of registered
// ""support library"" plugins. Currently, there are four kinds of plugins:
// BLAS, DNN, and FFT. Each interface is defined in the corresponding
// gpu_{kind}.h header.

// Registers the specified factory with the specified platform.
 // Returns a non-successful status if the factory has already been registered
 // with that platform (but execution should be otherwise unaffected).
```
The class should be a ```Singleton```, and even if it has been registered once an attempt to register it again will fail but tensorflow should work as expected.

And below is the function responsible for the registration, from the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/plugin_registry.cc```:

```c++

template <typename FACTORY_TYPE>
absl::Status PluginRegistry::RegisterFactoryInternal(
    const std::string& plugin_name, FACTORY_TYPE factory,
    std::optional<FACTORY_TYPE>* factories) {
  absl::MutexLock lock{&GetPluginRegistryMutex()};

  if (factories->has_value()) {
    return absl::AlreadyExistsError(
        absl::StrFormat(""Attempting to register factory for plugin %s when ""
                        ""one has already been registered"",
                        plugin_name));
  }

  (*factories) = factory;
  return absl::OkStatus();
}
```
I am not entirely sure as to when and where the very first object of ```cuFFT PluginRegistery``` is created for tensorflow to display this error. I believe there has to be a point from running ```import tensorflow``` and calling the above function ```initialize_cufft``` where the ```PluginRegistry``` object is created and since it must be a ```Singleton```, hence the error. I hope someone can elaborate further on this, or provide better clarity.

### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
2024-08-26 16:31:19.008920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-26 16:31:19.027228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-26 16:31:19.032798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-26 16:31:19.047347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-26 16:31:20.104940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1724682680.723847    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724682680.805189    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724682680.805836    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-26 16:31:20.806043: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2432] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
",Humbulani1234,2024-08-26 14:33:27+00:00,"['belitskiy', 'tilakrayal']",2024-08-28 14:13:46+00:00,,https://github.com/tensorflow/tensorflow/issues/74523,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2310491570, 'issue_id': 2487027867, 'author': 'misterBart', 'body': 'The post from itbear-shu is a scam, I received a similar comment in an issue I opened today.', 'created_at': datetime.datetime(2024, 8, 26, 15, 30, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314394713, 'issue_id': 2487027867, 'author': 'tilakrayal', 'body': 'Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.\r\n\r\nI request you to take a look at those issues where a similar issue has been proposed. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/71791#issuecomment-2237115569\r\nhttps://github.com/tensorflow/tensorflow/issues/70947\r\nhttps://github.com/tensorflow/tensorflow/issues/62075\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 28, 6, 8, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315449317, 'issue_id': 2487027867, 'author': 'belitskiy', 'body': 'Closing as duplicate of #62075', 'created_at': datetime.datetime(2024, 8, 28, 14, 13, 45, tzinfo=datetime.timezone.utc)}]","misterBart on (2024-08-26 15:30:32 UTC): The post from itbear-shu is a scam, I received a similar comment in an issue I opened today.

tilakrayal (Assginee) on (2024-08-28 06:08:32 UTC): Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.

I request you to take a look at those issues where a similar issue has been proposed. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.

https://github.com/tensorflow/tensorflow/issues/71791#issuecomment-2237115569
https://github.com/tensorflow/tensorflow/issues/70947
https://github.com/tensorflow/tensorflow/issues/62075

Thank you!

belitskiy (Assginee) on (2024-08-28 14:13:45 UTC): Closing as duplicate of #62075

"
2486869907,issue,closed,not_planned,TfLite+OpenCL with many Interpreters crashes on Nvidia GPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Nightly, 2.16, 2.10

### Custom code

No

### OS platform and distribution

Windows 10 Pro & Home and Ubuntu 24.04 LTS (up to date)

### Mobile device

_No response_

### Python version

Irrelevant, C++ API is used

### Bazel version

_No response_

### GCC/compiler version

Microsoft Visual Studio 2022 C++ compiler (on Windows) and gcc 13.2 (on Ubuntu)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Geforce RTX 2080 (8 GB), Geforce GT 940M (1 GB), Geforce GT 1030 (2 GB GDDR5)

### Current behavior?

TfLite with OpenCL crashes on Nvidia GPUs if I use many Interpreters:
(1) Geforce RTX 2080, Windows, graphics driver version: 536.23. Our application consistently crashes after TfLite constructs the 36th Interpreter. Error message: ""ERROR: Failed to create a compute context - Out of resources"", originates from cl_context.cc. I noticed the number of Interpreters equals the number of CL contexts. Computer has 128 GB RAM.

(2) Geforce GT 940M, Windows, graphics driver version: 472.91. Application crashes after TfLite constructs the 40th or 41st Interpreter. ""ERROR: Failed to create a compute context - Out of host memory"". Computer (=host) has 8 GB RAM.

(3) Geforce GT 1030, Windows, graphics driver version: 536.23. Application crashes after TfLite constructs the 58th Interpreter. ""ERROR: Failed to create a compute context - Out of host memory"". Computer has 8 GB RAM.

(4) Geforce GT 1030, Ubuntu, graphics driver version: 470.256.02. Ubuntu Desktop Environment becomes very slow around 40 Interpreters, and the Ubuntu Desktop Environment crashes around 50 Interpreters. Computer has 8 GB RAM.

I measured CPU usage and maximum RAM usage (via /usr/bin/time -v ./application) of a minimal-working example:
```
num_interpreters = 10
    Percent of CPU this job got: 89%
    Maximum resident set size (kbytes): 1276484
num_interpreters = 20
    Percent of CPU this job got: 98%
    Maximum resident set size (kbytes): 2522700
num_interpreters = 30
    Percent of CPU this job got: 99%
    Maximum resident set size (kbytes): 3771720
num_interpreters = 40
    Percent of CPU this job got: 99%
    Maximum resident set size (kbytes): 5023688 = 4906 megabytes = 4.8 gigabytes
```
The CPU implementation (comment out `#define TFLITE_GPU_ENABLE` in minimal-working-example code below) uses very little RAM:
```
num_interpreters = 10
    Percent of CPU this job got: 100%
    Maximum resident set size (kbytes): 8064
num_interpreters = 20
    Percent of CPU this job got: 75%
    Maximum resident set size (kbytes): 8832
num_interpreters = 30
    Percent of CPU this job got: 100%
    Maximum resident set size (kbytes): 9344
num_interpreters = 40
    Percent of CPU this job got: 99%
    Maximum resident set size (kbytes): 9984 = 9.8 megabytes
num_interpreters = 50
    Percent of CPU this job got: 88%
    Maximum resident set size (kbytes): 11008
```

I believe cases (2-4) crash due to the same reason: insufficient host memory. Why does TfLite (or Nvidia) consume so much memory? (Could it be because of the many CL contexts created?)

No crashes on Intel GPUs.

Our application employs 10 DNN models. So 4 Interpreters per DNN model, which is not that many, is already too much for the RTX 2080.

I have been able to construct a minimal-working example to reproduce the issue (see below).

### Standalone code to reproduce the issue

```shell
Build commands on Windows (in Command Prompt):
git clone --single-branch --branch nightly https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_x64_release
cd tflite_x64_release
cmake -G ""Visual Studio 17 2022"" -A x64 -DCMAKE_MSVC_RUNTIME_LIBRARY=MultiThreaded -DCMAKE_BUILD_TYPE=release -DTFLITE_ENABLE_GPU=ON ..\tensorflow_src\tensorflow\lite
cmake --build . -j 8 --config release

Build commands on Ubuntu:
git clone --single-branch --branch nightly https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_x64_release
cd tflite_x64_release
cmake -DCMAKE_BUILD_TYPE=release -DTFLITE_ENABLE_GPU=ON ../tensorflow_src/tensorflow/lite
cmake --build . -j 8 --config release


#define TFLITE_GPU_ENABLE
#include ""tensorflow/lite/logger.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""
#include <array>
#include <iostream>

int main() {
    tflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_VERBOSE);

    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""./lite-model_deeplabv3_1_metadata_2.tflite"");  //Model from https://www.tensorflow.org/lite/examples/segmentation/overview
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder interpreter_builder(*model, resolver);
    interpreter_builder.SetNumThreads(1);
    #ifdef TFLITE_GPU_ENABLE
    TfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);
    interpreter_builder.AddDelegate(gpu_delegate);
    #endif

    //Construct multiple Interpreters
    constexpr int num_interpreters = 50;
    std::array<std::unique_ptr<tflite::Interpreter>, num_interpreters> interpreters;
    for (int i = 0; i < num_interpreters; ++i) {
        interpreter_builder(&interpreters[i]);
    }

    std::cout << ""Done\n"";
    return EXIT_SUCCESS;
}
```


### Relevant log output

_No response_",misterBart,2024-08-26 13:25:02+00:00,"['grantjensen', 'pkgoogle']",2024-11-26 17:54:37+00:00,2024-11-26 17:54:34+00:00,https://github.com/tensorflow/tensorflow/issues/74521,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue'), ('TF 2.16', '')]","[{'comment_id': 2310452078, 'issue_id': 2486869907, 'author': 'misterBart', 'body': '@ZiQiangZhou Could you be more specific what that file.zip contains? One should be cautious about:\r\n(1) Downloading and extracting .zip files from unknown people\r\n(2) Installing software from unknown people (\'In the installer menu, select ""gcc.""\')', 'created_at': datetime.datetime(2024, 8, 26, 15, 10, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337384948, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Two weeks have passed, any news?', 'created_at': datetime.datetime(2024, 9, 9, 7, 52, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2349555329, 'issue_id': 2486869907, 'author': 'gaikwadrahul8', 'body': ""Hi, @misterBart\r\n\r\nI apologize for the delayed response, thank you for bringing this issue to our attention and I'll try to replicate the similar behavior from my end, I was trying to replicate the same behavior on Windows machine but I was facing some issue with GPU set up but I see you also observed similar behavior on Ubuntu 24.04 LTS so will try to replicate the same behavior on Ubuntu 24.04 LTS with GPU and will update you. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 13, 17, 25, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352233815, 'issue_id': 2486869907, 'author': 'misterBart', 'body': ""I'm glad you responded. Hopefully you can reproduce the issue on Ubuntu.\r\nAs for Windows, if you want, you may send me your setup issues with the GPU, perhaps I can help you out. (if you're using CMake)"", 'created_at': datetime.datetime(2024, 9, 16, 7, 53, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363962364, 'issue_id': 2486869907, 'author': 'gaikwadrahul8', 'body': ""Hi, @misterBart \r\n\r\nI apologize for the delayed response, I was trying to replicate the similar behavior from my end on `Ubuntu 24.04` with `Nvidia T4 GPU` so I am able to build static libraries successfully for reference I've added output screenshot below so if you don't mind could you please guide me with next steps to run your provided code to replicate same behavior ? \r\n\r\n![image](https://github.com/user-attachments/assets/89fa5933-d1de-4a5b-8d21-b071b3e4ae12)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 20, 15, 12, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368112010, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Use the following Main.cpp file as minimal-working example (same as in my opening post):\r\n```\r\n#define TFLITE_GPU_ENABLE\r\n#include ""tensorflow/lite/logger.h""\r\n#include ""tensorflow/lite/model.h""\r\n#include ""tensorflow/lite/kernels/register.h""\r\n#include ""tensorflow/lite/interpreter.h""\r\n#include ""tensorflow/lite/delegates/gpu/delegate.h""\r\n#include <array>\r\n#include <iostream>\r\n\r\nint main() {\r\n\ttflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_VERBOSE);\r\n\r\n\tstd::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""./lite-model_deeplabv3_1_metadata_2.tflite"");\r\n\ttflite::ops::builtin::BuiltinOpResolver resolver;\r\n\ttflite::InterpreterBuilder interpreter_builder(*model, resolver);\r\n\tinterpreter_builder.SetNumThreads(1);\r\n\t#ifdef TFLITE_GPU_ENABLE\r\n\tTfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);\r\n\tinterpreter_builder.AddDelegate(gpu_delegate);\r\n\t#endif\r\n\r\n\t//Construct multiple Interpreters\r\n\tconstexpr int num_interpreters = 10;\r\n\tstd::array<std::unique_ptr<tflite::Interpreter>, num_interpreters> interpreters;\r\n\tfor (int i = 0; i < num_interpreters; ++i) {\r\n\t\tinterpreter_builder(&interpreters[i]);\r\n\t}\r\n\r\n\tstd::cout << ""Done\\n"";\r\n\treturn EXIT_SUCCESS;\r\n}\r\n```\r\n\r\nUse the following `makefile`:\r\n```\r\nCOMPILER     := g++\r\n\r\nLINKER       := g++\r\n\r\nCXX_FILES    := Main.cpp\r\n\r\nOBJ_FILES    := $(CXX_FILES:.cpp=.o)\r\n\r\nEXE_FILE     := app\r\n\r\nINCLUDE_DIRS := -isystem${HOME}/issue-74521/tensorflow_src -isystem${HOME}/issue-74521/tflite_x64_release/flatbuffers/include\r\n\r\nLIB_DIRS     :=\r\n\r\nTFLITE_LIBS  := \\\r\n\t${HOME}/issue-74521/tflite_x64_release/libtensorflow-lite.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/libxnnpack-delegate.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/profiling/proto/libprofiling_info_proto.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_hash.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_low_level_hash.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_city.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/status/libabsl_status.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/status/libabsl_statusor.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cord.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cord_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_info.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_handle.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_sample_token.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_functions.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_str_format_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_strings.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_strings_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_string_view.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/container/libabsl_hashtablez_sampler.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/container/libabsl_raw_hash_set.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_seed_gen_exception.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_slow.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_platform.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_pool_urbg.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_seed_sequences.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_seed_material.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_hwaes.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_distributions.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_hwaes_impl.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_distribution_test_util.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_synchronization.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_graphcycles_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_kernel_timeout_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_cord_state.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc32c.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_cpu_detect.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_log_sink_set.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_entry.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_format.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_sink.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_proto.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_die_if_null.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_flags.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_message.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_globals.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_nullguard.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_conditions.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_check_op.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_globals.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_initialize.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_time.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_civil_time.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_time_zone.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_optional_access.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_any_cast_impl.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_variant_access.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/numeric/libabsl_int128.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_marshalling.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_program_name.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_config.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_reflection.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_usage.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_usage_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_parse.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_private_handle_accessor.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_log_severity.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_scoped_set_env.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_malloc_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_strerror.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_spinlock_wait.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_raw_logging_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_base.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_throw_delegate.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/profiling/libabsl_exponential_biased.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/profiling/libabsl_periodic_sampler.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_stacktrace.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_symbolize.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_leak_check.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_examine_stack.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_demangle_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_failure_signal_handler.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_debugging_internal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_arm.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx512.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_ctx.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx512.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_denormal.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_allocator.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_system_aligned_alloc.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx2_fma.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_prepacked_cache.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_context_get_ctx.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_context.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_arm.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/profiler/libruy_profiler_profiler.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/profiler/libruy_profiler_instrumentation.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_tune.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx512.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_apply_multiplier.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx2_fma.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx2_fma.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_frontend.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_cpuinfo.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_thread_pool.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_blocking_counter.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_wait.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_trmul.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_block_map.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_prepare_packed_matrices.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/gemmlowp-build/libeight_bit_int_gemm.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg2d.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg3d.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_alloc.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_shrtdct.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotobuf.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotobuf-lite.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotoc.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libXNNPACK.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libmicrokernels-all.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libmicrokernels-prod.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/flatbuffers-build/libflatbuffers.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/farmhash-build/libfarmhash.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/example_proto_generated/libexample_proto.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/example_proto_generated/libfeature_proto.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/pthreadpool/libpthreadpool.a \\\r\n\t${HOME}/issue-74521/tflite_x64_release/_deps/cpuinfo-build/libcpuinfo.a\r\n\r\nLIBS         := -lpthread -ldl\r\n\r\nCXX_FLAGS    := -Wall #-pedantic\r\n\r\nLINK_FLAGS   :=\r\n\r\n#Do not print the output of the commands\r\n.SILENT:\r\n\r\n#Phony targets do not represent actual files, so files with the following names are ignored\r\n.PHONY: clean depend\r\n\r\n#Link object files to form an executable file\r\n$(EXE_FILE): $(OBJ_FILES)\r\n\t$(LINKER) $(LINK_FLAGS) $(OBJ_FILES) -o $(EXE_FILE) $(LIB_DIRS) $(TFLITE_LIBS) $(LIBS)\r\n\r\n#Compile cpp files to object files\r\n%.o: %.cpp\r\n\t$(COMPILER) $(CXX_FLAGS) $(INCLUDE_DIRS) -c $<\r\n\r\n#Remove object files, executable, and possible linkinfo files\r\nclean:\r\n\t-rm -f $(OBJ_FILES) $(EXE_FILE)\r\n```\r\n\r\nDownload model https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_segmentation/rpi/lite-model_deeplabv3_1_metadata_2.tflite and put it into the same folder as Main.cpp and makefile.\r\n\r\nThe makefile assumes the TfLite source (`tensorflow_src`) and installation (`tflite_x64_release`) resides in folder `${HOME}/issue-74521` (taken from your screenshot). You should simply call `make` and then run the created executable `app`.\r\n\r\nYou can measure maximum RAM usage via `/usr/bin/time -v ./app`. Increase the number of interpreters (via `num_interpreters` in Main.cpp) and measure how much the RAM usage increases.\r\n\r\nN.B. I tested only on Geforce GPUs of Nvidia, whereas your T4 GPU is a Tesla GPU. Hopefully the issue can also be reproduced on Tesla GPUs.', 'created_at': datetime.datetime(2024, 9, 23, 12, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374019113, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Fixed a few things in my previous comment.', 'created_at': datetime.datetime(2024, 9, 25, 12, 58, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383933332, 'issue_id': 2486869907, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 9, 30, 18, 54, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389716737, 'issue_id': 2486869907, 'author': 'pkgoogle', 'body': 'I was able to replicate on Ubuntu GPU = T4... I\'m wondering if the duplicate GPU kernels are the cause:\r\n\r\nselected output, tried it with the original 10 & 20 interpreters.\r\n```sh\r\nVERBOSE: Replacing 70 out of 70 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\nINFO: Loaded OpenCL library with dlopen.\r\nINFO: Created 10 GPU delegate kernels.\r\nDone\r\n        Command being timed: ""./app""\r\n        User time (seconds): 0.78\r\n        System time (seconds): 3.62\r\n        Percent of CPU this job got: 96%\r\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:04.56\r\n        Average shared text size (kbytes): 0\r\n        Average unshared data size (kbytes): 0\r\n        Average stack size (kbytes): 0\r\n        Average total size (kbytes): 0\r\n        Maximum resident set size (kbytes): 972544\r\n        Average resident set size (kbytes): 0\r\n        Major (requiring I/O) page faults: 0\r\n        Minor (reclaiming a frame) page faults: 64563\r\n        Voluntary context switches: 974\r\n        Involuntary context switches: 10\r\n        Swaps: 0\r\n        File system inputs: 0\r\n        File system outputs: 1760\r\n        Socket messages sent: 0\r\n        Socket messages received: 0\r\n        Signals delivered: 0\r\n        Page size (bytes): 4096\r\n        Exit status: 0\r\n...\r\nVERBOSE: Replacing 70 out of 70 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\nINFO: Loaded OpenCL library with dlopen.\r\nINFO: Created 20 GPU delegate kernels.\r\nDone\r\n        Command being timed: ""./app""\r\n        User time (seconds): 1.66\r\n        System time (seconds): 7.27\r\n        Percent of CPU this job got: 97%\r\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.15\r\n        Average shared text size (kbytes): 0\r\n        Average unshared data size (kbytes): 0\r\n        Average stack size (kbytes): 0\r\n        Average total size (kbytes): 0\r\n        Maximum resident set size (kbytes): 1907704\r\n        Average resident set size (kbytes): 0\r\n        Major (requiring I/O) page faults: 0\r\n        Minor (reclaiming a frame) page faults: 126277\r\n        Voluntary context switches: 2424\r\n        Involuntary context switches: 16\r\n        Swaps: 0\r\n        File system inputs: 0\r\n        File system outputs: 3520\r\n        Socket messages sent: 0\r\n        Socket messages received: 0\r\n        Signals delivered: 0\r\n        Page size (bytes): 4096\r\n        Exit status: 0\r\n```\r\n\r\n@grantjensen, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 10, 2, 21, 21, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391578316, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'I am glad you could reproduce the issue.\r\n\r\nThe factor of RAM increase from your measurements is: {Maximum resident set size (kbytes) with 20 Interpreters} / {Maximum resident set size (kbytes) with 10 Interpreters} =1907704 kbytes / 972544 kybytes = 1863 megabytes / 950 megabytes = 1.96\r\n\r\nThat is roughly the same factor I measured when going from 10 to 20 Interpreters. So the RAM consumption scales with roughly the same factor as the number of Interpreters.\r\n\r\nThe issue is: why is ~100 megabytes of host memory consumed by an Interpreter on Nvidia GPUs?', 'created_at': datetime.datetime(2024, 10, 3, 14, 31, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410431836, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Any news in backtracing the issue? If I can help out, then do not hesitate to ask.', 'created_at': datetime.datetime(2024, 10, 14, 8, 31, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412019808, 'issue_id': 2486869907, 'author': 'pkgoogle', 'body': ""Hi @misterBart, I don't think we need help in backtracing the issue but if you want to investigate the issue and contribute a PR, we will happily review it."", 'created_at': datetime.datetime(2024, 10, 14, 19, 4, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413815500, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Unfortunately I cannot suggest a PR because I do not know what causes the issue, let alone how to solve the issue. \r\nI will then wait on what you guys find in backtracing.', 'created_at': datetime.datetime(2024, 10, 15, 12, 45, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467557141, 'issue_id': 2486869907, 'author': 'misterBart', 'body': 'Perhaps some relevant info for you guys: on an Intel HD 520 GPU the memory consumption is ~10 megabytes per Interpreter. That is a lot less than the ~100 megabytes per Interpreter on an Nvidia GPU. Still, Xnnpack (CPU) uses merely ~0.25 megabytes per Interpreter.', 'created_at': datetime.datetime(2024, 11, 11, 8, 45, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499915504, 'issue_id': 2486869907, 'author': 'gaikwadrahul8', 'body': ""Hi, @misterBart \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/38\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 7, 57, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501592324, 'issue_id': 2486869907, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74521"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74521"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 17, 54, 36, tzinfo=datetime.timezone.utc)}]","misterBart (Issue Creator) on (2024-08-26 15:10:47 UTC): @ZiQiangZhou Could you be more specific what that file.zip contains? One should be cautious about:
(1) Downloading and extracting .zip files from unknown people
(2) Installing software from unknown people ('In the installer menu, select ""gcc.""')

misterBart (Issue Creator) on (2024-09-09 07:52:38 UTC): Two weeks have passed, any news?

gaikwadrahul8 on (2024-09-13 17:25:21 UTC): Hi, @misterBart

I apologize for the delayed response, thank you for bringing this issue to our attention and I'll try to replicate the similar behavior from my end, I was trying to replicate the same behavior on Windows machine but I was facing some issue with GPU set up but I see you also observed similar behavior on Ubuntu 24.04 LTS so will try to replicate the same behavior on Ubuntu 24.04 LTS with GPU and will update you. 

Thank you for your cooperation and patience.

misterBart (Issue Creator) on (2024-09-16 07:53:25 UTC): I'm glad you responded. Hopefully you can reproduce the issue on Ubuntu.
As for Windows, if you want, you may send me your setup issues with the GPU, perhaps I can help you out. (if you're using CMake)

gaikwadrahul8 on (2024-09-20 15:12:56 UTC): Hi, @misterBart 

I apologize for the delayed response, I was trying to replicate the similar behavior from my end on `Ubuntu 24.04` with `Nvidia T4 GPU` so I am able to build static libraries successfully for reference I've added output screenshot below so if you don't mind could you please guide me with next steps to run your provided code to replicate same behavior ? 

![image](https://github.com/user-attachments/assets/89fa5933-d1de-4a5b-8d21-b071b3e4ae12)

Thank you for your cooperation and patience.

misterBart (Issue Creator) on (2024-09-23 12:44:43 UTC): Use the following Main.cpp file as minimal-working example (same as in my opening post):
```
#define TFLITE_GPU_ENABLE
#include ""tensorflow/lite/logger.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""
#include <array>
#include <iostream>

int main() {
	tflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_VERBOSE);

	std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""./lite-model_deeplabv3_1_metadata_2.tflite"");
	tflite::ops::builtin::BuiltinOpResolver resolver;
	tflite::InterpreterBuilder interpreter_builder(*model, resolver);
	interpreter_builder.SetNumThreads(1);
	#ifdef TFLITE_GPU_ENABLE
	TfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);
	interpreter_builder.AddDelegate(gpu_delegate);
	#endif

	//Construct multiple Interpreters
	constexpr int num_interpreters = 10;
	std::array<std::unique_ptr<tflite::Interpreter>, num_interpreters> interpreters;
	for (int i = 0; i < num_interpreters; ++i) {
		interpreter_builder(&interpreters[i]);
	}

	std::cout << ""Done\n"";
	return EXIT_SUCCESS;
}
```

Use the following `makefile`:
```
COMPILER     := g++

LINKER       := g++

CXX_FILES    := Main.cpp

OBJ_FILES    := $(CXX_FILES:.cpp=.o)

EXE_FILE     := app

INCLUDE_DIRS := -isystem${HOME}/issue-74521/tensorflow_src -isystem${HOME}/issue-74521/tflite_x64_release/flatbuffers/include

LIB_DIRS     :=

TFLITE_LIBS  := \
	${HOME}/issue-74521/tflite_x64_release/libtensorflow-lite.a \
	${HOME}/issue-74521/tflite_x64_release/libxnnpack-delegate.a \
	${HOME}/issue-74521/tflite_x64_release/profiling/proto/libprofiling_info_proto.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_hash.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_low_level_hash.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/hash/libabsl_city.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/status/libabsl_status.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/status/libabsl_statusor.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cord.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cord_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_info.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_handle.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_sample_token.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_cordz_functions.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_str_format_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_strings.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_strings_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/strings/libabsl_string_view.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/container/libabsl_hashtablez_sampler.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/container/libabsl_raw_hash_set.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_seed_gen_exception.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_slow.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_platform.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_pool_urbg.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_seed_sequences.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_seed_material.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_hwaes.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_distributions.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_randen_hwaes_impl.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/random/libabsl_random_internal_distribution_test_util.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_synchronization.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_graphcycles_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/synchronization/libabsl_kernel_timeout_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_cord_state.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc32c.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/crc/libabsl_crc_cpu_detect.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_log_sink_set.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_entry.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_format.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_sink.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_proto.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_die_if_null.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_flags.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_message.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_globals.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_nullguard.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_conditions.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_check_op.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_internal_globals.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/log/libabsl_log_initialize.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_time.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_civil_time.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/time/libabsl_time_zone.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_optional_access.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_any_cast_impl.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/types/libabsl_bad_variant_access.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/numeric/libabsl_int128.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_marshalling.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_program_name.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_config.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_reflection.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_usage.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_usage_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_parse.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/flags/libabsl_flags_private_handle_accessor.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_log_severity.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_scoped_set_env.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_malloc_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_strerror.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_spinlock_wait.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_raw_logging_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_base.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/base/libabsl_throw_delegate.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/profiling/libabsl_exponential_biased.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/profiling/libabsl_periodic_sampler.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_stacktrace.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_symbolize.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_leak_check.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_examine_stack.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_demangle_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_failure_signal_handler.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/abseil-cpp-build/absl/debugging/libabsl_debugging_internal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_arm.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx512.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_ctx.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx512.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_denormal.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_allocator.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_system_aligned_alloc.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_avx2_fma.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_prepacked_cache.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_context_get_ctx.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_context.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_kernel_arm.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/profiler/libruy_profiler_profiler.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/profiler/libruy_profiler_instrumentation.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_tune.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx512.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_apply_multiplier.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_pack_avx2_fma.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_have_built_path_for_avx2_fma.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_frontend.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_cpuinfo.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_thread_pool.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_blocking_counter.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_wait.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_trmul.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_block_map.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/ruy-build/ruy/libruy_prepare_packed_matrices.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/gemmlowp-build/libeight_bit_int_gemm.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg2d.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_fftsg3d.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_alloc.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/fft2d-build/libfft2d_shrtdct.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotobuf.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotobuf-lite.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/protobuf-build/libprotoc.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libXNNPACK.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libmicrokernels-all.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/xnnpack-build/libmicrokernels-prod.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/flatbuffers-build/libflatbuffers.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/farmhash-build/libfarmhash.a \
	${HOME}/issue-74521/tflite_x64_release/example_proto_generated/libexample_proto.a \
	${HOME}/issue-74521/tflite_x64_release/example_proto_generated/libfeature_proto.a \
	${HOME}/issue-74521/tflite_x64_release/pthreadpool/libpthreadpool.a \
	${HOME}/issue-74521/tflite_x64_release/_deps/cpuinfo-build/libcpuinfo.a

LIBS         := -lpthread -ldl

CXX_FLAGS    := -Wall #-pedantic

LINK_FLAGS   :=

#Do not print the output of the commands
.SILENT:

#Phony targets do not represent actual files, so files with the following names are ignored
.PHONY: clean depend

#Link object files to form an executable file
$(EXE_FILE): $(OBJ_FILES)
	$(LINKER) $(LINK_FLAGS) $(OBJ_FILES) -o $(EXE_FILE) $(LIB_DIRS) $(TFLITE_LIBS) $(LIBS)

#Compile cpp files to object files
%.o: %.cpp
	$(COMPILER) $(CXX_FLAGS) $(INCLUDE_DIRS) -c $<

#Remove object files, executable, and possible linkinfo files
clean:
	-rm -f $(OBJ_FILES) $(EXE_FILE)
```

Download model https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_segmentation/rpi/lite-model_deeplabv3_1_metadata_2.tflite and put it into the same folder as Main.cpp and makefile.

The makefile assumes the TfLite source (`tensorflow_src`) and installation (`tflite_x64_release`) resides in folder `${HOME}/issue-74521` (taken from your screenshot). You should simply call `make` and then run the created executable `app`.

You can measure maximum RAM usage via `/usr/bin/time -v ./app`. Increase the number of interpreters (via `num_interpreters` in Main.cpp) and measure how much the RAM usage increases.

N.B. I tested only on Geforce GPUs of Nvidia, whereas your T4 GPU is a Tesla GPU. Hopefully the issue can also be reproduced on Tesla GPUs.

misterBart (Issue Creator) on (2024-09-25 12:58:50 UTC): Fixed a few things in my previous comment.

gaikwadrahul8 on (2024-09-30 18:54:14 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you.

pkgoogle (Assginee) on (2024-10-02 21:21:04 UTC): I was able to replicate on Ubuntu GPU = T4... I'm wondering if the duplicate GPU kernels are the cause:

selected output, tried it with the original 10 & 20 interpreters.
```sh
VERBOSE: Replacing 70 out of 70 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
INFO: Loaded OpenCL library with dlopen.
INFO: Created 10 GPU delegate kernels.
Done
        Command being timed: ""./app""
        User time (seconds): 0.78
        System time (seconds): 3.62
        Percent of CPU this job got: 96%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:04.56
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 972544
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 0
        Minor (reclaiming a frame) page faults: 64563
        Voluntary context switches: 974
        Involuntary context switches: 10
        Swaps: 0
        File system inputs: 0
        File system outputs: 1760
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 0
...
VERBOSE: Replacing 70 out of 70 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
INFO: Loaded OpenCL library with dlopen.
INFO: Created 20 GPU delegate kernels.
Done
        Command being timed: ""./app""
        User time (seconds): 1.66
        System time (seconds): 7.27
        Percent of CPU this job got: 97%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.15
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 1907704
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 0
        Minor (reclaiming a frame) page faults: 126277
        Voluntary context switches: 2424
        Involuntary context switches: 16
        Swaps: 0
        File system inputs: 0
        File system outputs: 3520
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 0
```

@grantjensen, can you please take a look? Thanks.

misterBart (Issue Creator) on (2024-10-03 14:31:56 UTC): I am glad you could reproduce the issue.

The factor of RAM increase from your measurements is: {Maximum resident set size (kbytes) with 20 Interpreters} / {Maximum resident set size (kbytes) with 10 Interpreters} =1907704 kbytes / 972544 kybytes = 1863 megabytes / 950 megabytes = 1.96

That is roughly the same factor I measured when going from 10 to 20 Interpreters. So the RAM consumption scales with roughly the same factor as the number of Interpreters.

The issue is: why is ~100 megabytes of host memory consumed by an Interpreter on Nvidia GPUs?

misterBart (Issue Creator) on (2024-10-14 08:31:20 UTC): Any news in backtracing the issue? If I can help out, then do not hesitate to ask.

pkgoogle (Assginee) on (2024-10-14 19:04:22 UTC): Hi @misterBart, I don't think we need help in backtracing the issue but if you want to investigate the issue and contribute a PR, we will happily review it.

misterBart (Issue Creator) on (2024-10-15 12:45:16 UTC): Unfortunately I cannot suggest a PR because I do not know what causes the issue, let alone how to solve the issue. 
I will then wait on what you guys find in backtracing.

misterBart (Issue Creator) on (2024-11-11 08:45:07 UTC): Perhaps some relevant info for you guys: on an Intel HD 520 GPU the memory consumption is ~10 megabytes per Interpreter. That is a lot less than the ~100 megabytes per Interpreter on an Nvidia GPU. Still, Xnnpack (CPU) uses merely ~0.25 megabytes per Interpreter.

gaikwadrahul8 on (2024-11-26 07:57:45 UTC): Hi, @misterBart 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/38

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 17:54:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74521"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74521"">No</a>

"
2486657157,issue,closed,completed,Crash at TfLiteGpuDelegateV2Delete,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Lots of crash at TfLiteGpuDelegateV2Delete

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
tensorflowLite = ""2.16.1""
tensorflowLiteGpuDelegatePlugin = ""0.4.4""
tensorflowLiteSupport = ""0.4.4""


tensorflow-lite = { group = ""org.tensorflow"", name = ""tensorflow-lite"", version.ref = ""tensorflowLite"" }
tensorflow-lite-gpu = { group = ""org.tensorflow"", name = ""tensorflow-lite-gpu"", version.ref = ""tensorflowLite"" }
tensorflow-lite-gpu-api = { group = ""org.tensorflow"", name = ""tensorflow-lite-gpu-api"", version.ref = ""tensorflowLite"" }
tensorflow-lite-gpu-delegate-plugin = { group = ""org.tensorflow"", name = ""tensorflow-lite-gpu-delegate-plugin"", version.ref = ""tensorflowLiteGpuDelegatePlugin"" }
tensorflow-lite-support = { group = ""org.tensorflow"", name = ""tensorflow-lite-support"", version.ref = ""tensorflowLiteSupport"" }

-
Crashed: Thread: SIGSEGV  0x0000000000000007
#00 pc 0x12732c libGLESv2_adreno.so
#01 pc 0x280276 libGLESv2_adreno.so
#02 pc 0x2802a6 libGLESv2_adreno.so
#03 pc 0x280276 libGLESv2_adreno.so
#04 pc 0x280276 libGLESv2_adreno.so
#05 pc 0xf6ff3 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#06 pc 0xcaaf3 libGLESv2_adreno.so
#07 pc 0xf6f03 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#08 pc 0x4089b libc++.so
#09 pc 0x619b libEGL.so
#10 pc 0xae71 libEGL.so
#11 pc 0xf6b25 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#12 pc 0x20026 libEGL.so
#13 pc 0xf6ff3 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#14 pc 0xf6a5f libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#15 pc 0xf6861 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#16 pc 0xf66fb libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#17 pc 0xf65f7 libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#18 pc 0x62bdf libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#19 pc 0xf644d libtensorflowlite_gpu_jni.so (TfLiteGpuDelegateV2Delete)
#20 pc 0x5ee4f libtensorflowlite_gpu_jni.so (Java_org_tensorflow_lite_gpu_CompatibilityList_createCompatibilityList)
-
```
",shawn-lin-013,2024-08-26 11:43:20+00:00,['pkgoogle'],2024-09-24 02:01:43+00:00,2024-09-24 02:01:40+00:00,https://github.com/tensorflow/tensorflow/issues/74518,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TF 2.16', '')]","[{'comment_id': 2311703834, 'issue_id': 2486657157, 'author': 'tilakrayal', 'body': '@shawn-lin-013,\r\nCould you please share a reproducible code that supports your statement so that the issue can be debugged in the effective way? Thank you!', 'created_at': datetime.datetime(2024, 8, 27, 6, 48, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311735419, 'issue_id': 2486657157, 'author': 'shawn-lin-013', 'body': '> @shawn-lin-013, Could you please share a reproducible code that supports your statement so that the issue can be debugged in the effective way? Thank you!\r\n\r\nUnfortunately, we saw this on Firebase Crashlytics and were unable to reproduce it. 😢', 'created_at': datetime.datetime(2024, 8, 27, 7, 8, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338340249, 'issue_id': 2486657157, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nCould you please look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 9, 9, 14, 50, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338986061, 'issue_id': 2486657157, 'author': 'pkgoogle', 'body': ""Hi @shawn-lin-013, does Firebase Crashlytics provide any more information? ... is this crashing on a build step or when the code calls something? Is there a stack trace from source or anything like that? As it currently stands, this isn't quite enough information to continue."", 'created_at': datetime.datetime(2024, 9, 9, 20, 6, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354345469, 'issue_id': 2486657157, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 17, 1, 48, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369959078, 'issue_id': 2486657157, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369959156, 'issue_id': 2486657157, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74518"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74518"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-27 06:48:20 UTC): @shawn-lin-013,
Could you please share a reproducible code that supports your statement so that the issue can be debugged in the effective way? Thank you!

shawn-lin-013 (Issue Creator) on (2024-08-27 07:08:42 UTC): Unfortunately, we saw this on Firebase Crashlytics and were unable to reproduce it. 😢

gaikwadrahul8 on (2024-09-09 14:50:55 UTC): Hi, @pkgoogle

Could you please look into this issue. Thank you.

pkgoogle (Assginee) on (2024-09-09 20:06:25 UTC): Hi @shawn-lin-013, does Firebase Crashlytics provide any more information? ... is this crashing on a build step or when the code calls something? Is there a stack trace from source or anything like that? As it currently stands, this isn't quite enough information to continue.

github-actions[bot] on (2024-09-17 01:48:13 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-24 02:01:39 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-24 02:01:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74518"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74518"">No</a>

"
2485112350,issue,closed,not_planned,GRU behaves very differently with GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Google colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

GRU layer seems broken beyond repair with GPU execution. This is not the only problem, but the returned values are totally malformed from call.

Another problem I don't have minimal code to reproduce for, is that calling fit() on model with gru, it seems to call len() on some tensor if you use GPU, but not on CPU.

I do not have access to GPU of my own, so I'm relying on Google Colab to give me GPU access. It's possible Google Colab is to blame, or perhaps this bug has been fixed already since Google Colab uses Keras 3.4.1. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class TestModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.gru = tf.keras.layers.GRU(10, return_sequences=True, return_state=True)

    def call(self, inputs):
        return self.gru(inputs)

# Create and test the model
model = TestModel()
test_input = tf.random.uniform((2, 3, 5))  # Batch size = 2, sequence length = 3, feature size = 5
output = model(test_input)
print(""Output types and shapes:"", [(type(o), o.shape) for o in output])
```


### Relevant log output

```shell
With GPU:
Output types and shapes: [(<class 'tensorflow.python.framework.ops.EagerTensor'>, TensorShape([2, 3, 10])), (<class 'tensorflow.python.framework.ops.EagerTensor'>, TensorShape([10])), (<class 'tensorflow.python.framework.ops.EagerTensor'>, TensorShape([10]))]


With CPU:
Output types and shapes: [(<class 'tensorflow.python.framework.ops.EagerTensor'>, TensorShape([2, 3, 10])), (<class 'tensorflow.python.framework.ops.EagerTensor'>, TensorShape([2, 10]))]
```
",Jonii,2024-08-25 07:57:20+00:00,['Venkat6871'],2024-09-30 06:24:37+00:00,2024-09-30 06:24:34+00:00,https://github.com/tensorflow/tensorflow/issues/74475,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2311693810, 'issue_id': 2485112350, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.17.0 with CPU & GPU and faced the same issue. Please find the [gist1](https://colab.research.google.com/gist/Venkat6871/42c220290c6907001f65feb58f7137aa/74475_2-17-0-v_cpu.ipynb), [gist2](https://colab.research.google.com/gist/Venkat6871/ac95a8a3a6df57eb8a0dfd944498afa5/74475_2-17-0-v_gpu.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 27, 6, 41, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311990682, 'issue_id': 2485112350, 'author': 'Venkat6871', 'body': 'Hi @**Jonii** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 27, 9, 15, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312084070, 'issue_id': 2485112350, 'author': 'Jonii', 'body': 'Done, I opened an issue on keras repo.', 'created_at': datetime.datetime(2024, 8, 27, 9, 58, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382201284, 'issue_id': 2485112350, 'author': 'Venkat6871', 'body': 'Hi **@Jonii** ,\r\nPlease feel free to close this issue since it is already being tracked on the Keras repository. It will be easier to track there.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 30, 6, 15, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382213666, 'issue_id': 2485112350, 'author': 'Jonii', 'body': 'Closed the issue as ""not planned"", since wrong repo for it.', 'created_at': datetime.datetime(2024, 9, 30, 6, 24, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382213697, 'issue_id': 2485112350, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74475"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74475"">No</a>', 'created_at': datetime.datetime(2024, 9, 30, 6, 24, 36, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-27 06:41:24 UTC): I tried to run your code on Colab using TF v2.17.0 with CPU & GPU and faced the same issue. Please find the [gist1](https://colab.research.google.com/gist/Venkat6871/42c220290c6907001f65feb58f7137aa/74475_2-17-0-v_cpu.ipynb), [gist2](https://colab.research.google.com/gist/Venkat6871/ac95a8a3a6df57eb8a0dfd944498afa5/74475_2-17-0-v_gpu.ipynb) here for reference.
Thank you!

Venkat6871 (Assginee) on (2024-08-27 09:15:44 UTC): Hi @**Jonii** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

Jonii (Issue Creator) on (2024-08-27 09:58:20 UTC): Done, I opened an issue on keras repo.

Venkat6871 (Assginee) on (2024-09-30 06:15:45 UTC): Hi **@Jonii** ,
Please feel free to close this issue since it is already being tracked on the Keras repository. It will be easier to track there.
Thank you!

Jonii (Issue Creator) on (2024-09-30 06:24:34 UTC): Closed the issue as ""not planned"", since wrong repo for it.

google-ml-butler[bot] on (2024-09-30 06:24:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74475"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74475"">No</a>

"
2484645406,issue,closed,completed,GPU not detected,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

unknown/unknown

### GPU model and memory

_No response_

### Current behavior?

Either the installation guide is missing information or Tensorflow isn't working with the recommended CUDA/cuDNN version.

### Standalone code to reproduce the issue

```shell
import * as tf from ""@tensorflow/tfjs-node-gpu"";

await tf.ready();
```


### Relevant log output

```shell
2024-08-24 15:40:49.275109: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-24 15:40:49.279326: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2024-08-24 15:40:49.279356: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2024-08-24 15:40:49.329972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-24 15:40:50.483861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-08-24 15:40:50.484116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.484169: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.484202: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.484236: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.526406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.526484: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2024-08-24 15:40:50.526494: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
",lukemovement,2024-08-24 14:46:11+00:00,['tilakrayal'],2024-08-24 19:45:25+00:00,2024-08-24 19:45:22+00:00,https://github.com/tensorflow/tensorflow/issues/74459,"[('type:bug', 'Bug')]","[{'comment_id': 2308516161, 'issue_id': 2484645406, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74459"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74459"">No</a>', 'created_at': datetime.datetime(2024, 8, 24, 19, 45, 24, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-08-24 19:45:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74459"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74459"">No</a>

"
2484097877,issue,closed,completed,can't get the correct output,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow-gpu 1.14.0

### Custom code

Yes

### OS platform and distribution

Window 11 pro version 23H2

### Mobile device

Window 11 pro version 23H2

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 10.0.130 cuDnn 7.6.5

### GPU model and memory

NVIDIA GeForce RTX 3060 12GB memory

### Current behavior?

When I use the tensorflow-gpu.14.0 to test the model, It does not immediately output the result, but rather gets stuck for tens of seconds at the ""sess.run"" point. This only happens with the first image input to the model, while the processing speed for subsequent inputs is normal. When I check the results, I also find that the output is not correct. 
![image](https://github.com/user-attachments/assets/43b7dba5-da8a-4ae0-93a0-dae31e1d3eaf)
I am confident that the model itself is not the problem, because I can obtain the correct results when using the CPU version of TensorFlow.
Therefore, I want to know where the error is occurring. Here are some specific configurations:
![image](https://github.com/user-attachments/assets/55f6172a-a746-4c72-999b-fa4d0a2c1421)
![image](https://github.com/user-attachments/assets/534e11c7-1b6d-47e7-ac1f-15b559f59ce4)
![image](https://github.com/user-attachments/assets/00fbdfed-09d9-45fc-8eb0-210ea59bfebd)
![image](https://github.com/user-attachments/assets/7e22f5e6-f76e-4ec0-abfe-cba3aa19d6dd)
![image](https://github.com/user-attachments/assets/5c3f0126-687e-400c-8171-73a6df7b0052)
Environment variables in “Path”:
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\lib
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp
![image](https://github.com/user-attachments/assets/9f5ce251-783e-4277-8785-aadf641f62c2)





### Standalone code to reproduce the issue

```shell
I am unable to provide a test code, because this is not a problem with the model itself. I would be very grateful if you can help me solve my problem!
```


### Relevant log output

_No response_",JJGNB,2024-08-24 01:21:26+00:00,['Venkat6871'],2024-12-07 02:06:56+00:00,2024-12-07 02:06:53+00:00,https://github.com/tensorflow/tensorflow/issues/74442,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 1.14', 'for issues seen with TF 1.14')]","[{'comment_id': 2311970658, 'issue_id': 2484097877, 'author': 'Venkat6871', 'body': 'Hi @JJGNB ,\r\nApologies for the dealy, We see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.17.0 and let us know if the issue still persists in newer versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 27, 9, 6, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312191085, 'issue_id': 2484097877, 'author': 'JJGNB', 'body': '> Hi @JJGNB , Apologies for the dealy, We see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.17.0 and let us know if the issue still persists in newer versions. Thank you!\r\n\r\n@Venkat6871 This is a project built on top of someone else\'s work, and it is difficult to convert it to tensorflow 2.0.  And when I use tensorflow 2.0 on another project , the problem still exists. \r\nTo be more specific, the command line output (use tensorflow 1.14.0)  is as follows:\r\n\r\n`D:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  np_resource = np.dtype([(""resource"", np.ubyte, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])\r\nD:\\ANA\\envs\\JJGNBTF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or \'1type\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \'(1,)type\'.\r\n  np_resource = np.dtype([(""resource"", np.ubyte, 1)])\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\evaluate_DDBF.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2024-08-27 18:36:51.499307: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2024-08-27 18:36:51.512359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: NVIDIA GeForce RTX 3060 major: 8 minor: 6 memoryClockRate(GHz): 1.867\r\npciBusID: 0000:01:00.0\r\n2024-08-27 18:36:51.512448: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2024-08-27 18:36:51.512639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2024-08-27 18:36:51.513841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2024-08-27 18:36:51.516747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: NVIDIA GeForce RTX 3060 major: 8 minor: 6 memoryClockRate(GHz): 1.867\r\npciBusID: 0000:01:00.0\r\n2024-08-27 18:36:51.516899: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2024-08-27 18:36:51.516997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2024-08-27 18:39:26.925472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2024-08-27 18:39:26.925596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2024-08-27 18:39:26.925631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2024-08-27 18:39:26.926202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9681 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\evaluate_DDBF.py:31: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\evaluate_DDBF.py:32: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\model.py:25: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\model.py:25: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\r\n\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B53C8>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CF8>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CC0>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5F98>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298940>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197282985C0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197282985C0>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B58D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B58D0>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001973D759128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001973D759128>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24D30>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24E48>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743112198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743112198>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298390>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743115E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743115E48>>: AttributeError: module \'gast\' has no attribute \'Str\'\r\nWARNING:tensorflow:From E:\\compare_algor\\DDBF-main\\DDBF-main\\evaluate_DDBF.py:44: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.`', 'created_at': datetime.datetime(2024, 8, 27, 10, 50, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493164308, 'issue_id': 2484097877, 'author': 'Venkat6871', 'body': 'Hi **@JJGNB** ,\r\nApologies for the delay. Unfortunately, we no longer support TensorFlow 1.x. It is recommended to migrate to TensorFlow 2.x. Here, I am providing the [documentation](https://www.tensorflow.org/guide/migrate) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 22, 8, 28, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508786415, 'issue_id': 2484097877, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 30, 2, 4, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804430, 'issue_id': 2484097877, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804453, 'issue_id': 2484097877, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74442"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74442"">No</a>', 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-27 09:06:15 UTC): Hi @JJGNB ,
Apologies for the dealy, We see that you are using old version of tensorflow (1.x) which is not actively supported, We recommend that you upgrade to 2.17.0 and let us know if the issue still persists in newer versions.
Thank you!

JJGNB (Issue Creator) on (2024-08-27 10:50:58 UTC): @Venkat6871 This is a project built on top of someone else's work, and it is difficult to convert it to tensorflow 2.0.  And when I use tensorflow 2.0 on another project , the problem still exists. 
To be more specific, the command line output (use tensorflow 1.14.0)  is as follows:

`D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
D:\ANA\envs\JJGNBTF\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\evaluate_DDBF.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2024-08-27 18:36:51.499307: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2024-08-27 18:36:51.512359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: NVIDIA GeForce RTX 3060 major: 8 minor: 6 memoryClockRate(GHz): 1.867
pciBusID: 0000:01:00.0
2024-08-27 18:36:51.512448: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2024-08-27 18:36:51.512639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2024-08-27 18:36:51.513841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2024-08-27 18:36:51.516747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: NVIDIA GeForce RTX 3060 major: 8 minor: 6 memoryClockRate(GHz): 1.867
pciBusID: 0000:01:00.0
2024-08-27 18:36:51.516899: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2024-08-27 18:36:51.516997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2024-08-27 18:39:26.925472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-08-27 18:39:26.925596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2024-08-27 18:39:26.925631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2024-08-27 18:39:26.926202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9681 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)
WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\evaluate_DDBF.py:31: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\evaluate_DDBF.py:32: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\model.py:25: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\model.py:25: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B53C8>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CF8>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5CC0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B5F98>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298940>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197282985C0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197282985C0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B58D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000197281B58D0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001973D759128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001973D759128>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24D30>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019742D24E48>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743112198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743112198>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019728298390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743115E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019743115E48>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From E:\compare_algor\DDBF-main\DDBF-main\evaluate_DDBF.py:44: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.`

Venkat6871 (Assginee) on (2024-11-22 08:28:46 UTC): Hi **@JJGNB** ,
Apologies for the delay. Unfortunately, we no longer support TensorFlow 1.x. It is recommended to migrate to TensorFlow 2.x. Here, I am providing the [documentation](https://www.tensorflow.org/guide/migrate) for your reference.
Thank you!

github-actions[bot] on (2024-11-30 02:04:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-07 02:06:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-07 02:06:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74442"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74442"">No</a>

"
2484033205,issue,closed,completed,Error loading history file,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.15.0-rc1-8-g6887368d6d4 2.15.0

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

pickle.load of a history file fails. File pickle.dump'd on a linux server, then loaded on windows PC. I expected it to just work, not sure what's going on.

### Standalone code to reproduce the issue

```shell
# On Linux server, `FROM tensorflow/tensorflow:2.15.0-gpu`
history = model.fit(...)
with open(model_file.replace("".h5"", ""_history""), ""wb"") as f:
    pickle.dump(history, f)

# On Windows PC, python 3.11.9, `pip list` output: tensorflow                   2.15.0
with open(file_path, 'rb') as file:
    history = pickle.load(file)
```


### Relevant log output

```shell
Error loading history file: 3c_128f_64f_32f_2l_100u_50u_without_implicit_adls_history Layer 'conv2d_3' expected 2 variables, but received 0 variables during loading. Expected: ['time_distributed_3/kernel:0', 'time_distributed_3/bias:0']
```
",Regenhardt,2024-08-23 23:19:25+00:00,['tilakrayal'],2024-11-02 02:01:11+00:00,2024-11-02 02:01:08+00:00,https://github.com/tensorflow/tensorflow/issues/74439,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2311696906, 'issue_id': 2484033205, 'author': 'tilakrayal', 'body': '@Regenhardt,\r\nCould you please provide the complete code to reproduce the issue and also please try to test the code in the latest tensorflow v2.17 which contains keras3.0 by default. Thank you!', 'created_at': datetime.datetime(2024, 8, 27, 6, 43, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312459468, 'issue_id': 2484033205, 'author': 'Regenhardt', 'body': 'Updated tensorflow to 2.17.0 on my local machine and tried to load the file, this is the log:\r\n```\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pickle\r\n>>> with open(""c:\\\\Studium\\\\Bachelorarbeit\\\\model\\\\3c_128f_64f_32f_2l_100u_50u_without_explicit_adls_history"", ""rb"") as file:\r\n...     history = pickle.load(file)\r\n...\r\n2024-08-27 14:21:09.215606: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-08-27 14:21:23.928352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 2, in <module>\r\nModuleNotFoundError: No module named \'keras.src.saving.pickle_utils\'\r\n```\r\n\r\nI thought the complete code is too much, but all of it is public in [this repository](https://gitlab.com/Regenhardt/bachelorarbeit).   \r\n\r\nThe container for training on the server is created using the (IMO) pretty standard [Dockerfile](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Dockerfile?ref_type=heads) and [requirements.txt](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/requirements.txt?ref_type=heads).  \r\nThe files were created running [big_models_training.py](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Code/big_models_training.py?ref_type=heads) on the (linux) server.\r\n\r\nLocally, I load the history files in [fall_detection.dib](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Code/fall_detection.dib?ref_type=heads#L297). But I can reproduce the error by just starting python in a terminal and running the minimal code there. Here\'s a log with tensorflow 2.15.0:\r\n```python\r\nPython 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import pickle\r\n>>> with open(""c:\\\\Studium\\\\Bachelorarbeit\\\\model\\\\3c_128f_64f_32f_2l_100u_50u_without_explicit_adls_history"", ""rb"") as file:\r\n...     history = pickle.load(file)\r\n...\r\n2024-08-27 14:41:24.615017: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nWARNING:tensorflow:From C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\r\n\r\n2024-08-27 14:41:35.050901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:From C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 2, in <module>\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\pickle_utils.py"", line 48, in deserialize_model_from_bytecode\r\n    raise e\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\pickle_utils.py"", line 46, in deserialize_model_from_bytecode\r\n    model = saving_lib.load_model(filepath, safe_mode=False)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 281, in load_model\r\n    raise e\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 269, in load_model\r\n    _load_state(\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 466, in _load_state\r\n    _load_container_state(\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 534, in _load_container_state\r\n    _load_state(\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 457, in _load_state\r\n    _load_state(\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py"", line 435, in _load_state\r\n    trackable.load_own_variables(weights_store.get(inner_path))\r\n  File ""C:\\Users\\mregenhardt\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer.py"", line 3531, in load_own_variables\r\n    raise ValueError(\r\nValueError: Layer \'conv2d_6\' expected 2 variables, but received 0 variables during loading. Expected: [\'time_distributed_6/kernel:0\', \'time_distributed_6/bias:0\']\r\n```', 'created_at': datetime.datetime(2024, 8, 27, 12, 42, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368139661, 'issue_id': 2484033205, 'author': 'tilakrayal', 'body': '@Regenhardt,\r\nApologies for the delay. Looks like this issue is more related to keras3.0 which default for the tensorflow v2.17. Could you please raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 9, 23, 12, 53, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373747318, 'issue_id': 2484033205, 'author': 'Regenhardt', 'body': ""> @Regenhardt, Apologies for the delay. Looks like this issue is more related to keras3.0 which default for the tensorflow v2.17. Could you please raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!\r\n\r\nEven though the original issue ist about tf 2.15.0? I just updated to 2.17 to test it. I'm actually using tf 2.15.0 on both server and client here."", 'created_at': datetime.datetime(2024, 9, 25, 10, 53, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422855570, 'issue_id': 2484033205, 'author': 'tilakrayal', 'body': '@Regenhardt,\r\nCould you please close this issue, as this issue is related to Keras and it can be tracked in the respective repo. Thank you!', 'created_at': datetime.datetime(2024, 10, 18, 16, 39, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439174088, 'issue_id': 2484033205, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 26, 2, 0, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797229, 'issue_id': 2484033205, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797253, 'issue_id': 2484033205, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74439"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74439"">No</a>', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 10, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-27 06:43:32 UTC): @Regenhardt,
Could you please provide the complete code to reproduce the issue and also please try to test the code in the latest tensorflow v2.17 which contains keras3.0 by default. Thank you!

Regenhardt (Issue Creator) on (2024-08-27 12:42:40 UTC): Updated tensorflow to 2.17.0 on my local machine and tried to load the file, this is the log:
```
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
...     history = pickle.load(file)
...
2024-08-27 14:21:09.215606: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-27 14:21:23.928352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
ModuleNotFoundError: No module named 'keras.src.saving.pickle_utils'
```

I thought the complete code is too much, but all of it is public in [this repository](https://gitlab.com/Regenhardt/bachelorarbeit).   

The container for training on the server is created using the (IMO) pretty standard [Dockerfile](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Dockerfile?ref_type=heads) and [requirements.txt](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/requirements.txt?ref_type=heads).  
The files were created running [big_models_training.py](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Code/big_models_training.py?ref_type=heads) on the (linux) server.

Locally, I load the history files in [fall_detection.dib](https://gitlab.com/Regenhardt/bachelorarbeit/-/blob/master/Code/fall_detection.dib?ref_type=heads#L297). But I can reproduce the error by just starting python in a terminal and running the minimal code there. Here's a log with tensorflow 2.15.0:
```python
Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
...     history = pickle.load(file)
...
2024-08-27 14:41:24.615017: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2024-08-27 14:41:35.050901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

WARNING:tensorflow:From C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\optimizers\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\pickle_utils.py"", line 48, in deserialize_model_from_bytecode
    raise e
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\pickle_utils.py"", line 46, in deserialize_model_from_bytecode
    model = saving_lib.load_model(filepath, safe_mode=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 281, in load_model
    raise e
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 269, in load_model
    _load_state(
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 466, in _load_state
    _load_container_state(
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 534, in _load_container_state
    _load_state(
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 457, in _load_state
    _load_state(
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\saving\saving_lib.py"", line 435, in _load_state
    trackable.load_own_variables(weights_store.get(inner_path))
  File ""C:\Users\mregenhardt\AppData\Roaming\Python\Python311\site-packages\keras\src\engine\base_layer.py"", line 3531, in load_own_variables
    raise ValueError(
ValueError: Layer 'conv2d_6' expected 2 variables, but received 0 variables during loading. Expected: ['time_distributed_6/kernel:0', 'time_distributed_6/bias:0']
```

tilakrayal (Assginee) on (2024-09-23 12:53:51 UTC): @Regenhardt,
Apologies for the delay. Looks like this issue is more related to keras3.0 which default for the tensorflow v2.17. Could you please raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!

Regenhardt (Issue Creator) on (2024-09-25 10:53:20 UTC): Even though the original issue ist about tf 2.15.0? I just updated to 2.17 to test it. I'm actually using tf 2.15.0 on both server and client here.

tilakrayal (Assginee) on (2024-10-18 16:39:02 UTC): @Regenhardt,
Could you please close this issue, as this issue is related to Keras and it can be tracked in the respective repo. Thank you!

github-actions[bot] on (2024-10-26 02:00:08 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-02 02:01:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-02 02:01:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74439"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74439"">No</a>

"
2483245218,issue,closed,completed,How to generate a tflite with a fixed input from a tensorflow saved model with dynamic input?  ,"### 1. System information

- OS Platform and Distribution (e.g., Windows 11):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (2.4.0):

### 2. Code

converter = tf.lite.TFLiteConverter.from_saved_model('./model_tf')
converter.allow_custom_ops = True  

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()


",moses574,2024-08-23 14:12:42+00:00,['gaikwadrahul8'],2024-09-18 01:58:43+00:00,2024-09-18 01:58:43+00:00,https://github.com/tensorflow/tensorflow/issues/74414,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.4', 'for issues related to TF 2.4')]","[{'comment_id': 2325311948, 'issue_id': 2483245218, 'author': 'gaikwadrahul8', 'body': 'Hi, @moses574 \r\n\r\nI apologize for delayed response and as far I know to generate TFLite model with a fixed input shape from TensorFlow SavedModel that originally has dynamic input shapes in that case before converting the model you need to define a fixed input shape. This involves creating a TensorFlow model with a fixed input shape and then converting it to TFLite\r\n\r\nIf you are starting with a model that has dynamic shapes, you will need to modify it to use a fixed shape for the conversion process. You can do this by specifying a fixed input shape by doing something like below, I tried in google colab with sample `ssd_mobilenet_v1` model and its seems like working as expected so could you please try from your end and let me know is it working as expected or Am I missing something here ?\r\n\r\nHere is Google colab [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/7c7f011f93304b23ca13866c0eb64d06/issue-74414.ipynb) for reference and I followed the exact same steps mentioned below :\r\n\r\n```\r\n!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz\r\n!tar -xzvf ""/content/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz"" -C ""/content/""\r\n\r\nmodel = tf.saved_model.load(\'/content/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/saved_model\')\r\nconcrete_func = model.signatures[\r\ntf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 300, 300, 3])\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open(\'detect.tflite\', \'wb\') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 2, 21, 23, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460111, 'issue_id': 2483245218, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336363, 'issue_id': 2483245218, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 42, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-02 21:23:49 UTC): Hi, @moses574 

I apologize for delayed response and as far I know to generate TFLite model with a fixed input shape from TensorFlow SavedModel that originally has dynamic input shapes in that case before converting the model you need to define a fixed input shape. This involves creating a TensorFlow model with a fixed input shape and then converting it to TFLite

If you are starting with a model that has dynamic shapes, you will need to modify it to use a fixed shape for the conversion process. You can do this by specifying a fixed input shape by doing something like below, I tried in google colab with sample `ssd_mobilenet_v1` model and its seems like working as expected so could you please try from your end and let me know is it working as expected or Am I missing something here ?

Here is Google colab [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/7c7f011f93304b23ca13866c0eb64d06/issue-74414.ipynb) for reference and I followed the exact same steps mentioned below :

```
!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz
!tar -xzvf ""/content/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz"" -C ""/content/""

model = tf.saved_model.load('/content/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/saved_model')
concrete_func = model.signatures[
tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
concrete_func.inputs[0].set_shape([1, 300, 300, 3])

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with open('detect.tflite', 'wb') as f:
  f.write(tflite_model)
```

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-10 01:58:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:42 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2482882874,issue,closed,completed,"While importing Tensorflow on CPU , a dynamic link library (DLL) initialization routine failed","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.10

### Custom code

No

### OS platform and distribution

Windows 10 x64

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Microsoft Windows [Version 10.0.10240]
(c) 2015 Microsoft Corporation. All rights reserved.

C:\Users\Samama>conda env list
# conda environments:
#
base                     C:\Users\Samama\anaconda3
myenv                    C:\Users\Samama\anaconda3\envs\myenv


C:\Users\Samama>conda activate myenv

(myenv) C:\Users\Samama>conda list
# packages in environment at C:\Users\Samama\anaconda3\envs\myenv:
#
# Name                    Version                   Build  Channel
_tflow_select             2.3.0                       mkl
abseil-cpp                20211102.0           hd77b12b_0
absl-py                   2.1.0           py310haa95532_0
aiohttp                   3.9.5           py310h2bbff1b_0
aiosignal                 1.2.0              pyhd3eb1b0_0
astunparse                1.6.3                      py_0
async-timeout             4.0.3           py310haa95532_0
attrs                     23.1.0          py310haa95532_0
blas                      1.0                         mkl
blinker                   1.6.2           py310haa95532_0
brotli-python             1.0.9           py310hd77b12b_8
bzip2                     1.0.8                h2bbff1b_6
c-ares                    1.19.1               h2bbff1b_0
ca-certificates           2024.7.2             haa95532_0
cachetools                5.3.3           py310haa95532_0
certifi                   2024.7.4        py310haa95532_0
cffi                      1.16.0          py310h2bbff1b_1
charset-normalizer        3.3.2              pyhd3eb1b0_0
click                     8.1.7           py310haa95532_0
colorama                  0.4.6           py310haa95532_0
cryptography              41.0.3          py310h3438e0d_0
flatbuffers               2.0.0                h6c2663c_0
frozenlist                1.4.0           py310h2bbff1b_0
gast                      0.4.0              pyhd3eb1b0_0
giflib                    5.2.1                h8cc25b3_3
google-auth               2.29.0          py310haa95532_0
google-auth-oauthlib      0.4.4              pyhd3eb1b0_0
google-pasta              0.2.0              pyhd3eb1b0_0
grpc-cpp                  1.48.2               hf108199_0
grpcio                    1.48.2          py310hf108199_0
h5py                      3.11.0          py310hed405ee_0
hdf5                      1.12.1               h51c971a_3
icc_rt                    2022.1.0             h6049295_2
icu                       58.2                 ha925a31_3
idna                      3.7             py310haa95532_0
intel-openmp              2023.1.0         h59b6b97_46320
jpeg                      9e                   h827c3e9_3
keras                     2.10.0          py310haa95532_0
keras-preprocessing       1.1.2              pyhd3eb1b0_0
libcurl                   8.7.1                h86230a5_0
libffi                    3.4.4                hd77b12b_1
libpng                    1.6.39               h8cc25b3_0
libprotobuf               3.20.3               h23ce68f_0
libssh2                   1.10.0               hcd4344a_2
markdown                  3.4.1           py310haa95532_0
markupsafe                2.1.3           py310h2bbff1b_0
mkl                       2023.1.0         h6b88ed4_46358
mkl-service               2.4.0           py310h2bbff1b_1
mkl_fft                   1.3.8           py310h2bbff1b_0
mkl_random                1.2.4           py310h59b6b97_0
multidict                 6.0.4           py310h2bbff1b_0
numpy                     1.26.4          py310h055cbcc_0
numpy-base                1.26.4          py310h65a83cf_0
oauthlib                  3.2.2           py310haa95532_0
openssl                   1.1.1w               h2bbff1b_0
opt_einsum                3.3.0              pyhd3eb1b0_1
packaging                 24.1            py310haa95532_0
pip                       24.2            py310haa95532_0
protobuf                  3.20.3          py310hd77b12b_0
pyasn1                    0.4.8              pyhd3eb1b0_0
pyasn1-modules            0.2.8                      py_0
pybind11-abi              5                    hd3eb1b0_0
pycparser                 2.21               pyhd3eb1b0_0
pyjwt                     2.8.0           py310haa95532_0
pyopenssl                 23.2.0          py310haa95532_0
pysocks                   1.7.1           py310haa95532_0
python                    3.10.0               h96c0403_3
python-flatbuffers        24.3.25         py310haa95532_0
re2                       2022.04.01           hd77b12b_0
requests                  2.32.3          py310haa95532_0
requests-oauthlib         2.0.0           py310haa95532_0
rsa                       4.7.2              pyhd3eb1b0_1
scipy                     1.13.1          py310h8640f81_0
setuptools                72.1.0          py310haa95532_0
six                       1.16.0             pyhd3eb1b0_1
snappy                    1.2.1                hcdb6601_0
sqlite                    3.45.3               h2bbff1b_0
tbb                       2021.8.0             h59b6b97_0
tensorboard               2.10.0          py310haa95532_0
tensorboard-data-server   0.6.1           py310haa95532_0
tensorboard-plugin-wit    1.8.1           py310haa95532_0
tensorflow                2.10.0          mkl_py310hd99672f_0
tensorflow-base           2.10.0          mkl_py310h6a7f48e_0
tensorflow-estimator      2.10.0          py310haa95532_0
termcolor                 2.1.0           py310haa95532_0
tk                        8.6.14               h0416ee5_0
typing_extensions         4.11.0          py310haa95532_0
tzdata                    2024a                h04d1e81_0
urllib3                   2.2.2           py310haa95532_0
vc                        14.40                h2eaa2aa_0
vs2015_runtime            14.40.33807          h98bb1dd_0
werkzeug                  3.0.3           py310haa95532_0
wheel                     0.43.0          py310haa95532_0
win_inet_pton             1.1.0           py310haa95532_0
wrapt                     1.14.1          py310h2bbff1b_0
xz                        5.4.6                h8cc25b3_1
yarl                      1.9.3           py310h2bbff1b_0
zlib                      1.2.13               h8cc25b3_1

(myenv) C:\Users\Samama>python
Python 3.10.0 | packaged by conda-forge | (default, Nov 10 2021, 13:20:59) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
>>>

### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Samama\anaconda3\envs\myenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",SamamaSaleem,2024-08-23 10:56:10+00:00,['tilakrayal'],2024-12-30 09:25:02+00:00,2024-12-18 17:57:14+00:00,https://github.com/tensorflow/tensorflow/issues/74405,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2308865413, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem  what version of Microsoft Visual C++ Redistributable are u using on your system ?', 'created_at': datetime.datetime(2024, 8, 25, 14, 7, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308909189, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': ""> @SamamaSaleem what version of Microsoft Visual C++ Redistributable are u using on your system ?\r\n\r\nIt's **Microsoft Visual C++ 2015-2022 Redistributable (x-64) - 14.40.33810**"", 'created_at': datetime.datetime(2024, 8, 25, 16, 9, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308958461, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem have u tried reinstalling anaconda , i had same issue once and reinstalling anaconda and packages fixed it for me  , this specific one ""ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ""', 'created_at': datetime.datetime(2024, 8, 25, 18, 59, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309429013, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': '> @SamamaSaleem have u tried reinstalling anaconda , i had same issue once and reinstalling anaconda and packages fixed it for me , this specific one ""ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ""\r\n\r\nYes. I tried this numerous times. In fact, I reinstalled Windows, but the issue persists. I tried creating new env, installing Python 3.10 and Tensorflow 2.10. I also set the PATH environmental variable to the Conda\'s path. No avail. I am not sure is it because of some version compatibility issue or system requirement issue ( as I am installing it on CPU system)', 'created_at': datetime.datetime(2024, 8, 26, 6, 29, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309910504, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem maybe  the issue is with protobuf, can you  tell me the verison of protobuf you are using ? and try to downgrade it and then run your code?', 'created_at': datetime.datetime(2024, 8, 26, 10, 46, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310292457, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': '@rakshitdabral I am not using protobuf.', 'created_at': datetime.datetime(2024, 8, 26, 14, 0, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310340456, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem can u try installing protobuf  then? and if it doesnt work try reducing your tensorflow version , i read that reducing tensorflow version helped some people for the same issue', 'created_at': datetime.datetime(2024, 8, 26, 14, 21, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311663937, 'issue_id': 2482882874, 'author': 'tilakrayal', 'body': '@SamamaSaleem,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 27, 6, 19, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311841801, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': ""@rakshitdabral I tried installing protobuf, and downgrading tensorflow version, but it hasn't resolved the issue"", 'created_at': datetime.datetime(2024, 8, 27, 8, 7, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311846883, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': ""@tilakrayal -\r\n- I tried Microsoft Visual C++ 2015-2022 Redistributable (x-64) - 14.40.33810\r\n- My CPU/Python is on 64 bits\r\n- I tried to update the PATH variable with the path of the installed libraries.\r\n\r\nUnfortunately, the error isn't gone."", 'created_at': datetime.datetime(2024, 8, 27, 8, 9, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317187568, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': 'Any way out please?', 'created_at': datetime.datetime(2024, 8, 29, 9, 48, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317607792, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '> Any way out please?\r\n\r\nLooking into it , will surely update you if I find something', 'created_at': datetime.datetime(2024, 8, 29, 13, 6, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322882675, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem which  cpu , are you running tensorflow on??', 'created_at': datetime.datetime(2024, 8, 31, 12, 26, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322884070, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': ""@SamamaSaleem can you try this once\r\n\r\nDo the following.\r\n\r\n1. Run the anaconda prompt as administrator.(right click-> run as administrator).\r\n2.  pip uninstall tensorflow. \r\n3. Close anaconda prompt.\r\n4 .Again run anaconda prompt as administrator.\r\n5.type:\r\n conda install tensorflow\r\n6. It will ask for some y / n\r\n7. Type y.\r\n\r\n\r\nNow after all done, change your python interpreter's environment to anaconda environment where you installed tensorflow. Now try importing tensorflow."", 'created_at': datetime.datetime(2024, 8, 31, 12, 32, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323932686, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': '@rakshitdabral CPU: Intel (R) Celeron (R) CPU B820 @ 1.70GHz', 'created_at': datetime.datetime(2024, 9, 2, 6, 42, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324190226, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': 'Facing this error now:\r\n(base) C:\\Windows\\system32>conda install tensorflow\r\nChannels:\r\n - defaults\r\nPlatform: win-64\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: | warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE\r\nfailed\r\n\r\nLibMambaUnsatisfiableError: Encountered problems while solving:\r\n  - nothing provides bleach 1.5.0 needed by tensorboard-1.7.0-py35he025d50_1\r\n\r\nCould not solve for environment specs\r\nThe following packages are incompatible\r\n├─ pin-1 is installable and it requires\r\n│  └─ python 3.11.* , which can be installed;\r\n└─ tensorflow is not installable because there are no viable options\r\n   ├─ tensorflow [1.10.0|1.9.0] would require\r\n   │  └─ python 3.5.* , which conflicts with any installable versions previously reported;\r\n   ├─ tensorflow [1.10.0|1.11.0|...|2.1.0] would require\r\n   │  └─ python 3.6.* , which conflicts with any installable versions previously reported;\r\n   ├─ tensorflow [1.13.1|1.14.0|...|2.9.1] would require\r\n   │  └─ python 3.7.* , which conflicts with any installable versions previously reported;\r\n   ├─ tensorflow [1.7.0|1.7.1|1.8.0] would require\r\n   │  └─ tensorboard [>=1.7.0,<1.8.0 |>=1.8.0,<1.9.0 ], which requires\r\n   │     └─ bleach 1.5.0 , which does not exist (perhaps a missing channel);\r\n   ├─ tensorflow [2.10.0|2.8.2|2.9.1] would require\r\n   │  └─ python 3.10.* , which conflicts with any installable versions previously reported;\r\n   ├─ tensorflow [2.10.0|2.3.0|...|2.9.1] would require\r\n   │  └─ python 3.8.* , which conflicts with any installable versions previously reported;\r\n   └─ tensorflow [2.10.0|2.5.0|2.6.0|2.8.2|2.9.1] would require\r\n      └─ python 3.9.* , which conflicts with any installable versions previously reported.', 'created_at': datetime.datetime(2024, 9, 2, 8, 55, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324222211, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': 'tried by creating a new env with python == 3.10 using the following:\r\nconda create --name myenv python==3.10\r\nand installed the tensorflow==2.10\r\nThough it has been installed, the same error of DLL initialization failed is popped again when importing', 'created_at': datetime.datetime(2024, 9, 2, 9, 10, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324286998, 'issue_id': 2482882874, 'author': 'SamamaSaleem', 'body': 'after the complete process, here is the output:\r\n![image](https://github.com/user-attachments/assets/c381ed7e-ccd6-4885-a9c8-ea16efa74ba4)', 'created_at': datetime.datetime(2024, 9, 2, 9, 39, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327893470, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '> @rakshitdabral CPU: Intel (R) Celeron (R) CPU B820 @ 1.70GHz\r\n\r\nok so your cpu do supports AVX', 'created_at': datetime.datetime(2024, 9, 4, 4, 32, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327894966, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '> * nothing provides bleach 1.5.0 needed by tensorboard-1.7.0-py35he025d50_1\r\n> \r\n> Could not solve for environment specs The following packages are incompatible ├─ pin-1 is installable and it requires │ └─ python 3.11.* , which can be installed; └─ tensorflow is not installable because there are no viable options ├─ tensorflow [1.10.0|1.9.0] would require │ └─ python 3.5.* , which conflicts with any installable versions previously reported; ├─ tensorflow [1.10.0|1.11.0|...|2.1.0] would require │ └─ python 3.6.* , which conflicts with any installable versions previously reported; ├─ tensorflow [1.13.1|1.14.0|...|2.9.1] would require │ └─ python 3.7.* , which conflicts with any installable versions previously reported; ├─ tensorflow [1.7.0|1.7.1|1.8.0] would require │ └─ tensorboard [>=1.7.0,<1.8.0 |>=1.8.0,<1.9.0 ], which requires │ └─ bleach 1.5.0 , which does not exist (perhaps a missing channel); ├─ tensorflow [2.10.0|2.8.2|2.9.1] would require │ └─ python 3.10.* , which conflicts with any installable versions previously reported; ├─ tensorflow [2.10.0|2.3.0|...|2.9.1] would require │ └─ python 3.8.* , which conflicts with any installable versions previously reported; └─ tensorflow [2.10.0|2.5.0|2.6.0|2.8.2|2.9.1] would require └─ python 3.9.* , which conflicts with any installable versions previously reported.\r\n\r\n\r\n\r\ntry doing this ,  conda config --add channels conda-forge', 'created_at': datetime.datetime(2024, 9, 4, 4, 34, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327910833, 'issue_id': 2482882874, 'author': 'rakshitdabral', 'body': '@SamamaSaleem can you tell me how you install your packages ?? are you using anaconda terminal or cmd??', 'created_at': datetime.datetime(2024, 9, 4, 4, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343955330, 'issue_id': 2482882874, 'author': 'tilakrayal', 'body': '@belitskiy', 'created_at': datetime.datetime(2024, 9, 11, 15, 11, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346154679, 'issue_id': 2482882874, 'author': 'tilakrayal', 'body': '@SamamaSaleem,\r\nCould you please try to use the latest tensorflow v2.17 and provide whether you are facing a similar issue in the stable version? Thank you!', 'created_at': datetime.datetime(2024, 9, 12, 12, 30, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362557757, 'issue_id': 2482882874, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 20, 1, 59, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259734, 'issue_id': 2482882874, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378259785, 'issue_id': 2482882874, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 2, 1, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551948301, 'issue_id': 2482882874, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 57, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551948356, 'issue_id': 2482882874, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 57, 16, tzinfo=datetime.timezone.utc)}]","rakshitdabral on (2024-08-25 14:07:32 UTC): @SamamaSaleem  what version of Microsoft Visual C++ Redistributable are u using on your system ?

SamamaSaleem (Issue Creator) on (2024-08-25 16:09:33 UTC): It's **Microsoft Visual C++ 2015-2022 Redistributable (x-64) - 14.40.33810**

rakshitdabral on (2024-08-25 18:59:44 UTC): @SamamaSaleem have u tried reinstalling anaconda , i had same issue once and reinstalling anaconda and packages fixed it for me  , this specific one ""ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ""

SamamaSaleem (Issue Creator) on (2024-08-26 06:29:41 UTC): Yes. I tried this numerous times. In fact, I reinstalled Windows, but the issue persists. I tried creating new env, installing Python 3.10 and Tensorflow 2.10. I also set the PATH environmental variable to the Conda's path. No avail. I am not sure is it because of some version compatibility issue or system requirement issue ( as I am installing it on CPU system)

rakshitdabral on (2024-08-26 10:46:25 UTC): @SamamaSaleem maybe  the issue is with protobuf, can you  tell me the verison of protobuf you are using ? and try to downgrade it and then run your code?

SamamaSaleem (Issue Creator) on (2024-08-26 14:00:43 UTC): @rakshitdabral I am not using protobuf.

rakshitdabral on (2024-08-26 14:21:26 UTC): @SamamaSaleem can u try installing protobuf  then? and if it doesnt work try reducing your tensorflow version , i read that reducing tensorflow version helped some people for the same issue

tilakrayal (Assginee) on (2024-08-27 06:19:13 UTC): @SamamaSaleem,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

SamamaSaleem (Issue Creator) on (2024-08-27 08:07:14 UTC): @rakshitdabral I tried installing protobuf, and downgrading tensorflow version, but it hasn't resolved the issue

SamamaSaleem (Issue Creator) on (2024-08-27 08:09:47 UTC): @tilakrayal -
- I tried Microsoft Visual C++ 2015-2022 Redistributable (x-64) - 14.40.33810
- My CPU/Python is on 64 bits
- I tried to update the PATH variable with the path of the installed libraries.

Unfortunately, the error isn't gone.

SamamaSaleem (Issue Creator) on (2024-08-29 09:48:19 UTC): Any way out please?

rakshitdabral on (2024-08-29 13:06:24 UTC): Looking into it , will surely update you if I find something

rakshitdabral on (2024-08-31 12:26:41 UTC): @SamamaSaleem which  cpu , are you running tensorflow on??

rakshitdabral on (2024-08-31 12:32:28 UTC): @SamamaSaleem can you try this once

Do the following.

1. Run the anaconda prompt as administrator.(right click-> run as administrator).
2.  pip uninstall tensorflow. 
3. Close anaconda prompt.
4 .Again run anaconda prompt as administrator.
5.type:
 conda install tensorflow
6. It will ask for some y / n
7. Type y.


Now after all done, change your python interpreter's environment to anaconda environment where you installed tensorflow. Now try importing tensorflow.

SamamaSaleem (Issue Creator) on (2024-09-02 06:42:26 UTC): @rakshitdabral CPU: Intel (R) Celeron (R) CPU B820 @ 1.70GHz

SamamaSaleem (Issue Creator) on (2024-09-02 08:55:45 UTC): Facing this error now:
(base) C:\Windows\system32>conda install tensorflow
Channels:
 - defaults
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: | warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE
failed

LibMambaUnsatisfiableError: Encountered problems while solving:
  - nothing provides bleach 1.5.0 needed by tensorboard-1.7.0-py35he025d50_1

Could not solve for environment specs
The following packages are incompatible
├─ pin-1 is installable and it requires
│  └─ python 3.11.* , which can be installed;
└─ tensorflow is not installable because there are no viable options
   ├─ tensorflow [1.10.0|1.9.0] would require
   │  └─ python 3.5.* , which conflicts with any installable versions previously reported;
   ├─ tensorflow [1.10.0|1.11.0|...|2.1.0] would require
   │  └─ python 3.6.* , which conflicts with any installable versions previously reported;
   ├─ tensorflow [1.13.1|1.14.0|...|2.9.1] would require
   │  └─ python 3.7.* , which conflicts with any installable versions previously reported;
   ├─ tensorflow [1.7.0|1.7.1|1.8.0] would require
   │  └─ tensorboard [>=1.7.0,<1.8.0 |>=1.8.0,<1.9.0 ], which requires
   │     └─ bleach 1.5.0 , which does not exist (perhaps a missing channel);
   ├─ tensorflow [2.10.0|2.8.2|2.9.1] would require
   │  └─ python 3.10.* , which conflicts with any installable versions previously reported;
   ├─ tensorflow [2.10.0|2.3.0|...|2.9.1] would require
   │  └─ python 3.8.* , which conflicts with any installable versions previously reported;
   └─ tensorflow [2.10.0|2.5.0|2.6.0|2.8.2|2.9.1] would require
      └─ python 3.9.* , which conflicts with any installable versions previously reported.

SamamaSaleem (Issue Creator) on (2024-09-02 09:10:23 UTC): tried by creating a new env with python == 3.10 using the following:
conda create --name myenv python==3.10
and installed the tensorflow==2.10
Though it has been installed, the same error of DLL initialization failed is popped again when importing

SamamaSaleem (Issue Creator) on (2024-09-02 09:39:52 UTC): after the complete process, here is the output:
![image](https://github.com/user-attachments/assets/c381ed7e-ccd6-4885-a9c8-ea16efa74ba4)

rakshitdabral on (2024-09-04 04:32:54 UTC): ok so your cpu do supports AVX

rakshitdabral on (2024-09-04 04:34:34 UTC): try doing this ,  conda config --add channels conda-forge

rakshitdabral on (2024-09-04 04:45:25 UTC): @SamamaSaleem can you tell me how you install your packages ?? are you using anaconda terminal or cmd??

tilakrayal (Assginee) on (2024-09-11 15:11:47 UTC): @belitskiy

tilakrayal (Assginee) on (2024-09-12 12:30:11 UTC): @SamamaSaleem,
Could you please try to use the latest tensorflow v2.17 and provide whether you are facing a similar issue in the stable version? Thank you!

github-actions[bot] on (2024-09-20 01:59:58 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-27 02:01:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-27 02:01:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">No</a>

mihaimaruseac on (2024-12-18 17:57:14 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:57:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74405"">No</a>

"
2482075094,issue,open,,Using --config=cuda_clang ignores compute capabilities and TensorRT values set by the configuration script,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the configuration script, using Clang for CUDA (`--config=cuda_clang`) is an option. When this is enabled, however, the compute capability and TensorRT settings (which set `TF_CUDA_COMPUTE_CAPABILITIES` and `TF_NEED_TENSORRT` respectively) are ignored, as `--config=cuda_clang` [sets these unconditionally](https://github.com/tensorflow/tensorflow/blob/v2.17.0/.bazelrc#L229-L243).

### Standalone code to reproduce the issue

```shell
$ TF_NEED_CUDA=1 TF_CUDA_CLANG=1 TF_CUDA_COMPUTE_CAPABILITIES=sm_87 TF_NEED_TENSORRT=0 ./configure

# The following does not respect the given compute capability and TensorRT options
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow

# As a workaround, the variables can be set manually in the build command
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=... --repo_env=TF_NEED_TENSORRT=...
```


### Relevant log output

_No response_",hacker1024,2024-08-23 01:42:03+00:00,['Venkat6871'],2025-01-02 10:13:14+00:00,,https://github.com/tensorflow/tensorflow/issues/74370,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2311805518, 'issue_id': 2482075094, 'author': 'Venkat6871', 'body': 'Hi **@hacker1024** ,\r\nApologies for the delay. Could you please tell what is the error you are facing exactly?\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 27, 7, 48, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316842482, 'issue_id': 2482075094, 'author': 'hacker1024', 'body': ""> Hi **@hacker1024** , Apologies for the delay. Could you please tell what is the error you are facing exactly? Thank you!\r\n\r\nWhen enabling CUDA with Clang in the configuration script, and building like so, my settings (`sm_87`, no TensorRT) are ignored:\r\n\r\n```console\r\n$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow\r\n$ python -c 'from tensorflow.python.platform import build_info; print(build_info.build_info)'\r\nOrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])\r\n```\r\n\r\nIn order to work around this, the variables need to be set again _after_ `.bazelrc` runs:\r\n\r\n```console\r\n$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_87 --repo_env=TF_NEED_TENSORRT=0\r\n$ python -c 'from tensorflow.python.platform import build_info; print(build_info.build_info)'\r\nOrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'), ('cuda_compute_capabilities', ['sm_87']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\r\n```\r\n\r\nBut this workaround depends on the implementation of the `tensorrt` Bazel configuration. `tensorrt` should never be enabled to begin with, and the selected CUDA capabilities should not be overwritten."", 'created_at': datetime.datetime(2024, 8, 29, 6, 51, 51, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-27 07:48:22 UTC): Hi **@hacker1024** ,
Apologies for the delay. Could you please tell what is the error you are facing exactly?
Thank you!

hacker1024 (Issue Creator) on (2024-08-29 06:51:51 UTC): When enabling CUDA with Clang in the configuration script, and building like so, my settings (`sm_87`, no TensorRT) are ignored:

```console
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
$ python -c 'from tensorflow.python.platform import build_info; print(build_info.build_info)'
OrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])
```

In order to work around this, the variables need to be set again _after_ `.bazelrc` runs:

```console
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_87 --repo_env=TF_NEED_TENSORRT=0
$ python -c 'from tensorflow.python.platform import build_info; print(build_info.build_info)'
OrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'), ('cuda_compute_capabilities', ['sm_87']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])
```

But this workaround depends on the implementation of the `tensorrt` Bazel configuration. `tensorrt` should never be enabled to begin with, and the selected CUDA capabilities should not be overwritten.

"
2481595583,issue,closed,completed,callbacks.ModelCheckpoint require the checkpoint file path end in  .weights.h5.  But tf.train.latest_checkpoint can't find any checkpoint,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly and tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the code runs perfectly well in tf 2.11
However, in tf2.17 and tf-nightly,  I got error:
ValueError: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=cp-{epoch}

If I save them as .weights.h5, they can't be recognized by tf.train.latest_checkpoint()

### Standalone code to reproduce the issue

```shell
import numpy as np

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model


# Define the model using the Functional API
x = tf.keras.Input(shape=(20,), dtype=tf.float32)
y = tf.keras.Input(shape=(), dtype=tf.float32)
tmp = Dense(64, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(tmp)


class DummyLossLayer(tf.keras.layers.Layer):
    def call(self, *x):
        self.add_loss(tf.keras.losses.BinaryCrossentropy(from_logits=True)(*x))
        return x

outputs, _ = DummyLossLayer()(outputs, y)
model = Model(inputs=[x, y], outputs=outputs)


# Compile the model with the custom metric
model.compile(optimizer='adam')

cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=""cp-{epoch}"",
    verbose=1,
    save_weights_only=True,
    save_freq='epoch')


# Dummy data for demonstration
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000,)).astype(np.float32)

# Train the model
model.fit([x_train, y_train], epochs=2, batch_size=32, callbacks=[cp_callback])
```


### Relevant log output

_No response_",MeowTheCat,2024-08-22 19:51:49+00:00,['tilakrayal'],2024-09-12 01:58:26+00:00,2024-09-12 01:58:24+00:00,https://github.com/tensorflow/tensorflow/issues/74342,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2306898622, 'issue_id': 2481595583, 'author': 'tilakrayal', 'body': ""@MeowTheCat,\r\nTensorflow 2.17 contains the keras3.0. In Keras 3, for checkpoint filepath, you need to provide in .keras format only, if you're saving only weights file name should end with .weights.h5.\r\n\r\nYou can save and load the model in .h5 using the method below\r\nhttps://keras.io/guides/migrating_to_keras_3/#saving-a-model-in-the-tf-savedmodel-format\r\n\r\nhttps://github.com/keras-team/keras-io/issues/1844\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 8, 23, 11, 31, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307303287, 'issue_id': 2481595583, 'author': 'MeowTheCat', 'body': ""@tilakrayal for some reason I have to save weights only and use .weights.h5. Are you suggesting tf.train.latest_checkpoint won't work with .weights.h5 ?"", 'created_at': datetime.datetime(2024, 8, 23, 15, 18, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314552182, 'issue_id': 2481595583, 'author': 'sanskarmodi8', 'body': ""@MeowTheCat,\r\n\r\nI was looking into this issue and found that the function tf.train.latest_checkpoint() is designed to work with TensorFlow's native checkpoint format. \r\n\r\nIn TF, checkpoints typically consist of multiple files, including a checkpoint file that keeps track of the latest checkpoints and .index files that describe the state of the model.\r\n\r\nInstead of using latest_checkpoint() you can follow an alternative custom approach like one given below.\r\n\r\nhttps://colab.research.google.com/drive/1wlVOo4Av-xo_AHZTK3aurKtu1f8JCi-I?usp=sharing"", 'created_at': datetime.datetime(2024, 8, 28, 7, 36, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330450462, 'issue_id': 2481595583, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 5, 1, 57, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102207, 'issue_id': 2481595583, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102269, 'issue_id': 2481595583, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74342"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74342"">No</a>', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 26, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-23 11:31:22 UTC): @MeowTheCat,
Tensorflow 2.17 contains the keras3.0. In Keras 3, for checkpoint filepath, you need to provide in .keras format only, if you're saving only weights file name should end with .weights.h5.

You can save and load the model in .h5 using the method below
https://keras.io/guides/migrating_to_keras_3/#saving-a-model-in-the-tf-savedmodel-format

https://github.com/keras-team/keras-io/issues/1844

Thank you!

MeowTheCat (Issue Creator) on (2024-08-23 15:18:21 UTC): @tilakrayal for some reason I have to save weights only and use .weights.h5. Are you suggesting tf.train.latest_checkpoint won't work with .weights.h5 ?

sanskarmodi8 on (2024-08-28 07:36:31 UTC): @MeowTheCat,

I was looking into this issue and found that the function tf.train.latest_checkpoint() is designed to work with TensorFlow's native checkpoint format. 

In TF, checkpoints typically consist of multiple files, including a checkpoint file that keeps track of the latest checkpoints and .index files that describe the state of the model.

Instead of using latest_checkpoint() you can follow an alternative custom approach like one given below.

https://colab.research.google.com/drive/1wlVOo4Av-xo_AHZTK3aurKtu1f8JCi-I?usp=sharing

github-actions[bot] on (2024-09-05 01:57:50 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-12 01:58:23 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-12 01:58:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74342"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74342"">No</a>

"
2480905472,issue,closed,completed,Compiling from source Error compiling Cython file,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

r2.10

### Custom code

No

### OS platform and distribution

Debian GNU/Linux 11 (bullseye)

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

5.1.1

### GCC/compiler version

gcc (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

NaN

### GPU model and memory

NaN

### Current behavior?

Cython version: '3.0.11'

I followed the instructions in [here](https://www.tensorflow.org/install/source). I had to do some changes, compile JAXlib, etc.
I am now stuck after running `bazel build //tensorflow/tools/pip_package:build_pip_package --repo_env=WHEEL_NAME=tensorflow_cpu`

I get:

```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/abarrachina/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/abarrachina/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/abarrachina/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/abarrachina/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/abarrachina/tensorflow/tensorflow/python/framework/BUILD:2183:12: Executing genrule //tensorflow/python/framework:fast_tensor_util.pyx_cython_translation failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
/home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/cython/Cython/Compiler/Main.py:346: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/execroot/org_tensorflow/tensorflow/python/framework/fast_tensor_util.pyx
  tree = Parsing.p_module(s, pxd, full_module_name)

Error compiling Cython file:
------------------------------------------------------------
...
        # directly accessing this field.
        cdef char byteorder
        cdef int type_num

        @property
        cdef inline npy_intp itemsize(self) noexcept nogil:
                                           ^
------------------------------------------------------------

/home/abarrachina/.local/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd:286:44: Syntax error in C variable declaration


Error compiling Cython file:
------------------------------------------------------------
...
    tensor_proto.scomplex_val.append(nparray[i].real)
    tensor_proto.scomplex_val.append(nparray[i].imag)


def AppendComplex128ArrayToTensorProto(
    tensor_proto, np.ndarray[np.complex128_t, ndim=1] nparray):
                 ^
------------------------------------------------------------

tensorflow/python/framework/fast_tensor_util.pyx:115:18: 'ndarray' is not a type identifier

Error compiling Cython file:
------------------------------------------------------------
...
  for i in range(n):
    tensor_proto.dcomplex_val.append(nparray[i].real)
    tensor_proto.dcomplex_val.append(nparray[i].imag)


def AppendObjectArrayToTensorProto(tensor_proto, np.ndarray nparray):
                                                ^
------------------------------------------------------------

tensorflow/python/framework/fast_tensor_util.pyx:123:49: 'ndarray' is not a type identifier
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 27.720s, Critical Path: 6.84s
INFO: 33 processes: 15 internal, 18 local.
FAILED: Build did NOT complete successfully
```

I tried changing cython version to '0.29.32' which is around the date of the of Tensorflow 2.10 but did not succeed.

### Standalone code to reproduce the issue

```shell
I don't know how to provide this.
```


### Relevant log output

_No response_",NEGU93,2024-08-22 14:14:09+00:00,['Venkat6871'],2024-12-07 02:06:59+00:00,2024-12-07 02:06:55+00:00,https://github.com/tensorflow/tensorflow/issues/74321,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2309401556, 'issue_id': 2480905472, 'author': 'Venkat6871', 'body': 'Hi **@NEGU93** ,\r\nApologies for the delay. The error might due to compatibility of Cython version. For tensorflow 2.10, versions around 0.29.30 to 0.29.32 are usually compatible. However, since 0.29.32 did not resolve the issue, try downgrading Cython further\r\n```\r\npip install cython==0.29.24\r\n```\r\nAnd clean the Bazel cache and rebuild\r\n```\r\nbazel clean --expunge\r\nbazel build //tensorflow/tools/pip_package:build_pip_package --repo_env=WHEEL_NAME=tensorflow_cpu\r\n```\r\nIt looks like you are using an older Version of Tensorflow (2.10). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.17) and let us know if the issue still persists? \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 26, 6, 11, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311804908, 'issue_id': 2480905472, 'author': 'NEGU93', 'body': ""So I managed to continue downgrading the numpy version to 1.23.3. I also had to downgrade `ml-dtypes==0.2.0`. \r\nI now have the issue that the next command is wrong. There is no `bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-version-tags.whl` I could get until `pip_package` and that is all. I just run `pip install` with said folder and apparently it worked. \r\n\r\nI have more issues but it's for another topic. However, it would be useful that the official build website has the instructions for other versions. Like the previous command or the fact that you should replace `//tensorflow/tools/pip_package:wheel` with `//tensorflow/tools/pip_package:build_pip_package` for building version 2.10. I really could not find the build documentation for version 2.10."", 'created_at': datetime.datetime(2024, 8, 27, 7, 48, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493022998, 'issue_id': 2480905472, 'author': 'Venkat6871', 'body': 'Hi **@NEGU93** ,\r\nApologies for the delay, and thank you for your patience. The latest version of TensorFlow now supports NumPy 2.0 by default. Could you please check the following [documentation](https://github.com/tensorflow/tensorflow/releases) and let us know if the issue still have any issues?', 'created_at': datetime.datetime(2024, 11, 22, 7, 10, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508786426, 'issue_id': 2480905472, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 30, 2, 4, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804446, 'issue_id': 2480905472, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804487, 'issue_id': 2480905472, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74321"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74321"">No</a>', 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 58, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-26 06:11:06 UTC): Hi **@NEGU93** ,
Apologies for the delay. The error might due to compatibility of Cython version. For tensorflow 2.10, versions around 0.29.30 to 0.29.32 are usually compatible. However, since 0.29.32 did not resolve the issue, try downgrading Cython further
```
pip install cython==0.29.24
```
And clean the Bazel cache and rebuild
```
bazel clean --expunge
bazel build //tensorflow/tools/pip_package:build_pip_package --repo_env=WHEEL_NAME=tensorflow_cpu
```
It looks like you are using an older Version of Tensorflow (2.10). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.17) and let us know if the issue still persists? 

Thank you!

NEGU93 (Issue Creator) on (2024-08-27 07:48:02 UTC): So I managed to continue downgrading the numpy version to 1.23.3. I also had to downgrade `ml-dtypes==0.2.0`. 
I now have the issue that the next command is wrong. There is no `bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-version-tags.whl` I could get until `pip_package` and that is all. I just run `pip install` with said folder and apparently it worked. 

I have more issues but it's for another topic. However, it would be useful that the official build website has the instructions for other versions. Like the previous command or the fact that you should replace `//tensorflow/tools/pip_package:wheel` with `//tensorflow/tools/pip_package:build_pip_package` for building version 2.10. I really could not find the build documentation for version 2.10.

Venkat6871 (Assginee) on (2024-11-22 07:10:33 UTC): Hi **@NEGU93** ,
Apologies for the delay, and thank you for your patience. The latest version of TensorFlow now supports NumPy 2.0 by default. Could you please check the following [documentation](https://github.com/tensorflow/tensorflow/releases) and let us know if the issue still have any issues?

github-actions[bot] on (2024-11-30 02:04:13 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-07 02:06:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-07 02:06:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74321"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74321"">No</a>

"
2480529623,issue,closed,completed,Results on PC and on Android are very different,"Hi everyone!
I'm using two models, for detection and then classification
Detector YOLOv8n, and it's okay with it
But classifier - EfficientNetB0 has some troubles

Here is how I'm running model on PC:
```
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""classifier_best.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_data = tens.detach().numpy()
# Test the model on random input data.
#input_shape = input_details[0]['shape']
#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(np.argmax(output_data))
```

and it's okay, but predictions on Android on the same image is too different

I'm running model on Android with cropped image from YOLO detector. Here's my cropping code:
```
val frame_x1 = it.x1*frame.width
            val frame_y1 = it.y1*frame.height
            val frame_w = it.w*frame.width
            val frame_h = it.h*frame.height
            try{
                val tmpBitmap = Bitmap.createBitmap(frame, frame_x1.toInt()-5, frame_y1.toInt()-5, frame_w.toInt()+5, frame_h.toInt()+5)
                classifier?.invoke(tmpBitmap)
                tmpBitmap.compress(Bitmap.CompressFormat.JPEG, 100, fos)
            } catch (e: Exception){
                Log.d(""183"", """"+e)
            }
```
And here is my invoke fun in classifier class:
```
fun invoke(frame: Bitmap) {
        if (tensorWidth == 0) return
        if (tensorHeight == 0) return

        var inferenceTime = SystemClock.uptimeMillis()

        val resizedBitmap = Bitmap.createScaledBitmap(frame, tensorWidth, tensorHeight, false)

        val tensorImage = TensorImage(INPUT_IMAGE_TYPE)
        tensorImage.load(resizedBitmap)
        val processedImage = imageProcessor.process(tensorImage)
        val imageBuffer = processedImage.buffer

        val output = TensorBuffer.createFixedSize(intArrayOf(1 , numClass) , OUTPUT_IMAGE_TYPE)
        interpreter.run(imageBuffer, output.buffer)

        val outputArray = output.floatArray

        val predictions = mutableListOf<Prediction>()

        outputArray.forEachIndexed { index, float ->
            if (float > CONFIDENCE_THRESHOLD) {
                predictions.add(
                    Prediction(
                        id = index,
                        name = labels[index],
                        score = float
                    )
                )
            }
        }
```
Am I doing something wrong?",NazaRik555,2024-08-22 11:19:16+00:00,['pkgoogle'],2024-12-27 02:01:27+00:00,2024-12-27 02:01:25+00:00,https://github.com/tensorflow/tensorflow/issues/74312,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2306378239, 'issue_id': 2480529623, 'author': 'tilakrayal', 'body': '@NazaRik555,\r\nCould you please provide the tensorflow version you are using to test the above mentioned code? Thank you!', 'created_at': datetime.datetime(2024, 8, 23, 6, 25, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306858325, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': '> @NazaRik555, Could you please provide the tensorflow version you are using to test the above mentioned code? Thank you!\r\n\r\nPC - 2.17.0\r\n\r\n\r\nAndroid:\r\n\r\n```\r\n    implementation(""org.tensorflow:tensorflow-lite:2.16.1"")\r\n    implementation(""org.tensorflow:tensorflow-lite-support:0.4.4"")\r\n\r\n    implementation(""org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4"")\r\n    implementation(""org.tensorflow:tensorflow-lite-gpu-api:2.16.1"")\r\n    implementation(""org.tensorflow:tensorflow-lite-api:2.16.1"")\r\n    implementation(""org.tensorflow:tensorflow-lite-gpu:2.16.1"")\r\n    implementation(""org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1"")\r\n\r\n    implementation(""org.tensorflow:tensorflow-lite-metadata:0.4.4"")\r\n```', 'created_at': datetime.datetime(2024, 8, 23, 11, 7, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309756614, 'issue_id': 2480529623, 'author': 'gaikwadrahul8', 'body': 'Hi, @NazaRik555 \r\n\r\nI apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help me with Github repo with minimal code to reproduce the same behavior from our end with complete steps and as much as details which will help us to investigate this issue further ?\r\n\r\nIf you followed any TFLite official documentation please help us with that to follow the exact steps to reproduce the same behavior from our end.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 26, 9, 23, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311617121, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': '> Hi, @NazaRik555\r\n> \r\n> I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help me with Github repo with minimal code to reproduce the same behavior from our end with complete steps and as much as details which will help us to investigate this issue further ?\r\n> \r\n> If you followed any TFLite official documentation please help us with that to follow the exact steps to reproduce the same behavior from our end.\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nYou need only android project or my pytorch and convertation codes too?', 'created_at': datetime.datetime(2024, 8, 27, 5, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312996117, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': 'Today I noticed strange thing:\nAndroid inference gives me 1.0 or 0.0 from last Softmax layout.\nMaybe it somehow connected with bad results?', 'created_at': datetime.datetime(2024, 8, 27, 16, 15, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315563942, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': ""@gaikwadrahul8 \n\nhttps://github.com/NazaRik555/CarDetectionClassification\nHere's android project\n\nAnd here's models for project\ndetector: https://drive.google.com/file/d/1I1A7gchVQnGh5-yMfkqYKVYVzUTh8SdW/view?usp=drivesdk\nclassifier: https://drive.google.com/file/d/1hy4soyldC9ZiMpcmfxYfMOLC3WS8yxSm/view?usp=drivesdk"", 'created_at': datetime.datetime(2024, 8, 28, 14, 46, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326165041, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': '@gaikwadrahul8 \nDo u have some ideas I can try?', 'created_at': datetime.datetime(2024, 9, 3, 10, 27, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328450405, 'issue_id': 2480529623, 'author': 'gaikwadrahul8', 'body': 'Hi, @NazaRik555 \r\n\r\nI apologize for the delayed response, may I know on which OS have you tried above experiment ? Thank you', 'created_at': datetime.datetime(2024, 9, 4, 10, 9, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328462571, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': '@gaikwadrahul8 \n\nWindows OS\nAnd if it can help, without Softmax activation I have about 2-5 max score on PC and about 20k max score on android', 'created_at': datetime.datetime(2024, 9, 4, 10, 13, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397035977, 'issue_id': 2480529623, 'author': 'gaikwadrahul8', 'body': ""Hi, @NazaRik555\r\n\r\nI apologize for the delayed response and It seems like your invitation request got expired I'm really sorry I missed that one if possible could you please send me once again ? Thank you."", 'created_at': datetime.datetime(2024, 10, 7, 14, 6, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412662552, 'issue_id': 2480529623, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 15, 2, 2, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430660755, 'issue_id': 2480529623, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 23, 2, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430660833, 'issue_id': 2480529623, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">No</a>', 'created_at': datetime.datetime(2024, 10, 23, 2, 1, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451560154, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': 'Hi, @gaikwadrahul8 \r\nSorry for late, could you, please, re-open this thread? And I sent invitation again', 'created_at': datetime.datetime(2024, 11, 1, 9, 10, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456965142, 'issue_id': 2480529623, 'author': 'gaikwadrahul8', 'body': ""Hi, @NazaRik555 \r\n\r\nThank you for sending me invitation again and now I'm able to access your Github repo, I've re-opened this issue and will try to replicate the similar behavior from my end.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 5, 11, 50, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497597433, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': '@gaikwadrahul8 So, any ideas?', 'created_at': datetime.datetime(2024, 11, 25, 10, 31, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498322183, 'issue_id': 2480529623, 'author': 'gaikwadrahul8', 'body': ""Hi, @NazaRik555\r\nI apologize for the delayed response, I was trying to replicate the similar behavior from my end after downloading both your models and keeping them into Assets folder and updated model name correctly but unfortunately I'm unable to replicate the issue with emulator device may be you're trying on actual android physical device \r\n\r\nHi, @pkgoogle \r\nCould you please take a look into this issue. Thank you."", 'created_at': datetime.datetime(2024, 11, 25, 15, 24, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499040068, 'issue_id': 2480529623, 'author': 'pkgoogle', 'body': 'Hi @NazaRik555, your android project link appears to be broken/404 now. If you can share your exported android project, that will help immensely. Thank you for your help.', 'created_at': datetime.datetime(2024, 11, 25, 21, 9, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499718587, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': ""@gaikwadrahul8 I used both emulator and smartphone. I've tried even to load image to assets and Log.d results"", 'created_at': datetime.datetime(2024, 11, 26, 5, 49, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499720198, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': ""@pkgoogle https://github.com/NazaRik555/CarDetectionClassification and I've sent u an invitation to contributors"", 'created_at': datetime.datetime(2024, 11, 26, 5, 51, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501579905, 'issue_id': 2480529623, 'author': 'pkgoogle', 'body': 'Hi @NazaRik555, I don\'t see you cropping the image on ""PC"", if you are please do share that code as well. If you are not, then that\'s possibly the source of the discrepancy.', 'created_at': datetime.datetime(2024, 11, 26, 17, 49, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502982018, 'issue_id': 2480529623, 'author': 'NazaRik555', 'body': 'Hi @pkgoogle, I\'ve tried to run classifier at start of the android app without detector so without cropping. I\'ve used same image, here is the link for it https://imgur.com/Dz9BZnk\r\nHere is the code of loading img from assets and getting results in CarDetector.kt during initializing\r\n```\r\nval ims = context.assets.open(""solara_cropped.jpg"")\r\nval bmp = BitmapFactory.decodeStream(ims)\r\n\r\nclassifier?.invoke(bmp)\r\n```\r\n\r\nHere is my onResult function:\r\n```\r\noverride fun onResult(data: List<Prediction>, inferenceTime: Long) {\r\n        Log.d(""clf_result"", data[0].name)\r\n        try {\r\n            clfResults.add(data[0].name+"" ""+plateResult)\r\n        } catch (e: Exception){\r\n            clfResults.add(data[0].name)\r\n        }\r\n\r\n        classifierConfs.add(data[0].score)\r\n        classifierTime += inferenceTime\r\n    }\r\n```', 'created_at': datetime.datetime(2024, 11, 27, 6, 9, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532702227, 'issue_id': 2480529623, 'author': 'pkgoogle', 'body': 'Hi @NazaRik555, I was able to get it running but I couldn\'t invoke your invoke function at least on emulator. Can you upload your modified code to your github repo (you can put it to a different branch if you prefer). Perhaps that will be easiest, also if it\'s not available anywhere, the ""solara_cropped.jpg"" file.', 'created_at': datetime.datetime(2024, 12, 10, 19, 38, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552621639, 'issue_id': 2480529623, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 19, 2, 5, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233447, 'issue_id': 2480529623, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233459, 'issue_id': 2480529623, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">No</a>', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 27, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-23 06:25:11 UTC): @NazaRik555,
Could you please provide the tensorflow version you are using to test the above mentioned code? Thank you!

NazaRik555 (Issue Creator) on (2024-08-23 11:07:09 UTC): PC - 2.17.0


Android:

```
    implementation(""org.tensorflow:tensorflow-lite:2.16.1"")
    implementation(""org.tensorflow:tensorflow-lite-support:0.4.4"")

    implementation(""org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4"")
    implementation(""org.tensorflow:tensorflow-lite-gpu-api:2.16.1"")
    implementation(""org.tensorflow:tensorflow-lite-api:2.16.1"")
    implementation(""org.tensorflow:tensorflow-lite-gpu:2.16.1"")
    implementation(""org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1"")

    implementation(""org.tensorflow:tensorflow-lite-metadata:0.4.4"")
```

gaikwadrahul8 on (2024-08-26 09:23:41 UTC): Hi, @NazaRik555 

I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help me with Github repo with minimal code to reproduce the same behavior from our end with complete steps and as much as details which will help us to investigate this issue further ?

If you followed any TFLite official documentation please help us with that to follow the exact steps to reproduce the same behavior from our end.

Thank you for your cooperation and patience.

NazaRik555 (Issue Creator) on (2024-08-27 05:43:00 UTC): You need only android project or my pytorch and convertation codes too?

NazaRik555 (Issue Creator) on (2024-08-27 16:15:58 UTC): Today I noticed strange thing:
Android inference gives me 1.0 or 0.0 from last Softmax layout.
Maybe it somehow connected with bad results?

NazaRik555 (Issue Creator) on (2024-08-28 14:46:16 UTC): @gaikwadrahul8 

https://github.com/NazaRik555/CarDetectionClassification
Here's android project

And here's models for project
detector: https://drive.google.com/file/d/1I1A7gchVQnGh5-yMfkqYKVYVzUTh8SdW/view?usp=drivesdk
classifier: https://drive.google.com/file/d/1hy4soyldC9ZiMpcmfxYfMOLC3WS8yxSm/view?usp=drivesdk

NazaRik555 (Issue Creator) on (2024-09-03 10:27:47 UTC): @gaikwadrahul8 
Do u have some ideas I can try?

gaikwadrahul8 on (2024-09-04 10:09:18 UTC): Hi, @NazaRik555 

I apologize for the delayed response, may I know on which OS have you tried above experiment ? Thank you

NazaRik555 (Issue Creator) on (2024-09-04 10:13:41 UTC): @gaikwadrahul8 

Windows OS
And if it can help, without Softmax activation I have about 2-5 max score on PC and about 20k max score on android

gaikwadrahul8 on (2024-10-07 14:06:40 UTC): Hi, @NazaRik555

I apologize for the delayed response and It seems like your invitation request got expired I'm really sorry I missed that one if possible could you please send me once again ? Thank you.

github-actions[bot] on (2024-10-15 02:02:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-23 02:01:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-23 02:01:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">No</a>

NazaRik555 (Issue Creator) on (2024-11-01 09:10:17 UTC): Hi, @gaikwadrahul8 
Sorry for late, could you, please, re-open this thread? And I sent invitation again

gaikwadrahul8 on (2024-11-05 11:50:50 UTC): Hi, @NazaRik555 

Thank you for sending me invitation again and now I'm able to access your Github repo, I've re-opened this issue and will try to replicate the similar behavior from my end.

Thank you for your cooperation and patience.

NazaRik555 (Issue Creator) on (2024-11-25 10:31:25 UTC): @gaikwadrahul8 So, any ideas?

gaikwadrahul8 on (2024-11-25 15:24:46 UTC): Hi, @NazaRik555
I apologize for the delayed response, I was trying to replicate the similar behavior from my end after downloading both your models and keeping them into Assets folder and updated model name correctly but unfortunately I'm unable to replicate the issue with emulator device may be you're trying on actual android physical device 

Hi, @pkgoogle 
Could you please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2024-11-25 21:09:36 UTC): Hi @NazaRik555, your android project link appears to be broken/404 now. If you can share your exported android project, that will help immensely. Thank you for your help.

NazaRik555 (Issue Creator) on (2024-11-26 05:49:43 UTC): @gaikwadrahul8 I used both emulator and smartphone. I've tried even to load image to assets and Log.d results

NazaRik555 (Issue Creator) on (2024-11-26 05:51:06 UTC): @pkgoogle https://github.com/NazaRik555/CarDetectionClassification and I've sent u an invitation to contributors

pkgoogle (Assginee) on (2024-11-26 17:49:11 UTC): Hi @NazaRik555, I don't see you cropping the image on ""PC"", if you are please do share that code as well. If you are not, then that's possibly the source of the discrepancy.

NazaRik555 (Issue Creator) on (2024-11-27 06:09:05 UTC): Hi @pkgoogle, I've tried to run classifier at start of the android app without detector so without cropping. I've used same image, here is the link for it https://imgur.com/Dz9BZnk
Here is the code of loading img from assets and getting results in CarDetector.kt during initializing
```
val ims = context.assets.open(""solara_cropped.jpg"")
val bmp = BitmapFactory.decodeStream(ims)

classifier?.invoke(bmp)
```

Here is my onResult function:
```
override fun onResult(data: List<Prediction>, inferenceTime: Long) {
        Log.d(""clf_result"", data[0].name)
        try {
            clfResults.add(data[0].name+"" ""+plateResult)
        } catch (e: Exception){
            clfResults.add(data[0].name)
        }

        classifierConfs.add(data[0].score)
        classifierTime += inferenceTime
    }
```

pkgoogle (Assginee) on (2024-12-10 19:38:47 UTC): Hi @NazaRik555, I was able to get it running but I couldn't invoke your invoke function at least on emulator. Can you upload your modified code to your github repo (you can put it to a different branch if you prefer). Perhaps that will be easiest, also if it's not available anywhere, the ""solara_cropped.jpg"" file.

github-actions[bot] on (2024-12-19 02:05:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-27 02:01:24 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-27 02:01:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74312"">No</a>

"
2480125150,issue,closed,completed,Check failed: old_node_input_size >= old_node_input_slots (4 vs. 3) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

run keras example code
Simple MNIST convnet
batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
coredump and shows:
F tensorflow/core/common_runtime/mkl_layout_pass.cc:2490] Check failed: old_node_input_size >= old_node_input_slots (4 vs. 3)

### Standalone code to reproduce the issue

```shell
import numpy as np
import keras
from keras import layers
# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()
batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
```


### Relevant log output

_No response_",weijianghn,2024-08-22 08:10:27+00:00,['Venkat6871'],2024-09-20 01:32:17+00:00,2024-09-10 01:58:50+00:00,https://github.com/tensorflow/tensorflow/issues/74303,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2304232382, 'issue_id': 2480125150, 'author': 'weijianghn', 'body': ""I follow the build instructions from the webpage, and use bazel build the src without intel mkl option.  \r\nbazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=dbg\r\nAfter I install the package,  I find that MKL is enabled.  that's the failure reason."", 'created_at': datetime.datetime(2024, 8, 22, 9, 43, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309343117, 'issue_id': 2480125150, 'author': 'Venkat6871', 'body': 'Hi **@weijianghn** ,\r\nApologies for the delay. I tried to run your code on Colab using TF v2.17.0 and nightly. It is working fine for me. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/97a9be9be4121a30716a7703cb874009/74303_2-17-nightly-v.ipynb) here for reference. If i am wrong please let me know.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 26, 5, 23, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464669, 'issue_id': 2480125150, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460141, 'issue_id': 2480125150, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460182, 'issue_id': 2480125150, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74303"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74303"">No</a>', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362530056, 'issue_id': 2480125150, 'author': 'parveenduhan', 'body': ""I am also facing the same issue. I have to disable TF_ENABLE_ONEDNN to get rid of this which I don't want.\r\n\r\ntensorflow/core/common_runtime/mkl_layout_pass.cc:2470] Check failed: old_node_input_size >= old_node_input_slots (6 vs. 5)"", 'created_at': datetime.datetime(2024, 9, 20, 1, 28, 51, tzinfo=datetime.timezone.utc)}]","weijianghn (Issue Creator) on (2024-08-22 09:43:21 UTC): I follow the build instructions from the webpage, and use bazel build the src without intel mkl option.  
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=dbg
After I install the package,  I find that MKL is enabled.  that's the failure reason.

Venkat6871 (Assginee) on (2024-08-26 05:23:25 UTC): Hi **@weijianghn** ,
Apologies for the delay. I tried to run your code on Colab using TF v2.17.0 and nightly. It is working fine for me. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/97a9be9be4121a30716a7703cb874009/74303_2-17-nightly-v.ipynb) here for reference. If i am wrong please let me know.
Thank you!

github-actions[bot] on (2024-09-03 01:56:44 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-10 01:58:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-10 01:58:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74303"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74303"">No</a>

parveenduhan on (2024-09-20 01:28:51 UTC): I am also facing the same issue. I have to disable TF_ENABLE_ONEDNN to get rid of this which I don't want.

tensorflow/core/common_runtime/mkl_layout_pass.cc:2470] Check failed: old_node_input_size >= old_node_input_slots (6 vs. 5)

"
2480048301,issue,closed,completed,16-bit activated integral calibration data setting issue,"Linux Ubuntu  22.04
TensorFlow installation (pip package)
TensorFlow library (2.16.2)


I am converting a causal audio processing model, using the GRU module, the model only processes one frame at a time, and I need to iterate between frames to update the GRU's state. That is, when the model processes the current frame, I also need to pass in the state of the previous frame, and return the status value of the current frame to the next frame after processing. 

Before converting tflite, I set the 16x8 integer quantization mode, but I noticed that the data used to calibrate the accuracy was set directly to ""converter.representative_dataset = representative_data_gen"". How can I iterate on the state mentioned above? 


",YadongChen-1016,2024-08-22 07:30:15+00:00,['gaikwadrahul8'],2024-09-10 01:58:56+00:00,2024-09-10 01:58:52+00:00,https://github.com/tensorflow/tensorflow/issues/74300,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.16', '')]","[{'comment_id': 2310552682, 'issue_id': 2480048301, 'author': 'gaikwadrahul8', 'body': 'Hi, @YadongChen-1016 \r\n\r\nI apologize for the delayed response and If possible could you please help us with Google colab notebook with your model along with complete steps to reproduce the same behavior from our end to investigate this issue further ?\r\n\r\nIf you encountered any `RuntimeError` please mention that also which will help us to investigate this issue further.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 26, 16, 2, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464692, 'issue_id': 2480048301, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460165, 'issue_id': 2480048301, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460252, 'issue_id': 2480048301, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74300"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74300"">No</a>', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 55, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-08-26 16:02:44 UTC): Hi, @YadongChen-1016 

I apologize for the delayed response and If possible could you please help us with Google colab notebook with your model along with complete steps to reproduce the same behavior from our end to investigate this issue further ?

If you encountered any `RuntimeError` please mention that also which will help us to investigate this issue further.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-03 01:56:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-10 01:58:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-10 01:58:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74300"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74300"">No</a>

"
2479976707,issue,open,,`tf.keras.Sequential` not a `tf.Module` since 2.16?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

google colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed that my custom module `.variables` attribute doesn't work properly in the newest versions of TF (since 2.16). That is, it doesn't contain the parameters of a `tf.keras.Sequential` model that is assigned to an attribute of my model in `__init__` (see MRE below).

Is this an intended change? What's the proper way to use a `tf.keras.Model` inside a `tf.Module` now?

### Standalone code to reproduce the issue

```python
import tensorflow as tf

print(f""{tf.__version__=}"")

class X(tf.Module):
    def __init__(self):
        super().__init__()
        self.xxx = tf.keras.Sequential([tf.keras.layers.Dense(4)])
    def __call__(self, X):
        tf.print(self, ""called"")
        return self.xxx(X)

x=X()
x(tf.random.normal(shape=(2, 3)))
print(f""{x.variables=}"")
print(f""{list(x.submodules)=}"")
print(f""{isinstance(x.xxx, tf.Module)=}"")
```

Output in tf 2.15.1:
```
tf.__version__='2.15.1'
<__main__.X object at 0x7cde2c935db0> called
x.variables=(<tf.Variable 'dense/kernel:0' shape=(3, 4) dtype=float32, numpy=
array([[-0.28077292,  0.2326845 ,  0.7107136 , -0.17309707],
       [ 0.5492227 , -0.37808335,  0.27558255, -0.5152782 ],
       [ 0.69788885, -0.5949336 ,  0.63280034,  0.08288908]],
      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)
list(x.submodules)=[<keras.src.engine.sequential.Sequential object at 0x7cdd9f3fb460>, <keras.src.engine.input_layer.InputLayer object at 0x7cdd9eac41f0>, <keras.src.layers.core.dense.Dense object at 0x7cdda7323910>]
isinstance(x.xxx, tf.Module)=True
```

Output in tf 2.16.1:
```
tf.__version__='2.16.1'
<__main__.X object at 0x7e7297de1f00> called
x.variables=()
list(x.submodules)=[]
isinstance(x.xxx, tf.Module)=False
```

Output in tf 2.17.0:
```
tf.__version__='2.17.0'
<__main__.X object at 0x783c2818d690> called
x.variables=()
list(x.submodules)=[]
isinstance(x.xxx, tf.Module)=False
```



### Relevant log output

_No response_",SiLiKhon,2024-08-22 06:48:20+00:00,['Venkat6871'],2025-01-02 10:07:30+00:00,,https://github.com/tensorflow/tensorflow/issues/74297,"[('type:support', 'Support issues'), ('comp:keras', 'Keras related issues'), ('TF 2.16', '')]","[{'comment_id': 2309312381, 'issue_id': 2479976707, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15, 2.17 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/474cedb612ddf641569ed46e61196dc7/74297_2-15-2-17-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 26, 4, 52, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560648269, 'issue_id': 2479976707, 'author': 'Venkat6871', 'body': 'Hi **@SiLiKhon** ,\r\nApologies for the delay, and thank you for your patience. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, `from tensorflow import keras` ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) defaults to Keras 3. To use Keras 2, you need to install it using the following command:\r\n```\r\n!pip install tf-keras\r\n```\r\nThen, import it as follows:\r\n```\r\nimport tf_keras as keras\r\n```\r\nI tried this, and it is working fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5e5221539054caf3e07ea61811cbe852/74297_tf_2-17-0-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 24, 4, 56, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560663164, 'issue_id': 2479976707, 'author': 'SiLiKhon', 'body': ""Thank you very much @Venkat6871 for the update!\r\nI confirm that using `tf_keras` solves the issue.\r\n\r\nIt's a pity that many features lose backward compatibility with TensorFlow updates. TF has been my tool of choice even before v2, but I'm shifting towards other frameworks because of this."", 'created_at': datetime.datetime(2024, 12, 24, 5, 14, 31, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-26 04:52:53 UTC): I tried to run your code on Colab using TF v2.15, 2.17 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/474cedb612ddf641569ed46e61196dc7/74297_2-15-2-17-0-nightly-v.ipynb) here for reference.
Thank you!

Venkat6871 (Assginee) on (2024-12-24 04:56:17 UTC): Hi **@SiLiKhon** ,
Apologies for the delay, and thank you for your patience. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, `from tensorflow import keras` ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) defaults to Keras 3. To use Keras 2, you need to install it using the following command:
```
!pip install tf-keras
```
Then, import it as follows:
```
import tf_keras as keras
```
I tried this, and it is working fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5e5221539054caf3e07ea61811cbe852/74297_tf_2-17-0-v.ipynb) here for your reference.
Thank you!

SiLiKhon (Issue Creator) on (2024-12-24 05:14:31 UTC): Thank you very much @Venkat6871 for the update!
I confirm that using `tf_keras` solves the issue.

It's a pity that many features lose backward compatibility with TensorFlow updates. TF has been my tool of choice even before v2, but I'm shifting towards other frameworks because of this.

"
2479686779,issue,closed,completed,no match for 'operator<<' (operand types are 'std::ostream' {aka 'std::basic_ostream<char>'} and 'const std::unique_ptr<tensorflow::data::TaskIterator>'),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.12.0

### Custom code

No

### OS platform and distribution

Linux Fedora 38

### Mobile device

_No response_

### Python version

3.11

### Bazel version

5.3.0

### GCC/compiler version

gcc version 13.3.1 20240614 (Arm GNU Toolchain 13.3.Rel1 (Build arm-13.24))

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I cross compile (target is aarch64) build tensorflow from source, I config my own cross compile tool downloaded from ARM website gcc version 13.3.1 20240614 (Arm GNU Toolchain 13.3.Rel1 (Build arm-13.24)). when I build I got these errors like log.
[err4tensorflow.log](https://github.com/user-attachments/files/16703640/err4tensorflow.log)



### Standalone code to reproduce the issue

```shell
when I use the cross compile tool to build from source, I get these errors like in logs.
```


### Relevant log output

_No response_",iysheng,2024-08-22 03:30:16+00:00,['tilakrayal'],2024-09-24 00:40:04+00:00,2024-09-24 00:40:01+00:00,https://github.com/tensorflow/tensorflow/issues/74289,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2303900058, 'issue_id': 2479686779, 'author': 'iysheng', 'body': 'based on the comment in file ``tensorflow/tsl/platform/default/logging.h`` as below. Now I can build tensorflow.sop success.\r\n![image](https://github.com/user-attachments/assets/7a591633-d9c2-4e89-8122-bcb9d465cb5e)', 'created_at': datetime.datetime(2024, 8, 22, 6, 41, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306906669, 'issue_id': 2479686779, 'author': 'tilakrayal', 'body': '@iysheng,\r\nTensorflow v2.12 is an older version, which we have limited support for. Could you please try to check with the latest tensorflow v2.17 & provide the update and also please provide the steps followed. Thank you!', 'created_at': datetime.datetime(2024, 8, 23, 11, 36, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307983469, 'issue_id': 2479686779, 'author': 'iysheng', 'body': '> @iysheng, Tensorflow v2.12 is an older version, which we have limited support for. Could you please try to check with the latest tensorflow v2.17 & provide the update and also please provide the steps followed. Thank you!\r\n\r\nI found this pic as:\r\n![image](https://github.com/user-attachments/assets/34d04289-1c53-4fca-b6d4-6b0a55600ec7)\r\n tensorflow v2.12 is the final version test with gcc compiler. can the new  tensorflow verison like v2.17 builed with gcc tool?', 'created_at': datetime.datetime(2024, 8, 24, 1, 45, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368132500, 'issue_id': 2479686779, 'author': 'tilakrayal', 'body': '@iysheng,\r\nApologies for the delay. Yeah from the tensorflow v2.13, the compiler has been moved to GCC to CLang as mentioned in the official document.\r\nhttps://www.tensorflow.org/install/source#cpu', 'created_at': datetime.datetime(2024, 9, 23, 12, 51, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369868935, 'issue_id': 2479686779, 'author': 'iysheng', 'body': '> @iysheng, Apologies for the delay. Yeah from the tensorflow v2.13, the compiler has been moved to GCC to CLang as mentioned in the official document. https://www.tensorflow.org/install/source#cpu\r\n\r\nThis issue can be closed as I use CHECK(val1 != val2) instead.', 'created_at': datetime.datetime(2024, 9, 24, 0, 40, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369868972, 'issue_id': 2479686779, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74289"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74289"">No</a>', 'created_at': datetime.datetime(2024, 9, 24, 0, 40, 3, tzinfo=datetime.timezone.utc)}]","iysheng (Issue Creator) on (2024-08-22 06:41:11 UTC): based on the comment in file ``tensorflow/tsl/platform/default/logging.h`` as below. Now I can build tensorflow.sop success.
![image](https://github.com/user-attachments/assets/7a591633-d9c2-4e89-8122-bcb9d465cb5e)

tilakrayal (Assginee) on (2024-08-23 11:36:16 UTC): @iysheng,
Tensorflow v2.12 is an older version, which we have limited support for. Could you please try to check with the latest tensorflow v2.17 & provide the update and also please provide the steps followed. Thank you!

iysheng (Issue Creator) on (2024-08-24 01:45:16 UTC): I found this pic as:
![image](https://github.com/user-attachments/assets/34d04289-1c53-4fca-b6d4-6b0a55600ec7)
 tensorflow v2.12 is the final version test with gcc compiler. can the new  tensorflow verison like v2.17 builed with gcc tool?

tilakrayal (Assginee) on (2024-09-23 12:51:47 UTC): @iysheng,
Apologies for the delay. Yeah from the tensorflow v2.13, the compiler has been moved to GCC to CLang as mentioned in the official document.
https://www.tensorflow.org/install/source#cpu

iysheng (Issue Creator) on (2024-09-24 00:40:01 UTC): This issue can be closed as I use CHECK(val1 != val2) instead.

google-ml-butler[bot] on (2024-09-24 00:40:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74289"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74289"">No</a>

"
2479685897,issue,open,,Proposal for Enhancing Neural Network Training: Unified Gradient Direction for Faster Convergence and Improved Accuracy,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

ALL

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the current implementation of neural network training within TensorFlow, gradient descent typically relies on the average of gradients during the optimization process. While this approach is effective, there may be potential to enhance training efficiency and accuracy by exploring alternative methods for determining the direction of gradient updates.

I have developed a method that identifies a unified direction rather than merely averaging the gradients, and preliminary experiments suggest that this approach leads to faster convergence and improved model accuracy. I believe this could be a valuable addition to TensorFlow's optimization techniques.

I would appreciate the TensorFlow Development Team's feedback on this idea and am happy to provide further details or collaborate on its implementation.

### Standalone code to reproduce the issue

```shell
https://github.com/TingjiaInFuture/UnionGradientDescent
```


### Relevant log output

_No response_",TingjiaInFuture,2024-08-22 03:29:05+00:00,['Venkat6871'],2025-01-10 05:26:00+00:00,,https://github.com/tensorflow/tensorflow/issues/74287,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:performance', 'Performance Issue')]","[{'comment_id': 2306400526, 'issue_id': 2479685897, 'author': 'Venkat6871', 'body': 'Hi @TingjiaInFuture ,\r\nCould you please provide specific use case where we need to implement this particular feature which helps the issue more effectively.\r\n\r\nThank You!', 'created_at': datetime.datetime(2024, 8, 23, 6, 43, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309203707, 'issue_id': 2479685897, 'author': 'TingjiaInFuture', 'body': 'I have already shown one in [https://github.com/TingjiaInFuture/UnionGradientDescent](url). Any question?', 'created_at': datetime.datetime(2024, 8, 26, 2, 57, 23, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-23 06:43:32 UTC): Hi @TingjiaInFuture ,
Could you please provide specific use case where we need to implement this particular feature which helps the issue more effectively.

Thank You!

TingjiaInFuture (Issue Creator) on (2024-08-26 02:57:23 UTC): I have already shown one in [https://github.com/TingjiaInFuture/UnionGradientDescent](url). Any question?

"
2479625120,issue,closed,completed,Issues when converting tensorflow built-in keras-based models to tensorflow lite,"Hello Tensorflow Support
I am contacting you because I am getting the following error while converting a model created with Keras embedded in Tensorflow to Tensorflow Lite.

The model I created used the basic structure of Bidirectional LSTM with dropout, regularization, and dense layers.

I've attached the code to convert my model to Tensorflow Lite and the message when the error occurred.

I hope this will solve your problem.

Best regards,
Yonghyun Lee

[Code to convert my model to Tensorflow Lite].

import tensorflow as tf

model = tf.keras.models.load_model(“./input_h5/input.h5”, compile=False, safe_mode=False)

tf.saved_model.save(model, './input_h5/saved_model_dir')
converter = tf.lite.TFLiteConverter.from_saved_model(“./input_h5/saved_model_dir”)
tflmodel = converter.convert()
with open(“./output_tflite/output.tflite”, “wb”) as file:
    file.write(tflmodel)

[Error Message]
2024-08-22 11:09:03.490341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1724292544.933616 42872 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.
W0000 00:00:1724292544.934307 42872 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.
2024-08-22 11:09:04.937226: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./input_h5/saved_model_dir
2024-08-22 11:09:04.941589: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-22 11:09:04.941738: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: ./input_h5/saved_model_dir
2024-08-22 11:09:04.961054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-08-22 11:09:04.964133: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-22 11:09:05.075893: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: ./input_h5/saved_model_dir
2024-08-22 11:09:05.112269: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 174732 microseconds.
2024-08-22 11:09:05.185339: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[“ReadVariableOp:”, “sequential_11_1/bidirectional_22_1/backward_lstm_22_1/while/lstm_cell_1/add_1/ReadVariableOp@sequential_11_1_bidirectional_22_1_backward_lstm_22_1_while_body_613”]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).",yonghyunee,2024-08-22 02:17:02+00:00,['gaikwadrahul8'],2024-09-18 01:58:51+00:00,2024-09-18 01:58:45+00:00,https://github.com/tensorflow/tensorflow/issues/74280,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2304436091, 'issue_id': 2479625120, 'author': 'tilakrayal', 'body': '@yonghyunee ,\r\nCould you please provide the tensorflow version you are using. This issue was resolved in latest tensorflow v2.17 and tf-nightly. Please try with those versions and let me know if you are facing the same issue. Thank you!', 'created_at': datetime.datetime(2024, 8, 22, 11, 28, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309119596, 'issue_id': 2479625120, 'author': 'yonghyunee', 'body': 'Hello\r\nI succeeded in converting it to keras h5 -> tensorflow lite using the method you told me above. In the process of applying the model, the following error appears, so can you tell me the solution to this error?\r\n\r\n[Error]\r\nException in thread Thread-2 (receive_message):\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File ""/usr/lib/python3.10/threading.py"", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""/home/root/gait_phase_estimation/main.py"", line 27, in receive_message\r\n    interpreter.allocate_tensors()\r\n  File ""/usr/lib/python3.10/site-packages/tflite_runtime/interpreter.py"", line 531, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 15 (FlexTensorListReserve) failed to prepare.Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 15 (FlexTensorListReserve) failed to prepare.\r\n\r\n[Model Convert Code]\r\nimport tensorflow as tf\r\n\r\n\r\nmodel = tf.keras.models.load_model(""./input_h5/input.h5"", compile=False, safe_mode=False)\r\n\r\ntf.saved_model.save(model, \'./input_h5/saved_model_dir\')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(""./input_h5/saved_model_dir"")\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter._experimental_lower_tensor_list_ops = False\r\nconverter.experimental_enable_resource_variables = True\r\ntflmodel = converter.convert()\r\nwith open(""./output_tflite/output.tflite"", ""wb"") as file:\r\n    file.write(tflmodel)\r\n    \r\n[Versions]\r\nWhere the tensorflow-lite model runs :\r\npython 3.10.14\r\ntflite-runtime 2.14.0\r\n\r\nWhere the tensorflow-lite model converter runs :\r\npython 3.11.9\r\ntf_keras                     2.16.0\r\ntf-nightly                   2.18.0.dev20240822\r\ntf_nightly_intel             2.18.0.dev20240822\r\ntensorflow                   2.17.0\r\ntensorflow-intel             2.17.0\r\n\r\nThank you\r\n\r\nBest regards,\r\nYonghyun Lee', 'created_at': datetime.datetime(2024, 8, 26, 1, 21, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326214926, 'issue_id': 2479625120, 'author': 'gaikwadrahul8', 'body': 'Hi, @yonghyunee \r\n\r\nI apologize for the delayed in my response, if possible could you please help me with your Google colab notebook where you created your model with bidirectional LSTM with dropout, regularization, and dense layers which will help us to investigate this issue further from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 3, 10, 50, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342462980, 'issue_id': 2479625120, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 58, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336412, 'issue_id': 2479625120, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336563, 'issue_id': 2479625120, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74280"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74280"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 50, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-22 11:28:49 UTC): @yonghyunee ,
Could you please provide the tensorflow version you are using. This issue was resolved in latest tensorflow v2.17 and tf-nightly. Please try with those versions and let me know if you are facing the same issue. Thank you!

yonghyunee (Issue Creator) on (2024-08-26 01:21:16 UTC): Hello
I succeeded in converting it to keras h5 -> tensorflow lite using the method you told me above. In the process of applying the model, the following error appears, so can you tell me the solution to this error?

[Error]
Exception in thread Thread-2 (receive_message):
Traceback (most recent call last):
  File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.10/threading.py"", line 953, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/root/gait_phase_estimation/main.py"", line 27, in receive_message
    interpreter.allocate_tensors()
  File ""/usr/lib/python3.10/site-packages/tflite_runtime/interpreter.py"", line 531, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 15 (FlexTensorListReserve) failed to prepare.Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 15 (FlexTensorListReserve) failed to prepare.

[Model Convert Code]
import tensorflow as tf


model = tf.keras.models.load_model(""./input_h5/input.h5"", compile=False, safe_mode=False)

tf.saved_model.save(model, './input_h5/saved_model_dir')

converter = tf.lite.TFLiteConverter.from_saved_model(""./input_h5/saved_model_dir"")
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter._experimental_lower_tensor_list_ops = False
converter.experimental_enable_resource_variables = True
tflmodel = converter.convert()
with open(""./output_tflite/output.tflite"", ""wb"") as file:
    file.write(tflmodel)
    
[Versions]
Where the tensorflow-lite model runs :
python 3.10.14
tflite-runtime 2.14.0

Where the tensorflow-lite model converter runs :
python 3.11.9
tf_keras                     2.16.0
tf-nightly                   2.18.0.dev20240822
tf_nightly_intel             2.18.0.dev20240822
tensorflow                   2.17.0
tensorflow-intel             2.17.0

Thank you

Best regards,
Yonghyun Lee

gaikwadrahul8 (Assginee) on (2024-09-03 10:50:25 UTC): Hi, @yonghyunee 

I apologize for the delayed in my response, if possible could you please help me with your Google colab notebook where you created your model with bidirectional LSTM with dropout, regularization, and dense layers which will help us to investigate this issue further from our end ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-11 01:58:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:50 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74280"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74280"">No</a>

"
2478499606,issue,closed,completed,How can I compile org.tensorflow:tensorflow-lite-gpu-delegate-plugin for android?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No


### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24

### Mobile device

Linux Ubuntu 24

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

11.4.0


### Current behavior?

How can we recreate the aar file for the package `org.tensorflow:tensorflow-lite-gpu-delegate-plugin`?

### Standalone code to reproduce the issue

I followed the instructions [here](https://www.tensorflow.org/lite/android/lite_build#build_tensorflow_lite_locally) to create the docker build environment in Linux Ubuntu 24, and I couldn't find the command to build this package.

I tried the following command:

```
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate


INFO: Reading 'startup' options from /host_dir/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=181
INFO: Reading rc options for 'build' from /host_dir/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1723940018419409631 --host_action_env=DOCKER_HOST_CACHEBUSTER=1723940018483040100
INFO: Reading rc options for 'build' from /host_dir/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /host_dir/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-18/bin/clang --repo_env=CC=/usr/lib/llvm-18/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-18/bin/clang
INFO: Found applicable config definition build:short_logs in file /host_dir/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /host_dir/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /host_dir/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /host_dir/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /host_dir/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Build options --cxxopt and --define have changed, discarding analysis cache.
WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 25. The major revisions supported by Bazel are [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. Bazel will attempt to treat the NDK as if it was r21. This may cause compilation and linkage problems. Please download a supported NDK version.
INFO: Analyzed target //tensorflow/lite/delegates/gpu:delegate (0 packages loaded, 9507 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/faa585346465e7978a465c0d1bdb5e9e/external/cpuinfo/BUILD.bazel:114:11: Compiling src/linux/multiline.c failed: (Exit 1): clang failed: error executing command (from target @cpuinfo//:cpuinfo_impl) external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 56 arguments skipped)
clang: error: unknown argument: '-gcc-toolchain'
clang: error: no such file or directory: 'external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64'
ERROR: /root/.cache/bazel/_bazel_root/faa585346465e7978a465c0d1bdb5e9e/external/cpuinfo/BUILD.bazel:114:11: Compiling src/linux/cpulist.c failed: (Exit 1): clang failed: error executing command (from target @cpuinfo//:cpuinfo_impl) external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 56 arguments skipped)
clang: error: unknown argument: '-gcc-toolchain'
clang: error: no such file or directory: 'external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64'
ERROR: /root/.cache/bazel/_bazel_root/faa585346465e7978a465c0d1bdb5e9e/external/cpuinfo/BUILD.bazel:114:11: Compiling src/arm/cache.c failed: (Exit 1): clang failed: error executing command (from target @cpuinfo//:cpuinfo_impl) external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 56 arguments skipped)
clang: error: unknown argument: '-gcc-toolchain'
clang: error: no such file or directory: 'external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64'
Target //tensorflow/lite/delegates/gpu:delegate failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.786s, Critical Path: 0.07s
INFO: 9 processes: 9 internal.
FAILED: Build did NOT complete successfully
```",H4dr1en,2024-08-21 16:42:34+00:00,['gaikwadrahul8'],2024-09-18 01:58:54+00:00,2024-09-18 01:58:46+00:00,https://github.com/tensorflow/tensorflow/issues/74234,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2310773420, 'issue_id': 2478499606, 'author': 'gaikwadrahul8', 'body': ""Hi, @H4dr1en \r\n\r\nI apologize for the delayed response and by looking at this `WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 25. The major revisions supported by Bazel are [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. Bazel will attempt to treat the NDK as if it was r21. This may cause compilation and linkage problems. Please download a supported NDK version.` it seems like there is some version compatibility issue between NDK and Bazel versions.\r\n\r\nThe `Android NDK` is required to build the native (C/C++) TensorFlow Lite code. The current recommended version is `25b`, which may be found [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-25b-downloads).\r\n\r\nCould you please give it try with NDK version `25b` and see is it resolving your issue ? \r\n\r\nTo confirm, did you follow the same steps as I mentioned below or different ? if you followed the different steps please help me with exact steps which you followed in the docker container which will help me to reproduce the same behavior from my end to investigate this issue further from our end ?\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit switch r2.16\r\ngit pull\r\n./configure #(configuring the TF source (./configure in the root of Github directory)\r\nbazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n```\r\n\r\nIf I have missed something here please let me know ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 8, 26, 18, 6, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324187541, 'issue_id': 2478499606, 'author': 'H4dr1en', 'body': 'Hi @gaikwadrahul8 ,\r\n\r\nThanks for your answer! Unless I am mistaken the steps you shared allow to build `libtensorflowlite_gpu_delegate.so`, not the AAR package `org.tensorflow:tensorflow-lite-gpu-delegate-plugin`, or am I missing something?', 'created_at': datetime.datetime(2024, 9, 2, 8, 54, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326905369, 'issue_id': 2478499606, 'author': 'gaikwadrahul8', 'body': ""Hi, @H4dr1en \r\n\r\nI believe you're following these [official documentation](https://www.tensorflow.org/lite/android/lite_build#set_up_build_environment_without_docker), [Ref-2](https://www.tensorflow.org/lite/android/delegates/gpu#standalone) and Once Bazel is properly configured, you can build the TensorFlow Lite AAR from the root checkout directory as follows:\r\n\r\n```\r\nbazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```\r\nI see you want to do it for `tensorflow-lite-gpu-delegate-plugin` so could you please try with below command and see is it working as expected or not ?\r\n\r\n```\r\nbazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  //tensorflow/lite/java:tensorflow-lite-gpu-delegate-plugin\r\n```\r\nYou can build smaller AAR files targeting only a set of models as follows:\r\n\r\n```\r\nbash tensorflow/lite/tools/build_aar.sh \\\r\n  --input_models=model1,model2 \\\r\n  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a\r\n```\r\n\r\nIf issue still persists please let us know with error log for further investigation ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 3, 16, 8, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342463040, 'issue_id': 2478499606, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 58, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336450, 'issue_id': 2478499606, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336642, 'issue_id': 2478499606, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74234"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74234"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 53, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-08-26 18:06:15 UTC): Hi, @H4dr1en 

I apologize for the delayed response and by looking at this `WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 25. The major revisions supported by Bazel are [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. Bazel will attempt to treat the NDK as if it was r21. This may cause compilation and linkage problems. Please download a supported NDK version.` it seems like there is some version compatibility issue between NDK and Bazel versions.

The `Android NDK` is required to build the native (C/C++) TensorFlow Lite code. The current recommended version is `25b`, which may be found [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-25b-downloads).

Could you please give it try with NDK version `25b` and see is it resolving your issue ? 

To confirm, did you follow the same steps as I mentioned below or different ? if you followed the different steps please help me with exact steps which you followed in the docker container which will help me to reproduce the same behavior from my end to investigate this issue further from our end ?

```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git switch r2.16
git pull
./configure #(configuring the TF source (./configure in the root of Github directory)
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so
```

If I have missed something here please let me know ? 

Thank you for your cooperation and patience.

H4dr1en (Issue Creator) on (2024-09-02 08:54:27 UTC): Hi @gaikwadrahul8 ,

Thanks for your answer! Unless I am mistaken the steps you shared allow to build `libtensorflowlite_gpu_delegate.so`, not the AAR package `org.tensorflow:tensorflow-lite-gpu-delegate-plugin`, or am I missing something?

gaikwadrahul8 (Assginee) on (2024-09-03 16:08:14 UTC): Hi, @H4dr1en 

I believe you're following these [official documentation](https://www.tensorflow.org/lite/android/lite_build#set_up_build_environment_without_docker), [Ref-2](https://www.tensorflow.org/lite/android/delegates/gpu#standalone) and Once Bazel is properly configured, you can build the TensorFlow Lite AAR from the root checkout directory as follows:

```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```
I see you want to do it for `tensorflow-lite-gpu-delegate-plugin` so could you please try with below command and see is it working as expected or not ?

```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite-gpu-delegate-plugin
```
You can build smaller AAR files targeting only a set of models as follows:

```
bash tensorflow/lite/tools/build_aar.sh \
  --input_models=model1,model2 \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a
```

If issue still persists please let us know with error log for further investigation ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-11 01:58:04 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74234"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74234"">No</a>

"
2477724687,issue,open,,Einsum optimization setting guidelines,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Existing tensorflow supports [einops](https://www.tensorflow.org/api_docs/python/tf/einsum), which enable seamless tensor reduction ops. Parameter to this functionality (kwarg) is the `optimize` parameter, i.e.
```
optimize: Optimization strategy to use to find contraction path using opt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or 'auto'. (optional, default: 'greedy').
```

Is it documented somewhere what the use cases that benefit from these options are in practice?  Further, are effects of these expected to this library perhaps? https://optimized-einsum.readthedocs.io/en/stable/

Simple benchmarking doesn't really reveal any (significant?) differences. Thanks!

### Standalone code to reproduce the issue

```python
A simple benchmark of form:

import tensorflow as tf
import numpy as np
import time

# Define three large matrices A, B, and C of size 1000x1000
A = tf.random.uniform((1000, 1000), dtype=tf.float32)
B = tf.random.uniform((1000, 1000), dtype=tf.float32)
C = tf.random.uniform((1000, 1000), dtype=tf.float32)
D = tf.random.uniform((1000, 1000), dtype=tf.float32)
E = tf.random.uniform((1000, 1000), dtype=tf.float32)

greedy = []
non_greedy = []
for _ in range(1000):
    # Perform matrix multiplication using tf.einsum with optimize='greedy'
    start_time_greedy = time.time()
    result_greedy = tf.einsum('ij,jk,kl,lm,mn->in', A, B, C, D, E, optimize='greedy')
    end_time_greedy = time.time()

    # Perform matrix multiplication using tf.einsum with optimize='optimal'
    start_time_branch_all = time.time()
    result_branch_all = tf.einsum('ij,jk,kl,lm,mn->in', A, B, C, D, E, optimize='optimal')
    end_time_branch_all = time.time()

    # Print the execution times
    greedy.append(end_time_greedy - start_time_greedy)
    non_greedy.append(end_time_branch_all - start_time_branch_all)

print(f""Greedy time: {np.mean(greedy)}"")
print(f""Optimal time: {np.mean(non_greedy)}"")
```

### Relevant log output

```
Greedy time: 0.067701251745224
Optimal time: 0.06806995582580566
```",SkBlaz,2024-08-21 10:53:25+00:00,['tilakrayal'],2024-08-28 08:48:19+00:00,,https://github.com/tensorflow/tensorflow/issues/74216,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:ops', 'OPs related issues'), ('type:performance', 'Performance Issue'), ('TF 2.16', '')]","[{'comment_id': 2304450521, 'issue_id': 2477724687, 'author': 'tilakrayal', 'body': '@SkBlaz,\r\nI tried to execute the mentioned code and observed that the Greedy and optimal time taken is almost similar. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d0810e82a4e5b7007038f42244769316/untitled.ipynb) and let me know if the understanding the right in this case. \r\n\r\n```python\r\nGreedy time: 0.002096245050430298\r\nOptimal time: 0.0019992468357086183\r\n\r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 22, 11, 36, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304725763, 'issue_id': 2477724687, 'author': 'SkBlaz', 'body': ""@tilakrayal thanks! Yes, you replicated the case. I'm asking, basically, when do these params actually yield something different in practice? I.e. how does one know when to use what (if at all possible to know his upfront)?"", 'created_at': datetime.datetime(2024, 8, 22, 13, 49, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-22 11:36:45 UTC): @SkBlaz,
I tried to execute the mentioned code and observed that the Greedy and optimal time taken is almost similar. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d0810e82a4e5b7007038f42244769316/untitled.ipynb) and let me know if the understanding the right in this case. 

```python
Greedy time: 0.002096245050430298
Optimal time: 0.0019992468357086183

```
Thank you!

SkBlaz (Issue Creator) on (2024-08-22 13:49:47 UTC): @tilakrayal thanks! Yes, you replicated the case. I'm asking, basically, when do these params actually yield something different in practice? I.e. how does one know when to use what (if at all possible to know his upfront)?

"
2477688670,issue,closed,completed,Is there a way to build tensorflow lite in a windows x86 environment?,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf.17

### Custom code

No

### OS platform and distribution

window 10 x86

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a general question.

### Standalone code to reproduce the issue

```shell
There is no code.
```


### Relevant log output

```shell
none.
```
",demian-j-lim,2024-08-21 10:35:34+00:00,['Venkat6871'],2024-09-06 01:57:52+00:00,2024-09-06 01:57:49+00:00,https://github.com/tensorflow/tensorflow/issues/74215,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:windows', 'Windows Build/Installation Issues')]","[{'comment_id': 2304151298, 'issue_id': 2477688670, 'author': 'Venkat6871', 'body': 'Hi **@demian-j-lim** ,\r\nBuilding TensorFlow Lite in a Windows x86 environment is certainly possible. For detailed instructions and troubleshooting tips, please refer to the official TensorFlow [documentation](https://www.tensorflow.org/lite/guide/build_cmake#windows).\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 22, 9, 3, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2319671395, 'issue_id': 2477688670, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 30, 1, 57, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039474, 'issue_id': 2477688670, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039518, 'issue_id': 2477688670, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74215"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74215"">No</a>', 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 51, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-22 09:03:35 UTC): Hi **@demian-j-lim** ,
Building TensorFlow Lite in a Windows x86 environment is certainly possible. For detailed instructions and troubleshooting tips, please refer to the official TensorFlow [documentation](https://www.tensorflow.org/lite/guide/build_cmake#windows).
Thank you!

github-actions[bot] on (2024-08-30 01:57:19 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-06 01:57:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-06 01:57:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74215"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74215"">No</a>

"
2476766735,issue,closed,completed,EffienceNet .keras model with base model trainable has poor performance in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Debian 

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am training an EfficientNet model with a custom head using TensorFlow and Keras, saving the model to a `.keras` format. If the base model `trainable` flag is set to False, such that I only train the head, then when I later load the `.keras` model and evaluate it on a dataset, I get the expected good performance. When I set the trainable flag to True and train a model (which converges well), then when I later load the model and evaluate it on the same dataset the performance has degraded significantly. (I am evaluating the model on the same dataset using the code both at the end of training, and later on in a separate notebook. It is in this separate notebook where the performance is bad, where again the same dataset is being used and the same code is being used in both evaluation places.)

Saving to a `.h5` model does not have this issue, and the performance of the saved model is good. I have spent the day trying different `trainable` and `training` flag values in various places to no improvement, thinking originally that it was something to do with the BatchNorm layers in the model. Recompiling the model has not helped.

When I switch back to an older TensorFlow version (2.15.0.post1) I do not see this issue. Both the trained `.keras` and `.h5` models perform well when later loaded and evaluated on my dataset of interest.

This seems like a bug to me, though I also acknowledge that perhaps I have missed something in the TF updates. I have searched the TensorFlow API docs for the various methods to no success. If it is the latter I would be very grateful for any advice, thank you.

### Standalone code to reproduce the issue

```shell
Code is relatively straightforward, I will provide some simple snippets here, though I cannot provide the full files.

Training code:

    model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=args.epochs,
        callbacks=callbacks,
        verbose=2
    )

Model:

        base_model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))
        base_model.trainable = True

        # Add a fully connected dense layer
        x = tf.keras.layers.Dense(512, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(rate=0.5)(x)
        
        # Another fully connected dense layer
        x = tf.keras.layers.Dense(256, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(rate=0.5)(x)

        x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

        # Create a new model with the modified head
        model = tf.keras.Model(inputs=base_model.input, outputs=x)

Evaluation code:

model = tf.keras.models.load_model('./models/quickevaltest_oldertfversion_img96_ep10_bs32_lr0.0001_20240820.keras')

    for batch in tqdm.tqdm(imgs):
        predictions.append(np.array(model.predict(batch, verbose=0))) # model.call(batch) or model(call) is faster (actually maybe not) but predict is a little easier to work with
```


### Relevant log output

_No response_",nkinnaird,2024-08-21 00:45:32+00:00,['tilakrayal'],2024-08-22 16:36:59+00:00,2024-08-22 16:36:56+00:00,https://github.com/tensorflow/tensorflow/issues/74170,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2303851315, 'issue_id': 2476766735, 'author': 'tilakrayal', 'body': '@nkinnaird,\r\nTensorflow v2.17 contains the keras3.0 by default and the tensorflow v2.15 contains keras2.0. Also the efficientnet model is related to keras application, could you please try to raise the issue in keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 22, 6, 3, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304582793, 'issue_id': 2476766735, 'author': 'nkinnaird', 'body': 'I have posted in the Keras issues section.', 'created_at': datetime.datetime(2024, 8, 22, 12, 45, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304816595, 'issue_id': 2476766735, 'author': 'tilakrayal', 'body': '@nkinnaird,\r\nCould you please feel free to move this issue to closed status, as this has been tracking in the keras repo. Thank you!', 'created_at': datetime.datetime(2024, 8, 22, 14, 27, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305192348, 'issue_id': 2476766735, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74170"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74170"">No</a>', 'created_at': datetime.datetime(2024, 8, 22, 16, 36, 58, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-22 06:03:20 UTC): @nkinnaird,
Tensorflow v2.17 contains the keras3.0 by default and the tensorflow v2.15 contains keras2.0. Also the efficientnet model is related to keras application, could you please try to raise the issue in keras-team/keras repo for the quick resolution. Thank you!

nkinnaird (Issue Creator) on (2024-08-22 12:45:48 UTC): I have posted in the Keras issues section.

tilakrayal (Assginee) on (2024-08-22 14:27:44 UTC): @nkinnaird,
Could you please feel free to move this issue to closed status, as this has been tracking in the keras repo. Thank you!

google-ml-butler[bot] on (2024-08-22 16:36:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74170"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74170"">No</a>

"
2475832569,issue,open,,tf.sparse.reduce_sum error in JIT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d

### Custom code

Yes

### OS platform and distribution

Ubuntu Mate 22.04

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.3

### GPU model and memory

_No response_

### Current behavior?

Error when using tf.sparse.reduce_sum and JIT compilation.

I have written a layer that passes all my unit tests except when I use in a model with predict or train. 
Would that make sense with the JIT compilation on? I am not fully certain how and when this works.

Anyway, I am not exactly sure why my layer code fails, but I think that this minimum reproducible example captures the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

batch_size = 4
input_shape = (3, 3)

indices = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [2, 0, 0], [2, 0, 1], [3, 0, 0], [3, 0, 1]])
inputs = tf.sparse.SparseTensor(dense_shape=(batch_size, *input_shape),
                                indices=indices,
                                values=[9, 1, 9, 1, 9, 1, 9, 1])


@tf.function(input_signature=[tf.SparseTensorSpec(
    shape=(4, 3, 3),
    dtype=tf.dtypes.int32)], jit_compile=True)
def get_batch_sum(inputs):
    # same problem with tf.sparse.reduce_max
    return tf.sparse.reduce_sum(tf.sparse.reduce_sum(inputs, axis=1, output_is_sparse=True), axis=1)


sum_out = get_batch_sum(inputs)
print(sum_out)
```


### Relevant log output

```shell
2024-08-20 16:31:01.690779: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSumSparse (No registered 'SparseReduceSumSparse' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSumSparse}}){{node SparseReduceSumSparse}}
The op is created at: 
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 21, in <module>
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
Traceback (most recent call last):
(...)  

File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_15]
```
",edbosne,2024-08-20 14:39:42+00:00,['Venkat6871'],2024-10-10 13:47:09+00:00,,https://github.com/tensorflow/tensorflow/issues/74131,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('comp:xla', 'XLA'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2302146273, 'issue_id': 2475832569, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/ccb48727fe4acd9ae9425bb6d1fb573c/74131_2-15-0-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 21, 14, 7, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405144944, 'issue_id': 2475832569, 'author': 'felbecker', 'body': 'This also happens for other sparse ops: `tf.sparse.reorder` and `tf.sparse.softmax`.\r\nReproducable with code very closely related to above.', 'created_at': datetime.datetime(2024, 10, 10, 13, 47, 8, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-21 14:07:54 UTC): I tried to run your code on Colab using TF v2.15.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/ccb48727fe4acd9ae9425bb6d1fb573c/74131_2-15-0-nightly-v.ipynb) here for reference.
Thank you!

felbecker on (2024-10-10 13:47:08 UTC): This also happens for other sparse ops: `tf.sparse.reorder` and `tf.sparse.softmax`.
Reproducable with code very closely related to above.

"
2473519932,issue,closed,completed,Xcode fails to build tensorflow-lite due to kleidia error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

After 2.17.0 - master branch (52be72f)

### Custom code

Yes

### OS platform and distribution

macOS 14.4.1 & macOS 14.6.1

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

AppleClang 15.0.0.15000309

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling with Xcode, generates the error while compiling tensorflow-lite:
```
[kleidiai] Compiling kai_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla.c
Error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'unsigned int' [-Werror,-Wshorten-64-to-32]
    unsigned int string_length = k;
```
because `size_t != unsigned int`.

[Here is the full log](https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin/actions/runs/10453547929/job/28944319091?pr=9#step:6:249) in Github Action.

KleidiAI seems to be included by XNNPACK. 
I use CMake to generate the project. Defining `set(XNNPACK_ENABLE_KLEIDIAI OFF)` solves the problem, but I suppose either this needs to be fixed directly in KleidAI or XNNPACK (or tensorflow) needs to ensure that KleidAI is not used. 

### Standalone code to reproduce the issue

```shell
This branch on my opensource project: https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin/tree/dependabot/submodules/tensorflow-acc45fe
```


### Relevant log output

```shell
https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin/actions/runs/10453547929/job/28944319091?pr=9#step:6:249
```
",pierreguillot,2024-08-19 14:34:33+00:00,"['pkgoogle', 'sawantkumar']",2024-09-18 01:58:57+00:00,2024-09-18 01:58:48+00:00,https://github.com/tensorflow/tensorflow/issues/74057,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('comp:lite-xnnpack', 'TensorFlow Lite XNNPack related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2327358261, 'issue_id': 2473519932, 'author': 'pkgoogle', 'body': ""Hi @pierreguillot, can you provide more detail on reproduce steps? Like what should I do w/ your branch (which I can't seem to access, just FYI). What commands do you run? Thanks."", 'created_at': datetime.datetime(2024, 9, 3, 20, 15, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342463068, 'issue_id': 2473519932, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 58, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336500, 'issue_id': 2473519932, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336703, 'issue_id': 2473519932, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74057"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74057"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 56, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-09-03 20:15:22 UTC): Hi @pierreguillot, can you provide more detail on reproduce steps? Like what should I do w/ your branch (which I can't seem to access, just FYI). What commands do you run? Thanks.

github-actions[bot] on (2024-09-11 01:58:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74057"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74057"">No</a>

"
2473197795,issue,closed,completed,Unexpected failure when preparing tensor allocations,"Hi!
I've trained torch EfficientNetB0 model, then converted it to tflite and faced this failure:

`Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:332 input->dims->data[3] != filter->dims->data[3] (4 != 3)`

I've loaded model as _New-Other-TensorflowLite Model_ and then tried to use:

```
try{
    MyModel myModel = MyModel.newInstance(MainActivity.this);
} catch (IOException e){
     // TODO
}
```

but get this error.
Here is part of my code for conversion:

```
dummy_inputs = (torch.randn(1,3,256,256),)

edge_model = ai_edge_torch.convert(model.eval(), dummy_inputs)
edge_model.export(""classifier_best.tflite"")
```

Am I doing something wrong?",NazaRik555,2024-08-19 12:06:41+00:00,['gaikwadrahul8'],2024-09-10 01:58:54+00:00,2024-09-10 01:58:54+00:00,https://github.com/tensorflow/tensorflow/issues/74049,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2309735763, 'issue_id': 2473197795, 'author': 'gaikwadrahul8', 'body': 'Hi, @NazaRik555 \r\n\r\nI apologize for the delayed response and If possible could you please help us with Google colab notebook with your model along with complete steps to reproduce the same behavior from our end to investigate this issue further ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 26, 9, 13, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464717, 'issue_id': 2473197795, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460205, 'issue_id': 2473197795, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 53, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-08-26 09:13:10 UTC): Hi, @NazaRik555 

I apologize for the delayed response and If possible could you please help us with Google colab notebook with your model along with complete steps to reproduce the same behavior from our end to investigate this issue further ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-03 01:56:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-10 01:58:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2473176274,issue,closed,completed,ImportError: Traceback (most recent call last):,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10

### Custom code

No

### OS platform and distribution

windows 11

### Mobile device

windows 11

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In jupyter notebook applying import tensorflow as tf gives the following error

### Standalone code to reproduce the issue

```shell
ImportError                               Traceback (most recent call last)
File ~\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\python\__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File ~\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\lenovo\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```


### Relevant log output

```shell
import tensorflow as tf
ImportError                               Traceback (most recent call last)
File ~\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:
```
",ronitmitra,2024-08-19 11:55:06+00:00,['tilakrayal'],2024-09-04 01:57:35+00:00,2024-09-04 01:57:32+00:00,https://github.com/tensorflow/tensorflow/issues/74048,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2298826998, 'issue_id': 2473176274, 'author': 'tilakrayal', 'body': '@ronitmitra,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 20, 13, 11, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935750, 'issue_id': 2473176274, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327758400, 'issue_id': 2473176274, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 4, 1, 57, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327758451, 'issue_id': 2473176274, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74048"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74048"">No</a>', 'created_at': datetime.datetime(2024, 9, 4, 1, 57, 34, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-20 13:11:13 UTC): @ronitmitra,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-08-28 01:56:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-04 01:57:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-04 01:57:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74048"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74048"">No</a>

"
2472897386,issue,closed,completed,SIGSEGV(SEGV_MAPERR),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow-lite:2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Got native crash when using Tensorflow lite on android7.0/android713/android14.
Only a few user got this crash, can't be reproduced on local.

### Standalone code to reproduce the issue

```shell
public final String recognizeSync(float[][] trace) {
        if (trace == null || trace.length == 0) {
            return ERROR_TEXT;
        }
        Bitmap bitmap = this.getImgFromTrace(trace);
        String result = this.recognizeFromBitmap(bitmap);

        if (TextUtils.isEmpty(result)) {
            return ERROR_TEXT;
        }
        return result;
    }

 private final Bitmap getImgFromTrace(float[][] points) {
        int MAX_WIDTH = 128;
        int MAX_HEIGHT = 64;
        float minX = points[0][0];
        float minY = points[0][1];
        float maxX = 0.0F;
        float maxY = 0.0F;

        float dx;
        float dy;
        for (int i = 0; i < points.length; ++i) {
            for (int j = 0; j < points[i].length; j += 2) {
                dx = points[i][j];
                dy = points[i][j + 1];
                if (dx < minX) {
                    minX = dx;
                }

                if (dy < minY) {
                    minY = dy;
                }

                if (dx > maxX) {
                    maxX = dx;
                }

                if (dy > maxY) {
                    maxY = dy;
                }
            }
        }

        float width = maxX - minX;
        float height = maxY - minY;
        float ratio = 1.0F;
        if (width > MAX_WIDTH) {
            ratio = MAX_WIDTH / width;
        }

        if (height > MAX_HEIGHT) {
            dx = MAX_HEIGHT / height;
            if (dx < ratio) {
                ratio = dx;
            }
        }

        dx = (float) MAX_WIDTH - ratio * width <= (float) 0 ? 0.0F : ((float) MAX_WIDTH - ratio * width) / (float) 2;
        dy = (float) MAX_HEIGHT - ratio * height <= (float) 0 ? 0.0F : ((float) MAX_HEIGHT - ratio * height) / (float) 2;

        for (int i = 0; i < points.length; ++i) {
            for (int j = 0; j < points[i].length; ++j) {
                if (j % 2 == 0) {
                    points[i][j] -= minX;
                    points[i][j] *= ratio;
                    points[i][j] += dx;
                } else {
                    points[i][j] -= minY;
                    points[i][j] *= ratio;
                    points[i][j] += dy;
                }
            }
        }

        return this.drawPoints(MAX_WIDTH, MAX_HEIGHT, points);
    }

 private String recognizeFromBitmap(Bitmap bitmap) throws IllegalStateException {
        if (!this.isInitialized) {
            throw new IllegalStateException(""TF Lite Interpreter is not initialized yet."");
        } else {

            // 获取轨迹，并将轨迹转为字节流
            ByteBuffer byteBuffer = this.convertBitmapToByteBuffer(bitmap);


            // 跑模型获取输出结果
            int one = 1;
            float[][][] result = new float[one][][];

            for (int i = 0; i < one; ++i) {
                byte two = 15;
                float[][] resultSub = new float[two][];

                for (int j = 0; j < two; ++j) {
                    resultSub[j] = new float[39];
                }
                result[i] = resultSub;
            }

            if (interpreter != null) {
                interpreter.run(byteBuffer, result);
            }

            // 解码网络输出，采用greedy search
            String res = this.greedyDecoder(result[0]);
            return res;
        }
    }
```


### Relevant log output

```shell
#00 pc 000000000024fc10 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
2
#01 pc 000000000024fca4 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
3
#02 pc 000000000024e7ac /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
4
#03 pc 000000000024e83c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
5
#04 pc 00000000000d9f5c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
6
#05 pc 000000000010754c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
7
#06 pc 00000000001073d8 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
8
#07 pc 000000000010654c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
9
#08 pc 00000000001113c4 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
10
#09 pc 000000000011059c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
11
#10 pc 0000000000102dbc /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
12
#11 pc 000000000030205c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
13
#12 pc 0000000000301b5c /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
14
#13 pc 00000000002f6124 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
15
#14 pc 000000000007f9c0 /data/app/com.xueersi.aitutor-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+88) [arm64-v8a::385d6d92f29b4b4cb3ffca33758d3471]
16
#15 pc 00000000000db090 /system/lib64/libart.so (art_quick_generic_jni_trampoline+144) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
17
#16 pc 00000000000d1d68 /system/lib64/libart.so (art_quick_invoke_static_stub+600) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
18
#17 pc 00000000000de7b0 /system/lib64/libart.so (_ZN3art9ArtMethod6InvokeEPNS_6ThreadEPjjPNS_6JValueEPKc+252) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
19
#18 pc 000000000028c27c /system/lib64/libart.so (_ZN3art11interpreter34ArtInterpreterToCompiledCodeBridgeEPNS_6ThreadEPNS_9ArtMethodEPKNS_7DexFile8CodeItemEPNS_11ShadowFrameEPNS_6JValueE+312) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
20
#19 pc 0000000000285258 /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+592) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
21
#20 pc 0000000000551c34 /system/lib64/libart.so (MterpInvokeStatic+356) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
22
#21 pc 00000000000c4614 /system/lib64/libart.so (ExecuteMterpImpl+14612) [arm64-v8a::0fd099bbe0fa25c875015e135e602114]
23
java:
24
org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:264)
25
org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
26
org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:95)
27
org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
28
org.tensorflow.lite.Interpreter.run(Interpreter.java:95)
```
",liuhuacheng-tal,2024-08-19 09:30:28+00:00,['gaikwadrahul8'],2025-01-09 09:01:02+00:00,2024-09-10 01:58:55+00:00,https://github.com/tensorflow/tensorflow/issues/74043,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2309618743, 'issue_id': 2472897386, 'author': 'gaikwadrahul8', 'body': 'Hi, @liuhuacheng-tal \r\n\r\nI apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help me with Github repo with minimal code to reproduce the same behavior from our end with complete steps and as much as details which will help us to investigate this issue further ? \r\n\r\nIf you followed any TFLite official documentation please help us with that also to reproduce the same behavior from our end.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 26, 8, 15, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464744, 'issue_id': 2472897386, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460239, 'issue_id': 2472897386, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460307, 'issue_id': 2472897386, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74043"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74043"">No</a>', 'created_at': datetime.datetime(2024, 9, 10, 1, 58, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579503578, 'issue_id': 2472897386, 'author': 'prilaga', 'body': ""Hi, the crash is not fixed. I am getting the same crash on firebase from different users, but can't reproduce it on my devices."", 'created_at': datetime.datetime(2025, 1, 9, 9, 1, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-08-26 08:15:02 UTC): Hi, @liuhuacheng-tal 

I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help me with Github repo with minimal code to reproduce the same behavior from our end with complete steps and as much as details which will help us to investigate this issue further ? 

If you followed any TFLite official documentation please help us with that also to reproduce the same behavior from our end.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-03 01:56:49 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-10 01:58:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-10 01:58:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74043"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74043"">No</a>

prilaga on (2025-01-09 09:01:00 UTC): Hi, the crash is not fixed. I am getting the same crash on firebase from different users, but can't reproduce it on my devices.

"
2472162862,issue,open,,HLO for TopK oddly casts uint8 input to uint32 before passing to radix sort. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2 (HEAD of internal repo)

### Custom code

Yes

### OS platform and distribution

Google-Internal Environment

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

V100 (also reproducible on other GPUs)

### Current behavior?

An Alphabet model invokes `tf.math.top_k` with a tensor of dtype uint8 and shape (1, 1,32768). 
![strange_hlo_text_with_uint32_radix_sort](https://github.com/user-attachments/assets/945af324-b00b-405b-8a68-06b7c33cb1e1)

For this call, XLA ends up calling radix sort. However, the radix sort is sub-optimal because TensorFlow casts the inputs to uint32 (instead of using original dtype uint8). Of course, radix sort is faster across smaller dtypes (with fewer bytes).

> @@(u32[1,32768]{1,0}, s32[1,32768]{1,0}, u8[271871]{0}) custom-call(u32[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort

I would expect HLO text more like this, where the uint8 inputs are passed directly:

> (u8[1,32768]{1,0}, s32[1,32768]{1,0}, u8[310527]{0}) custom-call(u8[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort""

We actually use TF indirectly via the jax2tf bridge, and I see this comment in the code hinting that uint8 is incompatible with `tf.math.top_k`:
https://github.com/google/jax/blob/0b87bf48f97ace10c7aee19c8f980788891a2df7/jax/experimental/jax2tf/jax2tf.py#L3167

However, recently, @dimitar-asenov (on XLA GPU) has made some changes to XLA sorting logic that provides support for radix-sorting uint8.

Could `tf.math.top_k` lower to HLO that avoids this up-cast to uint32 before radix sort? I believe this would halve the latency of radix sort for uint8.

### Standalone code to reproduce the issue

```shell
To repro, collect an XProf trace for the following snippet. See attached screenshot for sample trace.


import tensorflow as tf

data = tf.random.uniform(
          shape=(1, 32768),
          minval=0,
          maxval=256,
          dtype=tf.int32,
      )
data = tf.cast(data, tf.uint8)
_, box_indices = tf.math.top_k(data, k=1024)


Or feel free to just find and run my internal experimental `benchmark_tf_top_k_uint8` binary on a machine with a GPU.
```


### Relevant log output

_No response_",chihuahua,2024-08-18 21:52:59+00:00,['tilakrayal'],2024-08-21 04:30:10+00:00,,https://github.com/tensorflow/tensorflow/issues/74035,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2296603596, 'issue_id': 2472162862, 'author': 'tilakrayal', 'body': 'I tried to execute the mentioned code on tensorflow v2.17, tf-nightly on the google colab which contains the T4 Gpu. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/14615df9c829d80a223d2b078c77e039/untitled2077.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 8, 19, 13, 38, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297911802, 'issue_id': 2472162862, 'author': 'chihuahua', 'body': '> I tried to execute the mentioned code on tensorflow v2.17, tf-nightly on the google colab which contains the T4 Gpu. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/14615df9c829d80a223d2b078c77e039/untitled2077.ipynb). Thank you!\r\n\r\nThis issue files a performance issue. Per that Colab, the code runs correctly, but the HLO text produced to be passed to XLA is suboptimal because the uint8 input is up-casted to uint32 before radix sort happens. Thank you!', 'created_at': datetime.datetime(2024, 8, 20, 3, 53, 3, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-19 13:38:39 UTC): I tried to execute the mentioned code on tensorflow v2.17, tf-nightly on the google colab which contains the T4 Gpu. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/14615df9c829d80a223d2b078c77e039/untitled2077.ipynb). Thank you!

chihuahua (Issue Creator) on (2024-08-20 03:53:03 UTC): This issue files a performance issue. Per that Colab, the code runs correctly, but the HLO text produced to be passed to XLA is suboptimal because the uint8 input is up-casted to uint32 before radix sort happens. Thank you!

"
2471445401,issue,closed,completed,Resolving MKL-DNN Compilation Error in TensorFlow on RISC-V Architecture,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.12.1

### Custom code

Yes

### OS platform and distribution

OpenEuler24.03

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

5.3.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to the information gathered, mkl_dnn_v1 is Intel's acceleration library, and it should be disabled when compiling on the RISC-V architecture. The TensorFlow manual indicates that the MKL acceleration library should be enabled by setting --config=mkl. However, this option was not set during compilation, but the output indicates that the library still entered the compilation process.

The compile command:
bazel --output_user_root=`pwd`/../output_user_root build  --subcommands --host_copt=-Wno-stringop-truncation //tensorflow/tools/pip_package:build_pip_package --verbose_failures --experimental_local_memory_estimate --jobs=16

### Standalone code to reproduce the issue

```shell
https://build.tarsier-infra.isrc.ac.cn/package/show/home:Kaguya/tensorflow-gitee
```


### Relevant log output

```shell
external/mkl_dnn_v1/src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp:102:37: error: 'const struct dnnl::impl::cpu::rnn_utils::diff_src_brgemm_conf_t' has no member named 'isa'
102 | && rnn_.diff_src_brgemm.isa == x64::avx512_core_bf16_amx_bf16) {
| ^~~
```
",helloworld-star,2024-08-17 09:18:56+00:00,['Venkat6871'],2024-09-04 01:57:38+00:00,2024-09-04 01:57:34+00:00,https://github.com/tensorflow/tensorflow/issues/74015,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2297978725, 'issue_id': 2471445401, 'author': 'Venkat6871', 'body': 'Hi **@helloworld-star** ,\r\n- Is there any specific reason for using TensorFlow 2.12.1? We are suggesting to use latest version(2.17.0) is for better results. Please upgrade your version and let us know whether the issue persists with recent TF versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 20, 5, 11, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935798, 'issue_id': 2471445401, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327758438, 'issue_id': 2471445401, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 4, 1, 57, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327758503, 'issue_id': 2471445401, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74015"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74015"">No</a>', 'created_at': datetime.datetime(2024, 9, 4, 1, 57, 37, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-20 05:11:19 UTC): Hi **@helloworld-star** ,
- Is there any specific reason for using TensorFlow 2.12.1? We are suggesting to use latest version(2.17.0) is for better results. Please upgrade your version and let us know whether the issue persists with recent TF versions.
Thank you!

github-actions[bot] on (2024-08-28 01:56:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-04 01:57:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-04 01:57:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74015"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/74015"">No</a>

"
2471180470,issue,open,,Cannot dlopen some GPU libraries [can't find cuda driver] in rhel9,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Redhat enterprise 9.4 base image

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

Tesla T4, 15360MiB

### Current behavior?

What is your question?

Describe the bug

Error when I run GPU test, wondering if my docker linux kernel version is too low? The reason I am asking is that, my ubuntu22 is working fine.

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
My Kernel version,

bash-5.1# uname -a
Linux mlops-test-failed 5.10.220-209.869.amzn2.x86_64 https://github.com/rapidsai/cudf/issues/1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

My OS/Docker image distro,
""
bash-5.1# uname -m && cat /etc/*release
x86_64
NAME=""Red Hat Enterprise Linux""
VERSION=""9.4 (Plow)""
ID=""rhel""
ID_LIKE=""fedora""
VERSION_ID=""9.4""
PLATFORM_ID=""platform:el9""
PRETTY_NAME=""Red Hat Enterprise Linux 9.4 (Plow)""
ANSI_COLOR=""0;31""
LOGO=""fedora-logo-icon""
CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos""
HOME_URL=""https://www.redhat.com/""
DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""

REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 9""
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
REDHAT_SUPPORT_PRODUCT_VERSION=""9.4""
Red Hat Enterprise Linux release 9.4 (Plow)
Red Hat Enterprise Linux release 9.4 (Plow)
""
Both my ubuntu 22 and rhel9 were showing the nvidia-smi ok like the following.

bash-5.1# nvidia-smi
Fri Aug 16 22:29:43 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06 Driver Version: 535.183.06 CUDA Version: 12.2 |
|-----------------------------------------+----------------------+----------------------+
| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|=========================================+======================+======================|
| 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 |
| N/A 33C P8 11W / 70W | 0MiB / 15360MiB | 0% Default |
| | | N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes: |
| GPU GI CI PID Type Process name GPU Memory |
| ID ID Usage |
|=======================================================================================|
| No running processes found |
+---------------------------------------------------------------------------------------+

I am checking the following url,
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html

and it says the rhel9 kernel version has to be 5.14.0-427.

Your input is appreciated!

### Standalone code to reproduce the issue

```shell
def run_gpu_test():
    gpus = tf.config.list_logical_devices('GPU')
    print(""Num GPUs Available: "", len(gpus))

run_gpu_test()

In Ubuntu22, it prints gpu number 1 with all the gpu information and in rhel9 it does not.

This is a Pod we created in eks, and by exec to the pod, we pasted the debugging information in the above section. I did nvidia-smi and both ubuntu22 and rhel9 shows GPU fine. Ubuntu22 works fine but not rhel9. The node we created is a AWS G4 instance, so it has tensorflow 12.7 and cuda 12.2 and we installed nvidia-plugin. I think this should be very easy to reproduce not sure if the aws kernel version matters.
```


### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
```
",kevinli-webbertech,2024-08-16 23:39:02+00:00,['tilakrayal'],2024-09-23 12:59:10+00:00,,https://github.com/tensorflow/tensorflow/issues/73978,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2298823266, 'issue_id': 2471180470, 'author': 'tilakrayal', 'body': '@kevinli-webbertech,\r\nCould you please let us know if you are facing the same issue with ubuntu environment as well? I tried to install in ubuntu environment where the GPU was detected. Also please provide the steps followed. Thank you!', 'created_at': datetime.datetime(2024, 8, 20, 13, 9, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298887653, 'issue_id': 2471180470, 'author': 'kevinli-webbertech', 'body': ""@tilakrayal , I tried docker io's ubuntu22 with just pip install of tensorflow[with-cuda], and GPU test runs fine.\r\n\r\nNote this issue is only seen in rhel9. This setup is EKS node with AWS G4 instance + Nvidia plugin + ehel9 pod container set up (simple pip install of tensorflow[with-cuda]), nvidia-smi can see the nvidia GPU so that driver is ok, and from the fact that ubuntu22 is working, we can see the cuda is 12.2 which is funneled  through the aws G4 instance, we did not overwrite the cuda driver installation by installing the RPM. \r\n\r\nAnd we did have an attempt to install the cuda RPM so we got /usr/loca/cuda, and we had most of the .so files soft links created and put in LD_LIBRARY_PATH and did a ldconfig and it did not work either. The problem of the error is too vague so I do not know what SO files can not be opened. I have found the c++ code in tensorflow which issues this error but I haven't got time to compile and debug.\r\n\r\nPlease let me know your thoughts. Thanks!"", 'created_at': datetime.datetime(2024, 8, 20, 13, 38, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302174446, 'issue_id': 2471180470, 'author': 'kevinli-webbertech', 'body': '@tilakrayal, we got it working. So the problem that worth mentioning here is that, the image was fine with the [tensorflow-with cuda ] installation in rhel9 but some SO missing and we eventually figured out what that is.\r\n\r\nThe following is what I found,\r\n\r\n1/ the ubuntu22 installed the above packages and work right off the box. GPU tests passed.\r\n\r\n2/ In rhel9 we had to do the following. And the cuda/lib64 we mounted in yaml from hostPath of AWS G4 instance and create in our eks pod. But we just learned that nvidia/cudnn/lib/ is also needed. Without that it is not working. I am thinking that might be reason why rhel9 is not working right off the box like ubuntu22. If you happen to get a redhat equivalent image, make sure you can test it with simple installation. Or make sure document this library path somewhere.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: your-test\r\n  namespace: your_name_space\r\nspec:\r\n  nodeName: gpu-node\r\n  restartPolicy: Never\r\n  containers:\r\n  - name: mlops\r\n    image: ""your_rhel9_image""\r\n    volumeMounts:\r\n    - name: cuda-libs\r\n      mountPath: /usr/local/cuda\r\n    resources:\r\n      limits:\r\n        nvidia.com/gpu: 1\r\n    command: [""sleep"", ""infinity""]\r\n  volumes:\r\n  - name: cuda-libs\r\n    hostPath:\r\n      path: /usr/local/cuda\r\n ```     \r\n\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/local/lib/python3.11/site-packages/nvidia/cudnn/lib/\r\nldconfig\r\n\r\n```\r\n\r\n3/ during the course we eyeballed a lot of screens side by side with ubuntu22 and rhel9 using strace on `python3 test-gpu.py` process and was trying to see what is loaded and open and what not.\r\n4/ The solution that we used is that the /usr/local/cuda is mounted from hosting node from G4 instance while ubuntu22 does not need to do that. So the so shared lib of cuda must be in somewhere already from the pip installation. I still need to find that out.\r\nWhat make me less worried is it is not about linux kernel versions.', 'created_at': datetime.datetime(2024, 8, 21, 14, 20, 38, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-20 13:09:25 UTC): @kevinli-webbertech,
Could you please let us know if you are facing the same issue with ubuntu environment as well? I tried to install in ubuntu environment where the GPU was detected. Also please provide the steps followed. Thank you!

kevinli-webbertech (Issue Creator) on (2024-08-20 13:38:36 UTC): @tilakrayal , I tried docker io's ubuntu22 with just pip install of tensorflow[with-cuda], and GPU test runs fine.

Note this issue is only seen in rhel9. This setup is EKS node with AWS G4 instance + Nvidia plugin + ehel9 pod container set up (simple pip install of tensorflow[with-cuda]), nvidia-smi can see the nvidia GPU so that driver is ok, and from the fact that ubuntu22 is working, we can see the cuda is 12.2 which is funneled  through the aws G4 instance, we did not overwrite the cuda driver installation by installing the RPM. 

And we did have an attempt to install the cuda RPM so we got /usr/loca/cuda, and we had most of the .so files soft links created and put in LD_LIBRARY_PATH and did a ldconfig and it did not work either. The problem of the error is too vague so I do not know what SO files can not be opened. I have found the c++ code in tensorflow which issues this error but I haven't got time to compile and debug.

Please let me know your thoughts. Thanks!

kevinli-webbertech (Issue Creator) on (2024-08-21 14:20:38 UTC): @tilakrayal, we got it working. So the problem that worth mentioning here is that, the image was fine with the [tensorflow-with cuda ] installation in rhel9 but some SO missing and we eventually figured out what that is.

The following is what I found,

1/ the ubuntu22 installed the above packages and work right off the box. GPU tests passed.

2/ In rhel9 we had to do the following. And the cuda/lib64 we mounted in yaml from hostPath of AWS G4 instance and create in our eks pod. But we just learned that nvidia/cudnn/lib/ is also needed. Without that it is not working. I am thinking that might be reason why rhel9 is not working right off the box like ubuntu22. If you happen to get a redhat equivalent image, make sure you can test it with simple installation. Or make sure document this library path somewhere.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: your-test
  namespace: your_name_space
spec:
  nodeName: gpu-node
  restartPolicy: Never
  containers:
  - name: mlops
    image: ""your_rhel9_image""
    volumeMounts:
    - name: cuda-libs
      mountPath: /usr/local/cuda
    resources:
      limits:
        nvidia.com/gpu: 1
    command: [""sleep"", ""infinity""]
  volumes:
  - name: cuda-libs
    hostPath:
      path: /usr/local/cuda
 ```     

```
export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/local/lib/python3.11/site-packages/nvidia/cudnn/lib/
ldconfig

```

3/ during the course we eyeballed a lot of screens side by side with ubuntu22 and rhel9 using strace on `python3 test-gpu.py` process and was trying to see what is loaded and open and what not.
4/ The solution that we used is that the /usr/local/cuda is mounted from hosting node from G4 instance while ubuntu22 does not need to do that. So the so shared lib of cuda must be in somewhere already from the pip installation. I still need to find that out.
What make me less worried is it is not about linux kernel versions.

"
2470655369,issue,open,,TFLite Model used in official documentation doesn't compile on Edge TPU Compiler,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 
  - pip show tensorflow  = 2.17.0
  - tf.__version__ = 2.18.0-dev20240815

### 2. Code
Using the code from [Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant)  official tutorial to create and convert a TensorFlow model to TFlite:
 
``` python
import logging
logging.getLogger(""tensorflow"").setLevel(logging.DEBUG)

import tensorflow as tf
import numpy as np
print(""TensorFlow version: "", tf.__version__)

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Define the model architecture
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(
                  from_logits=True),
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8] 
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.experimental_new_quantizer = True

tflite_model_quant = converter.convert()

import pathlib

tflite_models_dir = pathlib.Path(""mnist_tflite_models/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_quant_file = tflite_models_dir/""mnist_model_quant.tflite""
tflite_model_quant_file.write_bytes(tflite_model_quant)
```

Which displays the following output:

```
2024-08-16 18:07:19.094940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-16 18:07:20.845501: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version:  2.18.0-dev20240815
2024-08-16 18:07:25.704749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - accuracy: 0.8672 - loss: 0.4860 - val_accuracy: 0.9722 - val_loss: 0.0965
Epoch 2/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9738 - loss: 0.0919 - val_accuracy: 0.9768 - val_loss: 0.0735
Epoch 3/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9797 - loss: 0.0696 - val_accuracy: 0.9799 - val_loss: 0.0622
Epoch 4/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - accuracy: 0.9833 - loss: 0.0565 - val_accuracy: 0.9800 - val_loss: 0.0600
Epoch 5/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - accuracy: 0.9843 - loss: 0.0524 - val_accuracy: 0.9813 - val_loss: 0.0595
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmptd9h6442\assets
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmptd9h6442\assets
Saved artifact at 'C:\Users\Me\AppData\Local\Temp\tmptd9h6442'. The following endpoints are available:

* Endpoint 'serve'
  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')
Output Type:
  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)
Captures:
  1897857613456: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897859106576: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897857613072: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897857612688: TensorSpec(shape=(), dtype=tf.resource, name=None)
C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1723828081.771633   14420 tf_tfl_flatbuffer_helpers.cc:359] Ignored output_format.
W0000 00:00:1723828081.772124   14420 tf_tfl_flatbuffer_helpers.cc:362] Ignored drop_control_dependency.
2024-08-16 18:08:01.772979: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
2024-08-16 18:08:01.774123: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-16 18:08:01.774323: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
I0000 00:00:1723828081.779428   14420 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled
2024-08-16 18:08:01.780783: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-16 18:08:01.819711: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
2024-08-16 18:08:01.830586: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 57614 microseconds.
2024-08-16 18:08:01.847290: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-08-16 18:08:02.073373: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8
```

### 3. Failure after conversion
When I then run the newly created TFLite file through the `edgetpu_compiler` (via Docker) it fails saying it still has dynamic-sized tensors:
```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_model_quant.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!
```
Any idea how I can fully convert the model to static-sized tensors. I tried the [suggestion](https://github.com/tensorflow/tensorflow/issues/57905#issuecomment-1292720032) of using `converter._experimental_new_quantizer` but that didn't help.",ADarkDividedGem,2024-08-16 17:17:22+00:00,"['pkgoogle', 'sawantkumar']",2024-11-26 08:05:49+00:00,,https://github.com/tensorflow/tensorflow/issues/73946,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2294449519, 'issue_id': 2470655369, 'author': 'ADarkDividedGem', 'body': ""It compiles if I remove the `Conv2D` and `MaxPooling2D` layers:\r\n``` python\r\n...\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(shape=(28, 28)),\r\n  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  #tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\r\n  #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n...\r\n```\r\n```\r\ndocker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_model_quant.tflite\r\nEdge TPU Compiler version 16.0.384591198\r\nStarted a compilation timeout timer of 180 seconds.\r\n\r\nModel compiled successfully in 62 ms.\r\n\r\nInput model: mnist_model_quant.tflite\r\nInput size: 9.87KiB\r\nOutput model: mnist_model_quant_edgetpu.tflite\r\nOutput size: 56.59KiB\r\nOn-chip memory used for caching model parameters: 49.75KiB\r\nOn-chip memory remaining for caching model parameters: 7.68MiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 4\r\nOperation log: mnist_model_quant_edgetpu.log\r\nSee the operation log file for individual operation details.\r\nCompilation child process completed within timeout period.\r\nCompilation succeeded!\r\n```\r\nThis is despite both [operations being supported](https://coral.ai/docs/edgetpu/models-intro/#supported-operations):\r\n\r\n| Operation name | Known limitations |\r\n| --- | --- |\r\n| Conv2d | Must use the same dilation in x and y dimensions. |\r\n| MaxPool2d | No fused activation function. |\r\n\r\nBut I am not seeing any of those limitations being described in the documentation for the [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) or [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layer.\r\n\r\nI also [read](https://stackoverflow.com/a/66683510/1180825) that even if there are no dynamic size tensors in the graph\r\n\r\n> graphs with control flow ops are regarded as dynamic graphs\r\n\r\nIf this concept is indeed true does anyone know how I can detect which layers have control flows and possible alternatives for the  `Conv2D` and `MaxPooling2D` layers?"", 'created_at': datetime.datetime(2024, 8, 16, 23, 35, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294513856, 'issue_id': 2470655369, 'author': 'ADarkDividedGem', 'body': 'It looks like this exact code [worked back in 2020](https://github.com/google-coral/edgetpu/issues/168#issuecomment-656115637) by turning off `experimental_new_quantizer` but doing that gives me an error:\r\n``` python\r\n...\r\nconverter.experimental_new_converter = False\r\n\r\ntflite_model_quant = converter.convert()\r\n...\r\n```\r\n```\r\n2024-08-17 02:17:57.917613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-08-17 02:17:59.684108: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nTensorFlow version:  2.18.0-dev20240815\r\n2024-08-17 02:18:04.296605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nEpoch 1/5\r\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.8559 - loss: 0.5174 - val_accuracy: 0.9631 - val_loss: 0.1282\r\nEpoch 2/5\r\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9665 - loss: 0.1188 - val_accuracy: 0.9715 - val_loss: 0.0961\r\nEpoch 3/5\r\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9772 - loss: 0.0821 - val_accuracy: 0.9763 - val_loss: 0.0716\r\nEpoch 4/5\r\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9806 - loss: 0.0677 - val_accuracy: 0.9774 - val_loss: 0.0701\r\nEpoch 5/5\r\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9824 - loss: 0.0587 - val_accuracy: 0.9798 - val_loss: 0.0606\r\nINFO:tensorflow:Assets written to: C:\\Users\\Me\\AppData\\Local\\Temp\\tmpmrfduqfx\\assets\r\nINFO:tensorflow:Assets written to: C:\\Users\\Me\\AppData\\Local\\Temp\\tmpmrfduqfx\\assets\r\nSaved artifact at \'C:\\Users\\Me\\AppData\\Local\\Temp\\tmpmrfduqfx\'. The following endpoints are available:\r\n\r\n* Endpoint \'serve\'\r\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=\'keras_tensor\')\r\nOutput Type:\r\n  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\r\nCaptures:\r\n  2332127870608: TensorSpec(shape=(), dtype=tf.resource, name=None)\r\n  2332129347344: TensorSpec(shape=(), dtype=tf.resource, name=None)\r\n  2332127870224: TensorSpec(shape=(), dtype=tf.resource, name=None)\r\n  2332127869840: TensorSpec(shape=(), dtype=tf.resource, name=None)\r\nTraceback (most recent call last):\r\n  File ""c:\\Users\\Me\\Documents\\Development\\Projects\\Digital Traits\\tflite-mental\\test.py"", line 47, in <module>\r\n    tflite_model_quant = converter.convert()\r\n                         ^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1231, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1183, in _convert_and_export_metrics  \r\n    result = convert_func(self, *args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1749, in convert\r\n    self._freeze_keras_model()\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py"", line 215, in wrapper\r\n    raise error from None  # Re-throws the exception.\r\n    ^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py"", line 205, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1690, in _freeze_keras_model\r\n    input_signature = _model_input_signature(\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""C:\\Users\\Me\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\site-packages\\tensorflow\\lite\\python\\tflite_keras_util.py"", line 119, in model_input_signature\r\n    input_specs = model._get_save_spec(  # pylint: disable=protected-access\r\n                  ^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'Sequential\' object has no attribute \'_get_save_spec\'. Did you mean: \'_set_save_spec\'?\r\n```', 'created_at': datetime.datetime(2024, 8, 17, 1, 20, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307546256, 'issue_id': 2470655369, 'author': 'pkgoogle', 'body': ""Hi @ADarkDividedGem, let's try to solve one issue at a time as solving one will probably affect everything else. To convert a static tensor model I think we can use [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) to do this. Can you replicate your model in PyTorch, then convert to tflite and see how that goes and use that with the edge-TPU? (Providing a static example input that the converter will trace -- should staticize the graph)"", 'created_at': datetime.datetime(2024, 8, 23, 17, 50, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308031818, 'issue_id': 2470655369, 'author': 'ADarkDividedGem', 'body': '> Can you replicate your model in PyTorch, then convert to tflite and see how that goes and use that with the edge-TPU?\r\n\r\nSorry, I only know how to run models, not create them. I simply used the official documentation from https://www.tensorflow.org/lite ([Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant)) that the `edgetpu_compiler` [successfully compiled back in 2020](https://github.com/google-coral/edgetpu/issues/168#issuecomment-656466716) to create a simple working example of the `edgetpu_compiler` now failing to compile a TFLite model. \r\n\r\nThe best I could do was make use of a [TensorFlow to PyTorch Converter](https://codeconverter.io/convert/tensorflow-to-pytorch) which suggested the following code:\r\n\r\n``` python\r\nimport logging\r\nlogging.getLogger(""torch"").setLevel(logging.DEBUG)\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torchvision.transforms as transforms\r\nfrom torchvision import datasets\r\nfrom torch.utils.data import DataLoader\r\nimport ai_edge_torch\r\n\r\n# Load MNIST dataset\r\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.float() / 255)])\r\ntrain_dataset = datasets.MNIST(root=\'.\', train=True, download=True, transform=transform)\r\ntest_dataset = datasets.MNIST(root=\'.\', train=False, download=True, transform=transform)\r\n\r\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\r\ntest_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\r\n\r\n# Define the model architecture\r\nclass ConvNet(nn.Module):\r\n    def __init__(self):\r\n        super(ConvNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=0)\r\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\r\n        self.flatten = nn.Flatten()\r\n        self.fc1 = nn.Linear(12 * 13 * 13, 10)  # Adjusted input size to fully connected layer\r\n\r\n    def forward(self, x):\r\n        x = self.pool(nn.ReLU()(self.conv1(x)))\r\n        x = self.flatten(x)\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n# Initialize the model, loss function and optimizer\r\nmodel = ConvNet()\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.Adam(model.parameters())\r\n\r\n# Train the digit classification model\r\nfor epoch in range(5):\r\n    model.train()\r\n    for images, labels in train_loader:\r\n        optimizer.zero_grad()\r\n        outputs = model(images)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\nedge_model = ai_edge_torch.convert(model.eval(), test_dataset.data[1])\r\nedge_model.export(""simple.tflite"")\r\n```\r\n\r\nBut I have no idea if that is even similar to the original model and it fails because `ai_edge_torch.convert()`\xa0expects a tuple of torch tensors:\r\n\r\n```\r\ndocker run --rm -it -v .:/home/edgetpu edgetpu-compiler2 python3.11 pytorch.py\r\n2024-08-24 03:27:08.398415: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-08-24 03:27:08.575714: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-08-24 03:27:08.709057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1724470028.830087       1 cuda_dnn.cc:8315] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1724470028.861485       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-24 03:27:09.163964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File ""/home/edgetpu/pytorch.py"", line 53, in <module>\r\n    edge_model = ai_edge_torch.convert(model.eval(), test_loader)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/converter.py"", line 195, in convert\r\n    return Converter().convert(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/converter.py"", line 134, in convert\r\n    return conversion.convert_signatures(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion.py"", line 90, in convert_signatures\r\n    exported_programs: torch.export.ExportedProgram = [\r\n                                                      ^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion.py"", line 91, in <listcomp>\r\n    torch.export.export(sig.module, sig.flat_args, dynamic_shapes=sig.dynamic_shapes)\r\n                                    ^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion_utils.py"", line 123, in flat_args\r\n    args, kwargs = self._normalized_sample_args_kwargs\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion_utils.py"", line 61, in _normalized_sample_args_kwargs\r\n    raise ValueError(""sample_args must be a tuple of torch tensors."")\r\nValueError: sample_args must be a tuple of torch tensors.\r\n```\r\n\r\nI will need to ask for help converting the model used in the TensorFlow documentation to PyTorch.', 'created_at': datetime.datetime(2024, 8, 24, 3, 36, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310789059, 'issue_id': 2470655369, 'author': 'pkgoogle', 'body': 'Hi @ADarkDividedGem, no worries -- let me give you a little more context with what we\'re trying to do here. I want to see if the model runs w/o training first (In most cases the exact model weights don\'t matter -- it\'s more the structure -- though this isn\'t always 100% true) so I\'m going to try that first:\r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport ai_edge_torch\r\n\r\n\r\n# Define the model architecture\r\nclass ConvNet(nn.Module):\r\n    def __init__(self):\r\n        super(ConvNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=0)\r\n        self.fc1 = nn.Linear(12 * 13 * 13, 10)  # Adjusted input size to fully connected layer\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = F.max_pool2d(F.relu(x), kernel_size=2, stride=2, padding=0)\r\n        x = torch.flatten(x)\r\n        x = self.fc1(x)\r\n        return x\r\n\r\nmodel = ConvNet()\r\nsample_input = (torch.randn(1, 1, 28, 28),)\r\n\r\nedge_model = ai_edge_torch.convert(model.eval(), sample_input)\r\nedge_model.export(""mnist_conv.tflite"")\r\n```\r\n\r\nCan you try running this first to see if it works? (The output values will be gibberish, and also any accuracy) but if this works then I believe the trained model should work fine as well. Thanks.', 'created_at': datetime.datetime(2024, 8, 26, 18, 14, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311057214, 'issue_id': 2470655369, 'author': 'ADarkDividedGem', 'body': 'Yes, the now model compiles but the operations won\'t run on the Edge TPU due to the use of unsupported data types.\r\n\r\nI tried all three [quantization recipes](https://github.com/google-ai-edge/ai-edge-torch/blob/main/ai_edge_torch/generative/quantize/quant_recipes.py) but the compiler still reported unsupported data types.\r\n``` python\r\nfrom ai_edge_torch.generative.quantize import quant_recipes\r\n...\r\nquant_config = quant_recipes.full_int8_dynamic_recipe()\r\nedge_model = ai_edge_torch.convert(model.eval(), sample_input, quant_config=quant_config)\r\n```\r\n\r\nFirst the PyTorch model is converted to `mnist_conv.tflite`:\r\n\r\n```\r\ndocker run --rm -it -v .:/home/edgetpu edgetpu-compiler2 python3.11 pytorch.py\r\n2024-08-26 20:35:50.987067: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-08-26 20:35:51.139615: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-08-26 20:35:51.267440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1724704551.371448       1 cuda_dnn.cc:8315] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1724704551.401996       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-26 20:35:51.683706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\r\nWARNING:root:Defaulting to PJRT_DEVICE=CPU\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1724704560.241015       1 cpu_client.cc:467] TfrtCpuClient created.\r\n2024-08-26 20:36:00.364906: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:216] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\r\nWARNING:absl:Reset all op configs under scope_regex .* with OpQuantizationRecipe(regex=\'.*\', operation=<TFLOperationName.ALL_SUPPORTED: \'*\'>, algorithm_key=<AlgorithmName.MIN_MAX_UNIFORM_QUANT: \'min_max_uniform_quantize\'>, op_config=OpQuantizationConfig(activation_tensor_config=None, weight_tensor_config=TensorQuantizationConfig(num_bits=8, symmetric=True, channel_wise=True, dtype=<TensorDataType.INT: \'INT\'>), execution_mode=<OpExecutionMode.DRQ: \'DRQ\'>)).\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1724704562.069626       1 tf_tfl_flatbuffer_helpers.cc:359] Ignored output_format.\r\nW0000 00:00:1724704562.069712       1 tf_tfl_flatbuffer_helpers.cc:362] Ignored drop_control_dependency.\r\n2024-08-26 20:36:02.071434: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpm9wcckoh\r\n2024-08-26 20:36:02.072148: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\r\n2024-08-26 20:36:02.072245: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpm9wcckoh\r\nI0000 00:00:1724704562.076545       1 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\r\n2024-08-26 20:36:02.077197: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\r\n2024-08-26 20:36:02.107944: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpm9wcckoh\r\n2024-08-26 20:36:02.112932: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 41506 microseconds.\r\n2024-08-26 20:36:02.143756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-08-26 20:36:02.261960: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3605] Estimated count of arithmetic ops: 0.203 M  ops, equivalently 0.101 M  MACs\r\nWARNING:absl:Reset all op configs under scope_regex .* with OpQuantizationRecipe(regex=\'.*\', operation=<TFLOperationName.ALL_SUPPORTED: \'*\'>, algorithm_key=<AlgorithmName.MIN_MAX_UNIFORM_QUANT: \'min_max_uniform_quantize\'>, op_config=OpQuantizationConfig(activation_tensor_config=None, weight_tensor_config=TensorQuantizationConfig(num_bits=8, symmetric=True, channel_wise=True, dtype=<TensorDataType.INT: \'INT\'>), execution_mode=<OpExecutionMode.DRQ: \'DRQ\'>)).\r\nI0000 00:00:1724704563.475274       1 cpu_client.cc:470] TfrtCpuClient destroyed.\r\n```\r\nThen the `mnist_conv.tflite` model successfully compiles but none of the operations will run on the Edge TPU:\r\n```\r\ndocker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_conv.tflite\r\nEdge TPU Compiler version 16.0.384591198\r\nStarted a compilation timeout timer of 180 seconds.\r\n\r\nModel compiled successfully in 2 ms.\r\n\r\nInput model: mnist_conv.tflite\r\nInput size: 23.02KiB\r\nOutput model: mnist_conv_edgetpu.tflite\r\nOutput size: 22.41KiB\r\nOn-chip memory used for caching model parameters: 0.00B\r\nOn-chip memory remaining for caching model parameters: 0.00B\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 0\r\nTotal number of operations: 7\r\nOperation log: mnist_conv_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 0\r\nNumber of operations that will run on CPU: 7\r\nSee the operation log file for individual operation details.\r\nCompilation child process completed within timeout period.\r\nCompilation succeeded!\r\n```\r\nThe log file reports that the operations are using ""an unsupported data type"":\r\n```\r\nEdge TPU Compiler version 16.0.384591198\r\nInput: mnist_conv.tflite\r\nOutput: mnist_conv_edgetpu.tflite\r\n\r\nOperator                       Count      Status\r\n\r\nFULLY_CONNECTED                1          Operation is working on an unsupported data type\r\nDEPTHWISE_CONV_2D              1          Operation is working on an unsupported data type\r\nTRANSPOSE                      1          Operation is working on an unsupported data type\r\nRESHAPE                        3          Operation is working on an unsupported data type\r\nMAX_POOL_2D                    1          Operation is working on an unsupported data type\r\n```\r\n\r\nI can ask [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch/issues) directly why this model is not working fully on the Edge TPU.\r\n\r\nPart of the [model requirements](https://coral.ai/docs/edgetpu/models-intro/#model-requirements) for Edge TPU is that ""Tensor sizes are constant at compile-time (no dynamic sizes)""\r\n\r\nEDIT: Looks like you are already aware that static quantization is not currently supported:\r\nhttps://github.com/google-ai-edge/ai-edge-torch/issues/150#issuecomment-2299777420', 'created_at': datetime.datetime(2024, 8, 26, 20, 44, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313185363, 'issue_id': 2470655369, 'author': 'pkgoogle', 'body': 'I see, i see, can you do me a favor and write a comment on that thread that it is blocking this issue? That way we can properly track things.\r\n\r\nblocked by https://github.com/google-ai-edge/ai-edge-torch/issues/150', 'created_at': datetime.datetime(2024, 8, 27, 17, 55, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499929384, 'issue_id': 2470655369, 'author': 'gaikwadrahul8', 'body': 'Hi, @ADarkDividedGem \r\nThanks for raising this issue. Are you aware of [AI-Edge-Torch](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/)? As we believe this issue is better supported by and more relevant to AI-Edge-Torch we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/ai-edge-torch/issues/384\r\n\r\nLet us know if you have any questions. Thanks.', 'created_at': datetime.datetime(2024, 11, 26, 8, 5, 48, tzinfo=datetime.timezone.utc)}]","ADarkDividedGem (Issue Creator) on (2024-08-16 23:35:04 UTC): It compiles if I remove the `Conv2D` and `MaxPooling2D` layers:
``` python
...
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  #tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])
...
```
```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_model_quant.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 62 ms.

Input model: mnist_model_quant.tflite
Input size: 9.87KiB
Output model: mnist_model_quant_edgetpu.tflite
Output size: 56.59KiB
On-chip memory used for caching model parameters: 49.75KiB
On-chip memory remaining for caching model parameters: 7.68MiB
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 1
Total number of operations: 4
Operation log: mnist_model_quant_edgetpu.log
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
```
This is despite both [operations being supported](https://coral.ai/docs/edgetpu/models-intro/#supported-operations):

| Operation name | Known limitations |
| --- | --- |
| Conv2d | Must use the same dilation in x and y dimensions. |
| MaxPool2d | No fused activation function. |

But I am not seeing any of those limitations being described in the documentation for the [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) or [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layer.

I also [read](https://stackoverflow.com/a/66683510/1180825) that even if there are no dynamic size tensors in the graph


If this concept is indeed true does anyone know how I can detect which layers have control flows and possible alternatives for the  `Conv2D` and `MaxPooling2D` layers?

ADarkDividedGem (Issue Creator) on (2024-08-17 01:20:56 UTC): It looks like this exact code [worked back in 2020](https://github.com/google-coral/edgetpu/issues/168#issuecomment-656115637) by turning off `experimental_new_quantizer` but doing that gives me an error:
``` python
...
converter.experimental_new_converter = False

tflite_model_quant = converter.convert()
...
```
```
2024-08-17 02:17:57.917613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-17 02:17:59.684108: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version:  2.18.0-dev20240815
2024-08-17 02:18:04.296605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.8559 - loss: 0.5174 - val_accuracy: 0.9631 - val_loss: 0.1282
Epoch 2/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9665 - loss: 0.1188 - val_accuracy: 0.9715 - val_loss: 0.0961
Epoch 3/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9772 - loss: 0.0821 - val_accuracy: 0.9763 - val_loss: 0.0716
Epoch 4/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9806 - loss: 0.0677 - val_accuracy: 0.9774 - val_loss: 0.0701
Epoch 5/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9824 - loss: 0.0587 - val_accuracy: 0.9798 - val_loss: 0.0606
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmpmrfduqfx\assets
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmpmrfduqfx\assets
Saved artifact at 'C:\Users\Me\AppData\Local\Temp\tmpmrfduqfx'. The following endpoints are available:

* Endpoint 'serve'
  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')
Output Type:
  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)
Captures:
  2332127870608: TensorSpec(shape=(), dtype=tf.resource, name=None)
  2332129347344: TensorSpec(shape=(), dtype=tf.resource, name=None)
  2332127870224: TensorSpec(shape=(), dtype=tf.resource, name=None)
  2332127869840: TensorSpec(shape=(), dtype=tf.resource, name=None)
Traceback (most recent call last):
  File ""c:\Users\Me\Documents\Development\Projects\Digital Traits\tflite-mental\test.py"", line 47, in <module>
    tflite_model_quant = converter.convert()
                         ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1231, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1183, in _convert_and_export_metrics  
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1749, in convert
    self._freeze_keras_model()
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1690, in _freeze_keras_model
    input_signature = _model_input_signature(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\tflite_keras_util.py"", line 119, in model_input_signature
    input_specs = model._get_save_spec(  # pylint: disable=protected-access
                  ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Sequential' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?
```

pkgoogle (Assginee) on (2024-08-23 17:50:43 UTC): Hi @ADarkDividedGem, let's try to solve one issue at a time as solving one will probably affect everything else. To convert a static tensor model I think we can use [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) to do this. Can you replicate your model in PyTorch, then convert to tflite and see how that goes and use that with the edge-TPU? (Providing a static example input that the converter will trace -- should staticize the graph)

ADarkDividedGem (Issue Creator) on (2024-08-24 03:36:54 UTC): Sorry, I only know how to run models, not create them. I simply used the official documentation from https://www.tensorflow.org/lite ([Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant)) that the `edgetpu_compiler` [successfully compiled back in 2020](https://github.com/google-coral/edgetpu/issues/168#issuecomment-656466716) to create a simple working example of the `edgetpu_compiler` now failing to compile a TFLite model. 

The best I could do was make use of a [TensorFlow to PyTorch Converter](https://codeconverter.io/convert/tensorflow-to-pytorch) which suggested the following code:

``` python
import logging
logging.getLogger(""torch"").setLevel(logging.DEBUG)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import ai_edge_torch

# Load MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.float() / 255)])
train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='.', train=False, download=True, transform=transform)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Define the model architecture
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=0)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(12 * 13 * 13, 10)  # Adjusted input size to fully connected layer

    def forward(self, x):
        x = self.pool(nn.ReLU()(self.conv1(x)))
        x = self.flatten(x)
        x = self.fc1(x)
        return x

# Initialize the model, loss function and optimizer
model = ConvNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Train the digit classification model
for epoch in range(5):
    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

edge_model = ai_edge_torch.convert(model.eval(), test_dataset.data[1])
edge_model.export(""simple.tflite"")
```

But I have no idea if that is even similar to the original model and it fails because `ai_edge_torch.convert()` expects a tuple of torch tensors:

```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler2 python3.11 pytorch.py
2024-08-24 03:27:08.398415: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-08-24 03:27:08.575714: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-08-24 03:27:08.709057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1724470028.830087       1 cuda_dnn.cc:8315] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1724470028.861485       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-24 03:27:09.163964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/edgetpu/pytorch.py"", line 53, in <module>
    edge_model = ai_edge_torch.convert(model.eval(), test_loader)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/converter.py"", line 195, in convert
    return Converter().convert(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/converter.py"", line 134, in convert
    return conversion.convert_signatures(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion.py"", line 90, in convert_signatures
    exported_programs: torch.export.ExportedProgram = [
                                                      ^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion.py"", line 91, in <listcomp>
    torch.export.export(sig.module, sig.flat_args, dynamic_shapes=sig.dynamic_shapes)
                                    ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion_utils.py"", line 123, in flat_args
    args, kwargs = self._normalized_sample_args_kwargs
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/ai_edge_torch/convert/conversion_utils.py"", line 61, in _normalized_sample_args_kwargs
    raise ValueError(""sample_args must be a tuple of torch tensors."")
ValueError: sample_args must be a tuple of torch tensors.
```

I will need to ask for help converting the model used in the TensorFlow documentation to PyTorch.

pkgoogle (Assginee) on (2024-08-26 18:14:46 UTC): Hi @ADarkDividedGem, no worries -- let me give you a little more context with what we're trying to do here. I want to see if the model runs w/o training first (In most cases the exact model weights don't matter -- it's more the structure -- though this isn't always 100% true) so I'm going to try that first:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import ai_edge_torch


# Define the model architecture
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=0)
        self.fc1 = nn.Linear(12 * 13 * 13, 10)  # Adjusted input size to fully connected layer

    def forward(self, x):
        x = self.conv1(x)
        x = F.max_pool2d(F.relu(x), kernel_size=2, stride=2, padding=0)
        x = torch.flatten(x)
        x = self.fc1(x)
        return x

model = ConvNet()
sample_input = (torch.randn(1, 1, 28, 28),)

edge_model = ai_edge_torch.convert(model.eval(), sample_input)
edge_model.export(""mnist_conv.tflite"")
```

Can you try running this first to see if it works? (The output values will be gibberish, and also any accuracy) but if this works then I believe the trained model should work fine as well. Thanks.

ADarkDividedGem (Issue Creator) on (2024-08-26 20:44:04 UTC): Yes, the now model compiles but the operations won't run on the Edge TPU due to the use of unsupported data types.

I tried all three [quantization recipes](https://github.com/google-ai-edge/ai-edge-torch/blob/main/ai_edge_torch/generative/quantize/quant_recipes.py) but the compiler still reported unsupported data types.
``` python
from ai_edge_torch.generative.quantize import quant_recipes
...
quant_config = quant_recipes.full_int8_dynamic_recipe()
edge_model = ai_edge_torch.convert(model.eval(), sample_input, quant_config=quant_config)
```

First the PyTorch model is converted to `mnist_conv.tflite`:

```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler2 python3.11 pytorch.py
2024-08-26 20:35:50.987067: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-08-26 20:35:51.139615: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-08-26 20:35:51.267440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1724704551.371448       1 cuda_dnn.cc:8315] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1724704551.401996       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-26 20:35:51.683706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md
WARNING:root:Defaulting to PJRT_DEVICE=CPU
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1724704560.241015       1 cpu_client.cc:467] TfrtCpuClient created.
2024-08-26 20:36:00.364906: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:216] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
WARNING:absl:Reset all op configs under scope_regex .* with OpQuantizationRecipe(regex='.*', operation=<TFLOperationName.ALL_SUPPORTED: '*'>, algorithm_key=<AlgorithmName.MIN_MAX_UNIFORM_QUANT: 'min_max_uniform_quantize'>, op_config=OpQuantizationConfig(activation_tensor_config=None, weight_tensor_config=TensorQuantizationConfig(num_bits=8, symmetric=True, channel_wise=True, dtype=<TensorDataType.INT: 'INT'>), execution_mode=<OpExecutionMode.DRQ: 'DRQ'>)).
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1724704562.069626       1 tf_tfl_flatbuffer_helpers.cc:359] Ignored output_format.
W0000 00:00:1724704562.069712       1 tf_tfl_flatbuffer_helpers.cc:362] Ignored drop_control_dependency.
2024-08-26 20:36:02.071434: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpm9wcckoh
2024-08-26 20:36:02.072148: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-26 20:36:02.072245: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpm9wcckoh
I0000 00:00:1724704562.076545       1 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled
2024-08-26 20:36:02.077197: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-26 20:36:02.107944: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpm9wcckoh
2024-08-26 20:36:02.112932: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 41506 microseconds.
2024-08-26 20:36:02.143756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-08-26 20:36:02.261960: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3605] Estimated count of arithmetic ops: 0.203 M  ops, equivalently 0.101 M  MACs
WARNING:absl:Reset all op configs under scope_regex .* with OpQuantizationRecipe(regex='.*', operation=<TFLOperationName.ALL_SUPPORTED: '*'>, algorithm_key=<AlgorithmName.MIN_MAX_UNIFORM_QUANT: 'min_max_uniform_quantize'>, op_config=OpQuantizationConfig(activation_tensor_config=None, weight_tensor_config=TensorQuantizationConfig(num_bits=8, symmetric=True, channel_wise=True, dtype=<TensorDataType.INT: 'INT'>), execution_mode=<OpExecutionMode.DRQ: 'DRQ'>)).
I0000 00:00:1724704563.475274       1 cpu_client.cc:470] TfrtCpuClient destroyed.
```
Then the `mnist_conv.tflite` model successfully compiles but none of the operations will run on the Edge TPU:
```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_conv.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 2 ms.

Input model: mnist_conv.tflite
Input size: 23.02KiB
Output model: mnist_conv_edgetpu.tflite
Output size: 22.41KiB
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 0
Total number of operations: 7
Operation log: mnist_conv_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 7
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
```
The log file reports that the operations are using ""an unsupported data type"":
```
Edge TPU Compiler version 16.0.384591198
Input: mnist_conv.tflite
Output: mnist_conv_edgetpu.tflite

Operator                       Count      Status

FULLY_CONNECTED                1          Operation is working on an unsupported data type
DEPTHWISE_CONV_2D              1          Operation is working on an unsupported data type
TRANSPOSE                      1          Operation is working on an unsupported data type
RESHAPE                        3          Operation is working on an unsupported data type
MAX_POOL_2D                    1          Operation is working on an unsupported data type
```

I can ask [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch/issues) directly why this model is not working fully on the Edge TPU.

Part of the [model requirements](https://coral.ai/docs/edgetpu/models-intro/#model-requirements) for Edge TPU is that ""Tensor sizes are constant at compile-time (no dynamic sizes)""

EDIT: Looks like you are already aware that static quantization is not currently supported:
https://github.com/google-ai-edge/ai-edge-torch/issues/150#issuecomment-2299777420

pkgoogle (Assginee) on (2024-08-27 17:55:07 UTC): I see, i see, can you do me a favor and write a comment on that thread that it is blocking this issue? That way we can properly track things.

blocked by https://github.com/google-ai-edge/ai-edge-torch/issues/150

gaikwadrahul8 on (2024-11-26 08:05:48 UTC): Hi, @ADarkDividedGem 
Thanks for raising this issue. Are you aware of [AI-Edge-Torch](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/)? As we believe this issue is better supported by and more relevant to AI-Edge-Torch we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/ai-edge-torch/issues/384

Let us know if you have any questions. Thanks.

"
2470191994,issue,closed,completed,MSVC requires C++20 (instead of C++17),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Windows -  Microsoft Windows Server 2022 - 10.0.20348

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

Visual Studio 17 2022 - MSVC 19.40.33813.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The compilation of tensorflow-lite fails with the error ([log](https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin/actions/runs/10418185216/job/28853936586#step:6:608)):
```
tensorflow\tensorflow\lite\core\c\operator.cc(35,29): error C7555: use of designated initializers requires at least ‘/std:c++20’
```
And same errors are generated by XNNPACK ([log](https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin/actions/runs/10418185216/job/28853936586#step:6:639))

Using `set(CMAKE_CXX_STANDARD 20)` instead of `set(CMAKE_CXX_STANDARD 17)` [here](https://github.com/tensorflow/tensorflow/blob/v2.17.0/tensorflow/lite/CMakeLists.txt#L87) fixes the problem.

P.S. I use CMake 3.30

Here is an open-source project that compiles tensorflow via CMake 
https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin



### Standalone code to reproduce the issue

```shell
Here is an open-source project that compiles tensorflow via CMake 

https://github.com/Ircam-Partiels/basic-pitch-vamp-plugin
```


### Relevant log output

```shell
tensorflow\tensorflow\lite\core\c\operator.cc(35,29): error C7555: use of designated initializers requires at least ‘/std:c++20’
```
",pierreguillot,2024-08-16 12:35:34+00:00,"['pkgoogle', 'sawantkumar']",2024-09-19 02:00:07+00:00,2024-09-19 02:00:03+00:00,https://github.com/tensorflow/tensorflow/issues/73930,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2311618760, 'issue_id': 2470191994, 'author': 'demian-j-lim', 'body': 'i try to code.\r\n:)\r\n\r\n``\r\noperator.cc\r\n`\r\nTfLiteOperator* TfLiteOperatorCreateWithData(TfLiteBuiltinOperator builtin_code,\r\n                                             const char* custom_name,\r\n                                             int version, void* user_data) {\r\n    TfLiteOperator* op = new TfLiteOperator();\r\n    op->custom_name = custom_name;\r\n    op->version = version;\r\n    op->init = nullptr;\r\n    op->free = nullptr;\r\n    op->prepare = nullptr;\r\n    op->invoke = nullptr;\r\n    op->async_kernel = nullptr;\r\n    op->builtin_code = builtin_code;\r\n    op->node_index = -1;\r\n    op->inplace_operator = kTfLiteInplaceOpNone;\r\n    op->user_data = user_data;\r\n\r\n    return op;\r\n}\r\n`', 'created_at': datetime.datetime(2024, 8, 27, 5, 44, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329765512, 'issue_id': 2470191994, 'author': 'pkgoogle', 'body': 'Hi @pierreguillot, We currently only support CLANG 17.0.6 for Windows CPU environments: https://www.tensorflow.org/install/source_windows#cpu. Please ensure your versions match the table, thanks!', 'created_at': datetime.datetime(2024, 9, 4, 18, 56, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102293, 'issue_id': 2470191994, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826475, 'issue_id': 2470191994, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826528, 'issue_id': 2470191994, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73930"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73930"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 5, tzinfo=datetime.timezone.utc)}]","demian-j-lim on (2024-08-27 05:44:26 UTC): i try to code.
:)

``
operator.cc
`
TfLiteOperator* TfLiteOperatorCreateWithData(TfLiteBuiltinOperator builtin_code,
                                             const char* custom_name,
                                             int version, void* user_data) {
    TfLiteOperator* op = new TfLiteOperator();
    op->custom_name = custom_name;
    op->version = version;
    op->init = nullptr;
    op->free = nullptr;
    op->prepare = nullptr;
    op->invoke = nullptr;
    op->async_kernel = nullptr;
    op->builtin_code = builtin_code;
    op->node_index = -1;
    op->inplace_operator = kTfLiteInplaceOpNone;
    op->user_data = user_data;

    return op;
}
`

pkgoogle (Assginee) on (2024-09-04 18:56:22 UTC): Hi @pierreguillot, We currently only support CLANG 17.0.6 for Windows CPU environments: https://www.tensorflow.org/install/source_windows#cpu. Please ensure your versions match the table, thanks!

github-actions[bot] on (2024-09-12 01:58:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 02:00:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73930"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73930"">No</a>

"
2469973253,issue,open,,tritonserver preload trt plugin got warning message and many core files : Failed to compile generated PTX with ptxas. Falling back to compilation by driver.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tritonserver preload trt plugin got warning message and many core  dump files
`
2024-08-16 10:09:14.975649: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.033970: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.701031: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:17.498157: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:18.328719: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
`

![image](https://github.com/user-attachments/assets/c9cf824b-f27e-456d-aa43-af501aae0694)

I have an ensmble model, the first part is the tf model, the second part is the trt model. I have a trt plugin, and I load it as LD_PRELOAD. It won't be a problem if I load the two models separately. But when I load them at the same time this warning comes up and produces a lot of coredump files. Why is that? I don't understand how the trt plugin will affect tf

### Standalone code to reproduce the issue

```shell
LD_PRELOAD=/app/lib/ops/libtrtplugin.so --model-repository=/opt/model-repo-copy --model-control-mode=explicit --load-model=first_model --load-model=second_model  --load-model=ensmble_model  --log-verbose=0 --http-port=xxx--grpc-port=xxx --metrics-port=xxx --backend-config=tensorflow,version=2 --backend-config=tensorrt,version-compatible=true --disable-auto-complete-config 
```


### Relevant log output

_No response_",LinGeLin,2024-08-16 10:23:45+00:00,"['beckerhe', 'Venkat6871']",2024-08-20 08:15:11+00:00,,https://github.com/tensorflow/tensorflow/issues/73922,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2296158179, 'issue_id': 2469973253, 'author': 'Venkat6871', 'body': 'Hi **@LinGeLin** ,\r\n- Apologies for the delay. To expedite troubleshooting, please provide a code snippet that reproduces the issue reported here.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 19, 9, 54, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296218441, 'issue_id': 2469973253, 'author': 'LinGeLin', 'body': ""> Hi **@LinGeLin** ,\r\n> \r\n> * Apologies for the delay. To expedite troubleshooting, please provide a code snippet that reproduces the issue reported here.\r\n> \r\n> Thank you!\r\n\r\nIt's very complicated and not easy to share. This problem arises when the model is deployed. There is no python code, and the model is very complex and has a lot of dependencies. I noticed that version 2.17.0 has a pr that may be related to this warning. Can you help explain this [pr](https://github.com/tensorflow/tensorflow/pull/63212) for me? Should I try to solve this problem with the latest version? (Re-setup software environment, headache(⊙︿⊙))\r\n\r\nThank you！"", 'created_at': datetime.datetime(2024, 8, 19, 10, 18, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296227957, 'issue_id': 2469973253, 'author': 'LinGeLin', 'body': '@beckerhe help!', 'created_at': datetime.datetime(2024, 8, 19, 10, 22, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296601488, 'issue_id': 2469973253, 'author': 'beckerhe', 'body': 'It looks like there is an issue with `ptxas`. Can you try running with `--vmodule=cuda_asm_compiler=3,gpu_backend_lib=2`? This should hopefully give us the necessary debug output to see what version of ptxas if finds and why it fails.', 'created_at': datetime.datetime(2024, 8, 19, 13, 37, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296618451, 'issue_id': 2469973253, 'author': 'LinGeLin', 'body': '> It looks like there is an issue with `ptxas`. Can you try running with `--vmodule=cuda_asm_compiler=3,gpu_backend_lib=2`? This should hopefully give us the necessary debug output to see what version of ptxas if finds and why it fails.\r\n\r\nThe main process is tritonserver and this option is not supported. Is there a macro definition that can be set via export?', 'created_at': datetime.datetime(2024, 8, 19, 13, 45, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296625294, 'issue_id': 2469973253, 'author': 'beckerhe', 'body': '`XLA_FLAGS=--vmodule=cuda_asm_compiler=3,gpu_backend_lib=2` might work.', 'created_at': datetime.datetime(2024, 8, 19, 13, 48, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296670978, 'issue_id': 2469973253, 'author': 'LinGeLin', 'body': '> --vmodule=cuda_asm_compiler=3,gpu_backend_lib=2\r\n\r\nerror occurrd.\r\n 2024-08-19 14:06:41.603289: F external/local_xla/xla/parse_flags_from_env.cc:225] Unknown flags in XLA_FLAGS: --vmodule=cuda_asm_compiler=3,gpu_backend_lib=2', 'created_at': datetime.datetime(2024, 8, 19, 14, 7, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298073854, 'issue_id': 2469973253, 'author': 'beckerhe', 'body': ""Ah that's too bad. In that case let me describe how things work and you might be able to diagnose things.\r\n\r\n1. The CL that you mentioned is most likely not related. It didn't introduce a new behaviour but rather reverted a previous change. `ptxas` is so far the only supported way to compile PTX. More ways are coming but that's not yet enabled.\r\n2. I'm not familiar with the Triton server or with TRT plugins, but my theory is that loading the plugin potentially changes the environment variables and makes TF find a different ptxas binary which then crashes.\r\n3. You can try forcing TF to use a certain path to the CUDA installation and see if that makes a difference. With `XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda/installation` you can set a path to a CUDA toolkit which makes TF look for ptxas in `/path/to/cuda/installation/bin/ptxas`. (Note that `bin` is not part of the specificed path.). Example: `XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-12.4`  - so the full path to ptxas would be `/usr/local/cuda-12.4/bin/ptxas`.\r\n4. If that doesn't help the next step would be to look at the coredump files. I would assume they come from `ptxas`. If not then it get's more interesting.\r\n\r\nI hope that helps."", 'created_at': datetime.datetime(2024, 8, 20, 6, 34, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298125639, 'issue_id': 2469973253, 'author': 'LinGeLin', 'body': '> Ah that\'s too bad. In that case let me describe how things work and you might be able to diagnose things.啊，那太糟糕了。在这种情况下，让我描述一下事情是如何运作的，你也许能够诊断事情。\r\n> \r\n> 1. The CL that you mentioned is most likely not related. It didn\'t introduce a new behaviour but rather reverted a previous change. `ptxas` is so far the only supported way to compile PTX. More ways are coming but that\'s not yet enabled.您提到的 CL 很可能不相关。它没有引入新的行为，而是恢复了之前的更改。 `ptxas`是迄今为止唯一受支持的 PTX 编译方式。更多方法即将推出，但尚未启用。\r\n> 2. I\'m not familiar with the Triton server or with TRT plugins, but my theory is that loading the plugin potentially changes the environment variables and makes TF find a different ptxas binary which then crashes.我不熟悉 Triton 服务器或 TRT 插件，但我的理论是加载插件可能会更改环境变量，并使 TF 找到不同的 ptxas 二进制文件，然后崩溃。\r\n> 3. You can try forcing TF to use a certain path to the CUDA installation and see if that makes a difference. With `XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda/installation` you can set a path to a CUDA toolkit which makes TF look for ptxas in `/path/to/cuda/installation/bin/ptxas`. (Note that `bin` is not part of the specificed path.). Example: `XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-12.4`  - so the full path to ptxas would be `/usr/local/cuda-12.4/bin/ptxas`.您可以尝试强制 TF 使用 CUDA 安装的特定路径，看看是否有影响。使用`XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda/installation`您可以设置 CUDA 工具包的路径，使 TF 在`/path/to/cuda/installation/bin/ptxas`中查找 ptxas 。 （请注意， `bin`不是特定路径的一部分。）。示例： `XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-12.4` - 因此 ptxas 的完整路径为`/usr/local/cuda-12.4/bin/ptxas` 。\r\n> 4. If that doesn\'t help the next step would be to look at the coredump files. I would assume they come from `ptxas`. If not then it get\'s more interesting.如果这没有帮助，下一步就是查看核心转储文件。我假设它们来自`ptxas` 。如果没有的话那就更有趣了。\r\n> \r\n> I hope that helps.\xa0我希望这有帮助。\r\n\r\nThe core file looks like it came from nvinfer.\r\n\r\n```shell\r\n#0  __pthread_kill_implementation (no_tid=0, signo=6, threadid=140200996581376) at ./nptl/pthread_kill.c:44\r\n#1  __pthread_kill_internal (signo=6, threadid=140200996581376) at ./nptl/pthread_kill.c:78\r\n#2  __GI___pthread_kill (threadid=140200996581376, signo=signo@entry=6) at ./nptl/pthread_kill.c:89\r\n#3  0x00007f83256da476 in __GI_raise (sig=sig@entry=6) at ../sysdeps/posix/raise.c:26\r\n#4  0x00007f83256c07f3 in __GI_abort () at ./stdlib/abort.c:79\r\n#5  0x00007f8325721676 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7f8325873b77 ""%s\\n"") at ../sysdeps/posix/libc_fatal.c:155\r\n#6  0x00007f8325738cfc in malloc_printerr (str=str@entry=0x7f83258765b8 ""malloc_consolidate(): unaligned fastbin chunk detected"") at ./malloc/malloc.c:5664\r\n#7  0x00007f832573998c in malloc_consolidate (av=av@entry=0x7f83258b2c80 <main_arena>) at ./malloc/malloc.c:4750\r\n#8  0x00007f832573bbdb in _int_malloc (av=av@entry=0x7f83258b2c80 <main_arena>, bytes=bytes@entry=1864) at ./malloc/malloc.c:3965\r\n#9  0x00007f832573e5f9 in __libc_calloc (n=<optimized out>, elem_size=<optimized out>) at ./malloc/malloc.c:3679\r\n#10 0x00007f83181f3064 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10\r\n#11 0x00007f83181f5b49 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10\r\n#12 0x00007f83256dda56 in __cxa_finalize (d=0x7f832330f000) at ./stdlib/cxa_finalize.c:83\r\n#13 0x00007f831723def3 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10\r\n#14 0x00007ffe3a65db10 in ?? ()\r\n#15 0x00007f833883724e in _dl_fini () at ./elf/dl-fini.c:142\r\n```', 'created_at': datetime.datetime(2024, 8, 20, 7, 7, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298249589, 'issue_id': 2469973253, 'author': 'beckerhe', 'body': 'Okay, so this indicates some memory corruption issue - not necessarily triggered but discovered by nvinfer/TensorRT.\r\n\r\nAre you able to compile your TRT plugin with address sanitizer enabled? This is probably the quickest way to find out where this is coming from.', 'created_at': datetime.datetime(2024, 8, 20, 8, 15, 10, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-19 09:54:56 UTC): Hi **@LinGeLin** ,
- Apologies for the delay. To expedite troubleshooting, please provide a code snippet that reproduces the issue reported here.

Thank you!

LinGeLin (Issue Creator) on (2024-08-19 10:18:21 UTC): It's very complicated and not easy to share. This problem arises when the model is deployed. There is no python code, and the model is very complex and has a lot of dependencies. I noticed that version 2.17.0 has a pr that may be related to this warning. Can you help explain this [pr](https://github.com/tensorflow/tensorflow/pull/63212) for me? Should I try to solve this problem with the latest version? (Re-setup software environment, headache(⊙︿⊙))

Thank you！

LinGeLin (Issue Creator) on (2024-08-19 10:22:56 UTC): @beckerhe help!

beckerhe (Assginee) on (2024-08-19 13:37:40 UTC): It looks like there is an issue with `ptxas`. Can you try running with `--vmodule=cuda_asm_compiler=3,gpu_backend_lib=2`? This should hopefully give us the necessary debug output to see what version of ptxas if finds and why it fails.

LinGeLin (Issue Creator) on (2024-08-19 13:45:10 UTC): The main process is tritonserver and this option is not supported. Is there a macro definition that can be set via export?

beckerhe (Assginee) on (2024-08-19 13:48:02 UTC): `XLA_FLAGS=--vmodule=cuda_asm_compiler=3,gpu_backend_lib=2` might work.

LinGeLin (Issue Creator) on (2024-08-19 14:07:43 UTC): error occurrd.
 2024-08-19 14:06:41.603289: F external/local_xla/xla/parse_flags_from_env.cc:225] Unknown flags in XLA_FLAGS: --vmodule=cuda_asm_compiler=3,gpu_backend_lib=2

beckerhe (Assginee) on (2024-08-20 06:34:12 UTC): Ah that's too bad. In that case let me describe how things work and you might be able to diagnose things.

1. The CL that you mentioned is most likely not related. It didn't introduce a new behaviour but rather reverted a previous change. `ptxas` is so far the only supported way to compile PTX. More ways are coming but that's not yet enabled.
2. I'm not familiar with the Triton server or with TRT plugins, but my theory is that loading the plugin potentially changes the environment variables and makes TF find a different ptxas binary which then crashes.
3. You can try forcing TF to use a certain path to the CUDA installation and see if that makes a difference. With `XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda/installation` you can set a path to a CUDA toolkit which makes TF look for ptxas in `/path/to/cuda/installation/bin/ptxas`. (Note that `bin` is not part of the specificed path.). Example: `XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-12.4`  - so the full path to ptxas would be `/usr/local/cuda-12.4/bin/ptxas`.
4. If that doesn't help the next step would be to look at the coredump files. I would assume they come from `ptxas`. If not then it get's more interesting.

I hope that helps.

LinGeLin (Issue Creator) on (2024-08-20 07:07:53 UTC): The core file looks like it came from nvinfer.

```shell
#0  __pthread_kill_implementation (no_tid=0, signo=6, threadid=140200996581376) at ./nptl/pthread_kill.c:44
#1  __pthread_kill_internal (signo=6, threadid=140200996581376) at ./nptl/pthread_kill.c:78
#2  __GI___pthread_kill (threadid=140200996581376, signo=signo@entry=6) at ./nptl/pthread_kill.c:89
#3  0x00007f83256da476 in __GI_raise (sig=sig@entry=6) at ../sysdeps/posix/raise.c:26
#4  0x00007f83256c07f3 in __GI_abort () at ./stdlib/abort.c:79
#5  0x00007f8325721676 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7f8325873b77 ""%s\n"") at ../sysdeps/posix/libc_fatal.c:155
#6  0x00007f8325738cfc in malloc_printerr (str=str@entry=0x7f83258765b8 ""malloc_consolidate(): unaligned fastbin chunk detected"") at ./malloc/malloc.c:5664
#7  0x00007f832573998c in malloc_consolidate (av=av@entry=0x7f83258b2c80 <main_arena>) at ./malloc/malloc.c:4750
#8  0x00007f832573bbdb in _int_malloc (av=av@entry=0x7f83258b2c80 <main_arena>, bytes=bytes@entry=1864) at ./malloc/malloc.c:3965
#9  0x00007f832573e5f9 in __libc_calloc (n=<optimized out>, elem_size=<optimized out>) at ./malloc/malloc.c:3679
#10 0x00007f83181f3064 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10
#11 0x00007f83181f5b49 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10
#12 0x00007f83256dda56 in __cxa_finalize (d=0x7f832330f000) at ./stdlib/cxa_finalize.c:83
#13 0x00007f831723def3 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.10
#14 0x00007ffe3a65db10 in ?? ()
#15 0x00007f833883724e in _dl_fini () at ./elf/dl-fini.c:142
```

beckerhe (Assginee) on (2024-08-20 08:15:10 UTC): Okay, so this indicates some memory corruption issue - not necessarily triggered but discovered by nvinfer/TensorRT.

Are you able to compile your TRT plugin with address sanitizer enabled? This is probably the quickest way to find out where this is coming from.

"
2469967026,issue,closed,completed,How to achieve transfer training in TensorFlow2?,"I use TensorFlow2 train and save a checkpoint model with tf.train.Checkpoint, and I now want to conduct transfer training in a new dataset. I want to restore all vars except for the final layer for the new model from the saved checkpoint file, because the vocabulary size have been changed, (Exactly what lots of us needed. For Keras it was as simple as load the weight and ""skip_name"" to skip the mismatches as we have to replace the top layers for examples for different classification.) How can I achieve it?

I try to use the following code to do it.

		restore_vars = []
		for var in new_model.variables:
			var_shape = var.shape.as_list()
			if vocab_size in var_shape or (vocab_size+1) in var_shape:
				continue
			restore_vars.append(var)
		print(restore_vars[0])
		restore_dict = {var.name.split(':')[0]: var for var in restore_vars}
		restore_ckpt = tf.train.Checkpoint(**restore_dict)
		restore_ckpt.restore(checkpoint_path).expect_partial()

But it seems do not work, the value of the variables in the new_model did not change at all. Is there any solution?",yjiangling,2024-08-16 10:19:50+00:00,['tilakrayal'],2024-12-09 09:35:47+00:00,2024-12-09 09:35:44+00:00,https://github.com/tensorflow/tensorflow/issues/73921,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:model', 'Model related issues')]","[{'comment_id': 2293855285, 'issue_id': 2469967026, 'author': 'mamunur-cou', 'body': 'import tensorflow as tf\r\n\r\ncheckpoint_path = \'path/to/your/checkpoint\'\r\nvocab_size = new_vocab_size  \r\n\r\nrestore_vars = []\r\nfor var in new_model.variables:\r\n    var_shape = var.shape.as_list()\r\n    if vocab_size in var_shape or (vocab_size+1) in var_shape:\r\n        print(f""Skipping variable: {var.name} due to shape: {var_shape}"")\r\n        continue\r\n    restore_vars.append(var)\r\n\r\nrestore_dict = {var.name.split(\':\')[0]: var for var in restore_vars}\r\n\r\nrestore_ckpt = tf.train.Checkpoint(**restore_dict)\r\nrestore_ckpt.restore(checkpoint_path).expect_partial()\r\nfor var in restore_vars:\r\n    print(f""Restored variable: {var.name} with value: {var.numpy()}"")', 'created_at': datetime.datetime(2024, 8, 16, 17, 4, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295777174, 'issue_id': 2469967026, 'author': 'yjiangling', 'body': '> import tensorflow as tf\r\n> \r\n> checkpoint_path = \'path/to/your/checkpoint\' vocab_size = new_vocab_size\r\n> \r\n> restore_vars = [] for var in new_model.variables: var_shape = var.shape.as_list() if vocab_size in var_shape or (vocab_size+1) in var_shape: print(f""Skipping variable: {var.name} due to shape: {var_shape}"") continue restore_vars.append(var)\r\n> \r\n> restore_dict = {var.name.split(\':\')[0]: var for var in restore_vars}\r\n> \r\n> restore_ckpt = tf.train.Checkpoint(**restore_dict) restore_ckpt.restore(checkpoint_path).expect_partial() for var in restore_vars: print(f""Restored variable: {var.name} with value: {var.numpy()}"")\r\n\r\nThanks a lot for the help, but this solution seems did not work at all, the value of the variables seems did not change at all before and after restore from the checkpoint model. And I checked the value of the variable with the old model to restore all parameters like this:\r\n\r\n\r\n    ckpt = tf.train.Checkpoint(model=old_model) \r\n    ckpt.restore(checkpoint_path) \r\n    all_vars = [var for var in old_model.variables] \r\n    for var in all_vars: \r\n        print(f""Restored variable: {var.name} with value: {var.numpy()}"") \r\n\r\n\r\nThe value of the same variable is quite different, so, the method above really restore the value of the variables from the checkpoint file?', 'created_at': datetime.datetime(2024, 8, 19, 6, 38, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298014043, 'issue_id': 2469967026, 'author': 'yjiangling', 'body': 'xs_sig = tf.TensorSpec(shape=(None, None, self.hp.n_mels), dtype=tf.float32)\r\n\t\txlen_sig = tf.TensorSpec(shape=(None,), dtype=tf.int32)\r\n\t\tys_sig = tf.TensorSpec(shape=(None, None), dtype=tf.int32)\r\n\t\tylen_sig = tf.TensorSpec(shape=(None,), dtype=tf.int32)\r\n\t\tstate_sig = tf.TensorSpec(shape=(None), dtype=tf.bool)\r\n\r\n\t\ttrain_fun = tf.function(self.train_step).get_concrete_function(xs_sig, xlen_sig, ys_sig, ylen_sig, state_sig)\r\n\r\n\t\trestore_vars = []\r\n\t\tfor var in self.model.variables:\r\n\t\t\tvar_shape = var.shape.as_list()\r\n\t\t\tif self.hp.vocab_size in var_shape or (self.hp.vocab_size+1) in var_shape:\r\n\t\t\t\tcontinue\r\n\t\t\trestore_vars.append(var)\r\n\r\n\t\tvar = restore_vars[0]\r\n\t\tprint(f""Before restore, variable: {var.name} with value: {var.numpy()}"")\r\n\r\n\t\trestore_dict = {var.name.split(\':\')[0]: var for var in restore_vars}\r\n\t\trestore_ckpt = tf.train.Checkpoint(**restore_dict)\r\n\t\t\r\n\t\trestore_ckpt.restore(\'./model/ckpt-111\').expect_partial()\r\n\t\tvar = restore_vars[0]\r\n\t\tprint(f""After restore, variable: {var.name} with value: {var.numpy()}"")\r\n\r\nBefore and After restore from the checkpoint, the variable have the same value like this:\r\n\r\nBefore restore, variable: conv2d/kernel:0 with value: [[[[ 0.06563459  0.00840305 -0.09779331  0.10958676 -0.04209604\r\n     0.08968563 -0.01759356 -0.09149572 -0.1270998   0.067333\r\n    -0.07290326  0.13000713  0.08546856 -0.07255595  0.09380624\r\n     0.05008008  0.04844034  0.06551427 -0.03832343 -0.03300031\r\n     0.00607258  0.13278274 -0.03660216 -0.04040344  0.05728079\r\n     0.06849486 -0.07564054 -0.09481299  0.10913198  0.12305556\r\n    -0.10410744  0.09862402]]\r\n\r\n  [[ 0.08348374  0.04252926  0.03711365 -0.13923089 -0.00691505\r\n    -0.11210923  0.01271646  0.08069353 -0.0500799  -0.03470477\r\n     0.10451779  0.03921932  0.00808498 -0.12515476 -0.02671719\r\n     0.01703154 -0.0797071  -0.1370279   0.05244619 -0.08127043\r\n     0.00482281 -0.01106107 -0.01175928  0.04740907 -0.01123694\r\n    -0.11811949  0.08805212 -0.00259446  0.00244816  0.05788592\r\n    -0.09701566  0.02189627]]\r\n\r\n  [[-0.09813957  0.13127281 -0.09860488 -0.02824788  0.01669799\r\n     0.05194671  0.10739563  0.10136569  0.09967132 -0.07614696\r\n     0.01958828 -0.06805683  0.05184881  0.02014206  0.10933466\r\n     0.08968377 -0.04896273  0.07374194 -0.02363124  0.01056857\r\n    -0.1301135  -0.07710255  0.06642468  0.09275301 -0.02652738\r\n     0.11004217 -0.02997959  0.01762064  0.11606161 -0.09514837\r\n    -0.10670003  0.13081704]]]\r\n\r\n\r\n [[[ 0.01213205  0.10840206 -0.03927455  0.01651397 -0.01690453\r\n     0.07221566 -0.07510997 -0.08614457 -0.13383651  0.03703724\r\n    -0.01236174  0.04353252 -0.05596335  0.01667911  0.04289125\r\n    -0.02056064  0.07331941  0.11524598  0.02641955  0.01384376\r\n     0.06697597  0.07333417 -0.10861287  0.07806718 -0.05407285\r\n    -0.04556756  0.12141882  0.08252585  0.0860115  -0.03831191\r\n     0.13593917  0.03157572]]\r\n\r\n  [[-0.01660317  0.10147165  0.09894487 -0.12096284 -0.11507599\r\n    -0.04978275 -0.05566155  0.05712175  0.02109195  0.12883337\r\n     0.02091907 -0.06986401  0.09280713 -0.07688103  0.00173971\r\n     0.08652124 -0.0723936   0.04379457 -0.00285    -0.04172704\r\n    -0.01366517 -0.09906477  0.12725525 -0.13293567 -0.01025383\r\n     0.13878314  0.10983334 -0.04301392  0.0713973   0.13531612\r\n    -0.04094394  0.05427051]]\r\n\r\n  [[-0.00552706  0.08048162  0.11622094 -0.02093116 -0.01610628\r\n     0.01292579  0.12779085  0.00761476 -0.06791268  0.04948291\r\n     0.14023949 -0.10729912 -0.00996061 -0.08164443  0.01273449\r\n     0.07548277  0.04569027 -0.02749155 -0.08140583 -0.13028833\r\n    -0.12789097 -0.10573255  0.02804492 -0.11411302  0.01505966\r\n    -0.04191986  0.12602548  0.01951379 -0.01220857  0.0426316\r\n    -0.06860761 -0.1292061 ]]]\r\n\r\n\r\n [[[ 0.08118929 -0.05359229  0.00344963 -0.00117226  0.13679491\r\n    -0.08211896 -0.07821181  0.02441108 -0.07916161  0.00925118\r\n    -0.072765   -0.12557067 -0.09301825  0.10492741  0.01565959\r\n     0.07305914 -0.11428531 -0.13343579 -0.01604109  0.00415377\r\n     0.12891085  0.05635801 -0.12451731  0.04226863 -0.0437835\r\n    -0.14166194 -0.01247315 -0.07749605  0.1182047  -0.09100856\r\n    -0.11602301  0.00513332]]\r\n\r\n  [[ 0.05327897 -0.05282569 -0.13847588  0.11579688 -0.03227763\r\n     0.05073091 -0.11563087 -0.08239145 -0.05441392 -0.01218514\r\n    -0.02980805  0.03019193 -0.06550806 -0.11074035  0.00024992\r\n    -0.11976698 -0.00537026  0.07806922  0.12875895  0.04991359\r\n    -0.03856942 -0.06781539 -0.05026716 -0.00725389 -0.10366416\r\n     0.01443329 -0.05533047  0.00477575  0.0038531   0.0348838\r\n    -0.10772922 -0.00994276]]\r\n\r\n  [[ 0.1272202   0.02450709  0.09173717 -0.11343144 -0.02958456\r\n     0.08422862 -0.06543145  0.09191065  0.12058292 -0.06990939\r\n    -0.08898731 -0.03733413 -0.05812452 -0.12990645  0.0638507\r\n    -0.13532798  0.10505985 -0.11757522 -0.08331738 -0.13327062\r\n     0.01881111 -0.01903669 -0.01580404 -0.07075823 -0.06582467\r\n    -0.06637978  0.07962956 -0.00432335 -0.06549603 -0.08766235\r\n     0.05466402 -0.12913026]]]]\r\n\r\nAfter restore, variable: conv2d/kernel:0 with value: [[[[ 0.06563459  0.00840305 -0.09779331  0.10958676 -0.04209604\r\n     0.08968563 -0.01759356 -0.09149572 -0.1270998   0.067333\r\n    -0.07290326  0.13000713  0.08546856 -0.07255595  0.09380624\r\n     0.05008008  0.04844034  0.06551427 -0.03832343 -0.03300031\r\n     0.00607258  0.13278274 -0.03660216 -0.04040344  0.05728079\r\n     0.06849486 -0.07564054 -0.09481299  0.10913198  0.12305556\r\n    -0.10410744  0.09862402]]\r\n\r\n  [[ 0.08348374  0.04252926  0.03711365 -0.13923089 -0.00691505\r\n    -0.11210923  0.01271646  0.08069353 -0.0500799  -0.03470477\r\n     0.10451779  0.03921932  0.00808498 -0.12515476 -0.02671719\r\n     0.01703154 -0.0797071  -0.1370279   0.05244619 -0.08127043\r\n     0.00482281 -0.01106107 -0.01175928  0.04740907 -0.01123694\r\n    -0.11811949  0.08805212 -0.00259446  0.00244816  0.05788592\r\n    -0.09701566  0.02189627]]\r\n\r\n  [[-0.09813957  0.13127281 -0.09860488 -0.02824788  0.01669799\r\n     0.05194671  0.10739563  0.10136569  0.09967132 -0.07614696\r\n     0.01958828 -0.06805683  0.05184881  0.02014206  0.10933466\r\n     0.08968377 -0.04896273  0.07374194 -0.02363124  0.01056857\r\n    -0.1301135  -0.07710255  0.06642468  0.09275301 -0.02652738\r\n     0.11004217 -0.02997959  0.01762064  0.11606161 -0.09514837\r\n    -0.10670003  0.13081704]]]\r\n\r\n\r\n [[[ 0.01213205  0.10840206 -0.03927455  0.01651397 -0.01690453\r\n     0.07221566 -0.07510997 -0.08614457 -0.13383651  0.03703724\r\n    -0.01236174  0.04353252 -0.05596335  0.01667911  0.04289125\r\n    -0.02056064  0.07331941  0.11524598  0.02641955  0.01384376\r\n     0.06697597  0.07333417 -0.10861287  0.07806718 -0.05407285\r\n    -0.04556756  0.12141882  0.08252585  0.0860115  -0.03831191\r\n     0.13593917  0.03157572]]\r\n\r\n  [[-0.01660317  0.10147165  0.09894487 -0.12096284 -0.11507599\r\n    -0.04978275 -0.05566155  0.05712175  0.02109195  0.12883337\r\n     0.02091907 -0.06986401  0.09280713 -0.07688103  0.00173971\r\n     0.08652124 -0.0723936   0.04379457 -0.00285    -0.04172704\r\n    -0.01366517 -0.09906477  0.12725525 -0.13293567 -0.01025383\r\n     0.13878314  0.10983334 -0.04301392  0.0713973   0.13531612\r\n    -0.04094394  0.05427051]]\r\n\r\n  [[-0.00552706  0.08048162  0.11622094 -0.02093116 -0.01610628\r\n     0.01292579  0.12779085  0.00761476 -0.06791268  0.04948291\r\n     0.14023949 -0.10729912 -0.00996061 -0.08164443  0.01273449\r\n     0.07548277  0.04569027 -0.02749155 -0.08140583 -0.13028833\r\n    -0.12789097 -0.10573255  0.02804492 -0.11411302  0.01505966\r\n    -0.04191986  0.12602548  0.01951379 -0.01220857  0.0426316\r\n    -0.06860761 -0.1292061 ]]]\r\n\r\n\r\n [[[ 0.08118929 -0.05359229  0.00344963 -0.00117226  0.13679491\r\n    -0.08211896 -0.07821181  0.02441108 -0.07916161  0.00925118\r\n    -0.072765   -0.12557067 -0.09301825  0.10492741  0.01565959\r\n     0.07305914 -0.11428531 -0.13343579 -0.01604109  0.00415377\r\n     0.12891085  0.05635801 -0.12451731  0.04226863 -0.0437835\r\n    -0.14166194 -0.01247315 -0.07749605  0.1182047  -0.09100856\r\n    -0.11602301  0.00513332]]\r\n\r\n  [[ 0.05327897 -0.05282569 -0.13847588  0.11579688 -0.03227763\r\n     0.05073091 -0.11563087 -0.08239145 -0.05441392 -0.01218514\r\n    -0.02980805  0.03019193 -0.06550806 -0.11074035  0.00024992\r\n    -0.11976698 -0.00537026  0.07806922  0.12875895  0.04991359\r\n    -0.03856942 -0.06781539 -0.05026716 -0.00725389 -0.10366416\r\n     0.01443329 -0.05533047  0.00477575  0.0038531   0.0348838\r\n    -0.10772922 -0.00994276]]\r\n\r\n  [[ 0.1272202   0.02450709  0.09173717 -0.11343144 -0.02958456\r\n     0.08422862 -0.06543145  0.09191065  0.12058292 -0.06990939\r\n    -0.08898731 -0.03733413 -0.05812452 -0.12990645  0.0638507\r\n    -0.13532798  0.10505985 -0.11757522 -0.08331738 -0.13327062\r\n     0.01881111 -0.01903669 -0.01580404 -0.07075823 -0.06582467\r\n    -0.06637978  0.07962956 -0.00432335 -0.06549603 -0.08766235\r\n     0.05466402 -0.12913026]]]]', 'created_at': datetime.datetime(2024, 8, 20, 5, 45, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298835524, 'issue_id': 2469967026, 'author': 'tilakrayal', 'body': '@yjiangling,\r\nI tried to execute with the toy example where eliminating the last layer after loading the model weights from checkpoints. Kindly find the gist of it [here](https://colab.research.google.com/gist/kiransair/6c96e04005582293b7b16b7a8df8d31f/gist_73921.ipynb).\r\n\r\nAlso this question is better asked on [Google AI Forum](https://discuss.ai.google.dev/c/tensorflow/21) since it is not a bug or feature request. Thank you!', 'created_at': datetime.datetime(2024, 8, 20, 13, 15, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306066077, 'issue_id': 2469967026, 'author': 'yjiangling', 'body': '> @yjiangling, I tried to execute with the toy example where eliminating the last layer after loading the model weights from checkpoints. Kindly find the gist of it [here](https://colab.research.google.com/gist/kiransair/6c96e04005582293b7b16b7a8df8d31f/gist_73921.ipynb).\r\n> \r\n> Also this question is better asked on [Google AI Forum](https://discuss.ai.google.dev/c/tensorflow/21) since it is not a bug or feature request. Thank you!\r\n\r\nThanks a lot for the help, problem have been resolved. The issue will be closed as soon.', 'created_at': datetime.datetime(2024, 8, 23, 2, 24, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306066109, 'issue_id': 2469967026, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">No</a>', 'created_at': datetime.datetime(2024, 8, 23, 2, 24, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507035413, 'issue_id': 2469967026, 'author': 'yjiangling', 'body': ""For example, there is a saved checkpoint model trained with SourceModel, how to transfer the compatible parameters to the the TargetModel and start a new train?\r\n\r\n\t# 自定义层示例\r\n\tclass CustomLayer(tf.keras.layers.Layer):\r\n\t    def __init__(self, unit1, unit2, **kwargs):\r\n\t        super(CustomLayer, self).__init__(**kwargs)\r\n\t        self.dense1 = tf.keras.layers.Dense(unit1, activation='relu')\r\n\t        self.dense2 = tf.keras.layers.Dense(unit2, activation='softmax')\r\n\t \r\n\t    def call(self, inputs):\r\n\t        x = self.dense1(inputs)\r\n\t        return self.dense2(x)\r\n\r\n\t# 源模型示例\r\n\tclass SourceModel(tf.keras.Model):\r\n\t    def __init__(self):\r\n\t        super(SourceModel, self).__init__()\r\n\t        self.layer = tf.keras.layers.Dense(128, activation='relu')\r\n\t        self.custom_layer = CustomLayer(64, 10)\r\n\t \r\n\t    def call(self, inputs):\r\n\t        x = self.layer(inputs)\r\n\t        x = self.custom_layer(x)\r\n\t        return x\r\n\r\n\t# 目标模型示例\r\n\tclass TargetModel(tf.keras.Model):\r\n\t    def __init__(self):\r\n\t        super(SourceModel, self).__init__()\r\n\t        self.layer = tf.keras.layers.Dense(128, activation='relu')\r\n\t        self.custom_layer = CustomLayer(64, 5)\r\n\t \r\n\t    def call(self, inputs):\r\n\t        x = self.layer(inputs)\r\n\t        x = self.custom_layer(x)\r\n\t        return x\r\n\r\nHow to get the layer in the CustomLayer and transfer the weight to the TargetModel?"", 'created_at': datetime.datetime(2024, 11, 29, 3, 40, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523297179, 'issue_id': 2469967026, 'author': 'tilakrayal', 'body': '@yjiangling,\r\nCould you please feel free to move this issue closed status and raise the new request with the required details to analyse in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 12, 6, 13, 50, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527401361, 'issue_id': 2469967026, 'author': 'yjiangling', 'body': '> Contributor\r\n\r\nOkay, Sorry, I will close it right now.', 'created_at': datetime.datetime(2024, 12, 9, 9, 35, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527401430, 'issue_id': 2469967026, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">No</a>', 'created_at': datetime.datetime(2024, 12, 9, 9, 35, 46, tzinfo=datetime.timezone.utc)}]","mamunur-cou on (2024-08-16 17:04:22 UTC): import tensorflow as tf

checkpoint_path = 'path/to/your/checkpoint'
vocab_size = new_vocab_size  

restore_vars = []
for var in new_model.variables:
    var_shape = var.shape.as_list()
    if vocab_size in var_shape or (vocab_size+1) in var_shape:
        print(f""Skipping variable: {var.name} due to shape: {var_shape}"")
        continue
    restore_vars.append(var)

restore_dict = {var.name.split(':')[0]: var for var in restore_vars}

restore_ckpt = tf.train.Checkpoint(**restore_dict)
restore_ckpt.restore(checkpoint_path).expect_partial()
for var in restore_vars:
    print(f""Restored variable: {var.name} with value: {var.numpy()}"")

yjiangling (Issue Creator) on (2024-08-19 06:38:52 UTC): Thanks a lot for the help, but this solution seems did not work at all, the value of the variables seems did not change at all before and after restore from the checkpoint model. And I checked the value of the variable with the old model to restore all parameters like this:


    ckpt = tf.train.Checkpoint(model=old_model) 
    ckpt.restore(checkpoint_path) 
    all_vars = [var for var in old_model.variables] 
    for var in all_vars: 
        print(f""Restored variable: {var.name} with value: {var.numpy()}"") 


The value of the same variable is quite different, so, the method above really restore the value of the variables from the checkpoint file?

yjiangling (Issue Creator) on (2024-08-20 05:45:17 UTC): xs_sig = tf.TensorSpec(shape=(None, None, self.hp.n_mels), dtype=tf.float32)
		xlen_sig = tf.TensorSpec(shape=(None,), dtype=tf.int32)
		ys_sig = tf.TensorSpec(shape=(None, None), dtype=tf.int32)
		ylen_sig = tf.TensorSpec(shape=(None,), dtype=tf.int32)
		state_sig = tf.TensorSpec(shape=(None), dtype=tf.bool)

		train_fun = tf.function(self.train_step).get_concrete_function(xs_sig, xlen_sig, ys_sig, ylen_sig, state_sig)

		restore_vars = []
		for var in self.model.variables:
			var_shape = var.shape.as_list()
			if self.hp.vocab_size in var_shape or (self.hp.vocab_size+1) in var_shape:
				continue
			restore_vars.append(var)

		var = restore_vars[0]
		print(f""Before restore, variable: {var.name} with value: {var.numpy()}"")

		restore_dict = {var.name.split(':')[0]: var for var in restore_vars}
		restore_ckpt = tf.train.Checkpoint(**restore_dict)
		
		restore_ckpt.restore('./model/ckpt-111').expect_partial()
		var = restore_vars[0]
		print(f""After restore, variable: {var.name} with value: {var.numpy()}"")

Before and After restore from the checkpoint, the variable have the same value like this:

Before restore, variable: conv2d/kernel:0 with value: [[[[ 0.06563459  0.00840305 -0.09779331  0.10958676 -0.04209604
     0.08968563 -0.01759356 -0.09149572 -0.1270998   0.067333
    -0.07290326  0.13000713  0.08546856 -0.07255595  0.09380624
     0.05008008  0.04844034  0.06551427 -0.03832343 -0.03300031
     0.00607258  0.13278274 -0.03660216 -0.04040344  0.05728079
     0.06849486 -0.07564054 -0.09481299  0.10913198  0.12305556
    -0.10410744  0.09862402]]

  [[ 0.08348374  0.04252926  0.03711365 -0.13923089 -0.00691505
    -0.11210923  0.01271646  0.08069353 -0.0500799  -0.03470477
     0.10451779  0.03921932  0.00808498 -0.12515476 -0.02671719
     0.01703154 -0.0797071  -0.1370279   0.05244619 -0.08127043
     0.00482281 -0.01106107 -0.01175928  0.04740907 -0.01123694
    -0.11811949  0.08805212 -0.00259446  0.00244816  0.05788592
    -0.09701566  0.02189627]]

  [[-0.09813957  0.13127281 -0.09860488 -0.02824788  0.01669799
     0.05194671  0.10739563  0.10136569  0.09967132 -0.07614696
     0.01958828 -0.06805683  0.05184881  0.02014206  0.10933466
     0.08968377 -0.04896273  0.07374194 -0.02363124  0.01056857
    -0.1301135  -0.07710255  0.06642468  0.09275301 -0.02652738
     0.11004217 -0.02997959  0.01762064  0.11606161 -0.09514837
    -0.10670003  0.13081704]]]


 [[[ 0.01213205  0.10840206 -0.03927455  0.01651397 -0.01690453
     0.07221566 -0.07510997 -0.08614457 -0.13383651  0.03703724
    -0.01236174  0.04353252 -0.05596335  0.01667911  0.04289125
    -0.02056064  0.07331941  0.11524598  0.02641955  0.01384376
     0.06697597  0.07333417 -0.10861287  0.07806718 -0.05407285
    -0.04556756  0.12141882  0.08252585  0.0860115  -0.03831191
     0.13593917  0.03157572]]

  [[-0.01660317  0.10147165  0.09894487 -0.12096284 -0.11507599
    -0.04978275 -0.05566155  0.05712175  0.02109195  0.12883337
     0.02091907 -0.06986401  0.09280713 -0.07688103  0.00173971
     0.08652124 -0.0723936   0.04379457 -0.00285    -0.04172704
    -0.01366517 -0.09906477  0.12725525 -0.13293567 -0.01025383
     0.13878314  0.10983334 -0.04301392  0.0713973   0.13531612
    -0.04094394  0.05427051]]

  [[-0.00552706  0.08048162  0.11622094 -0.02093116 -0.01610628
     0.01292579  0.12779085  0.00761476 -0.06791268  0.04948291
     0.14023949 -0.10729912 -0.00996061 -0.08164443  0.01273449
     0.07548277  0.04569027 -0.02749155 -0.08140583 -0.13028833
    -0.12789097 -0.10573255  0.02804492 -0.11411302  0.01505966
    -0.04191986  0.12602548  0.01951379 -0.01220857  0.0426316
    -0.06860761 -0.1292061 ]]]


 [[[ 0.08118929 -0.05359229  0.00344963 -0.00117226  0.13679491
    -0.08211896 -0.07821181  0.02441108 -0.07916161  0.00925118
    -0.072765   -0.12557067 -0.09301825  0.10492741  0.01565959
     0.07305914 -0.11428531 -0.13343579 -0.01604109  0.00415377
     0.12891085  0.05635801 -0.12451731  0.04226863 -0.0437835
    -0.14166194 -0.01247315 -0.07749605  0.1182047  -0.09100856
    -0.11602301  0.00513332]]

  [[ 0.05327897 -0.05282569 -0.13847588  0.11579688 -0.03227763
     0.05073091 -0.11563087 -0.08239145 -0.05441392 -0.01218514
    -0.02980805  0.03019193 -0.06550806 -0.11074035  0.00024992
    -0.11976698 -0.00537026  0.07806922  0.12875895  0.04991359
    -0.03856942 -0.06781539 -0.05026716 -0.00725389 -0.10366416
     0.01443329 -0.05533047  0.00477575  0.0038531   0.0348838
    -0.10772922 -0.00994276]]

  [[ 0.1272202   0.02450709  0.09173717 -0.11343144 -0.02958456
     0.08422862 -0.06543145  0.09191065  0.12058292 -0.06990939
    -0.08898731 -0.03733413 -0.05812452 -0.12990645  0.0638507
    -0.13532798  0.10505985 -0.11757522 -0.08331738 -0.13327062
     0.01881111 -0.01903669 -0.01580404 -0.07075823 -0.06582467
    -0.06637978  0.07962956 -0.00432335 -0.06549603 -0.08766235
     0.05466402 -0.12913026]]]]

After restore, variable: conv2d/kernel:0 with value: [[[[ 0.06563459  0.00840305 -0.09779331  0.10958676 -0.04209604
     0.08968563 -0.01759356 -0.09149572 -0.1270998   0.067333
    -0.07290326  0.13000713  0.08546856 -0.07255595  0.09380624
     0.05008008  0.04844034  0.06551427 -0.03832343 -0.03300031
     0.00607258  0.13278274 -0.03660216 -0.04040344  0.05728079
     0.06849486 -0.07564054 -0.09481299  0.10913198  0.12305556
    -0.10410744  0.09862402]]

  [[ 0.08348374  0.04252926  0.03711365 -0.13923089 -0.00691505
    -0.11210923  0.01271646  0.08069353 -0.0500799  -0.03470477
     0.10451779  0.03921932  0.00808498 -0.12515476 -0.02671719
     0.01703154 -0.0797071  -0.1370279   0.05244619 -0.08127043
     0.00482281 -0.01106107 -0.01175928  0.04740907 -0.01123694
    -0.11811949  0.08805212 -0.00259446  0.00244816  0.05788592
    -0.09701566  0.02189627]]

  [[-0.09813957  0.13127281 -0.09860488 -0.02824788  0.01669799
     0.05194671  0.10739563  0.10136569  0.09967132 -0.07614696
     0.01958828 -0.06805683  0.05184881  0.02014206  0.10933466
     0.08968377 -0.04896273  0.07374194 -0.02363124  0.01056857
    -0.1301135  -0.07710255  0.06642468  0.09275301 -0.02652738
     0.11004217 -0.02997959  0.01762064  0.11606161 -0.09514837
    -0.10670003  0.13081704]]]


 [[[ 0.01213205  0.10840206 -0.03927455  0.01651397 -0.01690453
     0.07221566 -0.07510997 -0.08614457 -0.13383651  0.03703724
    -0.01236174  0.04353252 -0.05596335  0.01667911  0.04289125
    -0.02056064  0.07331941  0.11524598  0.02641955  0.01384376
     0.06697597  0.07333417 -0.10861287  0.07806718 -0.05407285
    -0.04556756  0.12141882  0.08252585  0.0860115  -0.03831191
     0.13593917  0.03157572]]

  [[-0.01660317  0.10147165  0.09894487 -0.12096284 -0.11507599
    -0.04978275 -0.05566155  0.05712175  0.02109195  0.12883337
     0.02091907 -0.06986401  0.09280713 -0.07688103  0.00173971
     0.08652124 -0.0723936   0.04379457 -0.00285    -0.04172704
    -0.01366517 -0.09906477  0.12725525 -0.13293567 -0.01025383
     0.13878314  0.10983334 -0.04301392  0.0713973   0.13531612
    -0.04094394  0.05427051]]

  [[-0.00552706  0.08048162  0.11622094 -0.02093116 -0.01610628
     0.01292579  0.12779085  0.00761476 -0.06791268  0.04948291
     0.14023949 -0.10729912 -0.00996061 -0.08164443  0.01273449
     0.07548277  0.04569027 -0.02749155 -0.08140583 -0.13028833
    -0.12789097 -0.10573255  0.02804492 -0.11411302  0.01505966
    -0.04191986  0.12602548  0.01951379 -0.01220857  0.0426316
    -0.06860761 -0.1292061 ]]]


 [[[ 0.08118929 -0.05359229  0.00344963 -0.00117226  0.13679491
    -0.08211896 -0.07821181  0.02441108 -0.07916161  0.00925118
    -0.072765   -0.12557067 -0.09301825  0.10492741  0.01565959
     0.07305914 -0.11428531 -0.13343579 -0.01604109  0.00415377
     0.12891085  0.05635801 -0.12451731  0.04226863 -0.0437835
    -0.14166194 -0.01247315 -0.07749605  0.1182047  -0.09100856
    -0.11602301  0.00513332]]

  [[ 0.05327897 -0.05282569 -0.13847588  0.11579688 -0.03227763
     0.05073091 -0.11563087 -0.08239145 -0.05441392 -0.01218514
    -0.02980805  0.03019193 -0.06550806 -0.11074035  0.00024992
    -0.11976698 -0.00537026  0.07806922  0.12875895  0.04991359
    -0.03856942 -0.06781539 -0.05026716 -0.00725389 -0.10366416
     0.01443329 -0.05533047  0.00477575  0.0038531   0.0348838
    -0.10772922 -0.00994276]]

  [[ 0.1272202   0.02450709  0.09173717 -0.11343144 -0.02958456
     0.08422862 -0.06543145  0.09191065  0.12058292 -0.06990939
    -0.08898731 -0.03733413 -0.05812452 -0.12990645  0.0638507
    -0.13532798  0.10505985 -0.11757522 -0.08331738 -0.13327062
     0.01881111 -0.01903669 -0.01580404 -0.07075823 -0.06582467
    -0.06637978  0.07962956 -0.00432335 -0.06549603 -0.08766235
     0.05466402 -0.12913026]]]]

tilakrayal (Assginee) on (2024-08-20 13:15:10 UTC): @yjiangling,
I tried to execute with the toy example where eliminating the last layer after loading the model weights from checkpoints. Kindly find the gist of it [here](https://colab.research.google.com/gist/kiransair/6c96e04005582293b7b16b7a8df8d31f/gist_73921.ipynb).

Also this question is better asked on [Google AI Forum](https://discuss.ai.google.dev/c/tensorflow/21) since it is not a bug or feature request. Thank you!

yjiangling (Issue Creator) on (2024-08-23 02:24:43 UTC): Thanks a lot for the help, problem have been resolved. The issue will be closed as soon.

google-ml-butler[bot] on (2024-08-23 02:24:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">No</a>

yjiangling (Issue Creator) on (2024-11-29 03:40:01 UTC): For example, there is a saved checkpoint model trained with SourceModel, how to transfer the compatible parameters to the the TargetModel and start a new train?

	# 自定义层示例
	class CustomLayer(tf.keras.layers.Layer):
	    def __init__(self, unit1, unit2, **kwargs):
	        super(CustomLayer, self).__init__(**kwargs)
	        self.dense1 = tf.keras.layers.Dense(unit1, activation='relu')
	        self.dense2 = tf.keras.layers.Dense(unit2, activation='softmax')
	 
	    def call(self, inputs):
	        x = self.dense1(inputs)
	        return self.dense2(x)

	# 源模型示例
	class SourceModel(tf.keras.Model):
	    def __init__(self):
	        super(SourceModel, self).__init__()
	        self.layer = tf.keras.layers.Dense(128, activation='relu')
	        self.custom_layer = CustomLayer(64, 10)
	 
	    def call(self, inputs):
	        x = self.layer(inputs)
	        x = self.custom_layer(x)
	        return x

	# 目标模型示例
	class TargetModel(tf.keras.Model):
	    def __init__(self):
	        super(SourceModel, self).__init__()
	        self.layer = tf.keras.layers.Dense(128, activation='relu')
	        self.custom_layer = CustomLayer(64, 5)
	 
	    def call(self, inputs):
	        x = self.layer(inputs)
	        x = self.custom_layer(x)
	        return x

How to get the layer in the CustomLayer and transfer the weight to the TargetModel?

tilakrayal (Assginee) on (2024-12-06 13:50:11 UTC): @yjiangling,
Could you please feel free to move this issue closed status and raise the new request with the required details to analyse in an effective way. Thank you!

yjiangling (Issue Creator) on (2024-12-09 09:35:44 UTC): Okay, Sorry, I will close it right now.

google-ml-butler[bot] on (2024-12-09 09:35:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73921"">No</a>

"
2469951788,issue,closed,completed,Support Protobuf 27 in Python package,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Maximum supported Protobuf version in Python is 4.x.x where the latest one is Protobuf 26:
https://github.com/tensorflow/tensorflow/blob/84c21ad90fd67e7b039f44402a08c378d17f5b72/tensorflow/tools/pip_package/setup.py#L94

Please support Protobuf 27 which is 5.x.x on pypi.org.

### Standalone code to reproduce the issue

```shell
Use the following requirements.txt file:

protobuf==5.27.1
tensorflow==2.17.0
```

It fails to resolve dependencies because of the above mentioned requirement.
```


### Relevant log output

_No response_",mering,2024-08-16 10:10:11+00:00,"['belitskiy', 'Venkat6871']",2024-09-25 18:02:13+00:00,2024-09-25 18:02:10+00:00,https://github.com/tensorflow/tensorflow/issues/73920,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2297967076, 'issue_id': 2469951788, 'author': 'Venkat6871', 'body': '@learning-to-play , @belitskiy .', 'created_at': datetime.datetime(2024, 8, 20, 4, 58, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374809569, 'issue_id': 2469951788, 'author': 'mering', 'body': 'Fixed in #76468', 'created_at': datetime.datetime(2024, 9, 25, 18, 2, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374809699, 'issue_id': 2469951788, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73920"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73920"">No</a>', 'created_at': datetime.datetime(2024, 9, 25, 18, 2, 12, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-20 04:58:07 UTC): @learning-to-play , @belitskiy .

mering (Issue Creator) on (2024-09-25 18:02:08 UTC): Fixed in #76468

google-ml-butler[bot] on (2024-09-25 18:02:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73920"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73920"">No</a>

"
2469267550,issue,closed,completed,"Model training hangs and is unresponsive when tensorflow is not imported, even if tensorflow is not accessed","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

MacOS Sonoma 14.6

### Mobile device

_No response_

### Python version

3.11.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

none

### GPU model and memory

Apple M2 16GB

### Current behavior?

When I `import tensorflow as tf` or `import tensorflow`, the model training process runs with success. However `tensorflow` was not accessed anywhere else in the program, so to clean things up I deleted that import line. Then when I ran the program, the model would fail to train and hangs unexpectedly. I think this is likely a dependency issue and `tensorflow` is being used by the program, even though not explicitly, such as by `keras.models` or a similar module. Though this was not apparent. 
Data is already extracted into a sample CSV file, so no need to run `music_processor.py`
Simply pass in the CSV path `v5_2_files.csv` to `read_raw_str_csv_and_split_df` in the `main_model.py` main function. Then run the program with `python main_model.py`

### Standalone code to reproduce the issue

```shell
To reproduce the bug, clone the source code here: https://github.com/hygtfrde/classically_punk
Then inside of the file `main_model.py` comment out or remove the import line: `import tensorflow as tf`
A sample CSV dataset is already processed and can be used to train the model.
In a Venv or Conda env, after installing the modules with Pip, run the model with `python main_model.py`
The program will hang right before the model training occurs. To fix this bug, include the `tensorflow` import statement above. Even though `tensorflow` was not accessed anywhere else in the program or used explicitly, it must be required by something that is not apparent, my best guess is perhaps keras.
```


### Relevant log output

```shell
Model is training successfully:

2024-08-15 18:07:27.266586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 609ms/step - accuracy: 0.0625 - loss: 3.0833 - val_accuracy: 0.0000e+00 - val_loss: 73.5159
Epoch 2/300
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - accuracy: 0.8125 - loss: 4.3979 - val_accuracy: 0.0000e+00 - val_loss: 108.2588
Epoch 3/300

and so on ...

However, the output above is missing and the program is unresponsive when tensorflow is not imported.
```
",hygtfrde,2024-08-16 01:42:58+00:00,['tilakrayal'],2024-10-09 02:01:29+00:00,2024-10-09 02:01:26+00:00,https://github.com/tensorflow/tensorflow/issues/73901,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:model', 'Model related issues'), ('TF 2.16', '')]","[{'comment_id': 2301410002, 'issue_id': 2469267550, 'author': 'tilakrayal', 'body': '@hygtfrde,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thank you!', 'created_at': datetime.datetime(2024, 8, 21, 8, 3, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302948265, 'issue_id': 2469267550, 'author': 'hygtfrde', 'body': 'Yes, essentially the model will not train if tensorflow is not imported. If it is imported then it will train, although tensorflow was not used or accessed by the code directly. \r\n<img width=""391"" alt=""Screenshot 2024-08-21 at 1 10 43\u202fPM"" src=""https://github.com/user-attachments/assets/08f74c59-9f13-45bd-9f16-1a10ec5130aa"">\r\nIt was not clear that tensorflow is actually a required dependency. To reproduce this issue just comment out or remove tensorflow from the imports and try to build and train the model. It should partially run but will be unresponsive and hang indefinitely. \r\nI spent a few days debugging the model, looking into CPU and GPU usage compatibility and other issues. But the bug appears much more simple. While not explicitly required as a dependency, tensorflow appears to be required implicitly, my guess is from Keras. By importing these modules:\r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Input\r\nfrom tensorflow.keras.utils import to_categorical\r\n```\r\nWe should see something like \'error, tensorflow is required for these modules\'\r\n\r\nTo summarize:\r\n- Tensorflow is not explicitly used elsewhere in the code\r\n- However, it is probably a dependency for other modules or code (Keras?)\r\n- If it is not imported then the model will fail to train and build\r\n\r\n```\r\n# remove the line below or comment it out to reproduce training bug\r\nimport tensorflow as tf\r\n```\r\n\r\nThis bug should be present for any model training like this one\r\n```\r\ndef build_and_train_model(X_train, y_train, X_test, y_test, num_features, num_classes):\r\n    model = Sequential([\r\n        Input(shape=(num_features,)),\r\n        Dense(64, activation=\'relu\'),\r\n        Dropout(0.2),\r\n        Dense(num_classes, activation=\'softmax\')\r\n    ])\r\n    model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\n    history = model.fit(\r\n        X_train, \r\n        y_train, \r\n        epochs=300, \r\n        batch_size=128, \r\n        validation_data=(X_test, y_test),\r\n        verbose=1\r\n    )\r\n    return model, history\r\n\r\n# Build and train model\r\nmodel, history = build_and_train_model(X_train, y_train, X_test, y_test, X_scaled.shape[1], num_classes)\r\n    \r\n```', 'created_at': datetime.datetime(2024, 8, 21, 20, 22, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368148941, 'issue_id': 2469267550, 'author': 'tilakrayal', 'body': '@hygtfrde,\r\nBy default Tensorflow v2.17 contains Keras3.0 which might be reason for the error. Could you please try using below commands to install the keras2.0 version and test the code.\r\n\r\n```\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 23, 12, 57, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384638367, 'issue_id': 2469267550, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 1, 2, 7, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123609, 'issue_id': 2469267550, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123660, 'issue_id': 2469267550, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73901"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73901"">No</a>', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 27, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-21 08:03:31 UTC): @hygtfrde,
The code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thank you!

hygtfrde (Issue Creator) on (2024-08-21 20:22:41 UTC): Yes, essentially the model will not train if tensorflow is not imported. If it is imported then it will train, although tensorflow was not used or accessed by the code directly. 
<img width=""391"" alt=""Screenshot 2024-08-21 at 1 10 43 PM"" src=""https://github.com/user-attachments/assets/08f74c59-9f13-45bd-9f16-1a10ec5130aa"">
It was not clear that tensorflow is actually a required dependency. To reproduce this issue just comment out or remove tensorflow from the imports and try to build and train the model. It should partially run but will be unresponsive and hang indefinitely. 
I spent a few days debugging the model, looking into CPU and GPU usage compatibility and other issues. But the bug appears much more simple. While not explicitly required as a dependency, tensorflow appears to be required implicitly, my guess is from Keras. By importing these modules:
```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.utils import to_categorical
```
We should see something like 'error, tensorflow is required for these modules'

To summarize:
- Tensorflow is not explicitly used elsewhere in the code
- However, it is probably a dependency for other modules or code (Keras?)
- If it is not imported then the model will fail to train and build

```
# remove the line below or comment it out to reproduce training bug
import tensorflow as tf
```

This bug should be present for any model training like this one
```
def build_and_train_model(X_train, y_train, X_test, y_test, num_features, num_classes):
    model = Sequential([
        Input(shape=(num_features,)),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(
        X_train, 
        y_train, 
        epochs=300, 
        batch_size=128, 
        validation_data=(X_test, y_test),
        verbose=1
    )
    return model, history

# Build and train model
model, history = build_and_train_model(X_train, y_train, X_test, y_test, X_scaled.shape[1], num_classes)
    
```

tilakrayal (Assginee) on (2024-09-23 12:57:27 UTC): @hygtfrde,
By default Tensorflow v2.17 contains Keras3.0 which might be reason for the error. Could you please try using below commands to install the keras2.0 version and test the code.

```
!pip install tf-keras

import tf_keras as keras
```

Thank you!

github-actions[bot] on (2024-10-01 02:07:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-09 02:01:25 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-09 02:01:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73901"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73901"">No</a>

"
2468433820,issue,closed,completed,Unexpected behaviour training custom Siamese Network,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15.1

### Custom code

Yes

### OS platform and distribution

Linux Mint 21.2 Cinnamon

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to train a custom built Siamese Network, following the Keras [documentation](https://keras.io/examples/vision/siamese_contrastive/) closely, only modifying the architecture and other things as needed.

What I'm trying to do differently is load the data in a different manner. Due to the dimensions of the dataset I will be working with, I can't store it all at once in arrays in memory as is done in the example.

I tried loading it iteratively to the model to be trained in two ways:
- Using a tf.data.Dataset that will load the data as needed
- Building a custom training loop that would only fetch the data batch by batch

To test these, I put together an example script (supplied below) to test these approaches and how they perform in comparison to supplying the whole dataset to the model from an array in memory 

Surprisingly, both of my methods produce a weird behaviour, where the trained model simply outputs the same class for every instance (classifying the image pairs as being distinct), while with the first method the model trains well and converges to a good solution. I don't understand if I'm doing something wrong, I have checked my produced tf.data.Dataset and they have the exact same data as the in-memory arrays (as it's supposed to) and I am doing things correctly according to the documentation as far as I am aware.  Can you reproduce the issue, and if yes, understand if I'm doing something wrong or if this behaviour is related to something internal of Tensorflow/Keras that I am not aware of? 

### Standalone code to reproduce the issue

```shell
import time
import sys
import numpy as np
import itertools
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential, Model
from random import shuffle, random

tf.random.set_seed(1234)
np.random.seed(1234)

np.set_printoptions(threshold=sys.maxsize)

mnist = tf.keras.datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

img_A_input = Input((28, 28, 3), name='img_A_input')
img_B_input = Input((28, 28, 3), name='img_B_input')


def euclidean_distance(vects):
    """"""Find the Euclidean distance between two vectors.

    Arguments:
        vects: List containing two tensors of same length.

    Returns:
        Tensor containing euclidean distance
        (as floating point value) between vectors.
    """"""

    x, y = vects
    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)
    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))

def loss(margin=1):
    """"""Provides 'contrastive_loss' an enclosing scope with variable 'margin'.

    Arguments:
        margin: Integer, defines the baseline for distance for which pairs
                should be classified as dissimilar. - (default is 1).

    Returns:
        'contrastive_loss' function with data ('margin') attached.
    """"""

    # Contrastive loss = mean( (1-true_value) * square(prediction) +
    #                         true_value * square( max(margin-prediction, 0) ))
    def contrastive_loss(y_true, y_pred):
        """"""Calculates the contrastive loss.

        Arguments:
            y_true: List of labels, each label is of type float32.
            y_pred: List of predictions of same length as of y_true,
                    each label is of type float32.

        Returns:
            A tensor containing contrastive loss as floating point value.
        """"""

        square_pred = tf.math.square(y_pred)
        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))
        return tf.math.reduce_mean((1 - y_true) * square_pred + (y_true) * margin_square)

    return contrastive_loss

cnn = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (2, 2), activation=""tanh"", padding=""same""),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Conv2D(64, (2, 2), activation=""tanh"", padding=""same""),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Conv2D(128, (2, 2), activation=""tanh"", padding=""same""),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Conv2D(256, (2, 2), activation=""tanh"", padding=""same""),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(64, activation='tanh')
])

feature_vector_A = cnn(img_A_input)
feature_vector_B = cnn(img_B_input)

merge_layer = tf.keras.layers.Lambda(euclidean_distance, output_shape=(1,))(
    [feature_vector_A, feature_vector_B]
)
normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)

output = Dense(1, activation='sigmoid')(normal_layer)

model = Model(inputs=[img_A_input, img_B_input], outputs=output)

random_indices = np.random.choice(X_train.shape[0], 500, replace=False)
X_train_sample, y_train_sample = X_train[random_indices], y_train[random_indices]

random_indices = np.random.choice(X_test.shape[0], 200, replace=False)
X_test_sample, y_test_sample = X_test[random_indices], y_test[random_indices]


def make_paired_dataset(X,y):
    X_pairs, y_pairs = [], []

    tuples = [(x1, y1) for x1, y1 in zip(X,y)]

    for t in itertools.product(tuples, tuples):
        img_A, label_A = t[0]
        img_B, label_B = t[1]

        img_A = tf.expand_dims(img_A, -1)
        img_A = tf.image.grayscale_to_rgb(img_A)

        img_B = tf.expand_dims(img_B, -1)
        img_B = tf.image.grayscale_to_rgb(img_B)

        new_label = float(label_A == label_B)

        X_pairs.append([img_A, img_B])
        y_pairs.append(new_label)

    pairs = [(x, y) for x, y in zip(X_pairs, y_pairs)]
    shuffle(pairs)

    X_pairs = np.array([x for x, _ in pairs])
    y_pairs = np.array([y for _, y in pairs])

    return X_pairs, y_pairs

def generate_paired_samples_dev(X, y):
    tuples = [(x1, y1) for x1, y1 in zip(X, y)]

    for t in itertools.product(tuples, tuples):
        img_A, label_A = t[0]
        img_B, label_B = t[1]

        img_A = tf.expand_dims(img_A, -1)
        img_A = tf.image.grayscale_to_rgb(img_A)

        img_B = tf.expand_dims(img_B, -1)
        img_B = tf.image.grayscale_to_rgb(img_B)

        new_label = float(label_A == label_B)
        yield [img_A, img_B], new_label


X_train_pairs, y_train_pairs = make_paired_dataset(X_train_sample, y_train_sample)
X_test_pairs, y_test_pairs = make_paired_dataset(X_test_sample, y_test_sample)

train_dataset = tf.data.Dataset.from_generator(
    generate_paired_samples_dev,
    args=(X_train_sample, y_train_sample),
    output_signature=(
        tf.TensorSpec(shape=(2,) + (28, 28, 3), dtype=tf.int32),
        tf.TensorSpec(shape=(), dtype=tf.float32)
    )
).map(lambda x, y: ({'img_A_input': x[0], 'img_B_input': x[1]}, y))
train_dataset = train_dataset.batch(batch_size=32)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_generator(
    generate_paired_samples_dev,
    args=(X_test_sample, y_test_sample),
    output_signature=(
        tf.TensorSpec(shape=(2,) + (28, 28, 3), dtype=tf.int32),
        tf.TensorSpec(shape=(), dtype=tf.float32)
    )
).map(lambda x, y: ({'img_A_input': x[0], 'img_B_input': x[1]}, y))
val_dataset = val_dataset.batch(batch_size=32)
val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)


model.compile(loss=loss(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
model.summary()

class_weight = {0: 0.1,
                1: 0.9}

weights = model.get_weights()

""""""
    Training with all the data pairs stored in arrays
""""""
model.fit(x=[X_train_pairs[:, 0, :, :], X_train_pairs[:, 1, :, :]], # numbers to differentiate the input images for each subnetwork
          y=y_train_pairs,
          validation_data=([X_test_pairs[:, 0, :, :], X_test_pairs[:, 1, :, :]], y_test_pairs),
          epochs=10, batch_size=32, class_weight=class_weight, verbose=2)

print(model.evaluate(x=[X_test_pairs[:, 0, :, :], X_test_pairs[:, 1, :, :]], y=y_test_pairs, batch_size=32, verbose=2))

model.set_weights(weights) # just to reset the initial state without any training

""""""
    Training with all the data using the tf.data.Dataset class, so that the data isn't all kept in memory at the same time
""""""
model.fit(train_dataset,
          validation_data=val_dataset,
          epochs=10, class_weight=class_weight, verbose=2)
print(model.evaluate(x=[X_test_pairs[:, 0, :, :], X_test_pairs[:, 1, :, :]], y=y_test_pairs, batch_size=32, verbose=2))

model.set_weights(weights) # just to reset the initial state without any training


batch_size = 32
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss_fn = loss()

train_acc_metric = tf.keras.metrics.Accuracy()
val_acc_metric = tf.keras.metrics.Accuracy()

""""""
    Custom training loop to train the model by batch. This is just a demo where the final use case is to only have in memory
    the file paths for the images, and load only each batch of images when needed, since the whole dataset wouldn't fit in memory
""""""
for epoch in range(10):
    tmp = [(x, y) for x, y in zip(X_train_pairs, y_train_pairs)]
    shuffle(tmp)
    X_train_pairs = np.array([x for x, _ in tmp])
    y_train_pairs = np.array([y for _, y in tmp])

    print(""Starting epoch "" + str(epoch + 1))
    start_time = time.time()
    for idx in range((len(y_train_pairs) // batch_size)):
        batch_x = X_train_pairs[idx * batch_size: (idx + 1) * batch_size]
        batch_y = y_train_pairs[idx * batch_size: (idx + 1) * batch_size]
        with tf.GradientTape() as tape:
            preds = model([batch_x[:, 0, :, :], batch_x[:, 1, :, :]], training=True)
            loss = loss_fn(batch_y, preds)
        grads = tape.gradient(loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
        train_acc_metric.update_state(batch_y, preds)

        # Log every 200 batches.
        if idx % 200 == 0:
            print(
                ""Training loss (for one batch) at step %d: %.4f""
                % (idx + 1, float(loss))
            )
            print(""Seen so far: %s samples"" % ((idx + 1) * batch_size))

    # Display metrics at the end of each epoch.
    train_acc = train_acc_metric.result()
    print(""Training acc over epoch: %.4f"" % (float(train_acc),))

    # Reset training metrics at the end of each epoch
    train_acc_metric.reset_states()

    # Run a validation loop at the end of each epoch.
    for idx in range((len(y_test_pairs) // batch_size)):
        batch_x = X_test_pairs[idx * batch_size: (idx + 1) * batch_size]
        batch_y = y_test_pairs[idx * batch_size: (idx + 1) * batch_size]
        val_preds = model([batch_x[:, 0, :, :], batch_x[:, 1, :, :]], training=False)
        val_acc_metric.update_state(batch_y, val_preds)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(""Validation acc: %.4f"" % (float(val_acc),))
    print(""Time taken: %.2fs"" % (time.time() - start_time))
```


### Relevant log output

```shell
First method training:
Epoch 1/10
7813/7813 - 113s - loss: 0.0109 - accuracy: 0.9274 - val_loss: 0.0238 - val_accuracy: 0.9714 - 113s/epoch - 14ms/step
Epoch 2/10
7813/7813 - 111s - loss: 0.0021 - accuracy: 0.9847 - val_loss: 0.0168 - val_accuracy: 0.9813 - 111s/epoch - 14ms/step
Epoch 3/10
7813/7813 - 111s - loss: 0.0016 - accuracy: 0.9877 - val_loss: 0.0239 - val_accuracy: 0.9728 - 111s/epoch - 14ms/step
Epoch 4/10
7813/7813 - 111s - loss: 0.0015 - accuracy: 0.9890 - val_loss: 0.0159 - val_accuracy: 0.9823 - 111s/epoch - 14ms/step
Epoch 5/10
7813/7813 - 111s - loss: 0.0014 - accuracy: 0.9895 - val_loss: 0.0191 - val_accuracy: 0.9786 - 111s/epoch - 14ms/step
Epoch 6/10
7813/7813 - 111s - loss: 0.0014 - accuracy: 0.9895 - val_loss: 0.0210 - val_accuracy: 0.9768 - 111s/epoch - 14ms/step
Epoch 7/10
7813/7813 - 111s - loss: 0.0012 - accuracy: 0.9909 - val_loss: 0.0205 - val_accuracy: 0.9771 - 111s/epoch - 14ms/step
Epoch 8/10
7813/7813 - 111s - loss: 0.0012 - accuracy: 0.9908 - val_loss: 0.0184 - val_accuracy: 0.9792 - 111s/epoch - 14ms/step
Epoch 9/10
7813/7813 - 111s - loss: 0.0012 - accuracy: 0.9911 - val_loss: 0.0177 - val_accuracy: 0.9798 - 111s/epoch - 14ms/step
Epoch 10/10
7813/7813 - 111s - loss: 0.0010 - accuracy: 0.9922 - val_loss: 0.0182 - val_accuracy: 0.9786 - 111s/epoch - 14ms/step
1250/1250 - 4s - loss: 0.0182 - accuracy: 0.9786 - 4s/epoch - 4ms/step
[0.01822792738676071, 0.9785500168800354]
1250/1250 - 41s - loss: 0.0182 - accuracy: 0.9786 - 41s/epoch - 32ms/step
[0.018227916210889816, 0.9785500168800354]

Second method training:
Epoch 1/10
2813/2813 - 43s - loss: 0.0237 - accuracy: 0.8624 - val_loss: 0.0925 - val_accuracy: 0.8972 - 43s/epoch - 15ms/step
Epoch 2/10
2813/2813 - 42s - loss: 0.0193 - accuracy: 0.8945 - val_loss: 0.0926 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 3/10
2813/2813 - 42s - loss: 0.0192 - accuracy: 0.8945 - val_loss: 0.0924 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 4/10
2813/2813 - 42s - loss: 0.0193 - accuracy: 0.8945 - val_loss: 0.0924 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 5/10
2813/2813 - 42s - loss: 0.0192 - accuracy: 0.8945 - val_loss: 0.0925 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 6/10
2813/2813 - 42s - loss: 0.0192 - accuracy: 0.8945 - val_loss: 0.0924 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 7/10
2813/2813 - 42s - loss: 0.0192 - accuracy: 0.8945 - val_loss: 0.0926 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 8/10
2813/2813 - 42s - loss: 0.0193 - accuracy: 0.8945 - val_loss: 0.0924 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 9/10
2813/2813 - 42s - loss: 0.0193 - accuracy: 0.8945 - val_loss: 0.0925 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
Epoch 10/10
2813/2813 - 42s - loss: 0.0192 - accuracy: 0.8945 - val_loss: 0.0925 - val_accuracy: 0.8972 - 42s/epoch - 15ms/step
704/704 - 2s - loss: 0.0925 - accuracy: 0.8972 - 2s/epoch - 4ms/step
[0.09247653186321259, 0.8971555829048157]
704/704 - 23s - loss: 0.0925 - accuracy: 0.8972 - 23s/epoch - 32ms/step
[0.09247657656669617, 0.8971555829048157]

Third method training:
(I won't put the log here since it's long and doesn'r add much information, the loss just keeps jumping between 0.05 and 1 randomly, without converging as the number of epochs passed increases)
```
",Carl0smvs,2024-08-15 16:27:32+00:00,['Venkat6871'],2025-01-07 02:02:47+00:00,2025-01-07 02:02:45+00:00,https://github.com/tensorflow/tensorflow/issues/73873,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2295969269, 'issue_id': 2468433820, 'author': 'Venkat6871', 'body': 'Hi **@Carl0smvs** ,\r\nSorry for the dealy, Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 19, 8, 25, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296313282, 'issue_id': 2468433820, 'author': 'Carl0smvs', 'body': 'I will, thank you!', 'created_at': datetime.datetime(2024, 8, 19, 11, 8, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559333869, 'issue_id': 2468433820, 'author': 'Venkat6871', 'body': 'Hi **@Carl0smvs** ,\r\nCould you please close this issue if it has already been raised in the Keras repository?\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 9, 54, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566070030, 'issue_id': 2468433820, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 31, 2, 1, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250891, 'issue_id': 2468433820, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250936, 'issue_id': 2468433820, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73873"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73873"">No</a>', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-19 08:25:19 UTC): Hi **@Carl0smvs** ,
Sorry for the dealy, Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

Carl0smvs (Issue Creator) on (2024-08-19 11:08:52 UTC): I will, thank you!

Venkat6871 (Assginee) on (2024-12-23 09:54:31 UTC): Hi **@Carl0smvs** ,
Could you please close this issue if it has already been raised in the Keras repository?
Thank you!

github-actions[bot] on (2024-12-31 02:01:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-07 02:02:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-07 02:02:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73873"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73873"">No</a>

"
2468158041,issue,closed,completed,Tensorflow lite build causing multiple undefined reference errors after running make,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.2

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Upon running make in the build directory after running cmake I get multiple undefined reference errors which I cannot understand why they occur. I am using all standard code from the minimal example except my CMakeLists.txt which is

```
cmake_minimum_required(VERSION 3.17)
project(TFLiteCheck)

set(CMAKE_CXX_STANDARD 14)

# include has 2 subdirectories: tensorflow and flatbuffers
INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/include/)

# lib has 1 file: libtensorflowlite.so
ADD_LIBRARY(tensorflowlite SHARED IMPORTED)
set_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/libs/linux_x64/libtensorflowlite.so)

add_executable(TFLiteCheck main.cpp)
target_link_libraries(TFLiteCheck tensorflowlite)
```

cmake runs perfectly fine and the tensorflow.so file was also created within the bazel-bin directory without any issue. I have spent days trying to solve it so it is sad to see it doesn't work even after coming so close.

My main.cpp is also the exact same as the minimal example



### Standalone code to reproduce the issue

```shell
#include <cstdio>

#include ""tensorflow/lite/core/interpreter_builder.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model_builder.h""
#include ""tensorflow/lite/optional_debug_tools.h""

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter with the InterpreterBuilder.
  // Note: all Interpreters should be built with the InterpreterBuilder,
  // which allocates memory for the Interpreter and does various set up
  // tasks so that the Interpreter can read the provided model.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Fill input buffers
  // TODO(user): Insert code to fill input tensors.
  // Note: The buffer of the input tensor with index `i` of type T can
  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`

  // Run inference
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Read output buffers
  // TODO(user): Insert getting data out code.
  // Note: The buffer of the output tensor with index `i` of type T can
  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`

  return 0;
}
```


### Relevant log output

```shell
(base) robocon2@robocon2:~/tflite_source_final/tflite/04_Linux_Installation/build$ make
[ 50%] Building CXX object CMakeFiles/TFLiteCheck.dir/main.cpp.o
[100%] Linking CXX executable TFLiteCheck
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `main':
main.cpp:(.text+0x91): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: main.cpp:(.text+0x117): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: main.cpp:(.text+0x13b): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: main.cpp:(.text+0x19e): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: main.cpp:(.text+0x1fe): undefined reference to `tflite::PrintInterpreterState(tflite::Interpreter*)'
/usr/bin/ld: main.cpp:(.text+0x215): undefined reference to `tflite::Interpreter::Invoke()'
/usr/bin/ld: main.cpp:(.text+0x275): undefined reference to `tflite::PrintInterpreterState(tflite::Interpreter*)'
/usr/bin/ld: main.cpp:(.text+0x298): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: main.cpp:(.text+0x2ee): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/TFLiteCheck.dir/build.make:98: TFLiteCheck] Error 1
make[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/TFLiteCheck.dir/all] Error 2
make: *** [Makefile:91: all] Error 2
```
",AD-lite24,2024-08-15 14:24:05+00:00,['sawantkumar'],2024-08-19 16:53:21+00:00,2024-08-19 12:49:51+00:00,https://github.com/tensorflow/tensorflow/issues/73867,"[('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2295070461, 'issue_id': 2468158041, 'author': 'abangwal', 'body': 'I am facing exact same issue\r\nOS : Ubuntu 22.04\r\n\r\nbuilt the libtensorflowlite.so from source with tf (v2.12.0) bazel (5.3.0) ([tested configuration](https://www.tensorflow.org/install/source#linux))\r\n\r\nand then I followed the following project structure\r\n\r\n```\r\n├── CMakeLists.txt\r\n├── include\r\n│\xa0\xa0 ├── flatbuffers\r\n│\xa0\xa0 └── tensorflow\r\n├── libs\r\n│\xa0\xa0 └── libtensorflowlite.so\r\n├── models\r\n│\xa0\xa0 └── face_detection_front.tflite\r\n└── src\r\n    └── main.cc\r\n```\r\nCMakeLists.txt\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.10)\r\nset(CMAKE_CXX_STANDARD 17)\r\nproject(tflite-project)\r\n\r\n# Add the include directory\r\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\r\n\r\nadd_library(tflite SHARED IMPORTED)\r\nset_target_properties(tflite PROPERTIES\r\n  IMPORTED_LOCATION ""${CMAKE_CURRENT_SOURCE_DIR}/libs/libtensorflowlite.so"")\r\n  #INTERFACE_INCLUDE_DIRECTORIES ""${CMAKE_CURRENT_SOURCE_DIR}/include"")\r\n\r\n# Add your executable\r\nadd_executable(project src/main.cc)\r\n\r\n# Link against TensorFlow Lite\r\ntarget_link_libraries(project tflite)\r\n```\r\n\r\nAnd I am facing exact same errors as @AD-lite24 ...', 'created_at': datetime.datetime(2024, 8, 18, 2, 33, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296500013, 'issue_id': 2468158041, 'author': 'AD-lite24', 'body': 'Hi @ashish-2005 my issue was resolved, use the official CMakeLists.txt provided\r\n\r\n```\r\nset(TENSORFLOW_SOURCE_DIR """" CACHE PATH\r\n  ""Directory that contains the TensorFlow project""\r\n)\r\nif(NOT TENSORFLOW_SOURCE_DIR)\r\n  get_filename_component(TENSORFLOW_SOURCE_DIR\r\n    ""${CMAKE_CURRENT_LIST_DIR}/../tensorflow-2.16.2/""\r\n    ABSOLUTE\r\n  )\r\nendif()\r\n\r\nadd_subdirectory(\r\n  ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""\r\n  ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite""\r\n  EXCLUDE_FROM_ALL\r\n)\r\n```\r\n\r\nAdd all the other parts as required', 'created_at': datetime.datetime(2024, 8, 19, 12, 49, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296500080, 'issue_id': 2468158041, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73867"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73867"">No</a>', 'created_at': datetime.datetime(2024, 8, 19, 12, 49, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296943910, 'issue_id': 2468158041, 'author': 'abangwal', 'body': 'Hi @AD-lite24, Thanks for the heads up. :rocket: \r\n\r\nI also followed, [Build with CMake](https://www.tensorflow.org/lite/guide/build_cmake) which make a STATIC library within the /build folder and uses it, and now its working :smile: .\r\n\r\nTurns out its hard to build the library from source once and use it in every project\r\n\r\n>[_source_](https://www.tensorflow.org/lite/guide/build_cmake#step_5_build_tensorflow_lite) \r\nNote: This generates a static library libtensorflow-lite.a in the current directory but the library isn\'t self-contained since all the transitive dependencies are not included. To use the library properly, you need to create a CMake project. Please refer the [""Create a CMake project which uses TensorFlow Lite""](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) section.\r\n\r\nI think for every new project we have to make a [CMake project](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) with sub-directories of source library. And build/make that freshly.', 'created_at': datetime.datetime(2024, 8, 19, 16, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296948183, 'issue_id': 2468158041, 'author': 'AD-lite24', 'body': 'You can just run sudo make install and install it your system libraries', 'created_at': datetime.datetime(2024, 8, 19, 16, 15, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296975131, 'issue_id': 2468158041, 'author': 'abangwal', 'body': 'Yup i read about that while searching for a solution [here](https://github.com/FloopCZ/tensorflow_cc) to build and install directly in the system...Are you aware of the pros and cons of this approach ? :thinking:', 'created_at': datetime.datetime(2024, 8, 19, 16, 30, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297015057, 'issue_id': 2468158041, 'author': 'AD-lite24', 'body': ""I am not very familiar with the dependency isolation systems in c++ tbh so I am not sure. But it shouldn't be a problem as such, you can always adjust your cmake if you wanted to install a different version of tensorflow for a certain project"", 'created_at': datetime.datetime(2024, 8, 19, 16, 53, 14, tzinfo=datetime.timezone.utc)}]","abangwal on (2024-08-18 02:33:05 UTC): I am facing exact same issue
OS : Ubuntu 22.04

built the libtensorflowlite.so from source with tf (v2.12.0) bazel (5.3.0) ([tested configuration](https://www.tensorflow.org/install/source#linux))

and then I followed the following project structure

```
├── CMakeLists.txt
├── include
│   ├── flatbuffers
│   └── tensorflow
├── libs
│   └── libtensorflowlite.so
├── models
│   └── face_detection_front.tflite
└── src
    └── main.cc
```
CMakeLists.txt

```
cmake_minimum_required(VERSION 3.10)
set(CMAKE_CXX_STANDARD 17)
project(tflite-project)

# Add the include directory
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)

add_library(tflite SHARED IMPORTED)
set_target_properties(tflite PROPERTIES
  IMPORTED_LOCATION ""${CMAKE_CURRENT_SOURCE_DIR}/libs/libtensorflowlite.so"")
  #INTERFACE_INCLUDE_DIRECTORIES ""${CMAKE_CURRENT_SOURCE_DIR}/include"")

# Add your executable
add_executable(project src/main.cc)

# Link against TensorFlow Lite
target_link_libraries(project tflite)
```

And I am facing exact same errors as @AD-lite24 ...

AD-lite24 (Issue Creator) on (2024-08-19 12:49:51 UTC): Hi @ashish-2005 my issue was resolved, use the official CMakeLists.txt provided

```
set(TENSORFLOW_SOURCE_DIR """" CACHE PATH
  ""Directory that contains the TensorFlow project""
)
if(NOT TENSORFLOW_SOURCE_DIR)
  get_filename_component(TENSORFLOW_SOURCE_DIR
    ""${CMAKE_CURRENT_LIST_DIR}/../tensorflow-2.16.2/""
    ABSOLUTE
  )
endif()

add_subdirectory(
  ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""
  ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite""
  EXCLUDE_FROM_ALL
)
```

Add all the other parts as required

google-ml-butler[bot] on (2024-08-19 12:49:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73867"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73867"">No</a>

abangwal on (2024-08-19 16:13:00 UTC): Hi @AD-lite24, Thanks for the heads up. :rocket: 

I also followed, [Build with CMake](https://www.tensorflow.org/lite/guide/build_cmake) which make a STATIC library within the /build folder and uses it, and now its working :smile: .

Turns out its hard to build the library from source once and use it in every project

Note: This generates a static library libtensorflow-lite.a in the current directory but the library isn't self-contained since all the transitive dependencies are not included. To use the library properly, you need to create a CMake project. Please refer the [""Create a CMake project which uses TensorFlow Lite""](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) section.

I think for every new project we have to make a [CMake project](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) with sub-directories of source library. And build/make that freshly.

AD-lite24 (Issue Creator) on (2024-08-19 16:15:12 UTC): You can just run sudo make install and install it your system libraries

abangwal on (2024-08-19 16:30:09 UTC): Yup i read about that while searching for a solution [here](https://github.com/FloopCZ/tensorflow_cc) to build and install directly in the system...Are you aware of the pros and cons of this approach ? :thinking:

AD-lite24 (Issue Creator) on (2024-08-19 16:53:14 UTC): I am not very familiar with the dependency isolation systems in c++ tbh so I am not sure. But it shouldn't be a problem as such, you can always adjust your cmake if you wanted to install a different version of tensorflow for a certain project

"
2468143155,issue,closed,completed,Tensorflow lite build causing multiple undefined reference errors after running make,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Upon running make in the build directory after running cmake I get multiple undefined reference errors which I cannot understand why they occur. I am using all standard code from the minimal example except my CMakeLists.txt which is 

```
cmake_minimum_required(VERSION 3.17)
project(TFLiteCheck)

set(CMAKE_CXX_STANDARD 14)

# include has 2 subdirectories: tensorflow and flatbuffers
INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/include/)

# lib has 1 file: libtensorflowlite.so
ADD_LIBRARY(tensorflowlite SHARED IMPORTED)
set_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/libs/linux_x64/libtensorflowlite.so)

add_executable(TFLiteCheck main.cpp)
target_link_libraries(TFLiteCheck tensorflowlite)
```

cmake runs perfectly fine and the tensorflow.so file was also created within the bazel-bin directory without any issue. I have spent days trying to solve it so it is sad to see it doesn't work even after coming so close. 

My main.cpp is also the exact same as the minimal example




### Standalone code to reproduce the issue

```shell
Just the main.cpp file which is the same as minimal.cc and the CMakeLists.txt I provided above.
```


### Relevant log output

```shell
(base) robocon2@robocon2:~/tflite_source_final/tflite/04_Linux_Installation/build$ make
[ 50%] Building CXX object CMakeFiles/TFLiteCheck.dir/main.cpp.o
[100%] Linking CXX executable TFLiteCheck
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `main':
main.cpp:(.text+0x91): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: main.cpp:(.text+0x117): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: main.cpp:(.text+0x13b): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: main.cpp:(.text+0x19e): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: main.cpp:(.text+0x1fe): undefined reference to `tflite::PrintInterpreterState(tflite::Interpreter*)'
/usr/bin/ld: main.cpp:(.text+0x215): undefined reference to `tflite::Interpreter::Invoke()'
/usr/bin/ld: main.cpp:(.text+0x275): undefined reference to `tflite::PrintInterpreterState(tflite::Interpreter*)'
/usr/bin/ld: main.cpp:(.text+0x298): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: main.cpp:(.text+0x2ee): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: CMakeFiles/TFLiteCheck.dir/main.cpp.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/TFLiteCheck.dir/build.make:98: TFLiteCheck] Error 1
make[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/TFLiteCheck.dir/all] Error 2
make: *** [Makefile:91: all] Error 2
```
",AD-lite24,2024-08-15 14:19:00+00:00,['sawantkumar'],2024-08-19 16:54:32+00:00,2024-08-19 16:54:29+00:00,https://github.com/tensorflow/tensorflow/issues/73866,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2291359163, 'issue_id': 2468143155, 'author': 'AD-lite24', 'body': ""Please don't assign someone whose only contribution to the source is a typo fix"", 'created_at': datetime.datetime(2024, 8, 15, 14, 20, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297017170, 'issue_id': 2468143155, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73866"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73866"">No</a>', 'created_at': datetime.datetime(2024, 8, 19, 16, 54, 30, tzinfo=datetime.timezone.utc)}]","AD-lite24 (Issue Creator) on (2024-08-15 14:20:42 UTC): Please don't assign someone whose only contribution to the source is a typo fix

google-ml-butler[bot] on (2024-08-19 16:54:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73866"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73866"">No</a>

"
2468046620,issue,closed,completed,memory crash when putting model.fit within a for loop,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

windows x64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I also read on the web that this issue is present for multiple users, I updated my TF but the problem is still present.

if you put the **model.fit**  inside the loop, it seems that TF loads the data each time and saves it in the memory finally RAM crashes.
clear session and garbage collection do not work.


### Standalone code to reproduce the issue

```shell
for r in range(0, 10000):
    model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))
    model.fit(data, epochs=1)
```


### Relevant log output

_No response_",gff77,2024-08-15 13:21:48+00:00,['tilakrayal'],2024-09-03 01:56:55+00:00,2024-09-03 01:56:52+00:00,https://github.com/tensorflow/tensorflow/issues/73864,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2293102575, 'issue_id': 2468046620, 'author': 'tilakrayal', 'body': '@gff77,\r\nCould you please provide the complete code you are trying which helps to reproduce the issue? Thank you!', 'created_at': datetime.datetime(2024, 8, 16, 8, 48, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295241536, 'issue_id': 2468046620, 'author': 'sanskarmodi8', 'body': ""Try removing the model.compile from the loop if it's unnecessary to compile the model each time. This can help reduce memory usage and improve performance."", 'created_at': datetime.datetime(2024, 8, 18, 12, 17, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419422, 'issue_id': 2468046620, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464775, 'issue_id': 2468046620, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464806, 'issue_id': 2468046620, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73864"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73864"">No</a>', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-16 08:48:45 UTC): @gff77,
Could you please provide the complete code you are trying which helps to reproduce the issue? Thank you!

sanskarmodi8 on (2024-08-18 12:17:33 UTC): Try removing the model.compile from the loop if it's unnecessary to compile the model each time. This can help reduce memory usage and improve performance.

github-actions[bot] on (2024-08-27 01:55:47 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-03 01:56:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-03 01:56:54 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73864"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73864"">No</a>

"
2468011075,issue,closed,not_planned,libtensorflowlite_c is slower than tf.lite,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.9.1

### Custom code

No

### OS platform and distribution

Linux NixOs

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running a model on my linux x86_64 machine in Python is two times faster than running the same model through symbols exposed by compiled libtensorflowlite_c.so. Why is the code exposed in Python's tensorflow.lite.Interpreter faster?

### Standalone code to reproduce the issue

```shell
Bazel Build Command: ""bazel"" ""--output_base=/target/x86_64-unknown-linux-gnu/release/build/tflitec-d675effdd7095bee/out/tensorflow_v2.9.1_output_base"" ""build"" ""-c"" ""opt"" ""--config=linux"" ""//tensorflow/lite/c/tmp:tensorflowlite_c"" ""--copt=-O3""
```


### Relevant log output

_No response_",Manuel030,2024-08-15 13:01:11+00:00,"['talumbau', 'pkgoogle', 'sawantkumar']",2024-11-26 18:01:31+00:00,2024-11-26 18:01:27+00:00,https://github.com/tensorflow/tensorflow/issues/73862,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue')]","[{'comment_id': 2301233553, 'issue_id': 2468011075, 'author': 'sawantkumar', 'body': 'Hi @Manuel030 ,\r\n\r\nCan you please provide me the script for both the cases so that i can replicate and debug your specific case?', 'created_at': datetime.datetime(2024, 8, 21, 6, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301527525, 'issue_id': 2468011075, 'author': 'Manuel030', 'body': 'Hi @sawantkumar,\r\n\r\nsure, you can replicate the performance difference with the official tensorflow lite example from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python\r\n\r\nThe example runs inference on a mobilenet model for an example image. On my machine I get this performance: \r\n\r\n`time: 12.286ms`\r\n\r\nOn the other side, we have the runtime provided by the compiled `libtensorflowlite_c.so`. To run inference on the same model and image you can use this script:\r\n\r\n```\r\nimport ctypes\r\nimport numpy as np\r\nfrom pathlib import Path\r\nfrom PIL import Image\r\nimport time\r\n\r\n# Load the shared library. This assumes the .so is in the same dir as this python script\r\nlib_path = Path(__file__).parent / \'libtensorflowlite_c.so\'\r\nif not lib_path.exists():\r\n    exit(1)\r\ntflite_lib = ctypes.CDLL(str(lib_path))\r\n\r\n# Define function prototypes\r\n# tflite_model_create_from_file\r\ntflite_lib.TfLiteModelCreateFromFile.restype = ctypes.POINTER(ctypes.c_void_p)\r\ntflite_lib.TfLiteModelCreateFromFile.argtypes = [ctypes.c_char_p]\r\n\r\n# tflite_interpreter_create\r\ntflite_lib.TfLiteInterpreterCreate.restype = ctypes.POINTER(ctypes.c_void_p)\r\ntflite_lib.TfLiteInterpreterCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p)]\r\n\r\n# tflite_interpreter_allocate_tensors\r\ntflite_lib.TfLiteInterpreterAllocateTensors.restype = ctypes.c_int\r\ntflite_lib.TfLiteInterpreterAllocateTensors.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\r\n\r\n# tflite_interpreter_invoke\r\ntflite_lib.TfLiteInterpreterInvoke.restype = ctypes.c_int\r\ntflite_lib.TfLiteInterpreterInvoke.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\r\n\r\n# tflite_interpreter_get_input_tensor\r\ntflite_lib.TfLiteInterpreterGetInputTensor.restype = ctypes.POINTER(ctypes.c_void_p)\r\ntflite_lib.TfLiteInterpreterGetInputTensor.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.c_int]\r\n\r\n# tflite_tensor_copy_from_buffer\r\ntflite_lib.TfLiteTensorCopyFromBuffer.restype = ctypes.c_int\r\ntflite_lib.TfLiteTensorCopyFromBuffer.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]\r\n\r\n# tflite_tensor_copy_to_buffer\r\ntflite_lib.TfLiteTensorCopyToBuffer.restype = ctypes.c_int\r\ntflite_lib.TfLiteTensorCopyToBuffer.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]\r\n\r\ndef load_model(model_path):\r\n    model_path_bytes = model_path.encode(\'utf-8\')\r\n    model = tflite_lib.TfLiteModelCreateFromFile(model_path_bytes)\r\n    return model\r\n\r\ndef create_interpreter(model):\r\n    interpreter = tflite_lib.TfLiteInterpreterCreate(model, None)\r\n    return interpreter\r\n\r\ndef allocate_tensors(interpreter):\r\n    status = tflite_lib.TfLiteInterpreterAllocateTensors(interpreter)\r\n    if status != 0:\r\n        raise RuntimeError(""Failed to allocate tensors"")\r\n\r\ndef set_input_tensor(interpreter, input_data):\r\n    input_tensor = tflite_lib.TfLiteInterpreterGetInputTensor(interpreter, 0)\r\n    input_data = np.array(input_data, dtype=np.float32)\r\n    input_data_ptr = input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_void_p))\r\n    size = input_data.nbytes\r\n    status = tflite_lib.TfLiteTensorCopyFromBuffer(input_tensor, ctypes.cast(input_data_ptr, ctypes.POINTER(ctypes.c_void_p)), size)\r\n    if status != 0:\r\n        raise RuntimeError(""Failed to copy data to input tensor"")\r\n\r\ndef invoke(interpreter):\r\n    status = tflite_lib.TfLiteInterpreterInvoke(interpreter)\r\n    if status != 0:\r\n        raise RuntimeError(""Failed to invoke the interpreter"")\r\n\r\ndef get_output_tensor(interpreter):\r\n    output_tensor = tflite_lib.TfLiteInterpreterGetOutputTensor(interpreter, 0)\r\n    output_shape = (1, 1001)\r\n    output_data = np.empty(output_shape, dtype=np.float32)\r\n    output_data_ptr = output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_void_p))\r\n    size = output_data.nbytes\r\n\r\n    status = tflite_lib.TfLiteTensorCopyToBuffer(ctypes.cast(output_tensor, ctypes.POINTER(ctypes.c_void_p)), \r\n                                                 ctypes.cast(output_data_ptr, ctypes.POINTER(ctypes.c_void_p)), \r\n                                                 size)\r\n    if status != 0:\r\n        raise RuntimeError(""Failed to copy data from output tensor"")\r\n    \r\n    return output_data\r\n\r\nmodel = load_model(""mobilenet_v1_1.0_224.tflite"")\r\ninterpreter = create_interpreter(model)\r\nallocate_tensors(interpreter)\r\n\r\nimg = Image.open(""grace_hopper.bmp"").resize((224, 224))\r\n\r\n# add N dim\r\ninput_data = np.expand_dims(img, axis=0)\r\n\r\nset_input_tensor(interpreter, input_data)\r\nstart_time = time.time()\r\ninvoke(interpreter)\r\nstop_time = time.time()\r\noutput_data = get_output_tensor(interpreter)\r\n\r\nprint(""Output data:"", output_data)\r\nprint(\'time: {:.3f}ms\'.format((stop_time - start_time) * 1000))\r\n```\r\n\r\nThe model pass in this case takes `time: 35.514ms` which is 2x slower.', 'created_at': datetime.datetime(2024, 8, 21, 8, 57, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309735459, 'issue_id': 2468011075, 'author': 'sawantkumar', 'body': 'Hi @Manuel030 ,\r\n\r\nI replicated your issue and my timings for the python interpreter and  runtime provided by the compiled libtensorflowlite_c.so were 18 seconds and 41 seconds respectively which is more than double . I want to know the command that you used to compile the libtensorflowlite_c.so , did you compile it with xnnpack support or without it?', 'created_at': datetime.datetime(2024, 8, 26, 9, 13, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309788145, 'issue_id': 2468011075, 'author': 'Manuel030', 'body': 'I\'ve compiled via:\r\n\r\n``` \r\nBazel Build Command: ""bazel"" ""--output_base=/target/x86_64-unknown-linux-gnu/release/build/tflitec-d675effdd7095bee/out/tensorflow_v2.9.1_output_base"" ""build"" ""-c"" ""opt"" ""--config=linux"" ""//tensorflow/lite/c/tmp:tensorflowlite_c"" ""--copt=-O3""\r\n```', 'created_at': datetime.datetime(2024, 8, 26, 9, 40, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314489497, 'issue_id': 2468011075, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle ,\r\n\r\nI replicated the issue and got almost the same numbers. Could you please take a look?', 'created_at': datetime.datetime(2024, 8, 28, 7, 8, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316008294, 'issue_id': 2468011075, 'author': 'pkgoogle', 'body': ""Hi @Manuel030, I don't necessarily dispute these numbers .. but is this a realistic use case? Generally the .so would be used directly in a mobile environment or via C++, not loaded via ctypes in Python. If you notice poor performance in a real setting, we can definitely take a look at that, but I'm not sure how useful solving this would be."", 'created_at': datetime.datetime(2024, 8, 28, 18, 31, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316872038, 'issue_id': 2468011075, 'author': 'Manuel030', 'body': 'It is a use case for me since I distribute software for mobile and desktop with a dependency on libtensorflowlite_c and have to distribute the shared library too. Also, the numbers are interesting as they show that the runtime could be theoretically way faster. \n\nWhy is there such a big performance gap and is it feasible to match the performance of the Python package?', 'created_at': datetime.datetime(2024, 8, 29, 7, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318918675, 'issue_id': 2468011075, 'author': 'pkgoogle', 'body': 'So I dug a little deeper... (This was all done against the nightly branch)\r\n\r\nIf I use the benchmark model tool: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\n\r\n```sh\r\n#in root TF source\r\nbazel build tensorflow/lite/tools/benchmark:benchmark_model\r\n# after it\'s done compiling\r\ncd bazel-bin/tensorflow/lite/tools/benchmark\r\n# copy the model file here\r\n./benchmark_model --graph=mobilenet_v1_1.0_224.tflite\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [mobilenet_v1_1.0_224.tflite]\r\nINFO: Signature to run: []\r\nINFO: Loaded model mobilenet_v1_1.0_224.tflite\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: The input model file size (MB): 16.9008\r\nINFO: Initialized session in 12.75ms.\r\nINFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nINFO: count=37 first=14744 curr=15520 min=13164 max=18295 avg=13764.3 std=967\r\n\r\nINFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\nINFO: count=74 first=14850 curr=13315 min=13105 max=15765 avg=13520.2 std=470\r\n\r\nINFO: Inference timings in us: Init: 12750, First inference: 14744, Warmup (avg): 13764.3, Inference (avg): 13520.2\r\nINFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nINFO: Memory footprint delta from the start of the tool (MB): init=40.7148 overall=42.2148\r\n```\r\n\r\nwhich is about **13.5** ms on average... If you go through the dependencies, it doesn\'t quite build against the entire tflite library: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/BUILD#L151\r\n\r\nSo I wasn\'t satisfied with this answer... so I timed it in the minimal example, which builds against the tensorflow-lite library: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal\r\n\r\nminimal.cc with my edits:\r\n```cpp\r\n/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the ""License"");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an ""AS IS"" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n#include <cstdio>\r\n#include <chrono>\r\n#include <iostream>\r\n\r\n#include ""tensorflow/lite/core/interpreter_builder.h""\r\n#include ""tensorflow/lite/interpreter.h""\r\n#include ""tensorflow/lite/kernels/register.h""\r\n#include ""tensorflow/lite/model_builder.h""\r\n#include ""tensorflow/lite/optional_debug_tools.h""\r\n\r\n// This is an example that is minimal to read a model\r\n// from disk and perform inference. There is no data being loaded\r\n// that is up to you to add as a user.\r\n//\r\n// NOTE: Do not add any dependencies to this that cannot be built with\r\n// the minimal makefile. This example must remain trivial to build with\r\n// the minimal build tool.\r\n//\r\n// Usage: minimal <tflite model>\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n  if (!(x)) {                                                \\\r\n    fprintf(stderr, ""Error at %s:%d\\n"", __FILE__, __LINE__); \\\r\n    exit(1);                                                 \\\r\n  }\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, ""minimal <tflite model>\\n"");\r\n    return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter with the InterpreterBuilder.\r\n  // Note: all Interpreters should be built with the InterpreterBuilder,\r\n  // which allocates memory for the Interpreter and does various set up\r\n  // tasks so that the Interpreter can read the provided model.\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(""=== Pre-invoke Interpreter State ===\\n"");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors.\r\n  // Note: The buffer of the input tensor with index `i` of type T can\r\n  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`\r\n\r\n  // Run inference\r\n  auto start = std::chrono::high_resolution_clock::now();\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  auto elapsed = std::chrono::high_resolution_clock::now() - start;\r\n\r\n  long long microseconds = std::chrono::duration_cast<std::chrono::microseconds>(elapsed).count();\r\n\r\n  printf(""\\n\\n=== Post-invoke Interpreter State ===\\n"");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Read output buffers\r\n  // TODO(user): Insert getting data out code.\r\n  // Note: The buffer of the output tensor with index `i` of type T can\r\n  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`\r\n\r\n  std::cout << ""Inference time is: "" << microseconds << "" microseconds.\\n"";\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n```sh\r\n./minimal mobilenet_v1_1.0_224.tflite\r\n# a lot of output about mobilenet\r\nInference time is: 33633 microseconds.\r\n```\r\n\r\nYeah... so something is dropping performance by using the whole library instead of just a subset.\r\n\r\n@talumbau, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 8, 29, 20, 31, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337577157, 'issue_id': 2468011075, 'author': 'Manuel030', 'body': 'any updates on this?', 'created_at': datetime.datetime(2024, 9, 9, 9, 15, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499934937, 'issue_id': 2468011075, 'author': 'gaikwadrahul8', 'body': ""Hi, @Manuel030 \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/39\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 8, 8, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501605694, 'issue_id': 2468011075, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73862"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73862"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 1, 29, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-08-21 06:28:00 UTC): Hi @Manuel030 ,

Can you please provide me the script for both the cases so that i can replicate and debug your specific case?

Manuel030 (Issue Creator) on (2024-08-21 08:57:28 UTC): Hi @sawantkumar,

sure, you can replicate the performance difference with the official tensorflow lite example from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python

The example runs inference on a mobilenet model for an example image. On my machine I get this performance: 

`time: 12.286ms`

On the other side, we have the runtime provided by the compiled `libtensorflowlite_c.so`. To run inference on the same model and image you can use this script:

```
import ctypes
import numpy as np
from pathlib import Path
from PIL import Image
import time

# Load the shared library. This assumes the .so is in the same dir as this python script
lib_path = Path(__file__).parent / 'libtensorflowlite_c.so'
if not lib_path.exists():
    exit(1)
tflite_lib = ctypes.CDLL(str(lib_path))

# Define function prototypes
# tflite_model_create_from_file
tflite_lib.TfLiteModelCreateFromFile.restype = ctypes.POINTER(ctypes.c_void_p)
tflite_lib.TfLiteModelCreateFromFile.argtypes = [ctypes.c_char_p]

# tflite_interpreter_create
tflite_lib.TfLiteInterpreterCreate.restype = ctypes.POINTER(ctypes.c_void_p)
tflite_lib.TfLiteInterpreterCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p)]

# tflite_interpreter_allocate_tensors
tflite_lib.TfLiteInterpreterAllocateTensors.restype = ctypes.c_int
tflite_lib.TfLiteInterpreterAllocateTensors.argtypes = [ctypes.POINTER(ctypes.c_void_p)]

# tflite_interpreter_invoke
tflite_lib.TfLiteInterpreterInvoke.restype = ctypes.c_int
tflite_lib.TfLiteInterpreterInvoke.argtypes = [ctypes.POINTER(ctypes.c_void_p)]

# tflite_interpreter_get_input_tensor
tflite_lib.TfLiteInterpreterGetInputTensor.restype = ctypes.POINTER(ctypes.c_void_p)
tflite_lib.TfLiteInterpreterGetInputTensor.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.c_int]

# tflite_tensor_copy_from_buffer
tflite_lib.TfLiteTensorCopyFromBuffer.restype = ctypes.c_int
tflite_lib.TfLiteTensorCopyFromBuffer.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]

# tflite_tensor_copy_to_buffer
tflite_lib.TfLiteTensorCopyToBuffer.restype = ctypes.c_int
tflite_lib.TfLiteTensorCopyToBuffer.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_void_p), ctypes.c_size_t]

def load_model(model_path):
    model_path_bytes = model_path.encode('utf-8')
    model = tflite_lib.TfLiteModelCreateFromFile(model_path_bytes)
    return model

def create_interpreter(model):
    interpreter = tflite_lib.TfLiteInterpreterCreate(model, None)
    return interpreter

def allocate_tensors(interpreter):
    status = tflite_lib.TfLiteInterpreterAllocateTensors(interpreter)
    if status != 0:
        raise RuntimeError(""Failed to allocate tensors"")

def set_input_tensor(interpreter, input_data):
    input_tensor = tflite_lib.TfLiteInterpreterGetInputTensor(interpreter, 0)
    input_data = np.array(input_data, dtype=np.float32)
    input_data_ptr = input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_void_p))
    size = input_data.nbytes
    status = tflite_lib.TfLiteTensorCopyFromBuffer(input_tensor, ctypes.cast(input_data_ptr, ctypes.POINTER(ctypes.c_void_p)), size)
    if status != 0:
        raise RuntimeError(""Failed to copy data to input tensor"")

def invoke(interpreter):
    status = tflite_lib.TfLiteInterpreterInvoke(interpreter)
    if status != 0:
        raise RuntimeError(""Failed to invoke the interpreter"")

def get_output_tensor(interpreter):
    output_tensor = tflite_lib.TfLiteInterpreterGetOutputTensor(interpreter, 0)
    output_shape = (1, 1001)
    output_data = np.empty(output_shape, dtype=np.float32)
    output_data_ptr = output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_void_p))
    size = output_data.nbytes

    status = tflite_lib.TfLiteTensorCopyToBuffer(ctypes.cast(output_tensor, ctypes.POINTER(ctypes.c_void_p)), 
                                                 ctypes.cast(output_data_ptr, ctypes.POINTER(ctypes.c_void_p)), 
                                                 size)
    if status != 0:
        raise RuntimeError(""Failed to copy data from output tensor"")
    
    return output_data

model = load_model(""mobilenet_v1_1.0_224.tflite"")
interpreter = create_interpreter(model)
allocate_tensors(interpreter)

img = Image.open(""grace_hopper.bmp"").resize((224, 224))

# add N dim
input_data = np.expand_dims(img, axis=0)

set_input_tensor(interpreter, input_data)
start_time = time.time()
invoke(interpreter)
stop_time = time.time()
output_data = get_output_tensor(interpreter)

print(""Output data:"", output_data)
print('time: {:.3f}ms'.format((stop_time - start_time) * 1000))
```

The model pass in this case takes `time: 35.514ms` which is 2x slower.

sawantkumar (Assginee) on (2024-08-26 09:13:02 UTC): Hi @Manuel030 ,

I replicated your issue and my timings for the python interpreter and  runtime provided by the compiled libtensorflowlite_c.so were 18 seconds and 41 seconds respectively which is more than double . I want to know the command that you used to compile the libtensorflowlite_c.so , did you compile it with xnnpack support or without it?

Manuel030 (Issue Creator) on (2024-08-26 09:40:13 UTC): I've compiled via:

``` 
Bazel Build Command: ""bazel"" ""--output_base=/target/x86_64-unknown-linux-gnu/release/build/tflitec-d675effdd7095bee/out/tensorflow_v2.9.1_output_base"" ""build"" ""-c"" ""opt"" ""--config=linux"" ""//tensorflow/lite/c/tmp:tensorflowlite_c"" ""--copt=-O3""
```

sawantkumar (Assginee) on (2024-08-28 07:08:16 UTC): Hi @pkgoogle ,

I replicated the issue and got almost the same numbers. Could you please take a look?

pkgoogle (Assginee) on (2024-08-28 18:31:29 UTC): Hi @Manuel030, I don't necessarily dispute these numbers .. but is this a realistic use case? Generally the .so would be used directly in a mobile environment or via C++, not loaded via ctypes in Python. If you notice poor performance in a real setting, we can definitely take a look at that, but I'm not sure how useful solving this would be.

Manuel030 (Issue Creator) on (2024-08-29 07:10:57 UTC): It is a use case for me since I distribute software for mobile and desktop with a dependency on libtensorflowlite_c and have to distribute the shared library too. Also, the numbers are interesting as they show that the runtime could be theoretically way faster. 

Why is there such a big performance gap and is it feasible to match the performance of the Python package?

pkgoogle (Assginee) on (2024-08-29 20:31:37 UTC): So I dug a little deeper... (This was all done against the nightly branch)

If I use the benchmark model tool: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark

```sh
#in root TF source
bazel build tensorflow/lite/tools/benchmark:benchmark_model
# after it's done compiling
cd bazel-bin/tensorflow/lite/tools/benchmark
# copy the model file here
./benchmark_model --graph=mobilenet_v1_1.0_224.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [mobilenet_v1_1.0_224.tflite]
INFO: Signature to run: []
INFO: Loaded model mobilenet_v1_1.0_224.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 16.9008
INFO: Initialized session in 12.75ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
INFO: count=37 first=14744 curr=15520 min=13164 max=18295 avg=13764.3 std=967

INFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
INFO: count=74 first=14850 curr=13315 min=13105 max=15765 avg=13520.2 std=470

INFO: Inference timings in us: Init: 12750, First inference: 14744, Warmup (avg): 13764.3, Inference (avg): 13520.2
INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
INFO: Memory footprint delta from the start of the tool (MB): init=40.7148 overall=42.2148
```

which is about **13.5** ms on average... If you go through the dependencies, it doesn't quite build against the entire tflite library: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/BUILD#L151

So I wasn't satisfied with this answer... so I timed it in the minimal example, which builds against the tensorflow-lite library: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal

minimal.cc with my edits:
```cpp
/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include <cstdio>
#include <chrono>
#include <iostream>

#include ""tensorflow/lite/core/interpreter_builder.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model_builder.h""
#include ""tensorflow/lite/optional_debug_tools.h""

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter with the InterpreterBuilder.
  // Note: all Interpreters should be built with the InterpreterBuilder,
  // which allocates memory for the Interpreter and does various set up
  // tasks so that the Interpreter can read the provided model.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Fill input buffers
  // TODO(user): Insert code to fill input tensors.
  // Note: The buffer of the input tensor with index `i` of type T can
  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`

  // Run inference
  auto start = std::chrono::high_resolution_clock::now();
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
  auto elapsed = std::chrono::high_resolution_clock::now() - start;

  long long microseconds = std::chrono::duration_cast<std::chrono::microseconds>(elapsed).count();

  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Read output buffers
  // TODO(user): Insert getting data out code.
  // Note: The buffer of the output tensor with index `i` of type T can
  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`

  std::cout << ""Inference time is: "" << microseconds << "" microseconds.\n"";

  return 0;
}
```

```sh
./minimal mobilenet_v1_1.0_224.tflite
# a lot of output about mobilenet
Inference time is: 33633 microseconds.
```

Yeah... so something is dropping performance by using the whole library instead of just a subset.

@talumbau, can you please take a look? Thanks.

Manuel030 (Issue Creator) on (2024-09-09 09:15:57 UTC): any updates on this?

gaikwadrahul8 on (2024-11-26 08:08:53 UTC): Hi, @Manuel030 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/39

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:01:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73862"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73862"">No</a>

"
2467080507,issue,closed,completed,Sharding Callback + Parameter Server Strategy partitioner crash when combined,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
Duplicate tensor keyed by embedding_0/.ATTRIBUTES/VARIABLE_VALUE encountered, when merging prefix: /var/tmp/tm
pc1sz0ucl/weights_temp/part-00001-of-00003
         [[{{node MergeV2Checkpoints}}]] [Op:__inference_tf_function_save_82]
Uncaught exception. Entering post mortem debugging
```

Create a toy model in parameter server strategy with variable partitioner and save checkpoint at end with ShardingCallback to slice large variables. Motivation is handling >10s of GB embedding tables efficiently. Variable partitioner is great for loading time and during training. Sharding callback is helpful at saving time to improve load efficiency for PS. But two together crash. The issue is variable_partitioner creates ShardedVariable with different PS having different slice. Those share same name and when combined with MaxShardSizePolicy you get a crash where each PS can save it's file, but they can't be merged correctly.

### Standalone code to reproduce the issue

Three files total. Run prepare model to create a toy model artifact and then load_model afterwards to try and load/save it in PS to reproduce the error.


toy_model.py

```python
import os

import tensorflow as tf
import tf_keras


class ToyModel(tf_keras.Model):
    def __init__(self, embedding_size_bytes: int, var_count: int):
        super().__init__()
        self.embedding_size_bytes = embedding_size_bytes
        self.var_count = var_count
        self.dense = tf_keras.layers.Dense(
            1, activation=""sigmoid"", kernel_initializer=tf_keras.initializers.GlorotNormal(seed=0)
        )

    def build(self, input_shape: tf.TensorShape):
        self.embeddings: dict[str, tf.Variable] = {}
        vocab_size = int(self.embedding_size_bytes / 256 / 4)
        for i in range(self.var_count):
            name = f""embedding_{i}""
            self.embeddings[name] = self.add_weight(
                name=name,
                shape=(vocab_size, 256),
                initializer=tf_keras.initializers.RandomUniform(seed=i),
                dtype=tf.float32,
                trainable=True,
            )

        self.dense.build(tf.TensorShape((None, 256)))
        self.built = True

    def call(self, inputs: tf.Tensor):
        embedding_values = []
        for i in range(self.var_count):
            name = f""embedding_{i}""
            embedding_values.append(tf.nn.embedding_lookup(self.embeddings[name], tf.squeeze(inputs, axis=1)))

        overall_embedding = tf.reduce_mean(embedding_values, axis=0)
        return self.dense(overall_embedding)


def get_weights_path(name: str):
    return os.path.join(os.path.dirname(__file__), ""toy_model_weights"", name, ""weights"")
```


prepare_model.py

```python
from typing import Sequence

import os
import shutil
import time
from collections import defaultdict

import tensorflow as tf
from tensorflow.train.experimental import MaxShardSizePolicy, ShardableTensor

from toy_model import ToyModel, get_weights_path, get_model_name


if __name__ == ""__main__"":
    num_gb = 20
    var_count = 1

    embedding_size_bytes = num_gb * 1024 * 1024 * 1024
    model = ToyModel(embedding_size_bytes, var_count)
    model.build(tf.TensorShape((None, 1)))

    model_name = ""test_model""
    weights_path = get_weights_path(model_name)
    shutil.rmtree(os.path.dirname(weights_path), ignore_errors=True)

    shard_size = 2 * 1024 * 1024 * 1024  # 2GB
    sharding_callback = MaxShardSizePolicy(shard_size)

    start_time = time.time()
    model.save_weights(weights_path, options=tf.train.CheckpointOptions(experimental_sharding_callback=sharding_callback))
    print(f""Saving weights took {time.time() - start_time:.2f} seconds"")
```

load_model.py (this is one that will crash)
```python
from typing import Any, Mapping

import json
import logging
import os
import shutil
import subprocess
import sys
import tempfile
import time

import tensorflow as tf
from tensorflow.train.experimental import MaxShardSizePolic

from prepare_model import SingleFileMaxShardSizePolicy
from toy_model import ToyModel, get_model_name, get_weights_path


def create_tf_configs(worker_count: int, ps_count: int, include_evaluator: bool):
    """"""Create TF_CONFIGs for a cluster.""""""
    cluster_dict: dict[str, list[str]] = {}
    if worker_count:
        cluster_dict[""worker""] = [f""localhost:{pick_unused_port()}"" for _ in range(worker_count)]
    if ps_count:
        cluster_dict[""ps""] = [f""localhost:{pick_unused_port()}"" for _ in range(ps_count)]

    cluster_dict[""chief""] = [f""localhost:{pick_unused_port()}""]

    tf_configs: list[TFConfig] = []
    for i in range(worker_count):
        tf_configs.append({""cluster"": cluster_dict, ""task"": {""type"": ""worker"", ""index"": i}})

    for i in range(ps_count):
        tf_configs.append({""cluster"": cluster_dict, ""task"": {""type"": ""ps"", ""index"": i}})

    if include_evaluator:
        tf_configs.append({""cluster"": cluster_dict, ""task"": {""type"": ""evaluator"", ""index"": 0}})

    tf_configs.append({""cluster"": cluster_dict, ""task"": {""type"": ""chief"", ""index"": 0}})

    return tf_configs


def _create_process(tf_config: Mapping[str, Any], log_dir: str):
    name = tf_config[""task""][""type""] + ""_"" + str(tf_config[""task""][""index""])
    command = [sys.executable, os.path.basename(__file__)]

    log_file_stdout = os.path.join(log_dir, f""stdout_{name}.log"")
    log_file_stderr = os.path.join(log_dir, f""stderr_{name}.log"")
    os.makedirs(log_dir, exist_ok=True)
    with open(log_file_stdout, ""a"") as stdout, open(log_file_stderr, ""a"") as stderr:
        print(f""Starting {name} process..."")
        env_ = os.environ.copy() | {""TF_CONFIG"": json.dumps(tf_config)}
        return subprocess.Popen(command, stdout=stdout, stderr=stderr, env=env_)


NUM_GB = 20
VAR_COUNT = 1

shard_size = 2 * 1024 * 1024 * 1024  # 2GB
PS_SAVE_SHARD_POLICY = MaxShardSizePolicy(shard_size)


def run():
    resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()

    task_type = resolver.task_type
    print(f""Task type: {task_type}"")
    if task_type in (""worker"", ""ps""):
        print(""Starting server..."")
        server = tf.distribute.Server(
            resolver.cluster_spec(),
            job_name=resolver.task_type,
            task_index=resolver.task_id,
            protocol=resolver.rpc_layer,
            start=True,
        )
        server.join()

    strategy = tf.distribute.experimental.ParameterServerStrategy(
        resolver,
        variable_partitioner=tf.distribute.experimental.partitioners.MaxSizePartitioner(
            max_shard_bytes=1 * 1024 * 1024 * 1024
        ),
    )
    print(""Building model..."")
    model_name = ""test_model""
    weights_path = get_weights_path(model_name)

    with strategy.scope(), tempfile.TemporaryDirectory() as temp_dir_name:
        embedding_size_bytes = NUM_GB * 1024 * 1024 * 1024
        model = ToyModel(embedding_size_bytes, VAR_COUNT)
        start_time = time.time()

        model.load_weights(weights_path)
        model.build(tf.TensorShape((None, 1)))
        done_loading_time = time.time()
        print(f""Loading weights took {done_loading_time - start_time:.2f} seconds"")
        save_path = os.path.join(temp_dir_name, ""weights"")
        model.save_weights(
            save_path, options=tf.train.CheckpointOptions(experimental_sharding_callback=PS_SAVE_SHARD_POLICY)
        )
        done_saving_time = time.time()
        print(f""Saving weights took {done_saving_time - done_loading_time:.2f} seconds"")
        print(""Done"")


def main():
    if ""TF_CONFIG"" in os.environ:
        run()
        return

    tf_configs = create_tf_configs(1, 2, False)
    chief_config = tf_configs[-1]
    model_name = ""test_model""
    print(f""Model: {model_name}"")
    log_dir = os.path.join(os.path.dirname(__file__), ""toy_model_weights"", model_name, ""logs"", ""run1"")
    shutil.rmtree(log_dir, ignore_errors=True)
    for tf_config in tf_configs[:-1]:
        _create_process(tf_config, log_dir)

    tf.debugging.disable_traceback_filtering()
    os.environ[""TF_CONFIG""] = json.dumps(chief_config)
    print(""Starting chief"")
    run()


if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
File ""<string>"", line 1, in <module>                                                                        
                                                                                                              
  File ""/home/mdrissi/training-platform/scratch/explore_sharding_perf/load_model.py"", line 126, in <module>   
                                                                                                              
  File ""/home/mdrissi/training-platform/scratch/explore_sharding_perf/load_model.py"", line 122, in main       
                                                                                                              
  File ""/home/mdrissi/training-platform/scratch/explore_sharding_perf/load_model.py"", line 97, in run         
                                                       
  File ""/home/mdrissi/.venvs/bento/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py"", line 61
, in error_handler                                                                                            
                                                       
  File ""/home/mdrissi/.venvs/bento/lib/python3.9/site-packages/tf_keras/src/engine/training.py"", line 3170, in
 save_weights

  File ""/home/mdrissi/.venvs/bento/lib/python3.9/site-packages/tf_keras/src/saving/saving_api.py"", line 284, i
n save_weights

  File ""/home/mdrissi/.venvs/bento/lib/python3.9/site-packages/tf_keras/src/saving/legacy/save.py"", line 374, 
in save_weights

  File ""/home/mdrissi/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.p
y"", line 555, in tf_function_save

Duplicate tensor keyed by embedding_0/.ATTRIBUTES/VARIABLE_VALUE encountered, when merging prefix: /var/tmp/tm
pc1sz0ucl/weights_temp/part-00001-of-00003
         [[{{node MergeV2Checkpoints}}]] [Op:__inference_tf_function_save_82]
```
",hmc-cs-mdrissi,2024-08-15 01:03:37+00:00,['tilakrayal'],2024-08-16 08:52:11+00:00,2024-08-16 08:13:37+00:00,https://github.com/tensorflow/tensorflow/issues/73836,"[('type:bug', 'Bug'), ('WIP', ''), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.16', '')]","[{'comment_id': 2292972661, 'issue_id': 2467080507, 'author': 'tilakrayal', 'body': '@hmc-cs-mdrissi,\r\nThank you for reporting the issue. Could you please confirm if the similar crash happened on the latest tensorflow v2.17 which contains the keras3.0 and provide the update. \r\nhttps://www.tensorflow.org/tutorials/distribute/parameter_server_training\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 16, 7, 23, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293012411, 'issue_id': 2467080507, 'author': 'hmc-cs-mdrissi', 'body': ""Keras 3.0 is backwards incompatible in a couple ways and has worse PS support so it's not really fit to answer this question. Core optimizers in keras (adam) have race condition issues in PS. I've filed couple bug reports related to keras/PS with response of unclear plans on support in future.\r\n\r\nI've been doing my testing with tf_keras keras 2 fork instead which continues to be updated with new tf versions.\r\n\r\nedit: Here's [specific quote](https://github.com/keras-team/keras/issues/18624#issuecomment-1765147583),\r\n\r\n> Parameter server training is not supported in Keras 3, generally speaking. The feature had very low usage. If you need it, I recommend you stick to tf.keras and use the legacy optimizers.\r\n\r\nIf answer is issues only apply for keras 3 then effectively some tf apis that are still documented normally have notable issues in practice now. Besides PS, ragged tensors are documented normally but are not supported in keras 3"", 'created_at': datetime.datetime(2024, 8, 16, 7, 52, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293044938, 'issue_id': 2467080507, 'author': 'hmc-cs-mdrissi', 'body': ""I'm surprised. TF 2.17 + tf-keras 2.17 (so sticking to keras 2 fork) works with no errors and resolves this issue. Here's fix [PR](https://github.com/tensorflow/tensorflow/commit/2f2aecfc97bb71209ece6cd1453506e0dce3d52a)"", 'created_at': datetime.datetime(2024, 8, 16, 8, 13, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293045092, 'issue_id': 2467080507, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73836"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73836"">No</a>', 'created_at': datetime.datetime(2024, 8, 16, 8, 13, 40, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-16 07:23:16 UTC): @hmc-cs-mdrissi,
Thank you for reporting the issue. Could you please confirm if the similar crash happened on the latest tensorflow v2.17 which contains the keras3.0 and provide the update. 
https://www.tensorflow.org/tutorials/distribute/parameter_server_training
Thank you!

hmc-cs-mdrissi (Issue Creator) on (2024-08-16 07:52:05 UTC): Keras 3.0 is backwards incompatible in a couple ways and has worse PS support so it's not really fit to answer this question. Core optimizers in keras (adam) have race condition issues in PS. I've filed couple bug reports related to keras/PS with response of unclear plans on support in future.

I've been doing my testing with tf_keras keras 2 fork instead which continues to be updated with new tf versions.

edit: Here's [specific quote](https://github.com/keras-team/keras/issues/18624#issuecomment-1765147583),


If answer is issues only apply for keras 3 then effectively some tf apis that are still documented normally have notable issues in practice now. Besides PS, ragged tensors are documented normally but are not supported in keras 3

hmc-cs-mdrissi (Issue Creator) on (2024-08-16 08:13:37 UTC): I'm surprised. TF 2.17 + tf-keras 2.17 (so sticking to keras 2 fork) works with no errors and resolves this issue. Here's fix [PR](https://github.com/tensorflow/tensorflow/commit/2f2aecfc97bb71209ece6cd1453506e0dce3d52a)

google-ml-butler[bot] on (2024-08-16 08:13:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73836"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73836"">No</a>

"
