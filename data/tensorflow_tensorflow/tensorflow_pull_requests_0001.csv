id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2839751752,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:57:33+00:00,[],2025-02-08 09:57:33+00:00,,https://github.com/tensorflow/tensorflow/pull/86895,[],[],
2839734535,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:20:25+00:00,[],2025-02-08 09:20:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86894,[],[],
2839734411,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:20:07+00:00,[],2025-02-08 09:20:07+00:00,,https://github.com/tensorflow/tensorflow/pull/86893,[],[],
2839733423,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:17:47+00:00,[],2025-02-08 10:08:44+00:00,,https://github.com/tensorflow/tensorflow/pull/86892,[],[],
2839733173,pull_request,open,,compat: Update forward compatibility horizon to 2025-02-08,"compat: Update forward compatibility horizon to 2025-02-08
",copybara-service[bot],2025-02-08 09:17:08+00:00,[],2025-02-08 09:17:08+00:00,,https://github.com/tensorflow/tensorflow/pull/86891,[],[],
2839732952,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:16:34+00:00,[],2025-02-08 09:16:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86890,[],[],
2839732356,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:15:45+00:00,[],2025-02-08 09:15:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86889,[],[],
2839732268,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:15:32+00:00,[],2025-02-08 10:07:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86888,[],[],
2839732207,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:15:25+00:00,[],2025-02-08 09:15:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86887,[],[],
2839732204,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:15:24+00:00,[],2025-02-08 09:15:24+00:00,,https://github.com/tensorflow/tensorflow/pull/86886,[],[],
2839731708,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:14:24+00:00,[],2025-02-08 09:48:30+00:00,,https://github.com/tensorflow/tensorflow/pull/86885,[],[],
2839731640,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:14:14+00:00,[],2025-02-08 09:14:14+00:00,,https://github.com/tensorflow/tensorflow/pull/86884,[],[],
2839731639,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:14:14+00:00,[],2025-02-08 10:09:06+00:00,,https://github.com/tensorflow/tensorflow/pull/86883,[],[],
2839731612,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:14:09+00:00,[],2025-02-08 10:07:01+00:00,,https://github.com/tensorflow/tensorflow/pull/86882,[],[],
2839731232,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:13:25+00:00,[],2025-02-08 10:09:14+00:00,,https://github.com/tensorflow/tensorflow/pull/86881,[],[],
2839730952,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-08 09:12:40+00:00,[],2025-02-08 09:12:40+00:00,,https://github.com/tensorflow/tensorflow/pull/86880,[],[],
2839666487,pull_request,open,,Improve structure of IR emission logic. #cleanup,"Improve structure of IR emission logic. #cleanup
",copybara-service[bot],2025-02-08 07:19:15+00:00,[],2025-02-08 07:19:15+00:00,,https://github.com/tensorflow/tensorflow/pull/86878,[],[],
2839572355,pull_request,open,,"Make xla_cc_test default to static linking, so that we can catch bugs where duplicated symbols are linked into the same test (e.g. having main()s from different libraries), which is undefined behavior.","Make xla_cc_test default to static linking, so that we can catch bugs where duplicated symbols are linked into the same test (e.g. having main()s from different libraries), which is undefined behavior.
",copybara-service[bot],2025-02-08 04:58:44+00:00,[],2025-02-08 04:58:44+00:00,,https://github.com/tensorflow/tensorflow/pull/86877,[],[],
2839474066,pull_request,open,,Enable weight sharing in LiteRt core.,"Enable weight sharing in LiteRt core.
",copybara-service[bot],2025-02-08 04:00:04+00:00,[],2025-02-08 04:00:04+00:00,,https://github.com/tensorflow/tensorflow/pull/86876,[],[],
2839384459,pull_request,open,,Add vlogging to aid debugging,"Add vlogging to aid debugging
",copybara-service[bot],2025-02-08 01:30:02+00:00,['frgossen'],2025-02-08 01:30:03+00:00,,https://github.com/tensorflow/tensorflow/pull/86875,[],[],
2839382752,pull_request,open,,Add test with two loops,"Add test with two loops
",copybara-service[bot],2025-02-08 01:27:32+00:00,['frgossen'],2025-02-08 01:27:33+00:00,,https://github.com/tensorflow/tensorflow/pull/86874,[],[],
2839382578,pull_request,open,,Find truely dominating collectives,"Find truely dominating collectives
",copybara-service[bot],2025-02-08 01:27:11+00:00,['frgossen'],2025-02-08 01:27:12+00:00,,https://github.com/tensorflow/tensorflow/pull/86873,[],[],
2839382293,pull_request,open,,Add control dependencies for peeled send/recv,"Add control dependencies for peeled send/recv

For send/recv we have to ensure that they ar enot pipelined beyond any conflicting collective.
",copybara-service[bot],2025-02-08 01:26:42+00:00,['frgossen'],2025-02-08 01:26:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86872,[],[],
2839377134,pull_request,open,,Add LiteRT Kotlin API.,"Add LiteRT Kotlin API.
",copybara-service[bot],2025-02-08 01:16:07+00:00,[],2025-02-08 06:14:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86871,[],[],
2839368956,pull_request,open,,Integrate the aie quantizer into the example backend and end2end flow,"Integrate the aie quantizer into the example backend and end2end flow
",copybara-service[bot],2025-02-08 01:02:03+00:00,['LukeBoyer'],2025-02-08 01:02:04+00:00,,https://github.com/tensorflow/tensorflow/pull/86870,[],[],
2839363926,pull_request,open,,[Stablehlo_ext] Remove manual construction of `stablehlo-ext-flatten-tuple` pass,"[Stablehlo_ext] Remove manual construction of `stablehlo-ext-flatten-tuple` pass
",copybara-service[bot],2025-02-08 00:53:49+00:00,[],2025-02-08 00:53:49+00:00,,https://github.com/tensorflow/tensorflow/pull/86869,[],[],
2839329918,pull_request,open,,Add simple wrapper for the aie quantizer.,"Add simple wrapper for the aie quantizer.
",copybara-service[bot],2025-02-08 00:18:15+00:00,['LukeBoyer'],2025-02-08 00:18:16+00:00,,https://github.com/tensorflow/tensorflow/pull/86868,[],[],
2839301357,pull_request,open,,Update release notes at HEAD,"Update release notes at HEAD
",copybara-service[bot],2025-02-07 23:43:41+00:00,['rtg0795'],2025-02-07 23:43:42+00:00,,https://github.com/tensorflow/tensorflow/pull/86867,[],[],
2839282322,pull_request,closed,,Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.,"Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.
",copybara-service[bot],2025-02-07 23:26:08+00:00,[],2025-02-08 00:02:09+00:00,2025-02-08 00:02:08+00:00,https://github.com/tensorflow/tensorflow/pull/86866,[],[],
2839234785,pull_request,open,,[XLA] Fix log message in hlo_pass_fix.,"[XLA] Fix log message in hlo_pass_fix.
",copybara-service[bot],2025-02-07 22:49:18+00:00,[],2025-02-07 22:49:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86865,[],[],
2839233208,pull_request,closed,,Update the compiler plugin to allow passing a partitioning directly.,"Update the compiler plugin to allow passing a partitioning directly.

This will facilitate manually device placement for debugging as well as handling partitions passed down from authoring layer.
",copybara-service[bot],2025-02-07 22:47:44+00:00,['LukeBoyer'],2025-02-08 00:25:55+00:00,2025-02-08 00:25:54+00:00,https://github.com/tensorflow/tensorflow/pull/86864,[],[],
2839231573,pull_request,open,,We need to insert a copy-from-host before X64SplitLow/High custom-calls.,"We need to insert a copy-from-host before X64SplitLow/High custom-calls.
",copybara-service[bot],2025-02-07 22:45:56+00:00,[],2025-02-07 22:45:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86863,[],[],
2839198485,pull_request,open,,Integrate LLVM at llvm/llvm-project@cea799afc632,"Integrate LLVM at llvm/llvm-project@cea799afc632

Updates LLVM usage to match
[cea799afc632](https://github.com/llvm/llvm-project/commit/cea799afc632)
",copybara-service[bot],2025-02-07 22:21:45+00:00,[],2025-02-07 22:21:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86862,[],[],
2839180164,pull_request,closed,,Simplify flag parsing in flag_types.cc using fixed_option_set_flag.,"Simplify flag parsing in flag_types.cc using fixed_option_set_flag.

Also extend fixed_option_set_flag to support aliases if desired.
",copybara-service[bot],2025-02-07 22:08:36+00:00,[],2025-02-08 03:01:42+00:00,2025-02-08 03:01:42+00:00,https://github.com/tensorflow/tensorflow/pull/86861,[],[],
2839172791,pull_request,closed,,Skip getting topolgy for proxy ifrt,"Skip getting topolgy for proxy ifrt
",copybara-service[bot],2025-02-07 22:02:44+00:00,[],2025-02-08 01:44:12+00:00,2025-02-08 01:44:12+00:00,https://github.com/tensorflow/tensorflow/pull/86860,[],[],
2839169737,pull_request,closed,,"Track the selected calculator source for final cost metric values, through OpCostManager's delegation system.","Track the selected calculator source for final cost metric values, through OpCostManager's delegation system.

This allows us to add a new column to cost model logging, indicating which calculator's value was chosen as the final value for a metric.

This was done by creating a Result class, returned by GetMetricValue(), which holds the selected metric value, its source, and the values calculated by all calculators (if asked to do so).
",copybara-service[bot],2025-02-07 22:00:41+00:00,['sparc1998'],2025-02-07 23:49:12+00:00,2025-02-07 23:49:12+00:00,https://github.com/tensorflow/tensorflow/pull/86859,[],[],
2839140174,pull_request,open,,[ODML] StablehloFlattenEntryFunctionTuplesPass  and StablehloFlattenTuplePass : replace MHLO passes with StableHLO ones.,"[ODML] StablehloFlattenEntryFunctionTuplesPass  and StablehloFlattenTuplePass : replace MHLO passes with StableHLO ones.
",copybara-service[bot],2025-02-07 21:40:21+00:00,[],2025-02-07 23:29:09+00:00,,https://github.com/tensorflow/tensorflow/pull/86858,[],[],
2839107250,pull_request,open,,[xla:cpu] Worker/WorkQueue micro-optimizations,"[xla:cpu] Worker/WorkQueue micro-optimizations
",copybara-service[bot],2025-02-07 21:18:30+00:00,['ezhulenev'],2025-02-07 21:18:31+00:00,,https://github.com/tensorflow/tensorflow/pull/86857,[],[],
2839038521,pull_request,open,,Create GlBase test.,"Create GlBase test.
",copybara-service[bot],2025-02-07 20:34:03+00:00,[],2025-02-07 23:23:13+00:00,,https://github.com/tensorflow/tensorflow/pull/86856,[],[],
2839026454,pull_request,closed,,Delete unused file. #cleanup,"Delete unused file. #cleanup
",copybara-service[bot],2025-02-07 20:25:35+00:00,[],2025-02-08 01:32:31+00:00,2025-02-08 01:32:30+00:00,https://github.com/tensorflow/tensorflow/pull/86855,[],[],
2839017124,pull_request,open,,Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *').,"Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *').

This is fixed by explicitly iterating and building the int64_t Span.
",copybara-service[bot],2025-02-07 20:19:07+00:00,[],2025-02-07 20:19:07+00:00,,https://github.com/tensorflow/tensorflow/pull/86854,[],[],
2839012600,pull_request,open,,Reverts 24e4c2eac63e72e0f8b328bfbd9e1fdddc3e6f11,"Reverts 24e4c2eac63e72e0f8b328bfbd9e1fdddc3e6f11
",copybara-service[bot],2025-02-07 20:15:53+00:00,[],2025-02-07 20:15:53+00:00,,https://github.com/tensorflow/tensorflow/pull/86853,[],[],
2838974754,pull_request,open,,Simplify flag parsing in gather_borg_symbols.cc using fixed_option_set_flag.,"Simplify flag parsing in gather_borg_symbols.cc using fixed_option_set_flag.
",copybara-service[bot],2025-02-07 19:49:43+00:00,[],2025-02-07 19:49:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86852,[],[],
2838936923,pull_request,closed,,Add change to hwloc that should have landed as part of an XLA PR,"Add change to hwloc that should have landed as part of an XLA PR

https://github.com/openxla/xla/pull/21708/files#diff-b4b632df0c60999fbc4cf64e16f4370620ba457d92f89e668cd1417b79080cbd should have removed this line but internal sync missed the file.
",copybara-service[bot],2025-02-07 19:30:20+00:00,['MichaelHudgins'],2025-02-07 20:11:50+00:00,2025-02-07 20:11:48+00:00,https://github.com/tensorflow/tensorflow/pull/86851,[],[],
2838926253,pull_request,closed,,"PR #22368: [XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name","PR #22368: [XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name

Imported from GitHub PR https://github.com/openxla/xla/pull/22368

B200 is the name of the released chip.
https://www.nvidia.com/en-us/data-center/dgx-b200/
Copybara import of the project:

--
124885af4f01cd4f54af08e39b270dce071f285a by Sergey Kozub <skozub@nvidia.com>:

[XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name

Merging this change closes #22368

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22368 from openxla:skozub/b200 124885af4f01cd4f54af08e39b270dce071f285a
",copybara-service[bot],2025-02-07 19:26:12+00:00,[],2025-02-08 01:16:47+00:00,2025-02-08 01:16:46+00:00,https://github.com/tensorflow/tensorflow/pull/86850,[],[],
2838848683,pull_request,closed,,Add genai ops library into LiteRT wheel.,"Add genai ops library into LiteRT wheel.
",copybara-service[bot],2025-02-07 18:45:44+00:00,['junjiang-lab'],2025-02-08 03:27:27+00:00,2025-02-08 03:27:26+00:00,https://github.com/tensorflow/tensorflow/pull/86849,[],[],
2838812820,pull_request,open,,Add no kotlin native interop aspect hint to signpost_profiler objc_library,"Add no kotlin native interop aspect hint to signpost_profiler objc_library
",copybara-service[bot],2025-02-07 18:25:31+00:00,[],2025-02-07 18:25:31+00:00,,https://github.com/tensorflow/tensorflow/pull/86848,[],[],
2838797482,pull_request,open,,litert: Do not rmark non-CPU allocation for intermediate Tensors,"litert: Do not rmark non-CPU allocation for intermediate Tensors

Intermediate Tensors are needed to be accessed by CPU kernels.
",copybara-service[bot],2025-02-07 18:15:59+00:00,['terryheo'],2025-02-07 18:16:00+00:00,,https://github.com/tensorflow/tensorflow/pull/86847,[],[],
2838681021,pull_request,open,,#sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings.,"#sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings.

This is needed so when free variables are lifted in StableHLO->HLO conversion, the ops will get a sharding with the free variable shardings.
",copybara-service[bot],2025-02-07 17:12:02+00:00,[],2025-02-07 20:16:00+00:00,,https://github.com/tensorflow/tensorflow/pull/86846,[],[],
2838672357,pull_request,open,,Update rules_python: 0.39.0 -> 1.1.0.,"Update rules_python: 0.39.0 -> 1.1.0.
",copybara-service[bot],2025-02-07 17:08:55+00:00,['belitskiy'],2025-02-07 17:08:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86845,[],[],
2838651841,pull_request,open,,Remove the dependency from stablehlo/passes to lite/transforms passes,"Remove the dependency from stablehlo/passes to lite/transforms passes

This dependency makes the `passes.h` from `lite` part of the `stablehlo/passes` public API, which exposes more details than intended.
",copybara-service[bot],2025-02-07 16:59:26+00:00,['vamsimanchala'],2025-02-07 16:59:27+00:00,,https://github.com/tensorflow/tensorflow/pull/86844,[],[],
2838561853,pull_request,open,,Add control deps when rotating send/recv,"Add control deps when rotating send/recv

For send/recv we have to ensure that they ar enot pipelined beyond any conflicting collective.
",copybara-service[bot],2025-02-07 16:22:27+00:00,['frgossen'],2025-02-08 01:27:33+00:00,,https://github.com/tensorflow/tensorflow/pull/86843,[],[],
2838535466,pull_request,closed,,PR #22454: [Shardy] Create dump directory.,"PR #22454: [Shardy] Create dump directory.

Imported from GitHub PR https://github.com/openxla/xla/pull/22454


Copybara import of the project:

--
ad713d8b5906a8bb535b2448a4a5eadd0d722cae by Yunlong Liu <yliu120@users.noreply.github.com>:

Update shardy_xla_pass.cc
--
d5bb15865b5601b6082ae871616b9611405e844e by Yunlong Liu <yliu120@users.noreply.github.com>:

Update BUILD

Merging this change closes #22454

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22454 from yliu120:create_dump_dir d5bb15865b5601b6082ae871616b9611405e844e
",copybara-service[bot],2025-02-07 16:09:57+00:00,[],2025-02-07 16:47:47+00:00,2025-02-07 16:47:45+00:00,https://github.com/tensorflow/tensorflow/pull/86842,[],[],
2838489175,pull_request,closed,,Enable peer access in Triton runtime,"Enable peer access in Triton runtime
",copybara-service[bot],2025-02-07 15:48:26+00:00,['gflegar'],2025-02-07 17:19:34+00:00,2025-02-07 17:19:33+00:00,https://github.com/tensorflow/tensorflow/pull/86841,[],[],
2838354915,pull_request,open,,[oneDNN CPU] Prevent perf penalty due to large constants copy,"This PR will help to avoid unnecessary copies of large constants among multiple clusters. It improves performance, especially on GNN models with large constant stems from graph embeddings.

fixes the issues in https://github.com/tensorflow/tensorflow/pull/76596",gaurides,2025-02-07 14:49:41+00:00,['gbaned'],2025-02-07 14:49:46+00:00,,https://github.com/tensorflow/tensorflow/pull/86840,"[('size:S', 'CL Change Size: Small')]",[],
2838288379,pull_request,open,,PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell,"PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell

Imported from GitHub PR https://github.com/openxla/xla/pull/22392

Skips the insertion of transposes of the operands of FP8 GEMMs not required on Blackwell systems.
Copybara import of the project:

--
c2921511c9657ec2acc8f9dc6ca4afa6e474736a by Philipp Hack <phack@nvidia.com>:

Skips the transposing of operands of FP8 GEMMs on Blackwell systems.

--
bcba1363156d89a78cd1d6e873ae0d30cfdd39bc by Philipp Hack <phack@nvidia.com>:

Skips the transposing of operands of FP8 GEMMs on Blackwell systems.

Merging this change closes #22392

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22392 from philipphack:u_fp8_transpose_blackwell_xla bcba1363156d89a78cd1d6e873ae0d30cfdd39bc
",copybara-service[bot],2025-02-07 14:22:15+00:00,[],2025-02-07 14:22:15+00:00,,https://github.com/tensorflow/tensorflow/pull/86839,[],[],
2838256745,pull_request,open,,Makes error message more descriptive when size mismatch is detected,"Makes error message more descriptive when size mismatch is detected
",copybara-service[bot],2025-02-07 14:07:48+00:00,[],2025-02-07 15:10:51+00:00,,https://github.com/tensorflow/tensorflow/pull/86838,[],[],
2838246329,pull_request,open,,litert::internal::OpenLib probes multiple share object paths and shouldn't output error messages for each missed probe attempt.,"litert::internal::OpenLib probes multiple share object paths and shouldn't output error messages for each missed probe attempt.
",copybara-service[bot],2025-02-07 14:03:13+00:00,[],2025-02-07 15:13:40+00:00,,https://github.com/tensorflow/tensorflow/pull/86837,[],[],
2838093406,pull_request,open,,Removing now redundant test.,"Removing now redundant test.

See https://github.com/triton-lang/triton/pull/5824 for details.
",copybara-service[bot],2025-02-07 12:53:23+00:00,[],2025-02-07 12:53:23+00:00,,https://github.com/tensorflow/tensorflow/pull/86836,[],[],
2838085734,pull_request,closed,,Integrate LLVM at llvm/llvm-project@c269182b13ab,"Integrate LLVM at llvm/llvm-project@c269182b13ab

Updates LLVM usage to match
[c269182b13ab](https://github.com/llvm/llvm-project/commit/c269182b13ab)
",copybara-service[bot],2025-02-07 12:49:29+00:00,[],2025-02-07 16:25:19+00:00,2025-02-07 16:25:18+00:00,https://github.com/tensorflow/tensorflow/pull/86835,[],[],
2838071295,pull_request,closed,,Update xla_builder link in documentation,"Update xla_builder link in documentation
",copybara-service[bot],2025-02-07 12:42:15+00:00,[],2025-02-07 16:12:36+00:00,2025-02-07 16:12:35+00:00,https://github.com/tensorflow/tensorflow/pull/86834,[],[],
2838070853,pull_request,open,,Fix build breakage introduced in,"Fix build breakage introduced in
https://github.com/tensorflow/tensorflow/commit/31a57e22308c6b5a0f2971fb48137613a95366ce

From within `core/` directory, depend on the `signature_runner` target
that is within `core/`, rather than the  `signature_runner`
target that is outside `core`.  That is, depend on
`tensorflow/lite/core:signature_runner` rather than
`tensorflow/lite:signature_runner`.
",copybara-service[bot],2025-02-07 12:42:02+00:00,[],2025-02-07 12:42:02+00:00,,https://github.com/tensorflow/tensorflow/pull/86833,[],[],
2837980836,pull_request,open,,PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,"PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline

Imported from GitHub PR https://github.com/openxla/xla/pull/22123


Copybara import of the project:

--
50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu <svaroglu@nvidia.com>:

Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline

--
3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu <svaroglu@nvidia.com>:

Add hlo_opt test

--
1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu <svaroglu@nvidia.com>:

Modify test

Merging this change closes #22123

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93
",copybara-service[bot],2025-02-07 11:56:09+00:00,[],2025-02-07 11:56:09+00:00,,https://github.com/tensorflow/tensorflow/pull/86832,[],[],
2837973893,pull_request,closed,,Improve negative stride handling in SymbolicTile,"Improve negative stride handling in SymbolicTile

Fix a bug in handling negative strides, and add a test case that exposes it.
We can have negative strides that are not just -1, e.g. with a combining
reshape.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93
",copybara-service[bot],2025-02-07 11:52:36+00:00,['akuegel'],2025-02-07 12:29:18+00:00,2025-02-07 12:29:17+00:00,https://github.com/tensorflow/tensorflow/pull/86831,[],[],
2837864328,pull_request,closed,,"[XLA:GPU] Add triton support test for ragged-all-to-all, rng-x and complex","[XLA:GPU] Add triton support test for ragged-all-to-all, rng-x and complex
",copybara-service[bot],2025-02-07 10:57:40+00:00,[],2025-02-07 14:56:37+00:00,2025-02-07 14:56:36+00:00,https://github.com/tensorflow/tensorflow/pull/86830,[],[],
2837702473,pull_request,closed,,FC supports all variants of Conv2D so re-write all suitable Convs as FC,"FC supports all variants of Conv2D so re-write all suitable Convs as FC
",copybara-service[bot],2025-02-07 09:42:42+00:00,['alankelly'],2025-02-07 10:56:56+00:00,2025-02-07 10:56:55+00:00,https://github.com/tensorflow/tensorflow/pull/86829,[],[],
2837687440,pull_request,open,,[NCCL] Upgrade TF NCCL version to 2.25.1,"[NCCL] Upgrade TF NCCL version to 2.25.1
",copybara-service[bot],2025-02-07 09:35:28+00:00,[],2025-02-07 13:58:05+00:00,,https://github.com/tensorflow/tensorflow/pull/86828,[],[],
2837678948,pull_request,closed,,[XLA] Tag timeout tests as `not_run:arm`,"[XLA] Tag timeout tests as `not_run:arm`

Similarly to cl/722883015 tagging also:

* //third_party/tensorflow/compiler/xla/tsl/distributed_runtime/coordination:client_server_test
",copybara-service[bot],2025-02-07 09:31:18+00:00,[],2025-02-07 10:20:29+00:00,2025-02-07 10:20:27+00:00,https://github.com/tensorflow/tensorflow/pull/86827,[],[],
2837659407,pull_request,open,,PR #22376: Fix erroneous HLO in testcases related to collective backend config.,"PR #22376: Fix erroneous HLO in testcases related to collective backend config.

Imported from GitHub PR https://github.com/openxla/xla/pull/22376

Collective backend config is expected to be under a key `""collective_backend_config""` under `backend_config`. In many testcases, this was not the case. This was not flagged by the testcases because the function `IsGPUSyncCollective` supressed any errors in the collective config and assumed `is_sync` to be `false` in those cases (which is generally the case we would like to test with collectives). Hence this went unnoticed so far. We have also stopped supressing the error in that function now.
Copybara import of the project:

--
1ebd7c8a2859ff9ff91cf65b334355e1f213681b by Shraiysh Vaishay <svaishay@nvidia.com>:

Fix erroneous HLO in testcases related to collective backend config.

Collective backend config is expected to be under a key
`""collective_backend_config""` under `backend_config`. In many
testcases, this was not the case. This was not flagged by the
testcases because the function `IsGPUSyncCollective` supressed any
errors in the collective config and assumed `is_sync` to be `false`
in those cases (which is generally the case we would like to test
with collectives). Hence this went unnoticed so far. We have also
stopped supressing the error in that function now.

--
1151e05ed74f57722d2599a837520773f46a7b77 by Shraiysh Vaishay <svaishay@nvidia.com>:

Addressed comments

Merging this change closes #22376

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22376 from shraiysh:fix_collective_backend_config 1151e05ed74f57722d2599a837520773f46a7b77
",copybara-service[bot],2025-02-07 09:21:47+00:00,[],2025-02-07 17:57:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86826,[],[],
2837655222,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:19:39+00:00,[],2025-02-08 09:38:53+00:00,2025-02-08 09:38:52+00:00,https://github.com/tensorflow/tensorflow/pull/86825,[],[],
2837653562,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:18:47+00:00,[],2025-02-07 09:18:47+00:00,,https://github.com/tensorflow/tensorflow/pull/86824,[],[],
2837651584,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:17:47+00:00,[],2025-02-08 06:40:53+00:00,2025-02-08 06:40:52+00:00,https://github.com/tensorflow/tensorflow/pull/86823,[],[],
2837651057,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:17:32+00:00,[],2025-02-07 09:17:32+00:00,,https://github.com/tensorflow/tensorflow/pull/86822,[],[],
2837649827,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:16:59+00:00,[],2025-02-07 09:16:59+00:00,,https://github.com/tensorflow/tensorflow/pull/86821,[],[],
2837649138,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:16:38+00:00,[],2025-02-07 09:16:38+00:00,,https://github.com/tensorflow/tensorflow/pull/86820,[],[],
2837647472,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:15:50+00:00,[],2025-02-07 09:15:50+00:00,,https://github.com/tensorflow/tensorflow/pull/86819,[],[],
2837646712,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:15:27+00:00,[],2025-02-07 09:15:27+00:00,,https://github.com/tensorflow/tensorflow/pull/86818,[],[],
2837645414,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:14:49+00:00,[],2025-02-08 10:21:45+00:00,2025-02-08 10:21:44+00:00,https://github.com/tensorflow/tensorflow/pull/86817,[],[],
2837645293,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93
",copybara-service[bot],2025-02-07 09:14:45+00:00,[],2025-02-07 11:55:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86816,[],[],
2837644083,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:14:09+00:00,[],2025-02-07 09:52:49+00:00,,https://github.com/tensorflow/tensorflow/pull/86815,[],[],
2837643791,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-07 09:14:00+00:00,[],2025-02-07 12:38:26+00:00,,https://github.com/tensorflow/tensorflow/pull/86814,[],[],
2837601261,pull_request,open,,Reverts 6a30431f98c4b01d83a7394b467528b6df2a666c,"Reverts 6a30431f98c4b01d83a7394b467528b6df2a666c
",copybara-service[bot],2025-02-07 08:53:46+00:00,['chsigg'],2025-02-07 08:53:47+00:00,,https://github.com/tensorflow/tensorflow/pull/86813,[],[],
2837515839,pull_request,closed,,Integrate LLVM at llvm/llvm-project@d8e0b130bd7b,"Integrate LLVM at llvm/llvm-project@d8e0b130bd7b

Updates LLVM usage to match
[d8e0b130bd7b](https://github.com/llvm/llvm-project/commit/d8e0b130bd7b)
",copybara-service[bot],2025-02-07 08:11:10+00:00,[],2025-02-07 11:43:13+00:00,2025-02-07 11:43:12+00:00,https://github.com/tensorflow/tensorflow/pull/86812,[],[],
2837433144,pull_request,open,,[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++,"[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++

Next step here is to write a specialization pass that takes the kernel loaded above and binds values to it (already done in prototype/scratch)
",copybara-service[bot],2025-02-07 07:24:59+00:00,[],2025-02-08 03:26:30+00:00,,https://github.com/tensorflow/tensorflow/pull/86811,[],[],
2837367767,pull_request,open,,Migrate to the new initializer,"Migrate to the new initializer
",copybara-service[bot],2025-02-07 06:41:42+00:00,['hhb'],2025-02-07 06:41:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86810,[],[],
2837223054,pull_request,open,,Integrate LLVM at llvm/llvm-project@d8e0b130bd7b,"Integrate LLVM at llvm/llvm-project@d8e0b130bd7b

Updates LLVM usage to match
[d8e0b130bd7b](https://github.com/llvm/llvm-project/commit/d8e0b130bd7b)
",copybara-service[bot],2025-02-07 04:42:02+00:00,[],2025-02-07 04:42:02+00:00,,https://github.com/tensorflow/tensorflow/pull/86809,[],[],
2837184102,pull_request,open,,litert: Integrate GPU Accelerator with CompiledModel,"litert: Integrate GPU Accelerator with CompiledModel

- Updated Accelerator APIs to create & destroy TfLiteDelegate objects.
- Deprecated ApplyToModel().
- Added GPU Delegate discovery logic.
",copybara-service[bot],2025-02-07 04:02:54+00:00,['terryheo'],2025-02-07 04:02:55+00:00,,https://github.com/tensorflow/tensorflow/pull/86808,[],[],
2837141433,pull_request,closed,,Leave the naming of the client graph to `GetOrCreateLoadedClientGraph()`,"Leave the naming of the client graph to `GetOrCreateLoadedClientGraph()`
",copybara-service[bot],2025-02-07 03:22:18+00:00,['AspirinSJL'],2025-02-07 22:53:01+00:00,2025-02-07 22:53:00+00:00,https://github.com/tensorflow/tensorflow/pull/86807,[],[],
2837063288,pull_request,closed,,Update TFRT dependency to use revision,"Update TFRT dependency to use revision
http://github.com/tensorflow/runtime/commit/604b26fdad348a0de07d7a9ef2d745b63abdf3b8.
",copybara-service[bot],2025-02-07 02:01:24+00:00,[],2025-02-07 03:23:19+00:00,2025-02-07 03:23:18+00:00,https://github.com/tensorflow/tensorflow/pull/86806,[],[],
2837055324,pull_request,open,,[xla:cpu] Add OneDnnFusionThunk,"[xla:cpu] Add OneDnnFusionThunk
",copybara-service[bot],2025-02-07 01:52:58+00:00,['ezhulenev'],2025-02-07 01:52:59+00:00,,https://github.com/tensorflow/tensorflow/pull/86805,[],[],
2837037398,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-07 01:37:33+00:00,['ddunl'],2025-02-07 01:37:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86804,[],[],
2837037150,pull_request,closed,,[XLA:Python] Remove unused and redundant dependency of :py_client on :py_client_gpu.,"[XLA:Python] Remove unused and redundant dependency of :py_client on :py_client_gpu.

This is unused.
",copybara-service[bot],2025-02-07 01:37:19+00:00,[],2025-02-07 16:38:39+00:00,2025-02-07 16:38:38+00:00,https://github.com/tensorflow/tensorflow/pull/86803,[],[],
2837025214,pull_request,open,,[xla:cpu] CompileAheadOfTime: disable thunks,"[xla:cpu] CompileAheadOfTime: disable thunks

Currently thunks are enabled by default, yet we do not use
thunks for AOT compilation. How can this be? The ahead-of-time
compilation path unconditionally uses the legacy emitters,
which explains why even if the xla_cpu_use_thunk_runtime flag
is set to true, we still do not get thunks in tfcompile.

Rather than audit all callers and disable thunks there,
here we explicitly disable the flag during AOT compilation
so that HLO passes correctly know whether or not we are using
thunks. This will prevent miscompiles when the no-thunks and
thunks HLO passes meaningfully diverge (so far their differences
are negligible), which is about to occur due to the imminent
landing of fusion emitters.
",copybara-service[bot],2025-02-07 01:29:50+00:00,['cota'],2025-02-07 01:29:51+00:00,,https://github.com/tensorflow/tensorflow/pull/86802,[],[],
2837003949,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-07 01:18:17+00:00,['ddunl'],2025-02-07 01:18:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86801,[],[],
2837002165,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-07 01:17:20+00:00,['ddunl'],2025-02-07 02:27:18+00:00,2025-02-07 02:27:18+00:00,https://github.com/tensorflow/tensorflow/pull/86800,[],[],
2836987528,pull_request,open,,Fix GPU plugin profiling for multihost_hlo_runner.,"Fix GPU plugin profiling for multihost_hlo_runner.
",copybara-service[bot],2025-02-07 01:05:39+00:00,['juliagmt-google'],2025-02-08 05:00:05+00:00,,https://github.com/tensorflow/tensorflow/pull/86799,[],[],
2836982491,pull_request,closed,,Internal only change,"Internal only change
",copybara-service[bot],2025-02-07 01:01:44+00:00,['LukeBoyer'],2025-02-08 03:48:02+00:00,2025-02-08 03:48:01+00:00,https://github.com/tensorflow/tensorflow/pull/86798,[],[],
2836944189,pull_request,closed,,litert: Mark non-CPU allocation for TensorBuffer mapped Tensors,"litert: Mark non-CPU allocation for TensorBuffer mapped Tensors

Otherwise, TFLite memory planner allocates memory for these Tensors.
",copybara-service[bot],2025-02-07 00:40:19+00:00,['terryheo'],2025-02-07 03:04:46+00:00,2025-02-07 03:04:45+00:00,https://github.com/tensorflow/tensorflow/pull/86797,[],[],
2836926451,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-07 00:23:03+00:00,['ddunl'],2025-02-07 02:17:00+00:00,,https://github.com/tensorflow/tensorflow/pull/86796,[],[],
2836870815,pull_request,open,,[XLA] Googly changes,"[XLA] Googly changes

Support fall through of handling custom calls when running dynamic dimension inference. We still want to handle the generic cases if we add handlers for specialized cases.
",copybara-service[bot],2025-02-06 23:55:07+00:00,[],2025-02-07 19:36:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86795,[],[],
2836845530,pull_request,closed,,Document the purpose of tsl/ under xla/,"Document the purpose of tsl/ under xla/
",copybara-service[bot],2025-02-06 23:35:24+00:00,[],2025-02-07 02:47:11+00:00,2025-02-07 02:47:10+00:00,https://github.com/tensorflow/tensorflow/pull/86794,[],[],
2836826894,pull_request,open,,PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell,"PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell

Imported from GitHub PR https://github.com/openxla/xla/pull/22392

Skips the insertion of transposes of the operands of FP8 GEMMs not required on Blackwell systems.
Copybara import of the project:

--
c2921511c9657ec2acc8f9dc6ca4afa6e474736a by Philipp Hack <phack@nvidia.com>:

Skips the transposing of operands of FP8 GEMMs on Blackwell systems.

--
bcba1363156d89a78cd1d6e873ae0d30cfdd39bc by Philipp Hack <phack@nvidia.com>:

Skips the transposing of operands of FP8 GEMMs on Blackwell systems.

Merging this change closes #22392

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22392 from philipphack:u_fp8_transpose_blackwell_xla bcba1363156d89a78cd1d6e873ae0d30cfdd39bc
",copybara-service[bot],2025-02-06 23:20:10+00:00,[],2025-02-07 03:01:11+00:00,,https://github.com/tensorflow/tensorflow/pull/86793,[],[],
2836821230,pull_request,closed,,[XLA] Allow synchronously invoking computations on separate threads,"[XLA] Allow synchronously invoking computations on separate threads

If we allow asynchronously invoking a computation on a separate thread by wrapping the kCall op in a async-{start,done} pair, then we should also allow a synchronous version of kCall to do the same. Same logic applies kCustomCall.
",copybara-service[bot],2025-02-06 23:15:05+00:00,[],2025-02-07 23:39:36+00:00,2025-02-07 23:39:35+00:00,https://github.com/tensorflow/tensorflow/pull/86792,[],[],
2836814566,pull_request,open,,Add highway implementation for sparse_matmul_op,"Add highway implementation for sparse_matmul_op
",copybara-service[bot],2025-02-06 23:10:41+00:00,[],2025-02-06 23:10:41+00:00,,https://github.com/tensorflow/tensorflow/pull/86791,[],[],
2836812080,pull_request,open,,Provide lax.composites to express quantization operations.,"Provide lax.composites to express quantization operations.
",copybara-service[bot],2025-02-06 23:08:46+00:00,['sdasgup3'],2025-02-08 03:13:19+00:00,,https://github.com/tensorflow/tensorflow/pull/86790,[],[],
2836807379,pull_request,open,,add an option to remove call-start/done to sparse core computations.,"add an option to remove call-start/done to sparse core computations.
used for hybridsim until it support sparsecore.
",copybara-service[bot],2025-02-06 23:04:51+00:00,[],2025-02-06 23:04:51+00:00,,https://github.com/tensorflow/tensorflow/pull/86789,[],[],
2836776254,pull_request,closed,,PjRt clients do not support ToLiteralSync with empty tuple shapes.,"PjRt clients do not support ToLiteralSync with empty tuple shapes.

Instead of calling ToLiteralSync we just construct a new empty tuple literal
instead.
",copybara-service[bot],2025-02-06 22:39:51+00:00,[],2025-02-07 19:00:44+00:00,2025-02-07 19:00:43+00:00,https://github.com/tensorflow/tensorflow/pull/86788,[],[],
2836768256,pull_request,open,,Integrate StableHLO at openxla/stablehlo@04c5a341,"Integrate StableHLO at openxla/stablehlo@04c5a341
",copybara-service[bot],2025-02-06 22:33:33+00:00,['GleasonK'],2025-02-07 15:27:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86787,[],[],
2836699806,pull_request,open,,[MultiHostHloRunner] Fix profiler lifetime in multihost hlo runner.,"[MultiHostHloRunner] Fix profiler lifetime in multihost hlo runner.
",copybara-service[bot],2025-02-06 21:53:03+00:00,['juliagmt-google'],2025-02-07 00:27:11+00:00,,https://github.com/tensorflow/tensorflow/pull/86786,[],[],
2836653531,pull_request,closed,,[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,"[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch
",copybara-service[bot],2025-02-06 21:27:26+00:00,[],2025-02-06 22:08:12+00:00,2025-02-06 22:08:11+00:00,https://github.com/tensorflow/tensorflow/pull/86785,[],[],
2836653449,pull_request,closed,,[XLA:Python] Remove unused reference to CUDA/ROCM headers.,"[XLA:Python] Remove unused reference to CUDA/ROCM headers.
",copybara-service[bot],2025-02-06 21:27:22+00:00,[],2025-02-07 15:18:46+00:00,2025-02-07 15:18:45+00:00,https://github.com/tensorflow/tensorflow/pull/86784,[],[],
2836612646,pull_request,open,,Remove Thunk::Cleanup method.,"Remove Thunk::Cleanup method.

The method was effectively unused, since it wasn't overridden by SequentialThunk, and so SequentialThunk wouldn't call Cleanup on its subthunks.

NcclRaggedAllToAllStartThunk overrode Cleanup to free some device buffers, but these were never freed since Cleanup was not called. The memory is now stored in DeviceMemoryHandles, which automatically free the buffers in the destructor.
",copybara-service[bot],2025-02-06 21:14:48+00:00,['reedwm'],2025-02-07 10:14:24+00:00,,https://github.com/tensorflow/tensorflow/pull/86783,[],[],
2836596771,pull_request,closed,,Internal change only.,"Internal change only.
",copybara-service[bot],2025-02-06 21:08:39+00:00,['BlaziusMaximus'],2025-02-07 19:09:22+00:00,2025-02-07 19:09:21+00:00,https://github.com/tensorflow/tensorflow/pull/86782,[],[],
2836590516,pull_request,open,,Rolling back due to broken tests.,"Rolling back due to broken tests.

Original change: Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices (roll-forward).

Reverts e3b0866fb48abd1d20a074a8361608a72c20ad3f
",copybara-service[bot],2025-02-06 21:05:15+00:00,[],2025-02-06 21:05:15+00:00,,https://github.com/tensorflow/tensorflow/pull/86781,[],[],
2836555762,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:47:17+00:00,['ddunl'],2025-02-06 21:42:20+00:00,2025-02-06 21:42:19+00:00,https://github.com/tensorflow/tensorflow/pull/86780,[],[],
2836549785,pull_request,closed,,Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,"Update and/or wrap `Executable` when calling into `HloRunnerInterface`.
",copybara-service[bot],2025-02-06 20:44:17+00:00,[],2025-02-06 22:31:56+00:00,2025-02-06 22:31:56+00:00,https://github.com/tensorflow/tensorflow/pull/86779,[],[],
2836544984,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:42:16+00:00,['ddunl'],2025-02-06 21:29:22+00:00,2025-02-06 21:29:21+00:00,https://github.com/tensorflow/tensorflow/pull/86778,[],[],
2836538172,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:39:34+00:00,['ddunl'],2025-02-07 22:41:40+00:00,2025-02-07 22:41:39+00:00,https://github.com/tensorflow/tensorflow/pull/86777,[],[],
2836520691,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:30:28+00:00,['ddunl'],2025-02-06 20:30:29+00:00,,https://github.com/tensorflow/tensorflow/pull/86776,[],[],
2836518522,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:29:11+00:00,['ddunl'],2025-02-07 23:30:30+00:00,2025-02-07 23:30:29+00:00,https://github.com/tensorflow/tensorflow/pull/86775,[],[],
2836515459,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 20:27:49+00:00,['ddunl'],2025-02-06 20:27:50+00:00,,https://github.com/tensorflow/tensorflow/pull/86774,[],[],
2836493704,pull_request,open,,Fix 03 broken links in play_services.md,"Hi, Team
I found 03 broken documentation links in this [play_services.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/play_services.md) file so I have updated those links to functional links. Please review and merge this change as appropriate.

Thank you for your consideration.",gaikwadrahul8,2025-02-06 20:15:28+00:00,['gbaned'],2025-02-06 20:15:32+00:00,,https://github.com/tensorflow/tensorflow/pull/86773,"[('size:XS', 'CL Change Size: Extra Small')]",[],
2836486309,pull_request,open,,Extract conflicting collective analysis,"Extract conflicting collective analysis
",copybara-service[bot],2025-02-06 20:11:30+00:00,['frgossen'],2025-02-07 16:13:13+00:00,,https://github.com/tensorflow/tensorflow/pull/86772,[],[],
2836475600,pull_request,open,,Make a continuous only Github Actions based TensorFlow CPU build,"Make a continuous only Github Actions based TensorFlow CPU build

In the future this will be promoted to presubmit

Passing run here: https://github.com/openxla/xla/actions/runs/13185352502/job/36806036630?pr=22264
",copybara-service[bot],2025-02-06 20:06:35+00:00,['ddunl'],2025-02-06 20:06:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86771,[],[],
2836453188,pull_request,closed,,Remove TensorFlow CPU Kokoro build now that we have equivalent GitHub Actions build,"Remove TensorFlow CPU Kokoro build now that we have equivalent GitHub Actions build
",copybara-service[bot],2025-02-06 19:56:45+00:00,['ddunl'],2025-02-06 21:01:19+00:00,2025-02-06 21:01:17+00:00,https://github.com/tensorflow/tensorflow/pull/86770,[],[],
2836416901,pull_request,closed,,Fix and add some log,"Fix and add some log
",copybara-service[bot],2025-02-06 19:38:40+00:00,[],2025-02-07 01:19:28+00:00,2025-02-07 01:19:28+00:00,https://github.com/tensorflow/tensorflow/pull/86769,[],[],
2836397864,pull_request,open,,Define TFL_GatherNd as a DRQ-able Op.,"Define TFL_GatherNd as a DRQ-able Op.
",copybara-service[bot],2025-02-06 19:30:07+00:00,['vamsimanchala'],2025-02-06 19:30:08+00:00,,https://github.com/tensorflow/tensorflow/pull/86768,[],[],
2836392103,pull_request,closed,,PR #22382: [ROCm] Fix test scripts under `build_tools/rocm`,"PR #22382: [ROCm] Fix test scripts under `build_tools/rocm`

Imported from GitHub PR https://github.com/openxla/xla/pull/22382


Copybara import of the project:

--
58d80b5b44e7dc1bdfd9567debdfb7b588e06e70 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Fix test scripts under build_tools/rocm

Merging this change closes #22382

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22382 from ROCm:ci_fix_rocm_test_scripts_20250205 58d80b5b44e7dc1bdfd9567debdfb7b588e06e70
",copybara-service[bot],2025-02-06 19:26:52+00:00,[],2025-02-07 11:50:25+00:00,2025-02-07 11:50:25+00:00,https://github.com/tensorflow/tensorflow/pull/86767,[],[],
2836380099,pull_request,open,,Add TfFunctionDb generation in OSS.,"Add TfFunctionDb generation in OSS.
",copybara-service[bot],2025-02-06 19:20:33+00:00,[],2025-02-06 19:20:33+00:00,,https://github.com/tensorflow/tensorflow/pull/86766,[],[],
2836324093,pull_request,closed,,"[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the key-value store.","[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the key-value store.

Previously, re-autotuning the same set of fusions on the same host at different
points during the lifetime of a given key-value store would cause sharded
autotuning to crash. This was surfaced more prominently after a
[recent change](https://github.com/openxla/xla/pull/22164) started
canonicalizing fusion strings before using their hash as a key in the
key-value store. Autotuning two different modules containing the same set of
fusions could therefore result in a collision during the second compilation,
and a subsequent crash.

Simply allowing to read from the cache did not seem like a good solution,
given that different modules may be compiled with different options, and the
cache can therefore hold stale results that shouldn't be accessed.

There didn't seem to be a very good way to resolve this issue given the current
design of (sharded) autotuning. The proper solution would be to remove
autotuning from the compilation pipeline. To fix the issue faster, we elected
here to also hash the module as part of the cache key, and to allow cache hits.
Fetching more runtime information to uniquify the cache key was deemed
undesirable: for one thing, the cache key derivation needs to be deterministic
across hosts; for a second, passing enough information down from the runtime
to truly make results unique would force us to further intertwine levels of
abstraction that should be independent.

There is still an issue whereby autotuning the same set of fusions within the
same module twice within the lifetime of the same PjRt client could result in
fetching stale cache data---if the user changes compilation options between
both runs. Nevertheless, this issue was already there, and this change makes
it more unlikely to occur in common scenarios.

The hope is that switching to a better compilation model (single host
compilation) and hoisting autotuning out of the compilation pipeline will
allow us to definitely resolve this issue in the future.
",copybara-service[bot],2025-02-06 18:48:32+00:00,[],2025-02-07 14:33:52+00:00,2025-02-07 14:33:51+00:00,https://github.com/tensorflow/tensorflow/pull/86765,[],[],
2836186176,pull_request,closed,,PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,"PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline

Imported from GitHub PR https://github.com/openxla/xla/pull/22123


Copybara import of the project:

--
50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu <svaroglu@nvidia.com>:

Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline

--
3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu <svaroglu@nvidia.com>:

Add hlo_opt test

--
1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu <svaroglu@nvidia.com>:

Modify test

Merging this change closes #22123

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93
",copybara-service[bot],2025-02-06 17:39:50+00:00,[],2025-02-07 12:03:49+00:00,2025-02-07 12:03:48+00:00,https://github.com/tensorflow/tensorflow/pull/86764,[],[],
2836131142,pull_request,open,,[XLA:GPU] add --xla_gpu_unsupported_enable_triton_gemm_fusions,"[XLA:GPU] add --xla_gpu_unsupported_enable_triton_gemm_fusions

it is a feature flag for new code path to emit tiled dots in triton
",copybara-service[bot],2025-02-06 17:11:34+00:00,['metaflow'],2025-02-06 17:11:35+00:00,,https://github.com/tensorflow/tensorflow/pull/86763,[],[],
2836062738,pull_request,closed,,[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.,"[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.

Instead, check for this case in `ResolveUsers` and `ResolveOperand`, by querying whether the `fused_expression_root` is part of the `HloFusionAdaptor`.

This prevents us from stepping into nested fusions.
",copybara-service[bot],2025-02-06 16:46:15+00:00,['chsigg'],2025-02-07 13:47:52+00:00,2025-02-07 13:47:51+00:00,https://github.com/tensorflow/tensorflow/pull/86762,[],[],
2836034120,pull_request,closed,,Add transitional header for mlir_roundtrip_flags,"Add transitional header for mlir_roundtrip_flags
",copybara-service[bot],2025-02-06 16:36:03+00:00,['rocketas'],2025-02-06 17:06:12+00:00,2025-02-06 17:06:11+00:00,https://github.com/tensorflow/tensorflow/pull/86761,[],[],
2835975096,pull_request,closed,,Fix race condition in FenceInsertionPass,"Fix race condition in FenceInsertionPass

It looks like the idea here was to add some basic memoization so we don't end
up with a ton of recursive calls, and potentially deadlock. However, doing that
through a static variable is problematic both because it's not thread-safe, and
because it's a silent memory leak, since we never free up the set (so a
long-running program would just continue adding stuff to it as we compile new
kernels indefinitely).

I'm still trying to get a good upstreamable reproducer, but this should fix the
issue for now so we don't crash in production.
",copybara-service[bot],2025-02-06 16:15:22+00:00,['gflegar'],2025-02-07 11:30:25+00:00,2025-02-07 11:30:24+00:00,https://github.com/tensorflow/tensorflow/pull/86760,[],[],
2835966515,pull_request,open,,PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option,"PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option

Imported from GitHub PR https://github.com/openxla/xla/pull/21965

Expose GPU option `should_stage_host_to_device_transfers ` as configurable in PJRT client create function.
This allow the end user to override the default value which is `true`.

ref: [openxla@xla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h](https://github.com/openxla/xla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h)

Since the option is living in a hashmap of type PJRT_NamedValue, it seems that I don't break the C PJRT API interface versioning.
Copybara import of the project:

--
18b34c19938814b68bfd863fd5e603f08e9e824c by Hugo Mano <hugo@zml.ai>:

[PJRT]  Expose should_stage_host_to_device_transfers  as create option

Expose GPU option `should_stage_host_to_device_transfers `
as configuration in PJRT client create func.

--
a412723fe4aa833dbfc0c27a2629d61bd68b34f7 by Hugo Mano <hugo@zml.ai>:

Address comments

Merging this change closes #21965

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21965 from hugomano:hugmano/pjrt-expose-should_stage_host_to_device_transfers a412723fe4aa833dbfc0c27a2629d61bd68b34f7
",copybara-service[bot],2025-02-06 16:11:51+00:00,[],2025-02-06 17:19:02+00:00,,https://github.com/tensorflow/tensorflow/pull/86759,[],[],
2835959799,pull_request,closed,,Move p2p pipeliner into its own pass,"Move p2p pipeliner into its own pass

This is in preparation of global post processing of control dependencies,
which is not possible in the local post processing callbacks.
",copybara-service[bot],2025-02-06 16:09:07+00:00,['frgossen'],2025-02-07 17:06:30+00:00,2025-02-07 17:06:29+00:00,https://github.com/tensorflow/tensorflow/pull/86758,[],[],
2835867126,pull_request,closed,,[xla:emitters] fix type mismatch for several passes,"[xla:emitters] fix type mismatch for several passes

I've seen Windows failures because of these after applying
some upcoming changes. Fix them.
",copybara-service[bot],2025-02-06 15:36:16+00:00,['cota'],2025-02-06 18:04:38+00:00,2025-02-06 18:04:37+00:00,https://github.com/tensorflow/tensorflow/pull/86757,[],[],
2835819435,pull_request,open,,[xla:cpu] CompileAheadOfTime: add nothunks check,"[xla:cpu] CompileAheadOfTime: add nothunks check
",copybara-service[bot],2025-02-06 15:17:14+00:00,['cota'],2025-02-06 23:37:26+00:00,,https://github.com/tensorflow/tensorflow/pull/86756,[],[],
2835817835,pull_request,open,,[XLA:GPU] Add collective perf table tool.,"[XLA:GPU] Add collective perf table tool.

This produces a derating curve of network throughput at an HLO op level.
",copybara-service[bot],2025-02-06 15:16:36+00:00,[],2025-02-07 15:36:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86755,[],[],
2835736102,pull_request,closed,,[XLA:GPU] Model bytes accessed.,"[XLA:GPU] Model bytes accessed.
",copybara-service[bot],2025-02-06 14:44:28+00:00,[],2025-02-07 16:03:32+00:00,2025-02-07 16:03:32+00:00,https://github.com/tensorflow/tensorflow/pull/86754,[],[],
2835663697,pull_request,open,,[tf:xla] tfcompile: explicitly disable thunk runtime for AOT,"[tf:xla] tfcompile: explicitly disable thunk runtime for AOT

Currently thunks are enabled by default, yet we do not use
thunks for AOT compilation. How can this be? The ahead-of-time
compilation path unconditionally uses the legacy emitters, which
explains why even if the xla_cpu_use_thunk_runtime flag is set to
true, we still do not get thunks in tfcompile.

Explicitly disable the flag so that HLO passes correctly know
whether or not we are using thunks. This will prevent miscompiles
when the no-thunks and thunks HLO passes meaningfully diverge (so
far their differences are negligible), which is about to occur
due to the imminent landing of fusion emitters.

tl;dr: we should have disabled thunks explicitly for tfcompile
from the beginning. Do it now.
",copybara-service[bot],2025-02-06 14:17:00+00:00,['cota'],2025-02-06 14:17:01+00:00,,https://github.com/tensorflow/tensorflow/pull/86753,[],[],
2835531584,pull_request,closed,,NFC: Polish HLO text in `hlo_traversal_test.cc`.,"NFC: Polish HLO text in `hlo_traversal_test.cc`.

* Improve readability of HLO IR and make op names consistent.
* Use `TF_ASSERT_OK_AND_ASSIGN` to prevent the test from crashing with a fatal error when `ParseAndReturnVerifiedModule` fails
* Define a formatter for `HloInstructionAdaptor` for more informative error messages

This CL does not change any test logic.
",copybara-service[bot],2025-02-06 13:25:39+00:00,['chsigg'],2025-02-06 17:42:13+00:00,2025-02-06 17:42:12+00:00,https://github.com/tensorflow/tensorflow/pull/86751,[],[],
2835531087,pull_request,open,,Upgrade Abseil to lts_2024_07_22.,"Upgrade Abseil to lts_2024_07_22.
",copybara-service[bot],2025-02-06 13:25:26+00:00,[],2025-02-06 14:05:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86750,[],[],
2835396575,pull_request,open,,Integrate LLVM at llvm/llvm-project@f308af757d72,"Integrate LLVM at llvm/llvm-project@f308af757d72

Updates LLVM usage to match
[f308af757d72](https://github.com/llvm/llvm-project/commit/f308af757d72)
",copybara-service[bot],2025-02-06 12:26:57+00:00,[],2025-02-06 12:26:57+00:00,,https://github.com/tensorflow/tensorflow/pull/86749,[],[],
2835239599,pull_request,open,,FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,"FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae
",copybara-service[bot],2025-02-06 11:15:56+00:00,[],2025-02-06 11:15:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86748,[],[],
2835191955,pull_request,closed,,Remove normalization for making stride always positive.,"Remove normalization for making stride always positive.

This causes issues down the line when we compute tile_offset_indexing.
If we want to emit a reverse op, we would actually need to normalize (but then
also adjust the tile offsets to make it work). We can bring normalization back
once we want to support emitting reverse, and have a fix for adjusting the tile
offsets. Without normalization, reverse can potentially be supported by
propagating it up to the load (by using negative stride). Only remaining
problem here would be to handle padding correctly.
",copybara-service[bot],2025-02-06 10:54:32+00:00,['akuegel'],2025-02-07 14:45:48+00:00,2025-02-07 14:45:47+00:00,https://github.com/tensorflow/tensorflow/pull/86747,[],[],
2835183102,pull_request,open,,Integrate LLVM at llvm/llvm-project@e151b1d1f678,"Integrate LLVM at llvm/llvm-project@e151b1d1f678

Updates LLVM usage to match
[e151b1d1f678](https://github.com/llvm/llvm-project/commit/e151b1d1f678)
",copybara-service[bot],2025-02-06 10:50:32+00:00,[],2025-02-06 10:50:32+00:00,,https://github.com/tensorflow/tensorflow/pull/86746,[],[],
2835147715,pull_request,open,,Allow sharing of subgraph inputs for dynamic update slice,"Allow sharing of subgraph inputs for dynamic update slice
",copybara-service[bot],2025-02-06 10:35:33+00:00,['alankelly'],2025-02-06 10:35:41+00:00,,https://github.com/tensorflow/tensorflow/pull/86745,[],[],
2835086653,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 10:08:47+00:00,[],2025-02-06 10:08:47+00:00,,https://github.com/tensorflow/tensorflow/pull/86744,[],[],
2835084353,pull_request,closed,,[XLA:GPU] NFC: move HloFusionInstructionAdaptor to implementation file.,"[XLA:GPU] NFC: move HloFusionInstructionAdaptor to implementation file.
",copybara-service[bot],2025-02-06 10:07:44+00:00,['chsigg'],2025-02-06 11:00:18+00:00,2025-02-06 11:00:17+00:00,https://github.com/tensorflow/tensorflow/pull/86743,[],[],
2835075275,pull_request,closed,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`
",copybara-service[bot],2025-02-06 10:03:55+00:00,[],2025-02-06 18:20:39+00:00,2025-02-06 18:20:38+00:00,https://github.com/tensorflow/tensorflow/pull/86742,[],[],
2835005535,pull_request,closed,,PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell

Imported from GitHub PR https://github.com/openxla/xla/pull/22256

Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph) - this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10).

The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32).
MXFP8 docs: https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf

The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness.

The pass is also enabled in the same PR.
Copybara import of the project:

--
145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub <skozub@nvidia.com>:

[XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell

Merging this change closes #22256

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930
",copybara-service[bot],2025-02-06 09:32:53+00:00,[],2025-02-06 11:56:41+00:00,2025-02-06 11:56:41+00:00,https://github.com/tensorflow/tensorflow/pull/86741,[],[],
2835000799,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:30:58+00:00,[],2025-02-06 09:30:58+00:00,,https://github.com/tensorflow/tensorflow/pull/86740,[],[],
2834992929,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:27:07+00:00,[],2025-02-07 05:26:57+00:00,2025-02-07 05:26:51+00:00,https://github.com/tensorflow/tensorflow/pull/86739,[],[],
2834990587,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:26:08+00:00,[],2025-02-07 07:08:20+00:00,,https://github.com/tensorflow/tensorflow/pull/86738,[],[],
2834985332,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:23:56+00:00,[],2025-02-06 09:23:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86737,[],[],
2834980392,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:21:44+00:00,[],2025-02-06 12:08:30+00:00,,https://github.com/tensorflow/tensorflow/pull/86736,[],[],
2834978555,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae
",copybara-service[bot],2025-02-06 09:20:54+00:00,[],2025-02-06 11:16:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86735,[],[],
2834975363,pull_request,closed,,Fix typos in documentation strings,"Hi, Team
I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",Venkat6871,2025-02-06 09:19:26+00:00,['gbaned'],2025-02-06 17:16:52+00:00,2025-02-06 17:16:49+00:00,https://github.com/tensorflow/tensorflow/pull/86734,"[('size:S', 'CL Change Size: Small')]",[],
2834974065,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:18:50+00:00,[],2025-02-06 09:18:50+00:00,,https://github.com/tensorflow/tensorflow/pull/86733,[],[],
2834972917,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:18:18+00:00,[],2025-02-06 11:52:58+00:00,,https://github.com/tensorflow/tensorflow/pull/86732,[],[],
2834970330,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:17:05+00:00,[],2025-02-07 05:35:24+00:00,,https://github.com/tensorflow/tensorflow/pull/86731,[],[],
2834969573,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:16:44+00:00,[],2025-02-06 11:25:29+00:00,,https://github.com/tensorflow/tensorflow/pull/86730,[],[],
2834966929,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae
",copybara-service[bot],2025-02-06 09:15:33+00:00,[],2025-02-06 11:19:03+00:00,,https://github.com/tensorflow/tensorflow/pull/86729,[],[],
2834966750,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:15:28+00:00,[],2025-02-07 08:02:40+00:00,,https://github.com/tensorflow/tensorflow/pull/86728,[],[],
2834966531,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:15:21+00:00,[],2025-02-06 09:15:21+00:00,,https://github.com/tensorflow/tensorflow/pull/86727,[],[],
2834966248,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:15:14+00:00,[],2025-02-06 09:15:14+00:00,,https://github.com/tensorflow/tensorflow/pull/86726,[],[],
2834966106,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:15:10+00:00,[],2025-02-06 11:21:55+00:00,,https://github.com/tensorflow/tensorflow/pull/86725,[],[],
2834965161,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:14:45+00:00,[],2025-02-06 09:14:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86724,[],[],
2834964937,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:14:39+00:00,[],2025-02-06 09:14:39+00:00,,https://github.com/tensorflow/tensorflow/pull/86723,[],[],
2834963922,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930
",copybara-service[bot],2025-02-06 09:14:11+00:00,[],2025-02-06 12:05:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86722,[],[],
2834962275,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 09:13:38+00:00,[],2025-02-06 09:13:38+00:00,,https://github.com/tensorflow/tensorflow/pull/86721,[],[],
2834918494,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 08:52:53+00:00,[],2025-02-06 08:52:53+00:00,,https://github.com/tensorflow/tensorflow/pull/86720,[],[],
2834916701,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 08:52:01+00:00,[],2025-02-06 08:52:01+00:00,,https://github.com/tensorflow/tensorflow/pull/86719,[],[],
2834887159,pull_request,open,,Integrate LLVM at llvm/llvm-project@8b448842c476,"Integrate LLVM at llvm/llvm-project@8b448842c476

Updates LLVM usage to match
[8b448842c476](https://github.com/llvm/llvm-project/commit/8b448842c476)
",copybara-service[bot],2025-02-06 08:37:14+00:00,[],2025-02-06 16:13:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86718,[],[],
2834805410,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-06 07:53:53+00:00,[],2025-02-07 05:52:56+00:00,2025-02-07 05:52:55+00:00,https://github.com/tensorflow/tensorflow/pull/86717,[],[],
2834681896,pull_request,open,,[ODML] StablehloOptimizePass : MHLO -> StableHLO,"[ODML] StablehloOptimizePass : MHLO -> StableHLO
",copybara-service[bot],2025-02-06 06:39:51+00:00,[],2025-02-08 00:21:00+00:00,,https://github.com/tensorflow/tensorflow/pull/86716,[],[],
2834652310,pull_request,open,,Qualcomm AI Engine Direct - Validate and update soc table,"Summary:

- Validate exisiting soc infos
- Add some new soc infos",chuntl,2025-02-06 06:19:43+00:00,['gbaned'],2025-02-06 07:32:07+00:00,,https://github.com/tensorflow/tensorflow/pull/86715,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:S', 'CL Change Size: Small')]",[],
2834650816,pull_request,open,,Simplify flag parsing in hlo_runner.cc using fixed_options_flag.,"Simplify flag parsing in hlo_runner.cc using fixed_options_flag.
",copybara-service[bot],2025-02-06 06:18:34+00:00,[],2025-02-06 16:13:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86714,[],[],
2834647150,pull_request,closed,,Qualcomm AI Engine Direct - Validate and update soc table,"Summary:

- Validate exisiting soc infos
- Add some new soc infos",chuntl,2025-02-06 06:15:51+00:00,['gbaned'],2025-02-06 06:16:12+00:00,2025-02-06 06:16:12+00:00,https://github.com/tensorflow/tensorflow/pull/86713,"[('size:S', 'CL Change Size: Small')]","[{'comment_id': 2638934118, 'issue_id': 2834647150, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86713/checks?check_run_id=36766498177) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 2, 6, 6, 15, 56, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-02-06 06:15:56 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86713/checks?check_run_id=36766498177) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2834619296,pull_request,open,,"Optimize pattern for mul(a, a) -> pow(a, 2)","Optimize pattern for mul(a, a) -> pow(a, 2)
Also,
- mul(pow(a, 2), a)) -> pow(a, 3)
- mul(a, square(a)) -> pow(a, 3)
",copybara-service[bot],2025-02-06 05:56:30+00:00,['vamsimanchala'],2025-02-06 05:56:31+00:00,,https://github.com/tensorflow/tensorflow/pull/86712,[],[],
2834504894,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-06 04:19:56+00:00,['ddunl'],2025-02-06 04:55:47+00:00,2025-02-06 04:55:46+00:00,https://github.com/tensorflow/tensorflow/pull/86711,[],[],
2834497932,pull_request,closed,,Simplify flag parsing in functional_hlo_runner.cc using the fixed_options_flag library.,"Simplify flag parsing in functional_hlo_runner.cc using the fixed_options_flag library.

The new implementation is more declarative and less error-prone.
",copybara-service[bot],2025-02-06 04:13:16+00:00,[],2025-02-07 21:57:56+00:00,2025-02-07 21:57:55+00:00,https://github.com/tensorflow/tensorflow/pull/86710,[],[],
2834423309,pull_request,closed,,Update nanobind to head.,"Update nanobind to head.

This includes https://github.com/wjakob/nanobind/commit/b9af9d448cbcbda2fd88f67804aafd9ae2ccb303, which fixes a crash bug in JAX under Python free threading on aarch64.

nanobind v2.5.0 is only a few commits behind this, so this is not far from a released version.
",copybara-service[bot],2025-02-06 03:12:32+00:00,[],2025-02-06 18:32:04+00:00,2025-02-06 18:32:03+00:00,https://github.com/tensorflow/tensorflow/pull/86709,[],[],
2834390959,pull_request,open,,Add vanilla support of multi subgraph in QNN compiler plugin.,"Add vanilla support of multi subgraph in QNN compiler plugin.
",copybara-service[bot],2025-02-06 02:39:51+00:00,[],2025-02-06 02:39:51+00:00,,https://github.com/tensorflow/tensorflow/pull/86708,[],[],
2834335881,pull_request,open,,[ODML] FuseStablehloConvolutionPass : migrated from MHLO -> StableHLO,"[ODML] FuseStablehloConvolutionPass : migrated from MHLO -> StableHLO
",copybara-service[bot],2025-02-06 01:53:34+00:00,[],2025-02-07 23:52:59+00:00,,https://github.com/tensorflow/tensorflow/pull/86707,[],[],
2834331336,pull_request,closed,,"lite: Keep signature input, output names order sync with SignatureDef.","lite: Keep signature input, output names order sync with SignatureDef.

In LiteRT, it assumes that input_names(), output_names() follows
the SignatureDef in the given model.
",copybara-service[bot],2025-02-06 01:50:24+00:00,['terryheo'],2025-02-06 18:48:08+00:00,2025-02-06 18:48:07+00:00,https://github.com/tensorflow/tensorflow/pull/86706,[],[],
2834285159,pull_request,closed,,Fix heuristic for determining if SparseCore is used.,"Fix heuristic for determining if SparseCore is used.
",copybara-service[bot],2025-02-06 01:26:06+00:00,[],2025-02-06 23:43:12+00:00,2025-02-06 23:43:11+00:00,https://github.com/tensorflow/tensorflow/pull/86705,[],[],
2834267605,pull_request,open,,[ROCm] Enable unsafe fp atomics and cleanup gpu_device_functions.h,,draganmladjenovic,2025-02-06 01:07:03+00:00,['gbaned'],2025-02-06 04:41:11+00:00,,https://github.com/tensorflow/tensorflow/pull/86704,"[('awaiting review', 'Pull request awaiting review'), ('size:M', 'CL Change Size: Medium'), ('comp:core', 'issues related to core part of tensorflow')]",[],
2834241194,pull_request,closed,,Delete JAX CPU Kokoro now that the JAX CPU GitHub Actions build blocks,"Delete JAX CPU Kokoro now that the JAX CPU GitHub Actions build blocks
",copybara-service[bot],2025-02-06 00:38:36+00:00,['ddunl'],2025-02-06 18:40:10+00:00,2025-02-06 18:40:09+00:00,https://github.com/tensorflow/tensorflow/pull/86703,[],[],
2834237778,pull_request,closed,,[XLA] Fix log message in hlo_pass_fix.,"[XLA] Fix log message in hlo_pass_fix.
",copybara-service[bot],2025-02-06 00:34:52+00:00,[],2025-02-07 23:00:38+00:00,2025-02-07 23:00:38+00:00,https://github.com/tensorflow/tensorflow/pull/86702,[],[],
2834226900,pull_request,closed,,Add python directory for litert,"Add python directory for litert
",copybara-service[bot],2025-02-06 00:24:03+00:00,['LukeBoyer'],2025-02-06 01:27:43+00:00,2025-02-06 01:27:42+00:00,https://github.com/tensorflow/tensorflow/pull/86701,[],[],
2834224421,pull_request,closed,,Fix incorrect unknown flags error message.,"Fix incorrect unknown flags error message.

Before, if both unknown and known flags were in XLA_FLAGS, the error would list the unknown flags and also incorrectly some known flags as being unknown. Now, only the unknown flags are shown.

The issue was tsl::Flags::Parse would set its argc and argv parameters to only have the unknown flags. The caller in parse_flags_from_env.cc would pass a pointer to the first element of a vector<char*> for the argv parameter. But then the caller would use the size of the vector<char*> when printing unknown flags, which was not mutated by tsl::Flags::Parse, instead of argc, which was mutated.
",copybara-service[bot],2025-02-06 00:21:40+00:00,['reedwm'],2025-02-06 01:51:50+00:00,2025-02-06 01:51:50+00:00,https://github.com/tensorflow/tensorflow/pull/86700,[],[],
2834215899,pull_request,closed,,Update Eigen to commit:4c38131a16803130b66266a912029504f2cf23cd,"Update Eigen to commit:4c38131a16803130b66266a912029504f2cf23cd
",copybara-service[bot],2025-02-06 00:13:35+00:00,['cantonios'],2025-02-06 23:09:53+00:00,2025-02-06 23:09:52+00:00,https://github.com/tensorflow/tensorflow/pull/86699,[],[],
2834210191,pull_request,closed,,Integrate LLVM at llvm/llvm-project@8b448842c476,"Integrate LLVM at llvm/llvm-project@8b448842c476

Updates LLVM usage to match
[8b448842c476](https://github.com/llvm/llvm-project/commit/8b448842c476)
",copybara-service[bot],2025-02-06 00:08:13+00:00,[],2025-02-06 23:22:51+00:00,2025-02-06 23:22:50+00:00,https://github.com/tensorflow/tensorflow/pull/86698,[],[],
2834191647,pull_request,closed,,Fix GatherClientLibraryTest under PjRt.,"Fix GatherClientLibraryTest under PjRt.

It was unclear to me what this test was actually trying to achieve because it is
not clearly documented. If it is trying to exercise a specific behavior with the
old non-PjRt `Client`, then that should probably live elsewhere. I've converted
it to something that can just run on top of `HloPjRtTestBase` directly. I added
some boilerplate to allow the `XlaBuilder` code to remain, though it might be
better to just use a HLO string directly.
",copybara-service[bot],2025-02-05 23:49:41+00:00,[],2025-02-06 18:14:42+00:00,2025-02-06 18:14:41+00:00,https://github.com/tensorflow/tensorflow/pull/86697,[],[],
2834178078,pull_request,closed,,Add TfFunctionDb generation in OSS.,"Add TfFunctionDb generation in OSS.
",copybara-service[bot],2025-02-05 23:38:52+00:00,[],2025-02-06 19:39:42+00:00,2025-02-06 19:39:41+00:00,https://github.com/tensorflow/tensorflow/pull/86696,[],[],
2834173831,pull_request,closed,,Expose InferenceStats method to be used internally.,"Expose InferenceStats method to be used internally.
",copybara-service[bot],2025-02-05 23:34:33+00:00,[],2025-02-07 00:53:17+00:00,2025-02-07 00:53:17+00:00,https://github.com/tensorflow/tensorflow/pull/86695,[],[],
2834172510,pull_request,open,,Update TFRT dependency to use revision,"Update TFRT dependency to use revision
http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.
",copybara-service[bot],2025-02-05 23:33:15+00:00,[],2025-02-05 23:33:15+00:00,,https://github.com/tensorflow/tensorflow/pull/86694,[],[],
2834157262,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-05 23:20:39+00:00,['ddunl'],2025-02-06 04:14:15+00:00,2025-02-06 04:14:14+00:00,https://github.com/tensorflow/tensorflow/pull/86693,[],[],
2834146406,pull_request,closed,,Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.,"Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.
",copybara-service[bot],2025-02-05 23:14:02+00:00,['sdasgup3'],2025-02-07 20:04:29+00:00,2025-02-07 20:04:29+00:00,https://github.com/tensorflow/tensorflow/pull/86692,[],[],
2834124147,pull_request,open,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`
",copybara-service[bot],2025-02-05 22:55:25+00:00,[],2025-02-05 22:55:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86691,[],[],
2834095287,pull_request,closed,,Change the compile function type to take a model rather than a list of subgraphs,"Change the compile function type to take a model rather than a list of subgraphs
",copybara-service[bot],2025-02-05 22:31:55+00:00,['LukeBoyer'],2025-02-06 02:04:56+00:00,2025-02-06 02:04:55+00:00,https://github.com/tensorflow/tensorflow/pull/86690,[],[],
2834081992,pull_request,open,,Integrate LLVM at llvm/llvm-project@f8287f6c373f,"Integrate LLVM at llvm/llvm-project@f8287f6c373f

Updates LLVM usage to match
[f8287f6c373f](https://github.com/llvm/llvm-project/commit/f8287f6c373f)
",copybara-service[bot],2025-02-05 22:22:21+00:00,[],2025-02-05 22:22:21+00:00,,https://github.com/tensorflow/tensorflow/pull/86689,[],[],
2834079505,pull_request,open,,Enables LiteRtTensorBufferRequirements lookup by tensor name.,"Enables LiteRtTensorBufferRequirements lookup by tensor name.
",copybara-service[bot],2025-02-05 22:21:09+00:00,[],2025-02-05 22:21:09+00:00,,https://github.com/tensorflow/tensorflow/pull/86688,[],[],
2834076506,pull_request,open,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`
",copybara-service[bot],2025-02-05 22:19:43+00:00,[],2025-02-05 22:19:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86687,[],[],
2834069489,pull_request,closed,,Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,"Update and/or wrap `Executable` when calling into `HloRunnerInterface`.
",copybara-service[bot],2025-02-05 22:15:21+00:00,[],2025-02-06 07:07:45+00:00,2025-02-06 07:07:44+00:00,https://github.com/tensorflow/tensorflow/pull/86686,[],[],
2834067269,pull_request,open,,AdvancedMatchShapeCoveringDynamicIndexInstruction now simulates index updates.,"AdvancedMatchShapeCoveringDynamicIndexInstruction now simulates index updates.
",copybara-service[bot],2025-02-05 22:13:45+00:00,[],2025-02-05 22:13:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86685,[],[],
2834062672,pull_request,closed,,Update TFRT dependency to use revision,"Update TFRT dependency to use revision
http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.
",copybara-service[bot],2025-02-05 22:10:17+00:00,[],2025-02-05 23:31:10+00:00,2025-02-05 23:31:09+00:00,https://github.com/tensorflow/tensorflow/pull/86684,[],[],
2834055231,pull_request,closed,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`
",copybara-service[bot],2025-02-05 22:04:59+00:00,[],2025-02-07 02:55:16+00:00,2025-02-07 02:55:16+00:00,https://github.com/tensorflow/tensorflow/pull/86683,[],[],
2834048144,pull_request,closed,,[xla:ifrt] Update ifrt::Future comment,"[xla:ifrt] Update ifrt::Future comment

(ifrt|pjrt)::Future already have correct move semantics and act as a move-only value if payload type is move-only.
",copybara-service[bot],2025-02-05 22:01:22+00:00,['ezhulenev'],2025-02-06 00:34:33+00:00,2025-02-06 00:34:33+00:00,https://github.com/tensorflow/tensorflow/pull/86682,[],[],
2834022382,pull_request,open,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`
",copybara-service[bot],2025-02-05 21:45:21+00:00,[],2025-02-05 21:45:21+00:00,,https://github.com/tensorflow/tensorflow/pull/86681,[],[],
2834013921,pull_request,closed,,Reverts changelist 723307829,"Reverts changelist 723307829
",copybara-service[bot],2025-02-05 21:39:33+00:00,['LukeBoyer'],2025-02-07 00:17:41+00:00,2025-02-07 00:17:41+00:00,https://github.com/tensorflow/tensorflow/pull/86680,[],[],
2834011871,pull_request,open,,"Include ""GenAIOpsRegisterer"" function into LiteRT pypi.","Include ""GenAIOpsRegisterer"" function into LiteRT pypi.
",copybara-service[bot],2025-02-05 21:38:09+00:00,['junjiang-lab'],2025-02-07 19:22:20+00:00,,https://github.com/tensorflow/tensorflow/pull/86679,[],[],
2833936522,pull_request,closed,,[easy] [XLA] s/CHECK/Status for a part of MSA,"[easy] [XLA] s/CHECK/Status for a part of MSA
",copybara-service[bot],2025-02-05 20:50:46+00:00,[],2025-02-07 20:40:30+00:00,2025-02-07 20:40:29+00:00,https://github.com/tensorflow/tensorflow/pull/86678,[],[],
2833897028,pull_request,open,,Update Eigen to commit:4c2611d27cb9a5fe12e18c8953097871db1d3abf,"Update Eigen to commit:4c2611d27cb9a5fe12e18c8953097871db1d3abf
",copybara-service[bot],2025-02-05 20:26:43+00:00,['cantonios'],2025-02-05 22:04:42+00:00,,https://github.com/tensorflow/tensorflow/pull/86677,[],[],
2833894557,pull_request,open,,Improve the format of the PJRT C `CHANGELOG.md`.,"Improve the format of the PJRT C `CHANGELOG.md`.
",copybara-service[bot],2025-02-05 20:25:10+00:00,[],2025-02-05 20:25:10+00:00,,https://github.com/tensorflow/tensorflow/pull/86676,[],[],
2833890849,pull_request,closed,,Gfx950 platform arch support,Added support for AMD GPU gfx950 platform,zahiqbal,2025-02-05 20:22:48+00:00,['gbaned'],2025-02-05 20:29:57+00:00,2025-02-05 20:29:54+00:00,https://github.com/tensorflow/tensorflow/pull/86675,"[('size:XL', 'CL Change Size:Extra Large')]",[],
2833882334,pull_request,closed,,Add `HloEvaluator` customization to PjRt `InterpreterClient`.,"Add `HloEvaluator` customization to PjRt `InterpreterClient`.

`InterpreterClient` hard-coded the `HloEvaluator` implementation that it uses
internally.
",copybara-service[bot],2025-02-05 20:18:04+00:00,[],2025-02-06 20:24:44+00:00,2025-02-06 20:24:44+00:00,https://github.com/tensorflow/tensorflow/pull/86674,[],[],
2833850008,pull_request,closed,,Add more logging for debug,"Add more logging for debug
",copybara-service[bot],2025-02-05 20:01:27+00:00,[],2025-02-05 22:34:36+00:00,2025-02-05 22:34:35+00:00,https://github.com/tensorflow/tensorflow/pull/86673,[],[],
2833829401,pull_request,closed,,First bits for experimental XnnGraphFusion pass.,"First bits for experimental XnnGraphFusion pass.
",copybara-service[bot],2025-02-05 19:50:11+00:00,[],2025-02-05 23:02:23+00:00,2025-02-05 23:02:22+00:00,https://github.com/tensorflow/tensorflow/pull/86672,[],[],
2833799266,pull_request,closed,,"Add RawBuffer pjrt API. Right now, only supports: creation, h2d and d2h.","Add RawBuffer pjrt API. Right now, only supports: creation, h2d and d2h.

The c-api support will be added in: https://github.com/openxla/xla/pull/22272 .
",copybara-service[bot],2025-02-05 19:33:40+00:00,['pschuh'],2025-02-06 00:00:19+00:00,2025-02-06 00:00:19+00:00,https://github.com/tensorflow/tensorflow/pull/86671,[],[],
2833748095,pull_request,closed,,Adds a library to make fixed options flags easier and safer to define.,"Adds a library to make fixed options flags easier and safer to define.

xla defines custom commandline flag parsing logic in several places. These
definitions follow a common pattern: there is a fixed set of options for the
flag type (e.g. the flag type is an enum).

Such definitions are repetitious and error-prone (one has to make sure that the name => value and value => name mappings are consistent and have no duplicate names or values). This library abstracts away the tedious details and will be used in subsequent changes to make the definitions simpler and safer.
",copybara-service[bot],2025-02-05 19:06:20+00:00,[],2025-02-07 19:54:17+00:00,2025-02-07 19:54:16+00:00,https://github.com/tensorflow/tensorflow/pull/86669,[],[],
2833745556,pull_request,closed,,Remove TSL's `requirements_lock_3_11.txt`,"Remove TSL's `requirements_lock_3_11.txt`

This file has no meaning now that TSL is no longer independently buildable
",copybara-service[bot],2025-02-05 19:04:58+00:00,['ddunl'],2025-02-05 19:57:36+00:00,2025-02-05 19:57:35+00:00,https://github.com/tensorflow/tensorflow/pull/86668,[],[],
2833724987,pull_request,closed,,"Rollback TF PR #82210 to fix LiteRT pypi error ""'Interpreter' object has no attribute '_interpreter'"".","Rollback TF PR #82210 to fix LiteRT pypi error ""'Interpreter' object has no attribute '_interpreter'"".

Reverts 0f26341d36164bab69cf162e3eed0ec1292987e7
",copybara-service[bot],2025-02-05 18:55:24+00:00,['junjiang-lab'],2025-02-05 19:24:59+00:00,2025-02-05 19:24:59+00:00,https://github.com/tensorflow/tensorflow/pull/86667,[],[],
2833690474,pull_request,open,,Exit checkpoints_iterator promptly when no more checkpoints are expected.,"Exit checkpoints_iterator promptly when no more checkpoints are expected.
",copybara-service[bot],2025-02-05 18:39:25+00:00,[],2025-02-05 18:39:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86666,[],[],
2833643323,pull_request,open,,[pjrt] Removed unused overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice`,"[pjrt] Removed unused overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice`
",copybara-service[bot],2025-02-05 18:16:04+00:00,['superbobry'],2025-02-05 18:45:21+00:00,,https://github.com/tensorflow/tensorflow/pull/86665,[],[],
2833634696,pull_request,closed,,"Remove top level `WORKSPACE`, `.bazelrc`, `.bazelversion` and `tools/` from TSL","Remove top level `WORKSPACE`, `.bazelrc`, `.bazelversion` and `tools/` from TSL

TSL is no longer independently buildable, so these files have no meaning
",copybara-service[bot],2025-02-05 18:11:48+00:00,['ddunl'],2025-02-05 19:00:09+00:00,2025-02-05 19:00:08+00:00,https://github.com/tensorflow/tensorflow/pull/86664,[],[],
2833628228,pull_request,open,,Update Eigen to commit:c079ee5e44f770c67f135e340dcbd5b59012e4a6,"Update Eigen to commit:c079ee5e44f770c67f135e340dcbd5b59012e4a6
",copybara-service[bot],2025-02-05 18:08:28+00:00,['cantonios'],2025-02-05 18:08:30+00:00,,https://github.com/tensorflow/tensorflow/pull/86663,[],[],
2833622357,pull_request,closed,,Skip TSAN tests as they are also not supported.,"Skip TSAN tests as they are also not supported.
",copybara-service[bot],2025-02-05 18:05:39+00:00,[],2025-02-05 18:40:55+00:00,2025-02-05 18:40:53+00:00,https://github.com/tensorflow/tensorflow/pull/86662,[],[],
2833612132,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-05 18:00:41+00:00,['ddunl'],2025-02-05 18:28:53+00:00,2025-02-05 18:28:52+00:00,https://github.com/tensorflow/tensorflow/pull/86661,[],[],
2833611820,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-05 18:00:29+00:00,['ddunl'],2025-02-07 02:05:17+00:00,2025-02-07 02:05:17+00:00,https://github.com/tensorflow/tensorflow/pull/86660,[],[],
2833579865,pull_request,closed,,"Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices (roll-forward).","Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices (roll-forward).

Reverts 814b7dcd21487da4be271cbcf58a5d463f5e1a1a
",copybara-service[bot],2025-02-05 17:45:00+00:00,[],2025-02-06 15:47:59+00:00,2025-02-06 15:47:58+00:00,https://github.com/tensorflow/tensorflow/pull/86659,[],[],
2833569516,pull_request,closed,,[XLA:CPU] Make loop unrolling on by default in IrCompiler.,"[XLA:CPU] Make loop unrolling on by default in IrCompiler.

Reverts 6ae9c29c1fcf8905812599fec4c692b5bf47fd0e
",copybara-service[bot],2025-02-05 17:39:34+00:00,[],2025-02-06 00:44:04+00:00,2025-02-06 00:44:03+00:00,https://github.com/tensorflow/tensorflow/pull/86658,[],[],
2833559841,pull_request,closed,,[pjrt] Removed the deprecated overload of `BufferFromHostLiteral`,"[pjrt] Removed the deprecated overload of `BufferFromHostLiteral`
",copybara-service[bot],2025-02-05 17:34:36+00:00,['superbobry'],2025-02-05 20:07:00+00:00,2025-02-05 20:07:00+00:00,https://github.com/tensorflow/tensorflow/pull/86657,[],[],
2833524861,pull_request,open,,[XLA:GPU] Remove the restrictions that prevent us from fusing the subchannel dequantisation sequence from Triton tiling propagation. ,"[XLA:GPU] Remove the restrictions that prevent us from fusing the subchannel dequantisation sequence from Triton tiling propagation. 

There are two cases for the broadcast:
1) Channel quantisation case: 
     a) we have 2d weights + 1d scales + 2d activations.
     b) In triton prolog we prepare the corresponding block pointers
     c) inside the for loop along the k-dim every time we load the same 1d tile for scalers, expand it to 2d [block_m,1], and broadcast to block_k elements along newly added dim to [block_m, block_k].
     d) then do the multiply and dot

2) Subchannel quantisation case:
     a) we have 2d weights [M,K] + 2d scales [M,K/q] + 2d activations where q is the subchannel size.
     b) In triton prolog we prepare the corresponding block pointers
     c) inside the for loop along the k-dim every time we load the 2d [M,1] tile for scalers and broadcast to block_k elements along the k dim to [block_m, block_k].
     d) then do the rest
    
I.e. the difference is that the scalers matrix is the 2d matrix from the very beginning but it is smaller along the k dim and we need to advance it along k dim only by one column instead of block_k columns. It is already 2d, so, we don't need to add the dimension.

We could emit the right code if we know that there was the subchannel broadcast and and we know the size of the subchannel. We do this analysis in the triton_tiling_propagation by detecting the broadcast with the follow up bitcast combination like [B,c,M] -> [B,c,q,M] -> [B,c*q,M]. I.e. we do the broadcast but the follow up bitcast merges the broadcasted dim with another nonempty dim.

This schema works for the cases when block_k == subchannel_size and for the case when we have split_k == 1.

These two restrictions could be addressed in the follow up cls.
",copybara-service[bot],2025-02-05 17:17:08+00:00,[],2025-02-07 17:07:15+00:00,,https://github.com/tensorflow/tensorflow/pull/86656,[],[],
2833464845,pull_request,closed,,[XLA] Fix scheduling annotations to avoid creating invalid overlap of instructions,"[XLA] Fix scheduling annotations to avoid creating invalid overlap of instructions
",copybara-service[bot],2025-02-05 16:49:44+00:00,[],2025-02-05 18:52:19+00:00,2025-02-05 18:52:18+00:00,https://github.com/tensorflow/tensorflow/pull/86655,[],[],
2833417022,pull_request,closed,,[XLA:GPU] Maximally combine sync collectives.,"[XLA:GPU] Maximally combine sync collectives.

We dry-run the scheduler to collect the set of collectives that don't overlap with compute. We then combine these maximally (i.e. up to the device memory limit).
",copybara-service[bot],2025-02-05 16:28:56+00:00,[],2025-02-06 16:03:43+00:00,2025-02-06 16:03:42+00:00,https://github.com/tensorflow/tensorflow/pull/86654,[],[],
2833410507,pull_request,closed,,Use nested namespace,,jpienaar,2025-02-05 16:26:05+00:00,['gbaned'],2025-02-05 17:28:30+00:00,2025-02-05 17:23:56+00:00,https://github.com/tensorflow/tensorflow/pull/86653,"[('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2637566616, 'issue_id': 2833410507, 'author': 'mihaimaruseac', 'body': ""Shouldn't merge this directly, this is breaking Copybara"", 'created_at': datetime.datetime(2025, 2, 5, 17, 24, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637571246, 'issue_id': 2833410507, 'author': 'MichaelHudgins', 'body': 'Hi @mihaimaruseac , we are testing to ensure copybara will actually overwrite as part of some partner onboarding', 'created_at': datetime.datetime(2025, 2, 5, 17, 26, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637574489, 'issue_id': 2833410507, 'author': 'mihaimaruseac', 'body': 'Ohh, sorry, did not know. It would be awesome if that works', 'created_at': datetime.datetime(2025, 2, 5, 17, 28, 28, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-02-05 17:24:47 UTC): Shouldn't merge this directly, this is breaking Copybara

MichaelHudgins on (2025-02-05 17:26:55 UTC): Hi @mihaimaruseac , we are testing to ensure copybara will actually overwrite as part of some partner onboarding

mihaimaruseac on (2025-02-05 17:28:28 UTC): Ohh, sorry, did not know. It would be awesome if that works

"
2833381950,pull_request,closed,,[XLA:GPU] Mark copy op as supported by triton,"[XLA:GPU] Mark copy op as supported by triton

* codegen succeeds for copy op with all triton supported datatypes
",copybara-service[bot],2025-02-05 16:13:57+00:00,[],2025-02-06 10:22:17+00:00,2025-02-06 10:22:16+00:00,https://github.com/tensorflow/tensorflow/pull/86652,[],[],
2833316813,pull_request,closed,,[XLA] Rollback 'Support nested fusions in HloFusionAdaptor',"[XLA] Rollback 'Support nested fusions in HloFusionAdaptor'

We don't want the HloFusionAdaptor to handle nested fusions transparently. Instead, *inner* fusion instructions should be treated like any other instruction. Handling this is not part of this change.

Reverts 8c446ecc5741f5daacb43e20410905cf51c7e05b
",copybara-service[bot],2025-02-05 15:47:13+00:00,['chsigg'],2025-02-06 09:47:37+00:00,2025-02-06 09:47:36+00:00,https://github.com/tensorflow/tensorflow/pull/86651,[],[],
2833270886,pull_request,closed,,"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22041 from sfvaroglu:sevin/channel_id_flag 335de387e78559e37a9a018aa1b2c367a208a97e
",copybara-service[bot],2025-02-05 15:28:32+00:00,[],2025-02-05 22:26:50+00:00,2025-02-05 22:26:49+00:00,https://github.com/tensorflow/tensorflow/pull/86650,[],[],
2833238174,pull_request,open,,[XLA] Preserve AUTO layout when converting from HLO to StableHLO,"[XLA] Preserve AUTO layout when converting from HLO to StableHLO

In HLO, AUTO layout is encoded as missing layout in `entry_computation_layout`.

In StableHLO, it's marked using `mhlo.layout_mode = ""auto""` attribute of the main@ function argument or return value.

The behavior is off by default everywhere except the ""hlo-translate"" tool.

In the next change, I'll switch it on by default, except for places where it causes problems.
",copybara-service[bot],2025-02-05 15:15:35+00:00,[],2025-02-06 13:34:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86649,[],[],
2833211397,pull_request,closed,,[XLA:CPU] Don't unroll dimensions except for the trailing one.,"[XLA:CPU] Don't unroll dimensions except for the trailing one.
",copybara-service[bot],2025-02-05 15:06:42+00:00,[],2025-02-05 16:46:06+00:00,2025-02-05 16:46:05+00:00,https://github.com/tensorflow/tensorflow/pull/86648,[],[],
2833101466,pull_request,closed,,PR #22041: Enable xla_ignore_channel_id flag by default,"PR #22041: Enable xla_ignore_channel_id flag by default

Imported from GitHub PR https://github.com/openxla/xla/pull/22041

Rename `xla_experimental_ignore_channel_id` flag as `xla_ignore_channel_id flag` and enable it by default.

cc @frgossen 
Copybara import of the project:

--
94baa4a5f32ff9421709471fc880fe59a0e164c6 by Sevin Varoglu <svaroglu@nvidia.com>:

Enable xla_ignore_channel_id flag by default

--
d63c6c022aa7b4c81c1bd38a07419da08fc6e3b9 by Sevin Varoglu <svaroglu@nvidia.com>:

Fix error

--
335de387e78559e37a9a018aa1b2c367a208a97e by Sevin Varoglu <svaroglu@nvidia.com>:

Fix format


Merging this change closes #22041

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22041 from sfvaroglu:sevin/channel_id_flag 335de387e78559e37a9a018aa1b2c367a208a97e
",copybara-service[bot],2025-02-05 14:26:47+00:00,[],2025-02-05 15:21:41+00:00,2025-02-05 15:21:40+00:00,https://github.com/tensorflow/tensorflow/pull/86647,[],[],
2833039335,pull_request,open,,[XLA:GPU] Add AllGather support to collective generation tool.,"[XLA:GPU] Add AllGather support to collective generation tool.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e
",copybara-service[bot],2025-02-05 14:02:59+00:00,[],2025-02-05 14:02:59+00:00,,https://github.com/tensorflow/tensorflow/pull/86646,[],[],
2833036240,pull_request,closed,,Rollback [XLA:CPU] Make loop unrolling on by default in IrCompiler.,"Rollback [XLA:CPU] Make loop unrolling on by default in IrCompiler.

Reverts 1e7976e5c5869d34eac64352d411dfb2c8ca4966

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e
",copybara-service[bot],2025-02-05 14:01:43+00:00,[],2025-02-05 15:09:37+00:00,2025-02-05 15:09:35+00:00,https://github.com/tensorflow/tensorflow/pull/86645,[],[],
2833034338,pull_request,open,,[XLA:GPU] Add ReduceScatter support to collective generation tool.,"[XLA:GPU] Add ReduceScatter support to collective generation tool.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e
",copybara-service[bot],2025-02-05 14:00:59+00:00,[],2025-02-05 14:00:59+00:00,,https://github.com/tensorflow/tensorflow/pull/86644,[],[],
2832975595,pull_request,open,,[XLA] Tag timeout tests as `not_run:arm`,"[XLA] Tag timeout tests as `not_run:arm`

Similarly to cl/722883015 tagging also:

* //third_party/tensorflow/compiler/xla/python/transfer:socket_bulk_transport_test
* //third_party/tensorflow/compiler/xla/python/transfer:socket-server_test
* //third_party/tensorflow/compiler/xla/python/transfer:event_loop_test

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 13:37:11+00:00,[],2025-02-05 13:37:11+00:00,,https://github.com/tensorflow/tensorflow/pull/86643,[],[],
2832958586,pull_request,closed,,Removing some patches by:,"Removing some patches by:
- Upstreaming internal testto remove file entirely. Also removing patch with redundant (already upstream) tests.
- Verifying an issue is already fixed.
- Attempting to upstream 2 changes. Added comments to remove 2 more patches in a follow-up if they land successfully.
",copybara-service[bot],2025-02-05 13:29:55+00:00,[],2025-02-06 11:24:09+00:00,2025-02-06 11:24:08+00:00,https://github.com/tensorflow/tensorflow/pull/86642,[],[],
2832856983,pull_request,open,,PR #22334: [ROCm] Fix flaky gpu compiler test when building with rocm,"PR #22334: [ROCm] Fix flaky gpu compiler test when building with rocm

Imported from GitHub PR https://github.com/openxla/xla/pull/22334

This change fixes the flaky gpu compiler test used to run on rocm CI pipeline gate.
Triton pipeline was wrongly using the TritonGPUAccelerateMatmul pass which supports cuda only.
In rocm there is a different pass which is now used in the rocm pipeline.

https://github.com/triton-lang/triton/blob/main/third_party/amd/lib/TritonAMDGPUTransforms/AccelerateAMDMatmul.cpp
Copybara import of the project:

--
c5f600f03aa87d155bb624bedb0584e635af190e by Alexandros Theodoridis <atheodor@amd.com>:

Fix flaky gpu compiler test when building with rocm


Merging this change closes #22334

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e
",copybara-service[bot],2025-02-05 12:45:56+00:00,[],2025-02-05 12:45:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86641,[],[],
2832790214,pull_request,open,,Use custom HLO deserialization for HloUnoptimizedSnapshot.,"Use custom HLO deserialization for HloUnoptimizedSnapshot.

This change makes it possible to read HloUnoptimizedSnapshot protos that are over 2GiB in size and were dumped using custom proto serialization.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 12:20:48+00:00,[],2025-02-05 12:57:47+00:00,,https://github.com/tensorflow/tensorflow/pull/86640,[],[],
2832782729,pull_request,open,,[xla:gpu:triton] Create tma_utils with functions & tests that help with emitting TMA through triton. (see child cl to see how most of these get used).,"[xla:gpu:triton] Create tma_utils with functions & tests that help with emitting TMA through triton. (see child cl to see how most of these get used).

This also helps to isolate TMA that can be used in other places.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 12:17:27+00:00,[],2025-02-05 13:35:41+00:00,,https://github.com/tensorflow/tensorflow/pull/86639,[],[],
2832613085,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 11:03:46+00:00,[],2025-02-05 11:32:37+00:00,,https://github.com/tensorflow/tensorflow/pull/86638,[],[],
2832508813,pull_request,closed,,PR #22091: [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot,"PR #22091: [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot

Imported from GitHub PR https://github.com/openxla/xla/pull/22091

TritonTest.NonstandardLayoutWithManyNonContractingDims
TritonTest.NonstandardLayoutWithManyNonContractingDimsReversedLayout

If it's not profitable to fuse dot and cuBLAS is not requiring extra padding then targeting custom call would be optimal. Align this check for both ROCm and CUDA here.
 
@xla-rotation would you please have a look?
Copybara import of the project:

--
a161c86d14b5c3e5a61d47474142646f141814ae by Jian Li <Jian.Li@amd.com>:

[ROCM] Try targeting cuBLAS if it's not profitable to fuse dot

Merging this change closes #22091

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae
",copybara-service[bot],2025-02-05 10:18:18+00:00,[],2025-02-06 11:11:50+00:00,2025-02-06 11:11:49+00:00,https://github.com/tensorflow/tensorflow/pull/86637,[],[],
2832508459,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 10:18:09+00:00,[],2025-02-05 10:18:09+00:00,,https://github.com/tensorflow/tensorflow/pull/86636,[],[],
2832507896,pull_request,open,,PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell

Imported from GitHub PR https://github.com/openxla/xla/pull/22256

Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph) - this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10).

The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32).
MXFP8 docs: https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf

The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness.

The pass is also enabled in the same PR.
Copybara import of the project:

--
145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub <skozub@nvidia.com>:

[XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell

Merging this change closes #22256

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930
",copybara-service[bot],2025-02-05 10:18:01+00:00,[],2025-02-07 09:38:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86635,[],[],
2832502038,pull_request,open,,test XLA build.,"test XLA build.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 10:16:01+00:00,[],2025-02-05 10:16:01+00:00,,https://github.com/tensorflow/tensorflow/pull/86634,[],[],
2832489700,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-02-05 10:10:42+00:00,['ddunl'],2025-02-07 01:17:21+00:00,,https://github.com/tensorflow/tensorflow/pull/86633,[],[],
2832451480,pull_request,open,,Remove dead code (NFC),"Remove dead code (NFC)

We compute the total number of tiles in a variable `num_tiles` but then never
use it. So remove it.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:54:39+00:00,['akuegel'],2025-02-05 09:54:40+00:00,,https://github.com/tensorflow/tensorflow/pull/86632,[],[],
2832420492,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:41:45+00:00,[],2025-02-05 09:41:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86631,[],[],
2832400676,pull_request,open,,Qualcomm AI Engine Direct - Support DUS and Pack Op in LiteRT,"# WHAT
Support DUS op and Pack op in LiteRT

- DUS are supported partial for specific cases.
- Pack is supported using QNN Concat to address type support issue.

This PR depends on https://github.com/tensorflow/tensorflow/pull/85477, so there are some additional changes.
Primary changes are in the latest 4 commits.

- 6bd5f390807dc32734e12a1725e67b9b132dfe5f
- 7f2f2b9cc04d814d9ee7b1ca42109c2f40a76789
- 03109f82c3903688e8d65e75cb69d0c24e08f34f
- 10d049a856133005cabb72aec4e9fd4d3b66aa02
# TESTS
```
bazel build -c opt --cxxopt=--std=c++20 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test
./bazel-bin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test
```
Disabled these models since they are missing in the code.

- kFeedForwardModel,
- kKeyEinsumModel,
- kQueryEinsumModel,
- kValueEinsumModel,
- kAttnVecEinsumModel,
- kROPEModel,
- kLookUpROPEModel,
- kRMSNormModel,
- kSDPAModel,
- kAttentionModel,
- kTransformerBlockModel,
- kQSimpleMul16x16Model,
- kQMulAdd16x16Model,
- kQQueryEinsum16x8Model,
- kQKeyEinsum16x8Model,
- kQVauleEinsum16x8Model,
- kQAttnVecEinsum16x8Model

```
[----------] 28 tests from SupportedOpsTest/QnnPluginOpCompatibilityTest (400 ms total)

[----------] Global test environment tear-down
[==========] 56 tests from 2 test suites ran. (692 ms total)
[  PASSED  ] 56 tests.
```",chunhsue,2025-02-05 09:32:58+00:00,['gbaned'],2025-02-06 06:10:10+00:00,,https://github.com/tensorflow/tensorflow/pull/86630,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:XL', 'CL Change Size:Extra Large')]","[{'comment_id': 2636201170, 'issue_id': 2832400676, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86630/checks?check_run_id=36707874606) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 2, 5, 9, 33, 3, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-02-05 09:33:03 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86630/checks?check_run_id=36707874606) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2832373944,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:21:40+00:00,[],2025-02-05 09:21:40+00:00,,https://github.com/tensorflow/tensorflow/pull/86629,[],[],
2832373879,pull_request,closed,,partition sample,,chuntl,2025-02-05 09:21:38+00:00,['gbaned'],2025-02-05 09:22:47+00:00,2025-02-05 09:22:47+00:00,https://github.com/tensorflow/tensorflow/pull/86628,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2636174281, 'issue_id': 2832373879, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86628/checks?check_run_id=36707228414) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 2, 5, 9, 21, 43, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-02-05 09:21:43 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86628/checks?check_run_id=36707228414) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2832372702,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:21:06+00:00,[],2025-02-05 09:21:06+00:00,,https://github.com/tensorflow/tensorflow/pull/86627,[],[],
2832366079,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:18:02+00:00,[],2025-02-05 10:18:46+00:00,,https://github.com/tensorflow/tensorflow/pull/86626,[],[],
2832364102,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 09:17:08+00:00,[],2025-02-06 07:34:30+00:00,,https://github.com/tensorflow/tensorflow/pull/86625,[],[],
2832363176,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 09:16:45+00:00,[],2025-02-06 08:29:56+00:00,2025-02-06 08:29:56+00:00,https://github.com/tensorflow/tensorflow/pull/86624,[],[],
2832362405,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 09:16:23+00:00,[],2025-02-05 11:23:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86623,[],[],
2832362260,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:16:20+00:00,[],2025-02-05 09:16:20+00:00,,https://github.com/tensorflow/tensorflow/pull/86622,[],[],
2832361411,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 09:15:59+00:00,[],2025-02-05 12:50:56+00:00,,https://github.com/tensorflow/tensorflow/pull/86621,[],[],
2832360644,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 09:15:39+00:00,[],2025-02-05 11:48:45+00:00,,https://github.com/tensorflow/tensorflow/pull/86620,[],[],
2832360534,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:15:36+00:00,[],2025-02-05 09:15:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86619,[],[],
2832360084,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:15:24+00:00,[],2025-02-05 09:15:24+00:00,,https://github.com/tensorflow/tensorflow/pull/86618,[],[],
2832358832,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 09:14:52+00:00,[],2025-02-06 07:28:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86617,[],[],
2832358211,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 09:14:34+00:00,[],2025-02-05 12:40:54+00:00,,https://github.com/tensorflow/tensorflow/pull/86616,[],[],
2832357638,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 09:14:20+00:00,[],2025-02-05 11:52:12+00:00,,https://github.com/tensorflow/tensorflow/pull/86615,[],[],
2832357596,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 09:14:18+00:00,[],2025-02-05 09:14:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86614,[],[],
2832353986,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 09:12:41+00:00,[],2025-02-08 07:49:04+00:00,2025-02-08 07:49:03+00:00,https://github.com/tensorflow/tensorflow/pull/86613,[],[],
2832352430,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 09:12:01+00:00,[],2025-02-06 08:17:28+00:00,2025-02-06 08:17:27+00:00,https://github.com/tensorflow/tensorflow/pull/86612,[],[],
2832214807,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-02-05 08:10:42+00:00,[],2025-02-06 06:54:40+00:00,2025-02-06 06:54:39+00:00,https://github.com/tensorflow/tensorflow/pull/86611,[],[],
2832140430,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 07:29:17+00:00,[],2025-02-05 07:29:17+00:00,,https://github.com/tensorflow/tensorflow/pull/86610,[],[],
2832094757,pull_request,closed,,internal change for visibilily,"internal change for visibilily
",copybara-service[bot],2025-02-05 07:06:29+00:00,[],2025-02-07 01:43:27+00:00,2025-02-07 01:43:26+00:00,https://github.com/tensorflow/tensorflow/pull/86609,[],[],
2832050175,pull_request,open,,Use matchers_oss in vendor code,"Use matchers_oss in vendor code

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 06:40:33+00:00,['LukeBoyer'],2025-02-05 06:40:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86608,[],[],
2831991780,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 06:02:15+00:00,['ddunl'],2025-02-05 06:02:16+00:00,,https://github.com/tensorflow/tensorflow/pull/86606,[],[],
2831946890,pull_request,open,,"Fix inference request analysis aggregated on batch size, by aggregating only the requests included in a single batch, as large request split into multiple batches will introduce confusing results (eg. the device time will be the sum of the 2 batch processing).","Fix inference request analysis aggregated on batch size, by aggregating only the requests included in a single batch, as large request split into multiple batches will introduce confusing results (eg. the device time will be the sum of the 2 batch processing).
",copybara-service[bot],2025-02-05 05:31:34+00:00,['zzzaries'],2025-02-05 05:31:36+00:00,,https://github.com/tensorflow/tensorflow/pull/86605,[],[],
2831922725,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 05:16:12+00:00,[],2025-02-05 10:20:50+00:00,,https://github.com/tensorflow/tensorflow/pull/86604,[],[],
2831894843,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 04:58:16+00:00,[],2025-02-05 04:58:16+00:00,,https://github.com/tensorflow/tensorflow/pull/86603,[],[],
2831868084,pull_request,open,,Update TFRT dependency to use revision,"Update TFRT dependency to use revision
http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.
",copybara-service[bot],2025-02-05 04:39:10+00:00,[],2025-02-05 23:29:54+00:00,,https://github.com/tensorflow/tensorflow/pull/86602,[],[],
2831829573,pull_request,open,,Fix race when accessing `runners_`.,"Fix race when accessing `runners_`.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 04:09:47+00:00,[],2025-02-05 07:01:01+00:00,,https://github.com/tensorflow/tensorflow/pull/86601,[],[],
2831792106,pull_request,open,,[XLA] Set OS-scope thread names for debuggability,"[XLA] Set OS-scope thread names for debuggability

This makes it easier for outside tooling to understand a crash or do other post-mortem debugging.
",copybara-service[bot],2025-02-05 03:44:24+00:00,['majnemer'],2025-02-05 03:44:25+00:00,,https://github.com/tensorflow/tensorflow/pull/86600,[],[],
2831783456,pull_request,open,,"In literal_util.cc, use absl::uniform_int_distribution.","In literal_util.cc, use absl::uniform_int_distribution.

absl::uniform_int_distribution is faster than std::uniform_int_distribution. This makes initializing literals in run_hlo_module faster. In particular, I tested the following HLO:

    ENTRY f {
      arg = s8[2000000000] parameter(0)
      ROOT add_result = s8[2000000000] add(arg, arg)
    }

It takes 7.8 seconds to initialize the input literal with the absl function, and 18.2 with the std function.

Unfortunately the absl version of uniform_real_distribution is not faster. It takes 25.5 seconds with absl and 8.3 with std on the HLO when s8 is replaced with f16.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 03:37:18+00:00,['reedwm'],2025-02-05 03:37:19+00:00,,https://github.com/tensorflow/tensorflow/pull/86599,[],[],
2831781712,pull_request,open,,Vectorize group_sizes by including more lhs dimensions.,"Vectorize group_sizes by including more lhs dimensions.

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805
",copybara-service[bot],2025-02-05 03:35:58+00:00,[],2025-02-05 04:13:43+00:00,,https://github.com/tensorflow/tensorflow/pull/86598,[],[],
2831737888,pull_request,open,,PR #22258: [GPU][NFC] Avoid always printing complete PGLE profiles.,"PR #22258: [GPU][NFC] Avoid always printing complete PGLE profiles.

Imported from GitHub PR https://github.com/openxla/xla/pull/22258


Copybara import of the project:

--
025352635a155e447559d83c471369559aad5981 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Avoid always printing complete PGLE profiles.


Merging this change closes #22258

Reverts changelist 723246423

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981
",copybara-service[bot],2025-02-05 02:59:06+00:00,[],2025-02-05 10:02:46+00:00,,https://github.com/tensorflow/tensorflow/pull/86597,[],[],
2831710374,pull_request,closed,,Add new allocation type kTfLiteNonCpu,"Add new allocation type kTfLiteNonCpu

This allocation type indicates the Tensor is associated with
HW buffer using litert::TensorBuffer.

Also added an experimental API of TfLiteOpaqueTensorSetNonCpuAllocation()
",copybara-service[bot],2025-02-05 02:35:02+00:00,['terryheo'],2025-02-07 00:26:52+00:00,2025-02-07 00:26:52+00:00,https://github.com/tensorflow/tensorflow/pull/86596,[],[],
2831643449,pull_request,open,,add an option to remove call-start/done to sparse core computations.,"add an option to remove call-start/done to sparse core computations.
used for hybridsim until it support sparsecore.
",copybara-service[bot],2025-02-05 01:32:53+00:00,['trisolaran'],2025-02-05 17:28:34+00:00,,https://github.com/tensorflow/tensorflow/pull/86595,[],[],
2831559563,pull_request,closed,,Introduce a `function_runs_at_most_once` parameter to LocalExecutorParams for cases where we know that no one will be using the executor again.,"Introduce a `function_runs_at_most_once` parameter to LocalExecutorParams for cases where we know that no one will be using the executor again.

This allows us to clear the executor state when we no longer need it, and thus reduce peak memory (and HBM) usage.
",copybara-service[bot],2025-02-05 00:11:10+00:00,['sagunb'],2025-02-06 19:47:53+00:00,2025-02-06 19:47:52+00:00,https://github.com/tensorflow/tensorflow/pull/86594,[],[],
2831501624,pull_request,open,,Add the list of Qualcomm SoCs supporting NPU.,"Add the list of Qualcomm SoCs supporting NPU.
",copybara-service[bot],2025-02-04 23:33:29+00:00,[],2025-02-04 23:57:18+00:00,,https://github.com/tensorflow/tensorflow/pull/86593,[],[],
2831495008,pull_request,closed,,Update on the README and the CONTRIBUTING files,"Hi tensorflow Team,

I hope this message finds you well. My name is Wadie Botros, and I am currently working on an assignment for a course at Potsdam University, Germany. As part of the assignment, I reviewed your repository to improve its readability and maintainability.

Changes Made:

- Update the README file with 'Common Issues and Troubleshooting' Section.
- Update the CONTRIBUTING file with 'QUICK START GUIDE' Section.

I believe these changes will make the code easier to maintain and understand for future contributors. If you have any questions or need further modifications, please feel free to discuss them with me. I am happy to make any adjustments as needed.

Thank you for considering my contribution. I look forward to your feedback.

Best regards,

Wadie Botros
",WadieBishoy25,2025-02-04 23:27:08+00:00,['gbaned'],2025-02-05 15:42:27+00:00,2025-02-05 15:42:27+00:00,https://github.com/tensorflow/tensorflow/pull/86592,"[('size:S', 'CL Change Size: Small'), ('invalid', 'Hacktoberfest spam PR')]","[{'comment_id': 2635289623, 'issue_id': 2831495008, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86592/checks?check_run_id=36686644774) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 2, 4, 23, 27, 12, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-02-04 23:27:12 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/86592/checks?check_run_id=36686644774) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
