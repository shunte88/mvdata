id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2839670714,issue,open,,Python 3.12 and further,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

So when are you guys going to make Tensorflow Compatible for Python version 3.13?

3.12 has been in the market for more than 1 year still the latest update doesn't work on 3.12, why is it so? Is Python 3.12 not for ML?

### Standalone code to reproduce the issue

```shell
The PIP won't install Tensorflow on 3.13, for now I had gone back to 3.11
```

### Relevant log output

```shell

```",Kush3007,2025-02-08 07:29:37+00:00,['Venkat6871'],2025-02-08 07:32:56+00:00,,https://github.com/tensorflow/tensorflow/issues/86879,"[('type:build/install', 'Build and install issues')]",[],
2835627707,issue,open,,unexpected import during stub creation from mypy-protobuf,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm doing a stub distribution for tensorflow, and when I used mypy-protobuf to create stubs for .proto files, it resulted in an import that I didn't expect


The format it should create is:
```python
import tensorflow.tsl.protobuf.histogram_pb2
```
but it does
```python
import xla.tsl.protobuf.histogram_pb2
```

I don't really understand how they structure the configurations to compile from Bazel

Is there a setting I'm missing, or should I make the change manually?

Is there a way for tensorflow to automatically create the stubs files from the compiled proto files?

Here I leave you the code to speed up your analysis.


third_party\xla\xla\tsl\protobuf\histogram.proto
```proto
syntax = ""proto3"";

package tensorflow;

option cc_enable_arenas = true;
option java_multiple_files = true;
option java_package = ""org.tensorflow.framework"";
option go_package = ""github.com/google/tsl/tsl/go/core/protobuf/summary_go_proto"";

// Serialization format for histogram module in
// tsl/lib/histogram/histogram.h
message HistogramProto {
  double min = 1;
  double max = 2;
  double num = 3;
  double sum = 4;
  double sum_squares = 5;

  // Parallel arrays encoding the bucket boundaries and the bucket values.
  // bucket(i) is the count for the bucket i.  The range for
  // a bucket is:
  //   i == 0:  -DBL_MAX .. bucket_limit(0)
  //   i != 0:  bucket_limit(i-1) .. bucket_limit(i)
  repeated double bucket_limit = 6 [packed = true];
  repeated double bucket = 7 [packed = true];
}
```

tensorflow\core\framework\summary.proto
```proto
syntax = ""proto3"";

package tensorflow;

import public ""xla/tsl/protobuf/histogram.proto"";

import ""tensorflow/core/framework/tensor.proto"";

option cc_enable_arenas = true;
option java_outer_classname = ""SummaryProtos"";
option java_multiple_files = true;
option java_package = ""org.tensorflow.framework"";
option go_package = ""github.com/tensorflow/tensorflow/tensorflow/go/core/framework/summary_go_proto"";

// Metadata associated with a series of Summary data
message SummaryDescription {
  // Hint on how plugins should process the data in this series.
  // Supported values include ""scalar"", ""histogram"", ""image"", ""audio""
  string type_hint = 1;
}

// A SummaryMetadata encapsulates information on which plugins are able to make
// use of a certain summary value.
message SummaryMetadata {
  message PluginData {
    // The name of the plugin this data pertains to.
    string plugin_name = 1;

    // The content to store for the plugin. The best practice is for this to be
    // a binary serialized protocol buffer.
    bytes content = 2;
  }

  // Data that associates a summary with a certain plugin.
  PluginData plugin_data = 1;

  // Display name for viewing in TensorBoard.
  string display_name = 2;

  // Longform readable description of the summary sequence. Markdown supported.
  string summary_description = 3;

  // Class of data stored in this time series. Required for compatibility with
  // TensorBoard's generic data facilities (`DataProvider`, et al.). This value
  // imposes constraints on the dtype and shape of the corresponding tensor
  // values. See `DataClass` docs for details.
  DataClass data_class = 4;
}

enum DataClass {
  // Unknown data class, used (implicitly) for legacy data. Will not be
  // processed by data ingestion pipelines.
  DATA_CLASS_UNKNOWN = 0;
  // Scalar time series. Each `Value` for the corresponding tag must have
  // `tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).
  DATA_CLASS_SCALAR = 1;
  // Tensor time series. Each `Value` for the corresponding tag must have
  // `tensor` set. The tensor value is arbitrary, but should be small to
  // accommodate direct storage in database backends: an upper bound of a few
  // kilobytes is a reasonable rule of thumb.
  DATA_CLASS_TENSOR = 2;
  // Blob sequence time series. Each `Value` for the corresponding tag must
  // have `tensor` set to a rank-1 tensor of bytestring dtype.
  DATA_CLASS_BLOB_SEQUENCE = 3;
}

// A Summary is a set of named values to be displayed by the
// visualizer.
//
// Summaries are produced regularly during training, as controlled by
// the ""summary_interval_secs"" attribute of the training operation.
// Summaries are also produced at the end of an evaluation.
message Summary {
  message Image {
    // Dimensions of the image.
    int32 height = 1;
    int32 width = 2;
    // Valid colorspace values are
    //   1 - grayscale
    //   2 - grayscale + alpha
    //   3 - RGB
    //   4 - RGBA
    //   5 - DIGITAL_YUV
    //   6 - BGRA
    int32 colorspace = 3;
    // Image data in encoded format.  All image formats supported by
    // image_codec::CoderUtil can be stored here.
    bytes encoded_image_string = 4;
  }

  message Audio {
    // Sample rate of the audio in Hz.
    float sample_rate = 1;
    // Number of channels of audio.
    int64 num_channels = 2;
    // Length of the audio in frames (samples per channel).
    int64 length_frames = 3;
    // Encoded audio data and its associated RFC 2045 content type (e.g.
    // ""audio/wav"").
    bytes encoded_audio_string = 4;
    string content_type = 5;
  }

  message Value {
    // This field is deprecated and will not be set.
    string node_name = 7;

    // Tag name for the data. Used by TensorBoard plugins to organize data. Tags
    // are often organized by scope (which contains slashes to convey
    // hierarchy). For example: foo/bar/0
    string tag = 1;

    // Contains metadata on the summary value such as which plugins may use it.
    // Take note that many summary values may lack a metadata field. This is
    // because the FileWriter only keeps a metadata object on the first summary
    // value with a certain tag for each tag. TensorBoard then remembers which
    // tags are associated with which plugins. This saves space.
    SummaryMetadata metadata = 9;

    // Value associated with the tag.
    oneof value {
      float simple_value = 2;
      bytes obsolete_old_style_histogram = 3;
      Image image = 4;
      HistogramProto histo = 5;
      Audio audio = 6;
      TensorProto tensor = 8;
    }
  }

  // Set of values for the summary.
  repeated Value value = 1;
}
```



### Standalone code to reproduce the issue

```shell
Commands to recreate

>>> git clone https://github.com/tensorflow/tensorflow.git
>>> cd tensorflow
>>> mkdir out
>>> protoc tensorflow/core/framework/summary.proto --mypy_out=out/ -I=""."" -I=""third_party/xla/""
```

### Relevant log output

```shell

```",AlanBogarin,2025-02-06 14:03:17+00:00,['tilakrayal'],2025-02-07 15:37:56+00:00,,https://github.com/tensorflow/tensorflow/issues/86752,"[('type:build/install', 'Build and install issues'), ('type:support', 'Support issues')]","[{'comment_id': 2642104264, 'issue_id': 2835627707, 'author': 'tilakrayal', 'body': '@AlanBogarin,\nCould you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2025, 2, 7, 7, 9, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643250446, 'issue_id': 2835627707, 'author': 'AlanBogarin', 'body': '> [@AlanBogarin](https://github.com/AlanBogarin), Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!\n\n```\nPS D:\\Alan\\github> git clone https://github.com/tensorflow/tensorflow.git\nCloning into \'tensorflow\'...\nremote: Enumerating objects: 1972623, done.\nremote: Counting objects: 100% (428/428), done.\nremote: Compressing objects: 100% (280/280), done.\nremote: Total 1972623 (delta 289), reused 169 (delta 148), pack-reused 1972195 (from 2)\nReceiving objects: 100% (1972623/1972623), 1.07 GiB | 11.73 MiB/s, done.\nResolving deltas: 100% (1623570/1623570), done.\nUpdating files: 100% (34720/34720), done.\nPS D:\\Alan\\github> cd tensorflow\nPS D:\\Alan\\github\\tensorflow> mkdir out\n\n\n    Directorio: D:\\Alan\\github\\tensorflow\n\n\nMode                 LastWriteTime         Length Name\n----                 -------------         ------ ----\nd-----        07/02/2025     12:25                out\n\n\nPS D:\\Alan\\github\\tensorflow> protoc tensorflow/core/framework/summary.proto --mypy_out=out/ -I=""."" -I=""third_party/xla/""\nWriting mypy to tensorflow/core/framework/summary_pb2.pyi\nPS D:\\Alan\\github\\tensorflow>\n```\n\n\nout/tensorflow/core/framework/summary_pb2.pyi\n```python\n""""""\n@generated by mypy-protobuf.  Do not edit manually!\nisort:skip_file\n""""""\n\nimport builtins\nimport collections.abc\nimport google.protobuf.descriptor\nimport google.protobuf.internal.containers\nimport google.protobuf.internal.enum_type_wrapper\nimport google.protobuf.message\nimport sys\nimport tensorflow.core.framework.tensor_pb2\nimport typing\nimport xla.tsl.protobuf.histogram_pb2\n\nif sys.version_info >= (3, 10):\n    import typing as typing_extensions\nelse:\n    import typing_extensions\nfrom xla.tsl.protobuf.histogram_pb2 import (\n    HistogramProto as HistogramProto,\n)\n\nDESCRIPTOR: google.protobuf.descriptor.FileDescriptor\n\nclass _DataClass:\n    ValueType = typing.NewType(""ValueType"", builtins.int)\n    V: typing_extensions.TypeAlias = ValueType\n\nclass _DataClassEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataClass.ValueType], builtins.type):\n    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor\n    DATA_CLASS_UNKNOWN: _DataClass.ValueType  # 0\n    """"""Unknown data class, used (implicitly) for legacy data. Will not be\n    processed by data ingestion pipelines.\n    """"""\n    DATA_CLASS_SCALAR: _DataClass.ValueType  # 1\n    """"""Scalar time series. Each `Value` for the corresponding tag must have\n    `tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).\n    """"""\n    DATA_CLASS_TENSOR: _DataClass.ValueType  # 2\n    """"""Tensor time series. Each `Value` for the corresponding tag must have\n    `tensor` set. The tensor value is arbitrary, but should be small to\n    accommodate direct storage in database backends: an upper bound of a few\n    kilobytes is a reasonable rule of thumb.\n    """"""\n    DATA_CLASS_BLOB_SEQUENCE: _DataClass.ValueType  # 3\n    """"""Blob sequence time series. Each `Value` for the corresponding tag must\n    have `tensor` set to a rank-1 tensor of bytestring dtype.\n    """"""\n\nclass DataClass(_DataClass, metaclass=_DataClassEnumTypeWrapper): ...\n\nDATA_CLASS_UNKNOWN: DataClass.ValueType  # 0\n""""""Unknown data class, used (implicitly) for legacy data. Will not be\nprocessed by data ingestion pipelines.\n""""""\nDATA_CLASS_SCALAR: DataClass.ValueType  # 1\n""""""Scalar time series. Each `Value` for the corresponding tag must have\n`tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).\n""""""\nDATA_CLASS_TENSOR: DataClass.ValueType  # 2\n""""""Tensor time series. Each `Value` for the corresponding tag must have\n`tensor` set. The tensor value is arbitrary, but should be small to\naccommodate direct storage in database backends: an upper bound of a few\nkilobytes is a reasonable rule of thumb.\n""""""\nDATA_CLASS_BLOB_SEQUENCE: DataClass.ValueType  # 3\n""""""Blob sequence time series. Each `Value` for the corresponding tag must\nhave `tensor` set to a rank-1 tensor of bytestring dtype.\n""""""\nglobal___DataClass = DataClass\n\n@typing.final\nclass SummaryDescription(google.protobuf.message.Message):\n    """"""Metadata associated with a series of Summary data""""""\n\n    DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n    TYPE_HINT_FIELD_NUMBER: builtins.int\n    type_hint: builtins.str\n    """"""Hint on how plugins should process the data in this series.\n    Supported values include ""scalar"", ""histogram"", ""image"", ""audio""\n    """"""\n    def __init__(\n        self,\n        *,\n        type_hint: builtins.str = ...,\n    ) -> None: ...\n    def ClearField(self, field_name: typing.Literal[""type_hint"", b""type_hint""]) -> None: ...\n\nglobal___SummaryDescription = SummaryDescription\n\n@typing.final\nclass SummaryMetadata(google.protobuf.message.Message):\n    """"""A SummaryMetadata encapsulates information on which plugins are able to make\n    use of a certain summary value.\n    """"""\n\n    DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n    @typing.final\n    class PluginData(google.protobuf.message.Message):\n        DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n        PLUGIN_NAME_FIELD_NUMBER: builtins.int\n        CONTENT_FIELD_NUMBER: builtins.int\n        plugin_name: builtins.str\n        """"""The name of the plugin this data pertains to.""""""\n        content: builtins.bytes\n        """"""The content to store for the plugin. The best practice is for this to be\n        a binary serialized protocol buffer.\n        """"""\n        def __init__(\n            self,\n            *,\n            plugin_name: builtins.str = ...,\n            content: builtins.bytes = ...,\n        ) -> None: ...\n        def ClearField(self, field_name: typing.Literal[""content"", b""content"", ""plugin_name"", b""plugin_name""]) -> None: ...\n\n    PLUGIN_DATA_FIELD_NUMBER: builtins.int\n    DISPLAY_NAME_FIELD_NUMBER: builtins.int\n    SUMMARY_DESCRIPTION_FIELD_NUMBER: builtins.int\n    DATA_CLASS_FIELD_NUMBER: builtins.int\n    display_name: builtins.str\n    """"""Display name for viewing in TensorBoard.""""""\n    summary_description: builtins.str\n    """"""Longform readable description of the summary sequence. Markdown supported.""""""\n    data_class: global___DataClass.ValueType\n    """"""Class of data stored in this time series. Required for compatibility with\n    TensorBoard\'s generic data facilities (`DataProvider`, et al.). This value\n    imposes constraints on the dtype and shape of the corresponding tensor\n    values. See `DataClass` docs for details.\n    """"""\n    @property\n    def plugin_data(self) -> global___SummaryMetadata.PluginData:\n        """"""Data that associates a summary with a certain plugin.""""""\n\n    def __init__(\n        self,\n        *,\n        plugin_data: global___SummaryMetadata.PluginData | None = ...,\n        display_name: builtins.str = ...,\n        summary_description: builtins.str = ...,\n        data_class: global___DataClass.ValueType = ...,\n    ) -> None: ...\n    def HasField(self, field_name: typing.Literal[""plugin_data"", b""plugin_data""]) -> builtins.bool: ...\n    def ClearField(self, field_name: typing.Literal[""data_class"", b""data_class"", ""display_name"", b""display_name"", ""plugin_data"", b""plugin_data"", ""summary_description"", b""summary_description""]) -> None: ...\n\nglobal___SummaryMetadata = SummaryMetadata\n\n@typing.final\nclass Summary(google.protobuf.message.Message):\n    """"""A Summary is a set of named values to be displayed by the\n    visualizer.\n\n    Summaries are produced regularly during training, as controlled by\n    the ""summary_interval_secs"" attribute of the training operation.\n    Summaries are also produced at the end of an evaluation.\n    """"""\n\n    DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n    @typing.final\n    class Image(google.protobuf.message.Message):\n        DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n        HEIGHT_FIELD_NUMBER: builtins.int\n        WIDTH_FIELD_NUMBER: builtins.int\n        COLORSPACE_FIELD_NUMBER: builtins.int\n        ENCODED_IMAGE_STRING_FIELD_NUMBER: builtins.int\n        height: builtins.int\n        """"""Dimensions of the image.""""""\n        width: builtins.int\n        colorspace: builtins.int\n        """"""Valid colorspace values are\n          1 - grayscale\n          2 - grayscale + alpha\n          3 - RGB\n          4 - RGBA\n          5 - DIGITAL_YUV\n          6 - BGRA\n        """"""\n        encoded_image_string: builtins.bytes\n        """"""Image data in encoded format.  All image formats supported by\n        image_codec::CoderUtil can be stored here.\n        """"""\n        def __init__(\n            self,\n            *,\n            height: builtins.int = ...,\n            width: builtins.int = ...,\n            colorspace: builtins.int = ...,\n            encoded_image_string: builtins.bytes = ...,\n        ) -> None: ...\n        def ClearField(self, field_name: typing.Literal[""colorspace"", b""colorspace"", ""encoded_image_string"", b""encoded_image_string"", ""height"", b""height"", ""width"", b""width""]) -> None: ...\n\n    @typing.final\n    class Audio(google.protobuf.message.Message):\n        DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n        SAMPLE_RATE_FIELD_NUMBER: builtins.int\n        NUM_CHANNELS_FIELD_NUMBER: builtins.int\n        LENGTH_FRAMES_FIELD_NUMBER: builtins.int\n        ENCODED_AUDIO_STRING_FIELD_NUMBER: builtins.int\n        CONTENT_TYPE_FIELD_NUMBER: builtins.int\n        sample_rate: builtins.float\n        """"""Sample rate of the audio in Hz.""""""\n        num_channels: builtins.int\n        """"""Number of channels of audio.""""""\n        length_frames: builtins.int\n        """"""Length of the audio in frames (samples per channel).""""""\n        encoded_audio_string: builtins.bytes\n        """"""Encoded audio data and its associated RFC 2045 content type (e.g.\n        ""audio/wav"").\n        """"""\n        content_type: builtins.str\n        def __init__(\n            self,\n            *,\n            sample_rate: builtins.float = ...,\n            num_channels: builtins.int = ...,\n            length_frames: builtins.int = ...,\n            encoded_audio_string: builtins.bytes = ...,\n            content_type: builtins.str = ...,\n        ) -> None: ...\n        def ClearField(self, field_name: typing.Literal[""content_type"", b""content_type"", ""encoded_audio_string"", b""encoded_audio_string"", ""length_frames"", b""length_frames"", ""num_channels"", b""num_channels"", ""sample_rate"", b""sample_rate""]) -> None: ...\n\n    @typing.final\n    class Value(google.protobuf.message.Message):\n        DESCRIPTOR: google.protobuf.descriptor.Descriptor\n\n        NODE_NAME_FIELD_NUMBER: builtins.int\n        TAG_FIELD_NUMBER: builtins.int\n        METADATA_FIELD_NUMBER: builtins.int\n        SIMPLE_VALUE_FIELD_NUMBER: builtins.int\n        OBSOLETE_OLD_STYLE_HISTOGRAM_FIELD_NUMBER: builtins.int\n        IMAGE_FIELD_NUMBER: builtins.int\n        HISTO_FIELD_NUMBER: builtins.int\n        AUDIO_FIELD_NUMBER: builtins.int\n        TENSOR_FIELD_NUMBER: builtins.int\n        node_name: builtins.str\n        """"""This field is deprecated and will not be set.""""""\n        tag: builtins.str\n        """"""Tag name for the data. Used by TensorBoard plugins to organize data. Tags\n        are often organized by scope (which contains slashes to convey\n        hierarchy). For example: foo/bar/0\n        """"""\n        simple_value: builtins.float\n        obsolete_old_style_histogram: builtins.bytes\n        @property\n        def metadata(self) -> global___SummaryMetadata:\n            """"""Contains metadata on the summary value such as which plugins may use it.\n            Take note that many summary values may lack a metadata field. This is\n            because the FileWriter only keeps a metadata object on the first summary\n            value with a certain tag for each tag. TensorBoard then remembers which\n            tags are associated with which plugins. This saves space.\n            """"""\n\n        @property\n        def image(self) -> global___Summary.Image: ...\n        @property\n        def histo(self) -> xla.tsl.protobuf.histogram_pb2.HistogramProto: ...\n        @property\n        def audio(self) -> global___Summary.Audio: ...\n        @property\n        def tensor(self) -> tensorflow.core.framework.tensor_pb2.TensorProto: ...\n        def __init__(\n            self,\n            *,\n            node_name: builtins.str = ...,\n            tag: builtins.str = ...,\n            metadata: global___SummaryMetadata | None = ...,\n            simple_value: builtins.float = ...,\n            obsolete_old_style_histogram: builtins.bytes = ...,\n            image: global___Summary.Image | None = ...,\n            histo: xla.tsl.protobuf.histogram_pb2.HistogramProto | None = ...,\n            audio: global___Summary.Audio | None = ...,\n            tensor: tensorflow.core.framework.tensor_pb2.TensorProto | None = ...,\n        ) -> None: ...\n        def HasField(self, field_name: typing.Literal[""audio"", b""audio"", ""histo"", b""histo"", ""image"", b""image"", ""metadata"", b""metadata"", ""obsolete_old_style_histogram"", b""obsolete_old_style_histogram"", ""simple_value"", b""simple_value"", ""tensor"", b""tensor"", ""value"", b""value""]) -> builtins.bool: ...\n        def ClearField(self, field_name: typing.Literal[""audio"", b""audio"", ""histo"", b""histo"", ""image"", b""image"", ""metadata"", b""metadata"", ""node_name"", b""node_name"", ""obsolete_old_style_histogram"", b""obsolete_old_style_histogram"", ""simple_value"", b""simple_value"", ""tag"", b""tag"", ""tensor"", b""tensor"", ""value"", b""value""]) -> None: ...\n        def WhichOneof(self, oneof_group: typing.Literal[""value"", b""value""]) -> typing.Literal[""simple_value"", ""obsolete_old_style_histogram"", ""image"", ""histo"", ""audio"", ""tensor""] | None: ...\n\n    VALUE_FIELD_NUMBER: builtins.int\n    @property\n    def value(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Summary.Value]:\n        """"""Set of values for the summary.""""""\n\n    def __init__(\n        self,\n        *,\n        value: collections.abc.Iterable[global___Summary.Value] | None = ...,\n    ) -> None: ...\n    def ClearField(self, field_name: typing.Literal[""value"", b""value""]) -> None: ...\n\nglobal___Summary = Summary\n```', 'created_at': datetime.datetime(2025, 2, 7, 15, 29, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643269769, 'issue_id': 2835627707, 'author': 'AlanBogarin', 'body': 'When I install tensorflow with pip, tsl is placed inside tensorflow\n\n```\ntensorflow/\n    tsl/\n        profiler/\n            ...\n        protobuf/\n            histogram_pb2.py\n            ...\n        ...\n    ...\n```', 'created_at': datetime.datetime(2025, 2, 7, 15, 37, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-02-07 07:09:30 UTC): @AlanBogarin,
Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!

AlanBogarin (Issue Creator) on (2025-02-07 15:29:10 UTC): ```
PS D:\Alan\github> git clone https://github.com/tensorflow/tensorflow.git
Cloning into 'tensorflow'...
remote: Enumerating objects: 1972623, done.
remote: Counting objects: 100% (428/428), done.
remote: Compressing objects: 100% (280/280), done.
remote: Total 1972623 (delta 289), reused 169 (delta 148), pack-reused 1972195 (from 2)
Receiving objects: 100% (1972623/1972623), 1.07 GiB | 11.73 MiB/s, done.
Resolving deltas: 100% (1623570/1623570), done.
Updating files: 100% (34720/34720), done.
PS D:\Alan\github> cd tensorflow
PS D:\Alan\github\tensorflow> mkdir out


    Directorio: D:\Alan\github\tensorflow


Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
d-----        07/02/2025     12:25                out


PS D:\Alan\github\tensorflow> protoc tensorflow/core/framework/summary.proto --mypy_out=out/ -I=""."" -I=""third_party/xla/""
Writing mypy to tensorflow/core/framework/summary_pb2.pyi
PS D:\Alan\github\tensorflow>
```


out/tensorflow/core/framework/summary_pb2.pyi
```python
""""""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
""""""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import tensorflow.core.framework.tensor_pb2
import typing
import xla.tsl.protobuf.histogram_pb2

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions
from xla.tsl.protobuf.histogram_pb2 import (
    HistogramProto as HistogramProto,
)

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _DataClass:
    ValueType = typing.NewType(""ValueType"", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _DataClassEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataClass.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    DATA_CLASS_UNKNOWN: _DataClass.ValueType  # 0
    """"""Unknown data class, used (implicitly) for legacy data. Will not be
    processed by data ingestion pipelines.
    """"""
    DATA_CLASS_SCALAR: _DataClass.ValueType  # 1
    """"""Scalar time series. Each `Value` for the corresponding tag must have
    `tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).
    """"""
    DATA_CLASS_TENSOR: _DataClass.ValueType  # 2
    """"""Tensor time series. Each `Value` for the corresponding tag must have
    `tensor` set. The tensor value is arbitrary, but should be small to
    accommodate direct storage in database backends: an upper bound of a few
    kilobytes is a reasonable rule of thumb.
    """"""
    DATA_CLASS_BLOB_SEQUENCE: _DataClass.ValueType  # 3
    """"""Blob sequence time series. Each `Value` for the corresponding tag must
    have `tensor` set to a rank-1 tensor of bytestring dtype.
    """"""

class DataClass(_DataClass, metaclass=_DataClassEnumTypeWrapper): ...

DATA_CLASS_UNKNOWN: DataClass.ValueType  # 0
""""""Unknown data class, used (implicitly) for legacy data. Will not be
processed by data ingestion pipelines.
""""""
DATA_CLASS_SCALAR: DataClass.ValueType  # 1
""""""Scalar time series. Each `Value` for the corresponding tag must have
`tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).
""""""
DATA_CLASS_TENSOR: DataClass.ValueType  # 2
""""""Tensor time series. Each `Value` for the corresponding tag must have
`tensor` set. The tensor value is arbitrary, but should be small to
accommodate direct storage in database backends: an upper bound of a few
kilobytes is a reasonable rule of thumb.
""""""
DATA_CLASS_BLOB_SEQUENCE: DataClass.ValueType  # 3
""""""Blob sequence time series. Each `Value` for the corresponding tag must
have `tensor` set to a rank-1 tensor of bytestring dtype.
""""""
global___DataClass = DataClass

@typing.final
class SummaryDescription(google.protobuf.message.Message):
    """"""Metadata associated with a series of Summary data""""""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TYPE_HINT_FIELD_NUMBER: builtins.int
    type_hint: builtins.str
    """"""Hint on how plugins should process the data in this series.
    Supported values include ""scalar"", ""histogram"", ""image"", ""audio""
    """"""
    def __init__(
        self,
        *,
        type_hint: builtins.str = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal[""type_hint"", b""type_hint""]) -> None: ...

global___SummaryDescription = SummaryDescription

@typing.final
class SummaryMetadata(google.protobuf.message.Message):
    """"""A SummaryMetadata encapsulates information on which plugins are able to make
    use of a certain summary value.
    """"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class PluginData(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        PLUGIN_NAME_FIELD_NUMBER: builtins.int
        CONTENT_FIELD_NUMBER: builtins.int
        plugin_name: builtins.str
        """"""The name of the plugin this data pertains to.""""""
        content: builtins.bytes
        """"""The content to store for the plugin. The best practice is for this to be
        a binary serialized protocol buffer.
        """"""
        def __init__(
            self,
            *,
            plugin_name: builtins.str = ...,
            content: builtins.bytes = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal[""content"", b""content"", ""plugin_name"", b""plugin_name""]) -> None: ...

    PLUGIN_DATA_FIELD_NUMBER: builtins.int
    DISPLAY_NAME_FIELD_NUMBER: builtins.int
    SUMMARY_DESCRIPTION_FIELD_NUMBER: builtins.int
    DATA_CLASS_FIELD_NUMBER: builtins.int
    display_name: builtins.str
    """"""Display name for viewing in TensorBoard.""""""
    summary_description: builtins.str
    """"""Longform readable description of the summary sequence. Markdown supported.""""""
    data_class: global___DataClass.ValueType
    """"""Class of data stored in this time series. Required for compatibility with
    TensorBoard's generic data facilities (`DataProvider`, et al.). This value
    imposes constraints on the dtype and shape of the corresponding tensor
    values. See `DataClass` docs for details.
    """"""
    @property
    def plugin_data(self) -> global___SummaryMetadata.PluginData:
        """"""Data that associates a summary with a certain plugin.""""""

    def __init__(
        self,
        *,
        plugin_data: global___SummaryMetadata.PluginData | None = ...,
        display_name: builtins.str = ...,
        summary_description: builtins.str = ...,
        data_class: global___DataClass.ValueType = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal[""plugin_data"", b""plugin_data""]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal[""data_class"", b""data_class"", ""display_name"", b""display_name"", ""plugin_data"", b""plugin_data"", ""summary_description"", b""summary_description""]) -> None: ...

global___SummaryMetadata = SummaryMetadata

@typing.final
class Summary(google.protobuf.message.Message):
    """"""A Summary is a set of named values to be displayed by the
    visualizer.

    Summaries are produced regularly during training, as controlled by
    the ""summary_interval_secs"" attribute of the training operation.
    Summaries are also produced at the end of an evaluation.
    """"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class Image(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        HEIGHT_FIELD_NUMBER: builtins.int
        WIDTH_FIELD_NUMBER: builtins.int
        COLORSPACE_FIELD_NUMBER: builtins.int
        ENCODED_IMAGE_STRING_FIELD_NUMBER: builtins.int
        height: builtins.int
        """"""Dimensions of the image.""""""
        width: builtins.int
        colorspace: builtins.int
        """"""Valid colorspace values are
          1 - grayscale
          2 - grayscale + alpha
          3 - RGB
          4 - RGBA
          5 - DIGITAL_YUV
          6 - BGRA
        """"""
        encoded_image_string: builtins.bytes
        """"""Image data in encoded format.  All image formats supported by
        image_codec::CoderUtil can be stored here.
        """"""
        def __init__(
            self,
            *,
            height: builtins.int = ...,
            width: builtins.int = ...,
            colorspace: builtins.int = ...,
            encoded_image_string: builtins.bytes = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal[""colorspace"", b""colorspace"", ""encoded_image_string"", b""encoded_image_string"", ""height"", b""height"", ""width"", b""width""]) -> None: ...

    @typing.final
    class Audio(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        SAMPLE_RATE_FIELD_NUMBER: builtins.int
        NUM_CHANNELS_FIELD_NUMBER: builtins.int
        LENGTH_FRAMES_FIELD_NUMBER: builtins.int
        ENCODED_AUDIO_STRING_FIELD_NUMBER: builtins.int
        CONTENT_TYPE_FIELD_NUMBER: builtins.int
        sample_rate: builtins.float
        """"""Sample rate of the audio in Hz.""""""
        num_channels: builtins.int
        """"""Number of channels of audio.""""""
        length_frames: builtins.int
        """"""Length of the audio in frames (samples per channel).""""""
        encoded_audio_string: builtins.bytes
        """"""Encoded audio data and its associated RFC 2045 content type (e.g.
        ""audio/wav"").
        """"""
        content_type: builtins.str
        def __init__(
            self,
            *,
            sample_rate: builtins.float = ...,
            num_channels: builtins.int = ...,
            length_frames: builtins.int = ...,
            encoded_audio_string: builtins.bytes = ...,
            content_type: builtins.str = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal[""content_type"", b""content_type"", ""encoded_audio_string"", b""encoded_audio_string"", ""length_frames"", b""length_frames"", ""num_channels"", b""num_channels"", ""sample_rate"", b""sample_rate""]) -> None: ...

    @typing.final
    class Value(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        NODE_NAME_FIELD_NUMBER: builtins.int
        TAG_FIELD_NUMBER: builtins.int
        METADATA_FIELD_NUMBER: builtins.int
        SIMPLE_VALUE_FIELD_NUMBER: builtins.int
        OBSOLETE_OLD_STYLE_HISTOGRAM_FIELD_NUMBER: builtins.int
        IMAGE_FIELD_NUMBER: builtins.int
        HISTO_FIELD_NUMBER: builtins.int
        AUDIO_FIELD_NUMBER: builtins.int
        TENSOR_FIELD_NUMBER: builtins.int
        node_name: builtins.str
        """"""This field is deprecated and will not be set.""""""
        tag: builtins.str
        """"""Tag name for the data. Used by TensorBoard plugins to organize data. Tags
        are often organized by scope (which contains slashes to convey
        hierarchy). For example: foo/bar/0
        """"""
        simple_value: builtins.float
        obsolete_old_style_histogram: builtins.bytes
        @property
        def metadata(self) -> global___SummaryMetadata:
            """"""Contains metadata on the summary value such as which plugins may use it.
            Take note that many summary values may lack a metadata field. This is
            because the FileWriter only keeps a metadata object on the first summary
            value with a certain tag for each tag. TensorBoard then remembers which
            tags are associated with which plugins. This saves space.
            """"""

        @property
        def image(self) -> global___Summary.Image: ...
        @property
        def histo(self) -> xla.tsl.protobuf.histogram_pb2.HistogramProto: ...
        @property
        def audio(self) -> global___Summary.Audio: ...
        @property
        def tensor(self) -> tensorflow.core.framework.tensor_pb2.TensorProto: ...
        def __init__(
            self,
            *,
            node_name: builtins.str = ...,
            tag: builtins.str = ...,
            metadata: global___SummaryMetadata | None = ...,
            simple_value: builtins.float = ...,
            obsolete_old_style_histogram: builtins.bytes = ...,
            image: global___Summary.Image | None = ...,
            histo: xla.tsl.protobuf.histogram_pb2.HistogramProto | None = ...,
            audio: global___Summary.Audio | None = ...,
            tensor: tensorflow.core.framework.tensor_pb2.TensorProto | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal[""audio"", b""audio"", ""histo"", b""histo"", ""image"", b""image"", ""metadata"", b""metadata"", ""obsolete_old_style_histogram"", b""obsolete_old_style_histogram"", ""simple_value"", b""simple_value"", ""tensor"", b""tensor"", ""value"", b""value""]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal[""audio"", b""audio"", ""histo"", b""histo"", ""image"", b""image"", ""metadata"", b""metadata"", ""node_name"", b""node_name"", ""obsolete_old_style_histogram"", b""obsolete_old_style_histogram"", ""simple_value"", b""simple_value"", ""tag"", b""tag"", ""tensor"", b""tensor"", ""value"", b""value""]) -> None: ...
        def WhichOneof(self, oneof_group: typing.Literal[""value"", b""value""]) -> typing.Literal[""simple_value"", ""obsolete_old_style_histogram"", ""image"", ""histo"", ""audio"", ""tensor""] | None: ...

    VALUE_FIELD_NUMBER: builtins.int
    @property
    def value(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Summary.Value]:
        """"""Set of values for the summary.""""""

    def __init__(
        self,
        *,
        value: collections.abc.Iterable[global___Summary.Value] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal[""value"", b""value""]) -> None: ...

global___Summary = Summary
```

AlanBogarin (Issue Creator) on (2025-02-07 15:37:54 UTC): When I install tensorflow with pip, tsl is placed inside tensorflow

```
tensorflow/
    tsl/
        profiler/
            ...
        protobuf/
            histogram_pb2.py
            ...
        ...
    ...
```

"
2833784075,issue,open,,Issue created for Rollback of PR #82210: Align num_threads parameter validation with documentation,"Merged PR #82210 is rolled back in d9039b966f5b2468b37a2e7d04208fe3713d2e0d.
    Please follow up with the reviewer and close this issue once its resolved.",github-actions[bot],2025-02-05 19:25:28+00:00,"['gbaned', 'gaikwadrahul8', 'chunnienc', 'Venkat6871']",2025-02-05 19:25:29+00:00,,https://github.com/tensorflow/tensorflow/issues/86670,[],[],
2832007193,issue,open,,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], 
                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(
[[[0.910156 2.14062]
  [2.92188 1.21875]]

 [[0.742188 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Output on GPU: tf.Tensor(
[[[0.914062 2.15625]
  [2.90625 1.21875]]

 [[0.738281 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",usmonali4,2025-02-05 06:12:29+00:00,['tilakrayal'],2025-02-06 17:35:49+00:00,,https://github.com/tensorflow/tensorflow/issues/86607,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]",[],
2829024522,issue,open,,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real = tf.constant([1.5634333], dtype=tf.float32)
imag = tf.constant([0.020735], dtype=tf.float32)

complex_tensor = tf.complex(real, imag)

with tf.device('/CPU:0'):
    result_cpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_cpu)

with tf.device('/GPU:0'):
    result_gpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_gpu)

##Comparing whole complex numbers
max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_cons.numpy())

##Comparing by parts
real_part_cpu = tf.math.real(result_cpu)
real_part_gpu = tf.math.real(result_gpu)
real_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()
real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)

imag_part_cpu = tf.math.imag(result_cpu)
imag_part_gpu = tf.math.imag(result_gpu)
imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()
imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)

print(""Real parts absolute difference:"", real_part_diff)
print(""Real parts Consistency check with atol=1e-5 and rtol=1e-6:"", real_part_cons.numpy())

print(""Imag parts absolute difference:"", imag_part_diff)
print(""Imag parts Consistency check with atol=1e-5 and rtol=1e-6:"", imag_part_cons.numpy())
```

### Relevant log output

```shell
tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)
tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)

Max absolute difference: 8.5064334e-05
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False

Real parts absolute difference: 2.861023e-05
Real parts Consistency check with atol=1e-5 and rtol=1e-6: False

Imag parts absolute difference: 8.010864e-05
Imag parts Consistency check with atol=1e-5 and rtol=1e-6: False
```",usmonali4,2025-02-04 03:18:15+00:00,['Venkat6871'],2025-02-04 08:16:19+00:00,,https://github.com/tensorflow/tensorflow/issues/86506,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2633179933, 'issue_id': 2829024522, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/674aae5a2568a82300a45e32097e560c/86506_tf_2-18-0-nightly-v-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f36a3ab7b66feed854f408fb2c2f76cb/86506_tf_2-18-0-nightly-v-gpu.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 2, 4, 8, 16, 18, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-02-04 08:16:18 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/674aae5a2568a82300a45e32097e560c/86506_tf_2-18-0-nightly-v-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f36a3ab7b66feed854f408fb2c2f76cb/86506_tf_2-18-0-nightly-v-gpu.ipynb) here for your reference.
Thank you!

"
2826301913,issue,open,,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

logits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)

Output on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",usmonali4,2025-02-03 03:30:58+00:00,['tilakrayal'],2025-02-04 04:12:03+00:00,,https://github.com/tensorflow/tensorflow/issues/86434,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]",[],
2825970512,issue,closed,completed,ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter': Auto-Configuration Error: Visual C++ build tools not found on your machine,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

windows 11 pro

### Mobile device

N/A

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

	CLANG 17.0.6

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

that it would be compiled, I followed the instructions for windows https://www.tensorflow.org/install/source_windows


### Standalone code to reproduce the issue

```shell
N/A
```

### Relevant log output

```shell
DEBUG: C:/users/user/_bazel_user/yopxtjri/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.12
INFO: Repository local_config_def_file_filter instantiated at:
  C:/users/user/tensorflow/WORKSPACE:64:14: in <toplevel>
  C:/users/user/tensorflow/tensorflow/workspace2.bzl:936:19: in workspace
  C:/users/user/tensorflow/tensorflow/workspace2.bzl:117:30: in _tf_toolchains
Repository rule def_file_filter_configure defined at:
  C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:54:44: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter':
   Traceback (most recent call last):
        File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl
                auto_configure_fail(""Visual C++ build tools not found on your machine"")
        File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail
                fail(""\n%sAuto-Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail:
Auto-Configuration Error: Visual C++ build tools not found on your machine
ERROR: C:/users/user/tensorflow/WORKSPACE:64:14: fetching def_file_filter_configure rule //external:local_config_def_file_filter: Traceback (most recent call last):
        File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl
                auto_configure_fail(""Visual C++ build tools not found on your machine"")
        File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail
                fail(""\n%sAuto-Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail:
Auto-Configuration Error: Visual C++ build tools not found on your machine
ERROR: C:/users/user/tensorflow/tensorflow/python/BUILD:978:8: //tensorflow/python:pywrap_tensorflow_filtered_def_file depends on @local_config_def_file_filter//:def_file_filter in repository @local_config_def_file_filter which failed to fetch. no such package '@local_config_def_file_filter//':
Auto-Configuration Error: Visual C++ build tools not found on your machine
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:
INFO: Elapsed time: 584.412s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (453 packages loaded, 9221 targets configured)
    currently loading: @jsoncpp_git// ... (6 packages)
    Fetching repository @eigen_archive; starting 6s
    Fetching ...r/yopxtjri/external/eigen_archive; Extracting eigen-33d0937c6bdf5ec999939fb17f2a553183d14a74.tar.gz 6s
    Fetching repository @pypi_markupsafe; starting
```",dicotom,2025-02-02 19:31:16+00:00,['Venkat6871'],2025-02-02 20:12:56+00:00,2025-02-02 20:12:04+00:00,https://github.com/tensorflow/tensorflow/issues/86410,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2629537998, 'issue_id': 2825970512, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86410"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86410"">No</a>', 'created_at': datetime.datetime(2025, 2, 2, 20, 12, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629538249, 'issue_id': 2825970512, 'author': 'dicotom', 'body': 'my fault.', 'created_at': datetime.datetime(2025, 2, 2, 20, 12, 55, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2025-02-02 20:12:06 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86410"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86410"">No</a>

dicotom (Issue Creator) on (2025-02-02 20:12:55 UTC): my fault.

"
2825696560,issue,open,,User Guide: Deprecated Nvidia Docker Link,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

any

### Custom code

No

### OS platform and distribution

Linux GPU

### Mobile device

_No response_

### Python version

any

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The link to the Nvidia Docker github

https://github.com/NVIDIA/nvidia-docker?tab=readme-ov-file

Reports:

This repository has been archived by the owner on Jan 22, 2024. It is now read-only. 

and provides a link to:

https://github.com/NVIDIA/nvidia-container-toolkit

Titled:

Build and run containers leveraging NVIDIA GPUs 


### Standalone code to reproduce the issue

```shell
This is a documentation bug and no code errors are involved.
```

### Relevant log output

```shell
See above
```",Cloudberrydotdev,2025-02-02 09:16:37+00:00,['tilakrayal'],2025-02-04 06:44:53+00:00,,https://github.com/tensorflow/tensorflow/issues/86407,"[('type:docs-bug', 'Document issues'), ('type:build/install', 'Build and install issues'), ('awaiting PR merge', 'awaiting PR merge')]","[{'comment_id': 2629313282, 'issue_id': 2825696560, 'author': 'Cloudberrydotdev', 'body': 'https://www.tensorflow.org/install/docker\n\nSorry I meant to include this url for the documentation page that holds the error link.', 'created_at': datetime.datetime(2025, 2, 2, 9, 21, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629327401, 'issue_id': 2825696560, 'author': 'Cloudberrydotdev', 'body': 'The relevant text is:\n\nDocker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA GPU driver is required on the host machine (the NVIDIA CUDA Toolkit does not need to be installed).\n\nThe link is in the text:\nNVIDIA GPU driver', 'created_at': datetime.datetime(2025, 2, 2, 10, 5, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629463820, 'issue_id': 2825696560, 'author': 'cuixue', 'body': '', 'created_at': datetime.datetime(2025, 2, 2, 16, 33, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629464145, 'issue_id': 2825696560, 'author': 'samfletche', 'body': ""The issue is that the TensorFlow documentation still links to the now-archived NVIDIA Docker repository. Instead, it should link to the new, active repository:  \n\nSolution:\n\n1. The TensorFlow documentation team needs to update the incorrect link.  \n2. The old link:  \n   ```\n   https://github.com/NVIDIA/nvidia-docker?tab=readme-ov-file\n   ```\n   Should be replaced with:  \n   ```\n   https://github.com/NVIDIA/nvidia-container-toolkit\n   ```\n3. If you're relying on the old NVIDIA Docker setup, transition to using the **NVIDIA Container Toolkit** as per the new repository. You can follow their official setup guide:  \n   \n\nFor now, you can manually install TensorFlow with GPU support using the NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker."", 'created_at': datetime.datetime(2025, 2, 2, 16, 35, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630472714, 'issue_id': 2825696560, 'author': 'Cloudberrydotdev', 'body': ""Sam,\r\n\r\nThanks for picking this up.\r\n\r\nI'm not sure what happens now. Do I wait for a reply from the Tensorflow\r\ndocumentation team before closing this issue?\r\n\r\nRegards\r\nIan Berry\r\n\r\nOn Sun, 2 Feb 2025, 16:35 Sam Fletcher, ***@***.***> wrote:\r\n\r\n> The issue is that the TensorFlow documentation still links to the\r\n> now-archived NVIDIA Docker repository. Instead, it should link to the new,\r\n> active repository:\r\n>\r\n> Solution:\r\n>\r\n>    1. The TensorFlow documentation team needs to update the incorrect\r\n>    link.\r\n>    2. The old link:\r\n>\r\n>    https://github.com/NVIDIA/nvidia-docker?tab=readme-ov-file\r\n>\r\n>    Should be replaced with:\r\n>\r\n>    https://github.com/NVIDIA/nvidia-container-toolkit\r\n>\r\n>    3. If you're relying on the old NVIDIA Docker setup, transition to\r\n>    using the *NVIDIA Container Toolkit* as per the new repository. You\r\n>    can follow their official setup guide:\r\n>\r\n> For now, you can manually install TensorFlow with GPU support using the\r\n> NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/86407#issuecomment-2629464145>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AIXIRM3UPR47IZDDB4NC6PT2NZCM7AVCNFSM6AAAAABWKGMYDKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMRZGQ3DIMJUGU>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 2, 3, 9, 58, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633017408, 'issue_id': 2825696560, 'author': 'tilakrayal', 'body': '@Cloudberrydotdev,\nThank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. Thank you!', 'created_at': datetime.datetime(2025, 2, 4, 6, 44, 47, tzinfo=datetime.timezone.utc)}]","Cloudberrydotdev (Issue Creator) on (2025-02-02 09:21:29 UTC): https://www.tensorflow.org/install/docker

Sorry I meant to include this url for the documentation page that holds the error link.

Cloudberrydotdev (Issue Creator) on (2025-02-02 10:05:35 UTC): The relevant text is:

Docker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA GPU driver is required on the host machine (the NVIDIA CUDA Toolkit does not need to be installed).

The link is in the text:
NVIDIA GPU driver

cuixue on (2025-02-02 16:33:58 UTC): 

samfletche on (2025-02-02 16:35:03 UTC): The issue is that the TensorFlow documentation still links to the now-archived NVIDIA Docker repository. Instead, it should link to the new, active repository:  

Solution:

1. The TensorFlow documentation team needs to update the incorrect link.  
2. The old link:  
   ```
   https://github.com/NVIDIA/nvidia-docker?tab=readme-ov-file
   ```
   Should be replaced with:  
   ```
   https://github.com/NVIDIA/nvidia-container-toolkit
   ```
3. If you're relying on the old NVIDIA Docker setup, transition to using the **NVIDIA Container Toolkit** as per the new repository. You can follow their official setup guide:  
   

For now, you can manually install TensorFlow with GPU support using the NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker.

Cloudberrydotdev (Issue Creator) on (2025-02-03 09:58:17 UTC): Sam,

Thanks for picking this up.

I'm not sure what happens now. Do I wait for a reply from the Tensorflow
documentation team before closing this issue?

Regards
Ian Berry

On Sun, 2 Feb 2025, 16:35 Sam Fletcher, ***@***.***> wrote:

tilakrayal (Assginee) on (2025-02-04 06:44:47 UTC): @Cloudberrydotdev,
Thank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. Thank you!

"
2825649807,issue,open,,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

t = tf.constant([
    [[0.9922, -1.4922], 
     [0.0376,  0.1504], 
     [0.6172,  1.2266]],

    [[-0.1387,  1.3047], 
     [0.3535, -0.0471], 
     [0.0437,  0.2637]]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)

Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",usmonali4,2025-02-02 07:05:17+00:00,['Venkat6871'],2025-02-04 08:15:30+00:00,,https://github.com/tensorflow/tensorflow/issues/86406,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2632997840, 'issue_id': 2825649807, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/fc9cba34c4d287ef8ceb9db339f08555/86406_tf_2-18-0-nightly-v-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f2bf0382e989553fcfde8458bd07fdf3/86406_tf_2-18-0-nightly-v-gpu.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 2, 4, 6, 31, 38, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-02-04 06:31:38 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/fc9cba34c4d287ef8ceb9db339f08555/86406_tf_2-18-0-nightly-v-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f2bf0382e989553fcfde8458bd07fdf3/86406_tf_2-18-0-nightly-v-gpu.ipynb) here for your reference.
Thank you!

"
2825603912,issue,open,,TF wheel shouldn't be built with CUDA dependencies,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

CentOS Stream 9

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

11.5.0

### CUDA/cuDNN version

12.6.1/8.9.7.29

### GPU model and memory

2080 Ti 11GB

### Current behavior?

Build fails with following error:

Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `--config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `--@local_config_cuda//cuda:override_include_cuda_libs=true`.

Current driver:

`
NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8
`

Build command:

`
bazel build --config=cuda --local_cpu_resources=HOST_CPUS*.8 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
`

What I missed?

### Standalone code to reproduce the issue

```shell
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3.9/site-packages
  /usr/lib64/python3.9/site-packages
  /usr/local/lib/python3.9/site-packages
  /usr/local/lib64/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.6.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version.


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/lib64/ccache/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

```

### Relevant log output

```shell
ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: in tf_wheel rule //tensorflow/tools/pip_package:wheel:
Traceback (most recent call last):
        File ""/usr/src/tensorflow/tensorflow/tools/pip_package/utils/tf_wheel.bzl"", line 72, column 13, in _tf_wheel_impl
                fail(""TF wheel shouldn't be built with CUDA dependencies."" +
Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `--config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `--@local_config_cuda//cuda:override_include_cuda_libs=true`.
ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: Analysis of target '//tensorflow/tools/pip_package:wheel' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:
INFO: Elapsed time: 189.679s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (790 packages loaded, 56723 targets configured)
```",regularRandom,2025-02-02 04:59:19+00:00,['tilakrayal'],2025-02-06 17:40:46+00:00,,https://github.com/tensorflow/tensorflow/issues/86405,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2640564790, 'issue_id': 2825603912, 'author': 'tilakrayal', 'body': '@regularRandom,\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, Tensorflow v2.18 is compatible with python 3.9-3.12, compiler(Clang)- 17.0.6, Bazel - 6.5.0,  cudNN-9.3, CUDA-12.5\n\nhttps://www.tensorflow.org/install/source#gpu\n\nAlso To build tensorflow GPU package:\n\n`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel`\n\nhttps://www.tensorflow.org/install/source#build_the_package\n\nThank you!', 'created_at': datetime.datetime(2025, 2, 6, 17, 40, 39, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-02-06 17:40:39 UTC): @regularRandom,
Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, Tensorflow v2.18 is compatible with python 3.9-3.12, compiler(Clang)- 17.0.6, Bazel - 6.5.0,  cudNN-9.3, CUDA-12.5

https://www.tensorflow.org/install/source#gpu

Also To build tensorflow GPU package:

`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel`

https://www.tensorflow.org/install/source#build_the_package

Thank you!

"
2825135479,issue,open,,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

out_backprop = tf.constant([
    [
        [
            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], 
            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]
        ],
        [
            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],
            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]
        ],
        [
            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],
            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]
        ]
    ],
    [
        [
            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],
            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]
        ],
        [
            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],
            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]
        ],
        [
            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],
            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]
        ]
    ]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
BiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)

BiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)

Max absolute difference: 0.03125
Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",usmonali4,2025-02-01 10:43:06+00:00,['tilakrayal'],2025-02-06 17:20:48+00:00,,https://github.com/tensorflow/tensorflow/issues/86378,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2635992537, 'issue_id': 2825135479, 'author': 'tilakrayal', 'body': '@usmonali4,\nI think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected.\nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.\nThe same behavior is not observed on Apple M1, using numpy or in CPU.\nhttps://github.com/tensorflow/tensorflow/issues/58479\n\nThank you!', 'created_at': datetime.datetime(2025, 2, 5, 7, 53, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636473519, 'issue_id': 2825135479, 'author': 'usmonali4', 'body': '@tilakrayal,\nsimilar reasoning as in this comment [#86256/comment](https://github.com/tensorflow/tensorflow/issues/86256#issuecomment-2636423339), but the acceptable tolerance thresholds have been set to atol=1e-2 and rtol=1e-3 considering that the dtypes of results are bfloat16 (16bit floatingpoint types). Glad to hear your thoughts on this; thanks for your time.', 'created_at': datetime.datetime(2025, 2, 5, 11, 30, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638983898, 'issue_id': 2825135479, 'author': 'tilakrayal', 'body': '@usmonali4,\nLooks like the other issues which were created are similar to #86256 . Could you please close the other issues which were raised which helps to track the issue in a better way. Thank you!', 'created_at': datetime.datetime(2025, 2, 6, 6, 54, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639602325, 'issue_id': 2825135479, 'author': 'usmonali4', 'body': ""@tilakrayal,\nif you take for example this report and #86256, I believe that the core issue is different since they are targeting different api's with different types of tensors"", 'created_at': datetime.datetime(2025, 2, 6, 11, 47, 34, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-02-05 07:53:50 UTC): @usmonali4,
I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected.
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.
The same behavior is not observed on Apple M1, using numpy or in CPU.
https://github.com/tensorflow/tensorflow/issues/58479

Thank you!

usmonali4 (Issue Creator) on (2025-02-05 11:30:04 UTC): @tilakrayal,
similar reasoning as in this comment [#86256/comment](https://github.com/tensorflow/tensorflow/issues/86256#issuecomment-2636423339), but the acceptable tolerance thresholds have been set to atol=1e-2 and rtol=1e-3 considering that the dtypes of results are bfloat16 (16bit floatingpoint types). Glad to hear your thoughts on this; thanks for your time.

tilakrayal (Assginee) on (2025-02-06 06:54:58 UTC): @usmonali4,
Looks like the other issues which were created are similar to #86256 . Could you please close the other issues which were raised which helps to track the issue in a better way. Thank you!

usmonali4 (Issue Creator) on (2025-02-06 11:47:34 UTC): @tilakrayal,
if you take for example this report and #86256, I believe that the core issue is different since they are targeting different api's with different types of tensors

"
2825026232,issue,open,,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([
    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],
      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],

     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],
      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],

    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],
      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],

     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],
      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]
], dtype=tf.bfloat16)

y = tf.constant([
    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], 
     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],

    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], 
     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]
], dtype=tf.bfloat16) 

with tf.device('CPU:0'):
    result_cpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_cpu)

with tf.device('GPU:0'):
    result_gpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_gpu)


max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
tf.Tensor(
[[[[[-0.761719 1.14062]
    [-1.125 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.257812]]]


  [[[1.01562 0.726562]
    [-0.193359 3.29688]]

   [[-0.0201416 -0.449219]
    [-0.597656 -0.65625]]]]



 [[[[-1.14062 -0.75]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.386719]]]


  [[[-1.34375 -1.21875]
    [1.14844 3.39062]]

   [[-0.195312 0.388672]
    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)

tf.Tensor(
[[[[[-0.761719 1.14844]
    [-1.13281 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.259766]]]


  [[[1.01562 0.726562]
    [-0.193359 3.3125]]

   [[-0.0201416 -0.451172]
    [-0.597656 -0.660156]]]]



 [[[[-1.14844 -0.753906]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.388672]]]


  [[[-1.35156 -1.22656]
    [1.15625 3.39062]]

   [[-0.196289 0.390625]
    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)


Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",usmonali4,2025-02-01 07:13:36+00:00,['Venkat6871'],2025-02-04 06:01:56+00:00,,https://github.com/tensorflow/tensorflow/issues/86350,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2632952374, 'issue_id': 2825026232, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/f0c3927d522e8eab536c0197dbb46abb/86350_tf_2-18-0-nightly-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/cb98fc9d6a3cdb13b113d38823566f62/86350_tf_nightly-gpu.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 2, 4, 6, 1, 54, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-02-04 06:01:54 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly on both CPU and GPU. Please find [gist1](https://colab.sandbox.google.com/gist/Venkat6871/f0c3927d522e8eab536c0197dbb46abb/86350_tf_2-18-0-nightly-cpu.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/cb98fc9d6a3cdb13b113d38823566f62/86350_tf_nightly-gpu.ipynb) here for your reference.
Thank you!

"
2824111517,issue,open,,Stateful LSTM bug with batch size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. 
Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061

Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Create some random training data
# We'll have sequences of length 5, with 1 feature per time step
sequence_length = 5
num_features = 1
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Reshape y_train to match expected output shape if needed
y_train = y_train.reshape(-1,1)

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units
                               batch_input_shape=(batch_size, sequence_length, num_features),
                               stateful=True,
                               return_sequences=False)) #often false for a final prediction

model.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

### Relevant log output

```shell

```",ceschi,2025-01-31 18:07:07+00:00,['tilakrayal'],2025-02-07 05:24:47+00:00,,https://github.com/tensorflow/tensorflow/issues/86310,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2629495681, 'issue_id': 2824111517, 'author': 'jordanreynold', 'body': ""In TensorFlow 2.18, there have been updates that affect the implementation of stateful LSTM models. The `batch_input_shape` parameter is still valid for specifying a fixed batch size in stateful RNNs. However, it's crucial to ensure that the batch size remains consistent across all batches during training and inference. Additionally, the `model.reset_states()` function is still available and should work as intended.\n\n**Key Considerations:**\n\n1. **Consistent Batch Size:** When using stateful RNNs, it's essential to maintain a consistent batch size across all batches. This consistency ensures that the model's states are correctly managed across batches. If there's a mismatch in batch sizes, it can lead to errors or unexpected behavior.\n\n2. **Resetting States:** The `model.reset_states()` method is used to reset the states of the model. This is particularly useful when you want to clear the model's state between different sequences or epochs. Ensure that this method is called appropriately during training or evaluation to manage the model's state effectively.\n\n**Example Implementation:**\n\nHere's an example of how to implement a stateful LSTM model with a fixed batch size:\n\n```python\nimport numpy as np\nimport tensorflow as tf\n\n# Set a fixed batch size\nbatch_size = 32\n\n# Define sequence length and number of features\nsequence_length = 5\nnum_features = 1\n\n# Create some random training data\nnum_samples = 100  # Total number of samples (must be divisible by batch_size)\n\n# Ensure num_samples is a multiple of batch_size\nnum_samples = (num_samples // batch_size) * batch_size\n\nX_train = np.random.rand(num_samples, sequence_length, num_features)\ny_train = np.random.rand(num_samples, 1)  # Example target values\n\n# Create the stateful LSTM model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.LSTM(\n    units=64,  # Number of LSTM units\n    batch_input_shape=(batch_size, sequence_length, num_features),\n    stateful=True,\n    return_sequences=False\n))\nmodel.add(tf.keras.layers.Dense(units=1))  # Output layer with 1 unit\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nepochs = 10\n\nfor epoch in range(epochs):\n    # Shuffle data indices for each epoch (important for stateful LSTMs)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_train = X_train[indices]\n    y_train = y_train[indices]\n\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False)  # Shuffle must be false\n\n    # Reset states after each epoch (essential for stateful LSTMs)\n    model.reset_states()\n```\n\n**Additional Resources:**\n\n This guide provides detailed information on handling RNNs, including stateful RNNs and managing states across batches.\n\nBy ensuring a consistent batch size and appropriately managing the model's states, you should be able to implement stateful LSTM models effectively in TensorFlow 2.18."", 'created_at': datetime.datetime(2025, 2, 2, 18, 4, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629499685, 'issue_id': 2824111517, 'author': 'ceschi', 'body': ""Hello Jordan, thanks for the reply. I am indeed using TF 2.18 (and Python 3.11.0 on Win11), though if I run your code I get precisely the error I referred to in the first place:\n`ValueError: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (32, 5, 1)}`"", 'created_at': datetime.datetime(2025, 2, 2, 18, 15, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630075846, 'issue_id': 2824111517, 'author': 'tilakrayal', 'body': '@ceschi,\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\n\n```\n!pip install tf-keras\n\nimport tf_keras as keras\n```\n\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/19abef94aadce66d84b6dadb4f3e1a54/untitled2309.ipynb). Take a look at this issue for reference. https://github.com/keras-team/keras/issues/20106\n\nThank you!', 'created_at': datetime.datetime(2025, 2, 3, 6, 24, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630548381, 'issue_id': 2824111517, 'author': 'ceschi', 'body': '@tilakrayal \nHello, thanks for the pointers. I am prototyping on TF 2.18 and Keras 3.8, to then do the training on TF 2.13. If I understand correctly [this post](https://github.com/keras-team/keras/issues/20106#issuecomment-2282654273), an Input layer with `batch_shape` does the trick. Would this work in both versions of TF?\n\nThanks a ton for the help, the documentation is quite confusing currently.', 'created_at': datetime.datetime(2025, 2, 3, 10, 29, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641975547, 'issue_id': 2824111517, 'author': 'tilakrayal', 'body': ""@ceschi,\nAs per above comments, I can sense that you tried the code in tensorflow 2.18, keras 3.8 and then training in TF 2.13. In such a scenario, the code might be having compatible issues with both 2.18 and 2.13 which wouldn't be suggestible.  Tensorflow v2.18 contains Keras3.0 version and tensorflow v2.13 contains keras2.0. where both versions are different.\n\nhttps://keras.io/keras_3/\n\nAnd also the code is working in tf_keras(keras2.0), and provided the error in keras3.0. So, please feel free to raise the issue in Keras-team/keras repo for the further inputs. Thank you!"", 'created_at': datetime.datetime(2025, 2, 7, 5, 24, 40, tzinfo=datetime.timezone.utc)}]","jordanreynold on (2025-02-02 18:04:48 UTC): In TensorFlow 2.18, there have been updates that affect the implementation of stateful LSTM models. The `batch_input_shape` parameter is still valid for specifying a fixed batch size in stateful RNNs. However, it's crucial to ensure that the batch size remains consistent across all batches during training and inference. Additionally, the `model.reset_states()` function is still available and should work as intended.

**Key Considerations:**

1. **Consistent Batch Size:** When using stateful RNNs, it's essential to maintain a consistent batch size across all batches. This consistency ensures that the model's states are correctly managed across batches. If there's a mismatch in batch sizes, it can lead to errors or unexpected behavior.

2. **Resetting States:** The `model.reset_states()` method is used to reset the states of the model. This is particularly useful when you want to clear the model's state between different sequences or epochs. Ensure that this method is called appropriately during training or evaluation to manage the model's state effectively.

**Example Implementation:**

Here's an example of how to implement a stateful LSTM model with a fixed batch size:

```python
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Define sequence length and number of features
sequence_length = 5
num_features = 1

# Create some random training data
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(
    units=64,  # Number of LSTM units
    batch_input_shape=(batch_size, sequence_length, num_features),
    stateful=True,
    return_sequences=False
))
model.add(tf.keras.layers.Dense(units=1))  # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False)  # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

**Additional Resources:**

 This guide provides detailed information on handling RNNs, including stateful RNNs and managing states across batches.

By ensuring a consistent batch size and appropriately managing the model's states, you should be able to implement stateful LSTM models effectively in TensorFlow 2.18.

ceschi (Issue Creator) on (2025-02-02 18:15:10 UTC): Hello Jordan, thanks for the reply. I am indeed using TF 2.18 (and Python 3.11.0 on Win11), though if I run your code I get precisely the error I referred to in the first place:
`ValueError: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (32, 5, 1)}`

tilakrayal (Assginee) on (2025-02-03 06:24:59 UTC): @ceschi,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```
!pip install tf-keras

import tf_keras as keras
```

Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/19abef94aadce66d84b6dadb4f3e1a54/untitled2309.ipynb). Take a look at this issue for reference. https://github.com/keras-team/keras/issues/20106

Thank you!

ceschi (Issue Creator) on (2025-02-03 10:29:44 UTC): @tilakrayal 
Hello, thanks for the pointers. I am prototyping on TF 2.18 and Keras 3.8, to then do the training on TF 2.13. If I understand correctly [this post](https://github.com/keras-team/keras/issues/20106#issuecomment-2282654273), an Input layer with `batch_shape` does the trick. Would this work in both versions of TF?

Thanks a ton for the help, the documentation is quite confusing currently.

tilakrayal (Assginee) on (2025-02-07 05:24:40 UTC): @ceschi,
As per above comments, I can sense that you tried the code in tensorflow 2.18, keras 3.8 and then training in TF 2.13. In such a scenario, the code might be having compatible issues with both 2.18 and 2.13 which wouldn't be suggestible.  Tensorflow v2.18 contains Keras3.0 version and tensorflow v2.13 contains keras2.0. where both versions are different.

https://keras.io/keras_3/

And also the code is working in tf_keras(keras2.0), and provided the error in keras3.0. So, please feel free to raise the issue in Keras-team/keras repo for the further inputs. Thank you!

"
2823699582,issue,open,,DRQ  (Dynamic Range Quantization) - which ops are affected?,"Hi, 

I am performing DRQ (Dynamic Range Quantization) using https://ai.google.dev/edge/litert/models/post_training_quant - how to get more details on which ops will be affected(and what exactly will happen with these ops)?

My understanding that in case of transformers only fully connected layers will be affected, is this correct? What would be the impact on op computations - would computations be happening with int8 (ie both weights and activations)?

Thank you.",Alexey234432,2025-01-31 14:46:34+00:00,['gaikwadrahul8'],2025-02-07 10:58:58+00:00,,https://github.com/tensorflow/tensorflow/issues/86293,"[('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2639835981, 'issue_id': 2823699582, 'author': 'gaikwadrahul8', 'body': ""Hi, @Alexey234432 \nI apologize for the delayed response, As far I know during DRQ the weights are quantized to `int8` but the activations remain in `float32`. This means that the multiplication is int8 * float32. You're correct in your understanding that fully connected layers are major focus in transformers. Transformers heavily rely on fully connected layers (in the Feed Forward Network(FFN) and in the attention mechanism).\n\nThe query, key and value transformations within the attention mechanism often use fully connected layers. The FFN which is typically a multi-layer perceptron (MLP) consists of multiple fully connected layers.Therefore DRQ will primarily affect the fully connected operations in these parts of the transformer architecture.\n\nI would suggest you to please use these tools [model-explorer](https://github.com/google-ai-edge/model-explorer) and [Netron](https://netron.app/) to visualize the architecture of your TensorFlow Lite (TFLite) model including the changes made by Dynamic Range Quantization (DRQ)\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 2, 6, 13, 29, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639890297, 'issue_id': 2823699582, 'author': 'Alexey234432', 'body': ""Thank you for your reply @gaikwadrahul8 \n\nYes, thanks for suggestion - I use these tools and they are extremely useful but to be honest I am still confused (let's concentrate on fully connected ops behaviour for the sake of simplicity) whether actual computations (mat muls) are happening in int8 or fp32.\n\nLooking into docs from https://ai.google.dev/edge/litert/models/post_training_quant\n```The activations are always stored in floating point. For ops that support quantized kernels, the activations are quantized to 8 bits of precision dynamically prior to processing and are de-quantized to float precision after processing. Depending on the model being converted, this can give a speedup over pure floating point computation.```\nthis per my understanding implies that compute is happening in int8.\n\nAlso inference time of DRQ quantized llama3 model vs Float TFLite llama3 model was ~2 times faster (on CPU using TFlite interpreter with 1 cpu only) - this is also a weak evidence of compute using different approach under the hood.\n\nAny chance you could please help me understand what's happening on lower level? Thank you."", 'created_at': datetime.datetime(2025, 2, 6, 13, 52, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640217133, 'issue_id': 2823699582, 'author': 'gaikwadrahul8', 'body': ""Hi, @Alexey234432\n\nYou're correct in your understanding, Dynamic Range Quantization (DRQ) in TensorFlow Lite stores activations in `float32` for range and precision. However, for operations with quantized kernels (like matrix multiplications) activations are dynamically quantized to `int8` immediately before computation.  The actual computation (e.g. matrix multiplication) happens in `int8` precision using both the `int8` quantized weights and the dynamically `int8` quantized activations.  Results are then dequantized back to `float32`.  Thus, the core computations occur in `int8` providing performance benefits despite `float32` activation storage.\n\nThank you for your understanding and cooperation."", 'created_at': datetime.datetime(2025, 2, 6, 15, 53, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640230015, 'issue_id': 2823699582, 'author': 'Alexey234432', 'body': 'Thank you - this is really helpful and detailed answer. Do you know how could I come to this conclusion on my own? ie any links to the relevant inference or quantization code (which I assume will be somewhere inside TFLite source code?) Thanks!', 'created_at': datetime.datetime(2025, 2, 6, 15, 58, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640786985, 'issue_id': 2823699582, 'author': 'gaikwadrahul8', 'body': ""Hi, @Alexey234432 \n\nUnfortunately, the exact source code for Dynamic Range Quantization (DRQ) within TensorFlow Lite is not readily available in a single, easily isolated file. This is because DRQ is implemented across several components of the TensorFlow Lite framework. However, I can point you to the key areas and files where the relevant logic resides \n1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h\n2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc\nThese files contain utility functions for quantization and dequantization operations, including dynamic quantization. They handle the scaling and conversion between `float32` and `int8` representations.\n3. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h\n4. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc\nThese files define the internal quantization structures and functions used within the kernels.\nThe actual DRQ logic is implemented within the individual kernel implementations for the operations that support quantization (e.g. fully connected layers, convolutions). You can find these kernels in the[ tensorflow/lite/kernels](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels) directory.\n\nWithin these kernel files you'll find code that uses the quantization utilities mentioned above to dynamically quantize the activations before performing the computation.\n\nIf I have missed something here please let me know. If you notice any omissions or discrepancies between the official documentation and the source code implementation,  we welcome a pull request (PR).  Our team will review your submission and facilitate its integration provided the changes align with our contribution guidelines.\n\nThank you for your understand and cooperation."", 'created_at': datetime.datetime(2025, 2, 6, 19, 21, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642593072, 'issue_id': 2823699582, 'author': 'Alexey234432', 'body': 'Thank you for your help!', 'created_at': datetime.datetime(2025, 2, 7, 10, 58, 56, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-02-06 13:29:43 UTC): Hi, @Alexey234432 
I apologize for the delayed response, As far I know during DRQ the weights are quantized to `int8` but the activations remain in `float32`. This means that the multiplication is int8 * float32. You're correct in your understanding that fully connected layers are major focus in transformers. Transformers heavily rely on fully connected layers (in the Feed Forward Network(FFN) and in the attention mechanism).

The query, key and value transformations within the attention mechanism often use fully connected layers. The FFN which is typically a multi-layer perceptron (MLP) consists of multiple fully connected layers.Therefore DRQ will primarily affect the fully connected operations in these parts of the transformer architecture.

I would suggest you to please use these tools [model-explorer](https://github.com/google-ai-edge/model-explorer) and [Netron](https://netron.app/) to visualize the architecture of your TensorFlow Lite (TFLite) model including the changes made by Dynamic Range Quantization (DRQ)

Thank you for your cooperation and patience.

Alexey234432 (Issue Creator) on (2025-02-06 13:52:19 UTC): Thank you for your reply @gaikwadrahul8 

Yes, thanks for suggestion - I use these tools and they are extremely useful but to be honest I am still confused (let's concentrate on fully connected ops behaviour for the sake of simplicity) whether actual computations (mat muls) are happening in int8 or fp32.

Looking into docs from https://ai.google.dev/edge/litert/models/post_training_quant
```The activations are always stored in floating point. For ops that support quantized kernels, the activations are quantized to 8 bits of precision dynamically prior to processing and are de-quantized to float precision after processing. Depending on the model being converted, this can give a speedup over pure floating point computation.```
this per my understanding implies that compute is happening in int8.

Also inference time of DRQ quantized llama3 model vs Float TFLite llama3 model was ~2 times faster (on CPU using TFlite interpreter with 1 cpu only) - this is also a weak evidence of compute using different approach under the hood.

Any chance you could please help me understand what's happening on lower level? Thank you.

gaikwadrahul8 (Assginee) on (2025-02-06 15:53:30 UTC): Hi, @Alexey234432

You're correct in your understanding, Dynamic Range Quantization (DRQ) in TensorFlow Lite stores activations in `float32` for range and precision. However, for operations with quantized kernels (like matrix multiplications) activations are dynamically quantized to `int8` immediately before computation.  The actual computation (e.g. matrix multiplication) happens in `int8` precision using both the `int8` quantized weights and the dynamically `int8` quantized activations.  Results are then dequantized back to `float32`.  Thus, the core computations occur in `int8` providing performance benefits despite `float32` activation storage.

Thank you for your understanding and cooperation.

Alexey234432 (Issue Creator) on (2025-02-06 15:58:16 UTC): Thank you - this is really helpful and detailed answer. Do you know how could I come to this conclusion on my own? ie any links to the relevant inference or quantization code (which I assume will be somewhere inside TFLite source code?) Thanks!

gaikwadrahul8 (Assginee) on (2025-02-06 19:21:54 UTC): Hi, @Alexey234432 

Unfortunately, the exact source code for Dynamic Range Quantization (DRQ) within TensorFlow Lite is not readily available in a single, easily isolated file. This is because DRQ is implemented across several components of the TensorFlow Lite framework. However, I can point you to the key areas and files where the relevant logic resides 
1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h
2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc
These files contain utility functions for quantization and dequantization operations, including dynamic quantization. They handle the scaling and conversion between `float32` and `int8` representations.
3. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h
4. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc
These files define the internal quantization structures and functions used within the kernels.
The actual DRQ logic is implemented within the individual kernel implementations for the operations that support quantization (e.g. fully connected layers, convolutions). You can find these kernels in the[ tensorflow/lite/kernels](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels) directory.

Within these kernel files you'll find code that uses the quantization utilities mentioned above to dynamically quantize the activations before performing the computation.

If I have missed something here please let me know. If you notice any omissions or discrepancies between the official documentation and the source code implementation,  we welcome a pull request (PR).  Our team will review your submission and facilitate its integration provided the changes align with our contribution guidelines.

Thank you for your understand and cooperation.

Alexey234432 (Issue Creator) on (2025-02-07 10:58:56 UTC): Thank you for your help!

"
2822685437,issue,open,,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```",usmonali4,2025-01-31 07:40:34+00:00,['tilakrayal'],2025-02-05 11:06:19+00:00,,https://github.com/tensorflow/tensorflow/issues/86256,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2629500831, 'issue_id': 2822685437, 'author': 'rileythompso', 'body': ""The discrepancy you're observing between CPU and GPU outputs when using `tf.image.adjust_hue` is primarily due to differences in floating-point precision and the underlying implementations on these platforms. Such variations are common in numerical computations across different hardware architectures.\n\n**Potential Causes:**\n\n1. **Floating-Point Precision:** CPUs and GPUs may handle floating-point arithmetic differently, leading to minor discrepancies in results.\n\n2. **Implementation Differences:** The algorithms or optimizations used for image processing operations can vary between CPU and GPU implementations, contributing to inconsistent outputs.\n\n**Recommended Solutions:**\n\n1. **Set a Global Seed:** Ensure reproducibility by setting a global random seed.\n\n   ```python\n   import tensorflow as tf\n   tf.random.set_seed(42)\n   ```\n\n2. **Use CPU Consistently:** If consistency is crucial, consider performing the hue adjustment exclusively on the CPU.\n\n   ```python\n   with tf.device('/CPU:0'):\n       adjusted_image = tf.image.adjust_hue(image, delta)\n   ```\n\n3. **Adjust Tolerance Levels:** When comparing CPU and GPU results, allow for minor differences by setting appropriate tolerance levels.\n\n   ```python\n   is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)\n   ```\n\n4. **Preprocess Data:** Normalize or preprocess your images to minimize the impact of precision differences.\n\n   ```python\n   image = tf.image.convert_image_dtype(image, tf.float32)\n   ```\n\n**Additional Considerations:**\n\n- **Library Versions:** Ensure that your TensorFlow and CUDA versions are compatible and up-to-date.\n\n- **Hardware Variations:** Different GPU models may exhibit varying behaviors due to architectural differences.\n\nBy implementing these strategies, you can mitigate the inconsistencies between CPU and GPU computations in TensorFlow."", 'created_at': datetime.datetime(2025, 2, 2, 18, 18, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635990287, 'issue_id': 2822685437, 'author': 'tilakrayal', 'body': '@usmonali4,\nI think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected. \nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.\nThe same behavior is not observed on Apple M1, using numpy or in CPU.\nhttps://github.com/tensorflow/tensorflow/issues/58479\n\n\nThank you!', 'created_at': datetime.datetime(2025, 2, 5, 7, 52, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636423339, 'issue_id': 2822685437, 'author': 'usmonali4', 'body': '@tilakrayal, \nI also suspect the issue to be GPU specific, and I understand that as api calculations vary across devices, minor differences between results are expected. However, each data type should have an acceptable tolerance range. I.e deviations beyond this range may indicate inadequate accuracy control on certain devices. e.g, current report shows a difference greater than 0.34 for float32 values, which is excessive given float32 precision. Even when considering relaxed tolerance thresholds as atol=1e-1 and rtol=1e-1 the difference would fall outside the range. Glad to hear your thoughts on this; thanks for your time.', 'created_at': datetime.datetime(2025, 2, 5, 11, 6, 16, tzinfo=datetime.timezone.utc)}]","rileythompso on (2025-02-02 18:18:40 UTC): The discrepancy you're observing between CPU and GPU outputs when using `tf.image.adjust_hue` is primarily due to differences in floating-point precision and the underlying implementations on these platforms. Such variations are common in numerical computations across different hardware architectures.

**Potential Causes:**

1. **Floating-Point Precision:** CPUs and GPUs may handle floating-point arithmetic differently, leading to minor discrepancies in results.

2. **Implementation Differences:** The algorithms or optimizations used for image processing operations can vary between CPU and GPU implementations, contributing to inconsistent outputs.

**Recommended Solutions:**

1. **Set a Global Seed:** Ensure reproducibility by setting a global random seed.

   ```python
   import tensorflow as tf
   tf.random.set_seed(42)
   ```

2. **Use CPU Consistently:** If consistency is crucial, consider performing the hue adjustment exclusively on the CPU.

   ```python
   with tf.device('/CPU:0'):
       adjusted_image = tf.image.adjust_hue(image, delta)
   ```

3. **Adjust Tolerance Levels:** When comparing CPU and GPU results, allow for minor differences by setting appropriate tolerance levels.

   ```python
   is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)
   ```

4. **Preprocess Data:** Normalize or preprocess your images to minimize the impact of precision differences.

   ```python
   image = tf.image.convert_image_dtype(image, tf.float32)
   ```

**Additional Considerations:**

- **Library Versions:** Ensure that your TensorFlow and CUDA versions are compatible and up-to-date.

- **Hardware Variations:** Different GPU models may exhibit varying behaviors due to architectural differences.

By implementing these strategies, you can mitigate the inconsistencies between CPU and GPU computations in TensorFlow.

tilakrayal (Assginee) on (2025-02-05 07:52:24 UTC): @usmonali4,
I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected. 
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors.
The same behavior is not observed on Apple M1, using numpy or in CPU.
https://github.com/tensorflow/tensorflow/issues/58479


Thank you!

usmonali4 (Issue Creator) on (2025-02-05 11:06:16 UTC): @tilakrayal, 
I also suspect the issue to be GPU specific, and I understand that as api calculations vary across devices, minor differences between results are expected. However, each data type should have an acceptable tolerance range. I.e deviations beyond this range may indicate inadequate accuracy control on certain devices. e.g, current report shows a difference greater than 0.34 for float32 values, which is excessive given float32 precision. Even when considering relaxed tolerance thresholds as atol=1e-1 and rtol=1e-1 the difference would fall outside the range. Glad to hear your thoughts on this; thanks for your time.

"
2819722564,issue,closed,completed,Tensorflow_datasets - image_classification - cats_vs_dogs.py file,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow-2.18.0

### Custom code

No

### OS platform and distribution

Edition: Windows 10 Pro, Version: 22H2, OS Build: 19045.5371

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I used a dataset named cats_vs_dogs for studying and training a model, MobileNet v2. Upon loading this dataset by using tensorflow_datasets.load() function which does the fetching too, the first image from arhive that should have been extracted throws an error that it cannot be found. I solved easily `_generate_examples()` from cats_vs_dogs.py by eliminating the context manager with ZipFile object and the one after. I suppose that attaching one ZipFile object to a BytesIO object does work in the context manger scope, but it crashes if another ZipFile object wraps it outside the context manager. I think that one object ZipFile should be used. I managed to do what I wanted with a simple modification, I will post the code separately. In the log some text is in Romanian in paths.

### Standalone code to reproduce the issue

```shell
def _generate_examples(self, archive):
    """"""Generate Cats vs Dogs images and labels given a directory path.""""""
    num_skipped = 0
    for fname, fobj in archive:
      res = _NAME_RE.match(fname)
      if not res:  # README file, ...
        continue
      label = res.group(1).lower()
      if tf.compat.as_bytes(""JFIF"") not in fobj.peek(10):
        num_skipped += 1
        continue

      # Some images caused 'Corrupt JPEG data...' messages during training or
      # any other iteration recoding them once fixes the issue (discussion:
      # https://github.com/tensorflow/datasets/issues/2188).
      # Those messages are now displayed when generating the dataset instead.
      img_data = fobj.read()
      img_tensor = tf.image.decode_image(img_data)
      img_recoded = tf.io.encode_jpeg(img_tensor)

      # Converting the recoded image back into a zip file container.
      #buffer = io.BytesIO()
      #with zipfile.ZipFile(buffer, ""w"") as new_zip:
      #  new_zip.writestr(fname, img_recoded.numpy())
      #new_zip = zipfile.ZipFile(buffer, ""w"")
      #new_zip.writestr(fname, img_recoded.numpy())
      #new_fobj = zipfile.ZipFile(buffer).open(fname)

      record = {
          ""image"": img_recoded.numpy(), #new_fobj,
          ""image/filename"": fname,
          ""label"": label,
      }
      yield fname, record

    if num_skipped != _NUM_CORRUPT_IMAGES:
      raise ValueError(
          ""Expected %d corrupt images, but found %d""
          % (_NUM_CORRUPT_IMAGES, num_skipped)
      )
    logging.warning(""%d images were corrupted and were skipped"", num_skipped)
```

### Relevant log output

```shell
(setups) E:\Facultate_etti\An_1_csi\ElemAI\setups>py ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py"" 
2025-01-30 03:49:43.258005: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-30 03:49:44.359191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\stane\tensorflow_datasets\cats_vs_dogs\4.0.1...
2025-01-30 03:50:03.247352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py"", line 542, in <module>
    train_ds, validation_ds, test_ds = tfds.load(
                                       ^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\load.py"", line 661, in load
    _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare
    self._download_and_prepare(
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare
    split_infos = self._generate_splits(dl_manager, download_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits
    future = split_builder.submit_split_generation(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation
    return self._build_from_generator(**build_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator
    for key, example in utils.tqdm(
  File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\site-packages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 119, in _generate_examples
    new_fobj = zipfile.ZipFile(buffer).open(fname)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1546, in open
    zinfo = self.getinfo(name)
            ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1475, in getinfo
    raise KeyError(
KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive""
```",bog739,2025-01-30 01:53:39+00:00,['Venkat6871'],2025-01-31 17:44:20+00:00,2025-01-31 17:44:17+00:00,https://github.com/tensorflow/tensorflow/issues/86177,"[('type:bug', 'Bug'), ('awaiting PR merge', 'awaiting PR merge'), ('TF 2.18', '')]","[{'comment_id': 2626870283, 'issue_id': 2819722564, 'author': 'Venkat6871', 'body': 'Hi **@bog739** ,\nWelcome to TensorFlow. This is a known issue, and a fix has already been merged. Once a new release is available, the problem should be resolved. I am providing a link to a similar issue hereplease follow it for further updates:\n#84104\nThank you!', 'created_at': datetime.datetime(2025, 1, 31, 10, 29, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627894017, 'issue_id': 2819722564, 'author': 'bog739', 'body': 'Hi @Venkat6871,\nThank you for suggestion, I will keep an eye on newer releases!', 'created_at': datetime.datetime(2025, 1, 31, 17, 44, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627894139, 'issue_id': 2819722564, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86177"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86177"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 17, 44, 19, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-31 10:29:27 UTC): Hi **@bog739** ,
Welcome to TensorFlow. This is a known issue, and a fix has already been merged. Once a new release is available, the problem should be resolved. I am providing a link to a similar issue hereplease follow it for further updates:
#84104
Thank you!

bog739 (Issue Creator) on (2025-01-31 17:44:17 UTC): Hi @Venkat6871,
Thank you for suggestion, I will keep an eye on newer releases!

google-ml-butler[bot] on (2025-01-31 17:44:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86177"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86177"">No</a>

"
2819074968,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.18.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

Python version: 3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an error when trying to import TensorFlow. The error message is:
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

Multiple times reinstalled it

Downgraded my Python, still no sucess.

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```

### Relevant log output

```shell

```",zainZayam,2025-01-29 19:28:01+00:00,['tilakrayal'],2025-01-30 14:03:43+00:00,2025-01-30 14:03:39+00:00,https://github.com/tensorflow/tensorflow/issues/86111,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.18', '')]","[{'comment_id': 2623960208, 'issue_id': 2819074968, 'author': 'tilakrayal', 'body': '@zainZayam,\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\n```\n- You need to install the MSVC 2019 redistributable\n- Your CPU does not support AVX2 instructions\n- Your CPU/Python is on 32 bits\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\n```\nAlso kindly provide the environment details and the steps followed to install the tensorflow.\n#61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\nThank you!', 'created_at': datetime.datetime(2025, 1, 30, 9, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624604904, 'issue_id': 2819074968, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86111"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86111"">No</a>', 'created_at': datetime.datetime(2025, 1, 30, 14, 3, 41, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-30 09:29:00 UTC): @zainZayam,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:
```
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```
Also kindly provide the environment details and the steps followed to install the tensorflow.
#61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584
Thank you!

google-ml-butler[bot] on (2025-01-30 14:03:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86111"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86111"">No</a>

"
2818921492,issue,open,,"Mac Air M2, TfLiteGpuDelegate, Cpp, C++ Building from source","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19.0

### Custom code

No

### OS platform and distribution

Macbook air m2 Macos Sequia 

### Mobile device

_No response_

### Python version

3.12

### Bazel version

8.0.1

### GCC/compiler version

Apple clang version 16.0.0 (clang-1600.0.26.6)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple-designed integrated GPU 8GB

### Current behavior?

I want to be able to inference using tflite gpu delegate on m2. First of all, I wonder if this is possible. [https://www.tensorflow.org/install/source?hl=tr#install_gpu_support_optional_linux_only](url) says tf do not have officail gpu support and apple give us metal plugin.

My ultimate goal is to first debug on macbook and then use tflite gpu delegate for inference on iphone 6s.

I am writing an sdk for this. But I could not build it for macbook.

here is the build script i use for.

```sh

bazel clean --expunge


bazel build --config=macos_arm64 \
    -c opt \
    --config=nogcp  --config=nonccl \
    --repo_env=HERMETIC_PYTHON_VERSION=3.12 \
    --define tflite_with_gpu=true \
    --define tflite_with_metal=true \
    --cxxopt=-std=c++17 \
    --cxxopt=-stdlib=libc++ \
    --host_cxxopt=-std=c++17 \
    --linkopt=-stdlib=libc++ \
    --copt=-std=c++17 \
    --copt=-stdlib=libc++ \
    //tensorflow/lite/delegates/gpu:metal_delegate \
    //tensorflow/lite:libtensorflowlite.dylib \
    //tensorflow/lite/delegates/gpu:metal_delegate.h

```


i have added logs this happend because of  --copt=-std=c++17 \ but when i remove it then i get

```

ERROR: /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/tensorflow/lite/delegates/gpu/metal/BUILD:68:13: Compiling tensorflow/lite/delegates/gpu/metal/common.mm failed: (Exit 1): wrapped_clang_pp failed: error executing command (from target //tensorflow/lite/delegates/gpu/metal:common) external/local_config_cc/wrapped_clang_pp -target arm64-apple-macosx11.0 '-stdlib=libc++' '-std=gnu++11' '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign ... (remaining 50 arguments skipped)
In file included from tensorflow/lite/delegates/gpu/metal/common.mm:16:
In file included from ./tensorflow/lite/delegates/gpu/metal/common.h:25:
In file included from ./tensorflow/lite/delegates/gpu/common/status.h:19:
In file included from external/com_google_absl/absl/status/status.h:58:
In file included from external/com_google_absl/absl/functional/function_ref.h:53:
In file included from external/com_google_absl/absl/base/attributes.h:37:
In file included from external/com_google_absl/absl/base/config.h:86:
external/com_google_absl/absl/base/policy_checks.h:79:2: error: ""C++ versions less than C++14 are not supported.""
   79 | #error ""C++ versions less than C++14 are not supported.""
      |  ^
In file included from tensorflow/lite/delegates/gpu/metal/common.mm:16:
In file included from ./tensorflow/lite/delegates/gpu/metal/common.h:25:
In file included from ./tensorflow/lite/delegates/gpu/common/status.h:19:
In file included from external/com_google_absl/absl/status/status.h:59:
In file included from external/com_google_absl/absl/status/internal/status_internal.h:22:
In file included from external/com_google_absl/absl/container/inlined_vector.h:53:
In file included from external/com_google_absl/absl/container/internal/inlined_vector.h:30:
external/com_google_absl/absl/container/internal/compressed_tuple.h:77:16: error: no member named 'is_final' in namespace 'std'
   77 |          !std::is_final<T>::value &&
```


this error. Even though everything is perfect i ended up with linking error. And i begin to think it is impossible to use mac for development.
Anyone have any idea where is wrong and how to solve


[ 73%] Linking CXX shared library libFaceBlurCameraSDK.dylib
Undefined symbols for architecture arm64:
  ""_TfLiteCoreMlDelegateCreate"", referenced from:
      faceblur::TFLiteDetector::applyDelegate(faceblur::TFLiteBackend) in tflite_detector.cpp.o
  ""_TfLiteCoreMlDelegateDelete"", referenced from:
      faceblur::TFLiteDetector::cleanupDelegate() in tflite_detector.cpp.o
  ""_TfLiteGpuDelegateOptionsV2Default"", referenced from:
      faceblur::TFLiteDetector::applyDelegate(faceblur::TFLiteBackend) in tflite_detector.cpp.o
  ""_TfLiteGpuDelegateV2Create"", referenced from:
      faceblur::TFLiteDetector::applyDelegate(faceblur::TFLiteBackend) in tflite_detector.cpp.o
  ""_TfLiteGpuDelegateV2Delete"", referenced from:
      faceblur::TFLiteDetector::cleanupDelegate() in tflite_detector.cpp.o
ld: symbol(s) not found for architecture arm64
c++: error: linker command failed with exit code 1 (use -v to see invocation)


### Standalone code to reproduce the issue

```shell
bazel build --config=macos_arm64 \
    -c opt \
    --config=nogcp  --config=nonccl \
    --repo_env=HERMETIC_PYTHON_VERSION=3.12 \
    --define tflite_with_gpu=true \
    --define tflite_with_metal=true \
    --cxxopt=-std=c++17 \
    --cxxopt=-stdlib=libc++ \
    --host_cxxopt=-std=c++17 \
    --linkopt=-stdlib=libc++ \
    --copt=-std=c++17 \
    --copt=-stdlib=libc++ \
    //tensorflow/lite/delegates/gpu:metal_delegate \
    //tensorflow/lite:libtensorflowlite.dylib \
    //tensorflow/lite/delegates/gpu:metal_delegate.h
```

### Relevant log output

```shell
(face)   FaceBlurCameraSDK git:(main)  ./tensorflow_setup.sh
Installing dependencies...
Warning: Treating cmake as a formula. For the cask, use homebrew/cask/cmake or specify the `--cask` flag. To silence this message, use the `--formula` flag.
Warning: cmake 3.31.5 is already installed and up-to-date.
To reinstall 3.31.5, run:
  brew reinstall cmake
Warning: python@3.12 3.12.8 is already installed and up-to-date.
To reinstall 3.12.8, run:
  brew reinstall python@3.12
Warning: bazelisk 1.25.0 is already installed and up-to-date.
To reinstall 1.25.0, run:
  brew reinstall bazelisk
Updating TensorFlow repository...
remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (2/2), done.
Unpacking objects: 100% (9/9), 1.45 KiB | 185.00 KiB/s, done.
remote: Total 9 (delta 7), reused 9 (delta 7), pack-reused 0 (from 0)
From https://github.com/tensorflow/tensorflow
 * [new branch]              exported_pr_714978844 -> origin/exported_pr_714978844
Already up to date.
Initializing and updating TensorFlow submodules...
Building TensorFlow Lite for macOS (ARM64)...
INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=88
INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=88
INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos_arm64 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --cpu=darwin_arm64 --macos_minimum_os=11.0
INFO: Found applicable config definition build:nogcp in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nonccl in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.12""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_12.txt""
=====================================
INFO: Analyzed 3 targets (157 packages loaded, 6973 targets configured).
INFO: Found 3 targets...
ERROR: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/fft2d/BUILD.bazel:27:11: Compiling fftsg2d.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @fft2d//:fft2d) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 31 arguments skipped)
error: invalid argument '-std=c++17' not allowed with 'C'
Error in child process '/usr/bin/xcrun'. 1
INFO: Elapsed time: 69.199s, Critical Path: 2.88s
INFO: 316 processes: 296 internal, 20 local.
FAILED: Build did NOT complete successfully
Locating macOS build output...
INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.12""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_12.txt""
=====================================
Could not find macOS build output. Searching for alternatives...
/private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/lite/libtensorflowlite.dylib.runfiles/org_tensorflow/tensorflow/lite/libtensorflowlite.dylib
Error: Failed to locate macOS build output.
```",rsemihkoca,2025-01-29 18:21:23+00:00,['gaikwadrahul8'],2025-02-06 14:39:58+00:00,,https://github.com/tensorflow/tensorflow/issues/86075,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2624949143, 'issue_id': 2818921492, 'author': 'rsemihkoca', 'body': '@Venkat6871', 'created_at': datetime.datetime(2025, 1, 30, 16, 18, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640011191, 'issue_id': 2818921492, 'author': 'gaikwadrahul8', 'body': 'Hi, @rsemihkoca \nI apologize for the delayed response, As far I know it is possible to use the TFLite GPU delegate on macOS with M2 chip.  You are correct that TensorFlow itself doesn\'t have official GPU support in the same way it does on Linux with CUDA.  However, Apple provides the Metal framework and the `tensorflow-metal` plugin (if needed building from source) allows TensorFlow Lite to leverage Metal for GPU acceleration please refer this official documentation of [Get started with tensorflow-metal](https://developer.apple.com/metal/tensorflow-plugin/) so I believe you\'re using `tensorflow-metal `plugin\n\nCould you please give it try with below command and see is it working as expected or not ? if not please help me with complete steps which you followed before encountering the mentioned errors to investigate this issue further from our end ?\n\n```\nbazel clean --expunge\n\nbazel build --config=macos_arm64 \\\n    -c opt \\\n    --config=nogcp --config=nonccl \\\n    --repo_env=HERMETIC_PYTHON_VERSION=3.12 \\  # Or your preferred Python version\n    --cxxopt=""-std=c++17"" \\\n    --host_cxxopt=""-std=c++17"" \\\n    --linkopt=""-stdlib=libc++"" \\ # Only if you get linker errors related to stdlib\n    //tensorflow/lite:libtensorflowlite.dylib\n```\n\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 2, 6, 14, 38, 48, tzinfo=datetime.timezone.utc)}]","rsemihkoca (Issue Creator) on (2025-01-30 16:18:53 UTC): @Venkat6871

gaikwadrahul8 (Assginee) on (2025-02-06 14:38:48 UTC): Hi, @rsemihkoca 
I apologize for the delayed response, As far I know it is possible to use the TFLite GPU delegate on macOS with M2 chip.  You are correct that TensorFlow itself doesn't have official GPU support in the same way it does on Linux with CUDA.  However, Apple provides the Metal framework and the `tensorflow-metal` plugin (if needed building from source) allows TensorFlow Lite to leverage Metal for GPU acceleration please refer this official documentation of [Get started with tensorflow-metal](https://developer.apple.com/metal/tensorflow-plugin/) so I believe you're using `tensorflow-metal `plugin

Could you please give it try with below command and see is it working as expected or not ? if not please help me with complete steps which you followed before encountering the mentioned errors to investigate this issue further from our end ?

```
bazel clean --expunge

bazel build --config=macos_arm64 \
    -c opt \
    --config=nogcp --config=nonccl \
    --repo_env=HERMETIC_PYTHON_VERSION=3.12 \  # Or your preferred Python version
    --cxxopt=""-std=c++17"" \
    --host_cxxopt=""-std=c++17"" \
    --linkopt=""-stdlib=libc++"" \ # Only if you get linker errors related to stdlib
    //tensorflow/lite:libtensorflowlite.dylib
```

Thank you for your cooperation and patience.

"
2818039112,issue,open,,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:

1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)

2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc):

```
absl::Status CreateClBuffer(cl_context context, int size_in_bytes,
                            bool read_only, void* data, cl_mem* result) 
```

where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).

3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.

The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).

### Standalone code to reproduce the issue

```shell
Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue.
```

### Relevant log output

```shell
ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size
```",space2,2025-01-29 12:11:28+00:00,['gaikwadrahul8'],2025-01-30 04:04:16+00:00,,https://github.com/tensorflow/tensorflow/issues/86048,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]",[],
2817980731,issue,open,,Building static c library for xtensa DSP processors,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

ubuntu 20.04

### Mobile device

ubuntu 20.04

### Python version

3.10.10

### Bazel version

3.1.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

--

### GPU model and memory

---

### Current behavior?

could someone suggest on how to build c libraries(.so or .a) for xtensa based processors or DSPs for linking with the applications ??

### Standalone code to reproduce the issue

```shell
compiler --xtensa-core=xyz -Wall -pedantic -Werror -Wextra -Wno-unused-parameter *.c -L xtensa-specific-libs -I /tensorflow/third_party/xla/ -I /tensorflow/ -I /tensorflow/third_party/xla/third_party/tsl/ -nostdlib -I /tensorflow/lite/ -I /tensorflow/lite/core/api/ -I /tensorflow/ -lc -lxmem
```

### Relevant log output

```shell
: dangerous relocation: cannot decode instruction opcode
when building library.

 undefined reference to `TF_NewGraph'...
when try running the application without tf library
```",enthusiast666,2025-01-29 11:46:06+00:00,['gaikwadrahul8'],2025-01-31 04:09:14+00:00,,https://github.com/tensorflow/tensorflow/issues/86045,"[('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.8', '')]",[],
2817973349,issue,closed,completed,Tensorflow lite cross compilation to aarch64 failing,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

No

### OS platform and distribution

Host: x86 Ubuntu 20.04

### Mobile device

target: aarch64

### Python version

N/A

### Bazel version

N/A

### GCC/compiler version

gcc 9

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

Build fails due to an exec format error with `protoc`

```
[ 97%] Built target protoc
/usr/bin/make  -f profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make profiling/proto/CMakeFiles/profiling_info_proto.dir/depend
make[2]: Entering directory '/home/root/build64'
[ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h
cd /home/root/build64/profiling/proto && protoc --cpp_out=/home/root/build64/profiling/proto --proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto
/bin/sh: 1: protoc: Exec format error
make[2]: *** [profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make:75: profiling/proto/profiling_info.pb.cc] Error 2
make[2]: Leaving directory '/home/root/build64'
make[1]: *** [CMakeFiles/Makefile2:7385: profiling/proto/CMakeFiles/profiling_info_proto.dir/all] Error 2
make[1]: Leaving directory '/home/root/build64'
make: *** [Makefile:136: all] Error 2
```
The only possible explanation for this seems to be that that the cmake build process is attempting to use `protoc` for this step 

```
[ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h
cd /home/root/build64/profiling/proto && protoc --cpp_out=/home/root/build64/profiling/proto --proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto
```
But since `protoc` was built using the cross compiler tool chain it is meant for aarch64 while my host machine is trying to run it.

This is likely a bug with the cross compilation process and in that case, please suggest a fix. I am not sure to what extent `protoc` is used in the build so any fix I would make cannot be completely correct. 

Edit: I installed the [x86 version](https://github.com/protocolbuffers/protobuf/releases/tag/v21.12) for `protoc` separately, specifically the version 3.21.x which is the exact version that the tflite build process creates (3.21.9) and I am still facing version incompatibility issues 

```
/home/root/build64/profiling/proto/profiling_info.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
   17 | #error This file was generated by an older version of protoc which is
      |  ^~~~~
/home/root/build64/profiling/proto/profiling_info.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
   18 | #error incompatible with your Protocol Buffer headers.  Please
      |  ^~~~~
/home/root/build64/profiling/proto/profiling_info.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
   19 | #error regenerate this file with a newer version of protoc.
      |  ^~~~~
```

### Standalone code to reproduce the issue

```shell
`cmake -DCMAKE_TOOLCHAIN_FILE=/opt/cross_toolchain/aarch64-gnu-9.toolchain.cmake -DTFLITE_ENABLE_GPU=ON -DTFLITE_ENABLE_NNAPI=ON -DXNNPACK_ENABLE_ARM_BF16=OFF -DXNNPACK_ENABLE_ARM_I8MM=OFF -DCMAKE_CXX_FL<CXX_FLAGS=""${CMAKE_CXX_FLAGS} -std=c++11"" ../tensorflow/tensorflow/lite`

Using the default Cmake build process.
```

### Relevant log output

```shell

```",AD-lite24,2025-01-29 11:42:26+00:00,['gaikwadrahul8'],2025-01-31 11:47:26+00:00,2025-01-31 11:47:23+00:00,https://github.com/tensorflow/tensorflow/issues/86044,"[('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2627016569, 'issue_id': 2817973349, 'author': 'AD-lite24', 'body': 'Figured it out. Apparently protobuf 3.21.12 is significantly different from 3.21.9 and there is no release for 3.21.9 so need to build it from source. Still there is a bug with with the cross compilation process that should be resolved. I will try to create a PR if I end up writing a seamless solution, but for now the workaround of manually building protobuf works. Closing the issue for now but the issue has not been resolved.', 'created_at': datetime.datetime(2025, 1, 31, 11, 47, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627016634, 'issue_id': 2817973349, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86044"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86044"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 11, 47, 24, tzinfo=datetime.timezone.utc)}]","AD-lite24 (Issue Creator) on (2025-01-31 11:47:23 UTC): Figured it out. Apparently protobuf 3.21.12 is significantly different from 3.21.9 and there is no release for 3.21.9 so need to build it from source. Still there is a bug with with the cross compilation process that should be resolved. I will try to create a PR if I end up writing a seamless solution, but for now the workaround of manually building protobuf works. Closing the issue for now but the issue has not been resolved.

google-ml-butler[bot] on (2025-01-31 11:47:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86044"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86044"">No</a>

"
2817969316,issue,closed,completed,Input pipeline with RaggedTensor no longer working in +2.16 - No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.11 & 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In version 2.15 and earlier i was able to have a batched `tf.data.Dataset` with `tf.RaggedTensor` as input for `model.fit` and `model.predict`, with `tf.keras.layers.Resizing` as the first layer of the model.

This is no longer works in +2.16 and Keras 3 (Edit: not Keras 2).

The error log contains `RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible`, which implies a missing implementation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

ds = tf.data.Dataset.from_tensor_slices(range(10)) \
    .map(lambda x: (
        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),
        0,
    )) \
    .batch(batch_size=4) \
    .prefetch(tf.data.AUTOTUNE)

model = tf.keras.Sequential([
    tf.keras.layers.Resizing(height=10, width=10),
    tf.keras.layers.GlobalMaxPool2D(),
    tf.keras.layers.Softmax(),
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())

model.fit(ds)
```

### Relevant log output

```shell
I0000 00:00:1738149672.657535   10590 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7070 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:0b:00.0, compute capability: 6.1
[...]/.venv/lib/python3.12/site-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
  warnings.warn(
[...]/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.
  warnings.warn(""The model does not have any trainable weights."")
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1738149672.931281   10665 service.cc:148] XLA service 0x79b50c0038a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1738149672.931302   10665 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1
2025-01-29 12:21:12.940706: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at if_op.cc:291 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}
The op is created at: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize
	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize

2025-01-29 12:21:12.940972: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}
The op is created at: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize
	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize

	 [[sequential_1/resizing_1/RaggedResizeImages/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
2025-01-29 12:21:12.941007: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}
The op is created at: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize
	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize

	 [[sequential_1/resizing_1/RaggedResizeImages/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]]
Traceback (most recent call last):
  File ""[...]/test.py"", line 43, in <module>
    model.fit(ds)
  File ""[...]/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""[...]/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):
<stack traces unavailable>
Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):
<stack traces unavailable>
Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}
The op is created at: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize
	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""test.py"", line 43, in <module>
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 213, in call
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 182, in call
File "".venv/lib/python3.12/site-packages/keras/src/ops/function.py"", line 171, in _run_through_graph
File "".venv/lib/python3.12/site-packages/keras/src/models/functional.py"", line 637, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 908, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/ops/operation.py"", line 46, in __call__
File "".venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call
File "".venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images
File "".venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/image.py"", line 293, in resize

	 [[sequential_1/resizing_1/RaggedResizeImages/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_298]
```",DamarXCV,2025-01-29 11:40:28+00:00,['Venkat6871'],2025-01-31 10:56:47+00:00,2025-01-31 10:56:44+00:00,https://github.com/tensorflow/tensorflow/issues/86043,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2626564296, 'issue_id': 2817969316, 'author': 'Venkat6871', 'body': 'Hi **@DamarXCV** ,\nApologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras installation. Starting from TensorFlow version 2.16.0, it defaults to Keras 3. If you want to use Keras 2, you need to install it manually. This is also mentioned in the documentation. I installed everything as required, and it is working fine for me. Here, I am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/6a5dc96e6c504a4337120b540fb1f7e9/86043_tf_2-18-0-v.ipynb) for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 31, 8, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626863355, 'issue_id': 2817969316, 'author': 'DamarXCV', 'body': '@Venkat6871 Thank you for your reply.\n\nSorry, i meant Keras 3 not Keras 2, i corrected it in my original post.\n\nIf i console log the version it prints `3.8.0` for Keras with the following code\n\n```\nimport tensorflow as tf\n\nprint(tf.__version__)\nprint(tf.keras.__version__)\n\nds = tf.data.Dataset.from_tensor_slices(range(10)) \\\n    .map(lambda x: (\n        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),\n        0,\n    )) \\\n    .batch(batch_size=4) \\\n    .prefetch(tf.data.AUTOTUNE)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Resizing(height=10, width=10),\n    tf.keras.layers.GlobalMaxPool2D(),\n    tf.keras.layers.Softmax(),\n])\n\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())\n\nmodel.fit(ds)\n```\n\nI know i should use the standalone keras import, as documented in the migration guide, but even i do so it still crashes with the error from my initial post\n```\nimport keras\nimport tensorflow as tf\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nds = tf.data.Dataset.from_tensor_slices(range(10)) \\\n    .map(lambda x: (\n        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),\n        0,\n    )) \\\n    .batch(batch_size=4) \\\n    .prefetch(tf.data.AUTOTUNE)\n\nmodel = keras.Sequential([\n    keras.layers.Resizing(height=10, width=10),\n    keras.layers.GlobalMaxPool2D(),\n    keras.layers.Softmax(),\n])\n\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy())\n\nmodel.fit(ds)\n```\nThe printed versions are in both code snipets `2.18.0` and `3.8.0`.\n\nIf i install tf-keras (aka Keras 2) with `pip install tf-keras` the following code works\n```\nimport tf_keras\nimport tensorflow as tf\n\nprint(tf.__version__)\nprint(tf_keras.__version__)\n\nds = tf.data.Dataset.from_tensor_slices(range(10)) \\\n    .map(lambda x: (\n        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),\n        0,\n    )) \\\n    .batch(batch_size=4) \\\n    .prefetch(tf.data.AUTOTUNE)\n\nmodel = tf_keras.Sequential([\n    tf_keras.layers.Resizing(height=10, width=10),\n    tf_keras.layers.GlobalMaxPool2D(),\n    tf_keras.layers.Softmax(),\n])\n\nmodel.compile(loss=tf_keras.losses.SparseCategoricalCrossentropy())\n\nmodel.fit(ds)\n```\n\nBut i would like to use Keras 3 and not the outdated Keras 2, which seems to not support `tf.RaggedTensor` as input for `keras.layers.Resizing`', 'created_at': datetime.datetime(2025, 1, 31, 10, 25, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626925350, 'issue_id': 2817969316, 'author': 'DamarXCV', 'body': 'I found keras-team/keras#18467 which mentions \n\n> No RaggedTensor support. We may add it back later.\n\nI guess that means that my input pipeline is not supported in Keras 3 for now.', 'created_at': datetime.datetime(2025, 1, 31, 10, 56, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626925404, 'issue_id': 2817969316, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86043"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86043"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 10, 56, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-31 08:09:15 UTC): Hi **@DamarXCV** ,
Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras installation. Starting from TensorFlow version 2.16.0, it defaults to Keras 3. If you want to use Keras 2, you need to install it manually. This is also mentioned in the documentation. I installed everything as required, and it is working fine for me. Here, I am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/6a5dc96e6c504a4337120b540fb1f7e9/86043_tf_2-18-0-v.ipynb) for your reference.
Thank you!

DamarXCV (Issue Creator) on (2025-01-31 10:25:56 UTC): @Venkat6871 Thank you for your reply.

Sorry, i meant Keras 3 not Keras 2, i corrected it in my original post.

If i console log the version it prints `3.8.0` for Keras with the following code

```
import tensorflow as tf

print(tf.__version__)
print(tf.keras.__version__)

ds = tf.data.Dataset.from_tensor_slices(range(10)) \
    .map(lambda x: (
        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),
        0,
    )) \
    .batch(batch_size=4) \
    .prefetch(tf.data.AUTOTUNE)

model = tf.keras.Sequential([
    tf.keras.layers.Resizing(height=10, width=10),
    tf.keras.layers.GlobalMaxPool2D(),
    tf.keras.layers.Softmax(),
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())

model.fit(ds)
```

I know i should use the standalone keras import, as documented in the migration guide, but even i do so it still crashes with the error from my initial post
```
import keras
import tensorflow as tf

print(tf.__version__)
print(keras.__version__)

ds = tf.data.Dataset.from_tensor_slices(range(10)) \
    .map(lambda x: (
        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),
        0,
    )) \
    .batch(batch_size=4) \
    .prefetch(tf.data.AUTOTUNE)

model = keras.Sequential([
    keras.layers.Resizing(height=10, width=10),
    keras.layers.GlobalMaxPool2D(),
    keras.layers.Softmax(),
])

model.compile(loss=keras.losses.SparseCategoricalCrossentropy())

model.fit(ds)
```
The printed versions are in both code snipets `2.18.0` and `3.8.0`.

If i install tf-keras (aka Keras 2) with `pip install tf-keras` the following code works
```
import tf_keras
import tensorflow as tf

print(tf.__version__)
print(tf_keras.__version__)

ds = tf.data.Dataset.from_tensor_slices(range(10)) \
    .map(lambda x: (
        tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),
        0,
    )) \
    .batch(batch_size=4) \
    .prefetch(tf.data.AUTOTUNE)

model = tf_keras.Sequential([
    tf_keras.layers.Resizing(height=10, width=10),
    tf_keras.layers.GlobalMaxPool2D(),
    tf_keras.layers.Softmax(),
])

model.compile(loss=tf_keras.losses.SparseCategoricalCrossentropy())

model.fit(ds)
```

But i would like to use Keras 3 and not the outdated Keras 2, which seems to not support `tf.RaggedTensor` as input for `keras.layers.Resizing`

DamarXCV (Issue Creator) on (2025-01-31 10:56:44 UTC): I found keras-team/keras#18467 which mentions 


I guess that means that my input pipeline is not supported in Keras 3 for now.

google-ml-butler[bot] on (2025-01-31 10:56:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86043"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86043"">No</a>

"
2817880721,issue,closed,completed,Title: ValueError when adding TensorFlow Hub KerasLayer to Sequential model,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

No

### OS platform and distribution

Google colab

### Mobile device

_No response_

### Python version

Python 3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


When attempting to add hub.KerasLayer to a tf.keras.Sequential model, TensorFlow raises a ValueError, stating that only instances of keras.Layer can be added. However, hub.KerasLayer is a subclass of keras.Layer, so this behavior seems unexpected.

I expected hub.KerasLayer to be accepted as a valid layer in the tf.keras.Sequential model, as per the TensorFlow documentation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_hub as hub

mobilenet_v2 = ""https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4""
inception_v3 = ""https://tfhub.dev/google/imagenet/inception_v3/classification/5""

classifier_model = mobilenet_v2  # @param [""mobilenet_v2"", ""inception_v3""] {type:""raw""}

IMAGE_SHAPE = (224, 224)

classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE + (3,))
])

link to notebook: ""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb""
```

### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-7db51c8d2d71> in <cell line: 0>()
      1 IMAGE_SHAPE = (224, 224)
      2 
----> 3 classifier = tf.keras.Sequential([
      4     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))
      5 ])

1 frames
/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py in add(self, layer, rebuild)
     94                 layer = origin_layer
     95         if not isinstance(layer, Layer):
---> 96             raise ValueError(
     97                 ""Only instances of `keras.Layer` can be ""
     98                 f""added to a Sequential model. Received: {layer} ""

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7d7e43bbeb10> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)
```",oscar066,2025-01-29 11:01:56+00:00,['tilakrayal'],2025-02-03 09:55:10+00:00,2025-02-03 09:55:07+00:00,https://github.com/tensorflow/tensorflow/issues/86041,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2623954257, 'issue_id': 2817880721, 'author': 'tilakrayal', 'body': '@oscar066,\nHi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\n\n```python\n!pip install tf-keras\n\nimport tf_keras as keras\n```\n\nAlso I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/68f3ec8e05d24b51fb351ba872995d69/untitled2308.ipynb).\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 30, 9, 26, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624010086, 'issue_id': 2817880721, 'author': 'mn-48', 'body': 'Thank you.\r\n\r\nOn Thu, 30 Jan 2025, 3:26 pm Tilak, ***@***.***> wrote:\r\n\r\n> @oscar066 <https://github.com/oscar066>,\r\n> @mn-48 <https://github.com/mn-48>,\r\n> Hi, By default the colab notebook is using tensorflow v2.17 which contains\r\n> keras3.0 which was causing the error. Could you please try to import\r\n> keras2.0 with the below commands.\r\n>\r\n> !pip install tf-keras\r\n> import tf_keras as keras\r\n>\r\n> Also I have changed some steps like modifying *tf_keras/keras.Sequential*\r\n> instead of *tf.keras.Sequential* and the code was executed without\r\n> error/fail. Kindly find the gist of it here\r\n> <https://colab.research.google.com/gist/tilakrayal/68f3ec8e05d24b51fb351ba872995d69/untitled2308.ipynb>\r\n> .\r\n>\r\n> Thank you!\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/86041#issuecomment-2623954257>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHLJUQ4RPD4TQGQZMNHWAYT2NHV5NAVCNFSM6AAAAABWCSBUFOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMRTHE2TIMRVG4>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 30, 9, 51, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624592319, 'issue_id': 2817880721, 'author': 'oscar066', 'body': 'The solution where I installed tf_keras worked for that section, but Im encountering a similar error in the ""Attach a classification head"" section of the same notebook. However, the previous solution does not seem to work in this case.\n\nHere is the code block :\n\nfeature_extractor_layer = hub.KerasLayer(\n    feature_extractor_model,\n    input_shape=(224, 224, 3),\n    trainable=False)\n\nnum_classes = len(class_names)\n\nmodel = tf.keras.Sequential([\n  feature_extractor_layer,\n  tf.keras.layers.Dense(num_classes)\n])\n\nmodel.summary()\n\nthe error :\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n[<ipython-input-26-0c90f00baee4>](https://localhost:8080/#) in <cell line: 0>()\n      1 num_classes = len(class_names)\n      2 \n----> 3 model = tf.keras.Sequential([\n      4   feature_extractor_layer,\n      5   tf.keras.layers.Dense(num_classes)\n\n1 frames\n[/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py](https://localhost:8080/#) in add(self, layer, rebuild)\n     95                 layer = origin_layer\n     96         if not isinstance(layer, Layer):\n---> 97             raise ValueError(\n     98                 ""Only instances of `keras.Layer` can be ""\n     99                 f""added to a Sequential model. Received: {layer} ""\n\nValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x791da8457090> (of type <class \'tensorflow_hub.keras_layer.KerasLayer\'>)\n\nError after trying using tf_keras:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n[<ipython-input-27-802aa2140267>](https://localhost:8080/#) in <cell line: 0>()\n      1 num_classes = len(class_names)\n      2 \n----> 3 model = keras.Sequential([\n      4   feature_extractor_layer,\n      5   tf.keras.layers.Dense(num_classes)\n\n2 frames\n[/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/sequential.py](https://localhost:8080/#) in add(self, layer)\n    175                 layer = functional.ModuleWrapper(layer)\n    176         else:\n--> 177             raise TypeError(\n    178                 ""The added layer must be an instance of class Layer. ""\n    179                 f""Received: layer={layer} of type {type(layer)}.""\n\nTypeError: The added layer must be an instance of class Layer. Received: layer=<Dense name=dense_3, built=False> of type <class \'keras.src.layers.core.dense.Dense\'>.\n\nLink to notebook: [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb](url)', 'created_at': datetime.datetime(2025, 1, 30, 13, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624662495, 'issue_id': 2817880721, 'author': 'tilakrayal', 'body': '@oscar066,\nCould you try to modify the tf.keras to keras and execute the code.  I have changed some steps like modifying tf_keras/keras.Sequential instead of tf.keras.Sequential and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6bbd317503ac36aa9b58004cd89b826c/transfer_learning_with_hub.ipynb).\n\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 30, 14, 27, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626984471, 'issue_id': 2817880721, 'author': 'oscar066', 'body': 'the gist notebook executed successfully however am still getting the error on this machine :\n\nhere is the code :\n(raw_train, raw_validation, raw_test) , metadata = tfds.load(\n    \'cats_vs_dogs\',\n    split=[\'train[:80%]\', \'train[80%:90%]\', \'train[90%:]\'],\n    with_info=True,\n    as_supervised=True,\n)\n\nnum_examples = metadata.splits[\'train\'].num_examples\nnum_classes = metadata.features[\'label\'].num_classes\n\nprint(num_examples)\nprint(num_classes)\n\nBATCH_SIZE = 32\ntrain_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE)\nvalidation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\ntest_batches = raw_test.map(format_image).batch(1)\n\nfor  image_batch, label_batch in train_batches.take(1):\n    pass\n\nimage_batch.shape\nmodule_selection = (""mobilenet_v2_100_224"", 224, 1280)  # Updated module name\nhandle_base, pixels, FV_SIZE = module_selection\n\n# Use the correct non-preview URL\nMODULE_HANDLE = ""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"".format(handle_base)\n\nIMAGE_SIZE = (pixels, pixels)\n\nfeature_extractor = hub.KerasLayer(\n    MODULE_HANDLE,\n    input_shape=(224, 224, 3),\n    trainable=False\n)\nmodel = keras.Sequential([\n    feature_extractor,\n    tf.keras.layers.Dense(num_classes, activation=\'softmax\')\n])\n\nand here is the error:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], [line 1](vscode-notebook-cell:?execution_count=5&line=1)\n----> [1](vscode-notebook-cell:?execution_count=5&line=1) model = keras.Sequential([\n      [2](vscode-notebook-cell:?execution_count=5&line=2)     feature_extractor,\n      [3](vscode-notebook-cell:?execution_count=5&line=3)     tf.keras.layers.Dense(num_classes, activation=\'softmax\')\n      [4](vscode-notebook-cell:?execution_count=5&line=4) ])\n      [6](vscode-notebook-cell:?execution_count=5&line=6) model.summary()\n\nFile /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204, in no_automatic_dependency_tracking.<locals>._method_wrapper(self, *args, **kwargs)\n    [202](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:202) self._self_setattr_tracking = False  # pylint: disable=protected-access\n    [203](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:203) try:\n--> [204](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204)   result = method(self, *args, **kwargs)\n    [205](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205) finally:\n    [206](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:206)   self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n\nFile /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\n     [67](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:67)     filtered_tb = _process_traceback_frames(e.__traceback__)\n     [68](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:68)     # To get the full stack trace, call:\n     [69](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:69)     # `tf.debugging.disable_traceback_filtering()`\n---> [70](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70)     raise e.with_traceback(filtered_tb) from None\n     [71](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:71) finally:\n     [72](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:72)     del filtered_tb\n\nFile /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:177, in Sequential.add(self, layer)\n...\n    [180](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:180)     )\n    [182](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:182) tf_utils.assert_no_legacy_layers([layer])\n    [183](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:183) if not self._is_layer_name_unique(layer):\n\nTypeError: The added layer must be an instance of class Layer. Received: layer=<Dense name=dense, built=False> of type <class \'keras.src.layers.core.dense.Dense\'>.', 'created_at': datetime.datetime(2025, 1, 31, 11, 28, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630465366, 'issue_id': 2817880721, 'author': 'oscar066', 'body': 'The issue has been resolved. Installing tf-keras and using keras from it instead of tf.keras fixed the problem. Thank you!', 'created_at': datetime.datetime(2025, 2, 3, 9, 55, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630465417, 'issue_id': 2817880721, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86041"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86041"">No</a>', 'created_at': datetime.datetime(2025, 2, 3, 9, 55, 9, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-30 09:26:18 UTC): @oscar066,
Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras

import tf_keras as keras
```

Also I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/68f3ec8e05d24b51fb351ba872995d69/untitled2308.ipynb).

Thank you!

mn-48 on (2025-01-30 09:51:20 UTC): Thank you.

On Thu, 30 Jan 2025, 3:26 pm Tilak, ***@***.***> wrote:

oscar066 (Issue Creator) on (2025-01-30 13:58:35 UTC): The solution where I installed tf_keras worked for that section, but Im encountering a similar error in the ""Attach a classification head"" section of the same notebook. However, the previous solution does not seem to work in this case.

Here is the code block :

feature_extractor_layer = hub.KerasLayer(
    feature_extractor_model,
    input_shape=(224, 224, 3),
    trainable=False)

num_classes = len(class_names)

model = tf.keras.Sequential([
  feature_extractor_layer,
  tf.keras.layers.Dense(num_classes)
])

model.summary()

the error :

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-26-0c90f00baee4>](https://localhost:8080/#) in <cell line: 0>()
      1 num_classes = len(class_names)
      2 
----> 3 model = tf.keras.Sequential([
      4   feature_extractor_layer,
      5   tf.keras.layers.Dense(num_classes)

1 frames
[/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py](https://localhost:8080/#) in add(self, layer, rebuild)
     95                 layer = origin_layer
     96         if not isinstance(layer, Layer):
---> 97             raise ValueError(
     98                 ""Only instances of `keras.Layer` can be ""
     99                 f""added to a Sequential model. Received: {layer} ""

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x791da8457090> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)

Error after trying using tf_keras:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[<ipython-input-27-802aa2140267>](https://localhost:8080/#) in <cell line: 0>()
      1 num_classes = len(class_names)
      2 
----> 3 model = keras.Sequential([
      4   feature_extractor_layer,
      5   tf.keras.layers.Dense(num_classes)

2 frames
[/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/sequential.py](https://localhost:8080/#) in add(self, layer)
    175                 layer = functional.ModuleWrapper(layer)
    176         else:
--> 177             raise TypeError(
    178                 ""The added layer must be an instance of class Layer. ""
    179                 f""Received: layer={layer} of type {type(layer)}.""

TypeError: The added layer must be an instance of class Layer. Received: layer=<Dense name=dense_3, built=False> of type <class 'keras.src.layers.core.dense.Dense'>.

Link to notebook: [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb](url)

tilakrayal (Assginee) on (2025-01-30 14:27:09 UTC): @oscar066,
Could you try to modify the tf.keras to keras and execute the code.  I have changed some steps like modifying tf_keras/keras.Sequential instead of tf.keras.Sequential and the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6bbd317503ac36aa9b58004cd89b826c/transfer_learning_with_hub.ipynb).


Thank you!

oscar066 (Issue Creator) on (2025-01-31 11:28:37 UTC): the gist notebook executed successfully however am still getting the error on this machine :

here is the code :
(raw_train, raw_validation, raw_test) , metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

num_examples = metadata.splits['train'].num_examples
num_classes = metadata.features['label'].num_classes

print(num_examples)
print(num_classes)

BATCH_SIZE = 32
train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE)
validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)
test_batches = raw_test.map(format_image).batch(1)

for  image_batch, label_batch in train_batches.take(1):
    pass

image_batch.shape
module_selection = (""mobilenet_v2_100_224"", 224, 1280)  # Updated module name
handle_base, pixels, FV_SIZE = module_selection

# Use the correct non-preview URL
MODULE_HANDLE = ""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"".format(handle_base)

IMAGE_SIZE = (pixels, pixels)

feature_extractor = hub.KerasLayer(
    MODULE_HANDLE,
    input_shape=(224, 224, 3),
    trainable=False
)
model = keras.Sequential([
    feature_extractor,
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

and here is the error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[5], [line 1](vscode-notebook-cell:?execution_count=5&line=1)
----> [1](vscode-notebook-cell:?execution_count=5&line=1) model = keras.Sequential([
      [2](vscode-notebook-cell:?execution_count=5&line=2)     feature_extractor,
      [3](vscode-notebook-cell:?execution_count=5&line=3)     tf.keras.layers.Dense(num_classes, activation='softmax')
      [4](vscode-notebook-cell:?execution_count=5&line=4) ])
      [6](vscode-notebook-cell:?execution_count=5&line=6) model.summary()

File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204, in no_automatic_dependency_tracking.<locals>._method_wrapper(self, *args, **kwargs)
    [202](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:202) self._self_setattr_tracking = False  # pylint: disable=protected-access
    [203](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:203) try:
--> [204](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204)   result = method(self, *args, **kwargs)
    [205](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205) finally:
    [206](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:206)   self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     [67](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:67)     filtered_tb = _process_traceback_frames(e.__traceback__)
     [68](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:68)     # To get the full stack trace, call:
     [69](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:69)     # `tf.debugging.disable_traceback_filtering()`
---> [70](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70)     raise e.with_traceback(filtered_tb) from None
     [71](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:71) finally:
     [72](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:72)     del filtered_tb

File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:177, in Sequential.add(self, layer)
...
    [180](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:180)     )
    [182](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:182) tf_utils.assert_no_legacy_layers([layer])
    [183](https://vscode-remote+vscode-002d01jjp4e5y522k69wx1xs17cqfx-002estudio-002elightning-002eai.vscode-resource.vscode-cdn.net/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tf_keras/src/engine/sequential.py:183) if not self._is_layer_name_unique(layer):

TypeError: The added layer must be an instance of class Layer. Received: layer=<Dense name=dense, built=False> of type <class 'keras.src.layers.core.dense.Dense'>.

oscar066 (Issue Creator) on (2025-02-03 09:55:08 UTC): The issue has been resolved. Installing tf-keras and using keras from it instead of tf.keras fixed the problem. Thank you!

google-ml-butler[bot] on (2025-02-03 09:55:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86041"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/86041"">No</a>

"
2816269712,issue,closed,completed,Status of C/C++ APIs?,"Do they fully support running operators and models, gradients and optimizers? What's the relationship between them? Are there up-to-date resources, tutorials etc on how to use them?",matteosal,2025-01-28 17:07:43+00:00,['Venkat6871'],2025-01-30 16:52:48+00:00,2025-01-30 16:52:45+00:00,https://github.com/tensorflow/tensorflow/issues/85958,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues')]","[{'comment_id': 2621005501, 'issue_id': 2816269712, 'author': 'Venkat6871', 'body': 'Hi **@matteosal** ,\nWelcome to TensorFlow! Yes, TensorFlow fully supports running models, computing gradients, and using optimizers. These are fundamental building blocks for building and training machine learning models in TensorFlow. Here, I am providing the [Doc1](https://www.tensorflow.org/api_docs/cc), [Doc2](https://www.tensorflow.org/install/lang_c) for your reference. If you have any further queries, please fill out all the required templates.\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 29, 8, 43, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621654578, 'issue_id': 2816269712, 'author': 'matteosal', 'body': ""@Venkat6871 you have linked a page about Javascript, I asked for C/C++. I am asking this because it's hard to find clear information about them. \n\n[This document](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/bindings.md) states at the bottom that the C API does not support gradients, but maybe that's for TF 1 so maybe it doesn't apply anymore.\n\nThen I found this [C++ API reference page](https://www.tensorflow.org/api_docs/cc/) which just lists a bunch of symbols with no contextual explanation or introduction. Looking around there only seem to be some isolated gradient primitives but I couldn't find anything analogous to a generic method of running backpropagation through an arbitrary set of operators."", 'created_at': datetime.datetime(2025, 1, 29, 13, 27, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621726427, 'issue_id': 2816269712, 'author': 'mihaimaruseac', 'body': ""The C API is very limited, it's used to create bindings to other languages.\n\nThe C++ API is used mostly for inference, I am not 100% it 100% covers training needs."", 'created_at': datetime.datetime(2025, 1, 29, 13, 56, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621758951, 'issue_id': 2816269712, 'author': 'matteosal', 'body': ""@mihaimaruseac \n\n> The C API is very limited, it's used to create bindings to other languages.\n\nThen it seems natural to expect it supports gradients, right? It's a core feature anyone wanting to create a binding for another language would need."", 'created_at': datetime.datetime(2025, 1, 29, 14, 9, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622214635, 'issue_id': 2816269712, 'author': 'mihaimaruseac', 'body': '[This](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h) is the entire C API. In particular [it supports gradients](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h#L994).', 'created_at': datetime.datetime(2025, 1, 29, 17, 4, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622456890, 'issue_id': 2816269712, 'author': 'matteosal', 'body': ""@mihaimaruseac \nYes I noticed that function in the header. It has a [note](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h#L990-L993) which states that not all gradients are supported. The link is broken but it's clear it's meant to lead [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients). This document states that implemented gradients should be declared as `<OpName>Grad(...)`, and looking for this template in the entire directory produces these matches:\n```\n[Abs, Acos, Acosh, Add, AddN, Angle, Asin, Asinh, Atan, Atan2, Atanh, BaseFusedBatchNorm, BatchMatMul, BatchMatMulV2, BatchToSpace, BatchToSpaceND, BroadcastTo, Cast, CheckNumerics, ClipByValue, Complex, ConcatV2, Conj, Conv2D, Conv2DBackpropInput, Cos, Cosh, Cumsum, DepthToSpace, DepthwiseConv2dNative, Diag, DiagPart, Div, DivNoNan, DynamicPartition, DynamicStitch, Einsum, Erf, Erfinv, Exp, ExpandDims, Expm1, Fill, FusedBatchNormV3, GatherNd, GatherV2, Identity, Imag, Inv, L2Loss, Lgamma, Log, Log1p, LogSoftmax, MatMul, MatrixBandPart, MatrixDiag, Maximum, Mean, Minimum, MinOrMax, MirrorPad, MirrorPadGrad, Mul, Ndtri, Neg, Pack, Pad, PartitionedCall, Pow, Prod, QuantizeAndDequantize, QuantizeAndDequantizeV3, ReadVariableOp, Real, RealDiv, RefIdentity, Reshape, Reverse, ReverseSequence, Roll, Rsqrt, ScatterNd, ScatterNdNonAliasingAdd, SegmentSum, Select, SelectV2, Sigmoid, Sign, Sin, Sinh, Slice, Softmax, SoftmaxCrossEntropyWithLogits, SpaceToBatch, SpaceToBatchND, SpaceToDepth, Split, SplitV, Sqrt, Square, SquaredDifference, Squeeze, Sub, Sum, Tan, Tanh, Tile, Transpose, Unpack, UnsortedSegmentMinOrMax, UnsortedSegmentSum]\n```\n\nSeveral things are missing here, e.g. 1D and 3D Convolutions, ConvTranspose, Pooling, spatial resampling, RNNs. Can anyone confirm this is correct and the above gradients are the only ones available?"", 'created_at': datetime.datetime(2025, 1, 29, 17, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622494836, 'issue_id': 2816269712, 'author': 'mihaimaruseac', 'body': 'That is correct. PRs welcome to add more support if needed, though I think JAX is a better replacement.', 'created_at': datetime.datetime(2025, 1, 29, 18, 17, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625044285, 'issue_id': 2816269712, 'author': 'matteosal', 'body': 'Thank you', 'created_at': datetime.datetime(2025, 1, 30, 16, 52, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625044362, 'issue_id': 2816269712, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85958"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85958"">No</a>', 'created_at': datetime.datetime(2025, 1, 30, 16, 52, 47, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-29 08:43:49 UTC): Hi **@matteosal** ,
Welcome to TensorFlow! Yes, TensorFlow fully supports running models, computing gradients, and using optimizers. These are fundamental building blocks for building and training machine learning models in TensorFlow. Here, I am providing the [Doc1](https://www.tensorflow.org/api_docs/cc), [Doc2](https://www.tensorflow.org/install/lang_c) for your reference. If you have any further queries, please fill out all the required templates.

Thank you!

matteosal (Issue Creator) on (2025-01-29 13:27:50 UTC): @Venkat6871 you have linked a page about Javascript, I asked for C/C++. I am asking this because it's hard to find clear information about them. 

[This document](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/bindings.md) states at the bottom that the C API does not support gradients, but maybe that's for TF 1 so maybe it doesn't apply anymore.

Then I found this [C++ API reference page](https://www.tensorflow.org/api_docs/cc/) which just lists a bunch of symbols with no contextual explanation or introduction. Looking around there only seem to be some isolated gradient primitives but I couldn't find anything analogous to a generic method of running backpropagation through an arbitrary set of operators.

mihaimaruseac on (2025-01-29 13:56:36 UTC): The C API is very limited, it's used to create bindings to other languages.

The C++ API is used mostly for inference, I am not 100% it 100% covers training needs.

matteosal (Issue Creator) on (2025-01-29 14:09:45 UTC): @mihaimaruseac 


Then it seems natural to expect it supports gradients, right? It's a core feature anyone wanting to create a binding for another language would need.

mihaimaruseac on (2025-01-29 17:04:15 UTC): [This](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h) is the entire C API. In particular [it supports gradients](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h#L994).

matteosal (Issue Creator) on (2025-01-29 17:58:40 UTC): @mihaimaruseac 
Yes I noticed that function in the header. It has a [note](https://github.com/tensorflow/tensorflow/blob/40e02fe076f822840e3d52c95d40613317aeb5de/tensorflow/c/c_api.h#L990-L993) which states that not all gradients are supported. The link is broken but it's clear it's meant to lead [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients). This document states that implemented gradients should be declared as `<OpName>Grad(...)`, and looking for this template in the entire directory produces these matches:
```
[Abs, Acos, Acosh, Add, AddN, Angle, Asin, Asinh, Atan, Atan2, Atanh, BaseFusedBatchNorm, BatchMatMul, BatchMatMulV2, BatchToSpace, BatchToSpaceND, BroadcastTo, Cast, CheckNumerics, ClipByValue, Complex, ConcatV2, Conj, Conv2D, Conv2DBackpropInput, Cos, Cosh, Cumsum, DepthToSpace, DepthwiseConv2dNative, Diag, DiagPart, Div, DivNoNan, DynamicPartition, DynamicStitch, Einsum, Erf, Erfinv, Exp, ExpandDims, Expm1, Fill, FusedBatchNormV3, GatherNd, GatherV2, Identity, Imag, Inv, L2Loss, Lgamma, Log, Log1p, LogSoftmax, MatMul, MatrixBandPart, MatrixDiag, Maximum, Mean, Minimum, MinOrMax, MirrorPad, MirrorPadGrad, Mul, Ndtri, Neg, Pack, Pad, PartitionedCall, Pow, Prod, QuantizeAndDequantize, QuantizeAndDequantizeV3, ReadVariableOp, Real, RealDiv, RefIdentity, Reshape, Reverse, ReverseSequence, Roll, Rsqrt, ScatterNd, ScatterNdNonAliasingAdd, SegmentSum, Select, SelectV2, Sigmoid, Sign, Sin, Sinh, Slice, Softmax, SoftmaxCrossEntropyWithLogits, SpaceToBatch, SpaceToBatchND, SpaceToDepth, Split, SplitV, Sqrt, Square, SquaredDifference, Squeeze, Sub, Sum, Tan, Tanh, Tile, Transpose, Unpack, UnsortedSegmentMinOrMax, UnsortedSegmentSum]
```

Several things are missing here, e.g. 1D and 3D Convolutions, ConvTranspose, Pooling, spatial resampling, RNNs. Can anyone confirm this is correct and the above gradients are the only ones available?

mihaimaruseac on (2025-01-29 18:17:35 UTC): That is correct. PRs welcome to add more support if needed, though I think JAX is a better replacement.

matteosal (Issue Creator) on (2025-01-30 16:52:45 UTC): Thank you

google-ml-butler[bot] on (2025-01-30 16:52:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85958"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85958"">No</a>

"
2810307411,issue,open,,"TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Centos 7.9, RHEL 8, RHEL 9

### Mobile device

_No response_

### Python version

3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

550.90.07

### GPU model and memory

_No response_

### Current behavior?

After discussing this on the [Apptainer Git](https://github.com/apptainer/apptainer/issues/2706#issuecomment-2613253071) we determined the latest TF-GPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1-gpu work just fine.

`apptainer run --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python`


```
import tensorflow as tf
2025-01-24 15:03:27.629215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737749008.639844   35316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737749008.847756   35316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-24 15:03:31.499335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

```
```
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
W0000 00:00:1737749068.599039   35316 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Num GPUs Available:  0

```
```
>>> print(tf.__version__)
2.18.0
```


### Standalone code to reproduce the issue

```shell
shpc install tensorflow/tensorflow:latest-gpu

or

apptainer pull docker://tensorflow/tensorflow:latest-gpu

apptainer run --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python

python
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

### Relevant log output

```shell
apptainer --debug exec --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python
DEBUG   [U=0,P=1355208]    persistentPreRun()            Apptainer version: 1.3.6-1
DEBUG   [U=0,P=1355208]    persistentPreRun()            Parsing configuration file /etc/apptainer/apptainer.conf
DEBUG   [U=0,P=1355208]    SetBinaryPath()               Setting binary path to /usr/libexec/apptainer/bin:/usr/share/Modules/bin:/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/TurboVNC/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
DEBUG   [U=0,P=1355208]    SetBinaryPath()               Using that path for all binaries
DEBUG   [U=0,P=1355208]    handleConfDir()               /root/.apptainer already exists. Not creating.
DEBUG   [U=0,P=1355208]    handleRemoteConf()            Ensuring file permission of 0600 on /root/.apptainer/remote.yaml
DEBUG   [U=0,P=1355208]    setUmask()                    Saving umask 0002 for propagation into container
DEBUG   [U=0,P=1355208]    checkEncryptionKey()          Checking for encrypted system partition
DEBUG   [U=0,P=1355208]    Init()                        Image format detection
DEBUG   [U=0,P=1355208]    Init()                        Check for sandbox image format
DEBUG   [U=0,P=1355208]    Init()                        sandbox format initializer returned: not a directory image
DEBUG   [U=0,P=1355208]    Init()                        Check for sif image format
DEBUG   [U=0,P=1355208]    Init()                        sif image format detected
VERBOSE [U=0,P=1355208]    SetGPUConfig()                'always use nv = yes' found in apptainer.conf
DEBUG   [U=0,P=1355208]    setNVLegacyConfig()           Using legacy binds for nv GPU setup
VERBOSE [U=0,P=1355208]    NvidiaIpcsPath()              persistenced socket /var/run/nvidia-persistenced/socket not found
DEBUG   [U=0,P=1355208]    findOnPath()                  Found ""ldconfig"" at ""/sbin/ldconfig""
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHELL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_GID environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTCONTROL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding no_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HOSTNAME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTSIZE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SBATCH_PARTITION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_OUTPUT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SLURM_PARTITION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_COMMAND environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_USER environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_DIR environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding PWD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOGNAME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULESHOME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MANPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_RESTORE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_SHARE_MANPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SSH_ASKPASS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LANG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LS_COLORS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_SETTARG_FULL_SUPPORT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_PS1 environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding https_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_VERSION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH_ROOT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_PKG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding TERM environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LESSOPEN environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding NO_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_RUN_QUARANTINE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOADEDMODULES environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHLVL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_ENV environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_sys environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTPS_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_INIT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTP_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding http_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding S_COLORS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_LMINIT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding which_declare environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding XDG_DATA_DIRS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_UID environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_CMD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MAIL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_CMD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_ml%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_which%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_module%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_scl%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC__module_raw%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding _ environment variable
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Not forwarding APPTAINER_DEBUG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER_PATH environment variable
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting HOME=/root
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355208]    SetuidMountAllowed()          Kernel squashfs mount allowed because running as root
DEBUG   [U=0,P=1355208]    init()                        Use starter binary /usr/libexec/apptainer/bin/starter
VERBOSE [U=0,P=1355208]    print()                       Set messagelevel to: 5
VERBOSE [U=0,P=1355208]    init()                        Starter initialization
VERBOSE [U=0,P=1355208]    is_suid()                     Check if we are running as setuid: 0
DEBUG   [U=0,P=1355208]    read_engine_config()          Read engine configuration
DEBUG   [U=0,P=1355208]    init()                        Wait completion of stage1
DEBUG   [U=0,P=1355224]    set_parent_death_signal()     Set parent death signal to 9
VERBOSE [U=0,P=1355224]    init()                        Spawn stage 1
DEBUG   [U=0,P=1355224]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355224]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355224]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355224]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355224]    startup()                     Execute stage 1
DEBUG   [U=0,P=1355224]    StageOne()                    Entering stage 1
DEBUG   [U=0,P=1355224]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355224]    prepareRootCaps()             Root full capabilities
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/proc/sys/fs/binfmt_misc"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/home"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/share"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/misc"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/net"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/locker"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/labshare"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/staging"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/localtime: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/hosts: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for home directory /root: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for current working directory /root: no mount point
DEBUG   [U=0,P=1355224]    Init()                        Image format detection
DEBUG   [U=0,P=1355224]    Init()                        Check for sandbox image format
DEBUG   [U=0,P=1355224]    Init()                        sandbox format initializer returned: not a directory image
DEBUG   [U=0,P=1355224]    Init()                        Check for sif image format
DEBUG   [U=0,P=1355224]    Init()                        sif image format detected
DEBUG   [U=0,P=1355224]    setSessionLayer()             Using overlay because it is not disabled
DEBUG   [U=0,P=1355224]    PrepareConfig()               image driver is 
VERBOSE [U=0,P=1355208]    wait_child()                  stage 1 exited with status 0
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 4
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 5
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 6
DEBUG   [U=0,P=1355208]    init()                        Set child signal mask
DEBUG   [U=0,P=1355208]    init()                        Create socketpair for master communication channel
DEBUG   [U=0,P=1355208]    init()                        Create RPC socketpair for communication between stage 2 and RPC server
VERBOSE [U=0,P=1355208]    init()                        Spawn master process
DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9
VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace
VERBOSE [U=0,P=1355208]    enter_namespace()             Entering in mount namespace
DEBUG   [U=0,P=1355208]    enter_namespace()             Opening namespace file ns/mnt
VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace
VERBOSE [U=0,P=1355231]    init()                        Spawn RPC server
DEBUG   [U=0,P=1355208]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355208]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355208]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355231]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355231]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355231]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355208]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355208]    startup()                     Execute master process
DEBUG   [U=0,P=1355231]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355231]    startup()                     Serve RPC requests
DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355208]    setupSessionLayout()          Using Layer system: overlay
DEBUG   [U=0,P=1355208]    setupOverlayLayout()          Creating overlay SESSIONDIR layout
DEBUG   [U=0,P=1355208]    addRootfsMount()              Mount rootfs in read-only mode
DEBUG   [U=0,P=1355208]    addRootfsMount()              Image type is 4096
DEBUG   [U=0,P=1355208]    addRootfsMount()              Mounting block [squashfs] image: /share/apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif
DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount proc'
DEBUG   [U=0,P=1355208]    addKernelMount()              Adding proc to mount list
VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /proc:/proc
DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount sys'
DEBUG   [U=0,P=1355208]    addKernelMount()              Adding sysfs to mount list
VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /sys:/sys
DEBUG   [U=0,P=1355208]    addDevMount()                 Checking configuration file for 'mount dev'
DEBUG   [U=0,P=1355208]    addDevMount()                 Adding dev to mount list
VERBOSE [U=0,P=1355208]    addDevMount()                 Default mount: /dev:/dev
DEBUG   [U=0,P=1355208]    addHostMount()                Not mounting host file systems per configuration
VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/localtime, /etc/localtime
VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/hosts, /etc/hosts
DEBUG   [U=0,P=1355208]    addHomeStagingDir()           Staging home directory (/root) at /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    addHomeMount()                Adding home directory mount [/var/lib/apptainer/mnt/session/root:/root] to list using layer: overlay
DEBUG   [U=0,P=1355208]    addTmpMount()                 Checking for 'mount tmp' in configuration file
DEBUG   [U=0,P=1355208]    addScratchMount()             Not mounting scratch directory: Not requested
DEBUG   [U=0,P=1355208]    addLibsMount()                Checking for 'user bind control' in configuration file
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-cfg.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-eglcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ml.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gtk3.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glvkspirv.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcudadebugger.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so.2 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ptxjitcompiler.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX_nvidia.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-tls.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-encode.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ptxjitcompiler.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-encode.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-nvvm.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glsi.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opticalflow.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opencl.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-egl-wayland.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvoptix.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gpucomp.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2_nvidia.so.2 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM_nvidia.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-nvvm.so.4 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gtk2.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ml.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-fbc.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-fbc.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opticalflow.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-cfg.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libvdpau_nvidia.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-rtcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL_nvidia.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Checking for 'user bind control' in configuration file
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-persistenced:/usr/bin/nvidia-persistenced to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-cuda-mps-control:/usr/bin/nvidia-cuda-mps-control to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-cuda-mps-server:/usr/bin/nvidia-cuda-mps-server to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-smi:/usr/bin/nvidia-smi to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-debugdump:/usr/bin/nvidia-debugdump to mount list
DEBUG   [U=0,P=1355208]    addResolvConfMount()          Adding /etc/resolv.conf to mount list
VERBOSE [U=0,P=1355208]    addResolvConfMount()          Default mount: /etc/resolv.conf:/etc/resolv.conf
DEBUG   [U=0,P=1355208]    addHostnameMount()            Skipping hostname mount, not virtualizing UTS namespace on user request
DEBUG   [U=0,P=1355208]    create()                      Mount all
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting tmpfs to /var/lib/apptainer/mnt/session
DEBUG   [U=0,P=1355208]    mountImage()                  Mounting loop device /dev/loop0 to /var/lib/apptainer/mnt/session/rootfs of type squashfs
DEBUG   [U=0,P=1355208]    createCwdDir()                Using /root as current working directory
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting overlay to /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355208]    mountGeneric()                Unmounting and remounting overlay
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355208]    setPropagationMount()         Set RPC mount propagation flag to SLAVE
VERBOSE [U=0,P=1355208]    Passwd()                      Checking for template passwd file: /var/lib/apptainer/mnt/session/rootfs/etc/passwd
VERBOSE [U=0,P=1355208]    Passwd()                      Creating passwd content
VERBOSE [U=0,P=1355208]    Passwd()                      Creating template passwd file and injecting user data: /var/lib/apptainer/mnt/session/rootfs/etc/passwd
DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/passwd to mount list
VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/passwd:/etc/passwd
VERBOSE [U=0,P=1355208]    Group()                       Checking for template group file: /var/lib/apptainer/mnt/session/rootfs/etc/group
VERBOSE [U=0,P=1355208]    Group()                       Creating group content
DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/group to mount list
VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/group:/etc/group
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /dev to /var/lib/apptainer/mnt/session/final/dev
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/localtime to /var/lib/apptainer/mnt/session/final/etc/localtime
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/localtime
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/hosts to /var/lib/apptainer/mnt/session/final/etc/hosts
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/hosts
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /proc to /var/lib/apptainer/mnt/session/final/proc
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/proc
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting sysfs to /var/lib/apptainer/mnt/session/final/sys
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /root to /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/root to /var/lib/apptainer/mnt/session/final/root
DEBUG   [U=0,P=1355208]    func1()                       Container /tmp resolves to ""/tmp""
DEBUG   [U=0,P=1355208]    func1()                       Container /var/tmp resolves to ""/var/tmp""
VERBOSE [U=0,P=1355208]    func1()                       Default mount: /tmp:/tmp
VERBOSE [U=0,P=1355208]    func1()                       Default mount: /var/tmp:/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /tmp to /var/lib/apptainer/mnt/session/final/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/tmp to /var/lib/apptainer/mnt/session/final/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so to /var/lib/apptainer/mnt/session/libs/libOpenCL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so.0 to /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-cfg.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so.1 to /var/lib/apptainer/mnt/session/libs/libEGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-eglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-eglcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-eglcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ml.so to /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so.1 to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gtk3.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gtk3.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gtk3.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glvkspirv.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glvkspirv.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glvkspirv.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcudadebugger.so.1 to /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ptxjitcompiler.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so to /var/lib/apptainer/mnt/session/libs/libGLESv2.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so to /var/lib/apptainer/mnt/session/libs/libGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-tls.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-tls.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-tls.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so.0 to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-encode.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so.1 to /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ptxjitcompiler.so to /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-encode.so to /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-nvvm.so to /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glsi.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glsi.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glsi.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opticalflow.so to /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opencl.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-opencl.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opencl.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-egl-wayland.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-egl-wayland.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-egl-wayland.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvoptix.so.1 to /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gpucomp.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gpucomp.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gpucomp.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so.1 to /var/lib/apptainer/mnt/session/libs/libGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2_nvidia.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM_nvidia.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-nvvm.so.4 to /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so.4
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so.4
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so to /var/lib/apptainer/mnt/session/libs/libEGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gtk2.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gtk2.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gtk2.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ml.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-fbc.so to /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-fbc.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opticalflow.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so to /var/lib/apptainer/mnt/session/libs/libGLX.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-cfg.so to /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so to /var/lib/apptainer/mnt/session/libs/libcuda.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libvdpau_nvidia.so to /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-rtcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-rtcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-rtcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so.1 to /var/lib/apptainer/mnt/session/libs/libcuda.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so to /var/lib/apptainer/mnt/session/libs/libOpenGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/libs to /var/lib/apptainer/mnt/session/final/.singularity.d/libs
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/.singularity.d/libs
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-persistenced to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-persistenced
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-persistenced
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-cuda-mps-control to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-control
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-control
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-cuda-mps-server to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-server
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-server
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-smi to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-smi
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-smi
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-debugdump to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-debugdump
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-debugdump
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/resolv.conf to /var/lib/apptainer/mnt/session/final/etc/resolv.conf
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/passwd to /var/lib/apptainer/mnt/session/final/etc/passwd
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/group to /var/lib/apptainer/mnt/session/final/etc/group
VERBOSE [U=0,P=1355208]    addCwdMount()                 /root found within container
DEBUG   [U=0,P=1355208]    create()                      Chroot into /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355231]    Chroot()                      Hold reference to host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Called pivot_root on /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355231]    Chroot()                      Change current directory to host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Apply slave mount propagation for host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Called unmount(/, syscall.MNT_DETACH)
DEBUG   [U=0,P=1355231]    Chroot()                      Changing directory to / to avoid getpwd issues
DEBUG   [U=0,P=1355208]    create()                      Chdir into / to avoid errors
VERBOSE [U=0,P=1355230]    wait_child()                  rpc server exited with status 0
DEBUG   [U=0,P=1355230]    init()                        Set container privileges
DEBUG   [U=0,P=1355230]    apply_privileges()            Effective capabilities:   0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Permitted capabilities:   0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Bounding capabilities:    0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Inheritable capabilities: 0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Ambient capabilities:     0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Set user ID to 0
DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9
DEBUG   [U=0,P=1355230]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355230]    func1()                       executablePath does not exist, assuming default prefix
DEBUG   [U=0,P=1355230]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355230]    startup()                     Execute stage 2
DEBUG   [U=0,P=1355230]    StageTwo()                    Entering stage 2
DEBUG   [U=0,P=1355230]    StartProcess()                Setting umask in container to 0002
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC__module_raw%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_ml%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_module%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_scl%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_which%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/01-base.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/10-docker2singularity.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/90-environment.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/94-appsbase.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/95-apps.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99-base.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99-runtimevars.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Running action command exec
DEBUG   [U=0,P=1355208]    PostStartProcess()            Post start process
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```",SomePersonSomeWhereInTheWorld,2025-01-24 20:16:31+00:00,"['MichaelHudgins', 'tilakrayal']",2025-01-28 08:59:12+00:00,,https://github.com/tensorflow/tensorflow/issues/85689,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2613840216, 'issue_id': 2810307411, 'author': 'ngaywood', 'body': 'The docker  TF container tries to load libcudnn.so.9\nHowever the container has only been built with libcudnn.so.8\n\nMore detail here:\n[tensorflow 2.18 requires libcudnn.so.9](https://github.com/tensorflow/tensorflow/issues/80538#issuecomment-2591286240)', 'created_at': datetime.datetime(2025, 1, 25, 8, 26, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614009743, 'issue_id': 2810307411, 'author': 'SomePersonSomeWhereInTheWorld', 'body': '> The docker TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8\n> \n> More detail here: [tensorflow 2.18 requires libcudnn.so.9](https://github.com/tensorflow/tensorflow/issues/80538#issuecomment-2591286240)\n\nThanks, how do we get the maintained to flx if?', 'created_at': datetime.datetime(2025, 1, 25, 15, 52, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618341310, 'issue_id': 2810307411, 'author': 'tilakrayal', 'body': '@SomePersonSomeWhereInTheWorld,\nI request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/80538) where a similar feature has been proposed and it is still open. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2025, 1, 28, 8, 59, 4, tzinfo=datetime.timezone.utc)}]","ngaywood on (2025-01-25 08:26:53 UTC): The docker  TF container tries to load libcudnn.so.9
However the container has only been built with libcudnn.so.8

More detail here:
[tensorflow 2.18 requires libcudnn.so.9](https://github.com/tensorflow/tensorflow/issues/80538#issuecomment-2591286240)

SomePersonSomeWhereInTheWorld (Issue Creator) on (2025-01-25 15:52:19 UTC): Thanks, how do we get the maintained to flx if?

tilakrayal (Assginee) on (2025-01-28 08:59:04 UTC): @SomePersonSomeWhereInTheWorld,
I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/80538) where a similar feature has been proposed and it is still open. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!

"
2809530446,issue,closed,duplicate,difficulty installing tensorflow,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected to import TensorFlow, but I keep getting the error message below:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\Desktop\Ban6440-Milestone2assignment.py"", line 77, in <module>
    import tensorflow as tf
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\Desktop\Ban6440-Milestone2assignment.py"", line 77, in <module>
    import tensorflow as tf
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```",Chiebuka-Chibuike2024,2025-01-24 14:00:16+00:00,['Venkat6871'],2025-01-24 14:29:06+00:00,2025-01-24 14:28:45+00:00,https://github.com/tensorflow/tensorflow/issues/85668,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2612663680, 'issue_id': 2809530446, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85668"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85668"">No</a>', 'created_at': datetime.datetime(2025, 1, 24, 14, 28, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612664366, 'issue_id': 2809530446, 'author': 'mihaimaruseac', 'body': 'Please perform a search for similar issues.', 'created_at': datetime.datetime(2025, 1, 24, 14, 29, 5, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2025-01-24 14:28:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85668"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85668"">No</a>

mihaimaruseac on (2025-01-24 14:29:05 UTC): Please perform a search for similar issues.

"
2809503213,issue,open,,tensorflow takes a long time to prepare before the first iteration,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.10.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.4/8.9.1

### GPU model and memory

Nvidia Tesla K20m

### Current behavior?

tensorflow takes a long time to prepare before the first iteration.I used my custom model for training, but it took 40-60 minutes from the time the data was ready to the first iteration. This was true even for a very small dataset. And my model only had 835,620 parameters.

This model is used to pick up the phase of seismic data. If an experiment is conducted, the data can be fabricated by itself.

### Standalone code to reproduce the issue

```shell
import numpy as np
import matplotlib
matplotlib.use('agg')
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs
from tensorflow.python import keras
from tensorflow.keras import backend as K
from tensorflow.keras.layers import (
    Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,
    Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec,
LayerNormalization,Layer, Dense, Dropout,Layer
)
from tensorflow.keras.optimizers import Adam

from tensorflow import keras
import tensorflow as tf
from tensorflow.keras import initializers, regularizers, constraints, activations




def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        '''Recall metric. Computes the recall, a metric for multi-label classification.'''
        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
        possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))
        recall = true_positives / (possible_positives + tf.keras.backend.epsilon())
        return recall

    def precision(y_true, y_pred):
        '''Precision metric. Computes the precision, a metric for multi-label classification.'''
        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
        predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())
        return precision
    
    precision_val = precision(y_true, y_pred)
    recall_val = recall(y_true, y_pred)
    
    # F1 score calculation
    return 2 * (precision_val * recall_val) / (precision_val + recall_val + tf.keras.backend.epsilon())





  
class LayerNormalization(keras.layers.Layer):
    
              
    def __init__(self,
                 center=True,
                 scale=True,
                 epsilon=None,
                 gamma_initializer='ones',
                 beta_initializer='zeros',
                 **kwargs):

        super(LayerNormalization, self).__init__(**kwargs)
        self.supports_masking = True
        self.center = center
        self.scale = scale
        if epsilon is None:
            epsilon = K.epsilon() * K.epsilon()
        self.epsilon = epsilon
        self.gamma_initializer = keras.initializers.get(gamma_initializer)
        self.beta_initializer = keras.initializers.get(beta_initializer)
      

    def get_config(self):
        config = {
            'center': self.center,
            'scale': self.scale,
            'epsilon': self.epsilon,
            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),
            'beta_initializer': keras.initializers.serialize(self.beta_initializer),
        }
        base_config = super(LayerNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def compute_output_shape(self, input_shape):
        return input_shape

    def compute_mask(self, inputs, input_mask=None):
        return input_mask

    def build(self, input_shape):
        self.input_spec = InputSpec(shape=input_shape)
        shape = input_shape[-1:]
        if self.scale:
            self.gamma = self.add_weight(
                shape=shape,
                initializer=self.gamma_initializer,
                name='gamma',
            )
        if self.center:
            self.beta = self.add_weight(
                shape=shape,
                initializer=self.beta_initializer,
                name='beta',
            )
        super(LayerNormalization, self).build(input_shape)

    def call(self, inputs, training=None):
        mean = K.mean(inputs, axis=-1, keepdims=True)
        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)
        std = K.sqrt(variance + self.epsilon)
        outputs = (inputs - mean) / std
        if self.scale:
            outputs *= self.gamma
        if self.center:
            outputs += self.beta
        return outputs

    
    
class FeedForward(keras.layers.Layer):
  
    
    def __init__(self,
                 units,
                 activation='relu',
                 use_bias=True,
                 kernel_initializer='glorot_normal',
                 bias_initializer='zeros',
                 dropout_rate=0.0,
                 **kwargs):
        self.supports_masking = True
        self.units = units
        self.activation = keras.activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.dropout_rate = dropout_rate
        self.W1, self.b1 = None, None
        self.W2, self.b2 = None, None
        super(FeedForward, self).__init__(**kwargs)

    def get_config(self):
        config = {
            'units': self.units,
            'activation': keras.activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),
            'bias_initializer': keras.initializers.serialize(self.bias_initializer),
            'dropout_rate': self.dropout_rate,
        }
        base_config = super(FeedForward, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def compute_output_shape(self, input_shape):
        return input_shape

    def compute_mask(self, inputs, input_mask=None):
        return input_mask

    def build(self, input_shape):
        feature_dim = int(input_shape[-1])
        self.W1 = self.add_weight(
            shape=(feature_dim, self.units),
            initializer=self.kernel_initializer,
            name='{}_W1'.format(self.name),
        )
        if self.use_bias:
            self.b1 = self.add_weight(
                shape=(self.units,),
                initializer=self.bias_initializer,
                name='{}_b1'.format(self.name),
            )
        self.W2 = self.add_weight(
            shape=(self.units, feature_dim),
            initializer=self.kernel_initializer,
            name='{}_W2'.format(self.name),
        )
        if self.use_bias:
            self.b2 = self.add_weight(
                shape=(feature_dim,),
                initializer=self.bias_initializer,
                name='{}_b2'.format(self.name),
            )
        super(FeedForward, self).build(input_shape)

    def call(self, x, mask=None, training=None):
        h = K.dot(x, self.W1)
        if self.use_bias:
            h = K.bias_add(h, self.b1)
        if self.activation is not None:
            h = self.activation(h)
        if 0.0 < self.dropout_rate < 1.0:
            def dropped_inputs():
                return K.dropout(h, self.dropout_rate, K.shape(h))
            h = K.in_train_phase(dropped_inputs, h, training=training)
        y = K.dot(h, self.W2)
        if self.use_bias:
            y = K.bias_add(y, self.b2)
        return y


class SeqSelfAttention(keras.layers.Layer):
        
    ATTENTION_TYPE_ADD = 'additive'
    ATTENTION_TYPE_MUL = 'multiplicative'

    def __init__(self,
                 units=32,
                 attention_width=None,
                 attention_type=ATTENTION_TYPE_ADD,
                 return_attention=False,
                 history_only=False,
                 kernel_initializer='glorot_normal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 use_additive_bias=True,
                 use_attention_bias=True,
                 attention_activation=None,
                 attention_regularizer_weight=0.0,
                 **kwargs):

        super().__init__(**kwargs)
        self.supports_masking = True
        self.units = units
        self.attention_width = attention_width
        self.attention_type = attention_type
        self.return_attention = return_attention
        self.history_only = history_only
        if history_only and attention_width is None:
            self.attention_width = int(1e9)

        self.use_additive_bias = use_additive_bias
        self.use_attention_bias = use_attention_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)
        self.kernel_constraint = keras.constraints.get(kernel_constraint)
        self.bias_constraint = keras.constraints.get(bias_constraint)
        self.attention_activation = keras.activations.get(attention_activation)
        self.attention_regularizer_weight = attention_regularizer_weight
        self._backend = keras.backend.backend()

        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self.Wx, self.Wt, self.bh = None, None, None
            self.Wa, self.ba = None, None
        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self.Wa, self.ba = None, None
        else:
            raise NotImplementedError('No implementation for attention type : ' + attention_type)

    def get_config(self):
        config = {
            'units': self.units,
            'attention_width': self.attention_width,
            'attention_type': self.attention_type,
            'return_attention': self.return_attention,
            'history_only': self.history_only,
            'use_additive_bias': self.use_additive_bias,
            'use_attention_bias': self.use_attention_bias,
            'kernel_initializer': keras.regularizers.serialize(self.kernel_initializer),
            'bias_initializer': keras.regularizers.serialize(self.bias_initializer),
            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),
            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),
            'bias_constraint': keras.constraints.serialize(self.bias_constraint),
            'attention_activation': keras.activations.serialize(self.attention_activation),
            'attention_regularizer_weight': self.attention_regularizer_weight,
        }
        base_config = super(SeqSelfAttention, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def build(self, input_shape):
        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            self._build_additive_attention(input_shape)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            self._build_multiplicative_attention(input_shape)
        super(SeqSelfAttention, self).build(input_shape)

    def _build_additive_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wt = self.add_weight(shape=(feature_dim, self.units),
                                  name='{}_Add_Wt'.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        self.Wx = self.add_weight(shape=(feature_dim, self.units),
                                  name='{}_Add_Wx'.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_additive_bias:
            self.bh = self.add_weight(shape=(self.units,),
                                      name='{}_Add_bh'.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

        self.Wa = self.add_weight(shape=(self.units, 1),
                                  name='{}_Add_Wa'.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name='{}_Add_ba'.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def _build_multiplicative_attention(self, input_shape):
        feature_dim = int(input_shape[2])

        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),
                                  name='{}_Mul_Wa'.format(self.name),
                                  initializer=self.kernel_initializer,
                                  regularizer=self.kernel_regularizer,
                                  constraint=self.kernel_constraint)
        if self.use_attention_bias:
            self.ba = self.add_weight(shape=(1,),
                                      name='{}_Mul_ba'.format(self.name),
                                      initializer=self.bias_initializer,
                                      regularizer=self.bias_regularizer,
                                      constraint=self.bias_constraint)

    def call(self, inputs, mask=None, **kwargs):
        input_len = K.shape(inputs)[1]

        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:
            e = self._call_additive_emission(inputs)
        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:
            e = self._call_multiplicative_emission(inputs)

        if self.attention_activation is not None:
            e = self.attention_activation(e)
        e = K.exp(e - K.max(e, axis=-1, keepdims=True))
        if self.attention_width is not None:
            if self.history_only:
                lower = K.arange(0, input_len) - (self.attention_width - 1)
            else:
                lower = K.arange(0, input_len) - self.attention_width // 2
            lower = K.expand_dims(lower, axis=-1)
            upper = lower + self.attention_width
            indices = K.expand_dims(K.arange(0, input_len), axis=0)
            e = e * K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx())
        if mask is not None:
            mask = K.cast(mask, K.floatx())
            mask = K.expand_dims(mask)
            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))

        # a_{t} = \text{softmax}(e_t)
        s = K.sum(e, axis=-1, keepdims=True)
        a = e / (s + K.epsilon())

        # l_t = \sum_{t'} a_{t, t'} x_{t'}
        v = K.batch_dot(a, inputs)
        if self.attention_regularizer_weight > 0.0:
            self.add_loss(self._attention_regularizer(a))

        if self.return_attention:
            return [v, a]
        return v

    def _call_additive_emission(self, inputs):
        input_shape = K.shape(inputs)
        batch_size = input_shape[0]
        input_len = inputs.get_shape().as_list()[1]

        # h_{t, t'} = \tanh(x_t^T W_t + x_{t'}^T W_x + b_h)
        q = K.expand_dims(K.dot(inputs, self.Wt), 2)
        k = K.expand_dims(K.dot(inputs, self.Wx), 1)
        if self.use_additive_bias:
            h = K.tanh(q + k + self.bh)
        else:
            h = K.tanh(q + k)

        # e_{t, t'} = W_a h_{t, t'} + b_a
        if self.use_attention_bias:
            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))
        else:
            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))
        return e

    def _call_multiplicative_emission(self, inputs):
        # e_{t, t'} = x_t^T W_a x_{t'} + b_a
        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))
        if self.use_attention_bias:
            e += self.ba[0]
        return e

    def compute_output_shape(self, input_shape):
        output_shape = input_shape
        if self.return_attention:
            attention_shape = (input_shape[0], output_shape[1], input_shape[1])
            return [output_shape, attention_shape]
        return output_shape

    def compute_mask(self, inputs, mask=None):
        if self.return_attention:
            return [mask, None]
        return mask

    def _attention_regularizer(self, attention):
        batch_size = K.cast(K.shape(attention)[0], K.floatx())
        input_len = K.shape(attention)[-1]
        indices = K.expand_dims(K.arange(0, input_len), axis=0)
        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)
        eye = K.cast(K.equal(indices, diagonal), K.floatx())
        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(
            attention,
            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size

    @staticmethod
    def get_custom_objects():
        return {'SeqSelfAttention': SeqSelfAttention}



def _block_BiLSTM(filters, drop_rate, padding, inpR):
    'Returns LSTM residual block'    
    prev = inpR
    # x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=drop_rate))(prev)
    #cudnnLSTM
    x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=0, activation='tanh', recurrent_activation='sigmoid', use_bias=True, unroll=False))(prev)
    NiN = Conv1D(filters, 1, padding = padding)(x_rnn)     
    res_out = BatchNormalization()(NiN)
    return res_out


def _block_CNN_1(filters, ker, drop_rate, activation, padding, inpC): 
    ' Returns CNN residual blocks '
    prev = inpC
    layer_1 = BatchNormalization()(prev) 
    act_1 = Activation(activation)(layer_1) 
    act_1 = SpatialDropout1D(drop_rate)(act_1, training=True)
    conv_1 = Conv1D(filters, ker, padding = padding)(act_1) 
    
    layer_2 = BatchNormalization()(conv_1) 
    act_2 = Activation(activation)(layer_2) 
    act_2 = SpatialDropout1D(drop_rate)(act_2, training=True)
    conv_2 = Conv1D(filters, ker, padding = padding)(act_2)
    
    res_out = add([prev, conv_2])
    
    return res_out 


def _transformer(drop_rate, width, name, inpC): 
    ' Returns a transformer block containing one addetive attention and one feed  forward layer with residual connections '
    x = inpC
    
    att_layer, weight = SeqSelfAttention(return_attention =True,                                       
                                         attention_width = width,
                                         name=name)(x)
   
#  att_layer = Dropout(drop_rate)(att_layer, training=True)    
    att_layer2 = add([x, att_layer])    
    norm_layer = LayerNormalization()(att_layer2)
    
    FF = FeedForward(units=128, dropout_rate=drop_rate)(norm_layer)
    
    FF_add = add([norm_layer, FF])    
    norm_out = LayerNormalization()(FF_add)
    
    return norm_out, weight 

     

def _encoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):
    ' Returns the encoder that is a combination of residual blocks and maxpooling.'        
    e = inpC
    for dp in range(depth):
        e = Conv1D(filter_number[dp], 
                   filter_size[dp], 
                   padding = padding, 
                   activation = activation,
                   kernel_regularizer = ker_regul,
                   bias_regularizer = bias_regul,
                   )(e)             
        e = MaxPooling1D(2, padding = padding)(e)            
    return(e) 


def _decoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):
    ' Returns the dencoder that is a combination of residual blocks and upsampling. '           
    d = inpC
    for dp in range(depth):        
        d = UpSampling1D(2)(d) 
        if dp == 2:
            d = Cropping1D(cropping=(1, 1))(d)           
        d = Conv1D(filter_number[dp], 
                   filter_size[dp], 
                   padding = padding, 
                   activation = activation,
                   kernel_regularizer = ker_regul,
                   bias_regularizer = bias_regul,
                   )(d)        
    return(d)  
 


def _lr_schedule(epoch):
    ' Learning rate is scheduled to be reduced after 40, 60, 80, 90 epochs.'
    
    lr = 1e-3
    if epoch > 90:
        lr *= 0.5e-3
    elif epoch > 60:
        lr *= 1e-3
    elif epoch > 40:
        lr *= 1e-2
    elif epoch > 20:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr



class cred2():
  

    def __init__(self,
                 nb_filters=[8, 16, 16, 32, 32, 96, 96, 128],
                 kernel_size=[11, 9, 7, 7, 5, 5, 3, 3],
                 padding='same',
                 activationf='relu',
                 endcoder_depth=7,
                 decoder_depth=7,
                 cnn_blocks=5,
                 BiLSTM_blocks=3,
                 drop_rate=0.1,
                 loss_weights=[0.2, 0.3, 0.5],
                 loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],                                 
                 kernel_regularizer=keras.regularizers.l1(1e-4),
                 bias_regularizer=keras.regularizers.l1(1e-4),
                 ):
        
        self.kernel_size = kernel_size
        self.nb_filters = nb_filters
        self.padding = padding
        self.activationf = activationf
        self.endcoder_depth= endcoder_depth
        self.decoder_depth= decoder_depth
        self.cnn_blocks= cnn_blocks
        self.BiLSTM_blocks= BiLSTM_blocks     
        self.drop_rate= drop_rate
        self.loss_weights= loss_weights  
        self.loss_types = loss_types       
        self.kernel_regularizer = kernel_regularizer     
        self.bias_regularizer = bias_regularizer 

        
    def __call__(self, inp):

        x = inp
        x = _encoder(self.nb_filters, 
                    self.kernel_size, 
                    self.endcoder_depth, 
                    self.drop_rate, 
                    self.kernel_regularizer, 
                    self.bias_regularizer,
                    self.activationf, 
                    self.padding,
                    x)    
        
        for cb in range(self.cnn_blocks):
            x = _block_CNN_1(self.nb_filters[6], 3, self.drop_rate, self.activationf, self.padding, x)
            if cb > 2:
                x = _block_CNN_1(self.nb_filters[6], 2, self.drop_rate, self.activationf, self.padding, x)

        for bb in range(self.BiLSTM_blocks):
            x = _block_BiLSTM(self.nb_filters[1], self.drop_rate, self.padding, x)

            
        x, weightdD0 = _transformer(self.drop_rate, None, 'attentionD0', x)             
        encoded, weightdD = _transformer(self.drop_rate, None, 'attentionD', x)             
            
        decoder_D = _decoder([i for i in reversed(self.nb_filters)], 
                             [i for i in reversed(self.kernel_size)], 
                             self.decoder_depth, 
                             self.drop_rate, 
                             self.kernel_regularizer, 
                             self.bias_regularizer,
                             self.activationf, 
                             self.padding,                             
                             encoded)
        d = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='detector')(decoder_D)
        '''
        The requirements to use the cuDNN implementation are:

        activation == tanh
        recurrent_activation == sigmoid
        recurrent_dropout == 0
        unroll is False
        use_bias is True
        Inputs, if use masking, are strictly right-padded.
        Eager execution is enabled in the outermost context.
        '''

        # PLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)
        #  self.nb_filters  self.drop_rate 
        PLSTM = LSTM(self.nb_filters[1], 
                    return_sequences=True, 
                    dropout=self.drop_rate, 
                    recurrent_dropout=0, 
                    activation='tanh', 
                    recurrent_activation='sigmoid', 
                    use_bias=True, 
                    unroll=False)(encoded)
        norm_layerP, weightdP = SeqSelfAttention(return_attention=True,
                                                 attention_width= 3,
                                                 name='attentionP')(PLSTM)
        
        decoder_P = _decoder([i for i in reversed(self.nb_filters)], 
                            [i for i in reversed(self.kernel_size)], 
                            self.decoder_depth, 
                            self.drop_rate, 
                            self.kernel_regularizer, 
                            self.bias_regularizer,
                            self.activationf, 
                            self.padding,                            
                            norm_layerP)
        P = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_P')(decoder_P)
        
        # SLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded) 

        SLSTM = LSTM(self.nb_filters[1], 
             return_sequences=True, 
             dropout=self.drop_rate, 
             recurrent_dropout=0, 
             activation='tanh', 
             recurrent_activation='sigmoid', 
             use_bias=True, 
             unroll=False)(encoded)
        norm_layerS, weightdS = SeqSelfAttention(return_attention=True,
                                                 attention_width= 3,
                                                 name='attentionS')(SLSTM)
        
        
        decoder_S = _decoder([i for i in reversed(self.nb_filters)], 
                            [i for i in reversed(self.kernel_size)],
                            self.decoder_depth, 
                            self.drop_rate, 
                            self.kernel_regularizer, 
                            self.bias_regularizer,
                            self.activationf, 
                            self.padding,                            
                            norm_layerS) 
        
        S = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_S')(decoder_S)
        

        model = keras.models.Model(inputs=inp, outputs=[d, P, S])

        model.compile(loss=self.loss_types, loss_weights=self.loss_weights,    
            optimizer=Adam(lr=_lr_schedule(0)), metrics=[f1])

        return model
    

# input=keras.layers.Input(shape=(12000,3))
# model=cred2()(input)
# model.summary()
```

### Relevant log output

```shell

```",Chuan1937,2025-01-24 13:47:24+00:00,['tilakrayal'],2025-02-04 01:59:40+00:00,,https://github.com/tensorflow/tensorflow/issues/85667,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('TF 2.10', '')]","[{'comment_id': 2612662336, 'issue_id': 2809503213, 'author': 'mihaimaruseac', 'body': 'This is likely the time spent JIT-ing the Python imperative code to the graph representation that TF uses.', 'created_at': datetime.datetime(2025, 1, 24, 14, 28, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613721113, 'issue_id': 2809503213, 'author': 'Chuan1937', 'body': 'How can I speed it up? Otherwise, I have to wait for nearly 60 minutes every time.', 'created_at': datetime.datetime(2025, 1, 25, 1, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614015645, 'issue_id': 2809503213, 'author': 'mihaimaruseac', 'body': ""I think that that's too much. You could try writing TF code in graph mode directly (or switching to JAX for even more performance gains -- since you use Keras, use Keras 3 and switch to Jax backend)."", 'created_at': datetime.datetime(2025, 1, 25, 16, 11, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614045841, 'issue_id': 2809503213, 'author': 'TechieGbemi', 'body': 'It is normal for the first epoch to take longer to train than subsequent epochs, as TensorFlow needs to compile the model and allocate resources. However, an hour does seem like a long time. \n\n**Here are a few things that could be causing the slow training time:**\n\n* Larger models with more parameters will take longer to train.\n* Training on a larger dataset will take longer.\n* The speed of your CPU, GPU, and available RAM will all affect training speed.\n* If you are using custom code in your training loop, it could be slowing things down.\n\n**Here are some tips to improve training speed:**\n\n* If you are just starting out, try using a smaller model with fewer parameters.\n* If you have a large dataset, try using a smaller subset for training.\n* If you have a GPU, you can use it to accelerate training.\n* Use a profiler to identify bottlenecks in your training code.\n* A smaller batch size can sometimes improve training speed.', 'created_at': datetime.datetime(2025, 1, 25, 17, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632586801, 'issue_id': 2809503213, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 4, 1, 59, 39, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-24 14:28:10 UTC): This is likely the time spent JIT-ing the Python imperative code to the graph representation that TF uses.

Chuan1937 (Issue Creator) on (2025-01-25 01:59:40 UTC): How can I speed it up? Otherwise, I have to wait for nearly 60 minutes every time.

mihaimaruseac on (2025-01-25 16:11:01 UTC): I think that that's too much. You could try writing TF code in graph mode directly (or switching to JAX for even more performance gains -- since you use Keras, use Keras 3 and switch to Jax backend).

TechieGbemi on (2025-01-25 17:50:09 UTC): It is normal for the first epoch to take longer to train than subsequent epochs, as TensorFlow needs to compile the model and allocate resources. However, an hour does seem like a long time. 

**Here are a few things that could be causing the slow training time:**

* Larger models with more parameters will take longer to train.
* Training on a larger dataset will take longer.
* The speed of your CPU, GPU, and available RAM will all affect training speed.
* If you are using custom code in your training loop, it could be slowing things down.

**Here are some tips to improve training speed:**

* If you are just starting out, try using a smaller model with fewer parameters.
* If you have a large dataset, try using a smaller subset for training.
* If you have a GPU, you can use it to accelerate training.
* Use a profiler to identify bottlenecks in your training code.
* A smaller batch size can sometimes improve training speed.

github-actions[bot] on (2025-02-04 01:59:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2808511867,issue,open,,LiteRT build for Android failing,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

bazel 6.5.0

### GCC/compiler version

NDK26,NDK28

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following this link https://ai.google.dev/edge/litert/build/android for building libtensorflowlite.so for my android JNI project, but its failing all  the time with below error snapshot

**Initial Steps**
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.17.0
./configure ( For Android Environment)

**Below is the content of .tf_configure.bazelrc in my build environment**

build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3.10/dist-packages""
build --python_path=""/usr/bin/python3""
build --action_env CLANG_COMPILER_PATH=""/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17""
build --repo_env=CC=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17
build --repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17
build --copt=-Wno-gnu-offsetof-extensions
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-Wno-sign-compare
build --action_env ANDROID_NDK_HOME=""/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/""
build --action_env ANDROID_NDK_VERSION=""26""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""35.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""35""
build --action_env ANDROID_SDK_HOME=""/media/avaish/aiwork/Android-sdk/""
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-v1only


**Expected Output**
 libtensorflowlite.so should get built successfully. 


### Standalone code to reproduce the issue

```shell
**Build Command**
avaish@avaish-dekstop:/media/avaish/linux-games/litebuild/tensorflow$ bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64   --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --define=android_dexmerger_tool=d8_dexmerger   --define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflow-lite
```

### Relevant log output

```shell
INFO: Reading 'startup' options from /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=153
INFO: Reading rc options for 'build' from /media/avaish/linux-games/litebuild/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /media/avaish/linux-games/litebuild/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /media/avaish/linux-games/litebuild/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.10/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=CC=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions --action_env ANDROID_NDK_HOME=/media/avaish/aiwork/Android-sdk/ndk/26.1.10909125/ --action_env ANDROID_NDK_VERSION=26 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=35.0.0 --action_env ANDROID_SDK_API_LEVEL=35 --action_env ANDROID_SDK_HOME=/media/avaish/aiwork/Android-sdk/
INFO: Found applicable config definition build:short_logs in file /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --define=xnn_enable_avxvnniint8=false --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /media/avaish/linux-games/litebuild/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Analyzed target //tensorflow/lite/java:tensorflow-lite (2 packages loaded, 8416 targets configured).
INFO: Found 1 target...
ERROR: /media/avaish/linux-games/litebuild/tensorflow/tensorflow/lite/c/jni/BUILD:12:43: Compiling tensorflow/lite/c/jni/jni_utils.cc failed: undeclared inclusion(s) in rule '//tensorflow/lite/c/jni:jni_utils':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/c/jni/jni_utils.cc':
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdarg.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdint.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stddef.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/__stddef_max_align_t.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdbool.h'
Target //tensorflow/lite/java:tensorflow-lite failed to build
Use --verbose_failures to see the command lines of failed build steps.
```",intelav,2025-01-24 05:07:38+00:00,['gaikwadrahul8'],2025-02-05 02:00:36+00:00,,https://github.com/tensorflow/tensorflow/issues/85631,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2611624977, 'issue_id': 2808511867, 'author': 'intelav', 'body': ""If I run the same build command again , similiar errors apperas from compiling other files\n\nERROR: /home/avaish/.cache/bazel/_bazel_avaish/d98cf14fd195122b7f9fe191efe765ef/external/ruy/ruy/BUILD:423:11: Compiling ruy/denormal.cc failed: undeclared inclusion(s) in rule '@ruy//ruy:denormal':\nthis rule is missing dependency declarations for the following files included by 'ruy/denormal.cc':\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdint.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stddef.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/__stddef_max_align_t.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/xmmintrin.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/mmintrin.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/mm_malloc.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdarg.h'\n  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/emmintrin.h'\nTarget //tensorflow/lite/java:tensorflow-lite failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 0.903s, Critical Path: 0.12s\nINFO: 12 processes: 12 internal.\nFAILED: Build did NOT complete successfully"", 'created_at': datetime.datetime(2025, 1, 24, 5, 22, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619501097, 'issue_id': 2808511867, 'author': 'gaikwadrahul8', 'body': ""Hi, @intelav\nI apologize for the delay in my response, I was trying to replicate the same behavior from my end but I'm getting different error and Build did NOT complete successfully so I have added error log below for reference please let me know if Am I missing something here to replicate same behavior which you reported here ?\n\n```\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/TFLite-Issue-#85631/tensorflow$ bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64   --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --define=android_dexmerger_tool=d8_dexmerger   --define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflow-lite\nExtracting Bazel installation...\nStarting local Bazel server and connecting to it...\nINFO: Reading 'startup' options from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/site-packages --python_path=/home/gaikwadrahul/miniconda3/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-17 --repo_env=CC=/usr/bin/clang-17 --repo_env=BAZEL_COMPILER=/usr/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions --action_env ANDROID_NDK_HOME=/home/gaikwadrahul/android-ndk-r25b --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=34.0.0 --action_env ANDROID_SDK_API_LEVEL=33 --action_env ANDROID_SDK_HOME=/home/gaikwadrahul/Android/Sdk\nINFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:android_arm64 in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\nINFO: Found applicable config definition build:android in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt\nINFO: Found applicable config definition build:no_tfrt in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\nDEBUG: /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/tensorflow/tools/toolchains/python/python_repo.bzl:32:14: \nTF_PYTHON_VERSION environment variable was not set correctly; using Python 3.11.\n\nTo set Python version, run:\nexport TF_PYTHON_VERSION=3.11\nINFO: Analyzed target //tensorflow/lite/java:tensorflow-lite (146 packages loaded, 15447 targets configured).\nINFO: Found 1 target...\nERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/17e9d465a5125118a485d68d85fbf68a/external/flatbuffers/src/BUILD.bazel:19:11: Compiling src/code_generators.cpp [for tool] failed: (Exit 1): clang-17 failed: error executing command (from target @flatbuffers//src:code_generators) /usr/bin/clang-17 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 ... (remaining 27 arguments skipped)\nIn file included from external/flatbuffers/src/code_generators.cpp:17:\nbazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/src/_virtual_includes/code_generators/flatbuffers/code_generators.h:20:10: fatal error: 'map' file not found\n   20 | #include <map>\n      |          ^~~~~\n1 error generated.\nTarget //tensorflow/lite/java:tensorflow-lite failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 126.844s, Critical Path: 12.54s\nINFO: 84 processes: 61 internal, 23 local.\nFAILED: Build did NOT complete successfully\n```\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 28, 16, 32, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534228, 'issue_id': 2808511867, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 35, tzinfo=datetime.timezone.utc)}]","intelav (Issue Creator) on (2025-01-24 05:22:16 UTC): If I run the same build command again , similiar errors apperas from compiling other files

ERROR: /home/avaish/.cache/bazel/_bazel_avaish/d98cf14fd195122b7f9fe191efe765ef/external/ruy/ruy/BUILD:423:11: Compiling ruy/denormal.cc failed: undeclared inclusion(s) in rule '@ruy//ruy:denormal':
this rule is missing dependency declarations for the following files included by 'ruy/denormal.cc':
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdint.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stddef.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/__stddef_max_align_t.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/xmmintrin.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/mmintrin.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/mm_malloc.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/stdarg.h'
  'external/androidndk/toolchains/llvm/prebuilt/linux-x86_64/lib/clang/17/include/emmintrin.h'
Target //tensorflow/lite/java:tensorflow-lite failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.903s, Critical Path: 0.12s
INFO: 12 processes: 12 internal.
FAILED: Build did NOT complete successfully

gaikwadrahul8 (Assginee) on (2025-01-28 16:32:10 UTC): Hi, @intelav
I apologize for the delay in my response, I was trying to replicate the same behavior from my end but I'm getting different error and Build did NOT complete successfully so I have added error log below for reference please let me know if Am I missing something here to replicate same behavior which you reported here ?

```
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/TFLite-Issue-#85631/tensorflow$ bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64   --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --define=android_dexmerger_tool=d8_dexmerger   --define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflow-lite
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/site-packages --python_path=/home/gaikwadrahul/miniconda3/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-17 --repo_env=CC=/usr/bin/clang-17 --repo_env=BAZEL_COMPILER=/usr/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions --action_env ANDROID_NDK_HOME=/home/gaikwadrahul/android-ndk-r25b --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=34.0.0 --action_env ANDROID_SDK_API_LEVEL=33 --action_env ANDROID_SDK_HOME=/home/gaikwadrahul/Android/Sdk
INFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /home/gaikwadrahul/TFLite-Issue-#85631/tensorflow/tensorflow/tools/toolchains/python/python_repo.bzl:32:14: 
TF_PYTHON_VERSION environment variable was not set correctly; using Python 3.11.

To set Python version, run:
export TF_PYTHON_VERSION=3.11
INFO: Analyzed target //tensorflow/lite/java:tensorflow-lite (146 packages loaded, 15447 targets configured).
INFO: Found 1 target...
ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/17e9d465a5125118a485d68d85fbf68a/external/flatbuffers/src/BUILD.bazel:19:11: Compiling src/code_generators.cpp [for tool] failed: (Exit 1): clang-17 failed: error executing command (from target @flatbuffers//src:code_generators) /usr/bin/clang-17 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 ... (remaining 27 arguments skipped)
In file included from external/flatbuffers/src/code_generators.cpp:17:
bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/src/_virtual_includes/code_generators/flatbuffers/code_generators.h:20:10: fatal error: 'map' file not found
   20 | #include <map>
      |          ^~~~~
1 error generated.
Target //tensorflow/lite/java:tensorflow-lite failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 126.844s, Critical Path: 12.54s
INFO: 84 processes: 61 internal, 23 local.
FAILED: Build did NOT complete successfully
```

Thank you for your cooperation and patience.

github-actions[bot] on (2025-02-05 02:00:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2808360395,issue,closed,duplicate,Can not import tensorflow as tf,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

### Standalone code to reproduce the issue

```shell
i always use python with VS Code, and yesterday my code finally success. but today when I run these code ""import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense"" . it always become error, even I had uninstall and install the tensorflow again, it not helped
```

### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[52], line 1
----> 1 import tensorflow as tf
      2 from tensorflow.keras.models import Sequential
      3 from tensorflow.keras.layers import Dense

File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
...


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```",ghinaaraf,2025-01-24 02:39:30+00:00,['tilakrayal'],2025-01-24 14:58:36+00:00,2025-01-24 14:58:32+00:00,https://github.com/tensorflow/tensorflow/issues/85629,"[('type:performance', 'Performance Issue')]","[{'comment_id': 2612734600, 'issue_id': 2808360395, 'author': 'mihaimaruseac', 'body': 'Please search for duplicates before opening a new issue', 'created_at': datetime.datetime(2025, 1, 24, 14, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612734687, 'issue_id': 2808360395, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85629"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85629"">No</a>', 'created_at': datetime.datetime(2025, 1, 24, 14, 58, 35, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-24 14:58:32 UTC): Please search for duplicates before opening a new issue

google-ml-butler[bot] on (2025-01-24 14:58:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85629"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85629"">No</a>

"
2808004638,issue,open,,TensorFlow warning shows whenever importing it,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.10 x86_64

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA:12.6

### GPU model and memory

_No response_

### Current behavior?

`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`

- OS: Ubuntu 24.10 x86_64
- Host: G5 5590
- Kernel: 6.11.0-13-generic
- CPU: Intel i7-9750H (12) @ 4.500GHz
- GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q
- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]
> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
> output:
```
2025-01-23 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-23 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

### Relevant log output

```shell

```",MahmoudBahar,2025-01-23 21:56:39+00:00,['Venkat6871'],2025-02-05 03:56:08+00:00,,https://github.com/tensorflow/tensorflow/issues/85604,"[('type:bug', 'Bug'), ('2.18.rc', '')]","[{'comment_id': 2614944494, 'issue_id': 2808004638, 'author': 'Venkat6871', 'body': 'Hi **@MahmoudBahar** ,\nApologies for the delay, and thank you for raising your concern here. It seems there might be a version mismatch. Could you please verify the compatibility of all the versions you are using? For your reference, I have provided relevant [documentation](https://www.tensorflow.org/install/source#gpu) below.\nThank you!', 'created_at': datetime.datetime(2025, 1, 27, 6, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632586839, 'issue_id': 2808004638, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 4, 1, 59, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635646274, 'issue_id': 2808004638, 'author': 'MahmoudBahar', 'body': 'Hello @Venkat6871,\nsorry for replying lately.\nI checked the versions as in the documentation you sent me as following:\n\nI am working in conda enviroment with the following configs:\npython=3.12.8\n\nrunning the following code to check the version of `TensorFlow`:\n```python\nimport tensorflow as tf\nprint(tf.__version__)\n```\nwould get the following output:\n```bash\n2025-02-05 05:42:43.524743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738726963.539270   17574 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738726963.543483   17574 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 05:42:43.558701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2.18.0\n```\nshowing that `TensorFlow` installed with version `2.18.0`\n\nrunning the following code in terminal to check compiler and build tools would get that output:\n```bash\n(tf) username@Name:~$ python -c ""import tensorflow as tf; print(tf.sysconfig.get_compile_flags())""\n2025-02-05 05:47:22.629352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738727242.644411   17974 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738727242.648762   17974 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 05:47:22.664596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[\'-I/home/mahmoudbahar/anaconda3/envs/tf/lib/python3.12/site-packages/tensorflow/include\', \'-D_GLIBCXX_USE_CXX11_ABI=1\', \'--std=c++17\', \'-DEIGEN_MAX_ALIGN_BYTES=64\']\n(tf) username@Name:~$ python -c ""import tensorflow as tf; print(tf.sysconfig.get_build_info())""\n2025-02-05 05:47:39.771357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738727259.787632   18027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738727259.792173   18027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 05:47:39.808482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nOrderedDict({\'cpu_compiler\': \'/usr/lib/llvm-18/bin/clang\', \'cuda_compute_capabilities\': [\'sm_60\', \'sm_70\', \'sm_80\', \'sm_89\', \'compute_90\'], \'cuda_version\': \'12.5.1\', \'cudnn_version\': \'9\', \'is_cuda_build\': True, \'is_rocm_build\': False, \'is_tensorrt_build\': False})\n```\nrunning the following code to check if `TensorFlow` can recognize the cudnn version:\n```python\nimport tensorflow as tf\nprint(tf.sysconfig.get_build_info()[\'cudnn_version\'])\n```\noutputs the following:\n```bash\n2025-02-05 05:31:31.865306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738726291.880059   16607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738726291.884535   16607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 05:31:31.900147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n9\n```\nshowing that `cuDNN` installed with the package is version is `9`\n\nrunning the following code to check if `TensorFlow` can recognize the cuda version:\n```python\nimport tensorflow as tf\nprint(tf.sysconfig.get_build_info()[\'cuda_version\'])\n```\noutputs the following:\n```bash\n2025-02-05 05:33:28.844110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738726408.859348   16754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738726408.863713   16754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 05:33:28.880097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n12.5.1\n```\nshowing that that `CUDA` installed with the package is `12.5.1`\n\n`nvidia-cuda-toolkit` is not installed globally in my machine.\nso the following would be expected:\n```bash\n(tf) username@Name:~$ nvcc --version\nCommand \'nvcc\' not found, but can be installed with:\nsudo apt install nvidia-cuda-toolkit\n```\nThere is a CUDA version installed globally which is 12.6 so the following output is as expected:\n```bash\n(tf) username@Name:~$ nvidia-smi\nWed Feb  5 05:39:56 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0 Off |                  N/A |\n| N/A   41C    P8              4W /   50W |       4MiB /   4096MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      3702      G   /usr/bin/gnome-shell                            1MiB |\n+-----------------------------------------------------------------------------------------+\n```\n\n> [!IMPORTANT]\n> running the following commands in terminal would show that they are not installed globally\n> cmake --version\n> bazel --version', 'created_at': datetime.datetime(2025, 2, 5, 3, 56, 4, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-27 06:32:40 UTC): Hi **@MahmoudBahar** ,
Apologies for the delay, and thank you for raising your concern here. It seems there might be a version mismatch. Could you please verify the compatibility of all the versions you are using? For your reference, I have provided relevant [documentation](https://www.tensorflow.org/install/source#gpu) below.
Thank you!

github-actions[bot] on (2025-02-04 01:59:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

MahmoudBahar (Issue Creator) on (2025-02-05 03:56:04 UTC): Hello @Venkat6871,
sorry for replying lately.
I checked the versions as in the documentation you sent me as following:

I am working in conda enviroment with the following configs:
python=3.12.8

running the following code to check the version of `TensorFlow`:
```python
import tensorflow as tf
print(tf.__version__)
```
would get the following output:
```bash
2025-02-05 05:42:43.524743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738726963.539270   17574 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738726963.543483   17574 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-05 05:42:43.558701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2.18.0
```
showing that `TensorFlow` installed with version `2.18.0`

running the following code in terminal to check compiler and build tools would get that output:
```bash
(tf) username@Name:~$ python -c ""import tensorflow as tf; print(tf.sysconfig.get_compile_flags())""
2025-02-05 05:47:22.629352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738727242.644411   17974 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738727242.648762   17974 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-05 05:47:22.664596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
['-I/home/mahmoudbahar/anaconda3/envs/tf/lib/python3.12/site-packages/tensorflow/include', '-D_GLIBCXX_USE_CXX11_ABI=1', '--std=c++17', '-DEIGEN_MAX_ALIGN_BYTES=64']
(tf) username@Name:~$ python -c ""import tensorflow as tf; print(tf.sysconfig.get_build_info())""
2025-02-05 05:47:39.771357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738727259.787632   18027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738727259.792173   18027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-05 05:47:39.808482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
OrderedDict({'cpu_compiler': '/usr/lib/llvm-18/bin/clang', 'cuda_compute_capabilities': ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90'], 'cuda_version': '12.5.1', 'cudnn_version': '9', 'is_cuda_build': True, 'is_rocm_build': False, 'is_tensorrt_build': False})
```
running the following code to check if `TensorFlow` can recognize the cudnn version:
```python
import tensorflow as tf
print(tf.sysconfig.get_build_info()['cudnn_version'])
```
outputs the following:
```bash
2025-02-05 05:31:31.865306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738726291.880059   16607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738726291.884535   16607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-05 05:31:31.900147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
9
```
showing that `cuDNN` installed with the package is version is `9`

running the following code to check if `TensorFlow` can recognize the cuda version:
```python
import tensorflow as tf
print(tf.sysconfig.get_build_info()['cuda_version'])
```
outputs the following:
```bash
2025-02-05 05:33:28.844110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738726408.859348   16754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738726408.863713   16754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-05 05:33:28.880097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
12.5.1
```
showing that that `CUDA` installed with the package is `12.5.1`

`nvidia-cuda-toolkit` is not installed globally in my machine.
so the following would be expected:
```bash
(tf) username@Name:~$ nvcc --version
Command 'nvcc' not found, but can be installed with:
sudo apt install nvidia-cuda-toolkit
```
There is a CUDA version installed globally which is 12.6 so the following output is as expected:
```bash
(tf) username@Name:~$ nvidia-smi
Wed Feb  5 05:39:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0 Off |                  N/A |
| N/A   41C    P8              4W /   50W |       4MiB /   4096MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      3702      G   /usr/bin/gnome-shell                            1MiB |
+-----------------------------------------------------------------------------------------+
```

"
2807705290,issue,closed,duplicate,Pip cant find TensorFlow package,"i tried installing the package by running ""pip install tensorflow"" but it always returns a error that it couldn't resolve the package name. i cant tell if there is a problem with my project or im just to dumb to install it",xXExilXx,2025-01-23 19:08:57+00:00,['tilakrayal'],2025-01-24 14:08:25+00:00,2025-01-24 14:08:00+00:00,https://github.com/tensorflow/tensorflow/issues/85595,[],"[{'comment_id': 2610861427, 'issue_id': 2807705290, 'author': 'mihaimaruseac', 'body': ""Please post information about your system, operating system, version of Python you are using, what command you are running, what is the error. Use \\`\\`\\` to wrap around code/error blocks. Don't post images.\n\nRight now, there is really no information to go on from your post. You can also search for similar issues (e.g. #85298 could be a candidate?)"", 'created_at': datetime.datetime(2025, 1, 23, 19, 38, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612457143, 'issue_id': 2807705290, 'author': 'xXExilXx', 'body': 'my sys info:\n```\nOperating System:\n\nMicrosoft Windows [Version 10.0.26100.2894]\n\nPython Version:\nPython 3.13.1\n\nSystem Architecture:\n64-bit\n\nMachine Name:\nEXILS\n\nCurrent User:\ndeion\n\nSystem Memory:\n\nManufacturer    Capacity Speed\n------------    -------- -----\nUnknown      17179869184 2667\n\n\nCPU Information:\n\nName                                            NumberOfCores MaxClockSpeed\n----                                            ------------- -------------\nAMD Ryzen 5 3600 6-Core Processor                           6          3600\n\n\nGPU Information:\n\nName\n----\nNVIDIA GeForce RTX 2060\n```\n\nThe console log: \n```\nWindows PowerShell\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nInstall the latest PowerShell for new features and improvements! https://aka.ms/PSWindows\n\n(.venv) PS C:\\Users\\deion\\PycharmProjects\\MusicMLM> pip install tensorflow\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\nERROR: No matching distribution found for tensorflow\n(.venv) PS C:\\Users\\deion\\PycharmProjects\\MusicMLM> \n```', 'created_at': datetime.datetime(2025, 1, 24, 12, 49, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612617429, 'issue_id': 2807705290, 'author': 'mihaimaruseac', 'body': ""It's a duplicate, please search for similar issues here before making new ones"", 'created_at': datetime.datetime(2025, 1, 24, 14, 8, 23, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-23 19:38:28 UTC): Please post information about your system, operating system, version of Python you are using, what command you are running, what is the error. Use \`\`\` to wrap around code/error blocks. Don't post images.

Right now, there is really no information to go on from your post. You can also search for similar issues (e.g. #85298 could be a candidate?)

xXExilXx (Issue Creator) on (2025-01-24 12:49:54 UTC): my sys info:
```
Operating System:

Microsoft Windows [Version 10.0.26100.2894]

Python Version:
Python 3.13.1

System Architecture:
64-bit

Machine Name:
EXILS

Current User:
deion

System Memory:

Manufacturer    Capacity Speed
------------    -------- -----
Unknown      17179869184 2667


CPU Information:

Name                                            NumberOfCores MaxClockSpeed
----                                            ------------- -------------
AMD Ryzen 5 3600 6-Core Processor                           6          3600


GPU Information:

Name
----
NVIDIA GeForce RTX 2060
```

The console log: 
```
Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows

(.venv) PS C:\Users\deion\PycharmProjects\MusicMLM> pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
(.venv) PS C:\Users\deion\PycharmProjects\MusicMLM> 
```

mihaimaruseac on (2025-01-24 14:08:23 UTC): It's a duplicate, please search for similar issues here before making new ones

"
2806474467,issue,closed,completed,Compatibility table for TensorFlow 2.18 with CUDA and cuDNN is missing on the official website,"In previous versions of TensorFlow, the official documentation included a clear compatibility table specifying which versions of TensorFlow worked with specific versions of CUDA and cuDNN. However, upon reviewing the documentation for TensorFlow 2.18, I noticed this information is no longer available.

These tables were  helpful for users, as they prevented installation and compatibility issues when setting up the development environment. It would be great if this compatibility table could be brought back to the official documentation. If it was removed due to any updates or errors that have been identified, it would also be helpful to notify users of such changes.

If you're reading this issue, I kindly ask that you notify users if any updates or changes regarding this topic are implemented.
",Redoxon33,2025-01-23 10:09:06+00:00,['Venkat6871'],2025-01-23 10:17:56+00:00,2025-01-23 10:17:55+00:00,https://github.com/tensorflow/tensorflow/issues/85560,[],"[{'comment_id': 2609414855, 'issue_id': 2806474467, 'author': 'Redoxon33', 'body': 'After further investigation, I discovered that the issue is specific to the Spanish versions of the documentation (both Latin American and European Spanish), which appear to be outdated compared to the English version. The compatibility table is available in the English documentation, but it is not present in the Spanish translations.\n\nIt would be helpful if an indicator could be added to show that the Spanish documentation is outdated or missing specific information, to prevent confusion for users relying on these versions.', 'created_at': datetime.datetime(2025, 1, 23, 10, 17, 55, tzinfo=datetime.timezone.utc)}]","Redoxon33 (Issue Creator) on (2025-01-23 10:17:55 UTC): After further investigation, I discovered that the issue is specific to the Spanish versions of the documentation (both Latin American and European Spanish), which appear to be outdated compared to the English version. The compatibility table is available in the English documentation, but it is not present in the Spanish translations.

It would be helpful if an indicator could be added to show that the Spanish documentation is outdated or missing specific information, to prevent confusion for users relying on these versions.

"
2803652742,issue,closed,completed,issue,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

import tensorflow as tf
from google.colab import files

uploaded_zip_file = list(files.upload().keys())[0]
!unzip $uploaded_zip_file
tf_model = tf.keras.models.load_model(uploaded_zip_file[:-4])

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from google.colab import files

uploaded_zip_file = list(files.upload().keys())[0]
!unzip $uploaded_zip_file
tf_model = tf.keras.models.load_model(uploaded_zip_file[:-4])
```

### Relevant log output

```shell

```",rizkyy702,2025-01-22 07:52:23+00:00,['Venkat6871'],2025-01-22 14:11:24+00:00,2025-01-22 14:11:21+00:00,https://github.com/tensorflow/tensorflow/issues/85449,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2607354049, 'issue_id': 2803652742, 'author': 'mihaimaruseac', 'body': ""Closing as there's no error message, no informative title and the template doesn't seem to be filled correctly (TF 2.0 and nightly surely have diverged)"", 'created_at': datetime.datetime(2025, 1, 22, 14, 11, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607354154, 'issue_id': 2803652742, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85449"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85449"">No</a>', 'created_at': datetime.datetime(2025, 1, 22, 14, 11, 24, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-22 14:11:21 UTC): Closing as there's no error message, no informative title and the template doesn't seem to be filled correctly (TF 2.0 and nightly surely have diverged)

google-ml-butler[bot] on (2025-01-22 14:11:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85449"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85449"">No</a>

"
2803220067,issue,closed,completed,Spam,Spam removed,RyanRankeval,2025-01-22 02:32:47+00:00,['tilakrayal'],2025-01-22 14:13:09+00:00,2025-01-22 14:12:51+00:00,https://github.com/tensorflow/tensorflow/issues/85439,[],"[{'comment_id': 2607357775, 'issue_id': 2803220067, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2025, 1, 22, 14, 12, 51, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-22 14:12:51 UTC): Please don't spam

"
2802707579,issue,closed,completed,How do you install this using poetry on macos ?,"This still doesn't work 


```
[project]
name = ""tf-001""
version = ""0.1.0""
description = """"
authors = [
    {name = ""Me""}
]
readme = ""README.md""
requires-python = "">=3.11""

[tool.poetry.dependencies]
python = ""~3.11""
tensorflow = ""*""

[build-system]
requires = [""poetry-core>=2.0.0,<3.0.0""]
build-backend = ""poetry.core.masonry.api""

```


```
  - Installing tensorflow (2.18.0): Failed

  RuntimeError

  Unable to find installation candidates for tensorflow (2.18.0)

  at ~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/poetry/installation/chooser.py:86 in choose_for
       82 
       83             links.append(link)
       84 
       85         if not links:
      86             raise RuntimeError(f""Unable to find installation candidates for {package}"")
       87 
       88         # Get the best link
       89         chosen = max(links, key=lambda link: self._sort_key(package, link))
       90 

Cannot install tensorflow.
```

```
$ poetry --version 
Poetry (version 2.0.0)
```


So how do I install this ?",primski,2025-01-21 20:05:28+00:00,['Venkat6871'],2025-02-05 02:00:41+00:00,2025-02-05 02:00:38+00:00,https://github.com/tensorflow/tensorflow/issues/85418,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2605689708, 'issue_id': 2802707579, 'author': 'primski', 'body': 'I removed it from pyproject.toml and used pip, it works with that.', 'created_at': datetime.datetime(2025, 1, 21, 20, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605724979, 'issue_id': 2802707579, 'author': 'mihaimaruseac', 'body': 'TF is developed with pip in mind, no one has tested if it can be installed with other systems (although likely it should, to the extent that they are compatible)', 'created_at': datetime.datetime(2025, 1, 21, 20, 58, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469048, 'issue_id': 2802707579, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534275, 'issue_id': 2802707579, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534312, 'issue_id': 2802707579, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85418"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85418"">No</a>', 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 39, tzinfo=datetime.timezone.utc)}]","primski (Issue Creator) on (2025-01-21 20:37:55 UTC): I removed it from pyproject.toml and used pip, it works with that.

mihaimaruseac on (2025-01-21 20:58:02 UTC): TF is developed with pip in mind, no one has tested if it can be installed with other systems (although likely it should, to the extent that they are compatible)

github-actions[bot] on (2025-01-29 01:59:01 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-05 02:00:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-05 02:00:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85418"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85418"">No</a>

"
2801215487,issue,open,,Tensorflow 2.14.0 installation/run on C++ in visual studio code,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

macos 14.4

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi I would like to try to run a custom model on C++, how should I setup the tensorflow so that i could call and use function like i did in python?

### Standalone code to reproduce the issue

```shell
tried bazel build //tensorflow/tools/pip_package, yet it pops up error 
Skipping '//tensorflow/tools/pip_package': no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
```

### Relevant log output

```shell

```",alvinwong64,2025-01-21 09:26:43+00:00,['tilakrayal'],2025-02-07 06:30:08+00:00,,https://github.com/tensorflow/tensorflow/issues/85385,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('awaiting PR merge', 'awaiting PR merge'), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2604684811, 'issue_id': 2801215487, 'author': 'arzoo0511', 'body': 'you need to set up TensorFlow for C++ development, the target //tensorflow/tools/pip_package:pip_package is specifically for creating Python pip packages, not for compiling TensorFlow for C++ use.\n Install TensorFlow C++ Library,Build TensorFlow C++ API,Configure Build Settings,Build TensorFlow Shared Libraries then load and run your model', 'created_at': datetime.datetime(2025, 1, 21, 13, 5, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604695326, 'issue_id': 2801215487, 'author': 'alvinwong64', 'body': 'Could you guide me on how to install tensorflow c++ library and build it?', 'created_at': datetime.datetime(2025, 1, 21, 13, 10, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604711493, 'issue_id': 2801215487, 'author': 'arzoo0511', 'body': ""yes ofc ! \nSystem Requirements: Linux-based system,C++ compiler ,Python installed,Bazel \nInstall C++ Build Dependencies: \nsudo apt-get install build-essential\nInstall Bazelisk \ncurl -LO https://github.com/bazelbuild/bazelisk/releases/download/v1.10.0/bazelisk-linux-amd64\nchmod +x bazelisk-linux-amd64\nmv bazelisk-linux-amd64 /usr/local/bin/bazel\n\nStep-by-Step Installation:\nClone the TensorFlow GitHub Repository\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\n\nConfigure the Build\n./configure\nYoull be prompted with several configuration options, including whether to enable CUDA (GPU support) and whether to build TensorFlow for specific systems or features. For building with GPU support, youll need to have the CUDA toolkit and cuDNN installed.\n\nBuild the C++ Library\nbazel build //tensorflow:libtensorflow_cc.so\n\nThis will build the TensorFlow C++ library as a shared object (.so) file. If you want to build a static library, you can modify the build command accordingly.\n\nFor debugging and testing, you want to build the tensorflow_cc library with the -c dbg flag:\nbazel build -c dbg //tensorflow:libtensorflow_cc.so\nThis will take some time, depending on your system's hardware.\n\nInclude TensorFlow C++ Headers and Libraries located in the bazel-bin/ directory. \nbazel-bin/ tensorflow"", 'created_at': datetime.datetime(2025, 1, 21, 13, 17, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605081752, 'issue_id': 2801215487, 'author': 'alvinwong64', 'body': ""Hi while building, the build failed, with the use of --verbose_failures\n\nERROR: /Users/001522/Documents/testing/tensorflow/tensorflow/lite/acceleration/configuration/BUILD:41:8: Executing genrule //tensorflow/lite/acceleration/configuration:configuration_schema failed: (Exit 127): bash failed: error executing command (from target //tensorflow/lite/acceleration/configuration:configuration_schema) \n  (cd /private/var/tmp/_bazel_001522/497fb1666e8c0e255ebeec937d6e9115/execroot/org_tensorflow && \\\n  exec env - \\\n    PATH=/Users/001522/Library/Caches/bazelisk/downloads/sha256/c6b6dc17efcdf13fba484c6fe0b6c3361b888ae7b9573bc25a2dbe8c502448eb/bin:/Users/001522/anaconda3/bin:/Users/001522/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools \\\n    PYTHON_BIN_PATH=/Users/001522/anaconda3/bin/python3 \\\n    PYTHON_LIB_PATH=/Users/001522/anaconda3/lib/python3.10/site-packages \\\n    TF2_BEHAVIOR=1 \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; \n        bazel-out/darwin_arm64-opt-exec-50AE0418/bin/external/flatbuffers/flatc --proto -o bazel-out/darwin_arm64-opt/bin/tensorflow/lite/acceleration/configuration tensorflow/lite/acceleration/configuration/configuration.proto\n        perl -p -i -e '\\''s/tflite.proto/tflite/'\\'' bazel-out/darwin_arm64-opt/bin/tensorflow/lite/acceleration/configuration/configuration.fbs\n    ')\n # Configuration: 156e18005636972e4bcf95905ead517f4bd9575d761ba24552404ce2ea964aec \n # Execution platform: @local_execution_config_platform//:platform\n: command not found\nTarget //tensorflow:libtensorflow_cc.so failed to build"", 'created_at': datetime.datetime(2025, 1, 21, 15, 42, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642052140, 'issue_id': 2801215487, 'author': 'tilakrayal', 'body': ""@alvinwong64,\n The build targets have changed. Please try the below command.\n\n`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu`\n\nAnd also it might fail if you don't have  dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them). Also tensorflow 2.14 is pretty old, please try to install the tensorflow v2.18 and also try to follow the steps mentioned in the below document.\n\nhttps://www.tensorflow.org/install/source#macos\n\nThank you!"", 'created_at': datetime.datetime(2025, 2, 7, 6, 30, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 13:05:48 UTC): you need to set up TensorFlow for C++ development, the target //tensorflow/tools/pip_package:pip_package is specifically for creating Python pip packages, not for compiling TensorFlow for C++ use.
 Install TensorFlow C++ Library,Build TensorFlow C++ API,Configure Build Settings,Build TensorFlow Shared Libraries then load and run your model

alvinwong64 (Issue Creator) on (2025-01-21 13:10:32 UTC): Could you guide me on how to install tensorflow c++ library and build it?

arzoo0511 on (2025-01-21 13:17:26 UTC): yes ofc ! 
System Requirements: Linux-based system,C++ compiler ,Python installed,Bazel 
Install C++ Build Dependencies: 
sudo apt-get install build-essential
Install Bazelisk 
curl -LO https://github.com/bazelbuild/bazelisk/releases/download/v1.10.0/bazelisk-linux-amd64
chmod +x bazelisk-linux-amd64
mv bazelisk-linux-amd64 /usr/local/bin/bazel

Step-by-Step Installation:
Clone the TensorFlow GitHub Repository
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

Configure the Build
./configure
Youll be prompted with several configuration options, including whether to enable CUDA (GPU support) and whether to build TensorFlow for specific systems or features. For building with GPU support, youll need to have the CUDA toolkit and cuDNN installed.

Build the C++ Library
bazel build //tensorflow:libtensorflow_cc.so

This will build the TensorFlow C++ library as a shared object (.so) file. If you want to build a static library, you can modify the build command accordingly.

For debugging and testing, you want to build the tensorflow_cc library with the -c dbg flag:
bazel build -c dbg //tensorflow:libtensorflow_cc.so
This will take some time, depending on your system's hardware.

Include TensorFlow C++ Headers and Libraries located in the bazel-bin/ directory. 
bazel-bin/ tensorflow

alvinwong64 (Issue Creator) on (2025-01-21 15:42:23 UTC): Hi while building, the build failed, with the use of --verbose_failures

ERROR: /Users/001522/Documents/testing/tensorflow/tensorflow/lite/acceleration/configuration/BUILD:41:8: Executing genrule //tensorflow/lite/acceleration/configuration:configuration_schema failed: (Exit 127): bash failed: error executing command (from target //tensorflow/lite/acceleration/configuration:configuration_schema) 
  (cd /private/var/tmp/_bazel_001522/497fb1666e8c0e255ebeec937d6e9115/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/001522/Library/Caches/bazelisk/downloads/sha256/c6b6dc17efcdf13fba484c6fe0b6c3361b888ae7b9573bc25a2dbe8c502448eb/bin:/Users/001522/anaconda3/bin:/Users/001522/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools \
    PYTHON_BIN_PATH=/Users/001522/anaconda3/bin/python3 \
    PYTHON_LIB_PATH=/Users/001522/anaconda3/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
        bazel-out/darwin_arm64-opt-exec-50AE0418/bin/external/flatbuffers/flatc --proto -o bazel-out/darwin_arm64-opt/bin/tensorflow/lite/acceleration/configuration tensorflow/lite/acceleration/configuration/configuration.proto
        perl -p -i -e '\''s/tflite.proto/tflite/'\'' bazel-out/darwin_arm64-opt/bin/tensorflow/lite/acceleration/configuration/configuration.fbs
    ')
 # Configuration: 156e18005636972e4bcf95905ead517f4bd9575d761ba24552404ce2ea964aec 
 # Execution platform: @local_execution_config_platform//:platform
: command not found
Target //tensorflow:libtensorflow_cc.so failed to build

tilakrayal (Assginee) on (2025-02-07 06:30:00 UTC): @alvinwong64,
 The build targets have changed. Please try the below command.

`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu`

And also it might fail if you don't have  dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them). Also tensorflow 2.14 is pretty old, please try to install the tensorflow v2.18 and also try to follow the steps mentioned in the below document.

https://www.tensorflow.org/install/source#macos

Thank you!

"
2801074036,issue,closed,completed,No matching distribution found for tensorflow==2.18.0 / Python 3.12.8 Alpine docker images," Try to install inside official Python 3.12.8 Alpine docker images --> [3.12-alpine](https://github.com/docker-library/python/blob/3d7b328b66525fe2e82af7063af10c176b6ee8cd/3.12/alpine3.21/Dockerfile)
 
Here is what I get:

```
ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12
ERROR: Could not find a version that satisfies the requirement tensorflow==2.18.0 (from versions: none)
ERROR: No matching distribution found for tensorflow==2.18.0
```
On 3.12.0 and 3.11 the same issue",malakhovks,2025-01-21 08:36:18+00:00,['Venkat6871'],2025-01-21 12:38:47+00:00,2025-01-21 12:38:46+00:00,https://github.com/tensorflow/tensorflow/issues/85371,[],"[{'comment_id': 2604602567, 'issue_id': 2801074036, 'author': 'arzoo0511', 'body': 'TensorFlow currently does not support Python 3.12 or Alpine Linux directly,Instead of Alpine, use a Debian-based image like python:3.11-slim, which is compatible with TensorFlow', 'created_at': datetime.datetime(2025, 1, 21, 12, 29, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604624233, 'issue_id': 2801074036, 'author': 'malakhovks', 'body': '> TensorFlow currently does not support Python 3.12 or Alpine Linux directly,Instead of Alpine, use a Debian-based image like python:3.11-slim, which is compatible with TensorFlow\n\nThanks! Will do!', 'created_at': datetime.datetime(2025, 1, 21, 12, 38, 46, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 12:29:03 UTC): TensorFlow currently does not support Python 3.12 or Alpine Linux directly,Instead of Alpine, use a Debian-based image like python:3.11-slim, which is compatible with TensorFlow

malakhovks (Issue Creator) on (2025-01-21 12:38:46 UTC): Thanks! Will do!

"
2800302791,issue,closed,completed,How to enable compiler flags for avx2 avx512f fma,"I've compiled Tensorflow on Debian 12 successfully but when use jupyter I get this message
**To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild Tensorflow with the appropriate compiler flags.**

```
the compilation is like this:
$ python ./configure.py

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-sign-compare -march=native -mavx2 -mavx512f -mfma -msse4.1 -msse4.2

$ bazelisk build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --local_ram_resources=2048 --config=monolithic --verbose_failures -c opt --config=opt --copt=-Wno-gnu-offsetof-extensions --copt=-march=native --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2
```

How to correctly compile tensorflow with AVX2 AVX512F FMA flags",Reyadeyat,2025-01-20 21:35:22+00:00,['tilakrayal'],2025-02-07 02:01:19+00:00,2025-02-07 02:01:18+00:00,https://github.com/tensorflow/tensorflow/issues/85365,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2604680307, 'issue_id': 2800302791, 'author': 'arzoo0511', 'body': ""Verify Hardware Support,Configure TensorFlow,Compile with Bazel,Verify Build Success,Test the Build with print(tf.config.list_physical_devices('CPU'))"", 'created_at': datetime.datetime(2025, 1, 21, 13, 3, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610123344, 'issue_id': 2800302791, 'author': 'tilakrayal', 'body': '@Reyadeyat,\nIn order to expedite the trouble-shooting process, could you please provide the CUDA/cuDNN version, GPU model and also provide the tensorflow version which you are trying. Along with the information try to provide the error log which helps to analyse the issue. Thank you!', 'created_at': datetime.datetime(2025, 1, 23, 15, 30, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127137, 'issue_id': 2800302791, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641749984, 'issue_id': 2800302791, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 18, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 13:03:38 UTC): Verify Hardware Support,Configure TensorFlow,Compile with Bazel,Verify Build Success,Test the Build with print(tf.config.list_physical_devices('CPU'))

tilakrayal (Assginee) on (2025-01-23 15:30:56 UTC): @Reyadeyat,
In order to expedite the trouble-shooting process, could you please provide the CUDA/cuDNN version, GPU model and also provide the tensorflow version which you are trying. Along with the information try to provide the error log which helps to analyse the issue. Thank you!

github-actions[bot] on (2025-01-31 01:59:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-07 02:01:18 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2799355866,issue,open,,"Tutorial ""Multi-worker training with Keras"" fails to complete","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107

### Custom code

No

### OS platform and distribution

Debian 6.1.123-1 (2025-01-02) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the tutorial everything goes well until you start the second worker. Then the below failure occures.

2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.

### Standalone code to reproduce the issue

```shell
python main.py &> job_1.log
```

### Relevant log output

```shell
2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
```",Chadster766,2025-01-20 14:03:18+00:00,['Venkat6871'],2025-01-27 08:07:58+00:00,,https://github.com/tensorflow/tensorflow/issues/85351,"[('type:bug', 'Bug'), ('TF 2.18', '')]","[{'comment_id': 2614911678, 'issue_id': 2799355866, 'author': 'Venkat6871', 'body': 'Hi **@Chadster766** ,\nApologies for the delay, and thank you for raising your concern here. Could you please provide more details, such as the versions of TensorFlow and any other relevant libraries you are using? Additionally, sharing your code would make it easier for us to troubleshoot the issue effectively. In the meantime, please ensure that all compatibility requirements are met. For your reference, here is the relevant [documentation](https://www.tensorflow.org/install/source#gpu).\nThank you!', 'created_at': datetime.datetime(2025, 1, 27, 6, 3, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615074950, 'issue_id': 2799355866, 'author': 'Chadster766', 'body': ""Hi @Venkat6871 ,\n\nI think I've provided the info regarding versions of TensorFlow and any other relevant libraries in the issue creation.\n\nI'm not running any of my code I'm just using the jupyter notebook of the tutorial."", 'created_at': datetime.datetime(2025, 1, 27, 8, 7, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-27 06:03:43 UTC): Hi **@Chadster766** ,
Apologies for the delay, and thank you for raising your concern here. Could you please provide more details, such as the versions of TensorFlow and any other relevant libraries you are using? Additionally, sharing your code would make it easier for us to troubleshoot the issue effectively. In the meantime, please ensure that all compatibility requirements are met. For your reference, here is the relevant [documentation](https://www.tensorflow.org/install/source#gpu).
Thank you!

Chadster766 (Issue Creator) on (2025-01-27 08:07:55 UTC): Hi @Venkat6871 ,

I think I've provided the info regarding versions of TensorFlow and any other relevant libraries in the issue creation.

I'm not running any of my code I'm just using the jupyter notebook of the tutorial.

"
2797852999,issue,open,,Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter,"Hi, I get ""Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate

I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at [https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114](https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114)

**System information**
- Android Device information: Samsung S23
- TensorFlow Lite in Play Services SDK version : 16.4.0
- Google Play Services version: 24.50.34

**Standalone code to reproduce the issue**
Clone and run project from [https://github.com/NLLAPPS/WhisperOffline/](https://github.com/NLLAPPS/WhisperOffline/)

**Any other info / logs**
`Created TensorFlow Lite delegate for GPU.
Created interpreter.
Created interpreter.
java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: 
at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method)
at com.google.android.gms.tflite.NativeInterpreterWrapper.zzs(com.google.android.gms:play-services-tflite-java@@16.4.0:34)
at com.google.android.gms.tflite.NativeInterpreterWrapper.<init>(com.google.android.gms:play-services-tflite-java@@16.4.0:14)
at com.google.android.gms.tflite.zzd.<init>(com.google.android.gms:play-services-tflite-java@@16.4.0:3)
at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:play-services-tflite-java@@16.4.0:4)
at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:373)
at com.whispertflite.engine.WhisperEngineJava.loadModel(WhisperEngineJava.java:131)
at com.whispertflite.engine.WhisperEngineJava.initialize(WhisperEngineJava.java:44)
at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:72)
at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:67)
at com.whispertflite.MainActivity.initModel(MainActivity.java:247)
at com.whispertflite.MainActivity.lambda$onCreate$3$com-whispertflite-MainActivity(MainActivity.java:155)
at com.whispertflite.MainActivity$$ExternalSyntheticLambda0.run(D8$$SyntheticClass:0)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:644)
at java.lang.Thread.run(Thread.java:1012)`
",NLLAPPS,2025-01-19 22:01:13+00:00,['gaikwadrahul8'],2025-02-03 17:34:09+00:00,,https://github.com/tensorflow/tensorflow/issues/85313,"[('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue')]","[{'comment_id': 2604611507, 'issue_id': 2797852999, 'author': 'arzoo0511', 'body': 'there is an issue with the configuration of the GpuDelegate or its compatibility with your environment, Test with CPU-only execution to confirm whether the issue is specific to the GPU Delegate', 'created_at': datetime.datetime(2025, 1, 21, 12, 32, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604630270, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': 'Hi and thank you. I have tested CPU only it works fine. What is the ""configuration of the GpuDelegate""? My configuration can be seen at [https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114](https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114)', 'created_at': datetime.datetime(2025, 1, 21, 12, 41, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604674270, 'issue_id': 2797852999, 'author': 'arzoo0511', 'body': ""'GpuDelegateFactory.Options' in your code uses generic settings. For better control, you can configure options like precision or inference preference. try this or Run a compatibility check \nGpuDelegateFactory.Options gpuOptions = new GpuDelegateFactory.Options();\ngpuOptions.setInferencePreference(GpuDelegateFactory.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER);\ngpuOptions.setPrecisionLossAllowed(true);"", 'created_at': datetime.datetime(2025, 1, 21, 13, 0, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604707954, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': 'Thanks, regarding ""compatibility check"". Is it possible to provide link to documentation for compatibility checking?', 'created_at': datetime.datetime(2025, 1, 21, 13, 15, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604718863, 'issue_id': 2797852999, 'author': 'arzoo0511', 'body': 'check out these links :\nhttps://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible\nhttps://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\nhttps://docs.nvidia.com/deeplearning/cudnn/latest/reference/support-matrix.html', 'created_at': datetime.datetime(2025, 1, 21, 13, 20, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604748595, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': 'Your links seem to be related to PCs. Have I misunderstood what GpuDelegateFactory does. I thought it would be for using GPU on the phone since the artifact is ""play-services-tflite-gpu""', 'created_at': datetime.datetime(2025, 1, 21, 13, 32, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604751828, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': 'I also just noticed you are not related to this project. Do you have experience on implementing tflite on Android?', 'created_at': datetime.datetime(2025, 1, 21, 13, 33, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604761650, 'issue_id': 2797852999, 'author': 'arzoo0511', 'body': ""you are correct in assuming that GpuDelegateFactory is intended for mobile devices to leverage the GPU for TensorFlow Lite inference, especially in Android using the play-services-tflite-gpu artifact. If you're targeting mobile platforms, this is the correct path to enable GPU acceleration\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.x.x'\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.x.x'\n}\nGpuDelegate delegate = new GpuDelegate();\nInterpreter.Options options = new Interpreter.Options().addDelegate(delegate);\nInterpreter interpreter = new Interpreter(modelFile, options);"", 'created_at': datetime.datetime(2025, 1, 21, 13, 37, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604763542, 'issue_id': 2797852999, 'author': 'arzoo0511', 'body': 'yes i am not a part of this project yet , im trying to contribute as much possible to be recognized by the organization before gsoc 2025', 'created_at': datetime.datetime(2025, 1, 21, 13, 38, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604783328, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': ""> yes i am not a part of this project yet , i'm trying to contribute as much possible to be recognized by the organization before gsoc 2025\n\nI don't think you will be able to help me in this case. Issue seems to be related to actual SDK/API. Hopefully @Venkat6871 will have a look at it."", 'created_at': datetime.datetime(2025, 1, 21, 13, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631400327, 'issue_id': 2797852999, 'author': 'gaikwadrahul8', 'body': ""Hi, @NLLAPPS \nI apologize for the delayed response, I was trying to reproduce the similar issue from my end after cloning your provided [repo](https://github.com/NLLAPPS/WhisperOffline/) but I'm getting below error message, if possible could you please help me to replicate the same issue from my end which you reported in the issue template to investigate this issue further from our end ?\n\n**Here is error log for reference :**\n```\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nA problem occurred configuring root project 'WhisperOffline'.\n> Could not resolve all files for configuration ':classpath'.\n   > Could not find com.android.tools.build:gradle:8.10.2.\n     Searched in the following locations:\n       - https://dl.google.com/dl/android/maven2/com/android/tools/build/gradle/8.10.2/gradle-8.10.2.pom\n       - https://repo.maven.apache.org/maven2/com/android/tools/build/gradle/8.10.2/gradle-8.10.2.pom\n     Required by:\n         project :\n\n* Try:\n> Run with --stacktrace option to get the stack trace.\n> Run with --info or --debug option to get more log output.\n> Run with --scan to get full insights.\n> Get more help at https://help.gradle.org.\n\nBUILD FAILED in 829ms\n```\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 2, 3, 15, 54, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631641383, 'issue_id': 2797852999, 'author': 'NLLAPPS', 'body': 'Hi, project is using com.android.tools.build:gradle:8.8.0 there is no com.android.tools.build:gradle:8.10.2.\n8.10.2 is a Gradle version, not Android build tools version.\n\nHave you changed anything? [This stack overflow post](https://stackoverflow.com/questions/70545646/could-not-find-com-android-tools-buildgradle7-3-3-error-found-in-build-gradle) seems to suggest it may be related to Android Studio Gradle Settings. \n\n\n\nHere is mine attached for the project\n\n![Image](https://github.com/user-attachments/assets/139a8292-7340-4166-8470-343138a4383f)', 'created_at': datetime.datetime(2025, 2, 3, 17, 34, 7, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 12:32:55 UTC): there is an issue with the configuration of the GpuDelegate or its compatibility with your environment, Test with CPU-only execution to confirm whether the issue is specific to the GPU Delegate

NLLAPPS (Issue Creator) on (2025-01-21 12:41:34 UTC): Hi and thank you. I have tested CPU only it works fine. What is the ""configuration of the GpuDelegate""? My configuration can be seen at [https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114](https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114)

arzoo0511 on (2025-01-21 13:00:49 UTC): 'GpuDelegateFactory.Options' in your code uses generic settings. For better control, you can configure options like precision or inference preference. try this or Run a compatibility check 
GpuDelegateFactory.Options gpuOptions = new GpuDelegateFactory.Options();
gpuOptions.setInferencePreference(GpuDelegateFactory.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER);
gpuOptions.setPrecisionLossAllowed(true);

NLLAPPS (Issue Creator) on (2025-01-21 13:15:58 UTC): Thanks, regarding ""compatibility check"". Is it possible to provide link to documentation for compatibility checking?

arzoo0511 on (2025-01-21 13:20:21 UTC): check out these links :
https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible
https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html
https://docs.nvidia.com/deeplearning/cudnn/latest/reference/support-matrix.html

NLLAPPS (Issue Creator) on (2025-01-21 13:32:14 UTC): Your links seem to be related to PCs. Have I misunderstood what GpuDelegateFactory does. I thought it would be for using GPU on the phone since the artifact is ""play-services-tflite-gpu""

NLLAPPS (Issue Creator) on (2025-01-21 13:33:37 UTC): I also just noticed you are not related to this project. Do you have experience on implementing tflite on Android?

arzoo0511 on (2025-01-21 13:37:31 UTC): you are correct in assuming that GpuDelegateFactory is intended for mobile devices to leverage the GPU for TensorFlow Lite inference, especially in Android using the play-services-tflite-gpu artifact. If you're targeting mobile platforms, this is the correct path to enable GPU acceleration
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:2.x.x'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.x.x'
}
GpuDelegate delegate = new GpuDelegate();
Interpreter.Options options = new Interpreter.Options().addDelegate(delegate);
Interpreter interpreter = new Interpreter(modelFile, options);

arzoo0511 on (2025-01-21 13:38:22 UTC): yes i am not a part of this project yet , im trying to contribute as much possible to be recognized by the organization before gsoc 2025

NLLAPPS (Issue Creator) on (2025-01-21 13:46:35 UTC): I don't think you will be able to help me in this case. Issue seems to be related to actual SDK/API. Hopefully @Venkat6871 will have a look at it.

gaikwadrahul8 (Assginee) on (2025-02-03 15:54:58 UTC): Hi, @NLLAPPS 
I apologize for the delayed response, I was trying to reproduce the similar issue from my end after cloning your provided [repo](https://github.com/NLLAPPS/WhisperOffline/) but I'm getting below error message, if possible could you please help me to replicate the same issue from my end which you reported in the issue template to investigate this issue further from our end ?

**Here is error log for reference :**
```
FAILURE: Build failed with an exception.

* What went wrong:
A problem occurred configuring root project 'WhisperOffline'.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/com/android/tools/build/gradle/8.10.2/gradle-8.10.2.pom
       - https://repo.maven.apache.org/maven2/com/android/tools/build/gradle/8.10.2/gradle-8.10.2.pom
     Required by:
         project :

* Try:

BUILD FAILED in 829ms
```

Thank you for your cooperation and patience.

NLLAPPS (Issue Creator) on (2025-02-03 17:34:07 UTC): Hi, project is using com.android.tools.build:gradle:8.8.0 there is no com.android.tools.build:gradle:8.10.2.
8.10.2 is a Gradle version, not Android build tools version.

Have you changed anything? [This stack overflow post](https://stackoverflow.com/questions/70545646/could-not-find-com-android-tools-buildgradle7-3-3-error-found-in-build-gradle) seems to suggest it may be related to Android Studio Gradle Settings. 



Here is mine attached for the project

![Image](https://github.com/user-attachments/assets/139a8292-7340-4166-8470-343138a4383f)

"
2797809263,issue,closed,completed,LD_LIBRARY_PATH to set when installing tensorflow[and-cuda] with pip or poetry,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Linux Fedora 41

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia RTX 3060

### Current behavior?

Installing `tensorflow[and-cuda]` is very easy and avoid installing cuda packages on our computers. It's then very confortable to be able to drop virtual envs.
But there is a bad behavior.

From scratch:

```bash
rm -rf .venv
mkdir .venv
poetry add ""tensorflow[and-cuda]""



# check
poetry run python mytest.py

W0000 00:00:1737317404.258825   10111 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
```

As you can see, no CUDA shared library is loaded.

The solution is to generate the `LD_LIBRARY_PATH` finding directories name of any "".so"" files related to ""nvidia"":

```bash
export LD_LIBRARY_PATH=$(find .venv -name ""*.so*"" | grep -P ""nvidia"" | xargs dirname | sort -u | paste -d: -s -)
echo $LD_LIBRARY_PATH
.venv/lib/python3.12/site-packages/nvidia/cublas/lib:.venv/lib/python3.12/site-packages/nvidia/cuda_cupti/lib:.venv/lib/python3.12/site-packages/nvidia/cuda_nvcc/nvvm/lib64:.venv/lib/python3.12/site-packages/nvidia/cuda_nvrtc/lib:.venv/lib/python3.12/site-packages/nvidia/cuda_runtime/lib:.venv/lib/python3.12/site-packages/nvidia/cudnn/lib:.venv/lib/python3.12/site-packages/nvidia/cufft/lib:.venv/lib/python3.12/site-packages/nvidia/curand/lib:.venv/lib/python3.12/site-packages/nvidia/cusolver/lib:.venv/lib/python3.12/site-packages/nvidia/cusparse/lib:.venv/lib/python3.12/site-packages/nvidia/nccl/lib:.venv/lib/python3.12/site-packages/nvidia/nvjitlink/lib

# and now:
poetry run python mytest.py

I0000 00:00:1737317342.317060    9825 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4169 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
```

I'm not sur if I have to create the issue here or to Nvidia (where?), but it could be interesting to propose this solution for who has the same problem.

### Standalone code to reproduce the issue

```shell
poetry run python -c ""import tensorflow;tensorflow.keras.Sequential().compile()""
```

### Relevant log output

```shell

```",metal3d,2025-01-19 20:13:12+00:00,['tilakrayal'],2025-02-07 02:01:23+00:00,2025-02-07 02:01:20+00:00,https://github.com/tensorflow/tensorflow/issues/85311,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]","[{'comment_id': 2604690910, 'issue_id': 2797809263, 'author': 'arzoo0511', 'body': 'TensorFlow installed with tensorflow[and-cuda] doesn\'t automatically set up the LD_LIBRARY_PATH environment variable to include the locations of the NVIDIA shared libraries packaged with the installation. Your solution is an effective way to resolve the problem by dynamically setting LD_LIBRARY_PATH to include the necessary directories containing the .so files. \net LD_LIBRARY_PATH dynamically to include the directories containing the NVIDIA .so files within the virtual environment\nuse this command \nexport LD_LIBRARY_PATH=$(find .venv -name ""*.so*"" | grep -P ""nvidia"" | xargs dirname | sort -u | paste -d: -s -)\nthen verify\necho $LD_LIBRARY_PATH', 'created_at': datetime.datetime(2025, 1, 21, 13, 8, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610111209, 'issue_id': 2797809263, 'author': 'tilakrayal', 'body': '@metal3d,\nThank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.\n\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.\n\nhttps://github.com/tensorflow/tensorflow/issues/70947\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 23, 15, 26, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127166, 'issue_id': 2797809263, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750031, 'issue_id': 2797809263, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750100, 'issue_id': 2797809263, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85311"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85311"">No</a>', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 22, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 13:08:37 UTC): TensorFlow installed with tensorflow[and-cuda] doesn't automatically set up the LD_LIBRARY_PATH environment variable to include the locations of the NVIDIA shared libraries packaged with the installation. Your solution is an effective way to resolve the problem by dynamically setting LD_LIBRARY_PATH to include the necessary directories containing the .so files. 
et LD_LIBRARY_PATH dynamically to include the directories containing the NVIDIA .so files within the virtual environment
use this command 
export LD_LIBRARY_PATH=$(find .venv -name ""*.so*"" | grep -P ""nvidia"" | xargs dirname | sort -u | paste -d: -s -)
then verify
echo $LD_LIBRARY_PATH

tilakrayal (Assginee) on (2025-01-23 15:26:20 UTC): @metal3d,
Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.

I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.

https://github.com/tensorflow/tensorflow/issues/70947

Thank you!

github-actions[bot] on (2025-01-31 01:59:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-07 02:01:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-07 02:01:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85311"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85311"">No</a>

"
2797637658,issue,open,,Force TF to log GPU memory allocation,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

RTX 3060 12Gb

### Current behavior?

If I ran out of GPU memory I get error saying

`Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB`

But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidia-smi etc but this is an indirect way to estimate very crudely whats going on.

What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card) - is to be able to force TF to log every and single one GPU memory allocation. 

So I can see whats going on my healthy models and whats in the failing.",maxima120,2025-01-19 13:50:42+00:00,"['MichaelHudgins', 'Venkat6871']",2025-01-27 05:17:59+00:00,,https://github.com/tensorflow/tensorflow/issues/85303,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:gpu', 'GPU related issues')]",[],
2797475074,issue,open,,Unable to install TensorFlow: No matching distribution found for TensorFlow!,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I expected to install TensorFlow

### Standalone code to reproduce the issue

Can't install TensorFlow with pip
```shell
> pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)

[notice] A new release of pip is available: 24.2 -> 24.3.1      
[notice] To update, run: python.exe -m pip install --upgrade pip
ERROR: No matching distribution found for tensorflow
```
Pls help

### Relevant log output

```shell

```",Animemchik,2025-01-19 07:18:59+00:00,['tilakrayal'],2025-02-03 02:00:11+00:00,,https://github.com/tensorflow/tensorflow/issues/85298,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.8', '')]","[{'comment_id': 2600949407, 'issue_id': 2797475074, 'author': 'mihaimaruseac', 'body': 'There was no python 2.13 at the time of tensorflow 2.8 release.\n\nIf you want to use TF 2.8 you need to use a Python version that is supported there.\n\nIf you want to use Python 3.13, please see #78774', 'created_at': datetime.datetime(2025, 1, 19, 17, 18, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601000166, 'issue_id': 2797475074, 'author': 'Animemchik', 'body': ""> There was no python 2.13 at the time of tensorflow 2.8 release.\n> \n> If you want to use TF 2.8 you need to use a Python version that is supported there.\n\nI also tried 2.7 and 2.6 but still can't do it"", 'created_at': datetime.datetime(2025, 1, 19, 19, 56, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601013204, 'issue_id': 2797475074, 'author': 'mihaimaruseac', 'body': ""TF 2.7 and TF 2.6 are even older, so of course they won't work with Python 3.13 either"", 'created_at': datetime.datetime(2025, 1, 19, 20, 37, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604598972, 'issue_id': 2797475074, 'author': 'arzoo0511', 'body': 'TensorFlow requires specific Python versions depending on the TensorFlow version. try Python 3.8 to 3.11', 'created_at': datetime.datetime(2025, 1, 21, 12, 27, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629727538, 'issue_id': 2797475074, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 3, 2, 0, 10, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-19 17:18:55 UTC): There was no python 2.13 at the time of tensorflow 2.8 release.

If you want to use TF 2.8 you need to use a Python version that is supported there.

If you want to use Python 3.13, please see #78774

Animemchik (Issue Creator) on (2025-01-19 19:56:57 UTC): I also tried 2.7 and 2.6 but still can't do it

mihaimaruseac on (2025-01-19 20:37:44 UTC): TF 2.7 and TF 2.6 are even older, so of course they won't work with Python 3.13 either

arzoo0511 on (2025-01-21 12:27:24 UTC): TensorFlow requires specific Python versions depending on the TensorFlow version. try Python 3.8 to 3.11

github-actions[bot] on (2025-02-03 02:00:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2796962703,issue,open,,Aborted in `tensorflow.nn.depthwise_conv2d_backprop_filter`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.nn.depthwise_conv2d_backprop_filterr` triggers crash.
The crash occurs in the  operator DepthwiseConv2dNativeBackpropFilter

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

filter_sizes = np.ones((13, 2, 5, 4), dtype=np.uint32)
input = 26262.2175547925
out_backprop = -14557.552005335412
strides = []
padding = 'VALID'
tensorflow.nn.depthwise_conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding)
```

### Relevant log output

```shell
2025-01-18 12:47:13.553183: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 0, 0, N
Aborted (core dumped)
```",LongZE666,2025-01-18 12:49:02+00:00,['Venkat6871'],2025-02-04 01:59:44+00:00,,https://github.com/tensorflow/tensorflow/issues/85258,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2614849401, 'issue_id': 2796962703, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here.\nI tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/2ddb53bd2212c87b4e4fe000064bcf46/85258_tf_2-17-2-18-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 27, 4, 57, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632586897, 'issue_id': 2796962703, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 4, 1, 59, 43, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-27 04:57:13 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here.
I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/2ddb53bd2212c87b4e4fe000064bcf46/85258_tf_2-17-2-18-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2025-02-04 01:59:43 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2796957488,issue,closed,completed,Aborted in `tensorflow.compat.v1.nn.conv2d_backprop_filter`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.compat.v1.nn.conv2d_backprop_filter` triggers crash.
The crash occurs in the  operator Conv2DBackpropFilter

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = np.array([1.2807603e+38], dtype=np.float32)
filter_sizes = np.ones((20, 6, 5, 3), dtype=np.uint16)
strides = [4538426903074820908]
padding = 'VALID'
out_backprop = np.ones((19, 5, 11, 4), dtype=np.float32)
tensorflow.compat.v1.nn.conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding)
```

### Relevant log output

```shell
2025-01-18 12:34:39.686592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] Non-OK-status: GetNodeAttr(orig_node->def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 4538426903074820908 out of range for an int32
Aborted (core dumped)
```",LongZE666,2025-01-18 12:35:54+00:00,['tilakrayal'],2025-02-06 02:00:40+00:00,2025-02-06 02:00:37+00:00,https://github.com/tensorflow/tensorflow/issues/85257,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2606684457, 'issue_id': 2796957488, 'author': 'tilakrayal', 'body': '@LongZE666,\nI tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32-bit signed integer variable. Could you try to provide the input which fits the stride and test the code. \n\nThank you!', 'created_at': datetime.datetime(2025, 1, 22, 9, 16, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623349781, 'issue_id': 2796957488, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 30, 1, 58, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638562300, 'issue_id': 2796957488, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638562825, 'issue_id': 2796957488, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85257"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85257"">No</a>', 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 38, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-22 09:16:38 UTC): @LongZE666,
I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32-bit signed integer variable. Could you try to provide the input which fits the stride and test the code. 

Thank you!

github-actions[bot] on (2025-01-30 01:58:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-06 02:00:36 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-06 02:00:38 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85257"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85257"">No</a>

"
2796955278,issue,closed,completed,Aborted in `tensorflow.raw_ops.ScatterNdNonAliasingAdd`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.raw_ops.ScatterNdNonAliasingAdd` triggers crash.
The crash occurs in the  operator ScatterNdNonAliasingAdd

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = np.array([False,  True,  True, False,  True, False,  True, False, False,
     False, False,  True,  True, False, False,  True,  True])
indices = 1547981256
updates = np.array([ True, False,  True, False,  True])
tensorflow.raw_ops.ScatterNdNonAliasingAdd(input=input, indices=indices, updates=updates)
```

### Relevant log output

```shell
2025-01-18 12:28:50.452498: F ./tensorflow/core/framework/tensor.h:847] Check failed: new_num_elements == NumElements() (1 vs. 5)
Aborted (core dumped)
```",LongZE666,2025-01-18 12:30:20+00:00,['Venkat6871'],2025-02-07 02:01:26+00:00,2025-02-07 02:01:22+00:00,https://github.com/tensorflow/tensorflow/issues/85256,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2609323514, 'issue_id': 2796955278, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here.\nI tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and initially faced the same issue. Upon investigating, the main cause seems to be a mismatch in the dimensions and values. After making the necessary adjustments, it worked fine for me. I have detailed the findings and corrections in a Colab gist. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f9afc1b8c2bc57eb0afdccee0a08361c/85256_tf_2-17-0-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 23, 9, 37, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127218, 'issue_id': 2796955278, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750099, 'issue_id': 2796955278, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750182, 'issue_id': 2796955278, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85256"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85256"">No</a>', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 25, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-23 09:37:05 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here.
I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and initially faced the same issue. Upon investigating, the main cause seems to be a mismatch in the dimensions and values. After making the necessary adjustments, it worked fine for me. I have detailed the findings and corrections in a Colab gist. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f9afc1b8c2bc57eb0afdccee0a08361c/85256_tf_2-17-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2025-01-31 01:59:42 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-07 02:01:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-07 02:01:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85256"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85256"">No</a>

"
2796953954,issue,open,,Aborted in `tensorflow.compat.v1.nn.depthwise_conv2d_native`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.compat.v1.nn.depthwise_conv2d_native` triggers crash.
The crash occurs in the  operator DepthwiseConv2DNative

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = -18261.482131447112
strides = [13762484222233278712, 17895496137899471100]
padding = 'VALID'
dilations = [11529923499597944069, 6104546329631715214, 16719022861480294065, 15641088296656225516, 5831796171324899039, 2019887057379389965, 6603155403475823832, 128243198633280011, 1962111378253501566, 12787126510677041415, 10754291262680183180]
filter = np.array([[ 36832. ,  -8776. ,    255.2, -31200. ,  -6408. , -12656. , -31696. , -65024. , -50688. ]], dtype=np.float16)
tensorflow.compat.v1.nn.depthwise_conv2d_native(input=input, filter=filter, strides=strides, padding=padding, dilations=dilations)
```

### Relevant log output

```shell
2025-01-18 12:25:34.220155: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2755] Non-OK-status: GetNodeAttr(orig_node->def(), ""dilations"", &dilations) status: INVALID_ARGUMENT: Attr dilations has value 6104546329631715214 out of range for an int32
Aborted (core dumped)
```",LongZE666,2025-01-18 12:26:48+00:00,['tilakrayal'],2025-02-04 01:59:46+00:00,,https://github.com/tensorflow/tensorflow/issues/85255,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2600378760, 'issue_id': 2796953954, 'author': 'jxmils', 'body': '### Explanation of the Error: `Attr dilations has value [...] out of range for an int32`\n\nThe error **`Attr dilations has value [...] out of range for an int32`** occurs because the `dilations` attribute passed to the TensorFlow operation contains values that exceed the range of a 32-bit integer.\n\n---\n\n### Why This Happens\n- The `dilations` parameter defines how the convolution kernel is spaced for height and width dimensions.\n- TensorFlow expects `dilations` to be a **list of small positive integers**, typically `[1, dilation_height, dilation_width, 1]`.\n- In your code, `dilations` contains excessively large values, such as:\n  ```python\n  dilations = [11529923499597944069, 6104546329631715214, ...]\n\nAnother error I ran into:\n\n### Explanation of the Error: `Sliding window strides field must specify 4 dimensions`\n\nThe error **`Sliding window strides field must specify 4 dimensions`** means that TensorFlow expects the `strides` parameter to be a list of exactly **4 integers**, corresponding to the following dimensions:\n\n1. **Batch size**: Stride for the batch dimension (usually set to `1`).\n2. **Height**: Stride for the height dimension.\n3. **Width**: Stride for the width dimension.\n4. **Channels**: Stride for the channels dimension (usually set to `1`).', 'created_at': datetime.datetime(2025, 1, 19, 1, 23, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615748773, 'issue_id': 2796953954, 'author': 'tilakrayal', 'body': '@LongZE666 \n\nI tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32-bit signed integer variable. Could you try to provide the input which fits the stride and test the code.\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 27, 13, 21, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632586960, 'issue_id': 2796953954, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 4, 1, 59, 46, tzinfo=datetime.timezone.utc)}]","jxmils on (2025-01-19 01:23:27 UTC): ### Explanation of the Error: `Attr dilations has value [...] out of range for an int32`

The error **`Attr dilations has value [...] out of range for an int32`** occurs because the `dilations` attribute passed to the TensorFlow operation contains values that exceed the range of a 32-bit integer.

---

### Why This Happens
- The `dilations` parameter defines how the convolution kernel is spaced for height and width dimensions.
- TensorFlow expects `dilations` to be a **list of small positive integers**, typically `[1, dilation_height, dilation_width, 1]`.
- In your code, `dilations` contains excessively large values, such as:
  ```python
  dilations = [11529923499597944069, 6104546329631715214, ...]

Another error I ran into:

### Explanation of the Error: `Sliding window strides field must specify 4 dimensions`

The error **`Sliding window strides field must specify 4 dimensions`** means that TensorFlow expects the `strides` parameter to be a list of exactly **4 integers**, corresponding to the following dimensions:

1. **Batch size**: Stride for the batch dimension (usually set to `1`).
2. **Height**: Stride for the height dimension.
3. **Width**: Stride for the width dimension.
4. **Channels**: Stride for the channels dimension (usually set to `1`).

tilakrayal (Assginee) on (2025-01-27 13:21:05 UTC): @LongZE666 

I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32-bit signed integer variable. Could you try to provide the input which fits the stride and test the code.

Thank you!

github-actions[bot] on (2025-02-04 01:59:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2796952009,issue,closed,completed,Aborted in `tensorflow.raw_ops.RecordInput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.raw_ops.RecordInput` triggers crash.
The crash occurs in the  operator RecordInput

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

file_pattern = '@\x0chnFvGe#83FxIIw.\';-$`E?8Y<>-C2p>\x0bl\\B*;z;]a4lrH{gWK}rAY{;T]2k&j-EB)pQu{JLy1K{Q|;g:>CEg<hA""^((UXJ?fMf\'NsZ@KK\rjp{mfE \tYx-3H3;5z\t(%g>\r<!G&_jqx-@o\rz""1 NS""eiM/ 6X/r+/\rj8h>6k{-B]P\n>[\'s0FOZ\x0b[aV\x0c1q\\!{D~2ZIBe{8v9;,oG=Hjsz&~.$T\x0b[_%/ 2;E:Hy8H\tx|dUPg2;[c_\'(>0z-v[Yob\n\x0c;aKX*F.h E@>\x0bK| zGnevJS7G=N_{c}\x0cS4p\']#1`T?FAep'
file_random_seed = 0
batch_size =  -8494458287779632258
tensorflow.raw_ops.RecordInput(file_pattern=file_pattern, file_random_seed=file_random_seed, batch_size=batch_size)
```

### Relevant log output

```shell
2025-01-18 12:21:05.765578: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -8494458287779632258
Aborted (core dumped)
```",LongZE666,2025-01-18 12:21:59+00:00,['Venkat6871'],2025-02-08 01:58:23+00:00,2025-02-08 01:58:20+00:00,https://github.com/tensorflow/tensorflow/issues/85254,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2608991752, 'issue_id': 2796952009, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here.\nI tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3b97a86e1078529f01415f4b5e1f5e51/85254_tf_2-17-0-2-18-0-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 23, 6, 48, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127241, 'issue_id': 2796952009, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425378, 'issue_id': 2796952009, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425403, 'issue_id': 2796952009, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85254"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85254"">No</a>', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 22, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-23 06:48:23 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here.
I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3b97a86e1078529f01415f4b5e1f5e51/85254_tf_2-17-0-2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2025-01-31 01:59:43 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-08 01:58:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-08 01:58:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85254"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85254"">No</a>

"
2796950022,issue,closed,completed,Aborted in `tensorflow.nn.max_pool3d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.nn.max_pool3d` triggers crash.
The crash occurs in the  operator MaxPool3D

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = np.ones((20, 19, 20, 5, 0), dtype=np.float32)
ksize = [3505357736]
strides = [2230364274603370470] 
padding = 'SAME'
tensorflow.nn.max_pool3d(input=input, ksize=ksize, strides=strides, padding=padding)
```

### Relevant log output

```shell
2025-01-18 12:15:49.067137: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] Non-OK-status: GetNodeAttr(n->def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3505357736 out of range for an int32
Aborted (core dumped)
```",LongZE666,2025-01-18 12:16:46+00:00,['tilakrayal'],2025-02-06 02:00:43+00:00,2025-02-06 02:00:39+00:00,https://github.com/tensorflow/tensorflow/issues/85253,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2606806786, 'issue_id': 2796950022, 'author': 'tilakrayal', 'body': '@LongZE666,\nAccording to the official document for the tensorflow.nn.max_pool3d, the input for the strides should be **An int or list of ints that has length 1, 3 or 5. The stride of the sliding window for each dimension of the input tensor.**\n\nhttps://www.tensorflow.org/api_docs/python/tf/nn/max_pool3d#args\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 22, 10, 7, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623349824, 'issue_id': 2796950022, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 30, 1, 58, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638562818, 'issue_id': 2796950022, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638563618, 'issue_id': 2796950022, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85253"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85253"">No</a>', 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 41, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-22 10:07:26 UTC): @LongZE666,
According to the official document for the tensorflow.nn.max_pool3d, the input for the strides should be **An int or list of ints that has length 1, 3 or 5. The stride of the sliding window for each dimension of the input tensor.**

https://www.tensorflow.org/api_docs/python/tf/nn/max_pool3d#args

Thank you!

github-actions[bot] on (2025-01-30 01:58:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-06 02:00:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-06 02:00:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85253"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85253"">No</a>

"
2796949006,issue,closed,completed,Floating point exception in `tensorflow.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.nn.max_pool1d` triggers crash.
The crash occurs in the  operator MaxPool

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = np.ones((16,19,18), dtype=np.float32)
ksize = [58486]
strides = [49657]
padding = 'VALID'
tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding)
```

### Relevant log output

```shell
Floating point exception (core dumped)
```",LongZE666,2025-01-18 12:14:05+00:00,['Venkat6871'],2025-02-08 01:58:26+00:00,2025-02-08 01:58:22+00:00,https://github.com/tensorflow/tensorflow/issues/85252,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2604693866, 'issue_id': 2796949006, 'author': 'arzoo0511', 'body': 'The crash is caused by invalid inputs, not a bug in TensorFlow. \nTo prevent crashes:\nValidate ksize and strides to ensure they align with the input dimensions.\nUse reasonable values for these parameters, adhering to the intended size and stride of your pooling operation.', 'created_at': datetime.datetime(2025, 1, 21, 13, 9, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608932283, 'issue_id': 2796949006, 'author': 'Venkat6871', 'body': ""Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here.\nI tested your code on colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions, and I did not encounter any issues. Please find the results in [gist1](https://colab.sandbox.google.com/gist/Venkat6871/26d359d41f5eb50b1b1c4ef9f8f67027/85252_tf_2-17-0.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/dde268f5c2e673244c86b5b039f4c872/85252_tf_2-18-0-nightly-v.ipynb) for reference.\nIf you are still facing issues, the main cause might be related to the kernel size and strides. Ensure that the kernel size is less than or equal to the input's second dimension, and the same applies to strides. Adjust these values accordingly, and the code should run smoothly.\nThank you!"", 'created_at': datetime.datetime(2025, 1, 23, 6, 1, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127274, 'issue_id': 2796949006, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425395, 'issue_id': 2796949006, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425435, 'issue_id': 2796949006, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85252"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85252"">No</a>', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 25, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 13:09:50 UTC): The crash is caused by invalid inputs, not a bug in TensorFlow. 
To prevent crashes:
Validate ksize and strides to ensure they align with the input dimensions.
Use reasonable values for these parameters, adhering to the intended size and stride of your pooling operation.

Venkat6871 (Assginee) on (2025-01-23 06:01:42 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here.
I tested your code on colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions, and I did not encounter any issues. Please find the results in [gist1](https://colab.sandbox.google.com/gist/Venkat6871/26d359d41f5eb50b1b1c4ef9f8f67027/85252_tf_2-17-0.ipynb) and [gist2](https://colab.sandbox.google.com/gist/Venkat6871/dde268f5c2e673244c86b5b039f4c872/85252_tf_2-18-0-nightly-v.ipynb) for reference.
If you are still facing issues, the main cause might be related to the kernel size and strides. Ensure that the kernel size is less than or equal to the input's second dimension, and the same applies to strides. Adjust these values accordingly, and the code should run smoothly.
Thank you!

github-actions[bot] on (2025-01-31 01:59:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-08 01:58:21 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-08 01:58:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85252"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85252"">No</a>

"
2796947730,issue,closed,completed,Aborted (core dumped) in `tensorflow.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.nn.max_pool1d` triggers crash.
The crash occurs in the  operator MaxPool

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

input = np.ones((17,7,14), dtype=np.float32)
ksize = [3134755050008138495]
strides = [48732]
padding = 'SAME'
tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding)
```

### Relevant log output

```shell
2025-01-18 12:09:37.053592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] Non-OK-status: GetNodeAttr(n->def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3134755050008138495 out of range for an int32
Aborted (core dumped)
```",LongZE666,2025-01-18 12:10:43+00:00,['tilakrayal'],2025-02-06 02:00:46+00:00,2025-02-06 02:00:40+00:00,https://github.com/tensorflow/tensorflow/issues/85251,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2606815005, 'issue_id': 2796947730, 'author': 'tilakrayal', 'body': '@LongZE666,\nThis indicates the problem is due to Memory issue where OS crashed in allocating required memory which is expected.Please refer to the developer https://github.com/tensorflow/tensorflow/issues/59168#issuecomment-1405633596 related to malloc with High input size which will eventually lead to OS crash.\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 22, 10, 10, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623349851, 'issue_id': 2796947730, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 30, 1, 58, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638563202, 'issue_id': 2796947730, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638564387, 'issue_id': 2796947730, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85251"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85251"">No</a>', 'created_at': datetime.datetime(2025, 2, 6, 2, 0, 45, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-22 10:10:59 UTC): @LongZE666,
This indicates the problem is due to Memory issue where OS crashed in allocating required memory which is expected.Please refer to the developer https://github.com/tensorflow/tensorflow/issues/59168#issuecomment-1405633596 related to malloc with High input size which will eventually lead to OS crash.

Thank you!

github-actions[bot] on (2025-01-30 01:58:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-06 02:00:40 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-06 02:00:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85251"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85251"">No</a>

"
2796945049,issue,closed,completed,Abort in `tensorflow.keras.backend.conv2d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1 tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tensorflow.keras.backend.conv2d` triggers crash.
The crash occurs in the  operator Conv2D

### Standalone code to reproduce the issue

```shell
import pickle
import numpy as np
import tensorflow

x = np.array([1.5841993e+38, 4.7480855e+36, -1.8916006e+38], dtype=np.float32)
kernel = np.ones((13, 8, 5, 14), dtype=np.float32)
tensorflow.keras.backend.conv2d(x=x, kernel=kernel)
```

### Relevant log output

```shell
2025-01-18 12:02:15.375499: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C
Aborted (core dumped)
```",LongZE666,2025-01-18 12:04:15+00:00,['Venkat6871'],2025-02-08 01:58:29+00:00,2025-02-08 01:58:23+00:00,https://github.com/tensorflow/tensorflow/issues/85250,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2608863267, 'issue_id': 2796945049, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here. It seems the issue might be caused by using the deprecated API `tensorflow.keras.backend.conv2d`. I recommend switching to the latest APIs for better compatibility and performance. Please find the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/backend/conv2d) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 23, 5, 3, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127305, 'issue_id': 2796945049, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425404, 'issue_id': 2796945049, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425469, 'issue_id': 2796945049, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85250"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85250"">No</a>', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-23 05:03:31 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here. It seems the issue might be caused by using the deprecated API `tensorflow.keras.backend.conv2d`. I recommend switching to the latest APIs for better compatibility and performance. Please find the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/backend/conv2d) here for your reference.
Thank you!

github-actions[bot] on (2025-01-31 01:59:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-08 01:58:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-08 01:58:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85250"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85250"">No</a>

"
2796933056,issue,closed,completed,Aborted in ` tf.nn.conv3d_transpose`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.nn.conv3d_transpose` triggers crash. 

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1wA5pUfm4HpNpHx-PawrZt4CKS2YCUR-4?usp=sharing
```

### Relevant log output

```shell
2025-01-18 11:31:33.696896: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] Non-OK-status: GetNodeAttr(orig_node->def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32
Aborted (core dumped)
```",LongZE666,2025-01-18 11:34:53+00:00,['Venkat6871'],2025-02-05 02:00:47+00:00,2025-02-05 02:00:44+00:00,https://github.com/tensorflow/tensorflow/issues/85246,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2599715867, 'issue_id': 2796933056, 'author': 'LongZE666', 'body': 'code\n```\nimport pickle\nimport numpy as np\nimport tensorflow\n\ninput = np.ones((9, 12, 18, 6), dtype=np.float32)\nfilters = np.ones((14, 1, 9, 10, 0), dtype=np.float32)\nstrides = [5717580618211388939]\npadding = \'SAME\'\noutput_shape = np.array([], dtype=np.complex128)\ntensorflow.nn.conv3d_transpose(input=input, filters=filters, output_shape=output_shape, strides=strides, padding=padding)\n```\nresult\n```\n2025-01-18 13:18:01.541522: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] Non-OK-status: GetNodeAttr(orig_node->def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32\nAborted (core dumped)\n```\nThe colab link given is inconsistent with the results on my machine. -_-', 'created_at': datetime.datetime(2025, 1, 18, 13, 20, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603854263, 'issue_id': 2796933056, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions. However, instead of aborting, the code is throwing an error.\nI have provided an example with all the necessary steps, which is working fine. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/784750ccd2c3f52eb5e3e946c476ccbd/85246_tf_2-18-0-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 21, 7, 31, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469084, 'issue_id': 2796933056, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534403, 'issue_id': 2796933056, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534439, 'issue_id': 2796933056, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85246"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85246"">No</a>', 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 46, tzinfo=datetime.timezone.utc)}]","LongZE666 (Issue Creator) on (2025-01-18 13:20:28 UTC): code
```
import pickle
import numpy as np
import tensorflow

input = np.ones((9, 12, 18, 6), dtype=np.float32)
filters = np.ones((14, 1, 9, 10, 0), dtype=np.float32)
strides = [5717580618211388939]
padding = 'SAME'
output_shape = np.array([], dtype=np.complex128)
tensorflow.nn.conv3d_transpose(input=input, filters=filters, output_shape=output_shape, strides=strides, padding=padding)
```
result
```
2025-01-18 13:18:01.541522: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] Non-OK-status: GetNodeAttr(orig_node->def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32
Aborted (core dumped)
```
The colab link given is inconsistent with the results on my machine. -_-

Venkat6871 (Assginee) on (2025-01-21 07:31:50 UTC): Hi **@LongZE666** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions. However, instead of aborting, the code is throwing an error.
I have provided an example with all the necessary steps, which is working fine. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/784750ccd2c3f52eb5e3e946c476ccbd/85246_tf_2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2025-01-29 01:59:03 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-05 02:00:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-05 02:00:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85246"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85246"">No</a>

"
2796881534,issue,open,,Aborted  in `tf.raw_ops.RaggedGather`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.

### Standalone code to reproduce the issue

```shell
params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)
params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)
indices = tf.constant(0, shape=[], dtype=tf.int64)
OUTPUT_RAGGED_RANK = 1
PARAMS_RAGGED_RANK = 1

tf.raw_ops.RaggedGather(
    params_nested_splits=[params_nested_splits],
    params_dense_values=params_dense_values,
    indices=indices,
    OUTPUT_RAGGED_RANK=1,
    name=None
)
```

### Relevant log output

```shell
2025-01-18 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```",LongZE666,2025-01-18 09:32:16+00:00,['Venkat6871'],2025-01-21 06:14:03+00:00,,https://github.com/tensorflow/tensorflow/issues/85242,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2603748883, 'issue_id': 2796881534, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cc91ef87e87c5d0a5581af70cbc8ad16/85242_tf_2-18-0-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 21, 6, 14, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-21 06:14:03 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cc91ef87e87c5d0a5581af70cbc8ad16/85242_tf_2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

"
2796877194,issue,open,,Segmentation fault (core dumped) in `RaggedTensorToTensor`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```",LongZE666,2025-01-18 09:27:19+00:00,['Venkat6871'],2025-01-21 06:07:07+00:00,,https://github.com/tensorflow/tensorflow/issues/85240,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2603740231, 'issue_id': 2796877194, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/2f7a31b5fe15d28ca1406f9805b96a4f/85240_tf_2-18-0-nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 21, 6, 6, 54, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-21 06:06:54 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/2f7a31b5fe15d28ca1406f9805b96a4f/85240_tf_2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

"
2796746011,issue,open,,TFnode on TensorflowonSpark 2.2.5,"I'm running some code on Microsoft Fabric, and I use the following line:
def map_fun(tf_args, ctx):
cluster, server = TFNode.start_cluster_server(ctx)
print('ctx')
if ctx.job_name == ""ps"":
server.join()
else:
print(""Hello from worker"", ctx.task_index)

I'm getting an error regarding TFnode, does anyone know why this could be?
TensorFlow version:2.12
TensorflowonSpark:2.2.5


The idea is to print a hello world to see if they are running on differents clusters.",cmilanes93,2025-01-18 05:18:16+00:00,['tilakrayal'],2025-02-07 02:01:25+00:00,,https://github.com/tensorflow/tensorflow/issues/85217,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2601155618, 'issue_id': 2796746011, 'author': 'cmilanes93', 'body': 'Hi, can someone help me? This is going to blow my mind ', 'created_at': datetime.datetime(2025, 1, 20, 1, 42, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603767601, 'issue_id': 2796746011, 'author': 'tilakrayal', 'body': '@cmilanes93,\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using. Thank you!', 'created_at': datetime.datetime(2025, 1, 21, 6, 28, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604595965, 'issue_id': 2796746011, 'author': 'arzoo0511', 'body': 'The error is likely due to the fact that TensorFlowOnSpark (TFoS) does not support TensorFlow versions greater than 1.x. Your TensorFlow version is 2.12, and TFoS 2.2.5 is designed for TensorFlow 1.x, which leads to compatibility issues. install TensorFlow 1.15 to match the compatibility of TensorFlowOnSpark', 'created_at': datetime.datetime(2025, 1, 21, 12, 25, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608217955, 'issue_id': 2796746011, 'author': 'cmilanes93', 'body': '> [@cmilanes93](https://github.com/cmilanes93), Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using. Thank you!\n\nHi this is the code:\n#La funcin TFNode.start_cluster_server utilizada en la biblioteca tensorflowonspark est obsoleta y no debera usarse en las versiones actuales de TensorFlow.\n\nimport os\nimport datetime\n\nimport numpy as np\nimport pandas as pd\n\n# PySpark / Spark\nfrom pyspark.sql import SparkSession\n\n# TensorFlowOnSpark\nfrom tensorflowonspark import TFCluster, TFNode\n\nspark = SparkSession.builder \\\n    .config(""spark.executor.instances"", ""2"") \\\n    .config(""spark.executor.cores"", ""1"") \\\n    .config(""spark.dynamicAllocation.enabled"", ""false"") \\\n    .config(""spark.shuffle.service.enabled"", ""false"") \\\n    .getOrCreate()\n\ndef map_fun(tf_args, ctx):\n    cluster, server = TFNode.start_cluster_server(ctx)\n    print(\'ctx\')\n    if ctx.job_name == ""ps"":\n        server.join()\n    else:\n        print(""Hello from worker"", ctx.task_index)\n\n# Configuracin consistente\ncluster = TFCluster.run(\n    sc=spark.sparkContext,\n    map_fun=map_fun,\n    tf_args={},\n    num_executors=2,  # Debe coincidir con spark.executor.instances\n    num_ps=0,         # Nmero de servidores de parmetros\n    input_mode=TFCluster.InputMode.SPARK\n)\n\n# RDD vaco para el entrenamiento\nrdd = spark.sparkContext.parallelize([])\ncluster.train(rdd, 1)\n#cluster.shutdown()', 'created_at': datetime.datetime(2025, 1, 22, 20, 39, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608219873, 'issue_id': 2796746011, 'author': 'cmilanes93', 'body': '> The error is likely due to the fact that TensorFlowOnSpark (TFoS) does not support TensorFlow versions greater than 1.x. Your TensorFlow version is 2.12, and TFoS 2.2.5 is designed for TensorFlow 1.x, which leads to compatibility issues. install TensorFlow 1.15 to match the compatibility of TensorFlowOnSpark\n\nIs there any other option, im using Microsoft Fabric, so there is no way to install a lower version than tensorflow 2.x.', 'created_at': datetime.datetime(2025, 1, 22, 20, 40, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624017347, 'issue_id': 2796746011, 'author': 'tilakrayal', 'body': ""@cmilanes93,\nLooks like this issue is not related to the Tensorflow. Take a look at TensorFlow on Spark if you already have Spark cluster:\nhttps://github.com/yahoo/TensorFlowOnSpark\n\nOr if you're running Kubernetes, you can use a configuration from https://github.com/tensorflow/ecosystem\n\nThank you!"", 'created_at': datetime.datetime(2025, 1, 30, 9, 54, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750162, 'issue_id': 2796746011, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 24, tzinfo=datetime.timezone.utc)}]","cmilanes93 (Issue Creator) on (2025-01-20 01:42:31 UTC): Hi, can someone help me? This is going to blow my mind 

tilakrayal (Assginee) on (2025-01-21 06:28:28 UTC): @cmilanes93,
Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using. Thank you!

arzoo0511 on (2025-01-21 12:25:59 UTC): The error is likely due to the fact that TensorFlowOnSpark (TFoS) does not support TensorFlow versions greater than 1.x. Your TensorFlow version is 2.12, and TFoS 2.2.5 is designed for TensorFlow 1.x, which leads to compatibility issues. install TensorFlow 1.15 to match the compatibility of TensorFlowOnSpark

cmilanes93 (Issue Creator) on (2025-01-22 20:39:20 UTC): Hi this is the code:
#La funcin TFNode.start_cluster_server utilizada en la biblioteca tensorflowonspark est obsoleta y no debera usarse en las versiones actuales de TensorFlow.

import os
import datetime

import numpy as np
import pandas as pd

# PySpark / Spark
from pyspark.sql import SparkSession

# TensorFlowOnSpark
from tensorflowonspark import TFCluster, TFNode

spark = SparkSession.builder \
    .config(""spark.executor.instances"", ""2"") \
    .config(""spark.executor.cores"", ""1"") \
    .config(""spark.dynamicAllocation.enabled"", ""false"") \
    .config(""spark.shuffle.service.enabled"", ""false"") \
    .getOrCreate()

def map_fun(tf_args, ctx):
    cluster, server = TFNode.start_cluster_server(ctx)
    print('ctx')
    if ctx.job_name == ""ps"":
        server.join()
    else:
        print(""Hello from worker"", ctx.task_index)

# Configuracin consistente
cluster = TFCluster.run(
    sc=spark.sparkContext,
    map_fun=map_fun,
    tf_args={},
    num_executors=2,  # Debe coincidir con spark.executor.instances
    num_ps=0,         # Nmero de servidores de parmetros
    input_mode=TFCluster.InputMode.SPARK
)

# RDD vaco para el entrenamiento
rdd = spark.sparkContext.parallelize([])
cluster.train(rdd, 1)
#cluster.shutdown()

cmilanes93 (Issue Creator) on (2025-01-22 20:40:32 UTC): Is there any other option, im using Microsoft Fabric, so there is no way to install a lower version than tensorflow 2.x.

tilakrayal (Assginee) on (2025-01-30 09:54:26 UTC): @cmilanes93,
Looks like this issue is not related to the Tensorflow. Take a look at TensorFlow on Spark if you already have Spark cluster:
https://github.com/yahoo/TensorFlowOnSpark

Or if you're running Kubernetes, you can use a configuration from https://github.com/tensorflow/ecosystem

Thank you!

github-actions[bot] on (2025-02-07 02:01:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2795716674,issue,closed,completed,Could not get sample weight from customized loss,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.1

### Custom code

Yes

### OS platform and distribution

CentOS 7.9

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We used customized loss for the model training, and would like to get sample weight to calculate the loss.
However, sample weight does not pass to loss function as expected.


### Standalone code to reproduce the issue

```shell
#Here are the code to reproduce the issue.

import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np

def weighted_zero_mean_r2_loss(y_true, y_pred, sample_weight=None):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    sample_weight = tf.cast(sample_weight, tf.float32)
    
    weighted_squared_error = sample_weight * (y_true - y_pred) ** 2
    weighted_true_squared = sample_weight * y_true ** 2
    
    numerator = tf.reduce_sum(weighted_squared_error)
    denominator = tf.reduce_sum(weighted_true_squared)
    
    r2_score = 1 - numerator / denominator
    return r2_score

def build_model():
    metrics = 'mae'
    loss = weighted_zero_mean_r2_loss #'mse'
    output_num = 1
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(output_num, activation='linear')(lstm_out)
    model = tf.keras.Model(inputs, output)
    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen(class_weights):
    while True:
        x_batch = np.random.rand(128, 40, 36)
        y_batch = np.random.randint(0, 3, (128, 1))
        # Apply class weights to the labels
        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)
        yield x_batch, y_batch, sample_weights

model = build_model()
cw = {0: 0.3, 1: 2.5, 2: 3.2}
model.fit(data_gen(cw), epochs=2, steps_per_epoch=10)
```

### Relevant log output

```shell
Error messages:
File ""/root/.virtualenvs/infinity_stock/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/data/release/kagglejanestreet/scripts/python/test.py"", line 8, in weighted_zero_mean_r2_loss  *
        sample_weight = tf.cast(sample_weight, tf.float32)

    ValueError: None values not supported.
```",henghamao,2025-01-17 15:16:39+00:00,['Venkat6871'],2025-02-01 13:46:26+00:00,2025-01-30 14:11:15+00:00,https://github.com/tensorflow/tensorflow/issues/85174,"[('type:feature', 'Feature requests'), ('comp:keras', 'Keras related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2603653276, 'issue_id': 2795716674, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow v2.13 and TF-nightly. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/711469bb5724c96183aa44393b22b5af/85174_tf_2-13-0.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f73f3cff9d907bfb67a32e15578d8144/85174_tf_nightly-v.ipynb) here for your reference.\nThank you!', 'created_at': datetime.datetime(2025, 1, 21, 4, 53, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614541153, 'issue_id': 2795716674, 'author': 'SanjaySG', 'body': ""Hi @henghamao \n\nWhen you are building a custom loss function, the loss function should have a signature of **```custom_loss_fn(y_true, y_pred)```** [(source)](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#custom_losses). When doing this, you are essentially trying to override the [`call()`](https://github.com/keras-team/keras/blob/734cd03b9413e65dea392e25c420676dd04f080a/keras/src/losses/loss.py#L87 ) function in `class Loss` in keras/src/losses/loss/py. \n\nYou can still pass in a `sample_weight` parameter to the `model.fit()` call with the custom loss function, but sample_weight will be multiplied element-wise with the loss terms in the [reduce_weighted_values()](https://github.com/keras-team/keras/blob/734cd03b9413e65dea392e25c420676dd04f080a/keras/src/losses/loss.py#L79-L84) call. So it won't work the way you want it to for the r2 loss.\n\nOne observation about your code is that sample_weight only depends on y_true, so you could move the sample_weight computation inside your function.  The following code would work:\n```\ndef weighted_zero_mean_r2_loss(y_true, y_pred):\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true = tf.cast(y_true, tf.float32)\n    table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(\n            keys=tf.constant([0, 1, 2], dtype=tf.int32), \n            values=tf.constant([0.3, 2.5, 3.2], dtype=tf.float32)),\n        default_value=-1)\n    sample_weight = table.lookup(tf.cast(y_true, tf.int32))\n\n    weighted_squared_error = sample_weight * (y_true - y_pred) ** 2\n    weighted_true_squared = sample_weight * y_true ** 2\n\n    numerator = tf.reduce_sum(weighted_squared_error)\n    denominator = tf.reduce_sum(weighted_true_squared)\n\n    r2_score = 1 - numerator / denominator\n    return r2_score\n```\n\n\nIf you want to use a custom logic for applying sample_weights, there's another way to do it by subclassing the `keras.losses.Loss` class. You would have to override the `__init__` and `call` functions. You can pass in a custom function during `__init__` and use it during `call()`. For example:\n```\nclass WeightedZeroMeanR2Loss((keras.losses.Loss):\n  def __init__(self, custom_weight_multiplier_fn, name='weighted_zero_mean_r2_loss'):\n    super().__init__(name=name)\n    self._custom_weight_multiplier_fn = custom_weight_multiplier_fn\n\n  def call(self, y_true, y_pred): \n    sample_weight = self._custom_weight_multiplier_fn(y_true)\n    \n    weighted_squared_error = sample_weight * (y_true - y_pred) ** 2\n    weighted_true_squared = sample_weight * y_true ** 2\n    \n    numerator = tf.reduce_sum(weighted_squared_error)\n    denominator = tf.reduce_sum(weighted_true_squared)\n    \n    r2_score = 1 - numerator / denominator\n    return r2_score\n```\n\nNote that you would probably need to use tensorflow operations and not python operations, since the loss function gets converted to a tf.function during the graph execution.\n\nYou can initialize the model with the loss class above as follows:\n```\n    model.compile(loss=WeightedZeroMeanR2Loss(custom_fn), optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n```\n\nI hope this explanation helps. Please let me know if you have any questions."", 'created_at': datetime.datetime(2025, 1, 26, 18, 34, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614717122, 'issue_id': 2795716674, 'author': 'henghamao', 'body': 'Hi @SanjaySG ,\n\nThanks for the reply.\nThe code is to reproduce the issue. And in real scenario, the sample weight could not refer by y_ture value. \nThe problme is the regression for multiple categories of data. The sample weight is to apply for different categories. y_ture is the data point at time stamp t. If it is a classify problem, we could refer sample weight by category label of y_ture value. However, for regression problems, we could not do that.\nIn torch, we could easily use customize loss to calcualte weighted_r2_loss. Example code as below\n```\nclass WeightedR2Loss(nn.Module):\n    """"""PyTorch loss function for weighted R.""""""\n    def __init__(self, epsilon: float = 1e-38) -> None:\n        """"""\n        Initialize the WeightedR2Loss class.\n\n        Args:\n            epsilon (float, optional): Small constant added to the denominator \n                for numerical stability. Defaults to 1e-38.\n        """"""\n        super(WeightedR2Loss, self).__init__()\n        self.epsilon = epsilon\n\n    def forward(\n        self,\n        y_pred: torch.Tensor,\n        y_true: torch.Tensor,\n        weights: torch.Tensor) -> torch.Tensor:\n        """"""Compute the weighted R loss.\n\n        Args:\n            y_true (torch.Tensor): Ground truth tensor.\n            y_pred (torch.Tensor): Predicted tensor.\n            weights (torch.Tensor): Weights for each observation (same shape as y_true).\n\n        Returns:\n            torch.Tensor: Computed weighted R loss.\n        """"""\n        numerator = torch.sum(weights * (y_pred - y_true) ** 2)\n        denominator = torch.sum(weights * (y_true) ** 2) + 1e-38\n        loss = numerator / denominator\n        return loss\n```\nHope tf could provide similar solution to solve the problem.', 'created_at': datetime.datetime(2025, 1, 27, 1, 56, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620859292, 'issue_id': 2795716674, 'author': 'SanjaySG', 'body': ""@henghamao \nThank you for going over your use case in detail. You can do this in Tensorflow by subclassing `keras.Model` and overriding the `compute_loss()` function.\n\n```\nclass CustomModel(keras.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def compute_loss(self, x, y_true, y_pred, sample_weight):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        sample_weight = tf.cast(sample_weight, tf.float32)\n        \n        weighted_squared_error = sample_weight * (y_true - y_pred) ** 2\n        weighted_true_squared = sample_weight * y_true ** 2\n        \n        numerator = tf.reduce_sum(weighted_squared_error)\n        denominator = tf.reduce_sum(weighted_true_squared)\n        \n        r2_score = 1 - numerator / denominator\n        return r2_score\n\ndef build_model():\n    metrics = ['mae']\n    output_num = 1\n    inputs = tf.keras.layers.Input(shape=(40, 36))\n    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)\n    output = Dense(output_num, activation='linear')(lstm_out)\n    model = CustomModel(inputs, output)\n    class_weights = {0: 0.3, 1: 2.5, 2: 3.2}\n    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n    model.summary()\n    return model\n\ndef data_gen(class_weights):\n    while True:\n        x_batch = np.random.rand(128, 40, 36)\n        y_batch = np.random.randint(0, 3, (128, 1))\n        # Apply class weights to the labels\n        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)\n        yield x_batch, y_batch, sample_weights\n\nmodel = build_model()\ncw = {0: 0.3, 1: 2.5, 2: 3.2}\nmodel.fit(data_gen(cw), epochs=2, steps_per_epoch=10)\n```"", 'created_at': datetime.datetime(2025, 1, 29, 7, 5, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624622392, 'issue_id': 2795716674, 'author': 'henghamao', 'body': '@SanjaySG \nGreat thanks for providing the solution.\nIt works for our problems.\nBTW, there is another issue about class weight and sample weight with the similar code to reproduce the issue.\nhttps://github.com/tensorflow/tensorflow/issues/77958\nWe submited the issue a few months ago, and it did not get any further updates.', 'created_at': datetime.datetime(2025, 1, 30, 14, 10, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626306401, 'issue_id': 2795716674, 'author': 'SanjaySG', 'body': 'Happy to help! \nLet me take a look at https://github.com/tensorflow/tensorflow/issues/77958.', 'created_at': datetime.datetime(2025, 1, 31, 5, 1, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627202552, 'issue_id': 2795716674, 'author': 'henghamao', 'body': ""@SanjaySG \nHi SanjaySG,\n\nWe found a problem with the code.\nBy using comput_loss(), the model tend to maximize the loss, rather than minimize the loss.\nHere is the code to repoduce the issue:\n```\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\n\n\nclass CustomModel(tf.keras.Model):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def compute_loss(self, x, y_true, y_pred, sample_weight):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        sample_weight = tf.cast(sample_weight, tf.float32)\n\n        weighted_squared_error = sample_weight * (y_true - y_pred) ** 2\n        weighted_true_squared = sample_weight * y_true ** 2\n\n        numerator = tf.reduce_sum(weighted_squared_error)\n        denominator = tf.reduce_sum(weighted_true_squared)\n\n        r2_score = 1 - numerator / denominator\n        return r2_score\n\ndef build_model():\n    metrics = ['mae']\n    output_num = 1\n    inputs = tf.keras.layers.Input(shape=(40, 36))\n    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)\n    output = Dense(output_num, activation='linear')(lstm_out)\n    model = CustomModel(inputs, output)\n    class_weights = {0: 0.3, 1: 2.5, 2: 3.2}\n    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n    model.summary()\n    return model\n\ndef data_gen(class_weights):\n    while True:\n        x_batch = np.random.rand(128, 40, 36)\n        y_batch = np.random.randint(0, 3, (128, 1))\n        # Apply class weights to the labels\n        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)\n        yield x_batch, y_batch, sample_weights\n\nmodel = build_model()\ncw = {0: 0.3, 1: 2.5, 2: 3.2}\nmodel.fit(data_gen(cw), epochs=50, steps_per_epoch=10, verbose=2)\n```\nWe observed the metric 'mae' keep growing, and the loss grows as well."", 'created_at': datetime.datetime(2025, 1, 31, 12, 42, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628764542, 'issue_id': 2795716674, 'author': 'SanjaySG', 'body': ""@henghamao \nThis is down to the objective chosen for optimization. The model.fit() call tries to **minimize** the loss. r2 on the other hand should increase when the model performs better. A naive way to do this is by providing the negative of r2. \nMore importantly, r2 is not differentiable. So ideally, it shouldn't be used as a loss function for gradient descent, but it can be used as a metric. Something like MSE or MAE would be a better loss function."", 'created_at': datetime.datetime(2025, 2, 1, 4, 33, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628959789, 'issue_id': 2795716674, 'author': 'henghamao', 'body': 'Great thanks for the explanations.', 'created_at': datetime.datetime(2025, 2, 1, 13, 46, 24, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-21 04:53:33 UTC): I was able to reproduce the issue on Colab using TensorFlow v2.13 and TF-nightly. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/711469bb5724c96183aa44393b22b5af/85174_tf_2-13-0.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/f73f3cff9d907bfb67a32e15578d8144/85174_tf_nightly-v.ipynb) here for your reference.
Thank you!

SanjaySG on (2025-01-26 18:34:20 UTC): Hi @henghamao 

When you are building a custom loss function, the loss function should have a signature of **```custom_loss_fn(y_true, y_pred)```** [(source)](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#custom_losses). When doing this, you are essentially trying to override the [`call()`](https://github.com/keras-team/keras/blob/734cd03b9413e65dea392e25c420676dd04f080a/keras/src/losses/loss.py#L87 ) function in `class Loss` in keras/src/losses/loss/py. 

You can still pass in a `sample_weight` parameter to the `model.fit()` call with the custom loss function, but sample_weight will be multiplied element-wise with the loss terms in the [reduce_weighted_values()](https://github.com/keras-team/keras/blob/734cd03b9413e65dea392e25c420676dd04f080a/keras/src/losses/loss.py#L79-L84) call. So it won't work the way you want it to for the r2 loss.

One observation about your code is that sample_weight only depends on y_true, so you could move the sample_weight computation inside your function.  The following code would work:
```
def weighted_zero_mean_r2_loss(y_true, y_pred):
    y_pred = tf.cast(y_pred, tf.float32)
    y_true = tf.cast(y_true, tf.float32)
    table = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            keys=tf.constant([0, 1, 2], dtype=tf.int32), 
            values=tf.constant([0.3, 2.5, 3.2], dtype=tf.float32)),
        default_value=-1)
    sample_weight = table.lookup(tf.cast(y_true, tf.int32))

    weighted_squared_error = sample_weight * (y_true - y_pred) ** 2
    weighted_true_squared = sample_weight * y_true ** 2

    numerator = tf.reduce_sum(weighted_squared_error)
    denominator = tf.reduce_sum(weighted_true_squared)

    r2_score = 1 - numerator / denominator
    return r2_score
```


If you want to use a custom logic for applying sample_weights, there's another way to do it by subclassing the `keras.losses.Loss` class. You would have to override the `__init__` and `call` functions. You can pass in a custom function during `__init__` and use it during `call()`. For example:
```
class WeightedZeroMeanR2Loss((keras.losses.Loss):
  def __init__(self, custom_weight_multiplier_fn, name='weighted_zero_mean_r2_loss'):
    super().__init__(name=name)
    self._custom_weight_multiplier_fn = custom_weight_multiplier_fn

  def call(self, y_true, y_pred): 
    sample_weight = self._custom_weight_multiplier_fn(y_true)
    
    weighted_squared_error = sample_weight * (y_true - y_pred) ** 2
    weighted_true_squared = sample_weight * y_true ** 2
    
    numerator = tf.reduce_sum(weighted_squared_error)
    denominator = tf.reduce_sum(weighted_true_squared)
    
    r2_score = 1 - numerator / denominator
    return r2_score
```

Note that you would probably need to use tensorflow operations and not python operations, since the loss function gets converted to a tf.function during the graph execution.

You can initialize the model with the loss class above as follows:
```
    model.compile(loss=WeightedZeroMeanR2Loss(custom_fn), optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
```

I hope this explanation helps. Please let me know if you have any questions.

henghamao (Issue Creator) on (2025-01-27 01:56:14 UTC): Hi @SanjaySG ,

Thanks for the reply.
The code is to reproduce the issue. And in real scenario, the sample weight could not refer by y_ture value. 
The problme is the regression for multiple categories of data. The sample weight is to apply for different categories. y_ture is the data point at time stamp t. If it is a classify problem, we could refer sample weight by category label of y_ture value. However, for regression problems, we could not do that.
In torch, we could easily use customize loss to calcualte weighted_r2_loss. Example code as below
```
class WeightedR2Loss(nn.Module):
    """"""PyTorch loss function for weighted R.""""""
    def __init__(self, epsilon: float = 1e-38) -> None:
        """"""
        Initialize the WeightedR2Loss class.

        Args:
            epsilon (float, optional): Small constant added to the denominator 
                for numerical stability. Defaults to 1e-38.
        """"""
        super(WeightedR2Loss, self).__init__()
        self.epsilon = epsilon

    def forward(
        self,
        y_pred: torch.Tensor,
        y_true: torch.Tensor,
        weights: torch.Tensor) -> torch.Tensor:
        """"""Compute the weighted R loss.

        Args:
            y_true (torch.Tensor): Ground truth tensor.
            y_pred (torch.Tensor): Predicted tensor.
            weights (torch.Tensor): Weights for each observation (same shape as y_true).

        Returns:
            torch.Tensor: Computed weighted R loss.
        """"""
        numerator = torch.sum(weights * (y_pred - y_true) ** 2)
        denominator = torch.sum(weights * (y_true) ** 2) + 1e-38
        loss = numerator / denominator
        return loss
```
Hope tf could provide similar solution to solve the problem.

SanjaySG on (2025-01-29 07:05:03 UTC): @henghamao 
Thank you for going over your use case in detail. You can do this in Tensorflow by subclassing `keras.Model` and overriding the `compute_loss()` function.

```
class CustomModel(keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def compute_loss(self, x, y_true, y_pred, sample_weight):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        sample_weight = tf.cast(sample_weight, tf.float32)
        
        weighted_squared_error = sample_weight * (y_true - y_pred) ** 2
        weighted_true_squared = sample_weight * y_true ** 2
        
        numerator = tf.reduce_sum(weighted_squared_error)
        denominator = tf.reduce_sum(weighted_true_squared)
        
        r2_score = 1 - numerator / denominator
        return r2_score

def build_model():
    metrics = ['mae']
    output_num = 1
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(output_num, activation='linear')(lstm_out)
    model = CustomModel(inputs, output)
    class_weights = {0: 0.3, 1: 2.5, 2: 3.2}
    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen(class_weights):
    while True:
        x_batch = np.random.rand(128, 40, 36)
        y_batch = np.random.randint(0, 3, (128, 1))
        # Apply class weights to the labels
        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)
        yield x_batch, y_batch, sample_weights

model = build_model()
cw = {0: 0.3, 1: 2.5, 2: 3.2}
model.fit(data_gen(cw), epochs=2, steps_per_epoch=10)
```

henghamao (Issue Creator) on (2025-01-30 14:10:43 UTC): @SanjaySG 
Great thanks for providing the solution.
It works for our problems.
BTW, there is another issue about class weight and sample weight with the similar code to reproduce the issue.
https://github.com/tensorflow/tensorflow/issues/77958
We submited the issue a few months ago, and it did not get any further updates.

SanjaySG on (2025-01-31 05:01:04 UTC): Happy to help! 
Let me take a look at https://github.com/tensorflow/tensorflow/issues/77958.

henghamao (Issue Creator) on (2025-01-31 12:42:29 UTC): @SanjaySG 
Hi SanjaySG,

We found a problem with the code.
By using comput_loss(), the model tend to maximize the loss, rather than minimize the loss.
Here is the code to repoduce the issue:
```
import tensorflow as tf
from tensorflow.keras.layers import Dense
import numpy as np


class CustomModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def compute_loss(self, x, y_true, y_pred, sample_weight):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        sample_weight = tf.cast(sample_weight, tf.float32)

        weighted_squared_error = sample_weight * (y_true - y_pred) ** 2
        weighted_true_squared = sample_weight * y_true ** 2

        numerator = tf.reduce_sum(weighted_squared_error)
        denominator = tf.reduce_sum(weighted_true_squared)

        r2_score = 1 - numerator / denominator
        return r2_score

def build_model():
    metrics = ['mae']
    output_num = 1
    inputs = tf.keras.layers.Input(shape=(40, 36))
    lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
    output = Dense(output_num, activation='linear')(lstm_out)
    model = CustomModel(inputs, output)
    class_weights = {0: 0.3, 1: 2.5, 2: 3.2}
    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
    model.summary()
    return model

def data_gen(class_weights):
    while True:
        x_batch = np.random.rand(128, 40, 36)
        y_batch = np.random.randint(0, 3, (128, 1))
        # Apply class weights to the labels
        sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)
        yield x_batch, y_batch, sample_weights

model = build_model()
cw = {0: 0.3, 1: 2.5, 2: 3.2}
model.fit(data_gen(cw), epochs=50, steps_per_epoch=10, verbose=2)
```
We observed the metric 'mae' keep growing, and the loss grows as well.

SanjaySG on (2025-02-01 04:33:58 UTC): @henghamao 
This is down to the objective chosen for optimization. The model.fit() call tries to **minimize** the loss. r2 on the other hand should increase when the model performs better. A naive way to do this is by providing the negative of r2. 
More importantly, r2 is not differentiable. So ideally, it shouldn't be used as a loss function for gradient descent, but it can be used as a metric. Something like MSE or MAE would be a better loss function.

henghamao (Issue Creator) on (2025-02-01 13:46:24 UTC): Great thanks for the explanations.

"
2795041016,issue,closed,completed,Unequal width and height of stride in tf.nn.depthwise_conv2d not supported?,"Is that right?

IF YES, how can I convert the pretrained weights trained with unequal strides to tensorflow `dw-conv` with some other ops?

THX!",inkzk,2025-01-17 10:00:13+00:00,['tilakrayal'],2025-02-05 02:00:50+00:00,2025-02-05 02:00:47+00:00,https://github.com/tensorflow/tensorflow/issues/85131,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2603766273, 'issue_id': 2795041016, 'author': 'tilakrayal', 'body': '@inkzk,\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using. Thank you!', 'created_at': datetime.datetime(2025, 1, 21, 6, 27, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469127, 'issue_id': 2795041016, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534444, 'issue_id': 2795041016, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534507, 'issue_id': 2795041016, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85131"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85131"">No</a>', 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 49, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-21 06:27:21 UTC): @inkzk,
Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using. Thank you!

github-actions[bot] on (2025-01-29 01:59:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-05 02:00:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-05 02:00:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85131"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85131"">No</a>

"
2793395056,issue,closed,completed,Urgent help needed.,"Hello, hope anybody help me please!

I am going to train a model for information extraction from photo.
I am using Windows 10.
And am using python 3.10.10.
Actually I used python 3.12 but I couldn't install tensorflow-io package with python 3.12. Not sure reason yet.
So I use python 3.10 now.

I have prepared dataset and downloaded predefined model from Model Zoo.
Now I need to train.

Steps that I did:

1. git clone https://github.com/tensorflow/models.git
    cd models

2. pip install tensorflow-text (It will automatically install tensorflow==2.10.1)
    pip install tensorflow-io

3. cd official
    pip install -r requirements.txt
4. cd ..
    cd research
    in object_detection/packages/tf2/setup.py file
    change version of tf-models-official into 2.10.1 or 2.10.0
    python object_detection/packages/tf2/setup.py install

5. $env:PYTHONPATH = ""$(Get-Location):$(Get-Location)\slim""  in Powershell or
    set PYTHONPATH=%cd%;%cd%\slim  in Cmd

6. protoc object_detection/protos/*.proto --python_out=.
    protoc and protobuf version are same. 3.19.6

7. Run train command
    python object_detection/model_main_tf2.py --pipeline_config_path=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config 
    --model_dir=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint --num_train_steps=5000 -- 
    sample_1_of_n_eval_examples=1 --alsologtostderr

I have not succeded in here.
For not below error I have.

venv\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 34, in <module>
    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
TypeError: Unable to convert function return value to a Python type! The signature was
        () -> handle


But I tried with many other python versions, tensorflow versions, but every time I met version conflict issues and bugs inside packages.

Hope anybody help me run this process to train. I need help urgently.
Thanks.
",cg-tester,2025-01-16 17:50:21+00:00,['Venkat6871'],2025-02-07 02:01:29+00:00,2025-02-07 02:01:27+00:00,https://github.com/tensorflow/tensorflow/issues/85074,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2596649550, 'issue_id': 2793395056, 'author': 'mihaimaruseac', 'body': ""> Actually I used python 3.12 but I couldn't install tensorflow-io package with python 3.12. Not sure reason yet.\n\nThere is no support for Python 3.12 on Windows on tensorflow-io. See https://pypi.org/project/tensorflow-io/#files, there is no Windows file there. You can install on Python 3.10 (or 3.11 too) because an older version supports Windows: https://pypi.org/project/tensorflow-io/0.31.0/#files.\n\nBut that version, 0.31.0, was released nearly 2 years ago, so it is not compatible with Tensorflow 2.10 which was released in 2022.\n\nYou need to have everything at compatible versions. Ideally, you'd also use versions that are in the support window (last release and 1 or 2 releases behind that)."", 'created_at': datetime.datetime(2025, 1, 16, 19, 25, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596797964, 'issue_id': 2793395056, 'author': 'cg-tester', 'body': 'Thanks for your reply @mihaimaruseac \nWell, could you let me know versions that are compatible?\n\nI use python 3.10 now, because I am using Windows OS.\nI need to train model on Windows OS absolutely.\n\nSo could you give me list of compatible versions of packages like tensorflow, tensorflow-text, tensorflow-io, protobuf, tf-models-official and so on.\n\nHighest available version of tensorflow-text is 2.10, so I am using tensorflow 2.10\nbut still have many issues inside tensorflow package files.\nNot sure reason.\n\nHope you to provide me good set of packages and usage experience, if possible.\nThanks very much. Hope to build friendship!!!', 'created_at': datetime.datetime(2025, 1, 16, 20, 8, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596842658, 'issue_id': 2793395056, 'author': 'mihaimaruseac', 'body': 'TensorFlow is the main repo in the ecosystem. We guarantee support for the last released version and 1 or 2 behind it. You can use https://pypi.org/ to locate all of the packages you need and make sure they were released at around the same time.\n\nFor a package, if you go to the files tab you can see all combinations that are supported. Or you can try `pip install $package==$version` (`$package` and `$version` are placeholders here) and see if they get installed on your system. If you cannot install one, then most likely your system is not supported / too old (either the system or the software stack you are using).', 'created_at': datetime.datetime(2025, 1, 16, 20, 36, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127340, 'issue_id': 2793395056, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750230, 'issue_id': 2793395056, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750283, 'issue_id': 2793395056, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85074"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85074"">No</a>', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 28, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-16 19:25:32 UTC): There is no support for Python 3.12 on Windows on tensorflow-io. See https://pypi.org/project/tensorflow-io/#files, there is no Windows file there. You can install on Python 3.10 (or 3.11 too) because an older version supports Windows: https://pypi.org/project/tensorflow-io/0.31.0/#files.

But that version, 0.31.0, was released nearly 2 years ago, so it is not compatible with Tensorflow 2.10 which was released in 2022.

You need to have everything at compatible versions. Ideally, you'd also use versions that are in the support window (last release and 1 or 2 releases behind that).

cg-tester (Issue Creator) on (2025-01-16 20:08:22 UTC): Thanks for your reply @mihaimaruseac 
Well, could you let me know versions that are compatible?

I use python 3.10 now, because I am using Windows OS.
I need to train model on Windows OS absolutely.

So could you give me list of compatible versions of packages like tensorflow, tensorflow-text, tensorflow-io, protobuf, tf-models-official and so on.

Highest available version of tensorflow-text is 2.10, so I am using tensorflow 2.10
but still have many issues inside tensorflow package files.
Not sure reason.

Hope you to provide me good set of packages and usage experience, if possible.
Thanks very much. Hope to build friendship!!!

mihaimaruseac on (2025-01-16 20:36:42 UTC): TensorFlow is the main repo in the ecosystem. We guarantee support for the last released version and 1 or 2 behind it. You can use https://pypi.org/ to locate all of the packages you need and make sure they were released at around the same time.

For a package, if you go to the files tab you can see all combinations that are supported. Or you can try `pip install $package==$version` (`$package` and `$version` are placeholders here) and see if they get installed on your system. If you cannot install one, then most likely your system is not supported / too old (either the system or the software stack you are using).

github-actions[bot] on (2025-01-31 01:59:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-07 02:01:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-07 02:01:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85074"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85074"">No</a>

"
2793253924,issue,closed,duplicate,Unable to convert function return value to a Python type! Error Fix Solution Needed,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using Windows 10
I use python 3.10.10, because when I use python 3.12, I couldn't install tensorflow-io
I installed tensorflow 2.10.1, because only tensorflow-text 2.10.0 is available on Windows 10.

Actually I am going to train model.
So I cloned tensorflow models repository
and made dataset, ...

I want help to run models/research/object_detection/model_main_tf2.py file without error.
Please help me.

### Standalone code to reproduce the issue

```shell
in tensorflow/python/framework/dtypes.py
line 29
_np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()

Here I have error
TypeError: Unable to convert function return value to a Python type! The signature was
        () -> handle
```

### Relevant log output

```shell

```",cg-tester,2025-01-16 16:40:25+00:00,['tilakrayal'],2025-01-16 21:23:32+00:00,2025-01-16 21:23:29+00:00,https://github.com/tensorflow/tensorflow/issues/85070,"[('type:bug', 'Bug')]","[{'comment_id': 2596914915, 'issue_id': 2793253924, 'author': 'mihaimaruseac', 'body': ""This looks like a duplicate. Please don't open multiple issues for the same thing. Also, please don't ask for urgent help, this is an OSS community."", 'created_at': datetime.datetime(2025, 1, 16, 21, 23, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596914957, 'issue_id': 2793253924, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85070"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85070"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 21, 23, 31, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-16 21:23:29 UTC): This looks like a duplicate. Please don't open multiple issues for the same thing. Also, please don't ask for urgent help, this is an OSS community.

google-ml-butler[bot] on (2025-01-16 21:23:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85070"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85070"">No</a>

"
2793094568,issue,closed,completed,tf.math.bincount no longer broadcasts over weights,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18.0 (behaviour started 2.15)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Could not find cuda drivers on your machine

### GPU model and memory

_No response_

### Current behavior?

The `tf.math.bincount` implementation used to broadcast nicely when `values` was a flat tensor, allowing the same `values` to be used for all rows of the `weights` passed in. 

However, this is no longer the case. The new behaviour seems to make it impossible to broadcast `values` at all, even if reshaping them to match the shape of `weights`.

If you run the script below using tf 2.14, it runs fine and produces sensible output, two columns bincounted.

However, if run in 2.15 or later, it throws `weights must be the same shape as arr or a length-0 Tensor`.

I have tried reshaping the values to match the shape of weights (as per the commented out line of code in the snippet below), but this still does not return the same output as in 2.14 - the output is a single column, rather than sensibly bincounted data for two columns. This does not seem to be solvable by using the other arguments (e.g. axis=-1).


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

values = tf.constant(
    [1, 2, 3, 1, 2, 3],
)

# Alternative: Try repeating values to be same shape as weights
# values = tf.repeat(tf.reshape(values, (-1, 1)), 2, axis=1)

weights = tf.constant([
    [0.1,  0.5],  # corresponds to 1, as per `values` above
    [0.2,  0.1],  # corresponds to 2
    [0.3,  0.03],  # etc
    [0.01, 0.05],
    [0.02, 0.01],
    [0.03, 0.0],
])

print('values.shape', values.shape)
print('weights.shape', weights.shape)

out = tf.math.bincount(values, weights=weights)
print()
print(out)
print()

# The output describes the amount of weight associated
# with each value, per column of `weights`.
expected = [
    [0.0, 0.0],  # weight associated with 0
    [0.11, 0.55],  # with 1
    [0.22, 0.11],  # etc
    [0.33, 0.03],
]
np.testing.assert_allclose(out, expected)
```

### Relevant log output

```shell

```",gmjw,2025-01-16 15:32:01+00:00,['Venkat6871'],2025-01-16 15:59:46+00:00,2025-01-16 15:59:43+00:00,https://github.com/tensorflow/tensorflow/issues/85065,"[('type:support', 'Support issues')]","[{'comment_id': 2596106717, 'issue_id': 2793094568, 'author': 'gmjw', 'body': ""Apologies, I've realised that things work properly if the inputs to bincount are transposed. So, the example above works fine if the `Alternative:` approach is taken, repeating the values, then everything is transposed. Marking this issue as closed."", 'created_at': datetime.datetime(2025, 1, 16, 15, 59, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596106779, 'issue_id': 2793094568, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85065"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85065"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 15, 59, 44, tzinfo=datetime.timezone.utc)}]","gmjw (Issue Creator) on (2025-01-16 15:59:43 UTC): Apologies, I've realised that things work properly if the inputs to bincount are transposed. So, the example above works fine if the `Alternative:` approach is taken, repeating the values, then everything is transposed. Marking this issue as closed.

google-ml-butler[bot] on (2025-01-16 15:59:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85065"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85065"">No</a>

"
2793076212,issue,closed,duplicate,Tensorflow related issue,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Collecting tensorflowNote: you may need to restart the kernel to use updated packages.

  Using cached tensorflow-2.18.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)
Requirement already satisfied: tensorflow-intel==2.18.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow) (2.18.0)
Requirement already satisfied: absl-py>=1.0.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.0)
Requirement already satisfied: google-pasta>=0.1.1 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)
Requirement already satisfied: packaging in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)
Requirement already satisfied: requests<3,>=2.21.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)
Requirement already satisfied: setuptools in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (57.4.0)
Requirement already satisfied: six>=1.12.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)
Requirement already satisfied: termcolor>=1.1.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)
Requirement already satisfied: typing-extensions>=3.6.6 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.69.0)
Requirement already satisfied: tensorboard<2.19,>=2.18 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)
Requirement already satisfied: keras>=3.5.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)
Requirement already satisfied: numpy<2.1.0,>=1.26.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)
Requirement already satisfied: h5py>=3.11.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)
...
Requirement already satisfied: mdurl~=0.1 in [c:\users\avs](file:///C:/users/avs) mani\desktop\project\venv\lib\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)
Using cached tensorflow-2.18.0-cp310-cp310-win_amd64.whl (7.5 kB)
Installing collected packages: tensorflow
Successfully installed tensorflow-2.18.0
Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?53673fa6-b77f-4c17-a123-a336b3acc872) or open in a [text editor](command:workbench.action.openLargeOutput?53673fa6-b77f-4c17-a123-a336b3acc872). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...
WARNING: Error parsing dependencies of tensorflow-gpu: [Errno 2] No such file or directory: 'c:\\users\\avs mani\\desktop\\project\\venv\\lib\\site-packages\\tensorflow_gpu-2.10.1.dist-info\\METADATA'
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File c:\Users\AVS MANI\Desktop\Project\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     [69](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:69) try:
---> [70](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:70)   from tensorflow.python._pywrap_tensorflow_internal import *
     [71](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:71) # This try catch logic is because there is no bazel equivalent for py_extension.
     [72](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:72) # Externally in opensource we must enable exceptions to load the shared object
     [73](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:73) # by exposing the PyInit symbols with pybind. This error will only be
     [74](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:74) # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     [75](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:75) 
     [76](file:///C:/Users/AVS%20MANI/Desktop/Project/venv/lib/site-packages/tensorflow/python/pywrap_tensorflow.py:76) # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The operation completed successfully.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], [line 5](vscode-notebook-cell:?execution_count=1&line=5)
      [1](vscode-notebook-cell:?execution_count=1&line=1) # Install tensorflow package
      [3](vscode-notebook-cell:?execution_count=1&line=3) get_ipython().run_line_magic('pip', 'install tensorflow')
----> [5](vscode-notebook-cell:?execution_count=1&line=5) import tensorflow as tf  # type: ignore
      [7](vscode-notebook-cell:?execution_count=1&line=7) from tensorflow.keras.models import Sequential # type: ignore
      [9](vscode-notebook-cell:?execution_count=1&line=9) from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout # type: ignore

File c:\Users\AVS MANI\Desktop\Project\venv\lib\site-packages\tensorflow\__init__.py:40
...


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?bc43112b-77d6-46e2-a5eb-b7a794e67457) or open in a [text editor](command:workbench.action.openLargeOutput?bc43112b-77d6-46e2-a5eb-b7a794e67457). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...

### Standalone code to reproduce the issue

```shell
import tensorflow as tf  # type: ignore

from tensorflow.keras.models import Sequential # type: ignore

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout # type: ignore

from tensorflow.keras.datasets import mnist # type: ignore
```

### Relevant log output

```shell

```",SankuriJeyaSanjana,2025-01-16 15:24:33+00:00,['tilakrayal'],2025-01-16 19:27:03+00:00,2025-01-16 19:27:00+00:00,https://github.com/tensorflow/tensorflow/issues/85064,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2596656291, 'issue_id': 2793076212, 'author': 'mihaimaruseac', 'body': 'Please use \\`\\`\\` to quote error messages to make them more readable.\n\nAlso, please search previous issues, this issue has been discussed multiple times.', 'created_at': datetime.datetime(2025, 1, 16, 19, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596656366, 'issue_id': 2793076212, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85064"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85064"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 19, 27, 2, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-16 19:27:00 UTC): Please use \`\`\` to quote error messages to make them more readable.

Also, please search previous issues, this issue has been discussed multiple times.

google-ml-butler[bot] on (2025-01-16 19:27:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85064"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85064"">No</a>

"
2792311648,issue,closed,completed,Issues on trying to compile TensorFlow C API for JETSON AGX Xavier using Bazel,"On my JETSON AGX Xavier, with:

cuda: 11.4.315
cuDNN: 8.6.0
tensorrt: 8.5.2.2
jetpack: 5.1.3
python3 -c import tensorflow as tf; print(TensorFlow version:, tf.version)
TensorFlow version: 2.11.0

I cant compile tf with bazel ( bazel --version: bazel 5.3.0 ) , error:

~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow:libtensorflow.so

Starting local Bazel server and connecting to it
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading startup options from /home/redans/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
Inherited common options: --isatty=1 --terminal_columns=237
INFO: Reading rc options for build from /home/redans/tensorflow/.bazelrc:
Inherited common options: --experimental_repo_remote_exec
INFO: Reading rc options for build from /home/redans/tensorflow/.bazelrc:
build options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for build from /home/redans/tensorflow/.tf_configure.bazelrc:
build options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.9 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.9/dist-packages --python_path=/usr/bin/python3.9 --action_env PYTHONPATH=/usr/local/lib/python3.9/dist-packages:/usr/local/lib/python3.9/dist-packages:/home/redans/ros2_ws/install/yolov8_ros/lib/python3.9/site-packages:/home/redans/ros2_ws/install/yolov8_msgs/lib/python3.9/site-packages:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib/python3.9/site-packages:/opt/ros/humble/lib/python3.9/site-packages --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:/home/redans/local/lib/python3.8/dist-packages/tensorflow:/home/redans/ros2_ws/install/yolov8_msgs/lib:/home/redans/ros2_ws/install/realsense2_camera/lib:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/aarch64-linux-gnu:/opt/ros/humble/lib:/usr/local/cuda-11.4/lib64: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/aarch64-linux-gnu-gcc-9 --config=cuda
INFO: Found applicable config definition build:short_logs in file /home/redans/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/redans/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2
INFO: Found applicable config definition build:opt in file /home/redans/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2
INFO: Found applicable config definition build:linux in file /home/redans/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/redans/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
ERROR: Traceback (most recent call last):
File /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl, line 734, column 32, in
PLATFORMS = _generate_platforms()
File /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl, line 723, column 15, in _generate_platforms
} | v.flag_values,
Error: unsupported binary operation: dict | dict
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: error loading package : at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/local_tsl/third_party/py/python_init_repositories.bzl:3:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/repositories.bzl:24:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/private/python_register_multi_toolchains.bzl:22:6: initialization of module python/versions.bzl failed

Do you have any suggestions?",user-redans,2025-01-16 10:26:30+00:00,['tilakrayal'],2025-02-01 16:57:17+00:00,2025-02-01 16:57:14+00:00,https://github.com/tensorflow/tensorflow/issues/85034,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:bazel', 'Bazel related Build_Installation issues'), ('TF 2.11', 'Issues related to TF 2.11')]","[{'comment_id': 2603752621, 'issue_id': 2792311648, 'author': 'tilakrayal', 'body': '@user-redans,\nTensorflow v2.11 is a pretty older version which is not actively supported. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.11 which the respective configurations.\n\nhttps://www.tensorflow.org/install/source\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 21, 6, 17, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469151, 'issue_id': 2792311648, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629026733, 'issue_id': 2792311648, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85034"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85034"">No</a>', 'created_at': datetime.datetime(2025, 2, 1, 16, 57, 16, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-21 06:17:16 UTC): @user-redans,
Tensorflow v2.11 is a pretty older version which is not actively supported. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.11 which the respective configurations.

https://www.tensorflow.org/install/source

Thank you!

github-actions[bot] on (2025-01-29 01:59:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

google-ml-butler[bot] on (2025-02-01 16:57:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85034"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85034"">No</a>

"
2791925497,issue,open,,tf.config.LogicalDeviceConfiguration() not able to set the memory limit but tf.config.experimental.VirtualDeviceConfiguration() is able to,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

RTX A5000 24Gb

### Current behavior?

I was trying to set the memory limit of 10Gb on the virtual device using tf.config.LogicalDeviceConfiguration(), but when I trained the model it was taking way more than 10Gb of memory. Eventually I was able to set the memory limit using tf.config.experimental.VirtualDeviceConfiguration() but I'm not sure why

### Standalone code to reproduce the issue

```shell
# this was not able to set the memory limit
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.set_visible_devices(gpus[0], 'GPU')
        tf.config.set_logical_device_configuration(gpus[0],
        [tf.config.LogicalDeviceConfiguration(memory_limit=10*1024)])
        logical_gpus = tf.config.list_logical_devices('GPU')
    except RuntimeError as e:
        print(e) 

# this was able to set the memory limit
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_virtual_device_configuration(
            gpus[0],
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)]
        )
    except RuntimeError as e:
        print(e)
```

### Relevant log output

```shell

```",rogerci91,2025-01-16 07:37:49+00:00,['Venkat6871'],2025-02-05 02:00:48+00:00,,https://github.com/tensorflow/tensorflow/issues/85026,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2604697364, 'issue_id': 2791925497, 'author': 'arzoo0511', 'body': ""TensorFlow's tf.config.experimental.set_virtual_device_configuration is more focused on managing device resources at a lower level, such as limiting memory and managing the allocation between multiple virtual devices. This is why it worked for you while the tf.config.LogicalDeviceConfiguration did not.If you need to limit memory usage, you should use tf.config.experimental.set_virtual_device_configuration() to ensure the desired memory cap is respected."", 'created_at': datetime.datetime(2025, 1, 21, 13, 11, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618021962, 'issue_id': 2791925497, 'author': 'Venkat6871', 'body': 'Hi **@rogerci91** ,\nHi **@arzoo0511** Thank you for your pointers.\nApologies for the delay, and thank you for raising your concern here. As @arzoo0511 mentioned, `tf.config.experimental.set_virtual_device_configuration()` works by managing resource allocation at a lower level, including limiting memory usage and handling allocation across multiple virtual devices. Therefore, `tf.config.LogicalDeviceConfiguration()` will not achieve the desired results in this scenario.\nIt is recommended to use `tf.config.experimental.set_virtual_device_configuration()` for better outcomes. Here is the relevant TensorFlow [documentation](https://www.tensorflow.org/guide/gpu) for your reference.\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 28, 6, 24, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635534470, 'issue_id': 2791925497, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 5, 2, 0, 48, tzinfo=datetime.timezone.utc)}]","arzoo0511 on (2025-01-21 13:11:22 UTC): TensorFlow's tf.config.experimental.set_virtual_device_configuration is more focused on managing device resources at a lower level, such as limiting memory and managing the allocation between multiple virtual devices. This is why it worked for you while the tf.config.LogicalDeviceConfiguration did not.If you need to limit memory usage, you should use tf.config.experimental.set_virtual_device_configuration() to ensure the desired memory cap is respected.

Venkat6871 (Assginee) on (2025-01-28 06:24:42 UTC): Hi **@rogerci91** ,
Hi **@arzoo0511** Thank you for your pointers.
Apologies for the delay, and thank you for raising your concern here. As @arzoo0511 mentioned, `tf.config.experimental.set_virtual_device_configuration()` works by managing resource allocation at a lower level, including limiting memory usage and handling allocation across multiple virtual devices. Therefore, `tf.config.LogicalDeviceConfiguration()` will not achieve the desired results in this scenario.
It is recommended to use `tf.config.experimental.set_virtual_device_configuration()` for better outcomes. Here is the relevant TensorFlow [documentation](https://www.tensorflow.org/guide/gpu) for your reference.

Thank you!

github-actions[bot] on (2025-02-05 02:00:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2791389110,issue,closed,duplicate,4080 RTX not detected on windows 11 24H2,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Windows 11 24h2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda compilation tools, release 12.6, V12.6.85; cuDNN: Driver Version: 561.17 

### GPU model and memory

4080 RTX

### Current behavior?

physical_devices = tf.config.list_physical_devices() to return GPU information.  Instead, all I got was this:

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]



### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
physical_devices = tf.config.list_physical_devices() 
print(physical_devices)
```

### Relevant log output

```shell

```",ThomasHughesIV,2025-01-16 00:58:02+00:00,['Venkat6871'],2025-01-16 23:17:07+00:00,2025-01-16 19:42:26+00:00,https://github.com/tensorflow/tensorflow/issues/85006,"[('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2594243331, 'issue_id': 2791389110, 'author': 'ThomasHughesIV', 'body': ""I've tried different python versions, down to 3.10.  I cannot get tensor to detect my GPU so I can play around with building models and learn how to do this in vs2022"", 'created_at': datetime.datetime(2025, 1, 16, 0, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596731394, 'issue_id': 2791389110, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85006"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85006"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 19, 42, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597042188, 'issue_id': 2791389110, 'author': 'ThomasHughesIV', 'body': ""Resolving as duplicate and pointing to a ticket that has no resolution isn't a very good resolution.   At the very least I should get a ETA or some effort to explain this getting fixed."", 'created_at': datetime.datetime(2025, 1, 16, 22, 32, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597100686, 'issue_id': 2791389110, 'author': 'mihaimaruseac', 'body': 'That is discussed in the duplicated ticket.', 'created_at': datetime.datetime(2025, 1, 16, 23, 17, 6, tzinfo=datetime.timezone.utc)}]","ThomasHughesIV (Issue Creator) on (2025-01-16 00:59:27 UTC): I've tried different python versions, down to 3.10.  I cannot get tensor to detect my GPU so I can play around with building models and learn how to do this in vs2022

google-ml-butler[bot] on (2025-01-16 19:42:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85006"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/85006"">No</a>

ThomasHughesIV (Issue Creator) on (2025-01-16 22:32:53 UTC): Resolving as duplicate and pointing to a ticket that has no resolution isn't a very good resolution.   At the very least I should get a ETA or some effort to explain this getting fixed.

mihaimaruseac on (2025-01-16 23:17:06 UTC): That is discussed in the duplicated ticket.

"
2790444887,issue,open,,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.19

### Custom code

No

### OS platform and distribution

Linux Debian 6.1.119-1 (2024-11-22) x86_64 x86_64 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

bazel 6.5.0

### GCC/compiler version

gcc version 13.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

### Background

I follow the guidance here to run unit test locally. I use a google cloud compute engine.
[gcc version 13.1.0](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#running-unit-tests)

I run the follow command to docker image tensorflow/build:2.19-python3.10.
docker run -it -v $PWD:/tmp -w /tmp tensorflow/build:2.19-python3.10 bash -c ""bazel build --experimental_action_cache_store_output_metadata --disk_cache=~/.cache/bazel --jobs=3 --config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""

### The build failed and the error message is:
/usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
1070 | { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
|

### The full debug message is:
ERROR: /tmp/tensorflow/compiler/mlir/lite/BUILD:1295:11: Compiling tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 235 arguments skipped)
In file included from /usr/include/c++/13/memory:78,
from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16:
/usr/include/c++/13/bits/unique_ptr.h: In instantiation of 'std::__detail::__unique_ptr_t<_Tp> std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; __detail::__unique_ptr_t<_Tp> = __detail::__unique_ptr_tmlir::TFL::{anonymous}::DefaultQuantParamsPass]':
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50: required from here
/usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
1070 | { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
| ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:52:
bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/transforms/passes.h.inc:162:3: note: candidate: 'mlir::TFL::{anonymous}::impl::DefaultQuantParamsPassBase::DefaultQuantParamsPassBase(mlir::TFL::DefaultQuantParamsPassOptions) [with DerivedT = mlir::TFL::{anonymous}::DefaultQuantParamsPass]'
162 | DefaultQuantParamsPassBase(DefaultQuantParamsPassOptions options) : DefaultQuantParamsPassBase() {
| ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:57:37: note: inherited here
57 | using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase;
| ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:66:12: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)'
66 | explicit DefaultQuantParamsPass(
| ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::{anonymous}::DefaultQuantParamsPass&)'
54 | class DefaultQuantParamsPass
| ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted)
Target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 407.917s, Critical Path: 206.23s
INFO: 43 processes: 7 internal, 36 local.
FAILED: Build did NOT complete successfully

### Standalone code to reproduce the issue

```shell
I run the follow command to docker image tensorflow/build:2.19-python3.10.
docker run -it -v $PWD:/tmp -w /tmp tensorflow/build:2.19-python3.10 bash -c ""bazel build --experimental_action_cache_store_output_metadata --disk_cache=~/.cache/bazel --jobs=3 --config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""

### The build failed and the error message is:
/usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
1070 | { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
|
```

### Relevant log output

```shell

```",codinglover222,2025-01-15 17:34:09+00:00,['gaikwadrahul8'],2025-02-06 20:38:33+00:00,,https://github.com/tensorflow/tensorflow/issues/84977,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2593554590, 'issue_id': 2790444887, 'author': 'codinglover222', 'body': '@Venkat6871', 'created_at': datetime.datetime(2025, 1, 15, 17, 35, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601050605, 'issue_id': 2790444887, 'author': 'codinglover222', 'body': ""I also tested on new docker SIG image the the same error occurs.\n\ntensorflow/build:2.19-python3.12\n\n/usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous\n  962 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }"", 'created_at': datetime.datetime(2025, 1, 19, 22, 40, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609273462, 'issue_id': 2790444887, 'author': 'npanpaliya', 'body': 'We are also seeing the same error with gcc 12, python 3.11 and tensorflow master branch. Could someone please provide any pointers on this issue?', 'created_at': datetime.datetime(2025, 1, 23, 9, 17, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611121368, 'issue_id': 2790444887, 'author': 'gaikwadrahul8', 'body': ""Hi, @npanpaliya \nI apologize for the delayed response, thank you for bringing this issue to our attention I'll try to replicate the same behavior from my end and will update you. \n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 23, 22, 11, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631173011, 'issue_id': 2790444887, 'author': 'gaikwadrahul8', 'body': 'Hi, @codinglover222 \nI apologize for the delayed response, I\'ve been attempting to reproduce the behavior you described using the provided command within the issue template.  Unfortunately, I\'ve encountered a different problem the build process is freezing on my end and preventing completion.  This is occurring on a GCP VM instance.  I\'ve included the output log below for your review. \n\nTo confirm, Did you face similar issue from your end ? If possible could you please help us with exact steps which you followed before encountering the reported error in the issue template which will help us to investigate this issue further from our end ?\n\n\n```\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tensorflow$ sudo docker run -it -v $PWD:/tmp -w /tmp tensorflow/build:2.19-python3.10 bash -c ""bazel build --experimental_action_cache_store_output_metadata --disk_cache=~/.cache/bazel --jobs=3 --config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""\n2025/02/03 13:17:27 Downloading https://releases.bazel.build/6.5.0/release/bazel-6.5.0-linux-x86_64...\nExtracting Bazel installation...\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nINFO: Invocation ID: c5c3346d-1cbd-4a23-aa60-2aa5d8cd318a\nINFO: Reading \'startup\' options from /tmp/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited \'common\' options: --isatty=1 --terminal_columns=190\nINFO: Reading rc options for \'build\' from /tmp/.bazelrc:\n  Inherited \'common\' options: --experimental_repo_remote_exec\nINFO: Reading rc options for \'build\' from /etc/bazel.bazelrc:\n  \'build\' options: --action_env=DOCKER_CACHEBUSTER=1738455273945121626 --host_action_env=DOCKER_HOST_CACHEBUSTER=1738455274013979537\nINFO: Reading rc options for \'build\' from /tmp/.bazelrc:\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Found applicable config definition build:short_logs in file /tmp/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /tmp/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:linux in file /tmp/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Found applicable config definition build:linux in file /tmp/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nDEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:154:14: \nHERMETIC_PYTHON_VERSION variable was not set correctly, using default version.\nPython 3.10 will be used.\nTo select Python version, either set HERMETIC_PYTHON_VERSION env variable in\nyour shell:\n  export HERMETIC_PYTHON_VERSION=3.12\nOR pass it as an argument to bazel command directly or inside your .bazelrc\nfile:\n  --repo_env=HERMETIC_PYTHON_VERSION=3.12\nDEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:87:10: \n=============================\nHermetic Python configuration:\nVersion: ""3.10""\nKind: """"\nInterpreter: ""default"" (provided by rules_python)\nRequirements_lock label: ""@python_version_repo//:requirements_lock_3_10.txt""\n=====================================\nWARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nINFO: Analyzed target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize (185 packages loaded, 8165 targets configured).\nINFO: Found 1 target...\n[2,289 / 2,432] 6 actions, 4 running\n    Compiling mlir/lib/Analysis/FlatLinearValueConstraints.cpp; 64s remote-cache\n    Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc; 40s local, remote-cache\n    Compiling mlir/lib/Dialect/Affine/IR/AffineValueMap.cpp; 40s remote-cache\n```\n\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 2, 3, 14, 33, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631722509, 'issue_id': 2790444887, 'author': 'npanpaliya', 'body': ""Hi @gaikwadrahul8  - Did you post the entire error log? I don't see any error in above log you shared."", 'created_at': datetime.datetime(2025, 2, 3, 18, 12, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640949861, 'issue_id': 2790444887, 'author': 'gaikwadrahul8', 'body': 'Hi, @npanpaliya \nI have provided the complete build log above. However, the build process stalled at step [2,289 / 2,432] and did not complete.  After waiting for over 30 minutes, there was no further progress.  Could you please share the precise steps you took prior to encountering this issue?  Your assistance in replicating the same issue from our end would be greatly appreciated. Thank you.', 'created_at': datetime.datetime(2025, 2, 6, 20, 38, 25, tzinfo=datetime.timezone.utc)}]","codinglover222 (Issue Creator) on (2025-01-15 17:35:46 UTC): @Venkat6871

codinglover222 (Issue Creator) on (2025-01-19 22:40:43 UTC): I also tested on new docker SIG image the the same error occurs.

tensorflow/build:2.19-python3.12

/usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
  962 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }

npanpaliya on (2025-01-23 09:17:57 UTC): We are also seeing the same error with gcc 12, python 3.11 and tensorflow master branch. Could someone please provide any pointers on this issue?

gaikwadrahul8 (Assginee) on (2025-01-23 22:11:09 UTC): Hi, @npanpaliya 
I apologize for the delayed response, thank you for bringing this issue to our attention I'll try to replicate the same behavior from my end and will update you. 

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-02-03 14:33:13 UTC): Hi, @codinglover222 
I apologize for the delayed response, I've been attempting to reproduce the behavior you described using the provided command within the issue template.  Unfortunately, I've encountered a different problem the build process is freezing on my end and preventing completion.  This is occurring on a GCP VM instance.  I've included the output log below for your review. 

To confirm, Did you face similar issue from your end ? If possible could you please help us with exact steps which you followed before encountering the reported error in the issue template which will help us to investigate this issue further from our end ?


```
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tensorflow$ sudo docker run -it -v $PWD:/tmp -w /tmp tensorflow/build:2.19-python3.10 bash -c ""bazel build --experimental_action_cache_store_output_metadata --disk_cache=~/.cache/bazel --jobs=3 --config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""
2025/02/03 13:17:27 Downloading https://releases.bazel.build/6.5.0/release/bazel-6.5.0-linux-x86_64...
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: c5c3346d-1cbd-4a23-aa60-2aa5d8cd318a
INFO: Reading 'startup' options from /tmp/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from /tmp/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1738455273945121626 --host_action_env=DOCKER_HOST_CACHEBUSTER=1738455274013979537
INFO: Reading rc options for 'build' from /tmp/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /tmp/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmp/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:154:14: 
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.10 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.10""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_10.txt""
=====================================
WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Analyzed target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize (185 packages loaded, 8165 targets configured).
INFO: Found 1 target...
[2,289 / 2,432] 6 actions, 4 running
    Compiling mlir/lib/Analysis/FlatLinearValueConstraints.cpp; 64s remote-cache
    Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc; 40s local, remote-cache
    Compiling mlir/lib/Dialect/Affine/IR/AffineValueMap.cpp; 40s remote-cache
```

Thank you for your cooperation and patience.

npanpaliya on (2025-02-03 18:12:47 UTC): Hi @gaikwadrahul8  - Did you post the entire error log? I don't see any error in above log you shared.

gaikwadrahul8 (Assginee) on (2025-02-06 20:38:25 UTC): Hi, @npanpaliya 
I have provided the complete build log above. However, the build process stalled at step [2,289 / 2,432] and did not complete.  After waiting for over 30 minutes, there was no further progress.  Could you please share the precise steps you took prior to encountering this issue?  Your assistance in replicating the same issue from our end would be greatly appreciated. Thank you.

"
2790433050,issue,closed,completed,Failed to load native TensorFlow Lite methods,"Hi,
I'm trying to use tensorflow lite version 2.15.0 to run my tflite model and I'm getting an error when initializing the interpreter.

I'm adding the tensor flow libraries in the gradle file 

`implementation('org.tensorflow:tensorflow-lite') { version { strictly(""2.15.0"") } }`

Code: 

```
isLibraryLoaded = false

private fun initInterpreter(): Interpreter? {
        val tfliteOptions = Interpreter.Options()
        tfliteOptions.setNumThreads(2)
        
        if (!isLibraryLoaded) {
            System.loadLibrary(""tensorflowlite_jni"")
            ARLog.d(""OmniSenseMLDepthImageProcessor"",""Tensor flow lite Library load successful"")
            isLibraryLoaded = true
        }
        return try {
            Log.d(""InterpreterDelegate"", ""Thread id getInterpreter ==  ${Thread.currentThread().name}"")
            val interpreter = org.tensorflow.lite.Interpreter(loadModelFile(resolveModelFilePath()), tfliteOptions)
            Log.d(""InterpreterDelegate"", ""Interpreter initialized successfully"")
            return interpreter
        } catch (e: Exception) {
            Log.e(""InterpreterDelegate"", ""Error: Could not initialize $tfLiteModel interpreter!: ${e.message}"")
            e.printStackTrace()
            null
        }
    }
```


TFLite version: 2.15.0
Device: Samsung S20

Error: 

```
 E  FATAL EXCEPTION: pool-140-thread-1
                                                                                                    Process: com.amazon.mShop.android.shopping, PID: 14755
                                                                                                    java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
                                                                                                      java.lang.UnsatisfiedLinkError: dlopen failed: library ""libtensorflowlite_jni_gms_client.so"" not found
                                                                                                    	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
                                                                                                    	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
                                                                                                    	at com.a9.fez.tflite.TFLiteInterpreterDelegate.initInterpreter(TFLiteInterpreterDelegate.kt:50)
                                                                                                    	at com.a9.fez.tflite.TFLiteInterpreterDelegate.getValue(TFLiteInterpreterDelegate.kt:29)
 Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__) - is the library loaded, e.g. System.loadLibrary?
                                                                                                    	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)
                                                                                                    	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)
                                                                                                    	... 14 more
```
",chandu464,2025-01-15 17:27:48+00:00,['gaikwadrahul8'],2025-02-07 02:01:32+00:00,2025-02-07 02:01:29+00:00,https://github.com/tensorflow/tensorflow/issues/84976,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2611073458, 'issue_id': 2790433050, 'author': 'gaikwadrahul8', 'body': 'Hi, @chandu464 \nI apologize for the delayed response, if possible could you please help us with your Github repo along with TFLite model and complete steps to replicate the same behavior from our end to investigate this issue further from our end ?\n\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 1, 23, 21, 40, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127379, 'issue_id': 2790433050, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750279, 'issue_id': 2790433050, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750367, 'issue_id': 2790433050, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84976"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84976"">No</a>', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 31, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-23 21:40:35 UTC): Hi, @chandu464 
I apologize for the delayed response, if possible could you please help us with your Github repo along with TFLite model and complete steps to replicate the same behavior from our end to investigate this issue further from our end ?

Thank you for your cooperation and patience.

github-actions[bot] on (2025-01-31 01:59:50 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-07 02:01:28 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-07 02:01:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84976"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84976"">No</a>

"
2790109834,issue,open,,"How can I compile TensorFlowLite for Swift without Bitcode?""","Hello!

I would like to use TensorFlowLite Swift without Bitcode, as Apple has discontinued the use of Bitcode. I am using version 2.17.0 available on CocoaPods, but the binaries already come with Bitcode. How can I resolve this?

And does TensorFlowLite Swift have a version available that does not require Rosetta to run on ARM architectures?
Thank you",arianehlima,2025-01-15 15:09:18+00:00,['gaikwadrahul8'],2025-02-07 02:01:31+00:00,,https://github.com/tensorflow/tensorflow/issues/84969,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('iOS', '')]","[{'comment_id': 2611021580, 'issue_id': 2790109834, 'author': 'gaikwadrahul8', 'body': ""Hi, @arianehlima \nI apologize for the delay in response, I believe you're following this official documentation [TensorFlow Lite for Swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift) and as far I know you can build the `TensorFlowLite` Swift library target using below command by disabling the bitcode \n\n\n\n```\nbazel build tensorflow/lite/swift:TensorFlowLite \\\n    --define=ios_arm64=true \\\n    --define=enable_bitcode=false\n```\n\nYou can also do manually enable/disable bitcode support go to **Project > Build Settings > search for 'bitcode' in the searchfield > set to YES/NO.**\n\nMay I know which Xcode version you're using ? Please make sure you're using the latest version of TensorFlow Lite from CocoaPods or by building from source also ensure your Xcode project is correctly configured for ARM architecture in your project's build settings.\n\nIf I have missed something here please let me know.\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 23, 21, 8, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621465645, 'issue_id': 2790109834, 'author': 'arianehlima', 'body': 'Hi @gaikwadrahul8 ! Thank for your response!\n\nI am using Xcode 15.4 and TensorflowLiteSwift 2.14.0 from CocoaPods.', 'created_at': datetime.datetime(2025, 1, 29, 12, 8, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2624269898, 'issue_id': 2790109834, 'author': 'gaikwadrahul8', 'body': 'Hi, @arianehlima \nThank you for the details, if possible could you please give it try with `TensorflowLiteSwift 2.17.0` and let us know is it working as expected or not ?\n\nIf issue still persists please let us know error log for investigate this issue further from our end. Thank you for your cooperation and understanding.', 'created_at': datetime.datetime(2025, 1, 30, 11, 46, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750314, 'issue_id': 2790109834, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 29, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-23 21:08:44 UTC): Hi, @arianehlima 
I apologize for the delay in response, I believe you're following this official documentation [TensorFlow Lite for Swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift) and as far I know you can build the `TensorFlowLite` Swift library target using below command by disabling the bitcode 



```
bazel build tensorflow/lite/swift:TensorFlowLite \
    --define=ios_arm64=true \
    --define=enable_bitcode=false
```

You can also do manually enable/disable bitcode support go to **Project > Build Settings > search for 'bitcode' in the searchfield > set to YES/NO.**

May I know which Xcode version you're using ? Please make sure you're using the latest version of TensorFlow Lite from CocoaPods or by building from source also ensure your Xcode project is correctly configured for ARM architecture in your project's build settings.

If I have missed something here please let me know.

Thank you for your cooperation and patience.

arianehlima (Issue Creator) on (2025-01-29 12:08:09 UTC): Hi @gaikwadrahul8 ! Thank for your response!

I am using Xcode 15.4 and TensorflowLiteSwift 2.14.0 from CocoaPods.

gaikwadrahul8 (Assginee) on (2025-01-30 11:46:23 UTC): Hi, @arianehlima 
Thank you for the details, if possible could you please give it try with `TensorflowLiteSwift 2.17.0` and let us know is it working as expected or not ?

If issue still persists please let us know error log for investigate this issue further from our end. Thank you for your cooperation and understanding.

github-actions[bot] on (2025-02-07 02:01:29 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2790025830,issue,closed,completed,TensorRT ( C++ ) inference strange behavior on Jetson AGX Xavier,"I developed 2 distinct models, for 2 use cases, to analyzed some vibration patterns: one of them when system is turn on and second when system is shut down (so there are no any vibration detected )
The entire training process uses TensorFlow 2.7.0 (an auto encoder in python) to create .h5 models, which are converted to .onnx models files and then to .engine files for the Jetson platform (Jetson AGX Xavier CUDA ).

Jetson AGX Xavier specs:
cuda: 11.4.315
cuDNN: 8.6.0
tensorRT: 8.5.2.2
jetpack: 5.1.3
python3 -c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__)""
TensorFlow version: 2.11.0


Auto encoder trainig script in python ( sample) :

```
input_img = tf.keras.layers.Input(shape=(2000, lines))

  # Encoder
  x = tf.keras.layers.Conv1D(12, 128, padding='same')(input_img)
  x = tf.keras.layers.MaxPooling1D(4)(x)  # Downsample: 2000 -> 500

  x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)
  x = tf.keras.layers.MaxPooling1D(2)(x)  # Downsample: 500 -> 250

  x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)
  x = tf.keras.layers.MaxPooling1D(2)(x)  # Downsample: 250 -> 125

  # Bottleneck
  x = tf.keras.layers.Flatten()(x)
  x = tf.keras.layers.Dense(self.__config['MODEL']['ENCODED_STATE_SIZE'])(x)

  # Decoder
  x = tf.keras.layers.Dense(125 * 12)(x)  # Expand to match last encoder feature size
  x = tf.keras.layers.Reshape((125, 12))(x)

  x = tf.keras.layers.UpSampling1D(2)(x)  # Upsample: 125 -> 250
  x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)

  x = tf.keras.layers.UpSampling1D(2)(x)  # Upsample: 250 -> 500
  x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)

  x = tf.keras.layers.UpSampling1D(4)(x)  # Upsample: 500 -> 2000
  x = tf.keras.layers.Conv1D(lines, 128, padding='same')(x)  # Correct Final Layer

  # Model definition
  self.__model = tf.keras.models.Model(input_img, x)
```

It doesn't matter which model I use, inference result values are the SAME, exactly the same values, as if the neural network learned nothing......
You can see below 2 comparative charts with the inference values

![Image](https://github.com/user-attachments/assets/e91356e3-0e4c-44c0-83e7-b336d601f5e2)

 
Don't assume that the data might be corrupted, I have collected enough data to train for both cases and I've checked their validity

The confusing part is that inference works in python, using TensorFlow 2.7.0 with GPU, an Ubuntu Focal x86_64...I mean, I saw different values between 2 charts

In Jetson I've made a py script to convert .h5 model file into .onnx and then into .engine format:

```
import tf2onnx
import tensorflow as tf
import argparse
import subprocess


def convert_h5_to_onnx(h5_model_path, onnx_model_path):
    print(""Converting .h5 model to ONNX..."")

    model = tf.keras.models.load_model(h5_model_path)

    model_proto, _ = tf2onnx.convert.from_keras(model, opset=13)
    
    with open(onnx_model_path, ""wb"") as f:
        f.write(model_proto.SerializeToString())
    
    print(f""ONNX model saved at {onnx_model_path}"")

def convert_onnx_to_trt(onnx_model_path, engine_model_path, trt_precision_mode):
    print(""Converting ONNX model to TensorRT Engine..."")

    fp_precision_flag = '--fp16' if trt_precision_mode.upper() == 'FP16' else ''
    
    trtexec_path = ""/usr/src/tensorrt/bin/trtexec""

    command = f""{trtexec_path} --onnx={onnx_model_path} --saveEngine={engine_model_path} {fp_precision_flag}""
    
    process = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    if process.returncode != 0:
        print(f""Error in converting to TensorRT engine:\n{process.stderr.decode('utf-8')}"")
    else:
        print(f""TensorRT engine saved at {engine_model_path}"")

# Main
if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Convert a .h5 model to ONNX and TensorRT engine format"")
    parser.add_argument(""--h5_model_path"", type=str, required=True, help=""Path to the .h5 model file"")
    parser.add_argument(""--onnx_model_path"", type=str, required=True, help=""Path to save the converted ONNX model"")
    parser.add_argument(""--engine_model_path"", type=str, required=True, help=""Path to save the converted TensorRT engine"")
    parser.add_argument(""--trt_precision_mode"", type=str, choices=['FP32', 'FP16'], default=""FP16"", help=""Precision mode for TensorRT engine (FP32 or FP16)"")

    args = parser.parse_args()

    convert_h5_to_onnx(args.h5_model_path, args.onnx_model_path)

    convert_onnx_to_trt(args.onnx_model_path, args.engine_model_path, args.trt_precision_mode)

```

""RunInference"" is my C/C++ inference function using TensorRT ( as input data , I used FFT s  of the raw values )

```
void RunInference(ICudaEngine* engine, IExecutionContext* context, int input_index, int output_index, kiss_fft_cpx* x_fft, kiss_fft_cpx* y_fft, kiss_fft_cpx* z_fft, float* predicted_output, int g_code, const char* clientName) {
	
    int batchSize = 1;
    int input_size = batchSize * 2000 * 3 * sizeof(float);  // [1, 2000, 3]
    int output_size = batchSize * 3 * sizeof(float);        // [1, 3]

    // Prepare normalized input data and set DC component to zero
    float input_data[2000 * 3];
    const int MN = 4000;
	
    for (int i = 0; i < 2000; i++) {
        input_data[i * 3 + 0] = sqrt(x_fft[i].r * x_fft[i].r + x_fft[i].i * x_fft[i].i) / MN; 
        input_data[i * 3 + 1] = sqrt(y_fft[i].r * y_fft[i].r + y_fft[i].i * y_fft[i].i) / MN; 
        input_data[i * 3 + 2] = sqrt(z_fft[i].r * z_fft[i].r + z_fft[i].i * z_fft[i].i) / MN; 
    }
	
    // Set DC component to zero
    input_data[0] = 0;  // X-axis
    input_data[1] = 0;  // Y-axis
    input_data[2] = 0;  // Z-axis

    ////Allocate GPU buffers for input and output
    void* buffers[2];
	
	write_log(LOG_DEBUG, ""RunInference for '%s' - input_index = %d, output_index = %d"", clientName, input_index, output_index);
	
	if (cudaMalloc(&buffers[input_index], input_size) != cudaSuccess) {
		write_log(LOG_ERROR, ""RunInference for '%s' - Failed to allocate memory for input buffer"", clientName);
		return;
	}
	if (cudaMalloc(&buffers[output_index], output_size) != cudaSuccess) {
		write_log(LOG_ERROR, ""RunInference for '%s' - Failed to allocate memory for output buffer"", clientName);
		cudaFree(buffers[input_index]);
		return;
	}
	
	if (cudaMemset(buffers[input_index], 0, input_size) != cudaSuccess) {
		write_log(LOG_ERROR, ""RunInference for '%s' - Failed to memset input buffer to zero"", clientName);
		return;
	}
	if (cudaMemset(buffers[output_index], 0, output_size) != cudaSuccess) {
		write_log(LOG_ERROR, ""RunInference for '%s' - Failed to memset output buffer to zero"", clientName);
		return;
	}
	///////////////////
				
    // Copy the input data to the GPU
    cudaMemcpy(buffers[input_index], input_data, input_size, cudaMemcpyHostToDevice);

    // Launch inference
    cudaStream_t stream;
    cudaStreamCreate(&stream);
    context->enqueueV2(buffers, stream, nullptr);
    cudaStreamSynchronize(stream);

    // Copy the output data from GPU to CPU
    cudaMemcpy(predicted_output, buffers[output_index], output_size, cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(buffers[input_index]);
    cudaFree(buffers[output_index]);
    cudaStreamDestroy(stream);
}

```

This is how I load one model in app and how I call inference function:

```
	IRuntime* runtime = createInferRuntime(gLogger);
	if (!runtime) {
		write_log(LOG_ERROR, ""client_handler: Failed to create runtime for client %s"", client.ClientName);
		return (void*)-1;
	}
	
	std::vector<char> engine_data = loadEngine(client.ModelPath, client.ClientName);	

          ICudaEngine* engine = runtime->deserializeCudaEngine(engine_data.data(), engine_data.size(), nullptr);
          if (!engine) {
          	write_log(LOG_ERROR, ""client_handler: Failed to create engine for thread %s"", client.ClientName);

          	return (void*)-1;
          }

            IExecutionContext* context = engine->createExecutionContext();
            if (!context) {
            write_log(LOG_ERROR, ""client_handler: Failed to create execution context for thread %s"", client.ClientName);							
            engine->destroy();
            return (void*)-1;
            }
            
            int input_index = engine->getBindingIndex(client.ModelInputBindingName) ;//get from config file
            int output_index = engine->getBindingIndex(client.ModelOutputBindingName); //get from config file

      	   RunInference(engine, context, input_index, output_index, x_fft, y_fft, z_fft, predicted_output, client.G_code, client.ClientName);
      	
      	// Synchronize the GPU to ensure all operations are completed
      	cudaDeviceSynchronize();
      
      	// Check for CUDA errors after synchronization
      	cudaError_t err = cudaGetLastError();
      	if (err != cudaSuccess) {
      		write_log(LOG_ERROR, ""CUDA error after synchronization in thread '%s': %s"", client.ClientName, cudaGetErrorString(err));
      	} else {
      		write_log(LOG_INFO, ""GPU synchronized successfully for thread '%s'"", client.ClientName);
      	}
      	
      	 context->destroy();
      	 engine->destroy();
         runtime->destroy();
	
```
I want to point out that the vibrations are detected by the application, but I dont understand why the range of values doesnt change depending on the trained model from the two scenarios. I suspect the problem might be with the model conversion or the inference process / function in TensorRT using C/C++.

Do you have any suggestions?
",user-redans,2025-01-15 14:39:02+00:00,['Venkat6871'],2025-02-01 17:40:55+00:00,2025-02-01 16:59:11+00:00,https://github.com/tensorflow/tensorflow/issues/84963,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('TF 2.11', 'Issues related to TF 2.11')]","[{'comment_id': 2615147569, 'issue_id': 2790025830, 'author': 'Venkat6871', 'body': 'Hi **@user-redans** ,\nApologies for the delay, and thank you for raising your concern here. I reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so I can analyze it further? Also, it seems there might be some compatibility issues, so please check those for a smoother run. I have attached the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference.\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 27, 8, 45, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629042236, 'issue_id': 2790025830, 'author': 'user-redans', 'body': ""I solved it! In py traning model code, I 've changed FFT calculation, I've added kissFFT lib, like in CPP inference code."", 'created_at': datetime.datetime(2025, 2, 1, 17, 40, 54, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-27 08:45:43 UTC): Hi **@user-redans** ,
Apologies for the delay, and thank you for raising your concern here. I reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so I can analyze it further? Also, it seems there might be some compatibility issues, so please check those for a smoother run. I have attached the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference.

Thank you!

user-redans (Issue Creator) on (2025-02-01 17:40:54 UTC): I solved it! In py traning model code, I 've changed FFT calculation, I've added kissFFT lib, like in CPP inference code.

"
2789992508,issue,open,,Windows libtensorflow size increased 4x with 2.17,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17+

### Custom code

No

### OS platform and distribution

Windows x86_64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Libtensorflow.dll for windows was 238MB with 2.16.2, is 909MB with 2.17.0, and is 931MB with 2.18.0.

At the same time the linux versions haven't changed significantly.

I would not expect the tensorflow binaries on windows to be over twice the size of linux, nor for the size to increase so much without any notice in the release notes.

I've tried to look through the bazel configs but there's nothing obvious to me which would cause this!

### 2.16.2 
versions_2.16.2_libtensorflow-cpu-windows-x86_64 238MB
versions_2.16.2_libtensorflow-cpu-linux-x86_64 422MB

### 2.17.0
 versions_2.17.0_libtensorflow-cpu-windows-x86_64 909MB
versions_2.17.0_libtensorflow-cpu-linux-x86_64 412MB

### 2.18.0
versions_2.18.0_libtensorflow-cpu-windows-x86_64 931MB

### Standalone code to reproduce the issue

```shell
Libtensorflow is provided by google via the GCS buckets documented here https://www.tensorflow.org/install/lang_c
```

### Relevant log output

```shell

```",tboby,2025-01-15 14:25:26+00:00,['tilakrayal'],2025-01-17 03:54:08+00:00,,https://github.com/tensorflow/tensorflow/issues/84962,"[('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]",[],
2788849050,issue,open,,"tensorflow cuda Unable to register cuDNN factory error in wsl2 with tf 2.17,18","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

wsl2 ubuntu 24.04lts

### Mobile device

windows 11 x86

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0

### CUDA/cuDNN version

12.5/9.3

### GPU model and memory

rtx 4060 laptop gpu/8gb vram/16gb ram

### Current behavior?

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
2025-01-15 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-15 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-15 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Standalone code to reproduce the issue
steps:
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#verify-you-have-a-cuda-capable-gpu
use wsl method

```shell
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Relevant log output

```shell
2025-01-15 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-15 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-15 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
v2.18.0-rc2-4-g6550e4bd802 2.18.0
```",Jaswanth28,2025-01-15 05:05:58+00:00,['Venkat6871'],2025-02-01 02:02:29+00:00,,https://github.com/tensorflow/tensorflow/issues/84921,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('wsl2', 'Windows Subsystem for Linux'), ('TF 2.18', '')]","[{'comment_id': 2611690333, 'issue_id': 2788849050, 'author': 'Venkat6871', 'body': 'Hi **@Jaswanth28** ,\n\nApologies for the delay, and thank you for reporting the issue. This is a known issue, and there are other related open issues that developers are actively working on.\nI would recommend taking a look at #62075, where a similar issue has been reported and is still under review. Additionally, please follow #70947, which tracks updates on a related issue for more information and progress.\nThank you for your patience and understanding!', 'created_at': datetime.datetime(2025, 1, 24, 6, 19, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628686190, 'issue_id': 2788849050, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 1, 2, 2, 27, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-24 06:19:48 UTC): Hi **@Jaswanth28** ,

Apologies for the delay, and thank you for reporting the issue. This is a known issue, and there are other related open issues that developers are actively working on.
I would recommend taking a look at #62075, where a similar issue has been reported and is still under review. Additionally, please follow #70947, which tracks updates on a related issue for more information and progress.
Thank you for your patience and understanding!

github-actions[bot] on (2025-02-01 02:02:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2788354876,issue,open,,Seg Fault when iterate dataset created from data service,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when trying to iterate dataset get from data service.

### Standalone code to reproduce the issue

```shell
# start the data service file start_dataservice.py

import tensorflow as tf

dispatcher = tf.data.experimental.service.DispatchServer(
    tf.data.experimental.service.DispatcherConfig(port=50050), start=True
)
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
)
print(""Starting Worker"")
worker.join()

# test file test_dataset_service.py
import tensorflow as tf
import numpy as np


flags = tf.compat.v1.app.flags

flags.DEFINE_bool(""local"", False, ""Run data service in process"")
flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"")
FLAGS = flags.FLAGS


def local_service():
    print(""Starting Local Service"")
    dispatcher = tf.data.experimental.service.DispatchServer(
        tf.data.experimental.service.DispatcherConfig(port=50050), start=True
    )
    dispatcher_address = dispatcher.target.split(""://"")[1]
    worker = tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
    )
    print(""Dispatcher target is "", dispatcher.target)
    return dispatcher, worker, dispatcher.target


def apply_transformations(ds_train):
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds_train = ds_train.cache()
    ds_train = ds_train.shuffle(60000)
    ds_train = ds_train.batch(128)
    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
    return ds_train


(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / np.float32(255)
y_train = y_train.astype(np.int64)
ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = apply_transformations(ds_train)
# Create dataset however you were before using the tf.data service.
dataset = ds_train
if FLAGS.local:
    dispatcher, worker, service = local_service()
else:
    dispatcher_address = ""localhost""
    dispatcher_port = ""50050""
    service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port)
if FLAGS.distribute:
    processing_mode = ""distributed_epoch""
else:
    processing_mode = ""parallel_epochs""

# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(
    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)
)

for (x1, y1), (x2, y2) in zip(dataset, ds_train):
    np.allclose(x1, x2)
    np.allclose(y1, y2)

print(""verified mnist dataset locally vs over service"")

# script to run 
python -m pip install --upgrade pip
python -m pip install tensorflow==2.18.0
python -m pip install 'protobuf<4'
screen -d -m python start_dataservice.py
python3 test_dataset_service.py --local=False
```

### Relevant log output

```shell
2025-01-14 21:56:19.778399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-14 21:56:19.815971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0
I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0
I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0
I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0
I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0
I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0
I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0
I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0
/test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py --local=False
```",Yadan-Wei,2025-01-14 21:57:20+00:00,['tilakrayal'],2025-01-17 08:33:19+00:00,,https://github.com/tensorflow/tensorflow/issues/84897,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]",[],
2788039293,issue,closed,completed,First project,https://github.com/nodejs/node/commit/05d6227a8898a7638fbffeae435ce73ecad525b2,Morehman27,2025-01-14 18:46:17+00:00,['Venkat6871'],2025-01-16 00:44:59+00:00,2025-01-16 00:44:58+00:00,https://github.com/tensorflow/tensorflow/issues/84880,"[('stat:awaiting response', 'Status  - Awaiting response from author')]","[{'comment_id': 2591683412, 'issue_id': 2788039293, 'author': 'Venkat6871', 'body': 'Hi **@Morehman27** ,\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature.\nThank you!', 'created_at': datetime.datetime(2025, 1, 15, 5, 29, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594224489, 'issue_id': 2788039293, 'author': 'mihaimaruseac', 'body': 'This is spam.', 'created_at': datetime.datetime(2025, 1, 16, 0, 44, 58, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-15 05:29:14 UTC): Hi **@Morehman27** ,
Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature.
Thank you!

mihaimaruseac on (2025-01-16 00:44:58 UTC): This is spam.

"
2786583388,issue,open,,Yolov8-seg.pt segmentation model is deployed on Android after training,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):

vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/release-keys

- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):

    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.9.0'

- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Here is the full code of the program
""""""
public class MainActivity extends AppCompatActivity {
    private String MODEL = ""best_float32_metadata.tflite"";

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.a3);
        bitmap = imageScale(bitmap, 640, 640);
        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);

        Log.e(""HENG"", String.valueOf(tensorImage.getBuffer()));
        Log.e(""HENG"", String.valueOf(tensorImage.getTensorBuffer()));
        Log.e(""HENG"", String.valueOf(tensorImage.getDataType()));
        Log.e(""HENG"", String.valueOf(tensorImage.getColorSpaceType()));

        ImageSegmenter.ImageSegmenterOptions options = ImageSegmenter.ImageSegmenterOptions.builder()
                .setBaseOptions(BaseOptions.builder().build())
                .setOutputType(OutputType.CONFIDENCE_MASK)
                .build();

        ImageSegmenter imageSegmenter = null;

        try {
            imageSegmenter = ImageSegmenter.createFromFileAndOptions(this, MODEL, options);
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        List<Segmentation> results = imageSegmenter.segment(tensorImage);
        Log.e(""HENG"", ""HELLO: ""+results.toString());
    }

    // 
    public static Bitmap imageScale(Bitmap bitmap, int new_w, int new_h) {
        Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, new_w, new_h, true);
        return scaledBitmap;
    }
}
""""""

**Any other info / logs**
Please allow me to repeat my question. Thank you,
First, (I may have solved the first problem, but I am not sure) I trained my data set with yolov8-seg.pt to get a model. I converted it to tflite format, copied the 32-bit model generated by best_float32.tflite into asssets in Android, and then modified the path of model to run the following original code. I got two error messages: ""1 Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.2java.lang.IllegalStateException: Error getting native address of native library: task_vision_jni, After I searched, I found https://stackoverflow.com/questions/66727627/failed-to-initialize-detector-input-tensor-has-type-ktflitefloat32-ml-kit I got a copy of the code in this link. After I tried to run it, I put the model I got into assets again. After running, it was not the above error message (the error message is the second problem).
Second, (from the follow-up to the first question), I also got two errors after running the modified model ""best_float32_metadata.tflite"". The first one is ""java.lang.illegalargumentexception: error occurred when initializing imagesegment: image segmentation models are expected to have only 1 output, found 2"". It says that the model actually returns two outputs, which is a very important question. The second one seems to be the same as the first one, ""java.lang.runtimeexception: unable to start action""activity componentinfo{com.example.yoloseg_android/com.example.yoloseg_android.mainactivity}: java.lang.illegalstateexception: error getting native address of native library: task_vision_jni "", I don't understand this.
These are my two problems. I think the second problem should be solved.

<!-- Failed to upload ""YoloSegAndroid.zip"" -->

Note: I can use the deeplabv3.tflite model officially provided by tensorflow to get the output smoothly",fsamekl,2025-01-14 08:50:42+00:00,['gaikwadrahul8'],2025-01-16 07:26:08+00:00,,https://github.com/tensorflow/tensorflow/issues/84829,"[('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('Android', '')]","[{'comment_id': 2589401754, 'issue_id': 2786583388, 'author': 'fsamekl', 'body': 'I uploaded the project to GitHub. Thank you for your helphttps://github.com/fsamekl/Yolov8seg-Android-tflite/tree/master', 'created_at': datetime.datetime(2025, 1, 14, 9, 19, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594655598, 'issue_id': 2786583388, 'author': 'gaikwadrahul8', 'body': 'Hi, @fsamekl \nI apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions.\n\nLiteRT Metadata Writer API provides an easy-to-use API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using [Image segmenters](https://ai.google.dev/edge/litert/models/metadata_writer_tutorial#image_segmenters), Please refer [TensorFlow Lite Image Segmentation Demo example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/android) which may help you to solve your issue.\n\nThank you for your understanding and patience.', 'created_at': datetime.datetime(2025, 1, 16, 6, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594712697, 'issue_id': 2786583388, 'author': 'fsamekl', 'body': '> Hi, [@fsamekl](https://github.com/fsamekl) I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions.\n> \n> LiteRT Metadata Writer API provides an easy-to-use API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using [Image segmenters](https://ai.google.dev/edge/litert/models/metadata_writer_tutorial#image_segmenters), Please refer [TensorFlow Lite Image Segmentation Demo example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/android) which may help you to solve your issue.\n> \n> Thank you for your understanding and patience.\n\nThe second question you didn\'t answer me. I used the segmentation model trained by yolov8. It has two outputs, but imagesegment=imagesegment.createfromfileandoptions (this, model, options); This can only accept one output. What should I do? I uploaded the code to GitHub. I hope you can help solve it', 'created_at': datetime.datetime(2025, 1, 16, 7, 26, 6, tzinfo=datetime.timezone.utc)}]","fsamekl (Issue Creator) on (2025-01-14 09:19:23 UTC): I uploaded the project to GitHub. Thank you for your helphttps://github.com/fsamekl/Yolov8seg-Android-tflite/tree/master

gaikwadrahul8 (Assginee) on (2025-01-16 06:44:43 UTC): Hi, @fsamekl 
I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions.

LiteRT Metadata Writer API provides an easy-to-use API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using [Image segmenters](https://ai.google.dev/edge/litert/models/metadata_writer_tutorial#image_segmenters), Please refer [TensorFlow Lite Image Segmentation Demo example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/android) which may help you to solve your issue.

Thank you for your understanding and patience.

fsamekl (Issue Creator) on (2025-01-16 07:26:06 UTC): The second question you didn't answer me. I used the segmentation model trained by yolov8. It has two outputs, but imagesegment=imagesegment.createfromfileandoptions (this, model, options); This can only accept one output. What should I do? I uploaded the code to GitHub. I hope you can help solve it

"
2785077413,issue,closed,duplicate,Tensorflow importing error please any help,"Traceback (most recent call last):
  File ""C:\anaconda\envs\Mymachenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\anaconda\envs\Mymachenv\lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\anaconda\envs\Mymachenv\lib\site-packages\tensorflow\python\__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\anaconda\envs\Mymachenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\anaconda\envs\Mymachenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
",kossivi4,2025-01-13 19:14:50+00:00,['Venkat6871'],2025-01-16 00:43:47+00:00,2025-01-16 00:43:44+00:00,https://github.com/tensorflow/tensorflow/issues/84783,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2591679797, 'issue_id': 2785077413, 'author': 'Venkat6871', 'body': 'Hi **@kossivi4** ,\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\n```\n- You need to install the MSVC 2019 redistributable\n- Your CPU does not support AVX2 instructions\n- Your CPU/Python is on 32 bits\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\n```\nAlso kindly provide the environment details and the steps followed to install the tensorflow.\n#61887\nThank you!', 'created_at': datetime.datetime(2025, 1, 15, 5, 25, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592540977, 'issue_id': 2785077413, 'author': 'muskandz', 'body': ""Hi @kossivi4 ,\n\nThis type of error in TensorFlow typically arises from compatibility issues or missing dependencies. You can try these common solutions:\n\n1. Ensure your TensorFlow version is compatible with your Python version and operating system. You can refer to official documentation for compatibility matrix.\n2. Try Reinstalling TensorFlow using pip or conda.\n3. Isolate TensorFlow and its dependencies (virtual environment), this will prevent conflicts with other packages.\n4. TensorFlow might require specific Visual C++ Redistributable packages. Install Microsoft Visual C++ Redistributable.\n5. Check you GPU drivers are compatible and up-to-date. Install correct versions of CUDA and cuDNN compatible with your TensorFlow version and GPU.\n6. Verify that the PATH environment variable includes the directories where TensorFlow's DLLs are located.\n7. After making any changes, restart your terminal or IDE to ensure the changes take effect."", 'created_at': datetime.datetime(2025, 1, 15, 11, 56, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594223010, 'issue_id': 2785077413, 'author': 'mihaimaruseac', 'body': 'This is a duplicate of #19584 Closing unless you have information that is different from that.', 'created_at': datetime.datetime(2025, 1, 16, 0, 43, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594223056, 'issue_id': 2785077413, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84783"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84783"">No</a>', 'created_at': datetime.datetime(2025, 1, 16, 0, 43, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-15 05:25:26 UTC): Hi **@kossivi4** ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:
```
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```
Also kindly provide the environment details and the steps followed to install the tensorflow.
#61887
Thank you!

muskandz on (2025-01-15 11:56:49 UTC): Hi @kossivi4 ,

This type of error in TensorFlow typically arises from compatibility issues or missing dependencies. You can try these common solutions:

1. Ensure your TensorFlow version is compatible with your Python version and operating system. You can refer to official documentation for compatibility matrix.
2. Try Reinstalling TensorFlow using pip or conda.
3. Isolate TensorFlow and its dependencies (virtual environment), this will prevent conflicts with other packages.
4. TensorFlow might require specific Visual C++ Redistributable packages. Install Microsoft Visual C++ Redistributable.
5. Check you GPU drivers are compatible and up-to-date. Install correct versions of CUDA and cuDNN compatible with your TensorFlow version and GPU.
6. Verify that the PATH environment variable includes the directories where TensorFlow's DLLs are located.
7. After making any changes, restart your terminal or IDE to ensure the changes take effect.

mihaimaruseac on (2025-01-16 00:43:44 UTC): This is a duplicate of #19584 Closing unless you have information that is different from that.

google-ml-butler[bot] on (2025-01-16 00:43:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84783"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84783"">No</a>

"
2783431552,issue,closed,completed,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build ,"docker run -it -v $PWD:/tmp -w /tmp tensorflow/build:2.19-python3.10  bash -c ""bazel build --experimental_action_cache_store_output_metadata --disk_cache=~/.cache/bazel  --jobs=3 --config=linux   //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""

### Error message is 
/usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
 1070 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
      |  



### Full debug message is:
ERROR: /tmp/tensorflow/compiler/mlir/lite/BUILD:1295:11: Compiling tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 235 arguments skipped)
In file included from /usr/include/c++/13/memory:78,
                 from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16:
/usr/include/c++/13/bits/unique_ptr.h: In instantiation of 'std::__detail::__unique_ptr_t<_Tp> std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; __detail::__unique_ptr_t<_Tp> = __detail::__unique_ptr_t<mlir::TFL::{anonymous}::DefaultQuantParamsPass>]':
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50:   required from here
/usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
 1070 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:52:
bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/transforms/passes.h.inc:162:3: note: candidate: 'mlir::TFL::{anonymous}::impl::DefaultQuantParamsPassBase<DerivedT>::DefaultQuantParamsPassBase(mlir::TFL::DefaultQuantParamsPassOptions) [with DerivedT = mlir::TFL::{anonymous}::DefaultQuantParamsPass]'
  162 |   DefaultQuantParamsPassBase(DefaultQuantParamsPassOptions options) : DefaultQuantParamsPassBase() {
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:57:37: note:   inherited here
   57 |   using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase;
      |                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:66:12: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)'
   66 |   explicit DefaultQuantParamsPass(
      |            ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::{anonymous}::DefaultQuantParamsPass&)'
   54 | class DefaultQuantParamsPass
      |       ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted)
Target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 407.917s, Critical Path: 206.23s
INFO: 43 processes: 7 internal, 36 local.
FAILED: Build did NOT complete successfully",codinglover222,2025-01-13 09:25:23+00:00,['Venkat6871'],2025-01-16 16:06:03+00:00,2025-01-16 16:06:03+00:00,https://github.com/tensorflow/tensorflow/issues/84739,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2591672245, 'issue_id': 2783431552, 'author': 'Venkat6871', 'body': 'Hi **@codinglover222** ,\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature.\nThank you!', 'created_at': datetime.datetime(2025, 1, 15, 5, 17, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2593553657, 'issue_id': 2783431552, 'author': 'codinglover222', 'body': 'Thanks @Venkat6871 I created Issue#84977\nshould I close this one?', 'created_at': datetime.datetime(2025, 1, 15, 17, 35, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594441235, 'issue_id': 2783431552, 'author': 'Venkat6871', 'body': 'Hi **@codinglover222** ,\nThank you for providing all the information. Yes, please feel free to close this issue as it is already being tracked in another issue opened by you.\nThank you!', 'created_at': datetime.datetime(2025, 1, 16, 4, 13, 49, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-15 05:17:26 UTC): Hi **@codinglover222** ,
Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature.
Thank you!

codinglover222 (Issue Creator) on (2025-01-15 17:35:18 UTC): Thanks @Venkat6871 I created Issue#84977
should I close this one?

Venkat6871 (Assginee) on (2025-01-16 04:13:49 UTC): Hi **@codinglover222** ,
Thank you for providing all the information. Yes, please feel free to close this issue as it is already being tracked in another issue opened by you.
Thank you!

"
2782397496,issue,closed,completed,tensorflow,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

windows

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

unable to load tensorflow as tf

### Standalone code to reproduce the issue

```shell
# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f""Test Accuracy: {test_acc:.2f}"")
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow as tf
      3 a = tf.constant(2)
      4 b = tf.constant(3)

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\lenovo\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```
",Cyprian-igban,2025-01-12 10:37:11+00:00,['tilakrayal'],2025-01-12 16:27:19+00:00,2025-01-12 16:27:15+00:00,https://github.com/tensorflow/tensorflow/issues/84692,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2585786269, 'issue_id': 2782397496, 'author': 'xionams', 'body': 'hey @Cyprian-igban check out #46738', 'created_at': datetime.datetime(2025, 1, 12, 15, 48, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585798823, 'issue_id': 2782397496, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584', 'created_at': datetime.datetime(2025, 1, 12, 16, 27, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585798831, 'issue_id': 2782397496, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84692"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84692"">No</a>', 'created_at': datetime.datetime(2025, 1, 12, 16, 27, 18, tzinfo=datetime.timezone.utc)}]","xionams on (2025-01-12 15:48:33 UTC): hey @Cyprian-igban check out #46738

mihaimaruseac on (2025-01-12 16:27:16 UTC): Duplicate of #19584

google-ml-butler[bot] on (2025-01-12 16:27:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84692"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84692"">No</a>

"
2780133136,issue,closed,completed,Tensorflow.math.floormod(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The operation tf.math.floormod supports float types, but when performing the operation on two float-type tensors with GPU, an internal error occurs.
`import tensorflow as tf x = tf.constant([10, -15, 7.5], dtype=tf.float32) y = tf.constant([3, -4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)`
**2025-01-10 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0.
2025-01-10 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-01-10 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6
2025-01-10 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2025-01-10 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1736501646.122604 70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.124987 70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.127360 70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.129708 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.132058 70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.135845 70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.137479 70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.139073 70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.140713 70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.142340 70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.143637 70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.144931 70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.159628 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2025-01-10 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION'

2025-01-10 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'

2025-01-10 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
2025-01-10 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
Traceback (most recent call last):
File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in
result_code = tf.math.floormod(x,y,name)
File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
return op(*args, kwargs)
File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod
_ops.raise_from_not_ok_status(e, name)
File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
raise core._status_to_exception(e) from None # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: {{function_node _wrapped__FloorMod_device/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation
The operation runs normally under integer types with GPU.
`import tensorflow as tf x = tf.constant([10, -15, 7], dtype=tf.int32) y = tf.constant([3, -4, 2], dtype=tf.int32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)`
2025-01-10 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-01-10 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6
2025-01-10 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2025-01-10 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6
!!!!!!!!!!!!!!!!!!!!!!!!!!!
tf.Tensor([ 1 -3 1], shape=(3,), dtype=int32)

The float type is correct for the CPU as well.
`import tensorflow as tf
x = tf.constant([10, -15, 7.8], dtype=tf.float32)
y = tf.constant([3, -4, 2.5], dtype=tf.float32)
name = ""random_floormod_operation""
with tf.device('/CPU:0'):
    result_code = tf.math.floormod(x,y,name)
print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"")
print(result_code)`2025-01-10 12:57:31.435249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-10 12:57:31.449893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 12:57:31.467647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 12:57:31.472965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 12:57:31.486020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 12:57:32.528966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-01-10 12:57:34.007683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 447 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6
2025-01-10 12:57:34.008219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2025-01-10 12:57:34.008642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6
!!!!!!!!!!!!!!!!!!!!!!!!!!!
tf.Tensor([ 1.        -3.         0.3000002], shape=(3,), dtype=float32)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10, -15, 7.5], dtype=tf.float32)
y = tf.constant([3, -4, 2.5], dtype=tf.float32)
name = ""random_floormod_operation""
result_code = tf.math.floormod(x,y,name)
```


### Relevant log output

_No response_",yangjingyuan000804,2025-01-10 12:58:11+00:00,['Venkat6871'],2025-01-29 01:59:12+00:00,2025-01-29 01:59:09+00:00,https://github.com/tensorflow/tensorflow/issues/84585,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2586223310, 'issue_id': 2780133136, 'author': 'Venkat6871', 'body': 'Hi **@yangjingyuan000804** ,\r\nApologies for the delay, and welcome to TensorFlow! I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions with GPU, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/965cfd993a4a8029fd1fcc5d5b53d21c/84585_tf_2-17-0-2-18-0-v-gpu.ipynb) attached here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 13, 5, 56, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603473853, 'issue_id': 2780133136, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 21, 1, 59, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469189, 'issue_id': 2780133136, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620469220, 'issue_id': 2780133136, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84585"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84585"">No</a>', 'created_at': datetime.datetime(2025, 1, 29, 1, 59, 10, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-13 05:56:12 UTC): Hi **@yangjingyuan000804** ,
Apologies for the delay, and welcome to TensorFlow! I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions with GPU, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/965cfd993a4a8029fd1fcc5d5b53d21c/84585_tf_2-17-0-2-18-0-v-gpu.ipynb) attached here for your reference.
Thank you!

github-actions[bot] on (2025-01-21 01:59:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-29 01:59:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-29 01:59:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84585"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84585"">No</a>

"
2779725991,issue,closed,completed,Tensorflow.math.floormod(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The operation tf.math.floormod supports float types, but when performing the operation on two float-type tensors, an internal error occurs.
`import tensorflow as tf
x = tf.constant([10, -15, 7.5], dtype=tf.float32)
y = tf.constant([3, -4, 2.5], dtype=tf.float32)
name = ""random_floormod_operation""
result_code = tf.math.floormod(x,y,name)
print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"")
print(result_code)`
**2025-01-10 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-10 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-01-10 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6
2025-01-10 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2025-01-10 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1736501646.122604   70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.124987   70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.127360   70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.129708   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.132058   70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.135845   70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.137479   70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.139073   70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.140713   70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.142340   70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.143637   70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.144931   70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
W0000 00:00:1736501646.159628   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2025-01-10 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION'

2025-01-10 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'

2025-01-10 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
2025-01-10 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
Traceback (most recent call last):
  File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in <module>
    result_code = tf.math.floormod(x,y,name)
  File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
  File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod
    _ops.raise_from_not_ok_status(e, name)
  File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation**
The operation runs normally under integer types.
`import tensorflow as tf
x = tf.constant([10, -15, 7], dtype=tf.int32)
y = tf.constant([3, -4, 2], dtype=tf.int32)
name = ""random_floormod_operation""
result_code = tf.math.floormod(x,y,name)
print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"")
print(result_code)`
**2025-01-10 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-01-10 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6
2025-01-10 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2025-01-10 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6
!!!!!!!!!!!!!!!!!!!!!!!!!!!
tf.Tensor([ 1 -3  1], shape=(3,), dtype=int32)**


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10, -15, 7.5], dtype=tf.float32)
y = tf.constant([3, -4, 2.5], dtype=tf.float32)
name = ""random_floormod_operation""
result_code = tf.math.floormod(x,y,name)
```


### Relevant log output

_No response_",yangjingyuan000804,2025-01-10 09:40:55+00:00,['Venkat6871'],2025-01-10 09:56:50+00:00,2025-01-10 09:56:47+00:00,https://github.com/tensorflow/tensorflow/issues/84577,"[('type:bug', 'Bug')]","[{'comment_id': 2582244756, 'issue_id': 2779725991, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84577"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84577"">No</a>', 'created_at': datetime.datetime(2025, 1, 10, 9, 56, 49, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2025-01-10 09:56:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84577"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84577"">No</a>

"
2779441654,issue,closed,not_planned,gen_quantized_function_library: clang-cl compilation file path error,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19 nightly

### Custom code

Yes

### OS platform and distribution

Windows 11 24H2

### Mobile device

_No response_

### Python version

Anaconda 2024.10-1

### Bazel version

6.5.0

### GCC/compiler version

Visual Studio 2022 (build tools 14.42) + LLVM 19.1.6 + msys2-x86_64-20241208

### CUDA/cuDNN version

CUDA 12.6.3 + CUDNN 9.6.0

### GPU model and memory

GTX 1050 Ti 4GB

### Current behavior?

`gen_quantized_function_library` is trying to read `'C:\\msys64\\home\\*\\_bazel_*\\*\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt-exec-*\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py'` on Windows.

However, `os.path.exists()` cannot resolve `\\` symbol. Correct path is just like:
`'C:/msys64/home/AMD/_bazel_amd/2oea4ayg/execroot/org_tensorflow/bazel-out/x64_windows-opt-exec-BB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library'`

### Standalone code to reproduce the issue

```shell
1. download https://github.com/johnnkp/tensorflow-wgpu-test/archive/refs/heads/default_memory_space_description.zip and extract
2. run `python configure.py` to configure Windows CUDA build
3. run `bazel build --config=win_clang --config=cuda_wheel --config=opt --define=no_tensorflow_py_deps=true --repo_env=TF_PYTHON_VERSION=3.12 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_gpu`
```


### Relevant log output

```shell
ERROR: C:/users/amd/downloads/tensorflow-wgpu-test/tensorflow/compiler/mlir/quantization/tensorflow/BUILD:38:8: Executing genrule //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library failed: (Exit 1): bash.exe failed: error executing command (from target //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library)
  cd /d C:/msys64/home/amd/_bazel_amd/2oea4ayg/execroot/org_tensorflow
  SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\extras\CUPTI\lib64;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin;C:\Program Files\LLVM\bin;C:\Users\AMD\anaconda3\Scripts;C:\Users\AMD\anaconda3;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl
    SET PYTHON_BIN_PATH=C:/Users/AMD/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/AMD/anaconda3/Lib/site-packages
    SET TF2_BEHAVIOR=1
  C:\msys64\usr\bin\bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-BB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library.exe --output_file bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.h --src 'tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_tf_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_xla_weight_only.mlir'
# Configuration: bb43855b4d3a8365141a732926c882a21d79a321eb57928ec09cdaf313f3403c
# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl
Traceback (most recent call last):
  File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazel-out\x64_windows-opt-exec-BB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 559, in <module>
    Main()
  File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazel-out\x64_windows-opt-exec-BB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 490, in Main
    assert os.path.exists(main_filename), \
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Cannot exec() 'C:\\msys64\\home\\AMD\\_bazel_amd\\2oea4ayg\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt-exec-BB41B15F\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py': file not found.
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 65.631s, Critical Path: 5.13s
INFO: 30 processes: 13 disk cache hit, 8 internal, 9 local.
FAILED: Build did NOT complete successfully
```
",johnnkp,2025-01-10 07:01:31+00:00,['Venkat6871'],2025-01-20 05:47:19+00:00,2025-01-18 08:12:39+00:00,https://github.com/tensorflow/tensorflow/issues/84558,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2582720561, 'issue_id': 2779441654, 'author': 'johnnkp', 'body': 'I found out `rules_python` provide the python file template, but modify `python/private/python_bootstrap_template.txt` will not affect the generated `.py`.', 'created_at': datetime.datetime(2025, 1, 10, 13, 31, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597688545, 'issue_id': 2779441654, 'author': 'Venkat6871', 'body': 'Hi **@johnnkp** ,\nApologies for the delay, and thank you for raising your issue here. The main cause appears to be related to your file path. On Windows, path handling is different, and the error suggests that os.path.exists() cannot correctly resolve the path due to the use of backslashes (`\\`). These are standard in Windows paths but need to be properly managed in Python.\nTo resolve this, configure your setup to use forward slashes (`/`) instead of backslashes (`\\`) for Windows paths. Python, especially when running in environments like MSYS2 or Git Bash, often handles forward slashes more consistently.\nIf you have already tried this, please rebuild the Bazel target that generates the `.py` file. If the issue persists, let us know so we can further assist you.\nThank you!', 'created_at': datetime.datetime(2025, 1, 17, 8, 28, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599609476, 'issue_id': 2779441654, 'author': 'johnnkp', 'body': 'Although I already found out a fix for this issue, I am not going to create a pull request because my compilation failed with nvcc error in windows. And my fix seems unnecessary for other builds.', 'created_at': datetime.datetime(2025, 1, 18, 8, 12, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599609590, 'issue_id': 2779441654, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84558"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84558"">No</a>', 'created_at': datetime.datetime(2025, 1, 18, 8, 12, 41, tzinfo=datetime.timezone.utc)}]","johnnkp (Issue Creator) on (2025-01-10 13:31:37 UTC): I found out `rules_python` provide the python file template, but modify `python/private/python_bootstrap_template.txt` will not affect the generated `.py`.

Venkat6871 (Assginee) on (2025-01-17 08:28:19 UTC): Hi **@johnnkp** ,
Apologies for the delay, and thank you for raising your issue here. The main cause appears to be related to your file path. On Windows, path handling is different, and the error suggests that os.path.exists() cannot correctly resolve the path due to the use of backslashes (`\`). These are standard in Windows paths but need to be properly managed in Python.
To resolve this, configure your setup to use forward slashes (`/`) instead of backslashes (`\`) for Windows paths. Python, especially when running in environments like MSYS2 or Git Bash, often handles forward slashes more consistently.
If you have already tried this, please rebuild the Bazel target that generates the `.py` file. If the issue persists, let us know so we can further assist you.
Thank you!

johnnkp (Issue Creator) on (2025-01-18 08:12:39 UTC): Although I already found out a fix for this issue, I am not going to create a pull request because my compilation failed with nvcc error in windows. And my fix seems unnecessary for other builds.

google-ml-butler[bot] on (2025-01-18 08:12:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84558"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84558"">No</a>

"
2779102599,issue,closed,completed,FAILURE: Build failed with an exception during step 3 of Digit Classifier Codelab,"I encountered an issue during step 3 of the [Digit Classifier Codelab](https://developer.android.com/codelabs/digit-classifier-tflite#2). Specifically, while syncing the Gradle project, I received the following error:

```
* What went wrong:
Failed to notify dependency resolution listener.
'void org.gradle.api.artifacts.DependencySubstitutions$Substitution.with(org.gradle.api.artifacts.component.ComponentSelector)'

```

Steps to Reproduce
Download the project as a .zip file from the Digit Classifier Codelab.
Open the project in Android Studio.
Run the Gradle sync.

Expected Behavior
Gradle sync should complete successfully, allowing me to continue with the codelab.

Actual Behavior
The sync fails with the above error message.

Is there a missing dependency or configuration step that I overlooked?
Could this be related to a mismatch in Gradle or JDK versions?

Environment:
Android Studio Version: Android Studio Ladybug | 2024.2.1 Patch 3
Gradle Version: gradle-8.5-bin.zip
Operating System: macOS 15.2 (24C101)
JDK Version: 17.0.13 and 21.0.3",andresserranodev,2025-01-10 02:00:19+00:00,['gaikwadrahul8'],2025-01-23 19:23:10+00:00,2025-01-23 19:23:09+00:00,https://github.com/tensorflow/tensorflow/issues/84529,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues')]","[{'comment_id': 2586369228, 'issue_id': 2779102599, 'author': 'gaikwadrahul8', 'body': ""Hi, @andresserranodev\r\nI apologize for the delayed response, I would suggest you to please clone this [repo](https://github.com/tensorflow/examples/tree/master) and follow below steps for building the project :\r\n\r\n- Open Android Studio. From the Welcome screen, select Open an existing Android Studio project.\r\n- From the Open File or Project window that appears, navigate to and select the `tensorflow-lite/examples/digit_classifier/android` directory. Click OK.\r\n- If it asks you to do a Gradle Sync, click OK.\r\n- With your Android device connected to your computer and developer mode enabled, click on the green Run arrow in Android Studio.\r\n\r\nThere might be `gradle version 8.5` compatibility issues at the moment so I tried with gradle version` 7.6.2 `instead of `8.5` so I changed this line `distributionUrl=https\\://services.gradle.org/distributions/gradle-8.5-bin.zip` to this `distributionUrl=https\\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle version `7.6.2` and our relevant team will fix this issue with gradle version `8.5` soon\r\n\r\n**Here is output screenshot for reference :**\r\n\r\n![image](https://github.com/user-attachments/assets/5d82a7b4-4c6c-4543-8987-f5831c66163f)\r\n\r\nPlease give it try from your end and let us know is it working as expected or not with gradle version `7.6.2` ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 13, 7, 25, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587199452, 'issue_id': 2779102599, 'author': 'andresserranodev', 'body': 'Hi @gaikwadrahul8 ! The gradle version 7.6.2 works. Many thanks!', 'created_at': datetime.datetime(2025, 1, 13, 14, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592410542, 'issue_id': 2779102599, 'author': 'gaikwadrahul8', 'body': 'Hi, @andresserranodev \nThank you for your kind words, Good to hear that things are working as expected with `gradle version 7.6.2` If your issue has been resolved please feel free to close this issue.\n\nWe understand this is a known issue and our relavant team is actively working on a permanent resolution.\n\nThank you for your patience and understanding', 'created_at': datetime.datetime(2025, 1, 15, 11, 15, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608685140, 'issue_id': 2779102599, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 23, 1, 59, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610832262, 'issue_id': 2779102599, 'author': 'andresserranodev', 'body': 'The gradle version 7.6.2 works. Many thanks!', 'created_at': datetime.datetime(2025, 1, 23, 19, 23, 9, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-13 07:25:05 UTC): Hi, @andresserranodev
I apologize for the delayed response, I would suggest you to please clone this [repo](https://github.com/tensorflow/examples/tree/master) and follow below steps for building the project :

- Open Android Studio. From the Welcome screen, select Open an existing Android Studio project.
- From the Open File or Project window that appears, navigate to and select the `tensorflow-lite/examples/digit_classifier/android` directory. Click OK.
- If it asks you to do a Gradle Sync, click OK.
- With your Android device connected to your computer and developer mode enabled, click on the green Run arrow in Android Studio.

There might be `gradle version 8.5` compatibility issues at the moment so I tried with gradle version` 7.6.2 `instead of `8.5` so I changed this line `distributionUrl=https\://services.gradle.org/distributions/gradle-8.5-bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle version `7.6.2` and our relevant team will fix this issue with gradle version `8.5` soon

**Here is output screenshot for reference :**

![image](https://github.com/user-attachments/assets/5d82a7b4-4c6c-4543-8987-f5831c66163f)

Please give it try from your end and let us know is it working as expected or not with gradle version `7.6.2` ?

Thank you for your cooperation and patience.

andresserranodev (Issue Creator) on (2025-01-13 14:07:00 UTC): Hi @gaikwadrahul8 ! The gradle version 7.6.2 works. Many thanks!

gaikwadrahul8 (Assginee) on (2025-01-15 11:15:36 UTC): Hi, @andresserranodev 
Thank you for your kind words, Good to hear that things are working as expected with `gradle version 7.6.2` If your issue has been resolved please feel free to close this issue.

We understand this is a known issue and our relavant team is actively working on a permanent resolution.

Thank you for your patience and understanding

github-actions[bot] on (2025-01-23 01:59:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

andresserranodev (Issue Creator) on (2025-01-23 19:23:09 UTC): The gradle version 7.6.2 works. Many thanks!

"
2779041274,issue,closed,completed,Memory Allocation Issues,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS on WSL2

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

90300

### GPU model and memory

RTX 4070 12GB

### Current behavior?

I create a virtual gpu with a hard limit of 10GB. I start training the network and it works for bit but then says out of memory and tries to allocate more than the set limit. What I expect to happen is that is stays within the 10GB limit and can train the network successfully.

### Standalone code to reproduce the issue

```shell
import numpy as np
import keras
from keras import layers
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

%matplotlib inline

tfds.disable_progress_bar()

gpus = tf.config.list_physical_devices('GPU')

if gpus:
  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
  try:
    tf.config.set_logical_device_configuration(
        gpus[0],
        [tf.config.LogicalDeviceConfiguration(memory_limit=10240)])
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
  except RuntimeError as e:
    # Virtual devices must be set before GPUs have been initialized
    print(e)

train_ds, validation_ds, test_ds = tfds.load(
    ""cats_vs_dogs"",
    # Reserve 10% for validation and 10% for test
    split=[""train[:40%]"", ""train[40%:50%]"", ""train[50%:60%]""],
    as_supervised=True,  # Include labels
)

resize_fn = keras.layers.Resizing(150, 150)
train_ds = train_ds.map(lambda x, y: (resize_fn(x), y))
validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))
test_ds = test_ds.map(lambda x, y: (resize_fn(x), y))

augmentation_layers = [
    layers.RandomFlip(""horizontal""),
    layers.RandomRotation(0.1),
]


def data_augmentation(x):
    for layer in augmentation_layers:
        x = layer(x)
    return x


train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))

from tensorflow import data as tf_data

batch_size = 16

train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()
validation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()
test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()

base_model = keras.applications.Xception(
    weights=""imagenet"",  # Load weights pre-trained on ImageNet.
    input_shape=(150, 150, 3),
    include_top=False,
)  # Do not include the ImageNet classifier at the top.

# Freeze the base_model
base_model.trainable = False

# Create new model on top
inputs = keras.Input(shape=(150, 150, 3))

# Pre-trained Xception weights requires that input be scaled
# from (0, 255) to a range of (-1., +1.), the rescaling layer
# outputs: `(inputs * scale) + offset`
scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)
x = scale_layer(inputs)

# The base model contains batchnorm layers. We want to keep them in inference mode
# when we unfreeze the base model for fine-tuning, so we make sure that the
# base_model is running in inference mode here.
x = base_model(x, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.summary(show_trainable=True)

model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

epochs = 2
print(""Fitting the top layer of the model"")
model.fit(train_ds, epochs=epochs, validation_data=validation_ds)
```


### Relevant log output

```shell
2025-01-09 19:39:57.953074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736469597.967544   14431 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736469597.971752   14431 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 19:39:57.986195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
1 Physical GPUs, 1 Logical GPUs
I0000 00:00:1736469600.052169   14431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10240 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:0a:00.0, compute capability: 8.9
Fitting the top layer of the model
Epoch 1/2
2025-01-09 19:40:04.479339: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1736469604.583055   14486 service.cc:148] XLA service 0x7f8df8002230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1736469604.583109   14486 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9
2025-01-09 19:40:04.722034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1736469605.234339   14486 cuda_dnn.cc:529] Loaded cuDNN version 90300
  7/582  12s 22ms/step - binary_accuracy: 0.5658 - loss: 0.6950 
I0000 00:00:1736469608.599510   14486 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
243/582  16s 48ms/step - binary_accuracy: 0.8715 - loss: 0.2755
2025-01-09 19:40:20.475756: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 1073741824 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469620.475821   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 1073741824
2025-01-09 19:40:20.615636: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 966367744 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469620.615700   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 966367744
2025-01-09 19:40:20.769033: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 869731072 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469620.769095   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 869731072
2025-01-09 19:40:20.906909: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 782758144 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469620.906973   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 782758144
2025-01-09 19:40:21.048863: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 704482304 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.048940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 704482304
2025-01-09 19:40:21.229614: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 634034176 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.229682   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 634034176
2025-01-09 19:40:21.371940: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 570630912 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.372000   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 570630912
2025-01-09 19:40:21.510751: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 513568000 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.510817   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 513568000
2025-01-09 19:40:21.650945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 462211328 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.651034   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 462211328
2025-01-09 19:40:21.814945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 415990272 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.815035   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 415990272
2025-01-09 19:40:21.954790: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 374391296 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469621.954851   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 374391296
2025-01-09 19:40:22.094150: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 336952320 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469622.094219   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 336952320
2025-01-09 19:40:22.267664: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 303257088 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
...
2025-01-09 19:40:23.128022: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 161164032 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469623.128090   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 161164032
2025-01-09 19:40:23.296856: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 145047808 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory
W0000 00:00:1736469623.296940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 145047808
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```
",Catakang,2025-01-10 00:49:17+00:00,['Venkat6871'],2025-02-01 02:52:16+00:00,2025-02-01 02:02:30+00:00,https://github.com/tensorflow/tensorflow/issues/84523,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2582222814, 'issue_id': 2779041274, 'author': 'RabJon', 'body': 'I am facing exactly the same issue for a while now on two slightly different systems:\r\n\r\n|                              | System 1                   | System 2                   |\r\n|------------------------------|----------------------------|----------------------------|\r\n| TensorFlow Version           | 2.17.0                     | 2.18.0                     |\r\n| OS platform and distribution | Ubuntu 22.04.3 LTS on WSL2 | Ubuntu 22.04.3 LTS on WSL2 |\r\n| Python version               | 3.9.20                     | 3.11.5                     |\r\n| CUDA/cuDNN version           | 12.3 / 8                   | 12.5.1 / 9                 |\r\n| GPU model and memory         | NVIDIA Titan RTX 24GB      | NVIDIA GeForce RTX 3090 24 GB    |\r\n\r\n\r\nIt even shows the warnings when setting `os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""3""`.\r\n\r\nIn my case, my code runs through despite these error messages, but on the one hand they are annoying and also a bit worrying and on the other hand I have the feeling that they affect the execution time, because I often observe strange behaviour regarding the execution times.', 'created_at': datetime.datetime(2025, 1, 10, 9, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587177338, 'issue_id': 2779041274, 'author': 'Catakang', 'body': ""It does affect execution time and my code does not run to completion with the errors as I let it run for an hour to see what happened and it finished with a message saying '0 successful operations'. I don't know why I am running into this error as I am just trying to follow a tutorial on the [keras website](https://keras.io/guides/transfer_learning/#an-endtoend-example-finetuning-an-image-classification-model-on-a-cats-vs-dogs-dataset). I may not have a multi GPU setup to train a bunch of networks in an optimized fashion but surely 10gb on my 4070 should be plenty to run 2 epochs with a batch size of 10. This is ridiculous and from what I am reading from other GitHub issues, this has been an issue for years on certain systems that they have simply not fixed(if I understand everything right) and I really just want to figure out tensorflow for my science fair project."", 'created_at': datetime.datetime(2025, 1, 13, 13, 58, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587179990, 'issue_id': 2779041274, 'author': 'Catakang', 'body': ""I will mention I have tried growing the memory, I tried the malloc cuda async flag, I tried slowly reducing the batch size smaller and smaller, this really shouldn't be that complicated."", 'created_at': datetime.datetime(2025, 1, 13, 13, 59, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597502313, 'issue_id': 2779041274, 'author': 'Venkat6871', 'body': 'Hi **@Catakang** ,\nApologies for the delay, and thank you for raising your concern here. I attempted to run your code on Colab using the TensorFlow nightly version but encountered a different issue. I have attached a [gist](https://colab.sandbox.google.com/gist/Venkat6871/780bab676b9d6063acd87de0b2297620/84523_tf-nightly-v.ipynb) for your reviewcould you please check and let me know if I made any mistakes while executing your code?\nAdditionally, in your setup, consider disabling XLA to potentially reduce memory usage. Let us know if you are still encountering the same issue.\nThank you!', 'created_at': datetime.datetime(2025, 1, 17, 6, 5, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613719356, 'issue_id': 2779041274, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 25, 1, 56, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628686227, 'issue_id': 2779041274, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 1, 2, 2, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628686255, 'issue_id': 2779041274, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84523"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84523"">No</a>', 'created_at': datetime.datetime(2025, 2, 1, 2, 2, 32, tzinfo=datetime.timezone.utc)}]","RabJon on (2025-01-10 09:49:00 UTC): I am facing exactly the same issue for a while now on two slightly different systems:

|                              | System 1                   | System 2                   |
|------------------------------|----------------------------|----------------------------|
| TensorFlow Version           | 2.17.0                     | 2.18.0                     |
| OS platform and distribution | Ubuntu 22.04.3 LTS on WSL2 | Ubuntu 22.04.3 LTS on WSL2 |
| Python version               | 3.9.20                     | 3.11.5                     |
| CUDA/cuDNN version           | 12.3 / 8                   | 12.5.1 / 9                 |
| GPU model and memory         | NVIDIA Titan RTX 24GB      | NVIDIA GeForce RTX 3090 24 GB    |


It even shows the warnings when setting `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""`.

In my case, my code runs through despite these error messages, but on the one hand they are annoying and also a bit worrying and on the other hand I have the feeling that they affect the execution time, because I often observe strange behaviour regarding the execution times.

Catakang (Issue Creator) on (2025-01-13 13:58:17 UTC): It does affect execution time and my code does not run to completion with the errors as I let it run for an hour to see what happened and it finished with a message saying '0 successful operations'. I don't know why I am running into this error as I am just trying to follow a tutorial on the [keras website](https://keras.io/guides/transfer_learning/#an-endtoend-example-finetuning-an-image-classification-model-on-a-cats-vs-dogs-dataset). I may not have a multi GPU setup to train a bunch of networks in an optimized fashion but surely 10gb on my 4070 should be plenty to run 2 epochs with a batch size of 10. This is ridiculous and from what I am reading from other GitHub issues, this has been an issue for years on certain systems that they have simply not fixed(if I understand everything right) and I really just want to figure out tensorflow for my science fair project.

Catakang (Issue Creator) on (2025-01-13 13:59:24 UTC): I will mention I have tried growing the memory, I tried the malloc cuda async flag, I tried slowly reducing the batch size smaller and smaller, this really shouldn't be that complicated.

Venkat6871 (Assginee) on (2025-01-17 06:05:12 UTC): Hi **@Catakang** ,
Apologies for the delay, and thank you for raising your concern here. I attempted to run your code on Colab using the TensorFlow nightly version but encountered a different issue. I have attached a [gist](https://colab.sandbox.google.com/gist/Venkat6871/780bab676b9d6063acd87de0b2297620/84523_tf-nightly-v.ipynb) for your reviewcould you please check and let me know if I made any mistakes while executing your code?
Additionally, in your setup, consider disabling XLA to potentially reduce memory usage. Let us know if you are still encountering the same issue.
Thank you!

github-actions[bot] on (2025-01-25 01:56:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-01 02:02:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-02-01 02:02:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84523"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84523"">No</a>

"
2777503267,issue,closed,completed,Build iOS tensorflowLite error with iOS library,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

iOS 

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When i build tensorflow lite with library, i encounter error belowthe error showUndefined symbols for architecture arm64:
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::find(char, unsigned long) const"", 

### Standalone code to reproduce the issue

```shell
Undefined symbols for architecture arm64:
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::find(char, unsigned long) const"", referenced from:
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::rfind(char, unsigned long) const"", referenced from:
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::compare(unsigned long, unsigned long, char const*) const"", referenced from:
      l001 in libNeuralnet_a.a[32](TensorFlowLiteC.a)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>::compare(unsigned long, unsigned long, char const*, unsigned long) const"", referenced from:
```


### Relevant log output

_No response_",fighting300,2025-01-09 10:59:27+00:00,['gaikwadrahul8'],2025-01-31 01:59:55+00:00,2025-01-31 01:59:52+00:00,https://github.com/tensorflow/tensorflow/issues/84466,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.8', '')]","[{'comment_id': 2586598008, 'issue_id': 2777503267, 'author': 'gaikwadrahul8', 'body': ""Hi, @fighting300 \nI apologize for the delayed response, if possible could you please help us with exact steps which you followed before encountering mentioned error in the issue template or if you followed any official documentation please help us with it to replicate the same behavior from our end ?\n\n**EDIT :** I believe you're following this [official documentation](https://ai.google.dev/edge/litert/build/ios) and I see you mentioned TensorFlow version 2.8 so could you please give it try with latest version of TensorFlow and see is it working as expected or not ?\n\nIf issue still persists please let us know with error log for further investigation\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 13, 9, 29, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611383635, 'issue_id': 2777503267, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 24, 1, 59, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127406, 'issue_id': 2777503267, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127444, 'issue_id': 2777503267, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84466"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84466"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 1, 59, 53, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-13 09:29:18 UTC): Hi, @fighting300 
I apologize for the delayed response, if possible could you please help us with exact steps which you followed before encountering mentioned error in the issue template or if you followed any official documentation please help us with it to replicate the same behavior from our end ?

**EDIT :** I believe you're following this [official documentation](https://ai.google.dev/edge/litert/build/ios) and I see you mentioned TensorFlow version 2.8 so could you please give it try with latest version of TensorFlow and see is it working as expected or not ?

If issue still persists please let us know with error log for further investigation

Thank you for your cooperation and patience.

github-actions[bot] on (2025-01-24 01:59:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-31 01:59:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-31 01:59:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84466"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84466"">No</a>

"
2777280312,issue,open,,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Python version

Python 3.12

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

A100 80GB

### Current behavior?

Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. 
No any memory profile events or OP profiler, but only trace view.

### Standalone code to reproduce the issue

**tf_allreduce.py**
```python
import tensorflow as tf
from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2
from tensorflow.python.eager import context
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib

cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()
cluster = cluster_resolver.cluster_spec()
task_type = cluster_resolver.task_type
task_id = cluster_resolver.task_id

experimental_config = config_pb2.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False
)
config = config_pb2.ConfigProto(experimental=experimental_config)
config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'
server = tf.distribute.Server(cluster,
                              job_name=task_type,
                              task_index=task_id,
                              protocol=""grpc"", # ""grpc+verbs""
                              config=config)
run_options = config_pb2.RunOptions()

with tf.compat.v1.Session(target=server.target, config=config) as sess:
    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)
    init = tf.compat.v1.global_variables_initializer()
    sess.run(init)
    sess.run(tf.print([""tensor:"",tensor]))

    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')
    run_options.experimental.collective_graph_key = 6
    while True:
        sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options)
```

Run script to start server.
```bash
CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py&
CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py&
```

 use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.
```python
tf.profiler.experimental.client.trace(
  'grpc://localhost:2223,grpc://localhost:2224',
   '/tmp/my_tb_dir',
   2000,
)
```

Try to convert xplane.pb to memory_profile, nothing show.
```python
from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper
json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"")
```

**Relevant log output**
```
{""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

Relative issue: #48146 ",MoFHeka,2025-01-09 09:26:20+00:00,['tilakrayal'],2025-01-10 16:16:59+00:00,,https://github.com/tensorflow/tensorflow/issues/84460,"[('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]",[],
2776964764,issue,closed,completed, expect the compilation to pass but there are errors how to fix it,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

LINUX Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

6.1.0

### GCC/compiler version

9.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 expect the compilation to pass but there are errors

ERROR: /home/aist/.cache/bazel/_bazel_aist/572037a80443c6142f9def570fe3ea55/external/XNNPACK/BUILD.bazel:3050:19: Compiling src/tensor.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:subgraph) /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 98 arguments skipped)
In file included from external/XNNPACK/src/tensor.c:19:
external/XNNPACK/src/xnnpack/subgraph.h:431:17: error: array type has incomplete element type 'xnn_timestamp' {aka 'struct timespec'}
  431 |   xnn_timestamp end_ts[XNN_MAX_OPERATOR_OBJECTS];
      |                 ^~~~~~
external/XNNPACK/src/xnnpack/subgraph.h:475:17: error: field 'start_ts' has incomplete type
  475 |   xnn_timestamp start_ts;
      |                 ^~~~~~~~
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 969.727s, Critical Path: 11.17s
INFO: 2534 processes: 701 internal, 1833 local.
FAILED: Build did NOT complete successfully


### Standalone code to reproduce the issue

```shell
bazel build --config=opt //tensorflow:libtensorflow_cc.so
```


### Relevant log output

_No response_",15001366252,2025-01-09 06:47:58+00:00,['Venkat6871'],2025-01-09 08:49:15+00:00,2025-01-09 08:49:11+00:00,https://github.com/tensorflow/tensorflow/issues/84443,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2579472466, 'issue_id': 2776964764, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84443"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84443"">No</a>', 'created_at': datetime.datetime(2025, 1, 9, 8, 49, 14, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2025-01-09 08:49:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84443"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84443"">No</a>

"
2776569152,issue,open,,Unable to connect to TPU through Cloud VM (metadata issue?),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

tpu-ubuntu2204-base

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.

It seems like there is an issue with getting TPU metadata.

It is able to connect to the metadata server when I request manually from the VM:

```
$ curl http://169.254.169.254/computeMetadata/v1/ -H ""Metadata-Flavor: Google""
instance/
oslogin/
project/
```

Any help would be appreciated!

### Standalone code to reproduce the issue

```shell
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)
tf.config.experimental_connect_to_cluster(resolver)
try:
    tf.tpu.experimental.initialize_tpu_system(resolver)
    print(""TPU initialized:"", resolver.master())
except Exception as e:
    print(""Failed to initialize TPU:"", e)
```


### Relevant log output

```shell
$ python hello.py
2025-01-08 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-08 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
2025-01-08 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/libtpu/libtpu.so
Failed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.
Failed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

WARNING: Logging before InitGoogle() is written to STDERR
E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173
2025-01-08 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus:
:{""created"":""@1736380609.730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= 
*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***
PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)
    @     0x7f1caa302841       1888  (unknown)
    @     0x7f1ccaf0e050   18460496  (unknown)
    @     0x7f1ccaed1c60  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= 
E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.
E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.
E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.
E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.
E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior
Aborted
```
",nathom,2025-01-09 00:04:51+00:00,['tilakrayal'],2025-01-15 02:23:17+00:00,,https://github.com/tensorflow/tensorflow/issues/84413,"[('type:bug', 'Bug'), ('comp:tpus', 'tpu, tpuestimator'), ('TF 2.18', '')]","[{'comment_id': 2583126605, 'issue_id': 2776569152, 'author': 'tilakrayal', 'body': ""@nathom,\r\nCould you please provide more information and also steps you have followed to use the TPU's which helps to debug the issue in an effective way. Thank you!"", 'created_at': datetime.datetime(2025, 1, 10, 16, 15, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591499946, 'issue_id': 2776569152, 'author': 'nathom', 'body': 'I think this was my mistake. I was using a Google Cloud VM and trying to connect to the TPU pods from there. I was able to resolve the issue by connecting directly to one of the TPU hosts, and running commands on all workers using `gcloud`. Maybe the error messages could be made more helpful, though.', 'created_at': datetime.datetime(2025, 1, 15, 2, 23, 15, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-10 16:15:27 UTC): @nathom,
Could you please provide more information and also steps you have followed to use the TPU's which helps to debug the issue in an effective way. Thank you!

nathom (Issue Creator) on (2025-01-15 02:23:15 UTC): I think this was my mistake. I was using a Google Cloud VM and trying to connect to the TPU pods from there. I was able to resolve the issue by connecting directly to one of the TPU hosts, and running commands on all workers using `gcloud`. Maybe the error messages could be made more helpful, though.

"
2776565648,issue,closed,completed,bug,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",rizkyy702,2025-01-09 00:02:56+00:00,['Venkat6871'],2025-01-09 13:56:30+00:00,2025-01-09 13:56:20+00:00,https://github.com/tensorflow/tensorflow/issues/84412,"[('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2580254337, 'issue_id': 2776565648, 'author': 'mihaimaruseac', 'body': 'Nothing provided in the template.', 'created_at': datetime.datetime(2025, 1, 9, 13, 56, 20, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-09 13:56:20 UTC): Nothing provided in the template.

"
2772809922,issue,open,,dictionaries in fit method of model load data in wrong order,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17; tf 2.18

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the code is running in google collab.
The code below is an example of a model with multiple inputs and multiple outputs.
NOT working code with using **dictionaries** in method **fit** of model.

the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd


### Standalone code to reproduce the issue

```shell
# collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
# gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd

# fast code
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

vocabulary_size = 10000
num_tags = 100
num_departments = 4

# define three model inputs
title = keras.Input(shape=(vocabulary_size,), name=""title"")
text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"")
tags = keras.Input(shape=(num_tags,), name=""tags"")

features = layers.Concatenate()([title, text_body, tags])
# one intermediate layer
features = layers.Dense(64, activation=""relu"")(features)

# Define two model outputs
priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)
department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)

# set the model
model = keras.Model(inputs=[title, text_body, tags],
                    outputs=[priority, department])
# prepare data
num_samples = 1280
# The data is filled in with zeros and ones
title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

# priority: [0., 1.]
priority_data = np.random.random(size=(num_samples, 1))
# class of 4 labels
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

# compile model
model.compile(optimizer=""rmsprop"",
              loss={""priority"": ""mean_squared_error"",
                    ""department"": ""categorical_crossentropy""},
              metrics={""priority"": [""mean_absolute_error""],
                       ""department"": [""accuracy""]})

# It doesn't matter how the model is compiled
# model.compile(optimizer=""rmsprop"",
#               loss=[""mean_squared_error"", ""categorical_crossentropy""],
#               metrics=[[""mean_absolute_error""], [""accuracy""]])


# NOT WORKING
# TRAIN MODEL WITH transferring the DICTIONARY to the method
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
)

# WORK
# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1
)

# ALSO WORK
# TRAIN MODEL WITH transferring the DICTIONARY to the method
# REPLACE priority and department
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": department_data, ""department"": priority_data},
          epochs=1
)
```


### Relevant log output

_No response_",moprules,2025-01-07 13:08:06+00:00,['tilakrayal'],2025-02-08 01:58:26+00:00,,https://github.com/tensorflow/tensorflow/issues/84278,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2580119954, 'issue_id': 2772809922, 'author': 'tilakrayal', 'body': '@moprules,\r\nLooks like this issue is more related to Keras. Could you please raise the issue in keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2025, 1, 9, 13, 10, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594736529, 'issue_id': 2772809922, 'author': 'SanjaySG', 'body': '@tilakrayal I agree that this is an issue with Keras, but it might not warrant an immediate fix.\n\n@moprules \nLet me try to explain why this is happening. As confusing as this is, I think it is by design. \n\nWhen you initialize your model using the following code, the functional API is used to create the model graph. The graph has two outputs, the first is priority with an output shape of (1) and the second is department with a shape of (4). Since a list is passed to the outputs argument, the order of the outputs are preserved.  \n```\n# Define two model outputs\npriority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)\ndepartment = layers.Dense(4, activation=""softmax"", name=""department"")(features)\n\n# set the model\nmodel = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])\n```\n\nSo the prediction `y_pred` generated by the model would be of the shape `[<Tensor shape=(, 1)>, <Tensor shape=(, 4)>]`.\n\nAfterwards, when you call model.fit() with a **dictionary** as the y argument, the behavior is slightly different.\n```\nmodel.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},\n          {""priority"": priority_data, ""department"": department_data},\n          epochs=1\n```\n\nThe dictionary values get converted to Tensors and the dictionary itself is eventually **flattened into a list using [tree.flatten()](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/tree/tree_api.py#L91-L121), which sorts the flattened elements by key**. [The actual tree.flatten() call happens in [keras/src/trainers/compile_utils.py](https://github.com/keras-team/keras/blob/master/keras/src/trainers/compile_utils.py)]. \n\nSo the label `y_true` goes from a dict to a list with the order of the elements reversed:\n`{\'priority\': <Tensor shape = (,1) name=\'priority_data\'>, \'department\': <Tensor shape = (,4) name=\'department_data\'>} ` to `[<Tensor shape = (,4) name=\'department_data\'>, <Tensor shape = (,1) name=\'priority_data\'>] `. \n\nThis flattening step for a dict type output is opaque to us and is the source of the confusion. Now the y_pred and y_true lists have different shapes in the same position and leads to the error message in your gist: \n```ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(32, 4), output.shape=(32, 1)```. \nThis is also why interchanging department_data and priority_data in your last example works, as the correct shapes are matched after the flatten operation.\n\n--- \n\nThe details of what happens to list vs dictionary objects in the model is not clear and the documentation doesn\'t state it clearly. So this could definitely be improved.\n\nOne way to avoid pitfalls like these is to be consistent with the format of the inputs/outputs i.e. pass all lists or all dicts.\n\nFor example, the following code would work:\n```\n...\n# set the model\nmodel = keras.Model(inputs={\'title\':title, \'text_body\':text_body, \'tags\':tags},\n                   outputs={\'priority\':priority, \'department\':department})\n...\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\nmodel.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},\n          {""priority"": priority_data, ""department"": department_data},\n          epochs=1\n..\n```\n\nI hope this explanation helps. Let me know if you have any questions.', 'created_at': datetime.datetime(2025, 1, 16, 7, 42, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425437, 'issue_id': 2772809922, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 25, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-09 13:10:55 UTC): @moprules,
Looks like this issue is more related to Keras. Could you please raise the issue in keras-team/keras repo for the quick resolution. Thank you!

SanjaySG on (2025-01-16 07:42:01 UTC): @tilakrayal I agree that this is an issue with Keras, but it might not warrant an immediate fix.

@moprules 
Let me try to explain why this is happening. As confusing as this is, I think it is by design. 

When you initialize your model using the following code, the functional API is used to create the model graph. The graph has two outputs, the first is priority with an output shape of (1) and the second is department with a shape of (4). Since a list is passed to the outputs argument, the order of the outputs are preserved.  
```
# Define two model outputs
priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)
department = layers.Dense(4, activation=""softmax"", name=""department"")(features)

# set the model
model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])
```

So the prediction `y_pred` generated by the model would be of the shape `[<Tensor shape=(, 1)>, <Tensor shape=(, 4)>]`.

Afterwards, when you call model.fit() with a **dictionary** as the y argument, the behavior is slightly different.
```
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
```

The dictionary values get converted to Tensors and the dictionary itself is eventually **flattened into a list using [tree.flatten()](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/tree/tree_api.py#L91-L121), which sorts the flattened elements by key**. [The actual tree.flatten() call happens in [keras/src/trainers/compile_utils.py](https://github.com/keras-team/keras/blob/master/keras/src/trainers/compile_utils.py)]. 

So the label `y_true` goes from a dict to a list with the order of the elements reversed:
`{'priority': <Tensor shape = (,1) name='priority_data'>, 'department': <Tensor shape = (,4) name='department_data'>} ` to `[<Tensor shape = (,4) name='department_data'>, <Tensor shape = (,1) name='priority_data'>] `. 

This flattening step for a dict type output is opaque to us and is the source of the confusion. Now the y_pred and y_true lists have different shapes in the same position and leads to the error message in your gist: 
```ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(32, 4), output.shape=(32, 1)```. 
This is also why interchanging department_data and priority_data in your last example works, as the correct shapes are matched after the flatten operation.

--- 

The details of what happens to list vs dictionary objects in the model is not clear and the documentation doesn't state it clearly. So this could definitely be improved.

One way to avoid pitfalls like these is to be consistent with the format of the inputs/outputs i.e. pass all lists or all dicts.

For example, the following code would work:
```
...
# set the model
model = keras.Model(inputs={'title':title, 'text_body':text_body, 'tags':tags},
                   outputs={'priority':priority, 'department':department})
...
# TRAIN MODEL WITH transferring the DICTIONARY to the method
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
..
```

I hope this explanation helps. Let me know if you have any questions.

github-actions[bot] on (2025-02-08 01:58:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2772483635,issue,open,,keras model.save does not respect `include_optimizer=False`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20250105

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing
```


### Relevant log output

_No response_",j-lootens,2025-01-07 10:33:38+00:00,['Venkat6871'],2025-02-03 12:54:47+00:00,,https://github.com/tensorflow/tensorflow/issues/84268,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2579228270, 'issue_id': 2772483635, 'author': 'Venkat6871', 'body': 'Hi **@j-lootens** ,\r\nApologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, from tensorflow import keras ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) defaults to Keras 3. To use Keras 2, you need to install it using the following command:\r\n```\r\n!pip install tf-keras\r\n```\r\nThen, import it as follows:\r\n```\r\nimport tf_keras as keras\r\n```\r\nI tried this, and it is working fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/0849288c1e9a99facb8aec4651c9dafd/84268_tf-nightly-v.ipynb) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 9, 5, 47, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579363675, 'issue_id': 2772483635, 'author': 'j-lootens', 'body': 'Thanks for the response. This does indeed work with the example provided. \r\n\r\nHowever, I am using this function by calling [mlflow.tensorflow.log_model](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.log_model), which defaults to the newer keras. \r\n\r\nAdditionally, I had quite some issues with deserializing the model if changes have been made to the custom layers/model code with the `.h5` format so would prefer to use the `.keras`format, if possible.', 'created_at': datetime.datetime(2025, 1, 9, 7, 46, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630355822, 'issue_id': 2772483635, 'author': 'Venkat6871', 'body': 'Hi **@j-lootens** ,\nApologies for the delay, and thank you for your patience. Starting from TensorFlow 2.16.0, the .keras format is required for saving and loading models. For your reference, here is the official [documentation](https://keras.io/guides/migrating_to_keras_3/).\nThank you!', 'created_at': datetime.datetime(2025, 2, 3, 9, 7, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630883879, 'issue_id': 2772483635, 'author': 'j-lootens', 'body': 'Hi @Venkat6871 \nI am trying to use the .keras format in  tf v2.18 (or newer) and here I run into the bug described in the issue where the `include_optimizer=False` option is ignored.', 'created_at': datetime.datetime(2025, 2, 3, 12, 54, 44, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-09 05:47:06 UTC): Hi **@j-lootens** ,
Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, from tensorflow import keras ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) defaults to Keras 3. To use Keras 2, you need to install it using the following command:
```
!pip install tf-keras
```
Then, import it as follows:
```
import tf_keras as keras
```
I tried this, and it is working fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/0849288c1e9a99facb8aec4651c9dafd/84268_tf-nightly-v.ipynb) for your reference.
Thank you!

j-lootens (Issue Creator) on (2025-01-09 07:46:09 UTC): Thanks for the response. This does indeed work with the example provided. 

However, I am using this function by calling [mlflow.tensorflow.log_model](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.log_model), which defaults to the newer keras. 

Additionally, I had quite some issues with deserializing the model if changes have been made to the custom layers/model code with the `.h5` format so would prefer to use the `.keras`format, if possible.

Venkat6871 (Assginee) on (2025-02-03 09:07:32 UTC): Hi **@j-lootens** ,
Apologies for the delay, and thank you for your patience. Starting from TensorFlow 2.16.0, the .keras format is required for saving and loading models. For your reference, here is the official [documentation](https://keras.io/guides/migrating_to_keras_3/).
Thank you!

j-lootens (Issue Creator) on (2025-02-03 12:54:44 UTC): Hi @Venkat6871 
I am trying to use the .keras format in  tf v2.18 (or newer) and here I run into the bug described in the issue where the `include_optimizer=False` option is ignored.

"
2770520809,issue,closed,completed,[XLA] can't compile the tf.keras.layers.Conv2D when padding='valid',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA can't compile the `tf.keras.layers.Conv2D` when `padding='valid'`. However, eager can pass the check.
There exists a misalignment

### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
tf.keras.utils.set_random_seed(42)
tf.random.set_seed(42)

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""


x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], dtype=tf.float32)
inputs = [x]



class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')

    def call(self, x):
        x = tf.reshape(x, [1, 3, 3, 1])
        x = self.conv(x)
        return x


model = Model()
model(*inputs)
print(""succeed on eager"")



class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')

    @tf.function(jit_compile=True)
    def call(self, x):
        x = tf.reshape(x, [1, 3, 3, 1])
        x = self.conv(x)
        return x


model = Model()
model(*inputs)
print(""succeed on XLA"")
```


### Relevant log output

```shell
succeed on eager
Negative dimension size caused by subtracting 4 from 3 for '{{node conv2d_1_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Reshape, conv2d_1_1/convolution/ReadVariableOp)' with input shapes: [1,3,3,1], [4,4,1,1].
```
",shaoyuyoung,2025-01-06 12:07:56+00:00,['tilakrayal'],2025-01-23 01:59:30+00:00,2025-01-23 01:59:27+00:00,https://github.com/tensorflow/tensorflow/issues/84205,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:xla', 'XLA'), ('TF 2.18', '')]","[{'comment_id': 2576954330, 'issue_id': 2770520809, 'author': 'tilakrayal', 'body': '@shaoyuyoung,\r\nI was able to reproduce the issue on TensorFlow v2.17, v2.18 and tf-nightly. Kindly find the gist of it here. Also Could you please confirm whether it was working with the versions which are older than the Tensorflow 2.16?  From v2.16, the TensorFlow contains Keras3.0 and the previous version contains Keras2.0. Thank you!', 'created_at': datetime.datetime(2025, 1, 8, 7, 44, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576959731, 'issue_id': 2770520809, 'author': 'shaoyuyoung', 'body': 'Thank you for your confirmation!\r\n\r\n\r\n> Also Could you please confirm whether it was working with the versions which are older than the Tensorflow 2.16?\r\n\r\nI will do this in my spare time :)', 'created_at': datetime.datetime(2025, 1, 8, 7, 47, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594309759, 'issue_id': 2770520809, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 16, 1, 58, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608685185, 'issue_id': 2770520809, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 23, 1, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608685226, 'issue_id': 2770520809, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84205"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84205"">No</a>', 'created_at': datetime.datetime(2025, 1, 23, 1, 59, 29, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-08 07:44:03 UTC): @shaoyuyoung,
I was able to reproduce the issue on TensorFlow v2.17, v2.18 and tf-nightly. Kindly find the gist of it here. Also Could you please confirm whether it was working with the versions which are older than the Tensorflow 2.16?  From v2.16, the TensorFlow contains Keras3.0 and the previous version contains Keras2.0. Thank you!

shaoyuyoung (Issue Creator) on (2025-01-08 07:47:09 UTC): Thank you for your confirmation!



I will do this in my spare time :)

github-actions[bot] on (2025-01-16 01:58:54 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-23 01:59:27 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-23 01:59:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84205"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84205"">No</a>

"
2770381009,issue,open,,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5
- TensorFlow version (or github SHA if from source): 2.18.0


**Provide the text output from tflite_convert**
In colab version, tflite_convert doesn't log anything, below log is in my local version
```
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-01-06 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x
2025-01-06 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-01-06 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x
2025-01-06 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-01-06 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x
2025-01-06 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.
2025-01-06 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice
Details:
	tf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2025-01-06 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):
Custom ops: XlaDynamicSlice
Details:
	tf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_custom
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",phandat128,2025-01-06 10:46:54+00:00,['gaikwadrahul8'],2025-02-07 08:11:30+00:00,,https://github.com/tensorflow/tensorflow/issues/84203,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.18', '')]","[{'comment_id': 2591825158, 'issue_id': 2770381009, 'author': 'gaikwadrahul8', 'body': ""Hi, @phandat128 \nI apologize for the delayed response, I tried to replicate the same behavior from my end with your Google colab [notebook](https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing) and I'm also getting the same error message `RuntimeError: Encountered unresolved custom op: XlaDynamicSlice.` for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/08fcff86560ed5f7c0d704949270ade4/tflite-issue-84203.ipynb) so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 15, 7, 27, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642205328, 'issue_id': 2770381009, 'author': 'gaikwadrahul8', 'body': ""Hi, @phandat128 \nI apologize for the delayed response, I see in provided output log it says : `The following operation(s) need TFLite custom op implementation(s):Custom ops: XlaDynamicSlice` so it's unsupported ops since the LiteRT( Formerly knowns as TFLite) builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to [operator compatibility](https://ai.google.dev/edge/litert/models/ops_compatibility.md).\n\nTo allow conversion, you'll have to provide their own custom implementation of an unsupported TensorFlow operator in LiteRT, known as a custom operator in your case `XlaDynamicSlice` Op for more details please refer this [official documentation](https://ai.google.dev/edge/litert/models/ops_custom)\n\nIf it's not mandatory to use T5 model in your use case or project then you can give it try with other models which can fulfill your use-case/project need the alternatives to the T5 model, some prominent options include: GPT-3 (and its variants like GPT-3.5 and GPT-4), BERT, RoBERTa, XLNet, Flan-T5 (an enhanced version of T5)\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 2, 7, 8, 9, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-15 07:27:58 UTC): Hi, @phandat128 
I apologize for the delayed response, I tried to replicate the same behavior from my end with your Google colab [notebook](https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing) and I'm also getting the same error message `RuntimeError: Encountered unresolved custom op: XlaDynamicSlice.` for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/08fcff86560ed5f7c0d704949270ade4/tflite-issue-84203.ipynb) so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-02-07 08:09:00 UTC): Hi, @phandat128 
I apologize for the delayed response, I see in provided output log it says : `The following operation(s) need TFLite custom op implementation(s):Custom ops: XlaDynamicSlice` so it's unsupported ops since the LiteRT( Formerly knowns as TFLite) builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to [operator compatibility](https://ai.google.dev/edge/litert/models/ops_compatibility.md).

To allow conversion, you'll have to provide their own custom implementation of an unsupported TensorFlow operator in LiteRT, known as a custom operator in your case `XlaDynamicSlice` Op for more details please refer this [official documentation](https://ai.google.dev/edge/litert/models/ops_custom)

If it's not mandatory to use T5 model in your use case or project then you can give it try with other models which can fulfill your use-case/project need the alternatives to the T5 model, some prominent options include: GPT-3 (and its variants like GPT-3.5 and GPT-4), BERT, RoBERTa, XLNet, Flan-T5 (an enhanced version of T5)

Thank you for your cooperation and patience.

"
2769442620,issue,open,,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d,"### 1. System information

- OS Platform and Distribution: Linux Mint 6.2.9
- TensorFlow installation: pip
- TensorFlow library: 2.18.0 (latest)

### 2. Code

Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.
The MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms#for_example)

```
import tensorflow as tf

class MFCCLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MFCCLayer, self).__init__(**kwargs)

    def call(self, pcm):
        # A 1024-point STFT with frames of 64 ms and 75% overlap.
        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)
        spectrograms = tf.abs(stfts)

        # Warp the linear scale spectrograms into the mel-scale.
        num_spectrogram_bins = stfts.shape[-1]
        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
            num_mel_bins,
            num_spectrogram_bins,
            sample_rate,
            lower_edge_hertz,
            upper_edge_hertz,
        )
        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)
        mel_spectrograms.set_shape(
            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])
        )

        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

        # Compute MFCCs from log_mel_spectrograms and take the first 13.
        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[
            ..., :13
        ]
        print(""mfccs.shape: "", mfccs.shape)
        return mfccs


def build_model(input_shape):
    input_layer = tf.keras.layers.Input(shape=input_shape)
    output_layer = MFCCLayer()(input_layer)
    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)


if __name__ == ""__main__"":
    batch_size, num_samples, sample_rate = 32, 32000, 16000.0
    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)
    print(""pcm.shape: "", pcm.shape)

    model = build_model(pcm.shape)
    model.summary()

    # Convert to TensorFlow Lite and Save
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    with open(""mfcc.tflite"", ""wb"") as f:
        f.write(tflite_model)

    # Load the model and run inference
    with open(""mfcc.tflite"", ""rb"") as f:
        tflite_model = f.read()

    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension

    interpreter.set_tensor(input_details[0][""index""], pcm)
    interpreter.invoke()  # <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.
    mfccs = interpreter.get_tensor(output_details[0][""index""])
    print(""mfccs.shape: "", mfccs.shape)
```


### 3. Failure after conversion
As far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?

I am unsure if this is just a user error from myself or this is a bug.
I couldn't find any info online, hence i ask here.

Is a MFCC-calculation model possible with TFlite?

Thanks for all help

",maxx-st,2025-01-05 20:45:45+00:00,['gaikwadrahul8'],2025-02-07 02:01:35+00:00,,https://github.com/tensorflow/tensorflow/issues/84171,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.18', '')]","[{'comment_id': 2579305058, 'issue_id': 2769442620, 'author': 'gaikwadrahul8', 'body': ""Hi, @maxx-st \r\nI apologize for the delayed response, I tried your code snippet from my end and I'm also getting same error message `# <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.` so for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ab58cf9d8b51f4a49d152b8308748703/tflite-issue-84171.ipynb),  we need to dig more into this issue and will update you, thank you for bringing this issue to our attention.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 9, 7, 0, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625496377, 'issue_id': 2769442620, 'author': 'gaikwadrahul8', 'body': ""Hi, @maxx-st \nI apologize for the delayed response, I see [Supported Select TensorFlow operators](https://ai.google.dev/edge/litert/models/op_select_allowlist.md) does not support **STFT** so it's causing the error during TFLite conversion process as far I know you can precompute MFCCs outside TFLite using libraries like [Librosa](https://github.com/librosa/librosa) or [Torchaudio](https://github.com/pytorch/audio) then feed the MFCCs into a TFLite model\n\nTFLite builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to [operator compatibility](https://ai.google.dev/edge/litert/models/ops_compatibility.md). To allow conversion, users can enable the usage of [certain TensorFlow ops](https://ai.google.dev/edge/litert/models/op_select_allowlist.md) in their TFLite model.\n\nPlease refer this comment in TFLM issue https://github.com/tensorflow/tflite-micro/issues/2676#issuecomment-2337246053\n\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 30, 20, 20, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641750408, 'issue_id': 2769442620, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 7, 2, 1, 33, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-09 07:00:37 UTC): Hi, @maxx-st 
I apologize for the delayed response, I tried your code snippet from my end and I'm also getting same error message `# <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.` so for reference here is [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/ab58cf9d8b51f4a49d152b8308748703/tflite-issue-84171.ipynb),  we need to dig more into this issue and will update you, thank you for bringing this issue to our attention.

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-01-30 20:20:58 UTC): Hi, @maxx-st 
I apologize for the delayed response, I see [Supported Select TensorFlow operators](https://ai.google.dev/edge/litert/models/op_select_allowlist.md) does not support **STFT** so it's causing the error during TFLite conversion process as far I know you can precompute MFCCs outside TFLite using libraries like [Librosa](https://github.com/librosa/librosa) or [Torchaudio](https://github.com/pytorch/audio) then feed the MFCCs into a TFLite model

TFLite builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to [operator compatibility](https://ai.google.dev/edge/litert/models/ops_compatibility.md). To allow conversion, users can enable the usage of [certain TensorFlow ops](https://ai.google.dev/edge/litert/models/op_select_allowlist.md) in their TFLite model.

Please refer this comment in TFLM issue https://github.com/tensorflow/tflite-micro/issues/2676#issuecomment-2337246053

Thank you for your cooperation and patience.

github-actions[bot] on (2025-02-07 02:01:33 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2769372272,issue,open,,Broken compatibility with tensorflow-metal in 2.18,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

MacOS 15.2

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2 Max GPU 38-cores

### Current behavior?

Apple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0

This is normal output:
```
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

Process finished with exit code 0
```

But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

if __name__ == '__main__':
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(gpus)
```


### Relevant log output

```shell
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
Traceback (most recent call last):
  File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/__init__.py"", line 437, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Process finished with exit code 1
```
",MSPanchenko,2025-01-05 17:26:17+00:00,['Venkat6871'],2025-02-03 06:23:11+00:00,,https://github.com/tensorflow/tensorflow/issues/84167,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2573885386, 'issue_id': 2769372272, 'author': 'mezza', 'body': 'Ditto. Was experimenting with `uv` and wasted an hour or so before reverting to `pyenv` setup. This failed to work with tensorflow 2.18 and tensorflow-metal 1.1.0 but worked with tensorflow 2.17', 'created_at': datetime.datetime(2025, 1, 6, 20, 38, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579240693, 'issue_id': 2769372272, 'author': 'Venkat6871', 'body': 'Hi **@MSPanchenko** ,\r\nApologies for the delay, and thank you for raising your concern here. Unfortunately, we do not support the tensorflow-metal plugin at the moment. However, as a workaround, I tried it with TensorFlow 2.16.2, and it worked fine for me. Im attaching a screenshot for your reference.\r\n<img width=""1728"" alt=""Screenshot 2025-01-09 at 9 42 48\u202fAM"" src=""https://github.com/user-attachments/assets/680abba1-9746-46b8-b4b1-893a57e813b2"" />\r\n\r\nI would request you to please post your issue [here](https://developer.apple.com/forums/) a faster resolution.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 9, 6, 0, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580919805, 'issue_id': 2769372272, 'author': 'MSPanchenko', 'body': ""I've created a bug, let's wait \r\nhttps://developer.apple.com/forums/thread/772147"", 'created_at': datetime.datetime(2025, 1, 9, 17, 49, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630073263, 'issue_id': 2769372272, 'author': 'Venkat6871', 'body': 'Hi **@MSPanchenko** ,\nThanks for raising your concern in that particular repo. Please check there for further updates.\nThank you!', 'created_at': datetime.datetime(2025, 2, 3, 6, 22, 57, tzinfo=datetime.timezone.utc)}]","mezza on (2025-01-06 20:38:40 UTC): Ditto. Was experimenting with `uv` and wasted an hour or so before reverting to `pyenv` setup. This failed to work with tensorflow 2.18 and tensorflow-metal 1.1.0 but worked with tensorflow 2.17

Venkat6871 (Assginee) on (2025-01-09 06:00:11 UTC): Hi **@MSPanchenko** ,
Apologies for the delay, and thank you for raising your concern here. Unfortunately, we do not support the tensorflow-metal plugin at the moment. However, as a workaround, I tried it with TensorFlow 2.16.2, and it worked fine for me. Im attaching a screenshot for your reference.
<img width=""1728"" alt=""Screenshot 2025-01-09 at 9 42 48AM"" src=""https://github.com/user-attachments/assets/680abba1-9746-46b8-b4b1-893a57e813b2"" />

I would request you to please post your issue [here](https://developer.apple.com/forums/) a faster resolution.
Thank you!

MSPanchenko (Issue Creator) on (2025-01-09 17:49:51 UTC): I've created a bug, let's wait 
https://developer.apple.com/forums/thread/772147

Venkat6871 (Assginee) on (2025-02-03 06:22:57 UTC): Hi **@MSPanchenko** ,
Thanks for raising your concern in that particular repo. Please check there for further updates.
Thank you!

"
2769372187,issue,open,,How can I use local CUDA instead of hermetic CUDA to build? ,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Configures about hermetic CUDA are confusing. It seems to bring extra dependencies into the output binary. I want to build a tf-lib depends on local CUDA libs.

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_",PeterRK,2025-01-05 17:26:06+00:00,['tilakrayal'],2025-01-08 02:18:24+00:00,,https://github.com/tensorflow/tensorflow/issues/84166,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2575423890, 'issue_id': 2769372187, 'author': 'tilakrayal', 'body': '@PeterRK,\r\nCould you please provide the specific use-case for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations.\r\n\r\nhttps://www.tensorflow.org/install/source#gpu\r\n\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 7, 14, 27, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576572015, 'issue_id': 2769372187, 'author': 'PeterRK', 'body': '> @PeterRK, Could you please provide the specific use-case for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations.\r\n> \r\n> https://www.tensorflow.org/install/source#gpu\r\n> \r\n> Thank you!\r\n\r\nMy local CUDA version is newer than the tested one for TF-2.18, but TF-2.16 can be built and work well with it. I think the real problem is output binary links to hermetic CUDA libs and fails to work with local CUDA libs. Is there any way to build TF-2.18 without hermetic CUDA just like building old version TF? @tilakrayal', 'created_at': datetime.datetime(2025, 1, 8, 2, 18, 22, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-07 14:27:51 UTC): @PeterRK,
Could you please provide the specific use-case for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations.

https://www.tensorflow.org/install/source#gpu

Thank you!

PeterRK (Issue Creator) on (2025-01-08 02:18:22 UTC): My local CUDA version is newer than the tested one for TF-2.18, but TF-2.16 can be built and work well with it. I think the real problem is output binary links to hermetic CUDA libs and fails to work with local CUDA libs. Is there any way to build TF-2.18 without hermetic CUDA just like building old version TF? @tilakrayal

"
2768073649,issue,closed,completed,Tensortflow import issue after installation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I resintalled Python and my Anaconda environment and reinstalled using pip from notebook.

Please see attached installation log and then import logs

### Standalone code to reproduce the issue

```shell
pip install tensorflow

Collecting tensorflow
  Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)
Collecting tensorflow-intel==2.18.0 (from tensorflow)
  Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)
Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)
Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)
Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)
Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)
Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)
Requirement already satisfied: packaging in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)
Requirement already satisfied: requests<3,>=2.21.0 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)
Requirement already satisfied: setuptools in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)
Requirement already satisfied: six>=1.12.0 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)
Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)
Requirement already satisfied: typing-extensions>=3.6.6 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)
Requirement already satisfied: wrapt>=1.11.0 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached grpcio-1.68.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)
Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)
Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)
Requirement already satisfied: h5py>=3.11.0 in c:\users\dhima\anaconda3\lib\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)
Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)
  Using cached ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)
Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\users\dhima\anaconda3\lib\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)
Requirement already satisfied: rich in c:\users\dhima\anaconda3\lib\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)
Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)
  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)
Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)
  Using cached optree-0.13.1-cp312-cp312-win_amd64.whl.metadata (48 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\dhima\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\dhima\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\dhima\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\dhima\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)
Requirement already satisfied: markdown>=2.6.8 in c:\users\dhima\anaconda3\lib\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)
  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)
Requirement already satisfied: werkzeug>=1.0.1 in c:\users\dhima\anaconda3\lib\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)
Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\dhima\anaconda3\lib\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in c:\users\dhima\anaconda3\lib\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\users\dhima\anaconda3\lib\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)
Requirement already satisfied: mdurl~=0.1 in c:\users\dhima\anaconda3\lib\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)
Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)
Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)
Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)
Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Using cached flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)
Using cached gast-0.6.0-py3-none-any.whl (21 kB)
Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Using cached grpcio-1.68.1-cp312-cp312-win_amd64.whl (4.4 MB)
Using cached keras-3.7.0-py3-none-any.whl (1.2 MB)
Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)
Using cached ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)
Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)
Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)
Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)
Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)
Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)
Using cached optree-0.13.1-cp312-cp312-win_amd64.whl (292 kB)
Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow
Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 keras-3.7.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0
```


### Relevant log output

```shell
import tensorflow as tf

O/p
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[2], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\dhima\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",dnmaster1,2025-01-03 19:35:37+00:00,['Venkat6871'],2025-01-06 13:18:43+00:00,2025-01-05 14:37:30+00:00,https://github.com/tensorflow/tensorflow/issues/84119,"[('type:bug', 'Bug')]","[{'comment_id': 2571259338, 'issue_id': 2768073649, 'author': 'manojvn2612', 'body': 'check python version ,I think it need 3.10 version because it was giving the similar type of error in 3.12\r\nif still error persists try with gpu', 'created_at': datetime.datetime(2025, 1, 4, 11, 21, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571413898, 'issue_id': 2768073649, 'author': 'dnmaster1', 'body': ""Hi Manoj - I tried 3.10 ad 3.12. I don't believe they are now available for\r\ndownload. I also tried tensorflow-cpu, but it didn't help.\r\n\r\nWhere do you find tensorflow-gpu? I don't have a GPU, but my CPU is ARM\r\narch. Thanks\r\nDhimant\r\n\r\nOn Sat, Jan 4, 2025 at 6:22\u202fAM Manoj Nayak ***@***.***> wrote:\r\n\r\n> check python version ,I think it need 3.10 version because it was giving\r\n> the similar type of error in 3.12\r\n> if still error persists try with gpu\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84119#issuecomment-2571259338>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ5YXCCGGJS2T2IU4IGT2I675TAVCNFSM6AAAAABUSHKZYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZRGI2TSMZTHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 4, 21, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571648088, 'issue_id': 2768073649, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. Please only open new issues if there is information (like your CPU specs) that make your problem different than the existing one.', 'created_at': datetime.datetime(2025, 1, 5, 14, 37, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571648097, 'issue_id': 2768073649, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84119"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84119"">No</a>', 'created_at': datetime.datetime(2025, 1, 5, 14, 37, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571680693, 'issue_id': 2768073649, 'author': 'dnmaster1', 'body': 'I did search. You closed my previous issue.\r\n\r\nOn Sun, Jan 5, 2025 at 9:37\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Duplicate of #19584\r\n> <https://github.com/tensorflow/tensorflow/issues/19584>. Please do a\r\n> search before opening new issues. Please only open new issues if there is\r\n> information (like your CPU specs) that make your problem different than the\r\n> existing one.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84119#issuecomment-2571648088>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ5ZHP7HZFBR75M5EAW32JE7UFAVCNFSM6AAAAABUSHKZYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZRGY2DQMBYHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 5, 16, 29, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571680794, 'issue_id': 2768073649, 'author': 'dnmaster1', 'body': 'Please reopen. This is not closed.\r\n\r\nOn Sun, Jan 5, 2025 at 9:37\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Closed #84119 <https://github.com/tensorflow/tensorflow/issues/84119> as\r\n> completed.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84119#event-15817564143>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ55MYCPK3XWSP3DFHF32JE7UJAVCNFSM6AAAAABUSHKZYKVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJVHAYTONJWGQYTIMY>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 5, 16, 29, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573095510, 'issue_id': 2768073649, 'author': 'mihaimaruseac', 'body': ""Opening multiple issues can be considered as spam. Let's move discussion to just one issue."", 'created_at': datetime.datetime(2025, 1, 6, 13, 18, 42, tzinfo=datetime.timezone.utc)}]","manojvn2612 on (2025-01-04 11:21:36 UTC): check python version ,I think it need 3.10 version because it was giving the similar type of error in 3.12
if still error persists try with gpu

dnmaster1 (Issue Creator) on (2025-01-04 21:17:18 UTC): Hi Manoj - I tried 3.10 ad 3.12. I don't believe they are now available for
download. I also tried tensorflow-cpu, but it didn't help.

Where do you find tensorflow-gpu? I don't have a GPU, but my CPU is ARM
arch. Thanks
Dhimant

On Sat, Jan 4, 2025 at 6:22AM Manoj Nayak ***@***.***> wrote:

mihaimaruseac on (2025-01-05 14:37:30 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. Please only open new issues if there is information (like your CPU specs) that make your problem different than the existing one.

google-ml-butler[bot] on (2025-01-05 14:37:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84119"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84119"">No</a>

dnmaster1 (Issue Creator) on (2025-01-05 16:29:07 UTC): I did search. You closed my previous issue.

On Sun, Jan 5, 2025 at 9:37AM Mihai Maruseac ***@***.***>
wrote:

dnmaster1 (Issue Creator) on (2025-01-05 16:29:26 UTC): Please reopen. This is not closed.

On Sun, Jan 5, 2025 at 9:37AM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-06 13:18:42 UTC): Opening multiple issues can be considered as spam. Let's move discussion to just one issue.

"
2767700983,issue,open,,"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.","### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

window11

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error.

What I've Tried
Re-downloading TensorFlow and TensorFlow Datasets to ensure they are up to date.
Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive.
Re-adjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 instead of Python 3.10.12.
Recreating the Virtual Environment in Anaconda to ensure a clean setup.
Downloading the Cats vs Dogs Dataset from Different Sources, but the issue persists.
Asking ChatGPT for assistance, but the issue remains unresolved.

Additional Information
On Google Colab, the same code runs without any issues, and the dataset downloads successfully.
In VS Code, the error consistently occurs during the dataset download process, indicating that 'PetImages/Cat/0.jpg' is missing from the archive.
Network Stability: I have a stable internet connection, and downloads complete without interruption, but the error persists.

Questions
Why does the KeyError occur in VS Code but not in Google Colab?
Could this be related to the way the dataset is being downloaded or unzipped locally?
Are there any compatibility issues between the Python/TensorFlow versions and the dataset?
Request for Help
I would greatly appreciate any guidance or suggestions on how to resolve this issue. Thank you in advance for your assistance!


### Standalone code to reproduce the issue

```shell
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np



CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
                  split=tfds.Split.TRAIN)
```


### Relevant log output

```shell
PS C:\Users\jbb86\\> & C:/Users/jbb86///.venv/Scripts/python.exe c:/Users/jbb86///.venv/CatsVsDogs.py
2025-01-03 22:38:53.599253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-03 22:38:54.247816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\jbb86\tensorflow_datasets\cats_vs_dogs\4.0.1...
Dl Size...: 100%|| 824887076/824887076 [00:00<00:00, 803489819418.28 MiB/s]
Dl Completed...: 100%|| 1/1 [00:00<00:00, 974.06 url/s]
Generating splits...:   0%|                                   2025-01-03 22:38:55.958212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""c:\Users\jbb86\\\.venv\CatsVsDogs.py"", line 7, in <module>
    CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 661, in load
    _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare
    self._download_and_prepare(
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare
    split_infos = self._generate_splits(dl_manager, download_config)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits
    future = split_builder.submit_split_generation(
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation
    return self._build_from_generator(**build_kwargs)
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator
    for key, example in utils.tqdm(
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tqdm\std.py"", line 1181, in __iter__
    for obj in iterable:
  File ""C:\Users\jbb86\\\.venv\lib\site-packages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 117, in _generate_examples
    new_fobj = zipfile.ZipFile(buffer).open(fname)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1516, in open
    zinfo = self.getinfo(name)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1443, in getinfo
    raise KeyError(
KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive""
```
",zzzHou01,2025-01-03 14:48:36+00:00,['tilakrayal'],2025-01-30 04:45:55+00:00,,https://github.com/tensorflow/tensorflow/issues/84104,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('awaiting PR merge', 'awaiting PR merge'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2573016775, 'issue_id': 2767700983, 'author': 'tilakrayal', 'body': '@zzzHou01,\r\nCould you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2025, 1, 6, 12, 30, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573638610, 'issue_id': 2767700983, 'author': 'zzzHou01', 'body': '> @zzzHou01, Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!\r\n\r\nThank you for your response!\r\n\r\nI am following the TensorFlow Cats vs Dogs tutorial provided on the official TensorFlow website. Source:https://www.tensorflow.org/tutorials/images/transfer_learning', 'created_at': datetime.datetime(2025, 1, 6, 18, 5, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614930174, 'issue_id': 2767700983, 'author': 'SanjaySG', 'body': ""I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflow-datasets 4.9.7. It doesn't happen on Colab or MacOS. \n\nI believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in [tensorflow/datasets](https://github.com/tensorflow/datasets/blob/b68aa45e2124975cea4bb658593a113dc9c2925f/tensorflow_datasets/image_classification/cats_vs_dogs.py#L4) by the way. \n\nAlso, this is probably a duplicate of [issues/3918](https://github.com/tensorflow/datasets/issues/3918) in tensorflow/datasets. @zzzHou01 There is a hacky fix available in the [issuecomment-1892835410](https://github.com/tensorflow/datasets/issues/3918#issuecomment-1892835410) in the meantime."", 'created_at': datetime.datetime(2025, 1, 27, 6, 20, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619428198, 'issue_id': 2767700983, 'author': 'SanjaySG', 'body': 'This fix for this was merged to tensorflow/datasets through https://github.com/tensorflow/datasets/commit/9969ce542f4b0e1cbf0a085e8e0df11bccea5c17. Once there is a new release, the problem should be fixed.', 'created_at': datetime.datetime(2025, 1, 28, 16, 5, 9, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-06 12:30:15 UTC): @zzzHou01,
Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!

zzzHou01 (Issue Creator) on (2025-01-06 18:05:53 UTC): Thank you for your response!

I am following the TensorFlow Cats vs Dogs tutorial provided on the official TensorFlow website. Source:https://www.tensorflow.org/tutorials/images/transfer_learning

SanjaySG on (2025-01-27 06:20:07 UTC): I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflow-datasets 4.9.7. It doesn't happen on Colab or MacOS. 

I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in [tensorflow/datasets](https://github.com/tensorflow/datasets/blob/b68aa45e2124975cea4bb658593a113dc9c2925f/tensorflow_datasets/image_classification/cats_vs_dogs.py#L4) by the way. 

Also, this is probably a duplicate of [issues/3918](https://github.com/tensorflow/datasets/issues/3918) in tensorflow/datasets. @zzzHou01 There is a hacky fix available in the [issuecomment-1892835410](https://github.com/tensorflow/datasets/issues/3918#issuecomment-1892835410) in the meantime.

SanjaySG on (2025-01-28 16:05:09 UTC): This fix for this was merged to tensorflow/datasets through https://github.com/tensorflow/datasets/commit/9969ce542f4b0e1cbf0a085e8e0df11bccea5c17. Once there is a new release, the problem should be fixed.

"
2767671507,issue,open,reopened,Tensorflow not supported on Windows + ARM CPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't import tensorflow


### Standalone code to reproduce the issue

```shell
I can't import tensorflow. Installation is successful. I uninstalled and reinstalled
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[9], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\dhima\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",dnmaster1,2025-01-03 14:33:01+00:00,['tilakrayal'],2025-01-21 13:15:56+00:00,,https://github.com/tensorflow/tensorflow/issues/84102,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2569541042, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 16, 58, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569541084, 'issue_id': 2767671507, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84102"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84102"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 16, 58, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569749488, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': 'I have brand new PC with snapdragon X plus. It is not working. It is in\r\nfact working on my old pc\r\n\r\nOn Fri, Jan 3, 2025, 11:59\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Duplicate of #19584\r\n> <https://github.com/tensorflow/tensorflow/issues/19584>. Please do a\r\n> search before opening new issues. This is a very old issue and manifests\r\n> because old PCs with Windows cannot load libraries needed by TF because\r\n> they have very old architecture sets.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2569541042>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ55TP7NMHL4PSSHRE7L2I26WJAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRZGU2DCMBUGI>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 3, 20, 1, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569750419, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': ""Please don't close the issue. Yourassumotion is incorrect\r\n\r\nOn Fri, Jan 3, 2025, 11:59\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Closed #84102 <https://github.com/tensorflow/tensorflow/issues/84102> as\r\n> completed.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#event-15809318538>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ574RWNBJEI2T2OL62D2I26WJAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJVHAYDSMZRHA2TGOA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 3, 20, 2, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571357951, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'What are your CPU specs?', 'created_at': datetime.datetime(2025, 1, 4, 17, 8, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571404941, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': '[image: image.png]\r\n\r\nOn Sat, Jan 4, 2025 at 12:09\u202fPM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> What are your CPU specs?\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2571357951>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ54B3NBHHHE7MHSQQVL2JAIUFAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZRGM2TOOJVGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 4, 20, 33, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571646223, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'The image is not getting displayed. Can you paste them instead as text?', 'created_at': datetime.datetime(2025, 1, 5, 14, 31, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571683415, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': 'Device name Master2024\r\nProcessor Snapdragon(R) X Plus - X1P42100 - Qualcomm(R) Oryon(TM) CPU\r\n3.24 GHz\r\nInstalled RAM 16.0 GB (15.6 GB usable)\r\nDevice ID BE820513-61FA-4460-944D-DA3541AEED2D\r\nProduct ID 00342-21332-97204-AAOEM\r\nSystem type 64-bit operating system, ARM-based processor\r\nPen and touch Pen and touch support with 10 touch points\r\n\r\n\r\nWindows specs\r\n\r\nEdition Windows 11 Home\r\nVersion 24H2\r\nInstalled on \u200e12/\u200e30/\u200e2024\r\nOS build 26100.2605\r\nSerial number YX0ECPSK\r\nExperience Windows Feature Experience Pack 1000.26100.36.0\r\n\r\n\r\nOn Sun, Jan 5, 2025 at 9:31\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> The image is not getting displayed. Can you paste them instead as text?\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2571646223>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ53NT5WQGP24RKHDVWT2JE64DAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZRGY2DMMRSGM>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 5, 16, 38, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573096410, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'I think TensorFlow on Windows is only supported on Intel, not ARM. But this is indeed different than the other one, so re-opening.', 'created_at': datetime.datetime(2025, 1, 6, 13, 19, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573117662, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'Confirmed that Windows support only exists for Intel CPUs. On https://pypi.org/project/tensorflow/#files there is no Windows + ARM wheel.\r\n\r\nOn https://pypi.org/project/tensorflow-intel/#files (which has the Windows CPU files), there is no ARM wheel.', 'created_at': datetime.datetime(2025, 1, 6, 13, 31, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573126881, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': 'This is also weird, because the pip installer should not have proceeded due to missing wheels.\r\n\r\nBut, can you try installing the linux wheel, via WSL? Not guaranteed to work, but it might.', 'created_at': datetime.datetime(2025, 1, 6, 13, 36, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573476537, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': 'So my CPU configuration is not supported?\r\n\r\nOn Mon, Jan 6, 2025, 8:36\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> This is also weird, because the pip installer should not have proceeded\r\n> due to missing wheels.\r\n>\r\n> But, can you try installing the linux wheel, via WSL? Not guaranteed to\r\n> work, but it might.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2573126881>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ57MJJCPVCQ7TPXQ7L32JKBHBAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZTGEZDMOBYGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 6, 16, 35, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573478669, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': 'What is ""missing wheels""? I will try WSL now and get back to you.\r\n\r\nOn Mon, Jan 6, 2025, 8:36\u202fAM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> This is also weird, because the pip installer should not have proceeded\r\n> due to missing wheels.\r\n>\r\n> But, can you try installing the linux wheel, via WSL? Not guaranteed to\r\n> work, but it might.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2573126881>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ57MJJCPVCQ7TPXQ7L32JKBHBAVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNZTGEZDMOBYGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 6, 16, 36, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573503184, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': '> So my CPU configuration is not supported?\r\n\r\nIt looks like that. It is different than #19584 in that that issue refers to Intel CPUs that are too old and don\'t have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family).\r\n\r\n> What is ""missing wheels""?\r\n\r\nThe unit of shipping a Python package is called [a wheel](https://pythonwheels.com/). For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU.\r\n\r\nWhen installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system.\r\n\r\nThere in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why #19584 exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported).\r\n\r\nIn your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags.\r\n\r\n> I will try WSL now and get back to you.\r\n\r\nI hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support.\r\n\r\nAlternatively, and something I would recommend, is to use [Colab](https://colab.google). I\'m actually using that for a lot of experiments and it\'s really nice.', 'created_at': datetime.datetime(2025, 1, 6, 16, 49, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596937171, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': ""I'm marking this as a subissue of #19584 since it has the same behavior: CPUs that don't support AVX instructions sets (includes ARM ones) are not able to run TF. The only difference, and why this is not marked as duplicate, is that this is a totally different family of CPUs, and likely compiling from source on your own system will produce a wheel that works."", 'created_at': datetime.datetime(2025, 1, 16, 21, 35, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599235350, 'issue_id': 2767671507, 'author': 'dnmaster1', 'body': ""Thank you Mihai! When can I expect resolution? Also, is this something I\r\ncan compile on my computer? If so, do you have instructions to compile?\r\n\r\nThank you again for your help.\r\n\r\nDhimant\r\n\r\nOn Thu, Jan 16, 2025 at 4:35\u202fPM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> I'm marking this as a subissue of #19584\r\n> <https://github.com/tensorflow/tensorflow/issues/19584> since it has the\r\n> same behavior: CPUs that don't support AVX instructions sets (includes ARM\r\n> ones) are not able to run TF. The only difference, and why this is not\r\n> marked as duplicate, is that this is a totally different family of CPUs,\r\n> and likely compiling from source on your own system will produce a wheel\r\n> that works.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/84102#issuecomment-2596937171>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOQLJ57YV3HMRZ7AW2UDGRL2LAQ33AVCNFSM6AAAAABURZMC7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKOJWHEZTOMJXGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 17, 21, 19, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599250500, 'issue_id': 2767671507, 'author': 'mihaimaruseac', 'body': ""It is unclear when support will come. There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear. @MichaelHudgins might have more details on the plan.\n\nRegarding compiling on own computer, that should definitely be possible. There are some instructions at https://www.tensorflow.org/install/source_windows but I haven't checked how up to date they are."", 'created_at': datetime.datetime(2025, 1, 17, 21, 31, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599260065, 'issue_id': 2767671507, 'author': 'MichaelHudgins', 'body': '> There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear\n\nTo my knowledge there are no current plans to support arm on windows natively.  We do currently publish wheels for Linux and macOS arm64.', 'created_at': datetime.datetime(2025, 1, 17, 21, 39, 38, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-03 16:58:50 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2025-01-03 16:58:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84102"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84102"">No</a>

dnmaster1 (Issue Creator) on (2025-01-03 20:01:55 UTC): I have brand new PC with snapdragon X plus. It is not working. It is in
fact working on my old pc

On Fri, Jan 3, 2025, 11:59AM Mihai Maruseac ***@***.***>
wrote:

dnmaster1 (Issue Creator) on (2025-01-03 20:02:46 UTC): Please don't close the issue. Yourassumotion is incorrect

On Fri, Jan 3, 2025, 11:59AM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-04 17:08:57 UTC): What are your CPU specs?

dnmaster1 (Issue Creator) on (2025-01-04 20:33:31 UTC): [image: image.png]

On Sat, Jan 4, 2025 at 12:09PM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-05 14:31:01 UTC): The image is not getting displayed. Can you paste them instead as text?

dnmaster1 (Issue Creator) on (2025-01-05 16:38:38 UTC): Device name Master2024
Processor Snapdragon(R) X Plus - X1P42100 - Qualcomm(R) Oryon(TM) CPU
3.24 GHz
Installed RAM 16.0 GB (15.6 GB usable)
Device ID BE820513-61FA-4460-944D-DA3541AEED2D
Product ID 00342-21332-97204-AAOEM
System type 64-bit operating system, ARM-based processor
Pen and touch Pen and touch support with 10 touch points


Windows specs

Edition Windows 11 Home
Version 24H2
Installed on 12/30/2024
OS build 26100.2605
Serial number YX0ECPSK
Experience Windows Feature Experience Pack 1000.26100.36.0


On Sun, Jan 5, 2025 at 9:31AM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-06 13:19:14 UTC): I think TensorFlow on Windows is only supported on Intel, not ARM. But this is indeed different than the other one, so re-opening.

mihaimaruseac on (2025-01-06 13:31:29 UTC): Confirmed that Windows support only exists for Intel CPUs. On https://pypi.org/project/tensorflow/#files there is no Windows + ARM wheel.

On https://pypi.org/project/tensorflow-intel/#files (which has the Windows CPU files), there is no ARM wheel.

mihaimaruseac on (2025-01-06 13:36:24 UTC): This is also weird, because the pip installer should not have proceeded due to missing wheels.

But, can you try installing the linux wheel, via WSL? Not guaranteed to work, but it might.

dnmaster1 (Issue Creator) on (2025-01-06 16:35:48 UTC): So my CPU configuration is not supported?

On Mon, Jan 6, 2025, 8:36AM Mihai Maruseac ***@***.***>
wrote:

dnmaster1 (Issue Creator) on (2025-01-06 16:36:57 UTC): What is ""missing wheels""? I will try WSL now and get back to you.

On Mon, Jan 6, 2025, 8:36AM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-06 16:49:30 UTC): It looks like that. It is different than #19584 in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family).


The unit of shipping a Python package is called [a wheel](https://pythonwheels.com/). For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU.

When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system.

There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why #19584 exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported).

In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags.


I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support.

Alternatively, and something I would recommend, is to use [Colab](https://colab.google). I'm actually using that for a lot of experiments and it's really nice.

mihaimaruseac on (2025-01-16 21:35:30 UTC): I'm marking this as a subissue of #19584 since it has the same behavior: CPUs that don't support AVX instructions sets (includes ARM ones) are not able to run TF. The only difference, and why this is not marked as duplicate, is that this is a totally different family of CPUs, and likely compiling from source on your own system will produce a wheel that works.

dnmaster1 (Issue Creator) on (2025-01-17 21:19:28 UTC): Thank you Mihai! When can I expect resolution? Also, is this something I
can compile on my computer? If so, do you have instructions to compile?

Thank you again for your help.

Dhimant

On Thu, Jan 16, 2025 at 4:35PM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2025-01-17 21:31:42 UTC): It is unclear when support will come. There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear. @MichaelHudgins might have more details on the plan.

Regarding compiling on own computer, that should definitely be possible. There are some instructions at https://www.tensorflow.org/install/source_windows but I haven't checked how up to date they are.

MichaelHudgins on (2025-01-17 21:39:38 UTC): To my knowledge there are no current plans to support arm on windows natively.  We do currently publish wheels for Linux and macOS arm64.

"
2767170232,issue,open,,GPU Delegate for Object detection and Image classification from Tensorflow lite for Android ,"I'm using Android Kotlin code for object detection and image classification 
https://github.com/tensorflow/examples/tree/master/lite/examples

var currentDelegate: Int = 1, 
 
 val optionsBuilder =
            ObjectDetector.ObjectDetectorOptions.builder()
                .setScoreThreshold(threshold)
                .setMaxResults(maxResults)

        // Set general detection options, including number of used threads
        val baseOptionsBuilder = BaseOptions.builder()

val compatList = CompatibilityList()
                if (compatList.isDelegateSupportedOnThisDevice) {
                    val delegateOptions = compatList.bestOptionsForThisDevice
                    baseOptionsBuilder.useGpu()
                    Utils.readLog(TAG, ""Using GPU delegate with options: $delegateOptions"")
                } else {
                    baseOptionsBuilder.setNumThreads(4)
                }
                
optionsBuilder.setBaseOptions(baseOptionsBuilder.build())

try {
            objectDetector =
                ObjectDetector.createFromFileAndOptions(
                    context,
                    modelName,
                    optionsBuilder.build()
                )
        } catch (e: IllegalStateException) {
            objectDetectorListener?.onError(
                ""Object detector failed to initialize. See error logs for details""
            )
            Utils.readLog(TAG, "" "" + e.message)
        }
        

//Image Classification 
try {
            imageClassifier =
                ImageClassifier.createFromFileAndOptions(
                    context,
                    modelName,
                    optionsBuilder.build()
                )
        } catch (e: IllegalStateException) {
            imageClassifierListener?.onError(
                ""Image classifier failed to initialize. See error logs for details""
            )
            Utils.readLog(TAG, ""TFLite failed to load model with error: "" + e.message)
        }
        
        
Here I'm Using above code baseOptionsBuilder using same for both object detection and image classification but if i'm using currentDelegate = 1 for both it is lagging a lot and object detection and image classification is not working with GPU. 

Is there anything else that i can use to make it work?",dayalatamai,2025-01-03 08:26:52+00:00,['gaikwadrahul8'],2025-01-21 11:45:50+00:00,,https://github.com/tensorflow/tensorflow/issues/84043,"[('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue')]","[{'comment_id': 2573020064, 'issue_id': 2767170232, 'author': 'gaikwadrahul8', 'body': 'Hi, @dayalatamai \r\nI apologize for the delayed response to confirm, have you followed this [official documentation ](https://ai.google.dev/edge/litert/android/gpu#enable_gpu_acceleration) ? If not please follow that and let us know is it resolving your issue  or not ?\r\n\r\nPlease make sure that models you are using are compatible with the GPU delegate. If models has unsupported ops it will fall back to CPU execution please check [GPU ML operations support](https://ai.google.dev/edge/litert/performance/gpu)\r\n\r\nIf issue still persists please help us with your Github repo along with complete steps to replicate same behavior from our end to investigate this issue from our end.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 1, 6, 12, 32, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577230540, 'issue_id': 2767170232, 'author': 'dayalatamai', 'body': '@gaikwadrahul8 , Thanks for your response. \r\nAs I\'m using tensorflow lite (already mentioned the url of library) for object detection and image classification.  \r\nI want to perform these operation on GPU delegate and in tensorflow already have GPU delegate support so with that how can i merge the google GPU delegate. Adding Object detection and image classification classes for your reference, Please let me know where should i make changes for the same. Thank you once again.\r\n\r\nclass ObjectDetectorHelper(\r\n    private var threshold: Float = 0.5f,\r\n    private var numThreads: Int = 1,\r\n    private var maxResults: Int = 5,\r\n    private var currentDelegate: Int = 1,\r\n    private var currentModel: Int = 0,\r\n    private val context: Context,\r\n    private val objectDetectorListener: DetectorListener?\r\n) {\r\n\r\n    private var objectDetector: ObjectDetector? = null\r\n\r\n    init {\r\n        setupObjectDetector()\r\n    }\r\n\r\n    fun clearObjectDetector() {\r\n        objectDetector = null\r\n    }\r\n    private fun setupObjectDetector() {\r\n        detectorScope.launch {\r\n            val optionsBuilder =\r\n                ObjectDetector.ObjectDetectorOptions.builder()\r\n                    .setScoreThreshold(threshold)\r\n                    .setMaxResults(maxResults)\r\n\r\n            val baseOptionsBuilder = BaseOptions.builder()\r\n\r\n            when (currentDelegate) {\r\n                DELEGATE_CPU -> {\r\n                    baseOptionsBuilder.setNumThreads(numThreads)\r\n                }\r\n\r\n                DELEGATE_GPU -> {\r\n                    val compatList = CompatibilityList()\r\n                    if (compatList.isDelegateSupportedOnThisDevice) {\r\n                        val delegateOptions = compatList.bestOptionsForThisDevice\r\n                        baseOptionsBuilder.useGpu()\r\n                    } else {\r\n                        baseOptionsBuilder.setNumThreads(4)\r\n                    }\r\n                }\r\n\r\n                DELEGATE_NNAPI -> {\r\n                    baseOptionsBuilder.setNumThreads(numThreads).useNnapi()\r\n                }\r\n            }\r\n\r\n            optionsBuilder.setBaseOptions(baseOptionsBuilder.build())\r\n\r\n            val modelName =\r\n                when (currentModel) {\r\n                    MODEL_MOBILENETV1 -> ""efficientdet_lite2_OD7.tflite""\r\n                    else -> ""efficientdet_lite2_OD7.tflite""\r\n                }\r\n\r\n            try {\r\n                objectDetector =\r\n                    ObjectDetector.createFromFileAndOptions(\r\n                        context,\r\n                        modelName,\r\n                        optionsBuilder.build()\r\n                    )\r\n            } catch (e: IllegalStateException) {\r\n                objectDetectorListener?.onError(\r\n                    ""Object detector failed to initialize. See error logs for details""\r\n                )\r\n            }\r\n        }\r\n    }\r\n\r\n    fun detect(image: Bitmap, imageRotation: Int, localAspectRation: Int) {\r\n        if (objectDetector == null) {\r\n            setupObjectDetector()\r\n        }\r\n\r\n        detectorScope.launch {\r\n            try {\r\n                var inferenceTime = SystemClock.uptimeMillis()\r\n                val imageProcessor =\r\n                    ImageProcessor.Builder()\r\n                        .add(Rot90Op(-imageRotation / 90))\r\n                        .build()\r\n\r\n                val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))\r\n\r\n                val results1 = objectDetector?.detect(tensorImage)\r\n                inferenceTime = SystemClock.uptimeMillis() - inferenceTime\r\n\r\n                var results: MutableList<Detection>? = null\r\n\r\n                if (results1!!.size > 1) {\r\n                    for (i in 0 until results1.size) {\r\n                        Utils.readLog(""objectDetector boundingBox1 Width - ${results1[i].boundingBox.width()}"")\r\n                        Utils.readLog(""objectDetector boundingBox1 Height - ${results1[i].boundingBox.height()}"")\r\n                        if (localAspectRation == AspectRatio.RATIO_4_3) {\r\n                            if (results1[i].boundingBox.height() > 300 || results1[i].boundingBox.width() > 400) {\r\n                                results = listOf(results1[i]).toMutableList()\r\n                                break\r\n                            }\r\n                        } else {\r\n                            if (results1[i].boundingBox.height() > 350) {\r\n                                results = listOf(results1[i]).toMutableList()\r\n                                break\r\n                            }\r\n                        }\r\n                    }\r\n                } else {\r\n                    results = results1\r\n                }\r\n\r\n                objectDetectorListener?.onResults(\r\n                    results,\r\n                    inferenceTime,\r\n                    tensorImage.height,\r\n                    tensorImage.width\r\n                )\r\n            } catch (e: Exception) {\r\n                objectDetectorListener?.onError(""Error detecting an image: ${e.message}"")\r\n            }\r\n        }\r\n    }\r\n\r\n    interface DetectorListener {\r\n        fun onError(error: String){}\r\n        fun onResults(\r\n            results: MutableList<Detection>?,\r\n            inferenceTime: Long,\r\n            imageHeight: Int,\r\n            imageWidth: Int\r\n        )\r\n    }\r\n\r\n    companion object {\r\n        const val DELEGATE_CPU = 0\r\n        const val DELEGATE_GPU = 1\r\n        const val DELEGATE_NNAPI = 2\r\n        const val MODEL_MOBILENETV1 = 0\r\n        private val detectorScope = CoroutineScope(newSingleThreadContext(""DetectorGpuThread""))\r\n        private const val TAG = ""ObjectDetectorHelper""\r\n    }\r\n}\r\n\r\n\r\nclass ImageClassifierHelper(\r\n    private var threshold: Float = 0.5f,\r\n    private var numThreads: Int = 1,\r\n    private var maxResults: Int = 1,\r\n    private var currentDelegate: Int = 0,\r\n    private val context: Context,\r\n    private val imageClassifierListener: ClassifierListener?\r\n) {\r\n    private var imageClassifier: ImageClassifier? = null\r\n    init {\r\n        setupImageClassifier()\r\n    }\r\n\r\n    fun clearImageClassifier() {\r\n        imageClassifier = null\r\n    }\r\n\r\n    private fun setupImageClassifier() {\r\n        val optionsBuilder = ImageClassifier.ImageClassifierOptions.builder()\r\n            .setScoreThreshold(threshold)\r\n            .setMaxResults(maxResults)\r\n\r\n        val baseOptionsBuilder = BaseOptions.builder()\r\n\r\n        when (currentDelegate) {\r\n            DELEGATE_CPU -> {\r\n                baseOptionsBuilder.setNumThreads(numThreads)\r\n            }\r\n            DELEGATE_GPU -> {\r\n                val compatList = CompatibilityList()\r\n                if (compatList.isDelegateSupportedOnThisDevice) {\r\n                    val delegateOptions = compatList.bestOptionsForThisDevice\r\n                    baseOptionsBuilder.useGpu()\r\n                } else {\r\n                    baseOptionsBuilder.setNumThreads(4)\r\n                }\r\n            }\r\n            DELEGATE_NNAPI -> {\r\n                baseOptionsBuilder.setNumThreads(numThreads).useNnapi()\r\n            }\r\n        }\r\n        optionsBuilder.setBaseOptions(baseOptionsBuilder.build())\r\n        val modelName = ""reshaped_model_optimized_graph.tflite""\r\n        try {\r\n            imageClassifier =\r\n                ImageClassifier.createFromFileAndOptions(context, modelName, optionsBuilder.build())\r\n        } catch (e: IllegalStateException) {\r\n            imageClassifierListener?.onError(\r\n                ""Image classifier failed to initialize. See error logs for details""\r\n            )\r\n        }\r\n    }\r\n\r\n    fun classify(image: Bitmap, rotation: Int) {\r\n        if (imageClassifier == null) {\r\n            setupImageClassifier()\r\n        }\r\n\r\n        var inferenceTime = SystemClock.uptimeMillis()\r\n\r\n        val imageProcessor =\r\n            ImageProcessor.Builder()\r\n                .build()\r\n\r\n        val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))\r\n\r\n        val imageProcessingOptions = ImageProcessingOptions.builder()\r\n            .setOrientation(getOrientationFromRotation(rotation))\r\n            .build()\r\n\r\n        val results = imageClassifier?.classify(tensorImage, imageProcessingOptions)\r\n        inferenceTime = SystemClock.uptimeMillis() - inferenceTime\r\n        imageClassifierListener?.onResults(\r\n            results,\r\n            inferenceTime\r\n        )\r\n    }\r\n\r\n    private fun getOrientationFromRotation(rotation: Int) : ImageProcessingOptions.Orientation {\r\n        return when (rotation) {\r\n            Surface.ROTATION_270 ->\r\n                ImageProcessingOptions.Orientation.BOTTOM_RIGHT\r\n            Surface.ROTATION_180 ->\r\n                ImageProcessingOptions.Orientation.RIGHT_BOTTOM\r\n            Surface.ROTATION_90 ->\r\n                ImageProcessingOptions.Orientation.TOP_LEFT\r\n            else ->\r\n                ImageProcessingOptions.Orientation.RIGHT_TOP\r\n        }\r\n    }\r\n\r\n    interface ClassifierListener {\r\n        fun onError(error: String)\r\n        fun onResults(\r\n            results: List<Classifications>?,\r\n            inferenceTime: Long\r\n        )\r\n    }\r\n\r\n    companion object {\r\n        const val DELEGATE_CPU = 0\r\n        const val DELEGATE_GPU = 1\r\n        const val DELEGATE_NNAPI = 2\r\n        private const val TAG = ""ImageClassifierHelper""\r\n    }\r\n}\r\n\r\n*********** Major Issue i\'m facing here *************\r\n\r\nIf I\'m trying to use Gpu in Image classification instead of object detection the entire camera screen is lagging(app is getting very slow) and Image is not classifying while classification we\'re getting the result : \r\n[Classifications{categories=[<Category ""unknown"" (displayName= score=0.99997544 index=28)>], headIndex=0}]\r\n\r\nCategory is \'unknown\' \r\nHere, I want to use Gpu delegate for image classification as this is consuming most of the memory and at the last it is crashing in Most of Pixels device.\r\nAs, you suggested here https://ai.google.dev/edge/litert/android/gpu#kotlin\r\nHow to use the same in my code snippet.', 'created_at': datetime.datetime(2025, 1, 8, 9, 44, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579277755, 'issue_id': 2767170232, 'author': 'gaikwadrahul8', 'body': ""Hi, @dayalatamai\r\nTo confirm, To enable access to the GPU delegate, add `com.google.ai.edge.litert:litert-gpu-delegate-plugin` to your app's `build.gradle` file and also try to Add this line to your `build.gradle` `implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.10.0'` and see does it solve your issue or not ? \r\n\r\nPlease refer this somewhat similar issues about GPU delegate https://github.com/tensorflow/tensorflow/issues/57934 and please refer this official [blog](https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html)\r\n\r\nIf possible could you please help us with your Github repo ( which includes all the dependancies) along with complete steps to replicate the same behavior from our end that will be more convenient to resolve the issue ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 9, 6, 35, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580494233, 'issue_id': 2767170232, 'author': 'dayalatamai', 'body': 'Thanks, @gaikwadrahul8 for the update but com.google.ai.edge.litert:litert-gpu-delegate-plugin where do i need to add into build.gradle file and the another is already there implementation \'org.tensorflow:tensorflow-lite-gpu-api:2.10.0\' in build.gradle \r\nStill i\'m getting the same if i\'m trying to enable gpu for image classification App is lagging and image is not classifying for the same and it is crashing most of the time so sharing a crash report for the same.\r\n<img width=""1262"" alt=""Screenshot 2025-01-09 at 8 27 22\u202fPM"" src=""https://github.com/user-attachments/assets/910f140c-7550-4278-a995-f5c60e6de2ba"" />\r\n<img width=""1113"" alt=""Screenshot 2025-01-09 at 8 26 24\u202fPM"" src=""https://github.com/user-attachments/assets/e02e415f-1a9b-43b9-bbc0-298aef5235e4"" />\r\n\r\n\r\nAs you have requested for git repo, i do not have the access to share and it is private repo so. Meanwhile i can share my build.gradle file with you for the reference.\r\n\r\n******** build.gradle *********\r\n\r\napply plugin: \'com.android.application\'\r\napply plugin: \'com.google.gms.google-services\'\r\napply plugin: \'com.google.firebase.crashlytics\'\r\napply plugin: \'kotlin-android\'\r\napply plugin: \'kotlinx-serialization\'\r\napply plugin: \'androidx.navigation.safeargs\'\r\n\r\nandroid {\r\n    ndkVersion ""25.1.8937393""\r\n    signingConfigs {\r\n        config {\r\n            keyAlias \'video inventory mobile manager\'\r\n            keyPassword \'vimmapp\'\r\n            storeFile file(\'/Users/nunc-it-pc-108/Documents/Android/VIMM_STILL_PHOTOS/vimmvidcomandroid/VIMM360/vimm.keystore\')\r\n            storePassword \'vimmapp\'\r\n        }\r\n    }\r\n    namespace ""com.tab.and2""\r\n    compileSdk 34\r\n    buildToolsVersion \'34.0.0\'\r\n    defaultConfig {\r\n        applicationId ""com.tab.and2""\r\n        minSdkVersion 28\r\n        targetSdkVersion 34\r\n        versionCode 65\r\n        versionName ""4.5.5.01.08.2025""\r\n        multiDexEnabled true\r\n        resConfig ""en""\r\n        ndk {\r\n            abiFilters \'armeabi-v7a\', \'arm64-v8a\'\r\n        }\r\n    }\r\n    splits {\r\n        abi {\r\n            include ""armeabi-v7a"", ""arm64-v8a""\r\n        }\r\n    }\r\n    applicationVariants.configureEach { variant ->\r\n        variant.outputs.each { output ->\r\n            def versionCodes = [""armeabi-v7a"": 1, ""arm64-v8a"": 2]\r\n            def abi = output.getFilter(com.android.build.OutputFile.ABI)\r\n            if (abi != null) {  // null for the universal-debug, universal-release variants\r\n                output.versionCodeOverride =\r\n                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode\r\n            }\r\n        }\r\n    }\r\n    def buildType // Your variable\r\n    android.applicationVariants.configureEach { variant ->\r\n        variant.outputs.configureEach {\r\n            buildType = variant.buildType.name // Sets the current build type\r\n            outputFileName = ""VIMM360_$buildType(${variant.versionCode}v_${variant.versionName})_"" + new Date().format(\'hh_mmaa\') + "".apk""\r\n        }\r\n    }\r\n\r\n    dexOptions {\r\n        preDexLibraries = true\r\n        javaMaxHeapSize ""4g"" // 2g should be also OK\r\n    }\r\n    lintOptions {\r\n        checkReleaseBuilds false\r\n        abortOnError false\r\n    }\r\n    packagingOptions {\r\n//        exclude \'META-INF/DEPENDENCIES\'\r\n        exclude(""META-INF/DEPENDENCIES"")\r\n        exclude(""META-INF/LICENSE"")\r\n        exclude(""META-INF/LICENSE.txt"")\r\n        exclude(""META-INF/license.txt"")\r\n        exclude(""META-INF/NOTICE"")\r\n        exclude(""META-INF/NOTICE.txt"")\r\n        exclude(""META-INF/notice.txt"")\r\n        exclude(""META-INF/ASL2.0"")\r\n        exclude(""META-INF/*.kotlin_module"")\r\n    }\r\n\r\n    buildTypes {\r\n        release {\r\n            debuggable false\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile(\'proguard-android.txt\'), \'proguard-rules.pro\'\r\n            signingConfig signingConfigs.config\r\n\r\n            firebaseCrashlytics {\r\n                mappingFileUploadEnabled false\r\n            }\r\n        }\r\n        debug {\r\n            applicationIdSuffix \'\'\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile(\'proguard-android.txt\'), \'proguard-rules.pro\'\r\n            firebaseCrashlytics {\r\n                mappingFileUploadEnabled false\r\n            }\r\n        }\r\n    }\r\n    buildFeatures {\r\n        compose true\r\n        buildConfig true\r\n        viewBinding true\r\n    }\r\n    composeOptions {\r\n        kotlinCompilerExtensionVersion compose_version\r\n    }\r\n\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_17\r\n        targetCompatibility JavaVersion.VERSION_17\r\n    }\r\n\r\n    kotlinOptions {\r\n        jvmTarget = \'17\'\r\n    }\r\n\r\n    sourceSets {\r\n        main {\r\n            jniLibs.srcDirs = [\'src/main/jniLibs\']\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    implementation \'androidx.multidex:multidex:2.0.1\'\r\n    implementation \'com.google.android.gms:play-services-vision:20.1.3\'\r\n    implementation \'me.dm7.barcodescanner:zbar:1.9.8\'\r\n    implementation files(\'libs/environment3.jar\')\r\n    implementation \'androidx.legacy:legacy-support-v13:1.0.0\'\r\n    implementation \'androidx.recyclerview:recyclerview:1.3.2\'\r\n    implementation \'androidx.appcompat:appcompat:1.6.1\'\r\n    implementation \'androidx.cardview:cardview:1.0.0\'\r\n    implementation \'com.google.apis:google-api-services-storage:v1-rev99-1.22.0\'\r\n    implementation \'com.github.bumptech.glide:glide:4.16.0\'\r\n    implementation \'androidx.constraintlayout:constraintlayout:2.1.4\'\r\n    annotationProcessor \'com.github.bumptech.glide:compiler:4.14.2\'\r\n    implementation \'com.arthenica:mobile-ffmpeg-full-gpl:4.4\'\r\n    implementation ""org.jetbrains.kotlin:kotlin-stdlib:1.8.10""\r\n    implementation(platform(""org.jetbrains.kotlin:kotlin-bom:1.8.10""))\r\n    implementation ""org.jetbrains.kotlinx:kotlinx-serialization-json:1.5.0""\r\n    implementation ""org.jetbrains.kotlin:kotlin-stdlib:1.8.20""\r\n    implementation ""androidx.fragment:fragment-ktx:1.5.7""\r\n    implementation \'com.google.ar:core:1.36.0\'\r\n    implementation ""androidx.window:window:1.3.0""\r\n    implementation \'com.squareup.okhttp3:okhttp:4.9.1\'\r\n    implementation \'com.google.code.gson:gson:2.8.9\'\r\n    androidTestImplementation \'androidx.test.espresso:espresso-core:3.4.0\'\r\n    androidTestImplementation \'androidx.test.ext:junit:1.1.3\'\r\n    testImplementation \'junit:junit:4.13.2\'\r\n    implementation \'com.google.android.material:material:1.9.0\'\r\n    implementation \'com.amazonaws:aws-android-sdk-core:2.7.7\'\r\n    implementation \'com.amazonaws:aws-android-sdk-s3:2.7.7\'\r\n    def lifecycle_version = ""2.7.0""\r\n    implementation ""com.ricoh360.thetaclient:theta-client:1.10.2""\r\n    implementation ""androidx.core:core-ktx:1.9.0""\r\n    //noinspection GradleDependency\r\n    implementation""org.jetbrains.kotlin:kotlin-stdlib-jdk8:1.8.0""\r\n    implementation ""androidx.compose.ui:ui:$compose_version""\r\n    implementation ""androidx.compose.material:material:$compose_version""\r\n    implementation ""androidx.compose.ui:ui-tooling-preview:$compose_version""\r\n    implementation ""androidx.lifecycle:lifecycle-runtime-ktx:$lifecycle_version""\r\n    implementation ""androidx.lifecycle:lifecycle-viewmodel-ktx:$lifecycle_version""\r\n    implementation ""androidx.lifecycle:lifecycle-viewmodel-compose:$lifecycle_version""\r\n    implementation \'androidx.activity:activity-compose:1.9.3\'\r\n    implementation ""androidx.ui:ui-framework:0.1.0-dev10""\r\n    implementation ""androidx.navigation:navigation-compose:2.7.7""\r\n    implementation \'androidx.webkit:webkit:1.10.0\'\r\n    implementation ""org.jetbrains.kotlinx:kotlinx-coroutines-android:$coroutines_version""\r\n    implementation \'org.jetbrains.kotlinx:kotlinx-serialization-json:1.6.0\'\r\n    implementation \'com.jakewharton.timber:timber:5.0.1\'\r\n    implementation \'io.coil-kt:coil-compose:2.2.2\'\r\n    implementation ""io.ktor:ktor-client-cio:2.3.9""\r\n    implementation ""androidx.activity:activity-ktx:$activity_version""\r\n\r\n    testImplementation \'org.junit.jupiter:junit-jupiter:5.9.0\'\r\n    testImplementation ""org.jetbrains.kotlinx:kotlinx-coroutines-test:$coroutines_version""\r\n    testImplementation \'org.junit.jupiter:junit-jupiter\'\r\n    androidTestImplementation \'androidx.test.ext:junit:1.1.5\'\r\n    androidTestImplementation \'androidx.test.espresso:espresso-core:3.5.1\'\r\n    androidTestImplementation ""androidx.compose.ui:ui-test-junit4:$compose_version""\r\n    debugImplementation ""androidx.compose.ui:ui-tooling:$compose_version""\r\n    debugImplementation ""androidx.compose.ui:ui-test-manifest:$compose_version""\r\n    implementation platform(\'com.google.firebase:firebase-bom:33.4.0\')\r\n    implementation(""com.google.firebase:firebase-crashlytics"")\r\n    implementation(""com.google.firebase:firebase-analytics"")\r\n    implementation \'androidx.localbroadcastmanager:localbroadcastmanager:1.1.0\'\r\n    implementation ""androidx.navigation:navigation-fragment-ktx:$nav_version""\r\n    implementation ""androidx.navigation:navigation-ui-ktx:$nav_version""\r\n    def camerax_version = \'1.3.4\' //\'1.1.0-beta03\'\r\n    implementation ""androidx.camera:camera-core:$camerax_version""\r\n    implementation ""androidx.camera:camera-camera2:$camerax_version""\r\n    implementation ""androidx.camera:camera-lifecycle:$camerax_version""\r\n    implementation ""androidx.camera:camera-view:$camerax_version""\r\n    implementation ""androidx.camera:camera-extensions:$camerax_version""\r\n    implementation \'com.google.android.gms:play-services-tflite-java:16.0.1\'\r\n    implementation \'com.google.android.gms:play-services-tflite-gpu:16.1.0\'\r\n    implementation \'org.tensorflow:tensorflow-lite-task-vision:0.4.0\'\r\n    implementation \'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0\'\r\n    implementation \'org.tensorflow:tensorflow-lite-gpu:2.12.0\'\r\n\r\n    implementation \'org.tensorflow:tensorflow-lite-gpu-api:2.10.0\'\r\n    implementation \'org.tensorflow:tensorflow-lite:2.12.0\'\r\n    implementation \'org.tensorflow:tensorflow-lite-support:0.4.1\'\r\n\r\n    implementation \'com.google.guava:listenablefuture:9999.0-empty-to-avoid-conflict-with-guava\'\r\n    configurations {\r\n        all*.exclude group: \'com.google.guava\', module: \'listenablefuture\'\r\n        all*.exclude module:\'guava-jdk5\'\r\n    }\r\n}', 'created_at': datetime.datetime(2025, 1, 9, 14, 59, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592371033, 'issue_id': 2767170232, 'author': 'gaikwadrahul8', 'body': 'Hi, @dayalatamai \nAs far I know not all TensorFlow operations (ops) are supported by the GPU delegate. If a model includes unsupported ops TensorFlow Lite will execute those ops on the CPU instead which can lead to suboptimal performance and increased inference time due to the overhead of CPU-GPU synchronization, please refer this official documentation for [GPU ML operations support](https://ai.google.dev/edge/litert/performance/gpu)\n\nNot all devices support the necessary GPU features for TensorFlow Lite\'s GPU delegate so if you don\'t mind could you please give it try on different devices and see, Is it working as expected or not ? certain models may inherently be less compatible with GPU acceleration due to their architecture or specific operations used within them so please make sure that used models support GPU acceleration\n\nIf possible please try it by Installing the required libraries in `build.gradle` like below :\n\n```\nimplementation(""org.tensorflow:tensorflow-lite:+"")\nimplementation(""org.tensorflow:tensorflow-lite-gpu:+"")\nimplementation(""org.tensorflow:tensorflow-lite-gpu-api:+"")\nimplementation(""org.tensorflow:tensorflow-lite-gpu-delegate-plugin:+"")\nimplementation(""org.tensorflow:tensorflow-lite-support:+"")\n```\n\nFor GPU Delegate settings import below module :\n\n```\nimport org.tensorflow.lite.Interpreter\nimport org.tensorflow.lite.gpu.CompatibilityList\nimport org.tensorflow.lite.gpu.GpuDelegate\n\n```\nSet `GPUDelegate` in the models Interpreter options like below :\n\n```\nval compatList = CompatibilityList()\n\nval options = Interpreter.Options().apply{\n    if(compatList.isDelegateSupportedOnThisDevice){\n        // if the device has a supported GPU, add the GPU delegate\n        val delegateOptions = compatList.bestOptionsForThisDevice\n        this.addDelegate(GpuDelegate(delegateOptions))\n    } else {\n        // if the GPU is not supported, run on 4 threads\n        this.setNumThreads(4)\n    }\n}\n\ninterpreter = Interpreter(model, options)\n\n```\n\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 1, 15, 11, 6, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604501069, 'issue_id': 2767170232, 'author': 'dayalatamai', 'body': ""Hey, @gaikwadrahul8 \n\nThank you for the response. As you said *Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate* Right! I've checked most of the device like samsung(s10,s20,s23 ultra,s22,m31) and pixels(2xl,3a,5,7).\nIf i'm enabling  **private var currentDelegate: Int = 1** for both image classification and object detection on given code snippet and checking which supports GPU delegate in above devices, App is getting very slow (getting lag) and in some pixel devices it is not detecting any object and not classifying any image or getting crash sometimes. \nIf any device does not support GPU then, i'm using 4 threads to run those TensorFlow operations (ops). \nSome observations : \nIf i'm enabling GPU only for image classification - App is getting very slow and lagging a lot.\nif i'm enabling GPU only for object detection - It is working some what fine. \n\n\nExpected Solution : \nWhen GPU is enable for image classification, It should work very smoothly as image classification TensorFlow operation is consuming a lot of memory than object detection operation.\n\n\nAdding a reference video for the same from Pixel 5 (image classification and object detection for both GPU is enabled)\n\nhttps://github.com/user-attachments/assets/fa93c0ef-4c95-415e-9565-d58dce585c0a"", 'created_at': datetime.datetime(2025, 1, 21, 11, 45, 47, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2025-01-06 12:32:15 UTC): Hi, @dayalatamai 
I apologize for the delayed response to confirm, have you followed this [official documentation ](https://ai.google.dev/edge/litert/android/gpu#enable_gpu_acceleration) ? If not please follow that and let us know is it resolving your issue  or not ?

Please make sure that models you are using are compatible with the GPU delegate. If models has unsupported ops it will fall back to CPU execution please check [GPU ML operations support](https://ai.google.dev/edge/litert/performance/gpu)

If issue still persists please help us with your Github repo along with complete steps to replicate same behavior from our end to investigate this issue from our end.

Thank you for your cooperation and patience.

dayalatamai (Issue Creator) on (2025-01-08 09:44:11 UTC): @gaikwadrahul8 , Thanks for your response. 
As I'm using tensorflow lite (already mentioned the url of library) for object detection and image classification.  
I want to perform these operation on GPU delegate and in tensorflow already have GPU delegate support so with that how can i merge the google GPU delegate. Adding Object detection and image classification classes for your reference, Please let me know where should i make changes for the same. Thank you once again.

class ObjectDetectorHelper(
    private var threshold: Float = 0.5f,
    private var numThreads: Int = 1,
    private var maxResults: Int = 5,
    private var currentDelegate: Int = 1,
    private var currentModel: Int = 0,
    private val context: Context,
    private val objectDetectorListener: DetectorListener?
) {

    private var objectDetector: ObjectDetector? = null

    init {
        setupObjectDetector()
    }

    fun clearObjectDetector() {
        objectDetector = null
    }
    private fun setupObjectDetector() {
        detectorScope.launch {
            val optionsBuilder =
                ObjectDetector.ObjectDetectorOptions.builder()
                    .setScoreThreshold(threshold)
                    .setMaxResults(maxResults)

            val baseOptionsBuilder = BaseOptions.builder()

            when (currentDelegate) {
                DELEGATE_CPU -> {
                    baseOptionsBuilder.setNumThreads(numThreads)
                }

                DELEGATE_GPU -> {
                    val compatList = CompatibilityList()
                    if (compatList.isDelegateSupportedOnThisDevice) {
                        val delegateOptions = compatList.bestOptionsForThisDevice
                        baseOptionsBuilder.useGpu()
                    } else {
                        baseOptionsBuilder.setNumThreads(4)
                    }
                }

                DELEGATE_NNAPI -> {
                    baseOptionsBuilder.setNumThreads(numThreads).useNnapi()
                }
            }

            optionsBuilder.setBaseOptions(baseOptionsBuilder.build())

            val modelName =
                when (currentModel) {
                    MODEL_MOBILENETV1 -> ""efficientdet_lite2_OD7.tflite""
                    else -> ""efficientdet_lite2_OD7.tflite""
                }

            try {
                objectDetector =
                    ObjectDetector.createFromFileAndOptions(
                        context,
                        modelName,
                        optionsBuilder.build()
                    )
            } catch (e: IllegalStateException) {
                objectDetectorListener?.onError(
                    ""Object detector failed to initialize. See error logs for details""
                )
            }
        }
    }

    fun detect(image: Bitmap, imageRotation: Int, localAspectRation: Int) {
        if (objectDetector == null) {
            setupObjectDetector()
        }

        detectorScope.launch {
            try {
                var inferenceTime = SystemClock.uptimeMillis()
                val imageProcessor =
                    ImageProcessor.Builder()
                        .add(Rot90Op(-imageRotation / 90))
                        .build()

                val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))

                val results1 = objectDetector?.detect(tensorImage)
                inferenceTime = SystemClock.uptimeMillis() - inferenceTime

                var results: MutableList<Detection>? = null

                if (results1!!.size > 1) {
                    for (i in 0 until results1.size) {
                        Utils.readLog(""objectDetector boundingBox1 Width - ${results1[i].boundingBox.width()}"")
                        Utils.readLog(""objectDetector boundingBox1 Height - ${results1[i].boundingBox.height()}"")
                        if (localAspectRation == AspectRatio.RATIO_4_3) {
                            if (results1[i].boundingBox.height() > 300 || results1[i].boundingBox.width() > 400) {
                                results = listOf(results1[i]).toMutableList()
                                break
                            }
                        } else {
                            if (results1[i].boundingBox.height() > 350) {
                                results = listOf(results1[i]).toMutableList()
                                break
                            }
                        }
                    }
                } else {
                    results = results1
                }

                objectDetectorListener?.onResults(
                    results,
                    inferenceTime,
                    tensorImage.height,
                    tensorImage.width
                )
            } catch (e: Exception) {
                objectDetectorListener?.onError(""Error detecting an image: ${e.message}"")
            }
        }
    }

    interface DetectorListener {
        fun onError(error: String){}
        fun onResults(
            results: MutableList<Detection>?,
            inferenceTime: Long,
            imageHeight: Int,
            imageWidth: Int
        )
    }

    companion object {
        const val DELEGATE_CPU = 0
        const val DELEGATE_GPU = 1
        const val DELEGATE_NNAPI = 2
        const val MODEL_MOBILENETV1 = 0
        private val detectorScope = CoroutineScope(newSingleThreadContext(""DetectorGpuThread""))
        private const val TAG = ""ObjectDetectorHelper""
    }
}


class ImageClassifierHelper(
    private var threshold: Float = 0.5f,
    private var numThreads: Int = 1,
    private var maxResults: Int = 1,
    private var currentDelegate: Int = 0,
    private val context: Context,
    private val imageClassifierListener: ClassifierListener?
) {
    private var imageClassifier: ImageClassifier? = null
    init {
        setupImageClassifier()
    }

    fun clearImageClassifier() {
        imageClassifier = null
    }

    private fun setupImageClassifier() {
        val optionsBuilder = ImageClassifier.ImageClassifierOptions.builder()
            .setScoreThreshold(threshold)
            .setMaxResults(maxResults)

        val baseOptionsBuilder = BaseOptions.builder()

        when (currentDelegate) {
            DELEGATE_CPU -> {
                baseOptionsBuilder.setNumThreads(numThreads)
            }
            DELEGATE_GPU -> {
                val compatList = CompatibilityList()
                if (compatList.isDelegateSupportedOnThisDevice) {
                    val delegateOptions = compatList.bestOptionsForThisDevice
                    baseOptionsBuilder.useGpu()
                } else {
                    baseOptionsBuilder.setNumThreads(4)
                }
            }
            DELEGATE_NNAPI -> {
                baseOptionsBuilder.setNumThreads(numThreads).useNnapi()
            }
        }
        optionsBuilder.setBaseOptions(baseOptionsBuilder.build())
        val modelName = ""reshaped_model_optimized_graph.tflite""
        try {
            imageClassifier =
                ImageClassifier.createFromFileAndOptions(context, modelName, optionsBuilder.build())
        } catch (e: IllegalStateException) {
            imageClassifierListener?.onError(
                ""Image classifier failed to initialize. See error logs for details""
            )
        }
    }

    fun classify(image: Bitmap, rotation: Int) {
        if (imageClassifier == null) {
            setupImageClassifier()
        }

        var inferenceTime = SystemClock.uptimeMillis()

        val imageProcessor =
            ImageProcessor.Builder()
                .build()

        val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))

        val imageProcessingOptions = ImageProcessingOptions.builder()
            .setOrientation(getOrientationFromRotation(rotation))
            .build()

        val results = imageClassifier?.classify(tensorImage, imageProcessingOptions)
        inferenceTime = SystemClock.uptimeMillis() - inferenceTime
        imageClassifierListener?.onResults(
            results,
            inferenceTime
        )
    }

    private fun getOrientationFromRotation(rotation: Int) : ImageProcessingOptions.Orientation {
        return when (rotation) {
            Surface.ROTATION_270 ->
                ImageProcessingOptions.Orientation.BOTTOM_RIGHT
            Surface.ROTATION_180 ->
                ImageProcessingOptions.Orientation.RIGHT_BOTTOM
            Surface.ROTATION_90 ->
                ImageProcessingOptions.Orientation.TOP_LEFT
            else ->
                ImageProcessingOptions.Orientation.RIGHT_TOP
        }
    }

    interface ClassifierListener {
        fun onError(error: String)
        fun onResults(
            results: List<Classifications>?,
            inferenceTime: Long
        )
    }

    companion object {
        const val DELEGATE_CPU = 0
        const val DELEGATE_GPU = 1
        const val DELEGATE_NNAPI = 2
        private const val TAG = ""ImageClassifierHelper""
    }
}

*********** Major Issue i'm facing here *************

If I'm trying to use Gpu in Image classification instead of object detection the entire camera screen is lagging(app is getting very slow) and Image is not classifying while classification we're getting the result : 
[Classifications{categories=[<Category ""unknown"" (displayName= score=0.99997544 index=28)>], headIndex=0}]

Category is 'unknown' 
Here, I want to use Gpu delegate for image classification as this is consuming most of the memory and at the last it is crashing in Most of Pixels device.
As, you suggested here https://ai.google.dev/edge/litert/android/gpu#kotlin
How to use the same in my code snippet.

gaikwadrahul8 (Assginee) on (2025-01-09 06:35:57 UTC): Hi, @dayalatamai
To confirm, To enable access to the GPU delegate, add `com.google.ai.edge.litert:litert-gpu-delegate-plugin` to your app's `build.gradle` file and also try to Add this line to your `build.gradle` `implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.10.0'` and see does it solve your issue or not ? 

Please refer this somewhat similar issues about GPU delegate https://github.com/tensorflow/tensorflow/issues/57934 and please refer this official [blog](https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html)

If possible could you please help us with your Github repo ( which includes all the dependancies) along with complete steps to replicate the same behavior from our end that will be more convenient to resolve the issue ?

Thank you for your cooperation and patience.

dayalatamai (Issue Creator) on (2025-01-09 14:59:15 UTC): Thanks, @gaikwadrahul8 for the update but com.google.ai.edge.litert:litert-gpu-delegate-plugin where do i need to add into build.gradle file and the another is already there implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.10.0' in build.gradle 
Still i'm getting the same if i'm trying to enable gpu for image classification App is lagging and image is not classifying for the same and it is crashing most of the time so sharing a crash report for the same.
<img width=""1262"" alt=""Screenshot 2025-01-09 at 8 27 22PM"" src=""https://github.com/user-attachments/assets/910f140c-7550-4278-a995-f5c60e6de2ba"" />
<img width=""1113"" alt=""Screenshot 2025-01-09 at 8 26 24PM"" src=""https://github.com/user-attachments/assets/e02e415f-1a9b-43b9-bbc0-298aef5235e4"" />


As you have requested for git repo, i do not have the access to share and it is private repo so. Meanwhile i can share my build.gradle file with you for the reference.

******** build.gradle *********

apply plugin: 'com.android.application'
apply plugin: 'com.google.gms.google-services'
apply plugin: 'com.google.firebase.crashlytics'
apply plugin: 'kotlin-android'
apply plugin: 'kotlinx-serialization'
apply plugin: 'androidx.navigation.safeargs'

android {
    ndkVersion ""25.1.8937393""
    signingConfigs {
        config {
            keyAlias 'video inventory mobile manager'
            keyPassword 'vimmapp'
            storeFile file('/Users/nunc-it-pc-108/Documents/Android/VIMM_STILL_PHOTOS/vimmvidcomandroid/VIMM360/vimm.keystore')
            storePassword 'vimmapp'
        }
    }
    namespace ""com.tab.and2""
    compileSdk 34
    buildToolsVersion '34.0.0'
    defaultConfig {
        applicationId ""com.tab.and2""
        minSdkVersion 28
        targetSdkVersion 34
        versionCode 65
        versionName ""4.5.5.01.08.2025""
        multiDexEnabled true
        resConfig ""en""
        ndk {
            abiFilters 'armeabi-v7a', 'arm64-v8a'
        }
    }
    splits {
        abi {
            include ""armeabi-v7a"", ""arm64-v8a""
        }
    }
    applicationVariants.configureEach { variant ->
        variant.outputs.each { output ->
            def versionCodes = [""armeabi-v7a"": 1, ""arm64-v8a"": 2]
            def abi = output.getFilter(com.android.build.OutputFile.ABI)
            if (abi != null) {  // null for the universal-debug, universal-release variants
                output.versionCodeOverride =
                        versionCodes.get(abi) * 1048576 + defaultConfig.versionCode
            }
        }
    }
    def buildType // Your variable
    android.applicationVariants.configureEach { variant ->
        variant.outputs.configureEach {
            buildType = variant.buildType.name // Sets the current build type
            outputFileName = ""VIMM360_$buildType(${variant.versionCode}v_${variant.versionName})_"" + new Date().format('hh_mmaa') + "".apk""
        }
    }

    dexOptions {
        preDexLibraries = true
        javaMaxHeapSize ""4g"" // 2g should be also OK
    }
    lintOptions {
        checkReleaseBuilds false
        abortOnError false
    }
    packagingOptions {
//        exclude 'META-INF/DEPENDENCIES'
        exclude(""META-INF/DEPENDENCIES"")
        exclude(""META-INF/LICENSE"")
        exclude(""META-INF/LICENSE.txt"")
        exclude(""META-INF/license.txt"")
        exclude(""META-INF/NOTICE"")
        exclude(""META-INF/NOTICE.txt"")
        exclude(""META-INF/notice.txt"")
        exclude(""META-INF/ASL2.0"")
        exclude(""META-INF/*.kotlin_module"")
    }

    buildTypes {
        release {
            debuggable false
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
            signingConfig signingConfigs.config

            firebaseCrashlytics {
                mappingFileUploadEnabled false
            }
        }
        debug {
            applicationIdSuffix ''
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
            firebaseCrashlytics {
                mappingFileUploadEnabled false
            }
        }
    }
    buildFeatures {
        compose true
        buildConfig true
        viewBinding true
    }
    composeOptions {
        kotlinCompilerExtensionVersion compose_version
    }

    compileOptions {
        sourceCompatibility JavaVersion.VERSION_17
        targetCompatibility JavaVersion.VERSION_17
    }

    kotlinOptions {
        jvmTarget = '17'
    }

    sourceSets {
        main {
            jniLibs.srcDirs = ['src/main/jniLibs']
        }
    }
}

dependencies {
    implementation 'androidx.multidex:multidex:2.0.1'
    implementation 'com.google.android.gms:play-services-vision:20.1.3'
    implementation 'me.dm7.barcodescanner:zbar:1.9.8'
    implementation files('libs/environment3.jar')
    implementation 'androidx.legacy:legacy-support-v13:1.0.0'
    implementation 'androidx.recyclerview:recyclerview:1.3.2'
    implementation 'androidx.appcompat:appcompat:1.6.1'
    implementation 'androidx.cardview:cardview:1.0.0'
    implementation 'com.google.apis:google-api-services-storage:v1-rev99-1.22.0'
    implementation 'com.github.bumptech.glide:glide:4.16.0'
    implementation 'androidx.constraintlayout:constraintlayout:2.1.4'
    annotationProcessor 'com.github.bumptech.glide:compiler:4.14.2'
    implementation 'com.arthenica:mobile-ffmpeg-full-gpl:4.4'
    implementation ""org.jetbrains.kotlin:kotlin-stdlib:1.8.10""
    implementation(platform(""org.jetbrains.kotlin:kotlin-bom:1.8.10""))
    implementation ""org.jetbrains.kotlinx:kotlinx-serialization-json:1.5.0""
    implementation ""org.jetbrains.kotlin:kotlin-stdlib:1.8.20""
    implementation ""androidx.fragment:fragment-ktx:1.5.7""
    implementation 'com.google.ar:core:1.36.0'
    implementation ""androidx.window:window:1.3.0""
    implementation 'com.squareup.okhttp3:okhttp:4.9.1'
    implementation 'com.google.code.gson:gson:2.8.9'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.4.0'
    androidTestImplementation 'androidx.test.ext:junit:1.1.3'
    testImplementation 'junit:junit:4.13.2'
    implementation 'com.google.android.material:material:1.9.0'
    implementation 'com.amazonaws:aws-android-sdk-core:2.7.7'
    implementation 'com.amazonaws:aws-android-sdk-s3:2.7.7'
    def lifecycle_version = ""2.7.0""
    implementation ""com.ricoh360.thetaclient:theta-client:1.10.2""
    implementation ""androidx.core:core-ktx:1.9.0""
    //noinspection GradleDependency
    implementation""org.jetbrains.kotlin:kotlin-stdlib-jdk8:1.8.0""
    implementation ""androidx.compose.ui:ui:$compose_version""
    implementation ""androidx.compose.material:material:$compose_version""
    implementation ""androidx.compose.ui:ui-tooling-preview:$compose_version""
    implementation ""androidx.lifecycle:lifecycle-runtime-ktx:$lifecycle_version""
    implementation ""androidx.lifecycle:lifecycle-viewmodel-ktx:$lifecycle_version""
    implementation ""androidx.lifecycle:lifecycle-viewmodel-compose:$lifecycle_version""
    implementation 'androidx.activity:activity-compose:1.9.3'
    implementation ""androidx.ui:ui-framework:0.1.0-dev10""
    implementation ""androidx.navigation:navigation-compose:2.7.7""
    implementation 'androidx.webkit:webkit:1.10.0'
    implementation ""org.jetbrains.kotlinx:kotlinx-coroutines-android:$coroutines_version""
    implementation 'org.jetbrains.kotlinx:kotlinx-serialization-json:1.6.0'
    implementation 'com.jakewharton.timber:timber:5.0.1'
    implementation 'io.coil-kt:coil-compose:2.2.2'
    implementation ""io.ktor:ktor-client-cio:2.3.9""
    implementation ""androidx.activity:activity-ktx:$activity_version""

    testImplementation 'org.junit.jupiter:junit-jupiter:5.9.0'
    testImplementation ""org.jetbrains.kotlinx:kotlinx-coroutines-test:$coroutines_version""
    testImplementation 'org.junit.jupiter:junit-jupiter'
    androidTestImplementation 'androidx.test.ext:junit:1.1.5'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.5.1'
    androidTestImplementation ""androidx.compose.ui:ui-test-junit4:$compose_version""
    debugImplementation ""androidx.compose.ui:ui-tooling:$compose_version""
    debugImplementation ""androidx.compose.ui:ui-test-manifest:$compose_version""
    implementation platform('com.google.firebase:firebase-bom:33.4.0')
    implementation(""com.google.firebase:firebase-crashlytics"")
    implementation(""com.google.firebase:firebase-analytics"")
    implementation 'androidx.localbroadcastmanager:localbroadcastmanager:1.1.0'
    implementation ""androidx.navigation:navigation-fragment-ktx:$nav_version""
    implementation ""androidx.navigation:navigation-ui-ktx:$nav_version""
    def camerax_version = '1.3.4' //'1.1.0-beta03'
    implementation ""androidx.camera:camera-core:$camerax_version""
    implementation ""androidx.camera:camera-camera2:$camerax_version""
    implementation ""androidx.camera:camera-lifecycle:$camerax_version""
    implementation ""androidx.camera:camera-view:$camerax_version""
    implementation ""androidx.camera:camera-extensions:$camerax_version""
    implementation 'com.google.android.gms:play-services-tflite-java:16.0.1'
    implementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'
    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.12.0'

    implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.10.0'
    implementation 'org.tensorflow:tensorflow-lite:2.12.0'
    implementation 'org.tensorflow:tensorflow-lite-support:0.4.1'

    implementation 'com.google.guava:listenablefuture:9999.0-empty-to-avoid-conflict-with-guava'
    configurations {
        all*.exclude group: 'com.google.guava', module: 'listenablefuture'
        all*.exclude module:'guava-jdk5'
    }
}

gaikwadrahul8 (Assginee) on (2025-01-15 11:06:51 UTC): Hi, @dayalatamai 
As far I know not all TensorFlow operations (ops) are supported by the GPU delegate. If a model includes unsupported ops TensorFlow Lite will execute those ops on the CPU instead which can lead to suboptimal performance and increased inference time due to the overhead of CPU-GPU synchronization, please refer this official documentation for [GPU ML operations support](https://ai.google.dev/edge/litert/performance/gpu)

Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate so if you don't mind could you please give it try on different devices and see, Is it working as expected or not ? certain models may inherently be less compatible with GPU acceleration due to their architecture or specific operations used within them so please make sure that used models support GPU acceleration

If possible please try it by Installing the required libraries in `build.gradle` like below :

```
implementation(""org.tensorflow:tensorflow-lite:+"")
implementation(""org.tensorflow:tensorflow-lite-gpu:+"")
implementation(""org.tensorflow:tensorflow-lite-gpu-api:+"")
implementation(""org.tensorflow:tensorflow-lite-gpu-delegate-plugin:+"")
implementation(""org.tensorflow:tensorflow-lite-support:+"")
```

For GPU Delegate settings import below module :

```
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.gpu.CompatibilityList
import org.tensorflow.lite.gpu.GpuDelegate

```
Set `GPUDelegate` in the models Interpreter options like below :

```
val compatList = CompatibilityList()

val options = Interpreter.Options().apply{
    if(compatList.isDelegateSupportedOnThisDevice){
        // if the device has a supported GPU, add the GPU delegate
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        // if the GPU is not supported, run on 4 threads
        this.setNumThreads(4)
    }
}

interpreter = Interpreter(model, options)

```

Thank you for your cooperation and patience.

dayalatamai (Issue Creator) on (2025-01-21 11:45:47 UTC): Hey, @gaikwadrahul8 

Thank you for the response. As you said *Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate* Right! I've checked most of the device like samsung(s10,s20,s23 ultra,s22,m31) and pixels(2xl,3a,5,7).
If i'm enabling  **private var currentDelegate: Int = 1** for both image classification and object detection on given code snippet and checking which supports GPU delegate in above devices, App is getting very slow (getting lag) and in some pixel devices it is not detecting any object and not classifying any image or getting crash sometimes. 
If any device does not support GPU then, i'm using 4 threads to run those TensorFlow operations (ops). 
Some observations : 
If i'm enabling GPU only for image classification - App is getting very slow and lagging a lot.
if i'm enabling GPU only for object detection - It is working some what fine. 


Expected Solution : 
When GPU is enable for image classification, It should work very smoothly as image classification TensorFlow operation is consuming a lot of memory than object detection operation.


Adding a reference video for the same from Pixel 5 (image classification and object detection for both GPU is enabled)

https://github.com/user-attachments/assets/fa93c0ef-4c95-415e-9565-d58dce585c0a

"
2767101996,issue,closed,completed,Error occured when compling TensorFlow C++ interface with Bazel,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.1

### GCC/compiler version

8.9

### CUDA/cuDNN version

12.2 / 8.9.6.50

### GPU model and memory

_No response_

### Current behavior?

I want to install Tensorflow C++ interface, and have followed the version matching and procedure using Bazel. Previously I have encountered the error for rules_python file. The corresponding file has been downloaded and the corresponding url link has been revised in the WORKSPACE for this file. But when fetching repository @pypi and @python_x86_64-unknown-linux-gnu, the following error occured. But I cannot find the url or the command for these two file that I can revise the link with the location of the corresponding file stated in the error information.

The configuration information is as follows:
./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /home/workspace/anaconda3/bin/python3]: 

Found possible Python library paths:
  /home/workspace/anaconda3/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/home/workspace/anaconda3/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 12.2 in:
    /usr/local/cuda-12.2/targets/x86_64-linux/lib
    /usr/local/cuda-12.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-12.2/targets/x86_64-linux/lib
    /usr/local/cuda-12.2/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]: 

Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished


### Standalone code to reproduce the issue

```shell
bazel build --config=opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so
```


### Relevant log output

```shell
WARNING: Download from https://github.com/indygreg/python-build-standalone/releases/download/20231002/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz failed: class java.io.IOException connect timed out
ERROR: An error occurred during the fetch of repository 'python_x86_64-unknown-linux-gnu':
   Traceback (most recent call last):
	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl
		rctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/python-build-standalone/releases/download/20231002/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64-unknown-linux-gnu/temp11172848094686838309/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz: connect timed out
ERROR: /home/workspace/Desktop/software/tensorflow-2.15.0/WORKSPACE:36:27: fetching python_repository rule //external:python_x86_64-unknown-linux-gnu: Traceback (most recent call last):
	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl
		rctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/python-build-standalone/releases/download/20231002/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64-unknown-linux-gnu/temp11172848094686838309/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz: connect timed out
ERROR: Error computing the main repository mapping: Encountered error while reading extension file 'requirements.bzl': no such package '@pypi//': no such package '@python_x86_64-unknown-linux-gnu//': java.io.IOException: Error downloading [https://github.com/indygreg/python-build-standalone/releases/download/20231002/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64-unknown-linux-gnu/temp11172848094686838309/cpython-3.10.13+20231002-x86_64-unknown-linux-gnu-install_only.tar.gz: connect timed out
Loading: 
    Fetching repository @pypi; Restarting. 55s
```
",Myre29,2025-01-03 07:21:59+00:00,['tilakrayal'],2025-01-21 01:59:28+00:00,2025-01-21 01:59:24+00:00,https://github.com/tensorflow/tensorflow/issues/84042,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2573024930, 'issue_id': 2767101996, 'author': 'tilakrayal', 'body': '@Myre29,\r\nAccording to the official document, for tensorflow v2.15, the compiler is Clang 16.0.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.\r\n\r\nhttps://www.tensorflow.org/install/source#gpu\r\n\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 6, 12, 35, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588613708, 'issue_id': 2767101996, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 14, 1, 58, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603473904, 'issue_id': 2767101996, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 21, 1, 59, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603473973, 'issue_id': 2767101996, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84042"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84042"">No</a>', 'created_at': datetime.datetime(2025, 1, 21, 1, 59, 26, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2025-01-06 12:35:24 UTC): @Myre29,
According to the official document, for tensorflow v2.15, the compiler is Clang 16.0.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.

https://www.tensorflow.org/install/source#gpu

Thank you!

github-actions[bot] on (2025-01-14 01:58:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-21 01:59:24 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-21 01:59:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84042"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84042"">No</a>

"
2766906966,issue,open,,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

3.7

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed.
![image](https://github.com/user-attachments/assets/e4b7b56d-2589-41fe-8395-1743c941dd49)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
graph_def = tf.GraphDef()
```


### Relevant log output

_No response_",ZerryNi,2025-01-03 03:03:30+00:00,['Venkat6871'],2025-02-03 06:20:03+00:00,,https://github.com/tensorflow/tensorflow/issues/84039,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('TF 2.4', 'for issues related to TF 2.4')]","[{'comment_id': 2568954561, 'issue_id': 2766906966, 'author': 'Venkat6871', 'body': ""Hi **@ZerryNi** ,\r\nWelcome to TensorFlow, and thank you for raising your concern here. The error you are facing occurs because 'GraphDef' is not directly accessible under the TensorFlow module in TensorFlow 2.x. To use 'GraphDef,' you need to import it from compat.v1. In TensorFlow 2.x, 'GraphDef' is located within the compat.v1 module to support backward compatibility with TensorFlow 1.x code. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/705b37ef4c12a05d390c62babcd7c5d7/84039_tf_2-18-0-v.ipynb) here for your reference. \r\nAdditionally, we recommend using the latest versions of TensorFlow for better performance and compatibility.\r\nThank you!"", 'created_at': datetime.datetime(2025, 1, 3, 9, 46, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568974509, 'issue_id': 2766906966, 'author': 'ZerryNi', 'body': ""Hi,l've used import tensorflow.compat.v1 as tf ,tf.disable_v2_behavior(),but the system still rreports that the incepion_v3_2016_08_28_frozem.pb file is missing,and the same error is reported after I download the file.\r\n\r\n![error-tensorflow](https://github.com/user-attachments/assets/fb122c41-eba4-497c-83ab-176f776eed3a)"", 'created_at': datetime.datetime(2025, 1, 3, 10, 2, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579286370, 'issue_id': 2766906966, 'author': 'Venkat6871', 'body': 'Hi **@ZerryNi** ,\r\nApologies for the delay, and thank you for your patience. It appears that you are still using an older version. Could you please upgrade to the latest TensorFlow version? Let us know if the issue still persists.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 9, 6, 43, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579314542, 'issue_id': 2766906966, 'author': 'ZerryNi', 'body': 'Hi,thank you vert much for your reply,but we require tensorflow 3.4.1.', 'created_at': datetime.datetime(2025, 1, 9, 7, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630069677, 'issue_id': 2766906966, 'author': 'Venkat6871', 'body': 'Hi **@ZerryNi** ,\nApologies for the delay, and thanks for your patience. We do not support older or deprecated versions. Please migrate to the latest version for better results. Here, I am providing documentation for [migration](https://www.tensorflow.org/guide/migrate).\nThank you!', 'created_at': datetime.datetime(2025, 2, 3, 6, 19, 57, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-03 09:46:07 UTC): Hi **@ZerryNi** ,
Welcome to TensorFlow, and thank you for raising your concern here. The error you are facing occurs because 'GraphDef' is not directly accessible under the TensorFlow module in TensorFlow 2.x. To use 'GraphDef,' you need to import it from compat.v1. In TensorFlow 2.x, 'GraphDef' is located within the compat.v1 module to support backward compatibility with TensorFlow 1.x code. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/705b37ef4c12a05d390c62babcd7c5d7/84039_tf_2-18-0-v.ipynb) here for your reference. 
Additionally, we recommend using the latest versions of TensorFlow for better performance and compatibility.
Thank you!

ZerryNi (Issue Creator) on (2025-01-03 10:02:34 UTC): Hi,l've used import tensorflow.compat.v1 as tf ,tf.disable_v2_behavior(),but the system still rreports that the incepion_v3_2016_08_28_frozem.pb file is missing,and the same error is reported after I download the file.

![error-tensorflow](https://github.com/user-attachments/assets/fb122c41-eba4-497c-83ab-176f776eed3a)

Venkat6871 (Assginee) on (2025-01-09 06:43:36 UTC): Hi **@ZerryNi** ,
Apologies for the delay, and thank you for your patience. It appears that you are still using an older version. Could you please upgrade to the latest TensorFlow version? Let us know if the issue still persists.
Thank you!

ZerryNi (Issue Creator) on (2025-01-09 07:08:26 UTC): Hi,thank you vert much for your reply,but we require tensorflow 3.4.1.

Venkat6871 (Assginee) on (2025-02-03 06:19:57 UTC): Hi **@ZerryNi** ,
Apologies for the delay, and thanks for your patience. We do not support older or deprecated versions. Please migrate to the latest version for better results. Here, I am providing documentation for [migration](https://www.tensorflow.org/guide/migrate).
Thank you!

"
2766651959,issue,closed,not_planned,TFLITE NMS kernel Inconsistent Outputs and Out of Memory issues,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2, tf 2.18, tf 2.19.0-dev2024122

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22

### Mobile device

Android

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The TFLITE NMS kernel output is not same as Tensorflow NMS output. Although the TFLITE NMS is a dynamic output shape layer, it is _**appending 0's**_ in the ""selected_indices"" output till ""max_output_size"", defeating the purpose of dynamic output.

**TFLITE NMS output must identically match with TF NMS output.**

For large ""max_output_size"", the TFLITE NMS results in super slow computation and many times it goes Out-of-memory on Android devices. The subsequent **Gather** ops, after NMS suffers heavily due to appended 0's in the TFLITE NMS output.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/non_max_suppression.cc#L190C44-L190C59 

**Requesting to fix this behavior and ensure both TF and TFLITE NMS output are exactly same.**

![image](https://github.com/user-attachments/assets/6fe5e2eb-b2bf-45f7-8eb8-7a5900f5bb28)



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Test inputs from : https://github.com/onnx/onnx/blob/main/docs/Operators.md#NonMaxSuppression : nonmaxsuppression_limit_output_size

boxes = np.array(
    [
        [0.0, 0.0, 1.0, 1.0],
        [0.0, 0.1, 1.0, 1.1],
        [0.0, -0.1, 1.0, 0.9],
        [0.0, 10.0, 1.0, 11.0],
        [0.0, 10.1, 1.0, 11.1],
        [0.0, 100.0, 1.0, 101.0],
    ]
).astype(np.float32)
scores = np.array([0.9, 0.75, 0.6, 0.95, 0.5, 0.3]).astype(np.float32)

import tensorflow as tf

max_output_size = tf.constant(tf.int32.max, dtype=tf.int32)
iou_threshold = 0.5

selected_indices = tf.image.non_max_suppression(
    boxes, scores, max_output_size, iou_threshold
)
print(selected_indices)    # returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32)

@tf.function(input_signature=[
    tf.TensorSpec(shape=[6, 4], dtype=tf.float32),
    tf.TensorSpec(shape=[6], dtype=tf.float32),
])
def nms_function(boxes, scores):
    return tf.image.non_max_suppression(boxes, scores, max_output_size=tf.constant(tf.int32.max, dtype=tf.int32), iou_threshold=0.5)

concrete_function = nms_function.get_concrete_function()
print(concrete_function(boxes, scores))    # returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32)


converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])
tflite_model = converter.convert()

with open('test_nms.tflite', 'wb') as f:
    f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path='test_nms.tflite')
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], boxes)
interpreter.set_tensor(input_details[1]['index'], scores)

interpreter.invoke()

selected_indices = interpreter.get_tensor(output_details[0]['index'])
print(selected_indices)    # returns incorrect output appended with 0's : [3 0 5 ... 0 0 0]
print(selected_indices.shape)    # incorrect output of shape : (2147483647,)

# above causes OOM error
```


### Relevant log output

```shell
TF output = tf.Tensor([3 0 5], shape=(3,), dtype=int32)

TF Lite output = [3 0 5 ... 0 0 0]  ; shape = (2147483647,)
```
",sdp009,2025-01-02 21:14:57+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-22 19:59:29+00:00,2025-01-22 19:59:26+00:00,https://github.com/tensorflow/tensorflow/issues/84033,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2568392544, 'issue_id': 2766651959, 'author': 'sdp009', 'body': 'Please add label ""comp:lite""', 'created_at': datetime.datetime(2025, 1, 2, 21, 20, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572812534, 'issue_id': 2766651959, 'author': 'gaikwadrahul8', 'body': ""Hi, @sdp009 \r\nI apologize for the delayed response, I tried to run provided code in Google colab but I'm getting `Your session crashed after using all available RAM.` so that is due to OOM issue for reference I've added [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/964d0d400c3c474441d2981de546688a/tflite-issue-83754.ipynb) and output log screenshot below so we'll have to dig more into this issue and update you\r\n\r\n![image](https://github.com/user-attachments/assets/354efe32-6673-4d40-8ec1-50a645ff3933)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 6, 10, 24, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573710288, 'issue_id': 2766651959, 'author': 'sdp009', 'body': ""For easier debugging, please reduce the 'max_output_size' to a smaller value to avoid OOM issues.\r\n\r\nBut in actual practice, there are some scenarios where higher 'max_output_size' did caused OOM issues on resource constraint Android devices."", 'created_at': datetime.datetime(2025, 1, 6, 18, 50, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578962210, 'issue_id': 2766651959, 'author': 'sdp009', 'body': 'Hi @gaikwadrahul8 , did you got chance to inspect it further ? Thanks.', 'created_at': datetime.datetime(2025, 1, 9, 0, 43, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594668439, 'issue_id': 2766651959, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2025, 1, 16, 6, 55, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608145125, 'issue_id': 2766651959, 'author': 'pkgoogle', 'body': 'Hi @sdp009, we will be moving this to [LiteRT](https://github.com/google-ai-edge/litert). Please follow progress there.', 'created_at': datetime.datetime(2025, 1, 22, 19, 57, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608148394, 'issue_id': 2766651959, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84033"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84033"">No</a>', 'created_at': datetime.datetime(2025, 1, 22, 19, 59, 28, tzinfo=datetime.timezone.utc)}]","sdp009 (Issue Creator) on (2025-01-02 21:20:34 UTC): Please add label ""comp:lite""

gaikwadrahul8 (Assginee) on (2025-01-06 10:24:52 UTC): Hi, @sdp009 
I apologize for the delayed response, I tried to run provided code in Google colab but I'm getting `Your session crashed after using all available RAM.` so that is due to OOM issue for reference I've added [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/964d0d400c3c474441d2981de546688a/tflite-issue-83754.ipynb) and output log screenshot below so we'll have to dig more into this issue and update you

![image](https://github.com/user-attachments/assets/354efe32-6673-4d40-8ec1-50a645ff3933)

Thank you for your cooperation and patience.

sdp009 (Issue Creator) on (2025-01-06 18:50:11 UTC): For easier debugging, please reduce the 'max_output_size' to a smaller value to avoid OOM issues.

But in actual practice, there are some scenarios where higher 'max_output_size' did caused OOM issues on resource constraint Android devices.

sdp009 (Issue Creator) on (2025-01-09 00:43:11 UTC): Hi @gaikwadrahul8 , did you got chance to inspect it further ? Thanks.

gaikwadrahul8 (Assginee) on (2025-01-16 06:55:03 UTC): Hi, @pkgoogle
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2025-01-22 19:57:41 UTC): Hi @sdp009, we will be moving this to [LiteRT](https://github.com/google-ai-edge/litert). Please follow progress there.

google-ml-butler[bot] on (2025-01-22 19:59:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84033"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84033"">No</a>

"
2766207607,issue,closed,completed,Tensorflow BackupAndRestore method does not work,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

conda-forge 

### TensorFlow version

2.17

### Custom code

No

### OS platform and distribution

Linux RHEL8

### Mobile device

_No response_

### Python version

3.11.8

### Bazel version

_No response_

### GCC/compiler version

GCC 11.2.8

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

BackupAndRestore example code does not work

### Standalone code to reproduce the issue

```shell
import keras
import numpy as np

class InterruptingCallback(keras.callbacks.Callback):
   def on_epoch_begin(self, epoch, logs=None):
     if epoch == 4:
       raise RuntimeError('Interrupting!')
callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")
model = keras.models.Sequential([keras.layers.Dense(10)])
model.compile(keras.optimizers.SGD(), loss='mse')
try:
   model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
             batch_size=1, callbacks=[callback, InterruptingCallback()],
             verbose=0)
except Exception as e:
   print(e)
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                     epochs=10, batch_size=1, callbacks=[callback],
                     verbose=0)
len(history.history['loss'])
```


### Relevant log output

```shell
ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data.
```
",antipisa,2025-01-02 15:20:08+00:00,['Venkat6871'],2025-01-30 23:32:18+00:00,2025-01-10 13:07:41+00:00,https://github.com/tensorflow/tensorflow/issues/84027,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2568638096, 'issue_id': 2766207607, 'author': 'micedevai', 'body': 'The error message you\'re encountering:\r\n\r\n```\r\nValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data.\r\n```\r\n\r\nindicates that the `BackupAndRestore` callback expects the model to be built before calling `fit()`, but in your case, the model is not explicitly built before the training loop starts.\r\n\r\n### Understanding the Problem:\r\n- **`BackupAndRestore` callback** requires the model to be ""built"" before starting training. The model needs to know the input shapes and architecture in order to correctly manage the backup and restoration processes.\r\n- **Model Building**: When you define a `Sequential` model without specifying input shapes, TensorFlow won\'t know the input shape until data is passed to the model. Therefore, you need to either specify the input shape when defining the model or pass a batch of data to the model before calling `fit()`.\r\n\r\n### Solution 1: Define the Input Shape in the Model\r\n\r\nYou can explicitly define the input shape when creating the model, which ensures the model is ""built"" before training starts:\r\n\r\n```python\r\nimport keras\r\nimport numpy as np\r\n\r\nclass InterruptingCallback(keras.callbacks.Callback):\r\n   def on_epoch_begin(self, epoch, logs=None):\r\n     if epoch == 4:\r\n       raise RuntimeError(\'Interrupting!\')\r\n\r\ncallback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")\r\n\r\n# Define the model with an explicit input shape\r\nmodel = keras.models.Sequential([\r\n    keras.layers.InputLayer(input_shape=(20,)),  # Specify the input shape\r\n    keras.layers.Dense(10)\r\n])\r\n\r\nmodel.compile(keras.optimizers.SGD(), loss=\'mse\')\r\n\r\n# Now the model is built before training\r\ntry:\r\n    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,\r\n              batch_size=1, callbacks=[callback, InterruptingCallback()],\r\n              verbose=0)\r\nexcept Exception as e:\r\n    print(e)\r\n\r\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\r\n                    epochs=10, batch_size=1, callbacks=[callback],\r\n                    verbose=0)\r\n\r\nprint(len(history.history[\'loss\']))\r\n```\r\n\r\n### Explanation:\r\n- **`InputLayer`**: By adding the `InputLayer` with an explicit `input_shape=(20,)`, you\'re telling Keras the expected shape of the input data, which ensures that the model is built before calling `fit()`.\r\n\r\n### Solution 2: Build the Model Before Calling `fit()`\r\n\r\nAlternatively, you can use a batch of data to build the model explicitly before training. This can be done using the `model.build()` method:\r\n\r\n```python\r\nimport keras\r\nimport numpy as np\r\n\r\nclass InterruptingCallback(keras.callbacks.Callback):\r\n   def on_epoch_begin(self, epoch, logs=None):\r\n     if epoch == 4:\r\n       raise RuntimeError(\'Interrupting!\')\r\n\r\ncallback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")\r\n\r\n# Define the model without specifying input shape\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Dense(10)\r\n])\r\n\r\nmodel.compile(keras.optimizers.SGD(), loss=\'mse\')\r\n\r\n# Build the model by passing a batch of data\r\nmodel.build(input_shape=(None, 20))  # Here, 20 is the number of features in your input data\r\n\r\n# Now the model is built before calling fit\r\ntry:\r\n    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,\r\n              batch_size=1, callbacks=[callback, InterruptingCallback()],\r\n              verbose=0)\r\nexcept Exception as e:\r\n    print(e)\r\n\r\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\r\n                    epochs=10, batch_size=1, callbacks=[callback],\r\n                    verbose=0)\r\n\r\nprint(len(history.history[\'loss\']))\r\n```\r\n\r\n### Explanation:\r\n- **`model.build()`**: This explicitly builds the model by providing the `input_shape`. After this call, the model is ready for training, and the `BackupAndRestore` callback will work correctly.\r\n\r\nTo fix the issue, you need to ensure that the model is built (either by specifying the input shape or by explicitly calling `model.build()`) before invoking `fit()`. Both solutions will address the error and allow the `BackupAndRestore` callback to function as expected.', 'created_at': datetime.datetime(2025, 1, 3, 3, 4, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568781536, 'issue_id': 2766207607, 'author': 'Venkat6871', 'body': 'Hi **@antipisa** ,\r\nThanks for raising your concern here. The raised [PR](https://github.com/keras-team/keras/pull/20714) has been merged. Could you please check and let us know if the issue still persists? \r\nThank you!', 'created_at': datetime.datetime(2025, 1, 3, 7, 13, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569269975, 'issue_id': 2766207607, 'author': 'antipisa', 'body': '@Venkat6871 \r\nDoes Solution 1 work when the input layer is a normalization layer? E.g for this example\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nx = tf.random.uniform((100, 1))\r\ny = tf.random.uniform((100, 1))\r\nz = tf.random.uniform((100, 1))\r\n\r\nxyz = tf.concat([x, y, z], 1)\r\n\r\nhorsepower_normalizer = tf.keras.layers.Normalization(input_shape=(3,), axis=-1)\r\nhorsepower_normalizer.adapt(xyz)\r\n\r\nhorsepower_model = tf.keras.models.Sequential([\r\n    horsepower_normalizer,\r\n    tf.keras.layers.Dense(units=1)\r\n])\r\n\r\nhorsepower_model(xyz)\r\n\r\n```\r\n\r\nEDIT: This raises a UserWarning about using input_shape in a layer. Is there a better way?', 'created_at': datetime.datetime(2025, 1, 3, 14, 5, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569634219, 'issue_id': 2766207607, 'author': 'antipisa', 'body': ""@Venkat6871 \r\nSolution 1 raises `ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: (<InputLayer, name='input_layer_6, built=True>), (of type <class='tuple'>) ' `"", 'created_at': datetime.datetime(2025, 1, 3, 18, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579453595, 'issue_id': 2766207607, 'author': 'Venkat6871', 'body': 'Hi **@antipisa** ,\r\nApologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions, but I am not facing any issues. Could you please check which warning you are encountering? Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3c9e9d89bc8c9adbb8177cb409d18080/84027_tf_2-18-0-2-17-0-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 9, 8, 41, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581212416, 'issue_id': 2766207607, 'author': 'antipisa', 'body': 'Hi @Venkat6871 , the problem is fixed if one imports the following way:\r\n\r\n```\r\nfrom tensorflow import keras\r\nfrom keras import layers\r\nfrom keras.layers import InputLayer, Dense\r\n```', 'created_at': datetime.datetime(2025, 1, 9, 20, 44, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581704882, 'issue_id': 2766207607, 'author': 'Venkat6871', 'body': 'Hi **@antipisa** ,\r\nGlad to see your issue is resolved! Please feel free to close this issue if everything is working as expected.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 10, 4, 11, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582675462, 'issue_id': 2766207607, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84027"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84027"">No</a>', 'created_at': datetime.datetime(2025, 1, 10, 13, 7, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625931385, 'issue_id': 2766207607, 'author': 'antipisa', 'body': 'Hi @Venkat6871 ,  May I ask why the discrepancy when specifying input_shape in an Input layer vs when calling model.build?\n\n```\n\nmodel.build(input_shape=(None, 20))  # Here, 20 is the number of features in your input data\n# input_shape is (None,  20) versus (20, None) in the below\nkeras.layers.InputLayer(input_shape=(20,)),  # Specify the input shape\n\n\n```', 'created_at': datetime.datetime(2025, 1, 30, 23, 32, 17, tzinfo=datetime.timezone.utc)}]","micedevai on (2025-01-03 03:04:43 UTC): The error message you're encountering:

```
ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data.
```

indicates that the `BackupAndRestore` callback expects the model to be built before calling `fit()`, but in your case, the model is not explicitly built before the training loop starts.

### Understanding the Problem:
- **`BackupAndRestore` callback** requires the model to be ""built"" before starting training. The model needs to know the input shapes and architecture in order to correctly manage the backup and restoration processes.
- **Model Building**: When you define a `Sequential` model without specifying input shapes, TensorFlow won't know the input shape until data is passed to the model. Therefore, you need to either specify the input shape when defining the model or pass a batch of data to the model before calling `fit()`.

### Solution 1: Define the Input Shape in the Model

You can explicitly define the input shape when creating the model, which ensures the model is ""built"" before training starts:

```python
import keras
import numpy as np

class InterruptingCallback(keras.callbacks.Callback):
   def on_epoch_begin(self, epoch, logs=None):
     if epoch == 4:
       raise RuntimeError('Interrupting!')

callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")

# Define the model with an explicit input shape
model = keras.models.Sequential([
    keras.layers.InputLayer(input_shape=(20,)),  # Specify the input shape
    keras.layers.Dense(10)
])

model.compile(keras.optimizers.SGD(), loss='mse')

# Now the model is built before training
try:
    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
              batch_size=1, callbacks=[callback, InterruptingCallback()],
              verbose=0)
except Exception as e:
    print(e)

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)

print(len(history.history['loss']))
```

### Explanation:
- **`InputLayer`**: By adding the `InputLayer` with an explicit `input_shape=(20,)`, you're telling Keras the expected shape of the input data, which ensures that the model is built before calling `fit()`.

### Solution 2: Build the Model Before Calling `fit()`

Alternatively, you can use a batch of data to build the model explicitly before training. This can be done using the `model.build()` method:

```python
import keras
import numpy as np

class InterruptingCallback(keras.callbacks.Callback):
   def on_epoch_begin(self, epoch, logs=None):
     if epoch == 4:
       raise RuntimeError('Interrupting!')

callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")

# Define the model without specifying input shape
model = keras.models.Sequential([
    keras.layers.Dense(10)
])

model.compile(keras.optimizers.SGD(), loss='mse')

# Build the model by passing a batch of data
model.build(input_shape=(None, 20))  # Here, 20 is the number of features in your input data

# Now the model is built before calling fit
try:
    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
              batch_size=1, callbacks=[callback, InterruptingCallback()],
              verbose=0)
except Exception as e:
    print(e)

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)

print(len(history.history['loss']))
```

### Explanation:
- **`model.build()`**: This explicitly builds the model by providing the `input_shape`. After this call, the model is ready for training, and the `BackupAndRestore` callback will work correctly.

To fix the issue, you need to ensure that the model is built (either by specifying the input shape or by explicitly calling `model.build()`) before invoking `fit()`. Both solutions will address the error and allow the `BackupAndRestore` callback to function as expected.

Venkat6871 (Assginee) on (2025-01-03 07:13:45 UTC): Hi **@antipisa** ,
Thanks for raising your concern here. The raised [PR](https://github.com/keras-team/keras/pull/20714) has been merged. Could you please check and let us know if the issue still persists? 
Thank you!

antipisa (Issue Creator) on (2025-01-03 14:05:58 UTC): @Venkat6871 
Does Solution 1 work when the input layer is a normalization layer? E.g for this example
```

import tensorflow as tf

x = tf.random.uniform((100, 1))
y = tf.random.uniform((100, 1))
z = tf.random.uniform((100, 1))

xyz = tf.concat([x, y, z], 1)

horsepower_normalizer = tf.keras.layers.Normalization(input_shape=(3,), axis=-1)
horsepower_normalizer.adapt(xyz)

horsepower_model = tf.keras.models.Sequential([
    horsepower_normalizer,
    tf.keras.layers.Dense(units=1)
])

horsepower_model(xyz)

```

EDIT: This raises a UserWarning about using input_shape in a layer. Is there a better way?

antipisa (Issue Creator) on (2025-01-03 18:16:00 UTC): @Venkat6871 
Solution 1 raises `ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: (<InputLayer, name='input_layer_6, built=True>), (of type <class='tuple'>) ' `

Venkat6871 (Assginee) on (2025-01-09 08:41:16 UTC): Hi **@antipisa** ,
Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions, but I am not facing any issues. Could you please check which warning you are encountering? Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3c9e9d89bc8c9adbb8177cb409d18080/84027_tf_2-18-0-2-17-0-v.ipynb) here for your reference.
Thank you!

antipisa (Issue Creator) on (2025-01-09 20:44:22 UTC): Hi @Venkat6871 , the problem is fixed if one imports the following way:

```
from tensorflow import keras
from keras import layers
from keras.layers import InputLayer, Dense
```

Venkat6871 (Assginee) on (2025-01-10 04:11:30 UTC): Hi **@antipisa** ,
Glad to see your issue is resolved! Please feel free to close this issue if everything is working as expected.
Thank you!

google-ml-butler[bot] on (2025-01-10 13:07:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84027"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/84027"">No</a>

antipisa (Issue Creator) on (2025-01-30 23:32:17 UTC): Hi @Venkat6871 ,  May I ask why the discrepancy when specifying input_shape in an Input layer vs when calling model.build?

```

model.build(input_shape=(None, 20))  # Here, 20 is the number of features in your input data
# input_shape is (None,  20) versus (20, None) in the below
keras.layers.InputLayer(input_shape=(20,)),  # Specify the input shape


```

"
2765809205,issue,open,,Mixing Keras Layers and TF modules.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.Module can trace tf.Variable but it cannot trace variables from tf.keras or tf.keras.Variable. 

### Standalone code to reproduce the issue

```shell
class MockLayer(tf.Module):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.m = tf.keras.Variable(tf.random.normal([5, 5]), name=""m"")
        self.w = tf.keras.Variable(tf.random.normal([5, 5]), name=""w"")

    def __call__(self, inputs):
        return self.m * inputs

layer1 = MockLayer()
print([v.name for v in layer1.trainable_variables])
```

is empty.

```
class MockLayer(tf.Module):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.m = tf.Variable(tf.random.normal([5, 5]), name=""m"")
        self.w = tf.Variable(tf.random.normal([5, 5]), name=""w"")

    def __call__(self, inputs):
        return self.m * inputs

layer1 = MockLayer()
print([v.name for v in layer1.trainable_variables])
```

Works.

Specifically I am more interested in keras layers like 

```

class MockLayer(tf.Module):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.norm = tf.keras.layers.LayerNormalization(*args, **kwargs)

    def __call__(self, inputs):
        return self.norm(inputs)
```

For which tracing does not appear to work. 

I am interested in trying https://github.com/google/sequence-layers but they seem mostly broken on this TF version and python version due to the tracing issues. 

I am curious to try fixing it, but not sure what's a supported path. Using keras layers in tf.Module does not work due to tracing issues.. and using Keras layers only does not work because Keras is missing many features like composite tensors. 

Is there a way around this? :)
```


### Relevant log output

_No response_",jonasrsv42,2025-01-02 10:46:55+00:00,['tilakrayal'],2025-02-08 01:58:28+00:00,,https://github.com/tensorflow/tensorflow/issues/84019,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2568303387, 'issue_id': 2765809205, 'author': 'willfrew', 'body': 'I just hit the same issue trying to upgrade some code that was previously working on tensorflow 2.14;\r\nMy setup is the same as described:\r\n  - Top level `tf.Module`\r\n  - Submodules are `keras.Model`s\r\n\r\nThe keras-based submodules are now not being detected at the `tf.Module` level because the reflection based implementation is explicitly looking for submodules that extend `tf.Module` ([here](https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/module/module.py#L211)).\r\n\r\nIt appears that since the [2.16 release](https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1) that switched to keras 3.X, `keras.Model` / `keras.layers.Layer` no longer extends `tf.Module` but instead only extends the underlying `AutoTrackable` class via its own `TFLayer` class ([here](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/layer.py#L46)).\r\n\r\nThis contradicts / invalidates [the tensorflow documentation here](https://www.tensorflow.org/guide/intro_to_modules#defining_models_and_layers_in_tensorflow):\r\n> [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/Layer) is the base class of all Keras layers, and it inherits from [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module).\r\n\r\nLooking through some issues in the keras repo, it appears this is intentional, unfortunately. Specifically [this comment](https://github.com/keras-team/keras/issues/20095#issuecomment-2344796083).\r\n\r\nThere is [a section in the keras documentation ](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility) about this and gives some direction for downgrading keras to v2 in order to support this structure.', 'created_at': datetime.datetime(2025, 1, 2, 20, 1, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568850199, 'issue_id': 2765809205, 'author': 'tilakrayal', 'body': '@jonasrsv42,\r\nBy default Tensorflow v2.17, v2.18 contains the Keras3.0.  As mentioned in this [comment](https://github.com/keras-team/keras/issues/20095#issuecomment-2344796083), Keras 3 design supports multiple backends (like JAX and PyTorch) in addition to TensorFlow. To achieve this, core classes like Model and Layer no longer rely on TensorFlow `tf.Module`. As a result, variable tracking within models and layers is no longer automatic.\r\n\r\nAs this issue is more related to Keras, Kindly raise the request in the Keras-team/keras repo for further discussion.\r\n\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 3, 8, 30, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568861283, 'issue_id': 2765809205, 'author': 'jonasrsv42', 'body': 'I can raise it in Keras. But out of curiousity, I am trying to use tensorflow without Keras because Keras is missing features I want. (E.g Composite objects for Tensors) \r\n\r\nBut it seems with this split Tensorflow loses implementations for many common layers? Is there some intended replacement implementation of these layers for tensorflow if Keras will no longer work? \r\n\r\nOr is the expectation that we should roll our own using tensorflow primitives for all the layers that used to be in tf.keras?', 'created_at': datetime.datetime(2025, 1, 3, 8, 40, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569302233, 'issue_id': 2765809205, 'author': 'mihaimaruseac', 'body': 'It is unlikely that TF would get these layers.', 'created_at': datetime.datetime(2025, 1, 3, 14, 29, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425460, 'issue_id': 2765809205, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 27, tzinfo=datetime.timezone.utc)}]","willfrew on (2025-01-02 20:01:35 UTC): I just hit the same issue trying to upgrade some code that was previously working on tensorflow 2.14;
My setup is the same as described:
  - Top level `tf.Module`
  - Submodules are `keras.Model`s

The keras-based submodules are now not being detected at the `tf.Module` level because the reflection based implementation is explicitly looking for submodules that extend `tf.Module` ([here](https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/module/module.py#L211)).

It appears that since the [2.16 release](https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1) that switched to keras 3.X, `keras.Model` / `keras.layers.Layer` no longer extends `tf.Module` but instead only extends the underlying `AutoTrackable` class via its own `TFLayer` class ([here](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/layers/layer.py#L46)).

This contradicts / invalidates [the tensorflow documentation here](https://www.tensorflow.org/guide/intro_to_modules#defining_models_and_layers_in_tensorflow):

Looking through some issues in the keras repo, it appears this is intentional, unfortunately. Specifically [this comment](https://github.com/keras-team/keras/issues/20095#issuecomment-2344796083).

There is [a section in the keras documentation ](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility) about this and gives some direction for downgrading keras to v2 in order to support this structure.

tilakrayal (Assginee) on (2025-01-03 08:30:50 UTC): @jonasrsv42,
By default Tensorflow v2.17, v2.18 contains the Keras3.0.  As mentioned in this [comment](https://github.com/keras-team/keras/issues/20095#issuecomment-2344796083), Keras 3 design supports multiple backends (like JAX and PyTorch) in addition to TensorFlow. To achieve this, core classes like Model and Layer no longer rely on TensorFlow `tf.Module`. As a result, variable tracking within models and layers is no longer automatic.

As this issue is more related to Keras, Kindly raise the request in the Keras-team/keras repo for further discussion.

Thank you!

jonasrsv42 (Issue Creator) on (2025-01-03 08:40:23 UTC): I can raise it in Keras. But out of curiousity, I am trying to use tensorflow without Keras because Keras is missing features I want. (E.g Composite objects for Tensors) 

But it seems with this split Tensorflow loses implementations for many common layers? Is there some intended replacement implementation of these layers for tensorflow if Keras will no longer work? 

Or is the expectation that we should roll our own using tensorflow primitives for all the layers that used to be in tf.keras?

mihaimaruseac on (2025-01-03 14:29:42 UTC): It is unlikely that TF would get these layers.

github-actions[bot] on (2025-02-08 01:58:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2765302659,issue,closed,completed,I'm having a problem in converting my (.h5) model to (.tflite),"import tensorflow as tf
#store .h5 file in your .py folder

#load h5 module
model=tf.keras.models.load_model('enhanced_model.h5')
tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)

#convert
tflite_model = tflite_converter.convert()
open(""final.tflite"", ""wb"").write(tflite_model)


####################################################################

Output: 

Exception encountered: int() argument must be a string, a bytes-like object or a real number, not 'list'
PS C:\Users\USER\Desktop\New folder (2)> 
",omarakl,2025-01-01 23:13:07+00:00,['Venkat6871'],2025-01-02 03:21:17+00:00,2025-01-02 03:21:17+00:00,https://github.com/tensorflow/tensorflow/issues/83986,"[('TFLiteConverter', 'For issues related to TFLite converter')]",[],
2764033434,issue,closed,completed,The TFLite get different results on Python and Android,"### 1. System information

- OS Platform and Distribution: `MacOS 14.6`, `Android 15`
- TensorFlow version: `TensorFlow 2.13.1 (Python)`, `TensorFlowLite 2.12.0 (Android)`
- Others: `Python 3.10.16`, `JDK17`, `Android Gradle Plugin 8.7.3`, `Gradle 8.9`

### 2. Code

#### Step 1: Convert the model to tflite
> The model is derived from the hanlp project, here is the url for model: https://file.hankcs.com/hanlp/ner/msra_ner_albert_base_20211228_173323.zip 
```
import tensorflow as tf
from transformers import TFAutoModel, AutoTokenizer

def build_model(transformer, max_seq_length=128, num_labels=14):

    tokenizer = AutoTokenizer.from_pretrained(transformer)

    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=""input_ids"")
    attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=""attention_mask"")
    token_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name=""token_type_ids"")

    bert = TFAutoModel.from_pretrained(transformer)

    output = bert(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids).last_hidden_state

    logits = tf.keras.layers.Dense(num_labels)(output)

    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=logits)
    model.build(input_shape=(None, max_seq_length))

    return model, tokenizer


model, tokenizer = build_model(""uer/albert-base-chinese-cluecorpussmall"")
model.load_weights(""models/albert-base/model.h5"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

with open(""models/ner_model.tflite"", ""wb"") as f:
    f.write(tflite_model)

print(""saved"")
```

#### Step 2: Run on Python (Get the right result)
```
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer

interpreter = tf.lite.Interpreter(model_path=""models/ner_model.tflite"")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(""Input Details:"", input_details)
print(""Output Details:"", output_details)

text = """"

tokenizer = AutoTokenizer.from_pretrained(""models/my-albert-base"")
encoding = tokenizer(
    text,
    padding=""max_length"",
    truncation=True,
    max_length=128,
    return_tensors=""np""
)

input_ids = encoding[""input_ids""].astype(input_details[1]['dtype'])
attention_mask = encoding[""attention_mask""].astype(input_details[0]['dtype'])
token_type_ids = encoding[""token_type_ids""].astype(input_details[2]['dtype'])

print('input_ids:\n', input_ids)
print('attention_mask:\n', attention_mask)
print('token_type_ids:\n', token_type_ids)

interpreter.set_tensor(input_details[0]['index'], attention_mask)
interpreter.set_tensor(input_details[1]['index'], input_ids)
interpreter.set_tensor(input_details[2]['index'], token_type_ids)

interpreter.invoke()

output = interpreter.get_tensor(output_details[0]['index'])
predictions = np.argmax(output, axis=-1)

id2label = {0: '<pad>', 1: 'O', 2: 'S-NS', 3: 'B-NS', 4: 'E-NS', 5: 'B-NT', 6: 'M-NT', 7: 'E-NT', 8: 'M-NS', 9: 'B-NR', 10: 'M-NR', 11: 'E-NR', 12: 'S-NR', 13: 'S-NT'}
predicted_tags = [id2label[pred] for pred in predictions[0]]

print(""Predicted Tags:"", predicted_tags)
```
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Input Details: [{'name': 'serving_default_attention_mask:0', 'index': 0, 'shape': array([  1, 128], dtype=int32), 'shape_signature': array([ -1, 128], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([  1, 128], dtype=int32), 'shape_signature': array([ -1, 128], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_token_type_ids:0', 'index': 2, 'shape': array([  1, 128], dtype=int32), 'shape_signature': array([ -1, 128], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Output Details: [{'name': 'StatefulPartitionedCall:0', 'index': 1158, 'shape': array([  1, 128,  14], dtype=int32), 'shape_signature': array([ -1, 128,  14], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input_ids:
 [[ 101 3746 2335 1765 7448 3198 8024 3216 4899 7252 1126  725 6158 3039
  3673 8024  852 4385 1762 6821 7027 4638 7028 2456 6375  782 2697 1168
  2361 3307 1469 1213 7030  511  102    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0]]
attention_mask:
 [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
token_type_ids:
 [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
Predicted Tags: ['<pad>', 'B-NS', 'E-NS', 'O', 'O', 'O', 'O', 'B-NS', 'M-NS', 'E-NS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']
```

#### Step 3: Run on Android (Get the wrong result)
> The BertTokenizer is derived from the github project: https://github.com/ankiteciitkgp/bertTokenizer
```
public class BertBaseNer {
    private static final int MAX_SEQ_LENGTH = 128;
    private static final int OUTPUT_CLASSES_COUNT = 14;

    private Interpreter interpreter;

    private Context context;

    public BertBaseNer(Context context) throws IOException {
        AssetManager assetManager = context.getAssets();
        ByteBuffer model = loadModelFile(assetManager, ""ner_model.tflite"");

        this.interpreter = new Interpreter(model);

        this.context = context;
    }

    private ByteBuffer loadModelFile(AssetManager assetManager, String filename) throws IOException {
        FileInputStream inputStream = new FileInputStream(assetManager.openFd(filename).getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = assetManager.openFd(filename).getStartOffset();
        long declaredLength = assetManager.openFd(filename).getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

    private int[][] padSequence(int[] data) {
        int[][] res = new int[1][MAX_SEQ_LENGTH];

        for (int i = 0; i < MAX_SEQ_LENGTH; i++) {
            if (i < data.length) {
                res[0][i] = data[i];
            } else {
                res[0][i] = 0;  // Padding
            }
        }

        return res;
    }

    public String extractLoc(String str) {
        BertTokenizer tokenizer = new BertTokenizer(context);
        List<String> tokens = tokenizer.tokenize(str);
        tokens.add(0, ""[CLS]"");
        tokens.add(""[SEP]"");
        List<Integer> inputIdsList = tokenizer.convert_tokens_to_ids(tokens);
        int[] inputIds = inputIdsList.stream().mapToInt(Integer::intValue).toArray();
        int seqLength = inputIds.length;
        int[] attentionMask = new int[seqLength];
        int[] tokenTypeIds = new int[seqLength];
        for (int i = 0; i < seqLength; i++) {
            attentionMask[i] = 1;
            tokenTypeIds[i] = 0;
        }

        int[][] paddedInputIds = padSequence(inputIds);
        int[][] paddedAttentionMask = padSequence(attentionMask);
        int[][] paddedTokenTypeIds = padSequence(tokenTypeIds);

        float[][][] output = new float[1][MAX_SEQ_LENGTH][OUTPUT_CLASSES_COUNT];

        System.out.println(""Input shapes:"");
        System.out.println(""Input 0: "" + Arrays.toString(interpreter.getInputTensor(0).shape()));
        System.out.println(""Input 1: "" + Arrays.toString(interpreter.getInputTensor(1).shape()));
        System.out.println(""Input 2: "" + Arrays.toString(interpreter.getInputTensor(2).shape()));
        System.out.println(""Input types:"");
        System.out.println(""Input 0: "" + interpreter.getInputTensor(0).dataType());
        System.out.println(""Input 1: "" + interpreter.getInputTensor(1).dataType());
        System.out.println(""Input 2: "" + interpreter.getInputTensor(2).dataType());

        System.out.println(""Output shapes:"");
        System.out.println(""Output: "" + Arrays.toString(interpreter.getOutputTensor(0).shape()));
        System.out.println(""Output types:"");
        System.out.println(""Output: "" + interpreter.getOutputTensor(0).dataType());

        System.out.println(""InputIds:"");
        System.out.println(Arrays.deepToString(paddedInputIds));
        System.out.println(""AttentionMask:"");
        System.out.println(Arrays.deepToString(paddedAttentionMask));
        System.out.println(""TokenTypeIDs:"");
        System.out.println(Arrays.deepToString(paddedTokenTypeIds));

        Object[] inputs = {paddedInputIds, paddedAttentionMask, paddedTokenTypeIds};
        Map<Integer, Object> outputs = new HashMap<>();
        outputs.put(0, output);
        
        interpreter.runForMultipleInputsOutputs(inputs, outputs);

        int[] predictedClasses = new int[MAX_SEQ_LENGTH];
        for (int i = 0; i < MAX_SEQ_LENGTH; i++) {
            int maxIndex = 0;
            float maxScore = output[0][i][0];
            for (int j = 1; j < OUTPUT_CLASSES_COUNT; j++) {
                if (output[0][i][j] > maxScore) {
                    maxScore = output[0][i][j];
                    maxIndex = j;
                }
            }
            predictedClasses[i] = maxIndex;
        }

        System.out.println(""Predicted Classes:"");
        System.out.println(Arrays.toString(predictedClasses));

        String[] id2label = {""<pad>"", ""O"", ""S-NS"", ""B-NS"", ""E-NS"", ""B-NT"", ""M-NT"", ""E-NT"", ""M-NS"", ""B-NR"", ""M-NR"", ""E-NR"", ""S-NR"", ""S-NT""};
        StringBuilder resultBuilder = new StringBuilder();
        for (int i = 0; i < MAX_SEQ_LENGTH; i++) {
            if (!id2label[predictedClasses[i]].equals(""O"")) {
                resultBuilder.append(tokens.get(i)).append(""("").append(id2label[predictedClasses[i]]).append("") "");
            }
        }

        return resultBuilder.toString().trim();
    }
}
```
```
2024-12-31 14:46:22.435  1688-1688  nativeloader            com.example.demo                     D  Load /data/app/~~RIgQWfeUJqsh3inKVvtFeA==/com.example.demo-k72846Cpv-CVwQPOTbp2iA==/base.apk!/lib/arm64-v8a/libtensorflowlite_jni.so using ns clns-7 from class loader (caller=/data/app/~~RIgQWfeUJqsh3inKVvtFeA==/com.example.demo-k72846Cpv-CVwQPOTbp2iA==/base.apk!classes4.dex): ok
2024-12-31 14:46:22.435  1688-1688  InterpreterApi          com.example.demo                     I  Loaded native library: tensorflowlite_jni
2024-12-31 14:46:22.436  1688-1688  nativeloader            com.example.demo                     D  Load libtensorflowlite_jni_gms_client.so using ns clns-7 from class loader (caller=/data/app/~~RIgQWfeUJqsh3inKVvtFeA==/com.example.demo-k72846Cpv-CVwQPOTbp2iA==/base.apk!classes4.dex): dlopen failed: library ""libtensorflowlite_jni_gms_client.so"" not found
2024-12-31 14:46:22.436  1688-1688  InterpreterApi          com.example.demo                     I  Didn't load native library: tensorflowlite_jni_gms_client
2024-12-31 14:46:22.437  1688-1688  tflite                  com.example.demo                     I  Initialized TensorFlow Lite runtime.
2024-12-31 14:46:22.440  1688-1688  libc                    com.example.demo                     W  Access denied finding property ""ro.hardware.chipname""
2024-12-31 14:46:22.440  1688-1688  tflite                  com.example.demo                     I  Created TensorFlow Lite XNNPACK delegate for CPU.
2024-12-31 14:46:22.501  1688-1688  System.out              com.example.demo                     I  Input shapes:
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 0: [1, 128]
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 1: [1, 128]
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 2: [1, 128]
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input types:
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 0: INT32
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 1: INT32
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Input 2: INT32
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Output shapes:
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Output: [1, 128, 14]
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Output types:
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  Output: FLOAT32
2024-12-31 14:46:22.503  1688-1688  System.out              com.example.demo                     I  InputIds:
2024-12-31 14:46:22.504  1688-1688  System.out              com.example.demo                     I  [[101, 3746, 2335, 1765, 7448, 3198, 8024, 3216, 4899, 7252, 1126, 725, 6158, 3039, 3673, 8024, 852, 4385, 1762, 6821, 7027, 4638, 7028, 2456, 6375, 782, 2697, 1168, 2361, 3307, 1469, 1213, 7030, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2024-12-31 14:46:22.504  1688-1688  System.out              com.example.demo                     I  AttentionMask:
2024-12-31 14:46:22.504  1688-1688  System.out              com.example.demo                     I  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2024-12-31 14:46:22.504  1688-1688  System.out              com.example.demo                     I  TokenTypeIDs:
2024-12-31 14:46:22.505  1688-1688  System.out              com.example.demo                     I  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2024-12-31 14:46:22.754  1688-1688  System.out              com.example.demo                     I  Predicted Classes:
2024-12-31 14:46:22.754  1688-1688  System.out              com.example.demo                     I  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

### 3. Failure after conversion
- The model produces right results on python.
- The model produces wrong result on android, it does not recognize any named entities. All values in `predictedClasses` are 1.

I checked the shape, data type, and each value of the input tensor, and checked the shape and data type of the output tensor. They were all fine. And no error logs were generated when the model was called.

I don't know how to solve it. :(",bbslzy001,2024-12-31 07:00:21+00:00,['gaikwadrahul8'],2025-01-02 02:19:35+00:00,2025-01-02 02:19:33+00:00,https://github.com/tensorflow/tensorflow/issues/83936,"[('comp:lite', 'TF Lite related issues'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2567230193, 'issue_id': 2764033434, 'author': 'bbslzy001', 'body': 'I solved it!!! I ignored the order of the input tensors. In Android, I mistakenly reversed the order of input_ids and attention_mask.\r\nI modify as follows: `Object[] inputs = {paddedAttentionMask, paddedInputIds, paddedTokenTypeIds};`', 'created_at': datetime.datetime(2025, 1, 2, 2, 19, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567230202, 'issue_id': 2764033434, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83936"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83936"">No</a>', 'created_at': datetime.datetime(2025, 1, 2, 2, 19, 35, tzinfo=datetime.timezone.utc)}]","bbslzy001 (Issue Creator) on (2025-01-02 02:19:33 UTC): I solved it!!! I ignored the order of the input tensors. In Android, I mistakenly reversed the order of input_ids and attention_mask.
I modify as follows: `Object[] inputs = {paddedAttentionMask, paddedInputIds, paddedTokenTypeIds};`

google-ml-butler[bot] on (2025-01-02 02:19:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83936"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83936"">No</a>

"
2763571786,issue,closed,completed,@llvm_toolchain error,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi there,

Apologies for any inconvenience, and I truly appreciate your time and help. Im currently trying to cross-compile TensorFlow Lite for my RISC system. Unfortunately, Ive encountered issues that I cant seem to resolve.

On my x86_64 machine, I attempted the following Bazel build command:

bazel build //tensorflow/tools/pip_package:build_pip_package --crosstool_top=@llvm_toolchain//:toolchain --cpu=riscv64 --host_cpu=x86_64 --copt=-march=rv64gc --copt=-mabi=lp64d --linkopt=-march=rv64gc --linkopt=-mabi=lp64d

However, I keep receiving the error:

no such package '@llvm_toolchain//': The repository '@llvm_toolchain' could not be resolved: Repository '@llvm_toolchain' is not defined

Ive tried modifying the WORKSPACE file and pointing to a local LLVM installation, but that hasnt resolved the issue either.

If you have any suggestions or guidance on how to address this problem, Id greatly appreciate your help.

Thank you so much in advance for your support!

Best regards,

### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package --crosstool_top=@llvm_toolchain//:toolchain --cpu=riscv64 --host_cpu=x86_64 --copt=-march=rv64gc --copt=-mabi=lp64d --linkopt=-march=rv64gc --linkopt=-mabi=lp64d
```


### Relevant log output

_No response_",cemery123,2024-12-30 18:44:18+00:00,['Venkat6871'],2025-01-18 01:57:17+00:00,2025-01-18 01:57:14+00:00,https://github.com/tensorflow/tensorflow/issues/83913,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2567301428, 'issue_id': 2763571786, 'author': 'Venkat6871', 'body': 'Hi **@cemery123** ,\r\nApologies for the delay, and thank you for raising your concern here. Could you please check the compatible version of Bazel? This might be causing the issue. I have attached the [documentation](https://www.tensorflow.org/install/source#cpu) for your reference. Additionally, consider upgrading to the latest version of TensorFlow, as it may help resolve the issue and ensure smoother execution.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 2, 5, 31, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581591714, 'issue_id': 2763571786, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599466497, 'issue_id': 2763571786, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 18, 1, 57, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599466534, 'issue_id': 2763571786, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83913"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83913"">No</a>', 'created_at': datetime.datetime(2025, 1, 18, 1, 57, 16, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-01-02 05:31:28 UTC): Hi **@cemery123** ,
Apologies for the delay, and thank you for raising your concern here. Could you please check the compatible version of Bazel? This might be causing the issue. I have attached the [documentation](https://www.tensorflow.org/install/source#cpu) for your reference. Additionally, consider upgrading to the latest version of TensorFlow, as it may help resolve the issue and ensure smoother execution.
Thank you!

github-actions[bot] on (2025-01-10 02:03:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-18 01:57:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-18 01:57:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83913"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83913"">No</a>

"
2763172795,issue,closed,completed,Custom Layer output shape isn't working with model.summary(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04 WSL

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

model.summary() output

**Current Behavior**
```
Model: ""Q_KAN""
____________________________________________________________________________
 Layer (type)                Output Shape              Param #   Trainable  
============================================================================
 input_68 (InputLayer)       [(None, 2)]               0         Y          
                                                                            
 DenseKAN (DenseQKan)        None                      18        Y          
                                                                            
 RescalePi (Rescale)         None                      0         N          
                                                                            
============================================================================
Total params: 18 (144.00 Byte)
Trainable params: 18 (144.00 Byte)
Non-trainable params: 0 (0.00 Byte)
____________________________________________________________________________
```

**Expected Behavior**
```
Model: ""Q_KAN""
____________________________________________________________________________
 Layer (type)                Output Shape              Param #   Trainable  
============================================================================
 input_68 (InputLayer)       [(None, 2)]               0         Y          
                                                                            
 DenseKAN (DenseQKan)         [(None, 10)]                      18        Y          
                                                                            
 RescalePi (Rescale)          [(None, 10)]                      0         N          
                                                                            
============================================================================
Total params: 18 (144.00 Byte)
Trainable params: 18 (144.00 Byte)
Non-trainable params: 0 (0.00 Byte)
____________________________________________________________________________
```

### Standalone code to reproduce the issue

**Custom layer code:**

```python
class Rescale(tf.keras.layers.Layer):
    def __init__(self,scale_limit=np.pi,**kwargs):
        super().__init__(trainable=False,**kwargs)
        self.scale_limit = scale_limit
    
    def call(self,inputs):
        s = tf.reduce_sum(inputs,axis=-1)
        return (inputs/s)*self.scale_limit

class DenseQKan(tf.keras.layers.Layer):
    def __init__(self,units:int,circuit:qml.QNode,layers:int,**kwargs):
        super().__init__(**kwargs)
        self.circuit = circuit
        self.qubits =  len(circuit.device.wires)
        self.units = units
        self.qbatches = None
        self.layers = layers
        
    def build(self,input_shape):
        if input_shape[-1]> self.qubits:
            self.qbatches = np.ceil(input_shape[-1]/self.qubits).astype(np.int32)
        else:
            self.qbatches = 1
        self.layer_weights = []
        for u in range(self.units):
            self.layer_weights.append(self.add_weight(shape=(self.qbatches,input_shape[-1]//self.qbatches,self.layers),
                                   initializer=tf.keras.initializers.RandomUniform(minval=-np.pi, maxval=np.pi, seed=None),
                                   trainable=True))
        self.built = True
        # W = np.random.uniform(low=-np.pi,high=np.pi,size=(self.units,self.qbatches,self.qubits,self.layers))
    # @tf.function(reduce_retracing=True)

    def compute_output_shape(self,input_shape):
        print(""Build Input Shape"",input_shape)
        return (input_shape[0],self.units)
        
    def call(self,inputs):
        assert self.qbatches != None 
        splits = tf.split(inputs,self.qbatches,-1) 
        out = []
        for u in range(self.units):
            unit_out = 0
            for qb in range(self.qbatches):
                qb_out = tf.reduce_sum(tf.stack(self.circuit(splits[qb],self.layer_weights[u][qb]),axis=-1),axis=-1)
                unit_out = unit_out+qb_out
            out.append(unit_out)
        out = tf.stack(out,axis=-1)
        return out
```

**Model definition code:**
```python
# def create_model(units,qubits,layers,circuit,input_shape=2):
inp = Input(shape=input_shape)
out = DenseQKan(units,circuit,layers,name=""DenseKAN"")(inp)
out = Rescale(name=""RescalePi"")(out)
model = Model(inputs=inp,outputs=out,name=""Q_KAN"")
model.summary(show_trainable=True)
```

### Relevant log output

_No response_",vinayak19th,2024-12-30 12:55:15+00:00,['tilakrayal'],2025-02-04 01:59:53+00:00,2025-02-04 01:59:53+00:00,https://github.com/tensorflow/tensorflow/issues/83907,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2565454708, 'issue_id': 2763172795, 'author': 'vinayak19th', 'body': 'I found the fix but this should not be expected behavior.\r\n\r\n**Solution**\r\nAdd the following line at the end of the call method\r\n```python \r\nout = tf.reshape(out,(tf.shape(inputs)[0],self.units))\r\n```', 'created_at': datetime.datetime(2024, 12, 30, 13, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567489761, 'issue_id': 2763172795, 'author': 'tilakrayal', 'body': '@vinayak19th,\r\nBy checking the code it looks like this issue is more related to Keras. Could you please raise the request in Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2025, 1, 2, 9, 32, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581591799, 'issue_id': 2763172795, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581698652, 'issue_id': 2763172795, 'author': 'vinayak19th', 'body': ""> @vinayak19th, By checking the code it looks like this issue is more related to Keras. Could you please raise the request in Keras-team/keras repo for the quick resolution. Thank you!\r\n\r\nSure I'll do that, thank you"", 'created_at': datetime.datetime(2025, 1, 10, 4, 3, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581698694, 'issue_id': 2763172795, 'author': 'vinayak19th', 'body': ""> @vinayak19th, By checking the code it looks like this issue is more related to Keras. Could you please raise the request in Keras-team/keras repo for the quick resolution. Thank you!\r\n\r\nSure I'll do that, thank you"", 'created_at': datetime.datetime(2025, 1, 10, 4, 4, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617533677, 'issue_id': 2763172795, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 28, 1, 59, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632587117, 'issue_id': 2763172795, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 2, 4, 1, 59, 52, tzinfo=datetime.timezone.utc)}]","vinayak19th (Issue Creator) on (2024-12-30 13:00:53 UTC): I found the fix but this should not be expected behavior.

**Solution**
Add the following line at the end of the call method
```python 
out = tf.reshape(out,(tf.shape(inputs)[0],self.units))
```

tilakrayal (Assginee) on (2025-01-02 09:32:45 UTC): @vinayak19th,
By checking the code it looks like this issue is more related to Keras. Could you please raise the request in Keras-team/keras repo for the quick resolution. Thank you!

github-actions[bot] on (2025-01-10 02:03:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

vinayak19th (Issue Creator) on (2025-01-10 04:03:58 UTC): Sure I'll do that, thank you

vinayak19th (Issue Creator) on (2025-01-10 04:04:02 UTC): Sure I'll do that, thank you

github-actions[bot] on (2025-01-28 01:59:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-02-04 01:59:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2762279854,issue,open,,No GPU support for tf.image.resize(method='bicubic') ,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.image.resize() does not support GPU execution with 'bicubic' interpolation method. 

*I can help add this feature* 

### Standalone code to reproduce the issue

```shell
print(tf.config.list_physical_devices())
tf.debugging.set_log_device_placement(True)

# runs on GPU
img = tf.ones([4,100,100,1])
img_resized = tf.image.resize(img, [50,50])

# runs on CPU (no GPU implementation available!)
img2 = tf.ones([4,100,100,1])
img2_resized = tf.image.resize(img2, [50,50], method='bicubic')
```


### Relevant log output

_No response_",avirajBevli,2024-12-29 14:43:15+00:00,['Venkat6871'],2025-01-03 14:59:16+00:00,,https://github.com/tensorflow/tensorflow/issues/83877,"[('type:support', 'Support issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2566195599, 'issue_id': 2762279854, 'author': 'Venkat6871', 'body': 'Hi **@avirajBevli** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 with GPU, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/807234ec4e859d0425b9e3e1cb177d89/83877_tf_2-17-0-v.ipynb) attached here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 31, 7, 16, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569358970, 'issue_id': 2762279854, 'author': 'avirajBevli', 'body': ""Sorry, perhaps I wasn't clear with the framing of my question.\r\nThere is no issue with running this code. My concern is that there is no GPU implementation for the tf.image.resize() operation with bicubic interpolation support.\r\n\r\neg:\r\nimg2 = tf.ones([4,100,100,1])\r\nimg2_resized = tf.image.resize(img2, [50,50], method='bicubic')\r\n\r\nI want this code to run on the GPU (for performance reasons). Instead, since there is no GPU implementation of the kernel available, tf defaults to running this operation on the CPU."", 'created_at': datetime.datetime(2025, 1, 3, 14, 59, 13, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-31 07:16:28 UTC): Hi **@avirajBevli** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 with GPU, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/807234ec4e859d0425b9e3e1cb177d89/83877_tf_2-17-0-v.ipynb) attached here for your reference.
Thank you!

avirajBevli (Issue Creator) on (2025-01-03 14:59:13 UTC): Sorry, perhaps I wasn't clear with the framing of my question.
There is no issue with running this code. My concern is that there is no GPU implementation for the tf.image.resize() operation with bicubic interpolation support.

eg:
img2 = tf.ones([4,100,100,1])
img2_resized = tf.image.resize(img2, [50,50], method='bicubic')

I want this code to run on the GPU (for performance reasons). Instead, since there is no GPU implementation of the kernel available, tf defaults to running this operation on the CPU.

"
2761011732,issue,closed,completed,Cannot Fine-Tune Hugging Face TF model on GPU (it works on CPU),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5.1 / 9

### GPU model and memory

Nvidia Tesla T4 GPU (16GB)

### Current behavior?

I'm trying to fine tune HuggingFace's `TFResNetModel` using `tf.keras`. The provided example works on CPU, however when I enable the GPU I get the following error. 

The GPU is running and working fine for a simple Keras model example. 

The problem seems to be related to the integration of tf.Keras and HuggingFace.

### Standalone code to reproduce the issue

```shell
os.environ['TF_USE_LEGACY_KERAS'] = '1'
import tensorflow as tf
from transformers import TFResNetModel

def create_model():
  
  base_model = TFResNetModel.from_pretrained(""microsoft/resnet-50"")
  inputs = tf.keras.Input((3,224,224), dtype='float32')

  x = base_model(inputs).pooler_output
  x = tf.keras.layers.Flatten()(x)
  x = tf.keras.layers.Dense(1, activation='sigmoid')(x)
  
  model = tf.keras.Model(inputs=inputs, outputs=x)
  return model

model = create_model()

model.layers[1].trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
              loss=tf.keras.losses.BinaryCrossentropy())

model.fit(x=np.random.random(size=(10,3,224,224)), 
          y=np.random.random(size=(10,)), 
          batch_size=2, 
          epochs=20, 
          shuffle=True)
```


### Relevant log output

```shell
I0000 00:00:1735312999.999419   28671 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFResNetModel: ['resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'classifier.1.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked']
- This IS expected if you are initializing TFResNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFResNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFResNetModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetModel for predictions without further training.
Epoch 1/20
2024-12-27 15:23:25.310845: I tensorflow/core/grappler/optimizers/generic_layout_optimizer.cc:403] Cancel Transpose nodes around Pad: transpose_before=model/tf_res_net_model/resnet/transpose pad=model/tf_res_net_model/resnet/embedder/embedder/Pad transpose_after=model/tf_res_net_model/resnet/embedder/embedder/convolution/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer
E0000 00:00:1735313005.557664   28780 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2024-12-27 15:23:25.558728: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at conv_ops_impl.h:1204 : INVALID_ARGUMENT: No DNN in stream executor.
2024-12-27 15:23:25.558788: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: No DNN in stream executor.
	 [[{{node model/tf_res_net_model/resnet/embedder/embedder/convolution/Conv2D}}]]

InvalidArgumentError: Graph execution error:

Detected at node model/tf_res_net_model/resnet/embedder/embedder/convolution/Conv2D defined at (most recent call last):
  File ""/databricks/python_shell/scripts/db_ipykernel_launcher.py"", line 242, in <module>

  File ""/databricks/python_shell/scripts/db_ipykernel_launcher.py"", line 238, in main

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/kernelapp.py"", line 701, in start

  File ""/databricks/python/lib/python3.12/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/usr/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever

  File ""/usr/lib/python3.12/asyncio/base_events.py"", line 1987, in _run_once

  File ""/usr/lib/python3.12/asyncio/events.py"", line 88, in _run

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 534, in dispatch_queue

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 523, in process_one

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 429, in dispatch_shell

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 767, in execute_request

  File ""/databricks/python_shell/dbruntime/DatabricksShell.py"", line 285, in do_execute

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py"", line 429, in do_execute

  File ""/databricks/python/lib/python3.12/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/root/.ipykernel/28671/command-3155791975121661-1239963347"", line 20, in <module>

  File ""/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py"", line 460, in safe_patch_function

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 1804, in fit

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 1398, in train_function

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 1381, in step_function

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 1370, in run_step

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 1147, in train_step

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 588, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/functional.py"", line 514, in call

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/functional.py"", line 671, in _run_internal_graph

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/training.py"", line 588, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/transformers/modeling_tf_utils.py"", line 499, in run_call_with_unpacked_inputs

  File ""/databricks/python/lib/python3.12/site-packages/transformers/models/resnet/modeling_tf_resnet.py"", line 503, in call

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/transformers/modeling_tf_utils.py"", line 499, in run_call_with_unpacked_inputs

  File ""/databricks/python/lib/python3.12/site-packages/transformers/models/resnet/modeling_tf_resnet.py"", line 432, in call

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/transformers/models/resnet/modeling_tf_resnet.py"", line 124, in call

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/transformers/models/resnet/modeling_tf_resnet.py"", line 82, in call

  File ""/databricks/python/lib/python3.12/site-packages/transformers/models/resnet/modeling_tf_resnet.py"", line 78, in convolution

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 65, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/engine/base_layer.py"", line 1142, in __call__

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py"", line 96, in error_handler

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/layers/convolutional/base_conv.py"", line 289, in call

  File ""/databricks/python/lib/python3.12/site-packages/tf_keras/src/layers/convolutional/base_conv.py"", line 261, in convolution_op

No DNN in stream executor.
	 [[{{node model/tf_res_net_model/resnet/embedder/embedder/convolution/Conv2D}}]] [Op:__inference_train_function_9105]
File <command-3155791975121661>, line 20
     15 model.layers[1].trainable = False
     17 model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
     18               loss=tf.keras.losses.BinaryCrossentropy())
---> 20 model.fit(x=np.random.random(size=(10,3,224,224)), 
     21           y=np.random.random(size=(10,)), 
     22           batch_size=2, 
     23           epochs=20, 
     24           shuffle=True)
File /databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:460, in safe_patch.<locals>.safe_patch_function(*args, **kwargs)
    441 if (
    442     active_session_failed
    443     or autologging_is_disabled(autologging_integration)
   (...)
    454     # warning behavior during original function execution, since autologging is being
    455     # skipped
    456     with set_non_mlflow_warnings_behavior_for_current_thread(
    457         disable_warnings=False,
    458         reroute_warnings=False,
    459     ):
--> 460         return original(*args, **kwargs)
    462 # Whether or not the original / underlying function has been called during the
    463 # execution of patched code
    464 original_has_been_called = False
File /databricks/python/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb
File /databricks/python/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:
```
",carlosg-m,2024-12-27 15:31:24+00:00,['tilakrayal'],2025-01-15 01:59:54+00:00,2025-01-15 01:59:51+00:00,https://github.com/tensorflow/tensorflow/issues/83802,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2565605424, 'issue_id': 2761011732, 'author': 'tilakrayal', 'body': '@carlosg-m,\r\nHi, By default the colab notebook is using tensorflow v2.17 and v2.18 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras == 2.18.0\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/12745b17800316a04bc64f499ec33437/untitled2292.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 30, 15, 12, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250248, 'issue_id': 2761011732, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476176, 'issue_id': 2761011732, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 15, 1, 59, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476244, 'issue_id': 2761011732, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83802"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83802"">No</a>', 'created_at': datetime.datetime(2025, 1, 15, 1, 59, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-30 15:12:25 UTC): @carlosg-m,
Hi, By default the colab notebook is using tensorflow v2.17 and v2.18 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras == 2.18.0

import tf_keras as keras
```

Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/12745b17800316a04bc64f499ec33437/untitled2292.ipynb).

Thank you!

github-actions[bot] on (2025-01-07 02:02:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-15 01:59:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-15 01:59:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83802"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83802"">No</a>

"
2760217772,issue,open,,Failing to convert MobileNetV3Large to TFLite w/ Integer q,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL
- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)

### 2. Code

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
test_image = np.clip(test_image, 0, 255).astype(np.float32)
preprocessed_image = preprocess_input(test_image.copy())

# Load model
model = MobileNetV3Large(
    weights='imagenet',
    include_top=True,
    input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter._experimental_disable_per_channel = True
converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0]['index'])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot([original_pred.min(), original_pred.max()],
         [original_pred.min(), original_pred.max()],
         'r--', label=f'Perfect Correlation\nActual: {correlation:.4f}')
plt.title('Original vs Quantized Predictions')
plt.xlabel('Original Model')
plt.ylabel('Quantized Model')
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),
         bins=50, alpha=0.75, label='Prediction Differences')
plt.title('Distribution of Prediction Differences')
plt.xlabel('|Original - Quantized|')
plt.ylabel('Count')
plt.legend()

plt.tight_layout()
plt.show()

print(f""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {len(model.get_weights()) / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
print(f""Size reduction: {(1 - len(tflite_model) / len(model.get_weights())) * 100:.1f}%"")
```

### 3. Failure after conversion
1. TF 2.10 in Win10 Log:
Model produces wrong results. See plot made from code:
![image](https://github.com/user-attachments/assets/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)

2. TF2.16 in WSL:
Model fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)


### 5. (optional) Any other info / logs
I ran this on 2 systems:

1. TF 2.10 in Win10 Log:
```
import sys; print('Python %s on %s' % (sys.version, sys.platform))
D:\code\ai_dev\venv\Scripts\python.exe ""C:/Program Files/JetBrains/PyCharm 2023.2.4/plugins/python/helpers/pydev/pydevd.py"" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\Users\Administrator\AppData\Roaming\JetBrains\PyCharm2023.2\scratches\tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:17:07.215039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:17:07.670016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6
2024-12-26 15:17:11.111912: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8906
2024-12-26 15:17:12.036555: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.
2024-12-26 15:17:44.746406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2024-12-26 15:17:44.746529: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2024-12-26 15:17:44.747230: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.771028: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2024-12-26 15:17:44.771129: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.886049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2024-12-26 15:17:44.904668: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2024-12-26 15:17:45.275249: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:45.402632: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.
2024-12-26 15:17:45.811466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

3. TF 2.16.2 in WSL log:
```
/root/ai_dev/.venv/bin/python /root/.pycharm_helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file /mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:18:56.434366: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:57.803991: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:58.473972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-26 15:18:58.972456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-26 15:18:58.975123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-26 15:18:59.910805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:19:06.124533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-12-26 15:19:15.989425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node
Your kernel may have been built without NUMA support.
2024-12-26 15:19:17.013008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.
W0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.
2024-12-26 15:19:30.948060: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:30.953309: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-12-26 15:19:30.953334: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.011901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-12-26 15:19:31.020594: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2024-12-26 15:19:31.231606: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.297302: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.
2024-12-26 15:19:31.779723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""MobileNetV3Large_1/conv_1/convolution/ReadVariableOp@__inference_serving_default_5035""(""/root/.pycharm_helpers/pydev/pydevd.py"":2199:1) at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":2181:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1493:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1500:1 at callsite(""/root/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"":18:1 at callsite(""/mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py"":34:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1175:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1129:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1636:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1614:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py"":205:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1537:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":58:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":112:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":182:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/function.py"":171:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":632:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":243:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":233:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/nn.py"":1183:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":301:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":274:1 at ""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).

Process finished with exit code 134
```


",rsandler00,2024-12-26 23:21:09+00:00,['gaikwadrahul8'],2025-01-06 18:53:39+00:00,,https://github.com/tensorflow/tensorflow/issues/83754,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.16', '')]","[{'comment_id': 2563473600, 'issue_id': 2760217772, 'author': 'gaikwadrahul8', 'body': 'Hi, @rsandler00 \r\nThank you for bringing this issue to our attention, I tried downgrading the TensorFlow version to `2.14.1` and it seems like code is working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/b4d72b3d249dd6bc364eb2a0432a9275/tflite-issue-83754.ipynb) so could you please give it try with TensorFlow version `2.14.1` by installing using this command `pip install tensorflow==2.14.1` for` Windows 10 WSL` and let us know is it working as expected or not ?\r\n\r\nIf issue still persists please let us know with updated error log for further investigation.\r\n\r\nThank you for your cooperation and understanding.', 'created_at': datetime.datetime(2024, 12, 27, 8, 51, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566139003, 'issue_id': 2760217772, 'author': 'james77777778', 'body': 'You can try this with the latest `keras==3.7.0`:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy.stats import pearsonr\r\n\r\nimport keras\r\n\r\n# Generate one sample image for testing\r\ntest_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\ntest_image = np.clip(test_image, 0, 255).astype(np.float32)\r\npreprocessed_image = keras.applications.mobilenet_v3.preprocess_input(\r\n    test_image.copy()\r\n)\r\n\r\n# Load model\r\nmodel = keras.applications.MobileNetV3Large(\r\n    weights=""imagenet"", include_top=True, input_shape=(224, 224, 3)\r\n)\r\n\r\n# Get original prediction\r\noriginal_pred = model.predict(preprocessed_image, verbose=0)\r\n\r\n# Convert to TFLite\r\nsaved_model_path = ""/tmp/mbv3""\r\nmodel.export(saved_model_path, verbose=0)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n]\r\n# converter._experimental_disable_per_channel = True\r\n# converter.experimental_new_converter = True\r\n\r\n# Convert\r\ntflite_model = converter.convert()\r\n\r\n# Get TFLite prediction\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0][""index""], preprocessed_image)\r\ninterpreter.invoke()\r\ntflite_pred = interpreter.get_tensor(output_details[0][""index""])\r\n\r\n# Calculate correlation\r\ncorrelation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())\r\n\r\n# Visualize\r\nplt.figure(figsize=(10, 5))\r\n\r\n# Scatter plot\r\nplt.subplot(1, 2, 1)\r\nplt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)\r\nplt.plot(\r\n    [original_pred.min(), original_pred.max()],\r\n    [original_pred.min(), original_pred.max()],\r\n    ""r--"",\r\n    label=f""Perfect Correlation\\nActual: {correlation:.4f}"",\r\n)\r\nplt.title(""Original vs Quantized Predictions"")\r\nplt.xlabel(""Original Model"")\r\nplt.ylabel(""Quantized Model"")\r\nplt.legend()\r\n\r\n# Distribution plot\r\nplt.subplot(1, 2, 2)\r\nplt.hist(\r\n    np.abs(original_pred.flatten() - tflite_pred.flatten()),\r\n    bins=50,\r\n    alpha=0.75,\r\n    label=""Prediction Differences"",\r\n)\r\nplt.title(""Distribution of Prediction Differences"")\r\nplt.xlabel(""|Original - Quantized|"")\r\nplt.ylabel(""Count"")\r\nplt.legend()\r\n\r\nplt.tight_layout()\r\nplt.savefig(""image.png"")\r\n\r\nmodel_params_size = float(model.count_params()) * 4  # To bytes\r\nprint(""\\nResults:"")\r\nprint(f""Prediction correlation: {correlation:.4f}"")\r\nprint(f""Original model size: {model_params_size / 1024 / 1024:.2f} MB"")\r\nprint(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")\r\nsize_reduction = (1 - len(tflite_model) / model_params_size)\r\nprint(f""Size reduction: {size_reduction * 100:.1f}%"")\r\n```\r\n\r\nThe outputs:\r\n\r\n```bash\r\nResults:\r\nPrediction correlation: 0.9720\r\nOriginal model size: 21.01 MB\r\nQuantized model size: 5.55 MB\r\nSize reduction: 73.6%\r\n```', 'created_at': datetime.datetime(2024, 12, 31, 5, 14, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566673005, 'issue_id': 2760217772, 'author': 'rsandler00', 'body': '> mmand\r\n\r\n@gaikwadrahul8 your collab precisely confirms the bug I am seeing - your Prediction correlation is -0.0192, meaning that the TFLite outputs are completely unrelated to the original model!', 'created_at': datetime.datetime(2024, 12, 31, 19, 26, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566757311, 'issue_id': 2760217772, 'author': 'rsandler00', 'body': '@james77777778 thanks for your reply! Your code did indeed work. I am getting decent results w/ my conversion on my actual model.\r\n\r\nHowever, now another issue arose. When I try to improve the results using a representative dataset, my results actually get **much worse**!\r\n\r\nI added the following to the code:\r\n```\r\nADD_REPRESENTATIVE_DATASET = True\r\ndef representative_dataset():\r\n    for _ in range(00):  # Generate 100 random samples\r\n        sample = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\n        sample = np.clip(sample, 0, 255).astype(np.float32)\r\n        # Important: Apply the same preprocessing as your actual inputs\r\n        sample = tf.keras.applications.mobilenet_v3.preprocess_input(sample)\r\n        yield [sample]\r\n\r\nif ADD_REPRESENTATIVE_DATASET:\r\n    converter.representative_dataset = representative_dataset\r\n```\r\nand get these results. The image on the left is from your original code. On the right is when `ADD_REPRESENTATIVE_DATASET = True`. The same exact pattern is repeating in my actual model and real representative dataset (of medical images). Why is adding a representative dataset making my results so much worse rather than improving them?\r\n\r\n![image](https://github.com/user-attachments/assets/b956e850-d39a-4dd2-a3be-ebaa6870c6e6)\r\n\r\n~~UPDATE: This only happens in TF2.10. It appears to be fixed in TF2.14~~\r\n\r\nUPDATE2: Confirmed it still happens in TF2.10, TF2.13, TF2.14, and TF2.16\r\n\r\n<details>\r\n<summary>**Fully reproducible code** (Click to expand)</summary>\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy.stats import pearsonr\r\n\r\nADD_REPRESENTATIVE_DATASET = True\r\n\r\n# Generate one sample image for testing\r\ntest_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\npreprocessed_image = np.clip(test_image, 0, 255).astype(np.float32)\r\n\r\n# Load model\r\nmodel = tf.keras.applications.MobileNetV3Large(\r\n    weights=""imagenet"", include_top=True, input_shape=(224, 224, 3)\r\n)\r\n\r\n# Get original prediction\r\noriginal_pred = model.predict(preprocessed_image, verbose=0)\r\n\r\n# Convert to TFLite\r\nsaved_model_path = ""/tmp/mbv3""\r\nmodel.export(saved_model_path)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\n# converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ndef representative_dataset():\r\n    for _ in range(100):  # Generate 100 random samples\r\n        sample = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\n        sample = np.clip(sample, 0, 255).astype(np.float32)\r\n        yield [sample]\r\n\r\nif ADD_REPRESENTATIVE_DATASET:\r\n    converter.representative_dataset = representative_dataset\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n]\r\n# converter._experimental_disable_per_channel = True\r\n# converter.experimental_new_converter = True\r\n\r\n# Convert\r\ntflite_model = converter.convert()\r\n\r\n# Get TFLite prediction\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0][""index""], preprocessed_image)\r\ninterpreter.invoke()\r\ntflite_pred = interpreter.get_tensor(output_details[0][""index""])\r\n\r\n# Calculate correlation\r\ncorrelation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())\r\n\r\n# Visualize\r\nplt.figure(figsize=(10, 5))\r\n\r\n# Scatter plot\r\nplt.subplot(1, 2, 1)\r\nplt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)\r\nplt.plot(\r\n    [original_pred.min(), original_pred.max()],\r\n    [original_pred.min(), original_pred.max()],\r\n    ""r--"",\r\n    label=f""Perfect Correlation\\nActual: {correlation:.4f}"",\r\n)\r\nplt.title(""Original vs Quantized Predictions"")\r\nplt.xlabel(""Original Model"")\r\nplt.ylabel(""Quantized Model"")\r\nplt.legend()\r\n\r\n# Distribution plot\r\nplt.subplot(1, 2, 2)\r\nplt.hist(\r\n    np.abs(original_pred.flatten() - tflite_pred.flatten()),\r\n    bins=50,\r\n    alpha=0.75,\r\n    label=""Prediction Differences"",\r\n)\r\nplt.title(""Distribution of Prediction Differences"")\r\nplt.xlabel(""|Original - Quantized|"")\r\nplt.ylabel(""Count"")\r\nplt.legend()\r\n\r\nplt.tight_layout()\r\nplt.savefig(""image.png"")\r\n# plt.show()\r\n\r\nmodel_params_size = float(model.count_params()) * 4  # To bytes\r\nprint(""\\nResults:"")\r\nprint(f""Prediction correlation: {correlation:.4f}"")\r\nprint(f""Original model size: {model_params_size / 1024 / 1024:.2f} MB"")\r\nprint(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")\r\nsize_reduction = (1 - len(tflite_model) / model_params_size)\r\nprint(f""Size reduction: {size_reduction * 100:.1f}%"")\r\n```\r\n</details>', 'created_at': datetime.datetime(2024, 12, 31, 23, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567236250, 'issue_id': 2760217772, 'author': 'james77777778', 'body': '> However, now another issue arose. When I try to improve the results using a representative dataset, my results actually get **much worse**!\r\n\r\n> UPDATE: This only happens in TF2.10. It appears to be fixed in TF2.14\r\n\r\nIm not an expert in TFLite, but as far as I can tell, newer versions of TensorFlow do address some bugs in TFLite as well. Ive had similar experiences in the past.', 'created_at': datetime.datetime(2025, 1, 2, 2, 36, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567321803, 'issue_id': 2760217772, 'author': 'rsandler00', 'body': '@james77777778 i made a mistake. it seems to occur in all versions up to 2.16 (didnt check past that). I added fully reproducible code on the bottom of last comment. Would love to know if you see the same on your end?', 'created_at': datetime.datetime(2025, 1, 2, 6, 11, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568806352, 'issue_id': 2760217772, 'author': 'gaikwadrahul8', 'body': 'Hi, @rsandler00\r\nHi, @james77777778, Thank you for your pointers and temporary workaround\r\n\r\nAs far I know it is happening because of [TensorFlow + Keras 2 backwards compatibility](https://keras.io/getting_started/) From TensorFlow 2.0 to TensorFlow 2.15 (included), doing pip install tensorflow will also install the corresponding version of Keras 2  for instance, `pip install tensorflow==2.14.0` will install `keras==2.14.0`. That version of Keras is then available via both `import keras` and `from tensorflow import keras `(the [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) namespace).\r\n\r\nStarting with **TensorFlow 2.16**, doing `pip install tensorflow `will `install Keras 3`. When you have TensorFlow >= 2.16 and Keras 3, then by default `from tensorflow import keras` ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) will be Keras 3.\r\n\r\n**EDIT :** I have tried code mentioned in this comment with different versions of TensorFlow versions from 2.13.* to 2.17.* and it seems like code working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/b1277942000f2b1abd736d6a0d8d779c/tflite-issue-83754.ipynb)\r\n\r\nCould you please help us with your complete code or Google colab notebook with representative dataset to try from my end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2025, 1, 3, 7, 45, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569644307, 'issue_id': 2760217772, 'author': 'rsandler00', 'body': 'Thanks @gaikwadrahul8 ! \r\n\r\nThe example you used does work, but it doesnt use a representative dataset. \r\n\r\nI created another [gist ](https://colab.research.google.com/gist/gaikwadrahul8/b1277942000f2b1abd736d6a0d8d779c/tflite-issue-83754.ipynb#scrollTo=unjL1YHRWdpk)w/ my code above and it produces the same results:\r\n\r\n\r\n**self_contained_example(add_representative_dataset=False)**\r\nResults:\r\nPrediction correlation: 0.9775\r\nOriginal model size: 21.01 MB\r\nQuantized model size: 5.55 MB\r\nSize reduction: 73.6%\r\n![image](https://github.com/user-attachments/assets/70d0504b-1b11-4e8c-b707-b0287783839d)\r\n\r\n**self_contained_example(add_representative_dataset=True)**\r\nResults:\r\nPrediction correlation: 0.5212\r\nOriginal model size: 21.01 MB\r\nQuantized model size: 5.77 MB\r\nSize reduction: 72.5%\r\n![image](https://github.com/user-attachments/assets/c7b442dc-090e-4469-ba7c-6c7886c3259d)\r\n\r\n\r\nI tried it w/ TF.2.13 and TF2.16 and got the same result.\r\n\r\nWith 2.17 it worked w/ `add_representative_dataset=False`, but with `add_representative_dataset=True`, it gave error:\r\n```\r\nRuntimeError: failed to create XNNPACK runtimeNode number 131 (TfLiteXNNPackDelegate) failed to prepare.\r\n```', 'created_at': datetime.datetime(2025, 1, 3, 18, 25, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572779934, 'issue_id': 2760217772, 'author': 'gaikwadrahul8', 'body': ""Hi, @rsandler00 \r\nI tried code with `ADD_REPRESENTATIVE_DATASET = False` code working as expected but when I am using `ADD_REPRESENTATIVE_DATASET = True` code is not working as expected please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/964d0d400c3c474441d2981de546688a/tflite-issue-83754.ipynb) so we'll have to dig more into this issue, I found a similar issue https://github.com/tensorflow/tensorflow/issues/77293\r\n\r\nif you're interested then please give it try with https://github.com/google-ai-edge/ai-edge-torch for that please refer this [article](https://medium.com/axinc-ai/convert-models-from-pytorch-to-tflite-with-ai-edge-torch-0e85623f8d56)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 1, 6, 10, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573715793, 'issue_id': 2760217772, 'author': 'rsandler00', 'body': '@gaikwadrahul8 thanks! is there any ETA on when you think this bug will be resolved?\r\n\r\nAlso, few questions:\r\n1. I assume w/ the PyTorch approach I would need to retrain the MobileNetV3 model in pytorch? (eg there is no way to ""convert"" the TF model to PyTorch)\r\n2. If I trained w/ QAT do you think that would make the issue irrelevant since its already optimized for INT8?', 'created_at': datetime.datetime(2025, 1, 6, 18, 53, 38, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-27 08:51:37 UTC): Hi, @rsandler00 
Thank you for bringing this issue to our attention, I tried downgrading the TensorFlow version to `2.14.1` and it seems like code is working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/b4d72b3d249dd6bc364eb2a0432a9275/tflite-issue-83754.ipynb) so could you please give it try with TensorFlow version `2.14.1` by installing using this command `pip install tensorflow==2.14.1` for` Windows 10 WSL` and let us know is it working as expected or not ?

If issue still persists please let us know with updated error log for further investigation.

Thank you for your cooperation and understanding.

james77777778 on (2024-12-31 05:14:08 UTC): You can try this with the latest `keras==3.7.0`:

```python
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from scipy.stats import pearsonr

import keras

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
test_image = np.clip(test_image, 0, 255).astype(np.float32)
preprocessed_image = keras.applications.mobilenet_v3.preprocess_input(
    test_image.copy()
)

# Load model
model = keras.applications.MobileNetV3Large(
    weights=""imagenet"", include_top=True, input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
saved_model_path = ""/tmp/mbv3""
model.export(saved_model_path, verbose=0)
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
# converter._experimental_disable_per_channel = True
# converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0][""index""], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0][""index""])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot(
    [original_pred.min(), original_pred.max()],
    [original_pred.min(), original_pred.max()],
    ""r--"",
    label=f""Perfect Correlation\nActual: {correlation:.4f}"",
)
plt.title(""Original vs Quantized Predictions"")
plt.xlabel(""Original Model"")
plt.ylabel(""Quantized Model"")
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(
    np.abs(original_pred.flatten() - tflite_pred.flatten()),
    bins=50,
    alpha=0.75,
    label=""Prediction Differences"",
)
plt.title(""Distribution of Prediction Differences"")
plt.xlabel(""|Original - Quantized|"")
plt.ylabel(""Count"")
plt.legend()

plt.tight_layout()
plt.savefig(""image.png"")

model_params_size = float(model.count_params()) * 4  # To bytes
print(""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {model_params_size / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
size_reduction = (1 - len(tflite_model) / model_params_size)
print(f""Size reduction: {size_reduction * 100:.1f}%"")
```

The outputs:

```bash
Results:
Prediction correlation: 0.9720
Original model size: 21.01 MB
Quantized model size: 5.55 MB
Size reduction: 73.6%
```

rsandler00 (Issue Creator) on (2024-12-31 19:26:49 UTC): @gaikwadrahul8 your collab precisely confirms the bug I am seeing - your Prediction correlation is -0.0192, meaning that the TFLite outputs are completely unrelated to the original model!

rsandler00 (Issue Creator) on (2024-12-31 23:50:09 UTC): @james77777778 thanks for your reply! Your code did indeed work. I am getting decent results w/ my conversion on my actual model.

However, now another issue arose. When I try to improve the results using a representative dataset, my results actually get **much worse**!

I added the following to the code:
```
ADD_REPRESENTATIVE_DATASET = True
def representative_dataset():
    for _ in range(00):  # Generate 100 random samples
        sample = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
        sample = np.clip(sample, 0, 255).astype(np.float32)
        # Important: Apply the same preprocessing as your actual inputs
        sample = tf.keras.applications.mobilenet_v3.preprocess_input(sample)
        yield [sample]

if ADD_REPRESENTATIVE_DATASET:
    converter.representative_dataset = representative_dataset
```
and get these results. The image on the left is from your original code. On the right is when `ADD_REPRESENTATIVE_DATASET = True`. The same exact pattern is repeating in my actual model and real representative dataset (of medical images). Why is adding a representative dataset making my results so much worse rather than improving them?

![image](https://github.com/user-attachments/assets/b956e850-d39a-4dd2-a3be-ebaa6870c6e6)

~~UPDATE: This only happens in TF2.10. It appears to be fixed in TF2.14~~

UPDATE2: Confirmed it still happens in TF2.10, TF2.13, TF2.14, and TF2.16

<details>
<summary>**Fully reproducible code** (Click to expand)</summary>
```
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from scipy.stats import pearsonr

ADD_REPRESENTATIVE_DATASET = True

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
preprocessed_image = np.clip(test_image, 0, 255).astype(np.float32)

# Load model
model = tf.keras.applications.MobileNetV3Large(
    weights=""imagenet"", include_top=True, input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
saved_model_path = ""/tmp/mbv3""
model.export(saved_model_path)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
# converter = tf.lite.TFLiteConverter.from_keras_model(model)

def representative_dataset():
    for _ in range(100):  # Generate 100 random samples
        sample = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
        sample = np.clip(sample, 0, 255).astype(np.float32)
        yield [sample]

if ADD_REPRESENTATIVE_DATASET:
    converter.representative_dataset = representative_dataset
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
# converter._experimental_disable_per_channel = True
# converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0][""index""], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0][""index""])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot(
    [original_pred.min(), original_pred.max()],
    [original_pred.min(), original_pred.max()],
    ""r--"",
    label=f""Perfect Correlation\nActual: {correlation:.4f}"",
)
plt.title(""Original vs Quantized Predictions"")
plt.xlabel(""Original Model"")
plt.ylabel(""Quantized Model"")
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(
    np.abs(original_pred.flatten() - tflite_pred.flatten()),
    bins=50,
    alpha=0.75,
    label=""Prediction Differences"",
)
plt.title(""Distribution of Prediction Differences"")
plt.xlabel(""|Original - Quantized|"")
plt.ylabel(""Count"")
plt.legend()

plt.tight_layout()
plt.savefig(""image.png"")
# plt.show()

model_params_size = float(model.count_params()) * 4  # To bytes
print(""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {model_params_size / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
size_reduction = (1 - len(tflite_model) / model_params_size)
print(f""Size reduction: {size_reduction * 100:.1f}%"")
```
</details>

james77777778 on (2025-01-02 02:36:45 UTC): Im not an expert in TFLite, but as far as I can tell, newer versions of TensorFlow do address some bugs in TFLite as well. Ive had similar experiences in the past.

rsandler00 (Issue Creator) on (2025-01-02 06:11:39 UTC): @james77777778 i made a mistake. it seems to occur in all versions up to 2.16 (didnt check past that). I added fully reproducible code on the bottom of last comment. Would love to know if you see the same on your end?

gaikwadrahul8 (Assginee) on (2025-01-03 07:45:11 UTC): Hi, @rsandler00
Hi, @james77777778, Thank you for your pointers and temporary workaround

As far I know it is happening because of [TensorFlow + Keras 2 backwards compatibility](https://keras.io/getting_started/) From TensorFlow 2.0 to TensorFlow 2.15 (included), doing pip install tensorflow will also install the corresponding version of Keras 2  for instance, `pip install tensorflow==2.14.0` will install `keras==2.14.0`. That version of Keras is then available via both `import keras` and `from tensorflow import keras `(the [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) namespace).

Starting with **TensorFlow 2.16**, doing `pip install tensorflow `will `install Keras 3`. When you have TensorFlow >= 2.16 and Keras 3, then by default `from tensorflow import keras` ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) will be Keras 3.

**EDIT :** I have tried code mentioned in this comment with different versions of TensorFlow versions from 2.13.* to 2.17.* and it seems like code working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/b1277942000f2b1abd736d6a0d8d779c/tflite-issue-83754.ipynb)

Could you please help us with your complete code or Google colab notebook with representative dataset to try from my end ? 

Thank you for your cooperation and patience.

rsandler00 (Issue Creator) on (2025-01-03 18:25:14 UTC): Thanks @gaikwadrahul8 ! 

The example you used does work, but it doesnt use a representative dataset. 

I created another [gist ](https://colab.research.google.com/gist/gaikwadrahul8/b1277942000f2b1abd736d6a0d8d779c/tflite-issue-83754.ipynb#scrollTo=unjL1YHRWdpk)w/ my code above and it produces the same results:


**self_contained_example(add_representative_dataset=False)**
Results:
Prediction correlation: 0.9775
Original model size: 21.01 MB
Quantized model size: 5.55 MB
Size reduction: 73.6%
![image](https://github.com/user-attachments/assets/70d0504b-1b11-4e8c-b707-b0287783839d)

**self_contained_example(add_representative_dataset=True)**
Results:
Prediction correlation: 0.5212
Original model size: 21.01 MB
Quantized model size: 5.77 MB
Size reduction: 72.5%
![image](https://github.com/user-attachments/assets/c7b442dc-090e-4469-ba7c-6c7886c3259d)


I tried it w/ TF.2.13 and TF2.16 and got the same result.

With 2.17 it worked w/ `add_representative_dataset=False`, but with `add_representative_dataset=True`, it gave error:
```
RuntimeError: failed to create XNNPACK runtimeNode number 131 (TfLiteXNNPackDelegate) failed to prepare.
```

gaikwadrahul8 (Assginee) on (2025-01-06 10:06:54 UTC): Hi, @rsandler00 
I tried code with `ADD_REPRESENTATIVE_DATASET = False` code working as expected but when I am using `ADD_REPRESENTATIVE_DATASET = True` code is not working as expected please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/964d0d400c3c474441d2981de546688a/tflite-issue-83754.ipynb) so we'll have to dig more into this issue, I found a similar issue https://github.com/tensorflow/tensorflow/issues/77293

if you're interested then please give it try with https://github.com/google-ai-edge/ai-edge-torch for that please refer this [article](https://medium.com/axinc-ai/convert-models-from-pytorch-to-tflite-with-ai-edge-torch-0e85623f8d56)

Thank you for your cooperation and patience.

rsandler00 (Issue Creator) on (2025-01-06 18:53:38 UTC): @gaikwadrahul8 thanks! is there any ETA on when you think this bug will be resolved?

Also, few questions:
1. I assume w/ the PyTorch approach I would need to retrain the MobileNetV3 model in pytorch? (eg there is no way to ""convert"" the TF model to PyTorch)
2. If I trained w/ QAT do you think that would make the issue irrelevant since its already optimized for INT8?

"
2759461577,issue,open,,failed to make cuFFT batched plan:5,"Hi,
    I installed nvidia-docker images and used tensorflow2.9.1 (as below).

![1735200614747](https://github.com/user-attachments/assets/25e1d5bc-0dd1-4571-b7c4-d2c4c79f2d20)

The container created by this images works fine on GTX-1080-TiRTX-3080A10 or A100 GPU, but when we use it on L40 GPU, it give the following error:

	>>> import numpy as np
	>>> import tensorflow as tf
	2024-12-26 16:21:08.701508: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation o
	rders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
	>>> gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
	>>> for gpu in gpus:
	...   tf.config.experimental.set_memory_growth(gpu, True)
	...
	>>> x = tf.constant(np.random.rand(1, 1600), dtype=tf.float32)
	2024-12-26 16:21:57.835357: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in perfor
	mance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
	To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
	2024-12-26 16:21:58.194129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43151 MB memory:  -> device: 0, name: NVIDIA L40, pci bus id: 0
	000:b1:00.0, compute capability: 8.9
	>>> y = tf.signal.stft(x, 400, 160, 512)
	2024-12-26 16:22:10.408524: E tensorflow/stream_executor/cuda/cuda_fft.cc:225] failed to make cuFFT batched plan:5
	2024-12-26 16:22:10.408568: E tensorflow/stream_executor/cuda/cuda_fft.cc:430] Initialize Params: rank: 1 elem_count: 512 input_embed: 512 input_stride: 1 input_distance: 512 output_embed: 257 output_stride: 1 out
	put_distance: 257 batch_count: 8
	2024-12-26 16:22:10.408584: F tensorflow/stream_executor/cuda/cuda_fft.cc:439] failed to initialize batched cufft plan with customized allocator: Failed to make cuFFT batched plan.
	Aborted (core dumped)

It seems the operation `tf.signal.stft` can not work on L40 GPU it's ok on the same machine with CPU, what's wrong? And you can see, even add **allow memory growth** as this issue [https://github.com/ContinuumIO/anaconda-issues/issues/11628](url) described, it does't work at all. ",yjiangling,2024-12-26 08:27:27+00:00,['Venkat6871'],2024-12-31 05:50:56+00:00,,https://github.com/tensorflow/tensorflow/issues/83724,"[('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2565048163, 'issue_id': 2759461577, 'author': 'Venkat6871', 'body': 'Hi **@yjiangling** ,\r\nApologies for the delay, and thank you for raising your concern here. Could you please elaborate on your issue and provide details about the versions of cuDNN and CUDA you are using? Additionally, I noticed you are using TensorFlow 2.9.1, which is an older version. Please try upgrading to the latest version of TensorFlow to avoid compatibility issues. Let us know if you are still facing issues with the latest version.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 30, 5, 43, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566154233, 'issue_id': 2759461577, 'author': 'yjiangling', 'body': '> Hi **@yjiangling** , Apologies for the delay, and thank you for raising your concern here. Could you please elaborate on your issue and provide details about the versions of cuDNN and CUDA you are using? Additionally, I noticed you are using TensorFlow 2.9.1, which is an older version. Please try upgrading to the latest version of TensorFlow to avoid compatibility issues. Let us know if you are still facing issues with the latest version. Thank you!\r\n\r\nThank you for the help. It seems that CUDA 11.6 and cuDNN 8.4.1 are installed in the docker images(see as below),\r\n![image](https://github.com/user-attachments/assets/627c5b8b-ff9f-4eb1-84f0-a2027189ab36)\r\n\r\nbut CUDA 11.7 are installed on the systems\r\n![image](https://github.com/user-attachments/assets/7220d976-208e-4cfe-a737-6023eddb0a51)\r\n\r\nYes, I have tried to use another docker image with TensorFlow2.14, it works fine. But for some hardware and software requirements in our project, we need to use this docker image so far, and I also tried to change the version of the TensorFlow in the container of the image, but it do not work. Is the version of CUDA and cuDNN not suitable for the TensorFlow? But why it works fine in other GPUs? Any suggestions?  Many thanks.', 'created_at': datetime.datetime(2024, 12, 31, 5, 50, 54, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-30 05:43:56 UTC): Hi **@yjiangling** ,
Apologies for the delay, and thank you for raising your concern here. Could you please elaborate on your issue and provide details about the versions of cuDNN and CUDA you are using? Additionally, I noticed you are using TensorFlow 2.9.1, which is an older version. Please try upgrading to the latest version of TensorFlow to avoid compatibility issues. Let us know if you are still facing issues with the latest version.
Thank you!

yjiangling (Issue Creator) on (2024-12-31 05:50:54 UTC): Thank you for the help. It seems that CUDA 11.6 and cuDNN 8.4.1 are installed in the docker images(see as below),
![image](https://github.com/user-attachments/assets/627c5b8b-ff9f-4eb1-84f0-a2027189ab36)

but CUDA 11.7 are installed on the systems
![image](https://github.com/user-attachments/assets/7220d976-208e-4cfe-a737-6023eddb0a51)

Yes, I have tried to use another docker image with TensorFlow2.14, it works fine. But for some hardware and software requirements in our project, we need to use this docker image so far, and I also tried to change the version of the TensorFlow in the container of the image, but it do not work. Is the version of CUDA and cuDNN not suitable for the TensorFlow? But why it works fine in other GPUs? Any suggestions?  Many thanks.

"
2758714078,issue,closed,completed,Is this a human designed framework?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

linux ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

I can only say that the tensorflow is designed like a shit after try to use. 

### Standalone code to reproduce the issue

```shell
shit
```


### Relevant log output

_No response_",DeepLearningfeng,2024-12-25 10:30:55+00:00,['tilakrayal'],2025-01-03 19:09:53+00:00,2025-01-03 19:09:52+00:00,https://github.com/tensorflow/tensorflow/issues/83703,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.16', '')]","[{'comment_id': 2563368827, 'issue_id': 2758714078, 'author': 'tilakrayal', 'body': '@DeepLearningfeng,\r\nCould you provide the issue which you are facing and also try to provide more context/information in which area you are facing the issue.\r\n\r\nTensorFlow makes it easy for beginners and experts to create machine learning models for desktop, mobile, web, and cloud.\r\nhttps://www.tensorflow.org/learn\r\n\r\nIf you are facing the issue while installing the Tensorflow, please try to follow the steps which are available in the official document.\r\nhttps://www.tensorflow.org/install\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 27, 6, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569693228, 'issue_id': 2758714078, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2025, 1, 3, 19, 9, 53, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-27 06:25:00 UTC): @DeepLearningfeng,
Could you provide the issue which you are facing and also try to provide more context/information in which area you are facing the issue.

TensorFlow makes it easy for beginners and experts to create machine learning models for desktop, mobile, web, and cloud.
https://www.tensorflow.org/learn

If you are facing the issue while installing the Tensorflow, please try to follow the steps which are available in the official document.
https://www.tensorflow.org/install

Thank you!

mihaimaruseac on (2025-01-03 19:09:53 UTC): Please don't spam

"
2758446871,issue,closed,completed, Adreno 750  compatList.isDelegateSupportedOnThisDevice:false,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:tensorflow-lite-gpu:2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to support GPU acceleration

### Standalone code to reproduce the issue

```shell
//  GPU
                CompatibilityList compatList = new CompatibilityList();

                if(compatList.isDelegateSupportedOnThisDevice()) {
                    //  GPU delegate
                    gpuDelegate = new GpuDelegate();
                    options.addDelegate(gpuDelegate);
                    isUsingGPU = true;
                    Log.i(TAG, "" GPU "");
                } else {
                    //  GPU CPU 
                    options.setNumThreads(Runtime.getRuntime().availableProcessors());
                    options.setUseXNNPACK(true);
                    Log.i(TAG, "" CPU "");
                }
```


### Relevant log output

_No response_",lizhiwen19900709,2024-12-25 03:17:13+00:00,['Venkat6871'],2025-01-10 02:03:17+00:00,2025-01-10 02:03:13+00:00,https://github.com/tensorflow/tensorflow/issues/83685,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2562383992, 'issue_id': 2758446871, 'author': 'Venkat6871', 'body': 'Hi **@lizhiwen19900709** ,\r\nThank you for raising your issue here. Could you please elaborate on your issue and provide reproducible code to help debug it? Additionally, it would be helpful if you could share details such as your operating system.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 26, 9, 54, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568606058, 'issue_id': 2758446871, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 3, 2, 1, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581591918, 'issue_id': 2758446871, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581592156, 'issue_id': 2758446871, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83685"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83685"">No</a>', 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 16, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-26 09:54:56 UTC): Hi **@lizhiwen19900709** ,
Thank you for raising your issue here. Could you please elaborate on your issue and provide reproducible code to help debug it? Additionally, it would be helpful if you could share details such as your operating system.
Thank you!

github-actions[bot] on (2025-01-03 02:01:33 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-10 02:03:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-10 02:03:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83685"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83685"">No</a>

"
2758314259,issue,closed,completed,Failed to support CUDA 12.1,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.1

### GPU model and memory

NVIDIA GeForce RTX 4060

### Current behavior?

I cannot use my installed CUDA with TensorFlow 2.17.0. I have CUDA 12.1, all the relevant cuDNN and other packages are installed, I already use it with Torch. But I cannot use it with TensorFlow and Keras.  I made a new environment on Conda with tf 2.13.0, but it did not solve the issue. I need help ASAP.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os


print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))


# Print detailed environment information
print(""TensorFlow version:"", tf.__version__)
print(""CUDA visible devices:"", tf.config.list_physical_devices('GPU'))
print(""CUDA built status:"", tf.test.is_built_with_cuda())

# Check environment variables
print(""\nCUDA environment variables:"")
print(""CUDA_VISIBLE_DEVICES:"", os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set'))
print(""LD_LIBRARY_PATH:"", os.environ.get('LD_LIBRARY_PATH', 'Not set'))

# Try a simple GPU operation
try:
    with tf.device('/GPU:0'):
        print(""\nAttempting GPU computation..."")
        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
        b = tf.constant([[5.0, 6.0], [7.0, 8.0]])
        c = tf.matmul(a, b)
        print(""GPU computation successful!"")
        print(c)
except RuntimeError as e:
    print(""GPU computation failed:"", str(e))
```


### Relevant log output

_No response_",O-Memis,2024-12-24 21:37:28+00:00,['tilakrayal'],2025-01-10 02:03:20+00:00,2025-01-10 02:03:15+00:00,https://github.com/tensorflow/tensorflow/issues/83684,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2562761499, 'issue_id': 2758314259, 'author': 'tilakrayal', 'body': '@O-Memis,\r\nCould you please provide the complete steps you followed to install and also provide the error log which helps to debug the issue. Also try to follow the compatible versions for the Tensorflow v2.17 &  TensorFlow with GPU access is supported for WSL2 on Windows 10 19044 or higher. This corresponds to Windows 10 version 21H2, the November 2021 update. You can get the latest update from here: [Download Windows 10](https://www.microsoft.com/software-download/windows10). For instructions, see [Install WSL2](https://docs.microsoft.com/windows/wsl/install) and [NVIDIAs setup docs](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) for CUDA in WSL.\r\n\r\n\r\nhttps://www.tensorflow.org/install/source_windows#gpu', 'created_at': datetime.datetime(2024, 12, 26, 13, 51, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568606073, 'issue_id': 2758314259, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 3, 2, 1, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581592015, 'issue_id': 2758314259, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581592254, 'issue_id': 2758314259, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83684"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83684"">No</a>', 'created_at': datetime.datetime(2025, 1, 10, 2, 3, 19, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-26 13:51:04 UTC): @O-Memis,
Could you please provide the complete steps you followed to install and also provide the error log which helps to debug the issue. Also try to follow the compatible versions for the Tensorflow v2.17 &  TensorFlow with GPU access is supported for WSL2 on Windows 10 19044 or higher. This corresponds to Windows 10 version 21H2, the November 2021 update. You can get the latest update from here: [Download Windows 10](https://www.microsoft.com/software-download/windows10). For instructions, see [Install WSL2](https://docs.microsoft.com/windows/wsl/install) and [NVIDIAs setup docs](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) for CUDA in WSL.


https://www.tensorflow.org/install/source_windows#gpu

github-actions[bot] on (2025-01-03 02:01:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-10 02:03:14 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-10 02:03:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83684"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83684"">No</a>

"
2756717996,issue,closed,not_planned,DLL Load Failed while Importing _pywrap_tensorflow_internal on Windows,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Microsoft Windows 11 Pro

### Mobile device

_No response_

### Python version

3.11.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I try to import TensorFlow using the command `import tensorflow as tf`, I get the following error message:

`ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.`

This error prevents TensorFlow from starting properly. I am using the CPU version of TensorFlow and do not require CUDA/cuDNN. I have ensured that all necessary components for TensorFlow to function correctly are installed on my system.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""D:\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python\Python311\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""D:\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```
",8180e,2024-12-23 21:08:35+00:00,['tilakrayal'],2024-12-24 12:14:15+00:00,2024-12-24 12:14:12+00:00,https://github.com/tensorflow/tensorflow/issues/83632,"[('type:bug', 'Bug')]","[{'comment_id': 2561059135, 'issue_id': 2756717996, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83632"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83632"">No</a>', 'created_at': datetime.datetime(2024, 12, 24, 12, 14, 14, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-12-24 12:14:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83632"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83632"">No</a>

"
2756588729,issue,closed,completed,Failed to load the native TensorFlow runtime.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It seems that the error is encountered is related to a failure in loading the TensorFlow native runtime. This type of issue is usually related to missing or incompatible dependencies, specifically dynamic link libraries (DLLs), which are required for TensorFlow to function prop

### Standalone code to reproduce the issue

```shell
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py:73
     72 try:
---> 73   from tensorflow.python._pywrap_tensorflow_internal import *
     74 # This try catch logic is because there is no bazel equivalent for py_extension.
     75 # Externally in opensource we must enable exceptions to load the shared object
     76 # by exposing the PyInit symbols with pybind. This error will only be
     77 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     78 
     79 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 4
      2 from sklearn.metrics import accuracy_score
      3 from sklearn.preprocessing import StandardScaler
----> 4 from keras.models import Sequential
      5 from keras.layers import LSTM, Dense, Dropout
      6 from pyswarms.single import GlobalBestPSO

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\__init__.py:2
      1 # DO NOT EDIT. Generated by api_gen.sh
----> 2 from keras.api import DTypePolicy
      3 from keras.api import FloatDTypePolicy
      4 from keras.api import Function

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\api\__init__.py:8
      1 """"""DO NOT EDIT.
      2 
      3 This file was autogenerated. Do not edit it by hand,
      4 since your modifications would be overwritten.
      5 """"""
----> 8 from keras.api import activations
      9 from keras.api import applications
     10 from keras.api import backend

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\api\activations\__init__.py:7
      1 """"""DO NOT EDIT.
      2 
      3 This file was autogenerated. Do not edit it by hand,
      4 since your modifications would be overwritten.
      5 """"""
----> 7 from keras.src.activations import deserialize
      8 from keras.src.activations import get
      9 from keras.src.activations import serialize

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\__init__.py:1
----> 1 from keras.src import activations
      2 from keras.src import applications
      3 from keras.src import backend

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\activations\__init__.py:3
      1 import types
----> 3 from keras.src.activations.activations import celu
      4 from keras.src.activations.activations import elu
      5 from keras.src.activations.activations import exponential

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\activations\activations.py:1
----> 1 from keras.src import backend
      2 from keras.src import ops
      3 from keras.src.api_export import keras_export

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\__init__.py:10
      7     import torch
      9 from keras.src.api_export import keras_export
---> 10 from keras.src.backend.common.dtypes import result_type
     11 from keras.src.backend.common.keras_tensor import KerasTensor
     12 from keras.src.backend.common.keras_tensor import any_symbolic_tensors

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\common\__init__.py:2
      1 from keras.src.backend.common import backend_utils
----> 2 from keras.src.backend.common.dtypes import result_type
      3 from keras.src.backend.common.variables import AutocastScope
      4 from keras.src.backend.common.variables import Variable as KerasVariable

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\common\dtypes.py:5
      3 from keras.src.api_export import keras_export
      4 from keras.src.backend import config
----> 5 from keras.src.backend.common.variables import standardize_dtype
      7 BOOL_TYPES = (""bool"",)
      8 INT_TYPES = (
      9     ""uint8"",
     10     ""uint16"",
   (...)
     16     ""int64"",
     17 )

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\common\variables.py:11
      9 from keras.src.backend.common.stateless_scope import get_stateless_scope
     10 from keras.src.backend.common.stateless_scope import in_stateless_scope
---> 11 from keras.src.utils.module_utils import tensorflow as tf
     12 from keras.src.utils.naming import auto_name
     15 class Variable:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\__init__.py:1
----> 1 from keras.src.utils.audio_dataset_utils import audio_dataset_from_directory
      2 from keras.src.utils.dataset_utils import split_dataset
      3 from keras.src.utils.file_utils import get_file

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\audio_dataset_utils.py:4
      1 import numpy as np
      3 from keras.src.api_export import keras_export
----> 4 from keras.src.utils import dataset_utils
      5 from keras.src.utils.module_utils import tensorflow as tf
      6 from keras.src.utils.module_utils import tensorflow_io as tfio

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\dataset_utils.py:9
      5 from multiprocessing.pool import ThreadPool
      7 import numpy as np
----> 9 from keras.src import tree
     10 from keras.src.api_export import keras_export
     11 from keras.src.utils import io_utils

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\tree\__init__.py:1
----> 1 from keras.src.tree.tree_api import assert_same_paths
      2 from keras.src.tree.tree_api import assert_same_structure
      3 from keras.src.tree.tree_api import flatten

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\tree\tree_api.py:8
      5 from keras.src.utils.module_utils import optree
      7 if optree.available:
----> 8     from keras.src.tree import optree_impl as tree_impl
      9 elif dmtree.available:
     10     from keras.src.tree import dmtree_impl as tree_impl

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\tree\optree_impl.py:13
     11 # Register backend-specific node classes
     12 if backend() == ""tensorflow"":
---> 13     from tensorflow.python.trackable.data_structures import ListWrapper
     14     from tensorflow.python.trackable.data_structures import _DictWrapper
     16     optree.register_pytree_node(
     17         ListWrapper,
     18         lambda x: (x, None),
     19         lambda metadata, children: ListWrapper(list(children)),
     20         namespace=""keras"",
     21     )

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py:88
     86     sys.setdlopenflags(_default_dlopen_flags)
     87 except ImportError:
---> 88   raise ImportError(
     89       f'{traceback.format_exc()}'
     90       f'\n\nFailed to load the native TensorFlow runtime.\n'
     91       f'See https://www.tensorflow.org/install/errors '
     92       f'for some common causes and solutions.\n'
     93       f'If you need help, create an issue '
     94       f'at https://github.com/tensorflow/tensorflow/issues '
     95       f'and include the entire stack trace above this error message.')
     97 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\hema2\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 73, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",Hemavarna12,2024-12-23 19:11:57+00:00,['Venkat6871'],2025-01-03 16:59:04+00:00,2025-01-03 16:59:01+00:00,https://github.com/tensorflow/tensorflow/issues/83626,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2560844256, 'issue_id': 2756588729, 'author': 'Venkat6871', 'body': 'Hi **@Hemavarna12** ,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 24, 8, 41, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804029, 'issue_id': 2756588729, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569541248, 'issue_id': 2756588729, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 16, 59, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569541293, 'issue_id': 2756588729, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83626"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83626"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 16, 59, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-24 08:41:37 UTC): Hi **@Hemavarna12** ,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

github-actions[bot] on (2025-01-01 02:06:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mihaimaruseac on (2025-01-03 16:59:01 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2025-01-03 16:59:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83626"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83626"">No</a>

"
2755089294,issue,closed,completed,failed to build tensorflow 2.10 GPU support,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

Windows 11. MSYS_NT-10.0-22631 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1.1

### GCC/compiler version

MSVC 2019

### CUDA/cuDNN version

CUDA 11.2/ cuDNN 8.1

### GPU model and memory

NVIDIA RTX A3000 12GB

### Current behavior?

Missing header files from cudnn_frontend, have tried modifying the bazel files to download other versions, same issue missing header from cudnn_frontend_EngineFallbackList.h

### Standalone code to reproduce the issue

```shell
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow:tensorflow.dll
```


### Relevant log output

```shell
ERROR: C:/users/Karma/tensorflow/tensorflow/stream_executor/cuda/BUILD:416:11: Compiling tensorflow/stream_executor/cuda/cuda_dnn.cc failed: (Exit 2): python.exe failed: error executing command
  cd /d C:/users/Karma/_bazel/aakz3rh4/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\msys64\tmp
    SET TMP=C:\msys64\tmp
  C:\Users\Karma\AppData\Local\Programs\Python\Python39\python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt-exec-50AE0418/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_google_absl /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_google_protobuf /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/eigen_archive /Iexternal/nsync /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/nsync /Iexternal/snappy /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/snappy /Iexternal/double_conversion /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/double_conversion /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_googlesource_code_re2 /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_tensorrt /Iexternal/gif /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/libjpeg_turbo /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/zlib /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/cudnn_frontend_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_google_protobuf/src /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/nsync/public /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/gif /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/gif/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/zlib /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /W0 /Zc:__cplusplus /D_USE_MATH_DEFINES /d2ReducedOptimizeHugeFunctions -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /std:c++17 -DNV_CUDNN_DISABLE_EXCEPTION -DTF_ENABLE_CUDNN_FRONTEND /Fobazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.obj /c tensorflow/stream_executor/cuda/cuda_dnn.cc
# Configuration: 700a84dace1f582340b6e629381e69377869457813c799fd8ea6fec0cb9b2de1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
C:\users\Karma\_bazel\aakz3rh4\execroot\org_tensorflow\bazel-out\x64_windows-opt-exec-50AE0418\bin\external\cudnn_frontend_archive\_virtual_includes\cudnn_frontend\third_party\cudnn_frontend\include\cudnn_frontend_Heuristics.h(34): fatal error C1083: Cannot open include file: 'cudnn_frontend_EngineFallbackList.h': No such file or directory
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 1109.945s, Critical Path: 209.14s
INFO: 6782 processes: 366 internal, 6416 local.
FAILED: Build did NOT complete successfully
```
",Karma5s,2024-12-23 02:46:26+00:00,['tilakrayal'],2025-01-09 02:15:35+00:00,2025-01-09 02:15:32+00:00,https://github.com/tensorflow/tensorflow/issues/83566,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2560888236, 'issue_id': 2755089294, 'author': 'tilakrayal', 'body': '@Karma5s,\r\nTensorflow v2.10 is a pretty older version which does not actively supported. Also GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61226\r\n\r\nhttps://www.tensorflow.org/install/source_windows#install_gpu_support_optional\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 24, 9, 22, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804058, 'issue_id': 2755089294, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579040633, 'issue_id': 2755089294, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 9, 2, 15, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579040687, 'issue_id': 2755089294, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83566"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83566"">No</a>', 'created_at': datetime.datetime(2025, 1, 9, 2, 15, 34, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-24 09:22:28 UTC): @Karma5s,
Tensorflow v2.10 is a pretty older version which does not actively supported. Also GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin.

https://github.com/tensorflow/tensorflow/issues/61226

https://www.tensorflow.org/install/source_windows#install_gpu_support_optional

Thank you!

github-actions[bot] on (2025-01-01 02:06:08 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-09 02:15:31 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-09 02:15:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83566"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83566"">No</a>

"
2753768096,issue,closed,completed,DLL load failure,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

i want a successful run

### Standalone code to reproduce the issue

```shell
ImportError                               Traceback (most recent call last)
File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[4], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[4], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\envs\tensorflow\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",Davischoice1,2024-12-21 05:15:18+00:00,['Venkat6871'],2025-01-03 16:59:40+00:00,2025-01-03 16:59:37+00:00,https://github.com/tensorflow/tensorflow/issues/83508,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('TF 2.18', '')]","[{'comment_id': 2558457885, 'issue_id': 2753768096, 'author': 'bb94user', 'body': 'same error here', 'created_at': datetime.datetime(2024, 12, 22, 13, 30, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558846617, 'issue_id': 2753768096, 'author': 'langtontdangare', 'body': 'Same error here', 'created_at': datetime.datetime(2024, 12, 23, 3, 42, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560845015, 'issue_id': 2753768096, 'author': 'Venkat6871', 'body': 'Hi **@Davischoice1** ,\r\nApologies for the delay. Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 24, 8, 42, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561671906, 'issue_id': 2753768096, 'author': 'langtontdangare', 'body': 'tensorflow version: 2.18.Orc2', 'created_at': datetime.datetime(2024, 12, 25, 7, 10, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561696190, 'issue_id': 2753768096, 'author': 'langtontdangare', 'body': 'actually is version 2.18.0', 'created_at': datetime.datetime(2024, 12, 25, 7, 57, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563356927, 'issue_id': 2753768096, 'author': 'Venkat6871', 'body': 'Hi **@langtontdangare** ,\r\nCould you please open another issue with all the relevant details? This will make it easier for us to track and assist you effectively.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 27, 6, 6, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569542044, 'issue_id': 2753768096, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 16, 59, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569542093, 'issue_id': 2753768096, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83508"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83508"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 16, 59, 39, tzinfo=datetime.timezone.utc)}]","bb94user on (2024-12-22 13:30:27 UTC): same error here

langtontdangare on (2024-12-23 03:42:11 UTC): Same error here

Venkat6871 (Assginee) on (2024-12-24 08:42:19 UTC): Hi **@Davischoice1** ,
Apologies for the delay. Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

langtontdangare on (2024-12-25 07:10:38 UTC): tensorflow version: 2.18.Orc2

langtontdangare on (2024-12-25 07:57:35 UTC): actually is version 2.18.0

Venkat6871 (Assginee) on (2024-12-27 06:06:21 UTC): Hi **@langtontdangare** ,
Could you please open another issue with all the relevant details? This will make it easier for us to track and assist you effectively.
Thank you!

mihaimaruseac on (2025-01-03 16:59:37 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2025-01-03 16:59:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83508"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83508"">No</a>

"
2752078136,issue,open,,tensorflow-opt-cuda: error running on Linux via GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18.0

### Custom code

No

### OS platform and distribution

manjaro

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4

### GPU model and memory

RTX 4090 - 24GB

### Current behavior?

Hello all,
thank you for tensorflow!
I have installed:

`sudo pacman -S python-tensorflow-opt-cuda`

Actually I cannot run a tensorflow programm in IDE that runs well in windows - without GPU usage only on CPU.

I have this error on Manjaro - with GPU:

`Process finished with exit code 134 (interrupted by signal 6:SIGABRT)`

Python PyTorch on GPU just runs fine, though!

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn import datasets


digits = datasets.load_digits()
#
#
# # 2. Skalieren der Merkmale auf den Bereich [0, 1]
scaler = MinMaxScaler()
digits_scaled = scaler.fit_transform(digits.data)
#

# # Creating the encoder
enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
#
result = enc.fit_transform(digits.target.reshape(-1, 1))

ratio = 0.2
X_train, X_test, y_train, y_test = train_test_split(digits_scaled, result, test_size=ratio, random_state=42)

model1 = tf.keras.models.Sequential()
model1.add(tf.keras.layers.Input(X_train.shape[1:])) #Process finished with exit code 134 (interrupted by signal 6:SIGABRT)

# model1.add(tf.keras.layers.Dense(128, input_dim=64, activation=""relu"")) # hidden1
# model1.add(tf.keras.layers.Dense(64, activation=""relu"")) # hidden2
# model1.add(tf.keras.layers.Dense(10, activation='softmax')) # outputlayer
# model1.summary()
# model1.compile(loss=""categorical_crossentropy"", optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), # Adam()
#                metrics=(['accuracy']))
```


### Relevant log output

```shell
   ~/PycharmProjects/alfatraining_projekt_4/week1    main  python digits_uebung.py                                                                                                                    
2024-12-20 07:53:19.219496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734677599.229682    9678 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734677599.232640    9678 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
onehot labels:
 [[1. 0. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 1. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 1. 0.]]
/usr/include/c++/14.1.1/bits/stl_vector.h:1130: constexpr std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](size_type) [with _Tp = pybind11::object; _Alloc = std::allocator<pybind11::object>; reference = pybind11::object&; size_type = long unsigned int]: Assertion '__n < this->size()' failed.
zsh: IOT instruction (core dumped)  python digits_uebung.py
```
",leder11011,2024-12-20 06:58:28+00:00,['tilakrayal'],2025-01-02 21:55:21+00:00,,https://github.com/tensorflow/tensorflow/issues/83379,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2557960732, 'issue_id': 2752078136, 'author': 'micedevai', 'body': 'The error you\'re encountering seems to be a combination of issues related to the CUDA environment, TensorFlow, and possibly mismatches in library versions on Manjaro. The crash (Exit Code 134, signal 6: SIGABRT) typically points to a problem in the underlying system, often a segmentation fault, memory corruption, or an internal assert failure.\r\n\r\nHere are some steps to troubleshoot and potentially resolve the issue:\r\n\r\n### 1. **Verify TensorFlow with GPU Setup**\r\n   - Ensure TensorFlow can detect your GPU correctly. You can run the following snippet to check if TensorFlow sees your GPU:\r\n\r\n   ```python\r\n   import tensorflow as tf\r\n   print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices(\'GPU\')))\r\n   ```\r\n\r\n   If the output is `0`, it means TensorFlow is not able to access your GPU, and this could be the reason you\'re getting the crash.\r\n\r\n### 2. **Check cuDNN, cuBLAS, cuFFT Initialization Issues**\r\n   The logs suggest that TensorFlow is struggling to register cuDNN, cuBLAS, and cuFFT plugins. These are crucial for accelerated computation on NVIDIA GPUs.\r\n\r\n   - **cuDNN Version Mismatch**: The error message about registering the cuDNN factory usually occurs when there are conflicts in the cuDNN versions installed on the system. Since you have CUDA 12.4 installed, ensure that you also have the compatible cuDNN version installed.\r\n   \r\n   - Try running this command to ensure you have the correct versions of both CUDA and cuDNN:\r\n   \r\n     ```bash\r\n     pacman -S nvidia-cuda-toolkit nvidia-cudnn\r\n     ```\r\n\r\n   - **TensorFlow Compatibility**: Verify that the version of TensorFlow you\'re using is compatible with your installed CUDA and cuDNN versions. According to the TensorFlow release notes, TensorFlow 2.18.0 is compatible with CUDA 11.2 or 11.8. Since you\'re using CUDA 12.4, this could be causing issues. You might want to either:\r\n     - Downgrade your CUDA/cuDNN version to match TensorFlow\'s supported versions.\r\n     - Alternatively, try using a nightly build of TensorFlow that might support newer CUDA versions.\r\n\r\n### 3. **Memory Issues / GPU Overload**\r\n   The assertion error and core dump could also indicate a memory issue. The RTX 4090 is a powerful GPU, but there could still be memory overloads or memory leaks in the TensorFlow code. \r\n\r\n   Try limiting the GPU memory growth using the following configuration before running your TensorFlow code:\r\n\r\n   ```python\r\n   physical_devices = tf.config.list_physical_devices(\'GPU\')\r\n   tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n   ```\r\n\r\n### 4. **Update TensorFlow and Dependencies**\r\n   Ensure you\'re using the latest stable version of TensorFlow compatible with your system. Since you\'re on Manjaro, it\'s possible that some dependencies might be outdated or incompatible.\r\n\r\n   Run the following commands to update your system and TensorFlow dependencies:\r\n\r\n   ```bash\r\n   sudo pacman -Syu        # Update system\r\n   pip install --upgrade tensorflow\r\n   pip install --upgrade numpy\r\n   pip install --upgrade keras\r\n   ```\r\n\r\n   If you\'re still using the `python-tensorflow-opt-cuda` binary package from `pacman`, consider switching to the `pip`-based TensorFlow installation as the Manjaro package might be outdated or incorrectly configured.\r\n\r\n### 5. **Testing with CPU Only**\r\n   If the issue is specifically with GPU support, you can try disabling GPU usage temporarily to verify if the issue is related to CUDA. This can be done by setting the environment variable before running the script:\r\n\r\n   ```bash\r\n   export CUDA_VISIBLE_DEVICES=""""\r\n   ```\r\n\r\n   This forces TensorFlow to run on the CPU only and will help isolate whether the GPU setup is the root cause.\r\n\r\n### 6. **Simplify the Code**\r\n   In your code, you\'re using a `Sequential` model without specifying the input layer explicitly. TensorFlow should usually infer the input shape from the data, but specifying the input layer explicitly might resolve the issue. You can try this:\r\n\r\n   ```python\r\n   model1 = tf.keras.models.Sequential()\r\n   model1.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\r\n   ```\r\n\r\n### 7. **Check Logs and Debugging**\r\n   Given the core dump, it\'s useful to run your script under a debugger (like `gdb`) to get more detailed error information:\r\n\r\n   ```bash\r\n   gdb --args python digits_uebung.py\r\n   ```\r\n\r\n   After it crashes, type `bt` to get a backtrace that might reveal the specific function causing the crash.\r\n\r\n### Conclusion\r\n- **GPU setup**: Double-check your TensorFlow with CUDA/cuDNN installation.\r\n- **Environment**: Consider downgrading your CUDA version or using TensorFlow nightly if you need CUDA 12.4 support.\r\n- **Memory Management**: Enable GPU memory growth.\r\n- **Isolation**: Test with CPU only by setting `CUDA_VISIBLE_DEVICES=""""` to narrow down the issue.\r\n\r\nBy following these steps, you should be able to resolve the issue or at least isolate the root cause. Let me know if you need more assistance!', 'created_at': datetime.datetime(2024, 12, 21, 2, 30, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558045125, 'issue_id': 2752078136, 'author': 'johnnkp', 'body': '@micedevai Are you sure? CUDA plugin registration error https://github.com/tensorflow/tensorflow/issues/62075 occurred over a year. Many users is waiting fix from tensorflow team.', 'created_at': datetime.datetime(2024, 12, 21, 8, 12, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558147408, 'issue_id': 2752078136, 'author': 'micedevai', 'body': ""You're absolutely right to point out that the CUDA plugin registration error #62075 has been a persistent issue for many users in the TensorFlow community, especially with newer CUDA versions like 12.x. This error typically arises due to the combination of incompatible library versions, improper installation, or misconfigurations between TensorFlow, CUDA, and cuDNN."", 'created_at': datetime.datetime(2024, 12, 21, 15, 10, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563212856, 'issue_id': 2752078136, 'author': 'learning-to-play', 'body': ""@tilakrayal Please clarify what devinfra actions you want me to take. If this isn't a devinfra issue, please reach out to the correct team and clarify what actions you want them to take."", 'created_at': datetime.datetime(2024, 12, 27, 1, 18, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563227998, 'issue_id': 2752078136, 'author': 'johnnkp', 'body': '@tilakrayal @learning-to-play I already posted the required code changes in https://github.com/openxla/xla/issues/20803 and https://github.com/tensorflow/tensorflow/issues/62075#issuecomment-2535391485. They  assigned it to @ddunl 2 weeks ago and no status update afterwards.', 'created_at': datetime.datetime(2024, 12, 27, 1, 51, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564671615, 'issue_id': 2752078136, 'author': 'leder11011', 'body': '> The error you\'re encountering seems to be a combination of issues related to the CUDA environment, TensorFlow, and possibly mismatches in library versions on Manjaro. The crash (Exit Code 134, signal 6: SIGABRT) typically points to a problem in the underlying system, often a segmentation fault, memory corruption, or an internal assert failure.\n> \n> Here are some steps to troubleshoot and potentially resolve the issue:\n> \n> ### 1. **Verify TensorFlow with GPU Setup**\n>    - Ensure TensorFlow can detect your GPU correctly. You can run the following snippet to check if TensorFlow sees your GPU:\n> \n>    ```python\n>    import tensorflow as tf\n>    print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices(\'GPU\')))\n>    ```\n> \n>    If the output is `0`, it means TensorFlow is not able to access your GPU, and this could be the reason you\'re getting the crash.\n> \n> ### 2. **Check cuDNN, cuBLAS, cuFFT Initialization Issues**\n>    The logs suggest that TensorFlow is struggling to register cuDNN, cuBLAS, and cuFFT plugins. These are crucial for accelerated computation on NVIDIA GPUs.\n> \n>    - **cuDNN Version Mismatch**: The error message about registering the cuDNN factory usually occurs when there are conflicts in the cuDNN versions installed on the system. Since you have CUDA 12.4 installed, ensure that you also have the compatible cuDNN version installed.\n>    \n>    - Try running this command to ensure you have the correct versions of both CUDA and cuDNN:\n>    \n>      ```bash\n>      pacman -S nvidia-cuda-toolkit nvidia-cudnn\n>      ```\n> \n>    - **TensorFlow Compatibility**: Verify that the version of TensorFlow you\'re using is compatible with your installed CUDA and cuDNN versions. According to the TensorFlow release notes, TensorFlow 2.18.0 is compatible with CUDA 11.2 or 11.8. Since you\'re using CUDA 12.4, this could be causing issues. You might want to either:\n>      - Downgrade your CUDA/cuDNN version to match TensorFlow\'s supported versions.\n>      - Alternatively, try using a nightly build of TensorFlow that might support newer CUDA versions.\n> \n> ### 3. **Memory Issues / GPU Overload**\n>    The assertion error and core dump could also indicate a memory issue. The RTX 4090 is a powerful GPU, but there could still be memory overloads or memory leaks in the TensorFlow code. \n> \n>    Try limiting the GPU memory growth using the following configuration before running your TensorFlow code:\n> \n>    ```python\n>    physical_devices = tf.config.list_physical_devices(\'GPU\')\n>    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n>    ```\n> \n> ### 4. **Update TensorFlow and Dependencies**\n>    Ensure you\'re using the latest stable version of TensorFlow compatible with your system. Since you\'re on Manjaro, it\'s possible that some dependencies might be outdated or incompatible.\n> \n>    Run the following commands to update your system and TensorFlow dependencies:\n> \n>    ```bash\n>    sudo pacman -Syu        # Update system\n>    pip install --upgrade tensorflow\n>    pip install --upgrade numpy\n>    pip install --upgrade keras\n>    ```\n> \n>    If you\'re still using the `python-tensorflow-opt-cuda` binary package from `pacman`, consider switching to the `pip`-based TensorFlow installation as the Manjaro package might be outdated or incorrectly configured.\n> \n> ### 5. **Testing with CPU Only**\n>    If the issue is specifically with GPU support, you can try disabling GPU usage temporarily to verify if the issue is related to CUDA. This can be done by setting the environment variable before running the script:\n> \n>    ```bash\n>    export CUDA_VISIBLE_DEVICES=""""\n>    ```\n> \n>    This forces TensorFlow to run on the CPU only and will help isolate whether the GPU setup is the root cause.\n> \n> ### 6. **Simplify the Code**\n>    In your code, you\'re using a `Sequential` model without specifying the input layer explicitly. TensorFlow should usually infer the input shape from the data, but specifying the input layer explicitly might resolve the issue. You can try this:\n> \n>    ```python\n>    model1 = tf.keras.models.Sequential()\n>    model1.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n>    ```\n> \n> ### 7. **Check Logs and Debugging**\n>    Given the core dump, it\'s useful to run your script under a debugger (like `gdb`) to get more detailed error information:\n> \n>    ```bash\n>    gdb --args python digits_uebung.py\n>    ```\n> \n>    After it crashes, type `bt` to get a backtrace that might reveal the specific function causing the crash.\n> \n> ### Conclusion\n> - **GPU setup**: Double-check your TensorFlow with CUDA/cuDNN installation.\n> - **Environment**: Consider downgrading your CUDA version or using TensorFlow nightly if you need CUDA 12.4 support.\n> - **Memory Management**: Enable GPU memory growth.\n> - **Isolation**: Test with CPU only by setting `CUDA_VISIBLE_DEVICES=""""` to narrow down the issue.\n> \n> By following these steps, you should be able to resolve the issue or at least isolate the root cause. Let me know if you need more assistance!\n\nI will try isolation next. As of now I do not see any more actions on my side. Considering the other open tickets. Please tell me if there appear any more options I can check: perhaps testing a possible fix. \n\nBTW: this is my test output of tensorflow GPU\n\n```\n\uf312 \ue0b0 \uf015 ~ \ue0b0 python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))"" \ue0b2  \ue0b3 \uf013 2024-12-11 20:13:47.064157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1733944427.098978 6650 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1733944427.106910 6650 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered [PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\n```', 'created_at': datetime.datetime(2024, 12, 29, 9, 57, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564724782, 'issue_id': 2752078136, 'author': 'johnnkp', 'body': '@leder11011 The error output is same as mine and the mentioned issues. If you know how to add the `HasFactory()` checking to code and computer is fast enough to compile, just try first and see whether my fix works. Expected result is no registration errors when using CUDA 12.5 & cuDNN 9.3 (or any newer versions).', 'created_at': datetime.datetime(2024, 12, 29, 13, 23, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564729382, 'issue_id': 2752078136, 'author': 'leder11011', 'body': '> @leder11011 The error output is same as mine and the mentioned issues. If you know how to add the `HasFactory()` checking to code and computer is fast enough to compile, just try first and see whether my fix works. Expected result is no registration errors when using CUDA 12.5 & cuDNN 9.3 (or any newer versions).\n\n@johnnkp I could update CUDA, but I do not know of `HasFactory()`, sorry for that.', 'created_at': datetime.datetime(2024, 12, 29, 13, 40, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567050935, 'issue_id': 2752078136, 'author': 'leder11011', 'body': '> > The error you\'re encountering seems to be a combination of issues related to the CUDA environment, TensorFlow, and possibly mismatches in library versions on Manjaro. The crash (Exit Code 134, signal 6: SIGABRT) typically points to a problem in the underlying system, often a segmentation fault, memory corruption, or an internal assert failure.\r\n> > Here are some steps to troubleshoot and potentially resolve the issue:\r\n> > ### 1. **Verify TensorFlow with GPU Setup**\r\n> > \r\n> > * Ensure TensorFlow can detect your GPU correctly. You can run the following snippet to check if TensorFlow sees your GPU:\r\n> > \r\n> > ```python\r\n> > import tensorflow as tf\r\n> > print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices(\'GPU\')))\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > If the output is `0`, it means TensorFlow is not able to access your GPU, and this could be the reason you\'re getting the crash.\r\n> > ### 2. **Check cuDNN, cuBLAS, cuFFT Initialization Issues**\r\n> > The logs suggest that TensorFlow is struggling to register cuDNN, cuBLAS, and cuFFT plugins. These are crucial for accelerated computation on NVIDIA GPUs.\r\n> > \r\n> > * **cuDNN Version Mismatch**: The error message about registering the cuDNN factory usually occurs when there are conflicts in the cuDNN versions installed on the system. Since you have CUDA 12.4 installed, ensure that you also have the compatible cuDNN version installed.\r\n> > * Try running this command to ensure you have the correct versions of both CUDA and cuDNN:\r\n> >   ```shell\r\n> >   pacman -S nvidia-cuda-toolkit nvidia-cudnn\r\n> >   ```\r\n> > * **TensorFlow Compatibility**: Verify that the version of TensorFlow you\'re using is compatible with your installed CUDA and cuDNN versions. According to the TensorFlow release notes, TensorFlow 2.18.0 is compatible with CUDA 11.2 or 11.8. Since you\'re using CUDA 12.4, this could be causing issues. You might want to either:\r\n> >   \r\n> >   * Downgrade your CUDA/cuDNN version to match TensorFlow\'s supported versions.\r\n> >   * Alternatively, try using a nightly build of TensorFlow that might support newer CUDA versions.\r\n> > \r\n> > ### 3. **Memory Issues / GPU Overload**\r\n> > The assertion error and core dump could also indicate a memory issue. The RTX 4090 is a powerful GPU, but there could still be memory overloads or memory leaks in the TensorFlow code.\r\n> > Try limiting the GPU memory growth using the following configuration before running your TensorFlow code:\r\n> > ```python\r\n> > physical_devices = tf.config.list_physical_devices(\'GPU\')\r\n> > tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > ### 4. **Update TensorFlow and Dependencies**\r\n> > Ensure you\'re using the latest stable version of TensorFlow compatible with your system. Since you\'re on Manjaro, it\'s possible that some dependencies might be outdated or incompatible.\r\n> > Run the following commands to update your system and TensorFlow dependencies:\r\n> > ```shell\r\n> > sudo pacman -Syu        # Update system\r\n> > pip install --upgrade tensorflow\r\n> > pip install --upgrade numpy\r\n> > pip install --upgrade keras\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > If you\'re still using the `python-tensorflow-opt-cuda` binary package from `pacman`, consider switching to the `pip`-based TensorFlow installation as the Manjaro package might be outdated or incorrectly configured.\r\n> > ### 5. **Testing with CPU Only**\r\n> > If the issue is specifically with GPU support, you can try disabling GPU usage temporarily to verify if the issue is related to CUDA. This can be done by setting the environment variable before running the script:\r\n> > ```shell\r\n> > export CUDA_VISIBLE_DEVICES=""""\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > This forces TensorFlow to run on the CPU only and will help isolate whether the GPU setup is the root cause.\r\n> > ### 6. **Simplify the Code**\r\n> > In your code, you\'re using a `Sequential` model without specifying the input layer explicitly. TensorFlow should usually infer the input shape from the data, but specifying the input layer explicitly might resolve the issue. You can try this:\r\n> > ```python\r\n> > model1 = tf.keras.models.Sequential()\r\n> > model1.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > ### 7. **Check Logs and Debugging**\r\n> > Given the core dump, it\'s useful to run your script under a debugger (like `gdb`) to get more detailed error information:\r\n> > ```shell\r\n> > gdb --args python digits_uebung.py\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > After it crashes, type `bt` to get a backtrace that might reveal the specific function causing the crash.\r\n> > ### Conclusion\r\n> > \r\n> > * **GPU setup**: Double-check your TensorFlow with CUDA/cuDNN installation.\r\n> > * **Environment**: Consider downgrading your CUDA version or using TensorFlow nightly if you need CUDA 12.4 support.\r\n> > * **Memory Management**: Enable GPU memory growth.\r\n> > * **Isolation**: Test with CPU only by setting `CUDA_VISIBLE_DEVICES=""""` to narrow down the issue.\r\n> > \r\n> > By following these steps, you should be able to resolve the issue or at least isolate the root cause. Let me know if you need more assistance!\r\n> \r\n> I will try isolation next. As of now I do not see any more actions on my side. Considering the other open tickets. Please tell me if there appear any more options I can check: perhaps testing a possible fix.\r\n> \r\n> BTW: this is my test output of tensorflow GPU\r\n> \r\n> ```\r\n> \uf312 \ue0b0 \uf015 ~ \ue0b0 python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))"" \ue0b2  \ue0b3 \uf013 2024-12-11 20:13:47.064157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1733944427.098978 6650 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1733944427.106910 6650 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered [PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\n> ```\r\n\r\nI tried isolation 5) now, but the script does not run on CPU: same GPU error\r\n\r\nWhat am I doing wrong?', 'created_at': datetime.datetime(2025, 1, 1, 15, 28, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567051655, 'issue_id': 2752078136, 'author': 'leder11011', 'body': '> > @leder11011 The error output is same as mine and the mentioned issues. If you know how to add the `HasFactory()` checking to code and computer is fast enough to compile, just try first and see whether my fix works. Expected result is no registration errors when using CUDA 12.5 & cuDNN 9.3 (or any newer versions).\r\n> \r\n> @johnnkp I could update CUDA, but I do not know of `HasFactory()`, sorry for that.\r\n\r\nIt would be best for me, as you suggested, to test a nightly build including the fix! I have to remove package `python-tensorflow` and install from GIT, right?', 'created_at': datetime.datetime(2025, 1, 1, 15, 30, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567210421, 'issue_id': 2752078136, 'author': 'johnnkp', 'body': ""Not yet. We need to wait tensorflow team to fix. Don't be distracted by micedevai."", 'created_at': datetime.datetime(2025, 1, 2, 1, 17, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568429443, 'issue_id': 2752078136, 'author': 'leder11011', 'body': ""> Not yet. We need to wait tensorflow team to fix. Don't be distracted by micedevai.\n\nI will wait and see..."", 'created_at': datetime.datetime(2025, 1, 2, 21, 55, 20, tzinfo=datetime.timezone.utc)}]","micedevai on (2024-12-21 02:30:18 UTC): The error you're encountering seems to be a combination of issues related to the CUDA environment, TensorFlow, and possibly mismatches in library versions on Manjaro. The crash (Exit Code 134, signal 6: SIGABRT) typically points to a problem in the underlying system, often a segmentation fault, memory corruption, or an internal assert failure.

Here are some steps to troubleshoot and potentially resolve the issue:

### 1. **Verify TensorFlow with GPU Setup**
   - Ensure TensorFlow can detect your GPU correctly. You can run the following snippet to check if TensorFlow sees your GPU:

   ```python
   import tensorflow as tf
   print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
   ```

   If the output is `0`, it means TensorFlow is not able to access your GPU, and this could be the reason you're getting the crash.

### 2. **Check cuDNN, cuBLAS, cuFFT Initialization Issues**
   The logs suggest that TensorFlow is struggling to register cuDNN, cuBLAS, and cuFFT plugins. These are crucial for accelerated computation on NVIDIA GPUs.

   - **cuDNN Version Mismatch**: The error message about registering the cuDNN factory usually occurs when there are conflicts in the cuDNN versions installed on the system. Since you have CUDA 12.4 installed, ensure that you also have the compatible cuDNN version installed.
   
   - Try running this command to ensure you have the correct versions of both CUDA and cuDNN:
   
     ```bash
     pacman -S nvidia-cuda-toolkit nvidia-cudnn
     ```

   - **TensorFlow Compatibility**: Verify that the version of TensorFlow you're using is compatible with your installed CUDA and cuDNN versions. According to the TensorFlow release notes, TensorFlow 2.18.0 is compatible with CUDA 11.2 or 11.8. Since you're using CUDA 12.4, this could be causing issues. You might want to either:
     - Downgrade your CUDA/cuDNN version to match TensorFlow's supported versions.
     - Alternatively, try using a nightly build of TensorFlow that might support newer CUDA versions.

### 3. **Memory Issues / GPU Overload**
   The assertion error and core dump could also indicate a memory issue. The RTX 4090 is a powerful GPU, but there could still be memory overloads or memory leaks in the TensorFlow code. 

   Try limiting the GPU memory growth using the following configuration before running your TensorFlow code:

   ```python
   physical_devices = tf.config.list_physical_devices('GPU')
   tf.config.experimental.set_memory_growth(physical_devices[0], True)
   ```

### 4. **Update TensorFlow and Dependencies**
   Ensure you're using the latest stable version of TensorFlow compatible with your system. Since you're on Manjaro, it's possible that some dependencies might be outdated or incompatible.

   Run the following commands to update your system and TensorFlow dependencies:

   ```bash
   sudo pacman -Syu        # Update system
   pip install --upgrade tensorflow
   pip install --upgrade numpy
   pip install --upgrade keras
   ```

   If you're still using the `python-tensorflow-opt-cuda` binary package from `pacman`, consider switching to the `pip`-based TensorFlow installation as the Manjaro package might be outdated or incorrectly configured.

### 5. **Testing with CPU Only**
   If the issue is specifically with GPU support, you can try disabling GPU usage temporarily to verify if the issue is related to CUDA. This can be done by setting the environment variable before running the script:

   ```bash
   export CUDA_VISIBLE_DEVICES=""""
   ```

   This forces TensorFlow to run on the CPU only and will help isolate whether the GPU setup is the root cause.

### 6. **Simplify the Code**
   In your code, you're using a `Sequential` model without specifying the input layer explicitly. TensorFlow should usually infer the input shape from the data, but specifying the input layer explicitly might resolve the issue. You can try this:

   ```python
   model1 = tf.keras.models.Sequential()
   model1.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))
   ```

### 7. **Check Logs and Debugging**
   Given the core dump, it's useful to run your script under a debugger (like `gdb`) to get more detailed error information:

   ```bash
   gdb --args python digits_uebung.py
   ```

   After it crashes, type `bt` to get a backtrace that might reveal the specific function causing the crash.

### Conclusion
- **GPU setup**: Double-check your TensorFlow with CUDA/cuDNN installation.
- **Environment**: Consider downgrading your CUDA version or using TensorFlow nightly if you need CUDA 12.4 support.
- **Memory Management**: Enable GPU memory growth.
- **Isolation**: Test with CPU only by setting `CUDA_VISIBLE_DEVICES=""""` to narrow down the issue.

By following these steps, you should be able to resolve the issue or at least isolate the root cause. Let me know if you need more assistance!

johnnkp on (2024-12-21 08:12:11 UTC): @micedevai Are you sure? CUDA plugin registration error https://github.com/tensorflow/tensorflow/issues/62075 occurred over a year. Many users is waiting fix from tensorflow team.

micedevai on (2024-12-21 15:10:20 UTC): You're absolutely right to point out that the CUDA plugin registration error #62075 has been a persistent issue for many users in the TensorFlow community, especially with newer CUDA versions like 12.x. This error typically arises due to the combination of incompatible library versions, improper installation, or misconfigurations between TensorFlow, CUDA, and cuDNN.

learning-to-play on (2024-12-27 01:18:47 UTC): @tilakrayal Please clarify what devinfra actions you want me to take. If this isn't a devinfra issue, please reach out to the correct team and clarify what actions you want them to take.

johnnkp on (2024-12-27 01:51:51 UTC): @tilakrayal @learning-to-play I already posted the required code changes in https://github.com/openxla/xla/issues/20803 and https://github.com/tensorflow/tensorflow/issues/62075#issuecomment-2535391485. They  assigned it to @ddunl 2 weeks ago and no status update afterwards.

leder11011 (Issue Creator) on (2024-12-29 09:57:12 UTC): I will try isolation next. As of now I do not see any more actions on my side. Considering the other open tickets. Please tell me if there appear any more options I can check: perhaps testing a possible fix. 

BTW: this is my test output of tensorflow GPU

```
   ~  python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""     2024-12-11 20:13:47.064157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1733944427.098978 6650 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1733944427.106910 6650 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

johnnkp on (2024-12-29 13:23:47 UTC): @leder11011 The error output is same as mine and the mentioned issues. If you know how to add the `HasFactory()` checking to code and computer is fast enough to compile, just try first and see whether my fix works. Expected result is no registration errors when using CUDA 12.5 & cuDNN 9.3 (or any newer versions).

leder11011 (Issue Creator) on (2024-12-29 13:40:37 UTC): @johnnkp I could update CUDA, but I do not know of `HasFactory()`, sorry for that.

leder11011 (Issue Creator) on (2025-01-01 15:28:32 UTC): I tried isolation 5) now, but the script does not run on CPU: same GPU error

What am I doing wrong?

leder11011 (Issue Creator) on (2025-01-01 15:30:55 UTC): It would be best for me, as you suggested, to test a nightly build including the fix! I have to remove package `python-tensorflow` and install from GIT, right?

johnnkp on (2025-01-02 01:17:09 UTC): Not yet. We need to wait tensorflow team to fix. Don't be distracted by micedevai.

leder11011 (Issue Creator) on (2025-01-02 21:55:20 UTC): I will wait and see...

"
2751686092,issue,closed,completed,TF - Problems when trying to use GPU on M3 Max,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.2

### Custom code

Yes

### OS platform and distribution

Sequoia 15.2

### Mobile device

_No response_

### Python version

3.9.15

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

30-core (M3 Max)

### Current behavior?

I have tried to troubleshoot the problem together with ChatGPT o1, and after a lot of testing, it came to the conclusion that it might be a problem with tensorflow running om my M3 Max. It works fine with the CPU, but as soon as I try to use the GPU, no matter the complexity of the program, it crashes immediately.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import numpy as np

# Dummy data
X_train = np.random.random((1000, 24, 9))
y_train = np.random.random((1000, 9))
X_val = np.random.random((200, 24, 9))
y_val = np.random.random((200, 9))

# Bygg enkel modell
model = Sequential([
    Flatten(input_shape=(24, 9)),
    Dense(64, activation='relu'),
    Dense(9, activation='linear')
])

model.compile(optimizer='adam', loss='mse')

# Tren modell
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)
```


### Relevant log output

```shell
UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
2024-12-20 01:55:05.196093: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max
2024-12-20 01:55:05.196117: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB
2024-12-20 01:55:05.196122: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB
2024-12-20 01:55:05.196138: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2024-12-20 01:55:05.196152: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
Epoch 1/10
2024-12-20 01:55:05.402051: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
python(31809,0x1fb8e4240) malloc: *** error for object 0x315008234: pointer being freed was not allocated
python(31809,0x1fb8e4240) malloc: *** set a breakpoint in malloc_error_break to debug
zsh: abort
```
",tbjerke04,2024-12-20 00:55:58+00:00,['Venkat6871'],2025-01-02 08:58:10+00:00,2025-01-02 08:58:06+00:00,https://github.com/tensorflow/tensorflow/issues/83371,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2557963430, 'issue_id': 2751686092, 'author': 'micedevai', 'body': 'The issue you\'re encountering with TensorFlow on the Apple M3 Max GPU seems to be related to the Metal backend for TensorFlow. From the logs, we can see the following key information:\r\n\r\n1. **TensorFlow Metal Plugin**: The logs indicate that TensorFlow is trying to use the Metal API (`metal_plugin/src/device/metal_device.cc`) to run on the GPU, which is Apple\'s framework for GPU acceleration.\r\n2. **Error Message**: The error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated` suggests a memory allocation issue, possibly related to TensorFlow interacting with the Metal API.\r\n\r\nHere\'s a step-by-step approach to troubleshoot and resolve the issue:\r\n\r\n### 1. **Verify Metal Backend Installation**\r\n   TensorFlow uses Metal (Apple\'s GPU API) for macOS devices with M1 and M2 chips, and now M3 chips as well. Make sure that you have the correct version of TensorFlow that supports the Metal backend. As of TensorFlow 2.16.2, Metal support should be in place, but compatibility with the M3 Max specifically may have issues that are not present in older M1 or M2 chips.\r\n\r\n   You can verify if TensorFlow is correctly using the Metal backend by running this code:\r\n\r\n   ```python\r\n   import tensorflow as tf\r\n   from tensorflow.python.keras import backend as K\r\n\r\n   devices = K.tensorflow_backend._get_available_devices()\r\n   print(devices)\r\n   ```\r\n\r\n   This will give you information about which devices TensorFlow is recognizing, including your Metal-compatible GPU. If the GPU is listed, it indicates that TensorFlow is correctly identifying and attempting to use the Metal GPU backend.\r\n\r\n### 2. **Use CPU Instead of GPU**\r\n   If the issue is exclusive to the Metal GPU backend, you might want to disable the GPU usage temporarily to verify that the CPU setup works fine. You can do this by setting the environment variable `CUDA_VISIBLE_DEVICES` to an empty string before running your script:\r\n\r\n   ```bash\r\n   export CUDA_VISIBLE_DEVICES=""""\r\n   python your_script.py\r\n   ```\r\n\r\n   This will force TensorFlow to use the CPU instead of the GPU, which should allow the program to run without crashing. \r\n\r\n### 3. **Ensure Latest macOS & TensorFlow Version**\r\n   Since you\'re using the M3 Max, which is relatively new, it\'s possible that TensorFlow\'s Metal plugin hasn\'t been fully optimized for the new architecture. Heres what you can do:\r\n\r\n   - **Update macOS**: Make sure your macOS is up to date. Sometimes, macOS updates contain critical updates for the Metal API, which can affect TensorFlows ability to utilize the GPU correctly.\r\n   \r\n   - **Update TensorFlow**: Ensure you\'re using the latest nightly build or stable version of TensorFlow. Some Metal backend optimizations might not have been included in TensorFlow 2.16.2.\r\n\r\n     You can upgrade TensorFlow with pip:\r\n\r\n     ```bash\r\n     pip install --upgrade tensorflow\r\n     ```\r\n\r\n     Alternatively, you can try the latest nightly build, which may have better support for the M3 Max:\r\n\r\n     ```bash\r\n     pip install tensorflow-macos\r\n     ```\r\n\r\n     And if you want to ensure the latest support for Metal, you can install the TensorFlow nightly release for macOS:\r\n\r\n     ```bash\r\n     pip install tensorflow-macos==2.18.0-nightly\r\n     ```\r\n\r\n     After updating, try running the same code again to check if the issue is resolved.\r\n\r\n### 4. **Metal Device Compatibility**\r\n   TensorFlows Metal support is relatively new and evolving, so there could be issues related to certain hardware configurations like the M3 Max. While the GPU is being recognized in the logs, there might be subtle bugs or configuration issues specific to this model.\r\n\r\n   You could try running a simple GPU-accelerated operation (such as matrix multiplication or convolution) to see if TensorFlow crashes in a minimal setup. This would help confirm if the issue is with TensorFlows Metal implementation or something specific in your code.\r\n\r\n   Here\'s a minimal test you can try:\r\n\r\n   ```python\r\n   import tensorflow as tf\r\n   import numpy as np\r\n\r\n   # Create a simple tensor on the GPU\r\n   with tf.device(\'/GPU:0\'):\r\n       A = tf.random.normal([100, 100])\r\n       B = tf.random.normal([100, 100])\r\n       C = tf.matmul(A, B)\r\n       print(C)\r\n   ```\r\n\r\n   This test will let you know if TensorFlow can successfully execute basic GPU operations on the M3 Max.\r\n\r\n### 5. **Inspect TensorFlow Error Logs**\r\n   The error message you\'ve provided indicates that TensorFlow is crashing due to a memory issue (pointer being freed that wasn\'t allocated). To gain more insight into what\'s causing this crash, you can inspect more detailed logs using a debugger like `lldb` or `gdb` on macOS:\r\n\r\n   ```bash\r\n   lldb -- python your_script.py\r\n   ```\r\n\r\n   When the crash happens, you can type `bt` (backtrace) in the `lldb` prompt to get more detailed debugging information. This can help pinpoint the exact location in TensorFlow where the error occurs.\r\n\r\n### 6. **Try Disabling GPU Memory Growth**\r\n   If the issue is related to GPU memory allocation, you can try limiting GPU memory growth, which might help with memory fragmentation issues:\r\n\r\n   ```python\r\n   import tensorflow as tf\r\n\r\n   physical_devices = tf.config.list_physical_devices(\'GPU\')\r\n   tf.config.set_visible_devices(physical_devices[0], \'GPU\')\r\n   tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n   ```\r\n\r\n   This ensures that TensorFlow doesnt try to allocate all available memory upfront, and it may help avoid memory allocation crashes.\r\n\r\nIf the problem persists, it might be worth reporting it to the [[TensorFlow GitHub repository](https://github.com/tensorflow/tensorflow/issues)](https://github.com/tensorflow/tensorflow/issues) to see if it\'s a known issue or if any fixes are planned for the M3 Max.', 'created_at': datetime.datetime(2024, 12, 21, 2, 39, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559355922, 'issue_id': 2751686092, 'author': 'Venkat6871', 'body': 'Hi **@tbjerke04** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your referenceplease check them once, and let me know if I made any mistakes.\r\n![image (7)](https://github.com/user-attachments/assets/f1cc4edc-e559-44c0-b338-8a634d1c07b3)\r\n![image (6)](https://github.com/user-attachments/assets/494166f9-1ac0-4ade-85b5-1991aac40628)\r\n![image (8)](https://github.com/user-attachments/assets/b056f8b3-0b2f-4ff9-af13-8b08e9f2a5a9)\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 10, 7, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559805291, 'issue_id': 2751686092, 'author': 'tbjerke04', 'body': 'Hmm, okey thats weird. Many it is because Im using Python 3.9.15 and not 3.9.6, as you do?\r\n\r\nIll have to troubleshoot a little more. I am using an Anaconda environment, so maybe thats a problem too.\r\n\r\nAnyways, thanks for the update!\r\n\r\n\r\nHave a nice Christmas:)\r\n\r\nFra: Venkat6871 ***@***.***>\r\nDato: mandag, 23. desember 2024 kl. 11:08\r\nTil: tensorflow/tensorflow ***@***.***>\r\nKopi: Tobias Nklegrd Bjerke ***@***.***>, Mention ***@***.***>\r\nEmne: Re: [tensorflow/tensorflow] TF - Problems when trying to use GPU on M3 Max (Issue #83371)\r\n\r\nHi @tbjerke04<https://github.com/tbjerke04> ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your referenceplease check them once, and let me know if I made any mistakes.\r\nimage.7.png (view on web)<https://github.com/user-attachments/assets/f1cc4edc-e559-44c0-b338-8a634d1c07b3>\r\nimage.6.png (view on web)<https://github.com/user-attachments/assets/494166f9-1ac0-4ade-85b5-1991aac40628>\r\nimage.8.png (view on web)<https://github.com/user-attachments/assets/b056f8b3-0b2f-4ff9-af13-8b08e9f2a5a9>\r\nThank you!\r\n\r\n\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/83371#issuecomment-2559355922>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BNVZWFMCPY7IHSYBOVWMQOL2G7OJDAVCNFSM6AAAAABT6DTN2SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNJZGM2TKOJSGI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 12, 23, 14, 26, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562002040, 'issue_id': 2751686092, 'author': 'Vincent-presh', 'body': 'I have the same exact issue but on M1 air, runs well on cpu but not on metal GPU', 'created_at': datetime.datetime(2024, 12, 25, 21, 19, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562003449, 'issue_id': 2751686092, 'author': 'Vincent-presh', 'body': 'Also @micedevai, the minimal code you provided works, the crash only seems to occur when you run an operation from keras', 'created_at': datetime.datetime(2024, 12, 25, 21, 28, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562004323, 'issue_id': 2751686092, 'author': 'tbjerke04', 'body': 'I tried to switch from an Anaconda environment to using Brew, and it actually solved the problem!\r\n\r\nSent from Outlook for iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nFrom: Vincent Precious ***@***.***>\r\nSent: Wednesday, December 25, 2024 10:29:13 PM\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Tobias Nklegrd Bjerke ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] TF - Problems when trying to use GPU on M3 Max (Issue #83371)\r\n\r\n\r\nAlso @micedevai<https://github.com/micedevai>, the minimal code you provided works, the crash only seems to occur when you run an operation from keras\r\n\r\n\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/83371#issuecomment-2562003449>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BNVZWFMZ2PSTNFWD4JAHR7T2HMPSTAVCNFSM6AAAAABT6DTN2SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRSGAYDGNBUHE>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 12, 25, 21, 33, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562097859, 'issue_id': 2751686092, 'author': 'micedevai', 'body': 'It appears that the crash occurs specifically when running operations from Keras on the M3 Max GPU, while basic GPU operations such as matrix multiplication work fine. Based on the error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated`, this seems to be a memory allocation or management issue related to TensorFlow\'s Metal backend.\r\n\r\nSince the minimal test works but Keras operations cause the crash, here are a few additional steps you can take to troubleshoot and resolve the issue:\r\n\r\n---\r\n\r\n### 1. **Try a Simplified Keras Model**\r\n\r\nSometimes, certain layers or configurations in Keras might trigger memory management issues. To narrow down the cause, try running a simple Keras model with the same Metal backend setup and see if the crash still occurs. This can help isolate whether it\'s a specific layer or operation causing the issue.\r\n\r\nHere\'s an example of a minimal Keras model:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\n\r\n# Define a simple model\r\nmodel = models.Sequential([\r\n    layers.Dense(64, activation=\'relu\', input_shape=(100,)),\r\n    layers.Dense(10, activation=\'softmax\')\r\n])\r\n\r\n# Compile the model\r\nmodel.compile(optimizer=\'adam\', loss=\'sparse_categorical_crossentropy\', metrics=[\'accuracy\'])\r\n\r\n# Generate some random input data\r\nimport numpy as np\r\nx_train = np.random.random((1000, 100))\r\ny_train = np.random.randint(10, size=(1000,))\r\n\r\n# Train the model\r\nmodel.fit(x_train, y_train, epochs=5)\r\n```\r\n\r\nThis minimal Keras model can help determine if the crash is related to specific layers or operations in your full model.\r\n\r\n---\r\n\r\n### 2. **Disable Keras GPU Acceleration (For Debugging)**\r\n\r\nYou can try forcing Keras to run on the CPU instead of the GPU temporarily to isolate whether the problem is specific to the Metal backend with Keras. Set the environment variable to disable GPU usage:\r\n\r\n```bash\r\nexport CUDA_VISIBLE_DEVICES=""""\r\n```\r\n\r\nAlternatively, you can configure TensorFlow to explicitly use only the CPU within your script:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Set TensorFlow to use only the CPU\r\ntf.config.set_visible_devices([], \'GPU\')\r\n```\r\n\r\nIf the crash stops occurring when using the CPU, it strongly suggests the issue lies with GPU acceleration in Keras or the Metal backend.\r\n\r\n---\r\n\r\n### 3. **Disable Keras Layer Caching (Memory Fragmentation)**\r\n\r\nIf the issue is related to memory fragmentation, disabling Keras\'s layer caching might help. You can do this by clearing the session before running your Keras operations:\r\n\r\n```python\r\nfrom tensorflow.keras import backend as K\r\n\r\n# Clear the Keras session to reset the model\'s state\r\nK.clear_session()\r\n```\r\n\r\nClearing the session may help resolve issues related to GPU memory fragmentation or improper memory allocation during model creation and training.\r\n\r\n---\r\n\r\n### 4. **Check TensorFlow\'s Memory Management Settings**\r\n\r\nThe memory issue might be related to TensorFlow\'s GPU memory allocation strategy. You can adjust TensorFlow\'s memory growth settings to prevent it from allocating all available memory at once, which can help avoid crashes due to memory fragmentation.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# List available GPUs\r\nphysical_devices = tf.config.list_physical_devices(\'GPU\')\r\n\r\n# Set memory growth on GPU to avoid TensorFlow allocating all GPU memory upfront\r\nfor device in physical_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n\r\n# Check that memory growth is enabled\r\nfor device in physical_devices:\r\n    print(f""Memory growth enabled for {device}"")\r\n```\r\n\r\nThis ensures that TensorFlow only allocates GPU memory as needed, which might reduce memory-related issues.\r\n\r\n---\r\n\r\n### 5. **Investigate the Crash Using Debugging Tools**\r\n\r\nIf the problem persists and you\'re getting a crash related to memory allocation, use **lldb** or **gdb** to get more information about the exact point of failure. You can use the following command in the terminal to debug your script:\r\n\r\n```bash\r\nlldb -- python your_script.py\r\n```\r\n\r\nOnce the crash happens, you can type `bt` in the `lldb` prompt to get a backtrace and pinpoint where the crash is occurring in TensorFlow\'s code. This may provide more insight into whether the issue lies with the TensorFlow Metal implementation, Keras, or the way resources are managed.\r\n\r\n---\r\n\r\n### 6. **Update TensorFlow and macOS**\r\n\r\nAs the M3 Max is a relatively new chip, TensorFlow may not yet be fully optimized for it. Ensure that you\'re running the latest versions of both macOS and TensorFlow to benefit from the latest fixes and improvements, especially related to Metal support.\r\n\r\nTo upgrade TensorFlow:\r\n\r\n```bash\r\npip install --upgrade tensorflow-macos\r\n```\r\n\r\nOr install the latest nightly version that may include better Metal support for newer hardware:\r\n\r\n```bash\r\npip install tensorflow-macos==2.18.0-nightly\r\n```\r\n\r\nAdditionally, make sure your macOS is up to date, as Apple frequently releases updates that improve compatibility with the Metal framework, which could help with TensorFlow\'s ability to utilize the GPU correctly.\r\n\r\n---\r\n\r\n### 7. **Report the Issue**\r\n\r\nIf none of these steps resolve the issue, it might be worth reporting the bug to TensorFlow\'s GitHub repository. The issue could be a known problem with the Metal backend on the M3 Max or an issue that has not been fixed yet. You can report the issue on TensorFlow\'s GitHub [[issue tracker](https://github.com/tensorflow/tensorflow/issues)](https://github.com/tensorflow/tensorflow/issues).\r\n\r\nProvide details about your hardware (M3 Max), macOS version, TensorFlow version, the error message, and any debugging information you\'ve gathered (such as crash logs or backtraces).\r\n\r\n---\r\n\r\nBy following these steps, you should be able to either resolve the issue or gather enough information to report the bug and potentially receive a fix.', 'created_at': datetime.datetime(2024, 12, 26, 2, 47, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567305819, 'issue_id': 2751686092, 'author': 'Venkat6871', 'body': 'Hi **@tbjerke04** ,\r\nGlad to see your issue is resolved. Please feel free to close this issue if everything is working as expected.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 2, 5, 41, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567449837, 'issue_id': 2751686092, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83371"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83371"">No</a>', 'created_at': datetime.datetime(2025, 1, 2, 8, 58, 8, tzinfo=datetime.timezone.utc)}]","micedevai on (2024-12-21 02:39:37 UTC): The issue you're encountering with TensorFlow on the Apple M3 Max GPU seems to be related to the Metal backend for TensorFlow. From the logs, we can see the following key information:

1. **TensorFlow Metal Plugin**: The logs indicate that TensorFlow is trying to use the Metal API (`metal_plugin/src/device/metal_device.cc`) to run on the GPU, which is Apple's framework for GPU acceleration.
2. **Error Message**: The error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated` suggests a memory allocation issue, possibly related to TensorFlow interacting with the Metal API.

Here's a step-by-step approach to troubleshoot and resolve the issue:

### 1. **Verify Metal Backend Installation**
   TensorFlow uses Metal (Apple's GPU API) for macOS devices with M1 and M2 chips, and now M3 chips as well. Make sure that you have the correct version of TensorFlow that supports the Metal backend. As of TensorFlow 2.16.2, Metal support should be in place, but compatibility with the M3 Max specifically may have issues that are not present in older M1 or M2 chips.

   You can verify if TensorFlow is correctly using the Metal backend by running this code:

   ```python
   import tensorflow as tf
   from tensorflow.python.keras import backend as K

   devices = K.tensorflow_backend._get_available_devices()
   print(devices)
   ```

   This will give you information about which devices TensorFlow is recognizing, including your Metal-compatible GPU. If the GPU is listed, it indicates that TensorFlow is correctly identifying and attempting to use the Metal GPU backend.

### 2. **Use CPU Instead of GPU**
   If the issue is exclusive to the Metal GPU backend, you might want to disable the GPU usage temporarily to verify that the CPU setup works fine. You can do this by setting the environment variable `CUDA_VISIBLE_DEVICES` to an empty string before running your script:

   ```bash
   export CUDA_VISIBLE_DEVICES=""""
   python your_script.py
   ```

   This will force TensorFlow to use the CPU instead of the GPU, which should allow the program to run without crashing. 

### 3. **Ensure Latest macOS & TensorFlow Version**
   Since you're using the M3 Max, which is relatively new, it's possible that TensorFlow's Metal plugin hasn't been fully optimized for the new architecture. Heres what you can do:

   - **Update macOS**: Make sure your macOS is up to date. Sometimes, macOS updates contain critical updates for the Metal API, which can affect TensorFlows ability to utilize the GPU correctly.
   
   - **Update TensorFlow**: Ensure you're using the latest nightly build or stable version of TensorFlow. Some Metal backend optimizations might not have been included in TensorFlow 2.16.2.

     You can upgrade TensorFlow with pip:

     ```bash
     pip install --upgrade tensorflow
     ```

     Alternatively, you can try the latest nightly build, which may have better support for the M3 Max:

     ```bash
     pip install tensorflow-macos
     ```

     And if you want to ensure the latest support for Metal, you can install the TensorFlow nightly release for macOS:

     ```bash
     pip install tensorflow-macos==2.18.0-nightly
     ```

     After updating, try running the same code again to check if the issue is resolved.

### 4. **Metal Device Compatibility**
   TensorFlows Metal support is relatively new and evolving, so there could be issues related to certain hardware configurations like the M3 Max. While the GPU is being recognized in the logs, there might be subtle bugs or configuration issues specific to this model.

   You could try running a simple GPU-accelerated operation (such as matrix multiplication or convolution) to see if TensorFlow crashes in a minimal setup. This would help confirm if the issue is with TensorFlows Metal implementation or something specific in your code.

   Here's a minimal test you can try:

   ```python
   import tensorflow as tf
   import numpy as np

   # Create a simple tensor on the GPU
   with tf.device('/GPU:0'):
       A = tf.random.normal([100, 100])
       B = tf.random.normal([100, 100])
       C = tf.matmul(A, B)
       print(C)
   ```

   This test will let you know if TensorFlow can successfully execute basic GPU operations on the M3 Max.

### 5. **Inspect TensorFlow Error Logs**
   The error message you've provided indicates that TensorFlow is crashing due to a memory issue (pointer being freed that wasn't allocated). To gain more insight into what's causing this crash, you can inspect more detailed logs using a debugger like `lldb` or `gdb` on macOS:

   ```bash
   lldb -- python your_script.py
   ```

   When the crash happens, you can type `bt` (backtrace) in the `lldb` prompt to get more detailed debugging information. This can help pinpoint the exact location in TensorFlow where the error occurs.

### 6. **Try Disabling GPU Memory Growth**
   If the issue is related to GPU memory allocation, you can try limiting GPU memory growth, which might help with memory fragmentation issues:

   ```python
   import tensorflow as tf

   physical_devices = tf.config.list_physical_devices('GPU')
   tf.config.set_visible_devices(physical_devices[0], 'GPU')
   tf.config.experimental.set_memory_growth(physical_devices[0], True)
   ```

   This ensures that TensorFlow doesnt try to allocate all available memory upfront, and it may help avoid memory allocation crashes.

If the problem persists, it might be worth reporting it to the [[TensorFlow GitHub repository](https://github.com/tensorflow/tensorflow/issues)](https://github.com/tensorflow/tensorflow/issues) to see if it's a known issue or if any fixes are planned for the M3 Max.

Venkat6871 (Assginee) on (2024-12-23 10:07:48 UTC): Hi **@tbjerke04** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your referenceplease check them once, and let me know if I made any mistakes.
![image (7)](https://github.com/user-attachments/assets/f1cc4edc-e559-44c0-b338-8a634d1c07b3)
![image (6)](https://github.com/user-attachments/assets/494166f9-1ac0-4ade-85b5-1991aac40628)
![image (8)](https://github.com/user-attachments/assets/b056f8b3-0b2f-4ff9-af13-8b08e9f2a5a9)
Thank you!

tbjerke04 (Issue Creator) on (2024-12-23 14:26:49 UTC): Hmm, okey thats weird. Many it is because Im using Python 3.9.15 and not 3.9.6, as you do?

Ill have to troubleshoot a little more. I am using an Anaconda environment, so maybe thats a problem too.

Anyways, thanks for the update!


Have a nice Christmas:)

Fra: Venkat6871 ***@***.***>
Dato: mandag, 23. desember 2024 kl. 11:08
Til: tensorflow/tensorflow ***@***.***>
Kopi: Tobias Nklegrd Bjerke ***@***.***>, Mention ***@***.***>
Emne: Re: [tensorflow/tensorflow] TF - Problems when trying to use GPU on M3 Max (Issue #83371)

Hi @tbjerke04<https://github.com/tbjerke04> ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your referenceplease check them once, and let me know if I made any mistakes.
image.7.png (view on web)<https://github.com/user-attachments/assets/f1cc4edc-e559-44c0-b338-8a634d1c07b3>
image.6.png (view on web)<https://github.com/user-attachments/assets/494166f9-1ac0-4ade-85b5-1991aac40628>
image.8.png (view on web)<https://github.com/user-attachments/assets/b056f8b3-0b2f-4ff9-af13-8b08e9f2a5a9>
Thank you!


Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/83371#issuecomment-2559355922>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BNVZWFMCPY7IHSYBOVWMQOL2G7OJDAVCNFSM6AAAAABT6DTN2SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNJZGM2TKOJSGI>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

Vincent-presh on (2024-12-25 21:19:23 UTC): I have the same exact issue but on M1 air, runs well on cpu but not on metal GPU

Vincent-presh on (2024-12-25 21:28:48 UTC): Also @micedevai, the minimal code you provided works, the crash only seems to occur when you run an operation from keras

tbjerke04 (Issue Creator) on (2024-12-25 21:33:48 UTC): I tried to switch from an Anaconda environment to using Brew, and it actually solved the problem!

Sent from Outlook for iOS<https://aka.ms/o0ukef>
________________________________
From: Vincent Precious ***@***.***>
Sent: Wednesday, December 25, 2024 10:29:13 PM
To: tensorflow/tensorflow ***@***.***>
Cc: Tobias Nklegrd Bjerke ***@***.***>; Mention ***@***.***>
Subject: Re: [tensorflow/tensorflow] TF - Problems when trying to use GPU on M3 Max (Issue #83371)


Also @micedevai<https://github.com/micedevai>, the minimal code you provided works, the crash only seems to occur when you run an operation from keras


Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/83371#issuecomment-2562003449>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BNVZWFMZ2PSTNFWD4JAHR7T2HMPSTAVCNFSM6AAAAABT6DTN2SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRSGAYDGNBUHE>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

micedevai on (2024-12-26 02:47:34 UTC): It appears that the crash occurs specifically when running operations from Keras on the M3 Max GPU, while basic GPU operations such as matrix multiplication work fine. Based on the error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated`, this seems to be a memory allocation or management issue related to TensorFlow's Metal backend.

Since the minimal test works but Keras operations cause the crash, here are a few additional steps you can take to troubleshoot and resolve the issue:

---

### 1. **Try a Simplified Keras Model**

Sometimes, certain layers or configurations in Keras might trigger memory management issues. To narrow down the cause, try running a simple Keras model with the same Metal backend setup and see if the crash still occurs. This can help isolate whether it's a specific layer or operation causing the issue.

Here's an example of a minimal Keras model:

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# Define a simple model
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(100,)),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Generate some random input data
import numpy as np
x_train = np.random.random((1000, 100))
y_train = np.random.randint(10, size=(1000,))

# Train the model
model.fit(x_train, y_train, epochs=5)
```

This minimal Keras model can help determine if the crash is related to specific layers or operations in your full model.

---

### 2. **Disable Keras GPU Acceleration (For Debugging)**

You can try forcing Keras to run on the CPU instead of the GPU temporarily to isolate whether the problem is specific to the Metal backend with Keras. Set the environment variable to disable GPU usage:

```bash
export CUDA_VISIBLE_DEVICES=""""
```

Alternatively, you can configure TensorFlow to explicitly use only the CPU within your script:

```python
import tensorflow as tf

# Set TensorFlow to use only the CPU
tf.config.set_visible_devices([], 'GPU')
```

If the crash stops occurring when using the CPU, it strongly suggests the issue lies with GPU acceleration in Keras or the Metal backend.

---

### 3. **Disable Keras Layer Caching (Memory Fragmentation)**

If the issue is related to memory fragmentation, disabling Keras's layer caching might help. You can do this by clearing the session before running your Keras operations:

```python
from tensorflow.keras import backend as K

# Clear the Keras session to reset the model's state
K.clear_session()
```

Clearing the session may help resolve issues related to GPU memory fragmentation or improper memory allocation during model creation and training.

---

### 4. **Check TensorFlow's Memory Management Settings**

The memory issue might be related to TensorFlow's GPU memory allocation strategy. You can adjust TensorFlow's memory growth settings to prevent it from allocating all available memory at once, which can help avoid crashes due to memory fragmentation.

```python
import tensorflow as tf

# List available GPUs
physical_devices = tf.config.list_physical_devices('GPU')

# Set memory growth on GPU to avoid TensorFlow allocating all GPU memory upfront
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

# Check that memory growth is enabled
for device in physical_devices:
    print(f""Memory growth enabled for {device}"")
```

This ensures that TensorFlow only allocates GPU memory as needed, which might reduce memory-related issues.

---

### 5. **Investigate the Crash Using Debugging Tools**

If the problem persists and you're getting a crash related to memory allocation, use **lldb** or **gdb** to get more information about the exact point of failure. You can use the following command in the terminal to debug your script:

```bash
lldb -- python your_script.py
```

Once the crash happens, you can type `bt` in the `lldb` prompt to get a backtrace and pinpoint where the crash is occurring in TensorFlow's code. This may provide more insight into whether the issue lies with the TensorFlow Metal implementation, Keras, or the way resources are managed.

---

### 6. **Update TensorFlow and macOS**

As the M3 Max is a relatively new chip, TensorFlow may not yet be fully optimized for it. Ensure that you're running the latest versions of both macOS and TensorFlow to benefit from the latest fixes and improvements, especially related to Metal support.

To upgrade TensorFlow:

```bash
pip install --upgrade tensorflow-macos
```

Or install the latest nightly version that may include better Metal support for newer hardware:

```bash
pip install tensorflow-macos==2.18.0-nightly
```

Additionally, make sure your macOS is up to date, as Apple frequently releases updates that improve compatibility with the Metal framework, which could help with TensorFlow's ability to utilize the GPU correctly.

---

### 7. **Report the Issue**

If none of these steps resolve the issue, it might be worth reporting the bug to TensorFlow's GitHub repository. The issue could be a known problem with the Metal backend on the M3 Max or an issue that has not been fixed yet. You can report the issue on TensorFlow's GitHub [[issue tracker](https://github.com/tensorflow/tensorflow/issues)](https://github.com/tensorflow/tensorflow/issues).

Provide details about your hardware (M3 Max), macOS version, TensorFlow version, the error message, and any debugging information you've gathered (such as crash logs or backtraces).

---

By following these steps, you should be able to either resolve the issue or gather enough information to report the bug and potentially receive a fix.

Venkat6871 (Assginee) on (2025-01-02 05:41:46 UTC): Hi **@tbjerke04** ,
Glad to see your issue is resolved. Please feel free to close this issue if everything is working as expected.
Thank you!

google-ml-butler[bot] on (2025-01-02 08:58:08 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83371"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83371"">No</a>

"
2751574824,issue,closed,completed,Tflite support for XNN Pack for x86 CPU/Intel GPU in Python package,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tflite_runtime 2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.4

### Mobile device

NA

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't find a way to use the XNN Pack to leverage a GPU in my python repo with tflite. Trying to find ways to help my models perform better on a ubuntu system. 

### Standalone code to reproduce the issue

```shell
self._interpreter = tflite.Interpreter(
            model_path=model_path,
        )
        self._interpreter.allocate_tensors()

        input_details = self._interpreter.get_input_details()
        self._input_tensor_index = input_details[0][""index""]

        output_details = self._interpreter.get_output_details()
        self._output_tensor_index = output_details[0][""index""]
```


### Relevant log output

_No response_",nathant-tommys,2024-12-19 22:54:55+00:00,['gaikwadrahul8'],2025-01-15 01:59:57+00:00,2025-01-15 01:59:54+00:00,https://github.com/tensorflow/tensorflow/issues/83369,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:lite-xnnpack', 'TensorFlow Lite XNNPack related issues'), ('TF 2.11', 'Issues related to TF 2.11')]","[{'comment_id': 2565107275, 'issue_id': 2751574824, 'author': 'gaikwadrahul8', 'body': ""Hi, @nathant-tommys\r\nI apologize for the delayed response, I believe you've referred this [official documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) and [blog](https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html), If I'm not wrong It seems like TFLite does not support XNNPACK for x86 CPU/Intel GPU in Python and we've updated LiteRT (Formerly known as TFLite) delegates support so please refer these official documentations [LiteRT Delegates](https://ai.google.dev/edge/litert/performance/delegates) and [GPU delegates for LiteRT ](https://ai.google.dev/edge/litert/performance/gpu)\r\n\r\nTo confirm, did you try to enable `XNNPACK` via Bazel build flags (recommended on desktop) when building TensorFlow Lite with Bazel, add `--define tflite_with_xnnpack=true`, and the TensorFlow Lite interpreter will use `XNNPACK` engine by default ?\r\n\r\nThe exact command depends on the target platform, e.g. for Android AAR you'd use\r\n\r\n```\r\nbazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --define android_dexmerger_tool=d8_dexmerger \\\r\n  --define android_incremental_dexing_tool=d8_dexbuilder \\\r\n  --define tflite_with_xnnpack=true \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n\r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 30, 7, 12, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250336, 'issue_id': 2751574824, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476252, 'issue_id': 2751574824, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 15, 1, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476314, 'issue_id': 2751574824, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83369"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83369"">No</a>', 'created_at': datetime.datetime(2025, 1, 15, 1, 59, 56, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-30 07:12:54 UTC): Hi, @nathant-tommys
I apologize for the delayed response, I believe you've referred this [official documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) and [blog](https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html), If I'm not wrong It seems like TFLite does not support XNNPACK for x86 CPU/Intel GPU in Python and we've updated LiteRT (Formerly known as TFLite) delegates support so please refer these official documentations [LiteRT Delegates](https://ai.google.dev/edge/litert/performance/delegates) and [GPU delegates for LiteRT ](https://ai.google.dev/edge/litert/performance/gpu)

To confirm, did you try to enable `XNNPACK` via Bazel build flags (recommended on desktop) when building TensorFlow Lite with Bazel, add `--define tflite_with_xnnpack=true`, and the TensorFlow Lite interpreter will use `XNNPACK` engine by default ?

The exact command depends on the target platform, e.g. for Android AAR you'd use

```
bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --define android_dexmerger_tool=d8_dexmerger \
  --define android_incremental_dexing_tool=d8_dexbuilder \
  --define tflite_with_xnnpack=true \
  //tensorflow/lite/java:tensorflow-lite

```

Thank you for your cooperation and patience.

github-actions[bot] on (2025-01-07 02:02:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-15 01:59:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-15 01:59:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83369"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83369"">No</a>

"
2750201703,issue,open,,How to run TFLite benchmark with QNN delegate in Android,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

macOS 15.2

### Mobile device

One Plus 7 Pro, Android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have built/installed/run TFLite benchmark following this [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https://github.com/tensorflow/tensorflow/issues/66015). I test the benchmark via the following commands and the output result seems correct.
```shell
adb push /Users/handleychen/Github/tensorflow/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell ""mkdir /data/local/tmp/models""
adb push /Users/handleychen/Github/tensorflow/models/*.tflite /data/local/tmp/models
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true
``` 
[benchmark result.txt](https://github.com/user-attachments/files/18197819/benchmark.result.txt)

Now I want to run the benchmark with QNN delegate. I [setup the on device environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https://docs.qualcomm.com/bundle/publicresource/topics/80-70015-54/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).  I tested the benchmark using the following commands, but the result was a failure.
```shell
adb shell ""mkdir /data/local/tmp/qnn_delegate""
adb push /Users/handleychen/Github/quic/SDK/qairt/2.26.0.240828/lib/aarch64-android/* /data/local/tmp/qnn_delegate
adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'
# I also tried setting htp_precision:1, but the result was the same.
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'
``` 
```shell
# for gpu delegate

INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.


# for npu delegate
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
``` 
The full output is attached. [benchmarkQNN result.txt](https://github.com/user-attachments/files/18206356/benchmarkQNN.result.txt)

I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.

Could anyone tell me how to deal with this?

### Standalone code to reproduce the issue

```shell
as described above
```


### Relevant log output

_No response_",4570235,2024-12-19 12:40:25+00:00,['pkgoogle'],2025-01-10 00:21:22+00:00,,https://github.com/tensorflow/tensorflow/issues/83344,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2572784343, 'issue_id': 2750201703, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2025, 1, 6, 10, 9, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576009408, 'issue_id': 2750201703, 'author': 'pkgoogle', 'body': ""Hi @4570235, can you give me a very detailed reproducible script for how you built the benchmark_model executable. I.e. I'm looking to answer these questions:\r\n\r\n1. Did you use the dockerfile?\r\n2. Which NDK/SDK versions did you use?\r\n3. Which commit or branch of the source code did you use?\r\n4. What is the OS you built it on (if not the dockerfile)\r\n5. What was your actual build command?\r\n6. Did you run the configure script? If so, what were your answers?\r\n\r\nThanks for your help."", 'created_at': datetime.datetime(2025, 1, 7, 18, 54, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579158466, 'issue_id': 2750201703, 'author': '4570235', 'body': '@pkgoogle \r\n\r\n1. Did you use the dockerfile?\r\nNo. I followed this guide ""[Set up build environment without Docker](https://ai.google.dev/edge/litert/android/lite_build#set_up_build_environment_without_docker)"".\r\n\r\n2. Which NDK/SDK versions did you use?\r\nin the `.tf_configure.bazelrc` file:\r\n```\r\nbuild --action_env ANDROID_NDK_VERSION=""25""\r\nbuild --action_env ANDROID_NDK_API_LEVEL=""26""\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=""30.0.3""\r\nbuild --action_env ANDROID_SDK_API_LEVEL=""30""\r\n```\r\n\r\n3. Which commit or branch of the source code did you use?\r\n[Release 2.15.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.15.1)\r\n\r\n4. What is the OS you built it on (if not the dockerfile)\r\nmacOS Sequoia 15.2\r\n\r\n5. What was your actual build command?\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n\r\n6. Did you run the configure script? If so, what were your answers?\r\nYes. Here is the resulting config file: \r\n[tf_configure.bazelrc.txt](https://github.com/user-attachments/files/18355986/tf_configure.bazelrc.txt)\r\n\r\n7. Detailed reproducible steps for how I built the benchmark_model executable.\r\n\r\n(1) Install the latest version of Bazel by Homebrew.\r\n`  tensorflow git:(v2.15.1) brew install bazel`\r\n\r\n(2) Run the `./configure` script. The resulting config file is attached.\r\n[tf_configure.bazelrc.txt](https://github.com/user-attachments/files/18355986/tf_configure.bazelrc.txt)\r\n\r\n(3) Change Bazel version as required.\r\n``` shell\r\n  tensorflow git:(v2.15.1) bazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\nERROR: The project you\'re trying to build requires Bazel 6.1.0 (specified in /Users/handleychen/Github/tensorflow/tensorflow/.bazelversion), but it wasn\'t found in /usr/local/Cellar/bazel/7.4.1/libexec/bin.\r\n\r\nBazel binaries for all official releases can be downloaded from here:\r\n  https://github.com/bazelbuild/bazel/releases\r\n\r\nYou can download the required version directly using this command:\r\n  (cd ""/usr/local/Cellar/bazel/7.4.1/libexec/bin"" && curl -fLO https://releases.bazel.build/6.1.0/release/bazel-6.1.0-darwin-x86_64 && chmod +x bazel-6.1.0-darwin-x86_64)\r\n  tensorflow git:(v2.15.1) cd ""/usr/local/Cellar/bazel/7.4.1/libexec/bin"" && curl -fLO https://releases.bazel.build/6.1.0/release/bazel-6.1.0-darwin-x86_64 && chmod +x bazel-6.1.0-darwin-x86_64\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 48.1M  100 48.1M    0     0  4009k      0  0:00:12  0:00:12 --:--:-- 4508k\r\n  bin cd /Users/handleychen/Github/tensorflow/tensorflow\r\n  tensorflow git:(v2.15.1) bazel --version\r\nbazel 6.1.0\r\n```\r\n\r\n(4) Build.\r\n``` shell\r\n  tensorflow git:(v2.15.1) bazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\nStarting local Bazel server and connecting to it...\r\nINFO: Reading \'startup\' options from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited \'common\' options: --isatty=1 --terminal_columns=222\r\nINFO: Reading rc options for \'build\' from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc:\r\n  Inherited \'common\' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for \'build\' from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc:\r\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for \'build\' from /Users/handleychen/Github/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  \'build\' options: --action_env PYTHON_BIN_PATH=/Users/handleychen/.pyenv/versions/3.12.4/bin/python3 --action_env PYTHON_LIB_PATH=/Users/handleychen/.pyenv/versions/3.12.4/lib/python3.12/site-packages --python_path=/Users/handleychen/.pyenv/versions/3.12.4/bin/python3 --action_env ANDROID_NDK_HOME=/Users/handleychen/Library/Android/sdk/ndk/25.2.9519653 --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=26 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.3 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/Users/handleychen/library/Android/Sdk\r\nINFO: Found applicable config definition build:short_logs in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm64 in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt\r\nINFO: Found applicable config definition build:no_tfrt in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\r\nINFO: Analyzed target //tensorflow/lite/tools/benchmark:benchmark_model (179 packages loaded, 9898 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools/benchmark:benchmark_model up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model\r\nINFO: Elapsed time: 17.738s, Critical Path: 0.15s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\n  tensorflow git:(v2.15.1)\r\n```', 'created_at': datetime.datetime(2025, 1, 9, 4, 28, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581493116, 'issue_id': 2750201703, 'author': 'pkgoogle', 'body': 'So I ended up building on linux instead and with tf-nightly rather than 2.15 to see if it would make a difference, I mostly get the same result:\r\n\r\n```sh\r\ndm1q:/ $ cd /data/local/tmp                                                                                                                                                             \r\ndm1q:/data/local/tmp $ export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate\r\ndm1q:/data/local/tmp $ export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""\r\nte --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options=\'backend_type:gpu;\'                                                       <\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: Signature to run: []\r\nINFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:gpu;]\r\nINFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nINFO: TfLiteQnnDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.\r\n\r\nINFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nVERBOSE: Replacing 29 out of 31 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 4 partitions for subgraph 0.\r\nINFO: The input model file size (MB): 4.28787\r\nINFO: Initialized session in 43.481ms.\r\nINFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nINFO: count=19 first=27742 curr=27005 min=26976 max=28641 avg=27224.8 std=433 p5=26976 median=27002 p95=28641\r\n\r\nINFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\nINFO: count=50 first=27018 curr=26986 min=26976 max=27807 avg=27075.8 std=200 p5=26981 median=27003 p95=27632\r\n\r\nINFO: Inference timings in us: Init: 43481, First inference: 27742, Warmup (avg): 27224.8, Inference (avg): 27075.8\r\nINFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nINFO: Memory footprint delta from the start of the tool (MB): init=30.1328 overall=30.3906\r\nte --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options=\'backend_type:htp;\'                                                       <\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: Signature to run: []\r\nINFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:htp;]\r\nINFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008\r\nERROR: Restored original execution plan after delegate application failure.\r\nERROR: Failed to apply EXTERNAL delegate.\r\nERROR: Benchmarking failed.\r\nlite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options=\'backend_type:htp;htp_precision:1\'                                      <\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: Signature to run: []\r\nINFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:htp;htp_precision:1]\r\nINFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008\r\nERROR: Restored original execution plan after delegate application failure.\r\nERROR: Failed to apply EXTERNAL delegate.\r\nERROR: Benchmarking failed.\r\n```\r\n\r\n@v-dziuba, can you please take a look? Thanks.', 'created_at': datetime.datetime(2025, 1, 10, 0, 20, 38, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2025-01-06 10:09:05 UTC): Hi, @pkgoogle
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2025-01-07 18:54:29 UTC): Hi @4570235, can you give me a very detailed reproducible script for how you built the benchmark_model executable. I.e. I'm looking to answer these questions:

1. Did you use the dockerfile?
2. Which NDK/SDK versions did you use?
3. Which commit or branch of the source code did you use?
4. What is the OS you built it on (if not the dockerfile)
5. What was your actual build command?
6. Did you run the configure script? If so, what were your answers?

Thanks for your help.

4570235 (Issue Creator) on (2025-01-09 04:28:44 UTC): @pkgoogle 

1. Did you use the dockerfile?
No. I followed this guide ""[Set up build environment without Docker](https://ai.google.dev/edge/litert/android/lite_build#set_up_build_environment_without_docker)"".

2. Which NDK/SDK versions did you use?
in the `.tf_configure.bazelrc` file:
```
build --action_env ANDROID_NDK_VERSION=""25""
build --action_env ANDROID_NDK_API_LEVEL=""26""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""30.0.3""
build --action_env ANDROID_SDK_API_LEVEL=""30""
```

3. Which commit or branch of the source code did you use?
[Release 2.15.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.15.1)

4. What is the OS you built it on (if not the dockerfile)
macOS Sequoia 15.2

5. What was your actual build command?
```
bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark:benchmark_model
```

6. Did you run the configure script? If so, what were your answers?
Yes. Here is the resulting config file: 
[tf_configure.bazelrc.txt](https://github.com/user-attachments/files/18355986/tf_configure.bazelrc.txt)

7. Detailed reproducible steps for how I built the benchmark_model executable.

(1) Install the latest version of Bazel by Homebrew.
`  tensorflow git:(v2.15.1) brew install bazel`

(2) Run the `./configure` script. The resulting config file is attached.
[tf_configure.bazelrc.txt](https://github.com/user-attachments/files/18355986/tf_configure.bazelrc.txt)

(3) Change Bazel version as required.
``` shell
  tensorflow git:(v2.15.1) bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark:benchmark_model
ERROR: The project you're trying to build requires Bazel 6.1.0 (specified in /Users/handleychen/Github/tensorflow/tensorflow/.bazelversion), but it wasn't found in /usr/local/Cellar/bazel/7.4.1/libexec/bin.

Bazel binaries for all official releases can be downloaded from here:
  https://github.com/bazelbuild/bazel/releases

You can download the required version directly using this command:
  (cd ""/usr/local/Cellar/bazel/7.4.1/libexec/bin"" && curl -fLO https://releases.bazel.build/6.1.0/release/bazel-6.1.0-darwin-x86_64 && chmod +x bazel-6.1.0-darwin-x86_64)
  tensorflow git:(v2.15.1) cd ""/usr/local/Cellar/bazel/7.4.1/libexec/bin"" && curl -fLO https://releases.bazel.build/6.1.0/release/bazel-6.1.0-darwin-x86_64 && chmod +x bazel-6.1.0-darwin-x86_64
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 48.1M  100 48.1M    0     0  4009k      0  0:00:12  0:00:12 --:--:-- 4508k
  bin cd /Users/handleychen/Github/tensorflow/tensorflow
  tensorflow git:(v2.15.1) bazel --version
bazel 6.1.0
```

(4) Build.
``` shell
  tensorflow git:(v2.15.1) bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark:benchmark_model
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=222
INFO: Reading rc options for 'build' from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/handleychen/Github/tensorflow/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/handleychen/.pyenv/versions/3.12.4/bin/python3 --action_env PYTHON_LIB_PATH=/Users/handleychen/.pyenv/versions/3.12.4/lib/python3.12/site-packages --python_path=/Users/handleychen/.pyenv/versions/3.12.4/bin/python3 --action_env ANDROID_NDK_HOME=/Users/handleychen/Library/Android/sdk/ndk/25.2.9519653 --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=26 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.3 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/Users/handleychen/library/Android/Sdk
INFO: Found applicable config definition build:short_logs in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/handleychen/Github/tensorflow/tensorflow/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Analyzed target //tensorflow/lite/tools/benchmark:benchmark_model (179 packages loaded, 9898 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/tools/benchmark:benchmark_model up-to-date:
  bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model
INFO: Elapsed time: 17.738s, Critical Path: 0.15s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
  tensorflow git:(v2.15.1)
```

pkgoogle (Assginee) on (2025-01-10 00:20:38 UTC): So I ended up building on linux instead and with tf-nightly rather than 2.15 to see if it would make a difference, I mostly get the same result:

```sh
dm1q:/ $ cd /data/local/tmp                                                                                                                                                             
dm1q:/data/local/tmp $ export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
dm1q:/data/local/tmp $ export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
te --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'                                                       <
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: Signature to run: []
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:gpu;]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
INFO: TfLiteQnnDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.

INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 29 out of 31 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 4 partitions for subgraph 0.
INFO: The input model file size (MB): 4.28787
INFO: Initialized session in 43.481ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
INFO: count=19 first=27742 curr=27005 min=26976 max=28641 avg=27224.8 std=433 p5=26976 median=27002 p95=28641

INFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
INFO: count=50 first=27018 curr=26986 min=26976 max=27807 avg=27075.8 std=200 p5=26981 median=27003 p95=27632

INFO: Inference timings in us: Init: 43481, First inference: 27742, Warmup (avg): 27224.8, Inference (avg): 27075.8
INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
INFO: Memory footprint delta from the start of the tool (MB): init=30.1328 overall=30.3906
te --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'                                                       <
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: Signature to run: []
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
lite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'                                      <
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: Signature to run: []
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
```

@v-dziuba, can you please take a look? Thanks.

"
2749710114,issue,closed,not_planned,Problems with TFLite operator performance profiling,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

Android Google Pixel 6

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to profile the operator performance of my custom tflite model in android phone 'Google Pixel 6'.
I have followed the tutorial [measurement](https://github.com/tensorflow/tensorflow/blob/2c2fa69bb57442e274777a277f7f8bb8256a5ef3/tensorflow/lite/g3doc/performance/measurement.md#trace-tensorflow-lite-internals) and built a very simple test program [[source code](https://github.com/4570235/LiteRTStarter)]. However, I encounter some problems.

Firstly, when I tested the model '[mobilenetv1.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite)' from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification), if my program depends on the newest version of TFLite
```groovy
implementation 'org.tensorflow:tensorflow-lite:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'
``` 
then the Android Studio CPU Profiler shows 
![image](https://github.com/user-attachments/assets/1f101ba0-07d4-48ce-a65f-be2974611766)

I cannot see all the operators. After some testing, I figure out that the maximum version of TFLite that works is `tensorflow-lite:2.12.0` with `tensorflow-lite-support:0.4.0.`
```groovy
implementation 'org.tensorflow:tensorflow-lite:2.12.0'
implementation 'org.tensorflow:tensorflow-lite-support:0.4.0'
``` 
Here's the result: 
![image](https://github.com/user-attachments/assets/ef16d2a4-5430-41a1-8649-ad97b51888a0)

**So, is this a bug? How can I trace operator performance with the newest version of TFLite?**

Secondly, with `tensorflow-lite:2.12.0` and `tensorflow-lite-support:0.4.0` mentioned above, I tried to trace my custom model which is very similar to Qualcomm's '[quicksrnetsmall.tflite](https://aihub.qualcomm.com/models/quicksrnetsmall)'. Let's take 'quicksrnetsmall.tflite' as an sample and the Profiler shows
![image](https://github.com/user-attachments/assets/2f45f1cd-afc7-443a-a1be-36cd3b10bc47)

I cannot see any operators. **How can I fix it?**

### Standalone code to reproduce the issue

```shell
https://github.com/4570235/LiteRTStarter/blob/master/app/src/main/java/com/handleychen/litertstarter/Benchmark.java
```


### Relevant log output

_No response_",4570235,2024-12-19 09:19:11+00:00,['pkgoogle'],2025-01-24 21:08:57+00:00,2025-01-24 21:08:53+00:00,https://github.com/tensorflow/tensorflow/issues/83331,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2558977717, 'issue_id': 2749710114, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take a look into this issue. Thank you', 'created_at': datetime.datetime(2024, 12, 23, 6, 10, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613382694, 'issue_id': 2749710114, 'author': 'pkgoogle', 'body': 'Hello, we will be moving this issue to [LiteRT](https://github.com/google-ai-edge/LiteRT). Please follow progress there.', 'created_at': datetime.datetime(2025, 1, 24, 21, 8, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613382729, 'issue_id': 2749710114, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83331"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83331"">No</a>', 'created_at': datetime.datetime(2025, 1, 24, 21, 8, 55, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-12-23 06:10:22 UTC): Hi, @pkgoogle
Please take a look into this issue. Thank you

pkgoogle (Assginee) on (2025-01-24 21:08:53 UTC): Hello, we will be moving this issue to [LiteRT](https://github.com/google-ai-edge/LiteRT). Please follow progress there.

google-ml-butler[bot] on (2025-01-24 21:08:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83331"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83331"">No</a>

"
2748241445,issue,closed,completed,transfer learning with TF hub,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If you use tensorflow_hub.KerasLayer(correct parmeters), in a squential model: it won't recognize it as an acceptable layer. 

### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-7db51c8d2d71> in <cell line: 3>()
      1 IMAGE_SHAPE = (224, 224)
      2 
----> 3 classifier = tf.keras.Sequential([
      4     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))
      5 ])

1 frames
/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py in add(self, layer, rebuild)
     94                 layer = origin_layer
     95         if not isinstance(layer, Layer):
---> 96             raise ValueError(
     97                 ""Only instances of `keras.Layer` can be ""
     98                 f""added to a Sequential model. Received: {layer} ""

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7aafb7ccd2a0> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)
```
",lincoln12833,2024-12-18 16:18:50+00:00,['tilakrayal'],2024-12-30 13:22:34+00:00,2024-12-30 13:22:31+00:00,https://github.com/tensorflow/tensorflow/issues/83279,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('TF 2.18', '')]","[{'comment_id': 2552892628, 'issue_id': 2748241445, 'author': 'tilakrayal', 'body': '@lincoln12833,\r\nHi, By default the colab notebook is using tensorflow v2.17 and v2.18 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n\r\n```python\r\n!pip install tf-keras == 2.18.0\r\n\r\nimport tf_keras as keras\r\n```\r\n\r\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a437f436540f2e799c37102e2fe7904b/transfer_learning_with_hub.ipynb).\r\nI have raised the PR in the Tensorflow/docs repo for the similar issue.\r\nhttps://github.com/tensorflow/docs/pull/2344\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 19, 6, 29, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553459733, 'issue_id': 2748241445, 'author': 'Edwinhei', 'body': '![image](https://github.com/user-attachments/assets/a93d2add-5c97-48f5-8bfc-5edcfead09da)\r\nThe same configuration is running in the background, but it neither outputs nor ends.', 'created_at': datetime.datetime(2024, 12, 19, 11, 20, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554434271, 'issue_id': 2748241445, 'author': 'lincoln12833', 'body': 'I got that part working but I am getting the same error in this one after adding those lines https://github.com/lincoln12833/tensor_flow_hub_transfer_learning/blob/main/courses/udacity_intro_to_tensorflow_for_deep_learning/l06c01_tensorflow_hub_and_transfer_learning.ipynb @tilakrayal', 'created_at': datetime.datetime(2024, 12, 19, 14, 53, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556354276, 'issue_id': 2748241445, 'author': 'tilakrayal', 'body': '@lincoln12833,\r\nIn the code you are trying to import the keras as **import tf_keras as keras** and using tf.keras.sequential in the code. Could you try to change from layers.dense to keras.layers.dense and execute the code. In the above attached  gist, I have changed the tf.keras to keras, and try to execute the code.\r\n\r\nFrom:\r\n![image](https://github.com/user-attachments/assets/49b44cb4-9845-4624-a8a5-f954087419a8)\r\n\r\n\r\nTo:\r\n![image](https://github.com/user-attachments/assets/13a478df-4190-4a35-9017-66ed1c8898fa)\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 20, 6, 8, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137683, 'issue_id': 2748241445, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564802925, 'issue_id': 2748241445, 'author': 'lincoln12833', 'body': 'Thank you so much for all your help tilakrayal. This seemed to have fixed it.', 'created_at': datetime.datetime(2024, 12, 29, 18, 12, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565033047, 'issue_id': 2748241445, 'author': 'tilakrayal', 'body': '@lincoln12833,\r\nGlad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 12, 30, 5, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565479512, 'issue_id': 2748241445, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83279"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83279"">No</a>', 'created_at': datetime.datetime(2024, 12, 30, 13, 22, 33, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-19 06:29:26 UTC): @lincoln12833,
Hi, By default the colab notebook is using tensorflow v2.17 and v2.18 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.

```python
!pip install tf-keras == 2.18.0

import tf_keras as keras
```

Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a437f436540f2e799c37102e2fe7904b/transfer_learning_with_hub.ipynb).
I have raised the PR in the Tensorflow/docs repo for the similar issue.
https://github.com/tensorflow/docs/pull/2344

Thank you!

Edwinhei on (2024-12-19 11:20:15 UTC): ![image](https://github.com/user-attachments/assets/a93d2add-5c97-48f5-8bfc-5edcfead09da)
The same configuration is running in the background, but it neither outputs nor ends.

lincoln12833 (Issue Creator) on (2024-12-19 14:53:23 UTC): I got that part working but I am getting the same error in this one after adding those lines https://github.com/lincoln12833/tensor_flow_hub_transfer_learning/blob/main/courses/udacity_intro_to_tensorflow_for_deep_learning/l06c01_tensorflow_hub_and_transfer_learning.ipynb @tilakrayal

tilakrayal (Assginee) on (2024-12-20 06:08:08 UTC): @lincoln12833,
In the code you are trying to import the keras as **import tf_keras as keras** and using tf.keras.sequential in the code. Could you try to change from layers.dense to keras.layers.dense and execute the code. In the above attached  gist, I have changed the tf.keras to keras, and try to execute the code.

From:
![image](https://github.com/user-attachments/assets/49b44cb4-9845-4624-a8a5-f954087419a8)


To:
![image](https://github.com/user-attachments/assets/13a478df-4190-4a35-9017-66ed1c8898fa)


Thank you!

github-actions[bot] on (2024-12-28 01:59:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

lincoln12833 (Issue Creator) on (2024-12-29 18:12:20 UTC): Thank you so much for all your help tilakrayal. This seemed to have fixed it.

tilakrayal (Assginee) on (2024-12-30 05:10:57 UTC): @lincoln12833,
Glad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!

google-ml-butler[bot] on (2024-12-30 13:22:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83279"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83279"">No</a>

"
2745021663,issue,closed,completed,Problen with tensorflow,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

Windows 11  x64

### Mobile device

_No response_

### Python version

Python 3.9.21

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

 cudatoolkit=11.2 cudnn=8.1.0

### GPU model and memory

3070 ti

### Current behavior?

I was trying to follow this guide: https://www.tensorflow.org/install/pip#windows-native, but it doesn't work at stage 7. I think that it's a problem with TensorFlow, but I can't understand what exactly is wrong.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
TypeError                                 Traceback (most recent call last)
Cell In[7], [line 1](vscode-notebook-cell:?execution_count=7&line=1)
----> [1](vscode-notebook-cell:?execution_count=7&line=1) import tensorflow as tf
      [2](vscode-notebook-cell:?execution_count=7&line=2) from tensorflow.python.client import device_lib
      [4](vscode-notebook-cell:?execution_count=7&line=4) # Check if GPU is available

File c:\Users\NEMIFIST\miniconda3\envs\tf\lib\site-packages\tensorflow\__init__.py:37
     [34](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/__init__.py:34) import sys as _sys
     [35](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/__init__.py:35) import typing as _typing
---> [37](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/__init__.py:37) from tensorflow.python.tools import module_util as _module_util
     [38](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/__init__.py:38) from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     [40](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/__init__.py:40) # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File c:\Users\NEMIFIST\miniconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py:42
     [37](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:37) from tensorflow.python.eager import context
     [39](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:39) # pylint: enable=wildcard-import
     [40](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:40) 
     [41](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:41) # Bring in subpackages.
---> [42](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:42) from tensorflow.python import data
     [43](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:43) from tensorflow.python import distribute
     [44](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/__init__.py:44) # from tensorflow.python import keras

File c:\Users\NEMIFIST\miniconda3\envs\tf\lib\site-packages\tensorflow\python\data\__init__.py:21
     [15](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/data/__init__.py:15) """"""`tf.data.Dataset` API for input pipelines.
...
     [37](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/framework/dtypes.py:37) class DTypeMeta(type(_dtypes.DType), abc.ABCMeta):
     [38](file:///C:/Users/NEMIFIST/miniconda3/envs/tf/lib/site-packages/tensorflow/python/framework/dtypes.py:38)   pass

TypeError: Unable to convert function return value to a Python type! The signature was
",Ivan21213232,2024-12-17 13:59:56+00:00,['Venkat6871'],2024-12-18 17:40:48+00:00,2024-12-18 17:36:10+00:00,https://github.com/tensorflow/tensorflow/issues/83170,"[('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2548951890, 'issue_id': 2745021663, 'author': 'mihaimaruseac', 'body': '```\r\nImportError: Module use of python38.dll conflicts with this version of Python.\r\n```\r\n\r\nThis is the error. What version of Python are you using?', 'created_at': datetime.datetime(2024, 12, 17, 16, 28, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551865729, 'issue_id': 2745021663, 'author': 'Ivan21213232', 'body': '> ```\r\n> ImportError: Module use of python38.dll conflicts with this version of Python.\r\n> ```\r\n> \r\n> This is the error. What version of Python are you using?\r\n\r\nI am using Python 3.9.21', 'created_at': datetime.datetime(2024, 12, 18, 17, 12, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551910668, 'issue_id': 2745021663, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83170"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83170"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 36, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551919215, 'issue_id': 2745021663, 'author': 'mihaimaruseac', 'body': 'The errors message yesterday was different from the one after the edit today', 'created_at': datetime.datetime(2024, 12, 18, 17, 40, 46, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-12-17 16:28:21 UTC): ```
ImportError: Module use of python38.dll conflicts with this version of Python.
```

This is the error. What version of Python are you using?

Ivan21213232 (Issue Creator) on (2024-12-18 17:12:40 UTC): I am using Python 3.9.21

google-ml-butler[bot] on (2024-12-18 17:36:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83170"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83170"">No</a>

mihaimaruseac on (2024-12-18 17:40:46 UTC): The errors message yesterday was different from the one after the edit today

"
2744818270,issue,open,,custom loss function only receives one/first output from model declaring multiple outputs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

CentOS8_64

### Mobile device

_No response_

### Python version

3.9.20

### Bazel version

_No response_

### GCC/compiler version

11.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Declare a model with 2 named outputs, one for the bounding box and the second for the label classification.  
Implicit declaration of loss functions in the compile configuration works ok.
Explicit declaration of a custom_loss function in the compile configuration calls the custom_loss function, but it only receives the first output (bbox), not the expected two (bbox + labels)

### Standalone code to reproduce the issue

```shell
Model as (num_classes = 6 for ex):
bbox= layers.Dense(4, name=""bbox"")(features)
classification_output = layers.Dense(num_classes, name=""classification"", activation=""softmax"")(features)

model = keras.Model(inputs=inputs, outputs=[bbox, classification_output], name='vit_object_detector_with_class')

# Dictionary for bounding box loss
bbox_loss_dict = {
    ""mse_loss"": tf.keras.losses.MeanSquaredError(),  # Mean Squared Error (for bounding box regression)
    ""mae_loss"": tf.keras.losses.MeanAbsoluteError()   # Mean Absolute Error (alternative for bounding boxes)
}

# Dictionary for classification loss
class_loss_dict = {
    ""sparse_categorical_crossentropy"": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # Cross-entropy loss for multi-class classification
    ""categorical_crossentropy"": tf.keras.losses.CategoricalCrossentropy(from_logits=False)  # Another option for multi-class classification if using one-hot encoded labels
}


Implicit declaration of configuration, inclusive the losses that works ok:
    model.compile(
        optimizer='adam',  # or any optimizer
        loss={
            ""bbox"": bbox_loss_dict[""mse_loss""], 
            ""classification"": class_loss_dict[""sparse_categorical_crossentropy""] 
        },
        loss_weights={
        ""bbox"": 1.0,
        ""classification"": 1.5
        },
        metrics={
            ""bbox"": [""mse"", ""mae""],
            ""classification"": [""accuracy""]
        }
    )

#Training:

targets = {
        ""bbox"": bbox_target,  # shape, for ex: (640,4)
        ""classification"": class_target   # shape for ex: (640,)
    }
model.fit(x_train, targets, epochs=10, batch_size=32)

#That training works correctly.
#Now, if declaring explicitly a custom_loss function:

def custom_loss(y_true, y_pred):

    bbox_true = y_true[0]  # Bounding boxes ground truth
    class_true = y_true[1]  # Class labels ground truth
    
    bbox_pred = y_pred[0]  # Bounding box predictions
    class_pred = y_pred[1]  # Class predictions
...... etc

and declaring the configuration as:
 model.compile(optimizer='adam', loss=custom_loss, metrics=[""accuracy""])

the custom_loss function only receives the bbox data (y_pred.shape = (32,4)) but not the label classification. It should be something like: y_pred.shape = [(32,4),(32,)]
```


### Relevant log output

_No response_",r25hbgh,2024-12-17 12:32:49+00:00,['tilakrayal'],2025-02-08 01:58:31+00:00,,https://github.com/tensorflow/tensorflow/issues/83168,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2548572050, 'issue_id': 2744818270, 'author': 'r25hbgh', 'body': 'Very probably, the implicit loss definition is training without errors, but only the bbox data, not including the label classificator.\r\nIt seems as at the end of the training the   ""model.predict(input)"" delivers a 2D array containing only the bbox, without label class.', 'created_at': datetime.datetime(2024, 12, 17, 14, 13, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552833885, 'issue_id': 2744818270, 'author': 'tilakrayal', 'body': '@r25hbgh,\r\nI ran the code and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/b128b12dec6afd501bbf4e5536476900/untitled2282.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error. Thank you!', 'created_at': datetime.datetime(2024, 12, 19, 5, 41, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561282657, 'issue_id': 2744818270, 'author': 'anuj8052', 'body': 'The issue you\'re encountering arises because TensorFlow does not automatically pass multiple outputs into a single `custom_loss` function when multiple outputs are defined for a model. By default, TensorFlow assumes the loss function applies to one specific output.\r\n\r\nTo handle this correctly, you need to design your custom loss function and model configuration in a way that explicitly handles multiple outputs. Below is a solution to address this:\r\n\r\n**Solution: Combine Outputs and Losses in custom_loss**\r\nUse a dictionary-like approach to group outputs and their corresponding losses.\r\nPass both the bbox and classification outputs and ground truths explicitly into the custom_loss function.\r\n\r\n```\r\ndef custom_loss(y_true, y_pred):\r\n    bbox_true = y_true[""bbox""]\r\n    class_true = y_true[""classification""]\r\n    \r\n    bbox_pred = y_pred[""bbox""]\r\n    class_pred = y_pred[""classification""]\r\n\r\n    bbox_loss = tf.keras.losses.MeanSquaredError()(bbox_true, bbox_pred)\r\n    class_loss = tf.keras.losses.SparseCategoricalCrossentropy()(class_true, class_pred)\r\n\r\n    total_loss = bbox_loss + 1.5 * class_loss\r\n    return total_loss\r\n\r\nmodel.compile(\r\n    optimizer=""adam"",\r\n    loss=custom_loss,\r\n    metrics={\r\n        ""bbox"": [""mse"", ""mae""],\r\n        ""classification"": [""accuracy""]\r\n    }\r\n)\r\n\r\ntargets = {\r\n    ""bbox"": bbox_target,  # Shape: (640, 4)\r\n    ""classification"": class_target  # Shape: (640,)\r\n}\r\n\r\nmodel.fit(x_train, targets, epochs=10, batch_size=32)\r\n```', 'created_at': datetime.datetime(2024, 12, 24, 16, 43, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425493, 'issue_id': 2744818270, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 30, tzinfo=datetime.timezone.utc)}]","r25hbgh (Issue Creator) on (2024-12-17 14:13:45 UTC): Very probably, the implicit loss definition is training without errors, but only the bbox data, not including the label classificator.
It seems as at the end of the training the   ""model.predict(input)"" delivers a 2D array containing only the bbox, without label class.

tilakrayal (Assginee) on (2024-12-19 05:41:54 UTC): @r25hbgh,
I ran the code and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/b128b12dec6afd501bbf4e5536476900/untitled2282.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error. Thank you!

anuj8052 on (2024-12-24 16:43:30 UTC): The issue you're encountering arises because TensorFlow does not automatically pass multiple outputs into a single `custom_loss` function when multiple outputs are defined for a model. By default, TensorFlow assumes the loss function applies to one specific output.

To handle this correctly, you need to design your custom loss function and model configuration in a way that explicitly handles multiple outputs. Below is a solution to address this:

**Solution: Combine Outputs and Losses in custom_loss**
Use a dictionary-like approach to group outputs and their corresponding losses.
Pass both the bbox and classification outputs and ground truths explicitly into the custom_loss function.

```
def custom_loss(y_true, y_pred):
    bbox_true = y_true[""bbox""]
    class_true = y_true[""classification""]
    
    bbox_pred = y_pred[""bbox""]
    class_pred = y_pred[""classification""]

    bbox_loss = tf.keras.losses.MeanSquaredError()(bbox_true, bbox_pred)
    class_loss = tf.keras.losses.SparseCategoricalCrossentropy()(class_true, class_pred)

    total_loss = bbox_loss + 1.5 * class_loss
    return total_loss

model.compile(
    optimizer=""adam"",
    loss=custom_loss,
    metrics={
        ""bbox"": [""mse"", ""mae""],
        ""classification"": [""accuracy""]
    }
)

targets = {
    ""bbox"": bbox_target,  # Shape: (640, 4)
    ""classification"": class_target  # Shape: (640,)
}

model.fit(x_train, targets, epochs=10, batch_size=32)
```

github-actions[bot] on (2025-02-08 01:58:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2744757610,issue,open,,Aborted (core dumped) in `LearnedUnigramCandidateSampler`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

true_classes = tf.constant([], dtype=tf.int64)
num_true = 3590707793247644003
num_sampled = 126
unique = False
range_max = 186785497093039093
seed = 8997
seed2 = 0

tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=unique,
    range_max=range_max,
    seed=seed,
    seed2=seed2,
    name=None
)
```


### Relevant log output

```shell
2024-12-17 11:36:29.305345: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32
2024-12-17 11:36:29.305378: F external/local_tsl/tsl/lib/random/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)
Aborted (core dumped)
```
",LongZE666,2024-12-17 12:05:50+00:00,['Venkat6871'],2025-01-02 06:23:51+00:00,,https://github.com/tensorflow/tensorflow/issues/83164,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2552878491, 'issue_id': 2744757610, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cb615f145a50f873157222d579c3c710/83164_tf-2-18-0-nightly-v.ipynb) here for reference.\r\nThank you', 'created_at': datetime.datetime(2024, 12, 19, 6, 16, 12, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-19 06:16:12 UTC): I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/cb615f145a50f873157222d579c3c710/83164_tf-2-18-0-nightly-v.ipynb) here for reference.
Thank you

"
2743957632,issue,closed,completed,keras.layers.GRU behaves differently in GPU and CPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18, tf 2.19

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6/9.6

### GPU model and memory

_No response_

### Current behavior?

The result of the following code is the length of the GRU output variable.

The result is 2 when I executed the code with no GPU by `export CUDA_VISIBLE_DEVICES= `.  One is the output sequence, and the other is the state of GRU. However, the result is 5 = 1 + batch_size when I executed the code with GPU. The first is the output sequence, and the others are the state of GRU with batch unpacking.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.layers import GRU

batch_size = 4
time_steps = 10
rnn_dimension = 16
input_dim = 8

x = tf.random.normal((batch_size, time_steps, input_dim))

gru = GRU(rnn_dimension, return_sequences=True, return_state=True)

gru_output = gru(x, training=True)
print(len(gru_output))
```


### Relevant log output

_No response_",lwk8891,2024-12-17 05:19:35+00:00,['Venkat6871'],2024-12-27 06:44:26+00:00,2024-12-27 06:44:23+00:00,https://github.com/tensorflow/tensorflow/issues/83119,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2552868590, 'issue_id': 2743957632, 'author': 'Venkat6871', 'body': 'Hi **@lwk8891** ,\r\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.18.0 with both CPU [gist](https://colab.sandbox.google.com/gist/Venkat6871/4c365dfcd83e2d3c2e2ebafa0479fcd6/83119_tf-2-18-0-cpu-v.ipynb) and GPU and faced the same issue. The reason behind this issue is that when running on a GPU, TensorFlow uses the cuDNN-optimized GRU kernel. Even if you set `implementation=2`, the output may still differ on the GPU because the GRU layer internally optimizes for cuDNN, which can unpack the batch dimension for hidden states.\r\n\r\nTo resolve this, you need to explicitly disable the cuDNN kernel for the GRU. I achieved this by setting `reset_after=False`, and it produced the expected results. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/78ae3171c3750f66b36298dbda06c94b/83119_tf_2-18-0-gpu-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 19, 6, 8, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233206, 'issue_id': 2743957632, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563307572, 'issue_id': 2743957632, 'author': 'lwk8891', 'body': '> Hi **@lwk8891** , Apologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.18.0 with both CPU [gist](https://colab.sandbox.google.com/gist/Venkat6871/4c365dfcd83e2d3c2e2ebafa0479fcd6/83119_tf-2-18-0-cpu-v.ipynb) and GPU and faced the same issue. The reason behind this issue is that when running on a GPU, TensorFlow uses the cuDNN-optimized GRU kernel. Even if you set `implementation=2`, the output may still differ on the GPU because the GRU layer internally optimizes for cuDNN, which can unpack the batch dimension for hidden states.\r\n> \r\n> To resolve this, you need to explicitly disable the cuDNN kernel for the GRU. I achieved this by setting `reset_after=False`, and it produced the expected results. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/78ae3171c3750f66b36298dbda06c94b/83119_tf_2-18-0-gpu-v.ipynb) here for your reference. Thank you!\r\n\r\nThank you. It solved the problem.\r\nHowever, would setting `reset_after=False` affect the performance of the program by disabling the cuDNN kernel?', 'created_at': datetime.datetime(2024, 12, 27, 4, 40, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563371527, 'issue_id': 2743957632, 'author': 'Venkat6871', 'body': 'Hi **@lwk8891** ,\r\nGlad to see your issue is resolved. However, I suspect there might be a performance impact when using reset_after=False.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 27, 6, 29, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563380739, 'issue_id': 2743957632, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83119"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83119"">No</a>', 'created_at': datetime.datetime(2024, 12, 27, 6, 44, 25, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-19 06:08:06 UTC): Hi **@lwk8891** ,
Apologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.18.0 with both CPU [gist](https://colab.sandbox.google.com/gist/Venkat6871/4c365dfcd83e2d3c2e2ebafa0479fcd6/83119_tf-2-18-0-cpu-v.ipynb) and GPU and faced the same issue. The reason behind this issue is that when running on a GPU, TensorFlow uses the cuDNN-optimized GRU kernel. Even if you set `implementation=2`, the output may still differ on the GPU because the GRU layer internally optimizes for cuDNN, which can unpack the batch dimension for hidden states.

To resolve this, you need to explicitly disable the cuDNN kernel for the GRU. I achieved this by setting `reset_after=False`, and it produced the expected results. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/78ae3171c3750f66b36298dbda06c94b/83119_tf_2-18-0-gpu-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-12-27 02:01:02 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

lwk8891 (Issue Creator) on (2024-12-27 04:40:48 UTC): Thank you. It solved the problem.
However, would setting `reset_after=False` affect the performance of the program by disabling the cuDNN kernel?

Venkat6871 (Assginee) on (2024-12-27 06:29:43 UTC): Hi **@lwk8891** ,
Glad to see your issue is resolved. However, I suspect there might be a performance impact when using reset_after=False.
Thank you!

google-ml-butler[bot] on (2024-12-27 06:44:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83119"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83119"">No</a>

"
2743562886,issue,closed,not_planned,"I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, ass-backwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 3-4 days just trying to make this thing function.. That's literally all I've done.","              I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, ass-backwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 3-4 days just trying to make this thing function.. That's literally all I've done. 

All I see everywhere I've gone 'bazel-bin' But this damned thing doesn't create a bazel-bin symlink or folder.. It creates bazel-out and bazel-tensorflow, but no bazel-bin. I just has the epiphany just now to rename the bazel-out to bazel-bin, and adjusted this line to 
`` bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg``
added .py and also went into the folder with that .py file and made the damn thing executable with chmod +x build_pip_package.py cuz it still wouldn't run unless I did. Upon running it, I got the strangest damn thing I ever seen.

```
``[]  bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 25: Tool to rearrange files and build the wheel.

In a nutshell this script does:
1) Takes lists of paths to .h/.py/.so/etc files.
2) Creates a temporary directory.
3) Copies files from #1 to #2 with some exceptions and corrections.
4) A wheel is created from the files in the temp directory.

Most of the corrections are related to tsl/xla vendoring:
These files used to be a part of source code but were moved to an external repo.
To not break the TF API, we pretend that it's still part of the it.
: No such file or directory
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `subprocess' @ error/import.c/ImportImageCommand/1289.
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `sys' @ error/import.c/ImportImageCommand/1289.
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `tempfile' @ error/import.c/ImportImageCommand/1289.
from: too many arguments
from: too many arguments
from: too many arguments
from: too many arguments
from: too many arguments
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: syntax error near unexpected token `('
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: `def parse_args() -> argparse.Namespace:' ``
```

It ran just fine, sorta.. It froze my display and turned my cursor into a crosshair, and when I clicked, my screen unfroze. I had to click several times just to get to the end of that before it un-hyjacked my display. After which, I looked in the folder with the tensorflow folder that I cloned from github, it had turned 8 files in that folder into screenshots.. 
![Screenshot_20241216_024823](https://github.com/user-attachments/assets/8691f2f1-82cb-4344-b371-2d8c9464db6a)

Don't get me wrong, I don't mean to get mad at anyone here(or look like I am), I'm just agitated in general from this whole experience. I mean, how hard does it have to be to flip a few switches on optimization flags. In all honesty, when it comes right down to it, I'm sure this will just boil down to outdated dependencies used by new programs, or permissions nonsense of somesort. It's ALWAYS one of those two things with Linux.

> I am also trying to build from source try bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu i think also bazel version is important could you try with bazel 6.5. check the docs also https://www.tensorflow.org/install/source

Yea... I looked at that, and I'm about to have an aneurysm/heart attack just trying to process that.. especially due to all the 'apt-get' this and that. I'm on Garuda-Linux. I don't apt, I pacman, and tried getting python3-dev via pacman and nothing, tried yay, even tried flatpak, no python3-dev.. 

Well, I'm gonna go catatonic now while I sulk over my fail of the day..

_Originally posted by @Xephier102 in https://github.com/tensorflow/tensorflow/issues/82361#issuecomment-2544858145_
            ",Xephier102,2024-12-16 22:57:47+00:00,['tilakrayal'],2024-12-17 19:28:04+00:00,2024-12-17 13:02:07+00:00,https://github.com/tensorflow/tensorflow/issues/83100,[],"[{'comment_id': 2548404716, 'issue_id': 2743562886, 'author': 'mihaimaruseac', 'body': ""Duplicate of #82361 \r\n\r\nPlease don't open an issue from a comment in another open issue as that could be seen as spamming."", 'created_at': datetime.datetime(2024, 12, 17, 13, 2, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549256196, 'issue_id': 2743562886, 'author': 'Xephier102', 'body': ""> Duplicate of #82361\r\n> \r\n> Please don't open an issue from a comment in another open issue as that could be seen as spamming.\r\n\r\nApparently the bot removed the 'awaiting response' flag before anyone even responded, hence I made another. I don't type shit just to send information into the void for no reason. I got better things to do with my time."", 'created_at': datetime.datetime(2024, 12, 17, 18, 19, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549350545, 'issue_id': 2743562886, 'author': 'mihaimaruseac', 'body': 'Thank you for the context. The ""awaiting response"" label is for PRs where someone from triage team pinged to see if the issue is still relevant and gets removed as soon as there is some reply there (in the hope that that means that the issue is still relevant, so it should not be autoclosed). It is poorly named but it\'s also extremely hard to change it.\r\n\r\nApologies if the closing message was a little bit too harsh and thank you for explaining the context', 'created_at': datetime.datetime(2024, 12, 17, 18, 56, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549422431, 'issue_id': 2743562886, 'author': 'Xephier102', 'body': '> Thank you for the context. The ""awaiting response"" label is for PRs where someone from triage team pinged to see if the issue is still relevant and gets removed as soon as there is some reply there (in the hope that that means that the issue is still relevant, so it should not be autoclosed). It is poorly named but it\'s also extremely hard to change it.\r\n> \r\n> Apologies if the closing message was a little bit too harsh and thank you for explaining the context\r\n\r\nThank you for your politeness and tact in this matter. It\'s refreshing, in scenarios like this, not to just get dumped on and ghosted/cancelled/banned. I know I don\'t always come across very kind in my own communications, but that\'s because I try my hardest to do things on my own. So, by the time I get around to asking for help, I\'ve generally reached an agitated state, so that\'s what I put off.', 'created_at': datetime.datetime(2024, 12, 17, 19, 28, 3, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-12-17 13:02:08 UTC): Duplicate of #82361 

Please don't open an issue from a comment in another open issue as that could be seen as spamming.

Xephier102 (Issue Creator) on (2024-12-17 18:19:38 UTC): Apparently the bot removed the 'awaiting response' flag before anyone even responded, hence I made another. I don't type shit just to send information into the void for no reason. I got better things to do with my time.

mihaimaruseac on (2024-12-17 18:56:53 UTC): Thank you for the context. The ""awaiting response"" label is for PRs where someone from triage team pinged to see if the issue is still relevant and gets removed as soon as there is some reply there (in the hope that that means that the issue is still relevant, so it should not be autoclosed). It is poorly named but it's also extremely hard to change it.

Apologies if the closing message was a little bit too harsh and thank you for explaining the context

Xephier102 (Issue Creator) on (2024-12-17 19:28:03 UTC): Thank you for your politeness and tact in this matter. It's refreshing, in scenarios like this, not to just get dumped on and ghosted/cancelled/banned. I know I don't always come across very kind in my own communications, but that's because I try my hardest to do things on my own. So, by the time I get around to asking for help, I've generally reached an agitated state, so that's what I put off.

"
2743527650,issue,closed,completed,pip install ERROR: Could not find a version that satisfies the requirement tensorflow==2.10.0rc4 (from versions: none),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

Latitude

### Python version

3.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I try to install TensorFlow using pip install tensorflow==2.10.0, the process fails with an error stating that no matching distribution was found for the specified version.

### Standalone code to reproduce the issue

```shell
(venv) PS C:\Users\navee\Desktop\workspace\Skincare_Recommendation> python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
    import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
    ^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'tensorflow'
```


### Relevant log output

```shell
ERROR: No matching distribution found for tensorflow==2.1.0
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
```
",sikandarsubhani,2024-12-16 22:31:46+00:00,['Venkat6871'],2024-12-20 09:33:52+00:00,2024-12-20 09:33:49+00:00,https://github.com/tensorflow/tensorflow/issues/83097,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2550625497, 'issue_id': 2743527650, 'author': 'Venkat6871', 'body': 'Hi **@sikandarsubhani** ,\r\nThank you for raising your concern. It seems there are version compatibility issues. With TensorFlow 2.10.0, the compatible Python versions are 3.73.10. Please install the appropriate versions to ensure smooth results.\r\nFor your reference, please find the [documentation](https://www.tensorflow.org/install/source_windows#cpu) attached.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 18, 8, 8, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556617802, 'issue_id': 2743527650, 'author': 'sikandarsubhani', 'body': 'Got It! Thanks!!', 'created_at': datetime.datetime(2024, 12, 20, 9, 33, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556617869, 'issue_id': 2743527650, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83097"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83097"">No</a>', 'created_at': datetime.datetime(2024, 12, 20, 9, 33, 51, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-18 08:08:45 UTC): Hi **@sikandarsubhani** ,
Thank you for raising your concern. It seems there are version compatibility issues. With TensorFlow 2.10.0, the compatible Python versions are 3.73.10. Please install the appropriate versions to ensure smooth results.
For your reference, please find the [documentation](https://www.tensorflow.org/install/source_windows#cpu) attached.
Thank you!

sikandarsubhani (Issue Creator) on (2024-12-20 09:33:49 UTC): Got It! Thanks!!

google-ml-butler[bot] on (2024-12-20 09:33:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83097"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83097"">No</a>

"
2742467494,issue,open,,[XLA] `tf.keras.layers.LSTM` behaves differently on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When executing LSTM on **XLA**, it fails.
However, when executing it without XLA, it passes.
The above failure is on GPU.
If I use CPU as backend, with or without XLA both pass the check.

### Standalone code to reproduce the issue

```python
import os
import tensorflow
import tensorflow as tf
tf.random.set_seed(42)
class RecurrentModel(tf.keras.Model):

    def __init__(self):
        super(RecurrentModel, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)

    @tf.function(jit_compile=True)
    def call(self, x):
        return self.lstm(x)


model = RecurrentModel()


input_shape = (10, 20, 1)
x = tf.random.normal(shape=input_shape)

inputs = [x]

output = model(*inputs)
print(output)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-0938fdccd1fa> in <cell line: 24>()
     22 inputs = [x]
     23 
---> 24 output = model(*inputs)
     25 print(output)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Exception encountered when calling RecurrentModel.call().

Detected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1/CudnnRNNV3}}){{node lstm_3_1/CudnnRNNV3}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-0938fdccd1fa>"", line 24, in <cell line: 24>
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 826, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1376, in _maybe_build
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 212, in compute_output_spec
File ""<ipython-input-1-0938fdccd1fa>"", line 13, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 901, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 570, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py"", line 406, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 537, in inner_loop
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 841, in lstm
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 933, in _cudnn_lstm
	tf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]

Arguments received by RecurrentModel.call():
   x=tf.Tensor(shape=(10, 20, 1), dtype=float32)
```
",shaoyuyoung,2024-12-16 13:57:22+00:00,['Venkat6871'],2025-01-02 06:49:49+00:00,,https://github.com/tensorflow/tensorflow/issues/83063,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues'), ('comp:xla', 'XLA'), ('TF 2.18', '')]","[{'comment_id': 2545704466, 'issue_id': 2742467494, 'author': 'shaoyuyoung', 'body': 'here is the [gist](https://colab.research.google.com/drive/1utMoKJIzBmF_UvYW4nwVNRloBq50X_RA?usp=sharing)', 'created_at': datetime.datetime(2024, 12, 16, 13, 58, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547795563, 'issue_id': 2742467494, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TensorFlow versions 2.18.0 and TF-nightly. Please [find](https://colab.sandbox.google.com/gist/Venkat6871/ba9c6514d528d012ae6ebc755c6f7aa5/83063_tf_2-18-0-nightly-v.ipynb) the gist here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 17, 8, 29, 23, tzinfo=datetime.timezone.utc)}]","shaoyuyoung (Issue Creator) on (2024-12-16 13:58:57 UTC): here is the [gist](https://colab.research.google.com/drive/1utMoKJIzBmF_UvYW4nwVNRloBq50X_RA?usp=sharing)

Venkat6871 (Assginee) on (2024-12-17 08:29:23 UTC): I was able to reproduce the issue on Colab using TensorFlow versions 2.18.0 and TF-nightly. Please [find](https://colab.sandbox.google.com/gist/Venkat6871/ba9c6514d528d012ae6ebc755c6f7aa5/83063_tf_2-18-0-nightly-v.ipynb) the gist here for your reference.
Thank you!

"
2741549903,issue,open,,`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a new issue in replacement for https://github.com/tensorflow/tensorflow/issues/59761 as suggested by @tilakrayal

I tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.
I run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.

Also, I am not getting as much debug information only this error

`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:
```
",fnhirwa,2024-12-16 07:22:01+00:00,['Venkat6871'],2024-12-20 06:35:04+00:00,,https://github.com/tensorflow/tensorflow/issues/83037,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2547507772, 'issue_id': 2741549903, 'author': 'Venkat6871', 'body': 'Hi **@fnhirwa** ,\r\nThank you for raising your concern and for your patience. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions, and it is working fine for me. I observed that the issue exists with TensorFlow 2.17.0, but it works fine in the latest versions. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4ba39a804a271e7f2f449a3aa0dd92ff/83037_tf_2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 17, 4, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547849291, 'issue_id': 2741549903, 'author': 'fnhirwa', 'body': 'The code works fine because of the `try`-`except` block I added to the Tensorflow execution.\r\n\r\n```python\r\ntry:\r\n    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))\r\n    b = tf.constant(np.arange(24).reshape(2, 3, 4))\r\n    print(a.ndim) # 4\r\n    print(b.ndim) # 3\r\n\r\n    y = tf.experimental.numpy.kron(a, b)\r\n\r\n    print(y.shape)\r\nexcept:\r\n    print(""Can\'t use tf.experimental.numpy.kron on multi-dimensional arrays"") # Expected to print this with both nightly and current versions\r\n```\r\nif you try the above code without the try-except block you will get the following error:\r\n```\r\nUnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: \r\n```\r\nI was expecting to get the shape `(2, 10, 6, 20)` as it is provided by numpy:\r\n\r\n```python\r\nx = np.arange(100).reshape(2, 5, 2, 5)\r\ny = np.arange(24).reshape(2, 3, 4)\r\n\r\nprint(x.ndim) # 4\r\nprint(y.ndim) # 3\r\n\r\nz = np.kron(x, y)\r\n\r\nprint(z.shape) #  (2, 10, 6, 20)\r\n```\r\n\r\nI hope this makes sense to you.\r\nThanks', 'created_at': datetime.datetime(2024, 12, 17, 8, 55, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556379642, 'issue_id': 2741549903, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on Colab using TF v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/28ca11c9dfd6bce1aaa6918d3d9d4364/83037_tf-2-18-0-nightly-v.ipynb) here for reference .Thanks!', 'created_at': datetime.datetime(2024, 12, 20, 6, 34, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-17 04:57:00 UTC): Hi **@fnhirwa** ,
Thank you for raising your concern and for your patience. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions, and it is working fine for me. I observed that the issue exists with TensorFlow 2.17.0, but it works fine in the latest versions. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4ba39a804a271e7f2f449a3aa0dd92ff/83037_tf_2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

fnhirwa (Issue Creator) on (2024-12-17 08:55:18 UTC): The code works fine because of the `try`-`except` block I added to the Tensorflow execution.

```python
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"") # Expected to print this with both nightly and current versions
```
if you try the above code without the try-except block you will get the following error:
```
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: 
```
I was expecting to get the shape `(2, 10, 6, 20)` as it is provided by numpy:

```python
x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) #  (2, 10, 6, 20)
```

I hope this makes sense to you.
Thanks

Venkat6871 (Assginee) on (2024-12-20 06:34:55 UTC): I was able to reproduce the issue on Colab using TF v2.18.0 and TF-nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/28ca11c9dfd6bce1aaa6918d3d9d4364/83037_tf-2-18-0-nightly-v.ipynb) here for reference .Thanks!

"
2740560048,issue,closed,completed,TensorFlow 2.10.0 cannot be installed on Python 2.7.5.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

2.7.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

i expected the code to run

### Standalone code to reproduce the issue

```shell
I am trying to run the project using the versions specified in the documentation:

Python: 2.7.5
TensorFlow: 2.10.0
Numpy: 1.23.2
Nibabel: 4.0.2
Scipy: 1.9
However, I encountered issues since these versions (e.g., TensorFlow 2.10.0 and Numpy 1.23.2) are incompatible with Python 2.7. TensorFlow 2.x requires Python 3.7 or above.
```


### Relevant log output

_No response_",braahkrayem,2024-12-15 11:23:18+00:00,['tilakrayal'],2025-01-01 02:06:15+00:00,2025-01-01 02:06:12+00:00,https://github.com/tensorflow/tensorflow/issues/83013,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2547579472, 'issue_id': 2740560048, 'author': 'tilakrayal', 'body': '@braahkrayem,\r\nCurrently Tensorflow v2.10 is not compatible with the python 2.7.5. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.\r\nhttps://www.tensorflow.org/install/source_windows#cpu\r\n\r\nAlso Tensorflow v2.10 is a pretty older version, please upgrade to the latest Tensorflow version. Thank you!', 'created_at': datetime.datetime(2024, 12, 17, 6, 6, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561542796, 'issue_id': 2740560048, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 25, 2, 0, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804107, 'issue_id': 2740560048, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804136, 'issue_id': 2740560048, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83013"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83013"">No</a>', 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 14, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-17 06:06:40 UTC): @braahkrayem,
Currently Tensorflow v2.10 is not compatible with the python 2.7.5. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.
https://www.tensorflow.org/install/source_windows#cpu

Also Tensorflow v2.10 is a pretty older version, please upgrade to the latest Tensorflow version. Thank you!

github-actions[bot] on (2024-12-25 02:00:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-01 02:06:12 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-01 02:06:14 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83013"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/83013"">No</a>

"
2740100736,issue,open,,nvidia 'container-toolkit/latest/install-guide.html' link points to depricated page/product,"
on this page
```
https://www.tensorflow.org/install/docker
```

this link ('install NVIDIA Docker support')
```
https://github.com/NVIDIA/nvidia-docker
```

found in this text:
```
TensorFlow Docker requirements
1. Install Docker on your local host machine.
2. For GPU support on Linux, install NVIDIA Docker support.
```

points to this ""DEPRECATION NOTICE""
```
DEPRECATION NOTICE
This project has been superseded by the NVIDIA Container Toolkit.
```

which points to this link: 'NVIDIA Container Toolkit.'
```
https://github.com/NVIDIA/nvidia-container-toolkit
```

it would be helpful to include a link to this (burried) sub-sub link:
to the installation info:
```
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
```
",lineality,2024-12-14 19:01:32+00:00,['Venkat6871'],2025-02-05 06:07:14+00:00,,https://github.com/tensorflow/tensorflow/issues/82999,"[('type:docs-bug', 'Document issues'), ('awaiting PR merge', 'awaiting PR merge')]","[{'comment_id': 2635781131, 'issue_id': 2740100736, 'author': 'Venkat6871', 'body': 'Hi **@lineality** ,\nThank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. \nThank you!', 'created_at': datetime.datetime(2025, 2, 5, 6, 6, 56, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2025-02-05 06:06:56 UTC): Hi **@lineality** ,
Thank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. 
Thank you!

"
2739912314,issue,closed,completed,tensorflow not running on pycharm,"ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.",engmoha80,2024-12-14 14:25:44+00:00,['tilakrayal'],2024-12-18 17:56:03+00:00,2024-12-18 17:56:00+00:00,https://github.com/tensorflow/tensorflow/issues/82992,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues')]","[{'comment_id': 2547503787, 'issue_id': 2739912314, 'author': 'tilakrayal', 'body': '@engmoha80,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nAlso kindly provide the environment details and the steps followed to install the tensorflow.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 17, 4, 52, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551946155, 'issue_id': 2739912314, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551946210, 'issue_id': 2739912314, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82992"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82992"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, 2, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-17 04:52:34 UTC): @engmoha80,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

Also kindly provide the environment details and the steps followed to install the tensorflow.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

mihaimaruseac on (2024-12-18 17:56:00 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:56:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82992"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82992"">No</a>

"
2739719597,issue,closed,completed,undefined symbol: tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(tflite::StatefulNnApiDelegate::Options),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

14.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I was compiling my own cpp file with ndk-build, I reported the following error
```
ld: error: undefined symbol: tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(tflite::StatefulNnApiDelegate::Options)
>>> referenced by TFLiteEngine.cpp:77
>>>               /Users/jiang/Downloads/ifeng/java/NdkBuilderne/app/src/main/obj/local/armeabi-v7a/objs/pose2dto3d_armeabi-v7a/TFLiteEngine.o:(TFLiteEngine::loadModel(AAssetManager*))
clang++: error: linker command failed with exit code 1 (use -v to see invocation
```

**compileSdkVersion 33**
**buildToolsVersion '30.0.3'**
**ndk.dir=/Users/jiang/Library/Android/sdk/ndk/22.1.7171670**

bazel shell
```
bazel build -c opt --config=android_arm --repo_env=HERMETIC_PYTHON_VERSION=3.12  --define tflite_with_nnapi=true --verbose_failures tensorflow/lite:libtensorflowlite.so
```

Android.mk
```
LOCAL_PATH := $(call my-dir)

cur_arch=armeabi-v7a
LOCAL_CPPFLAGS := -g

# .so
include $(CLEAR_VARS)
LOCAL_MODULE := libtensorflowlite
LOCAL_SRC_FILES := third/android/$(cur_arch)/lib/libtensorflowlite.so
include $(PREBUILT_SHARED_LIBRARY)

# c
include $(CLEAR_VARS)
LOCAL_MODULE    := pose2dto3d_$(cur_arch)
MY_C_LIST := $(wildcard $(LOCAL_PATH)/TFLiteEngineJNI.cpp)
MY_C_LIST += $(wildcard $(LOCAL_PATH)/TFLiteEngine.cpp)
MY_C_LIST += $(wildcard $(LOCAL_PATH)/DataProcess.cpp)
LOCAL_SRC_FILES := $(MY_C_LIST:$(LOCAL_PATH)/%=%)

# 
# tflite
MY_ALL_DIRS := $(wildcard $(LOCAL_PATH)/)
MY_ALL_DIRS += $(wildcard $(LOCAL_PATH)/third/android/$(cur_arch)/include)
LOCAL_C_INCLUDES := $(MY_ALL_DIRS)

LOCAL_LDLIBS += -llog -lz -landroid -lOpenSLES -lGLESv1_CM -lGLESv2 -lneuralnetworks

LOCAL_STATIC_LIBRARIES := libtensorflowlite

LOCAL_CFLAGS += -UNDEBUG -D_DEBUG
include $(BUILD_SHARED_LIBRARY)
```

Application.mk
```
APP_OPTIM :=release
APP_ABI := armeabi-v7a
APP_PLATFORM := android-27
APP_STL := c++_static
```

.cpp
```
# include <stdio.h>
# include <stdlib.h>
#include <android/asset_manager_jni.h>
#include <android/asset_manager.h>
#include <android/log.h>
# include <jni.h>
#include <sys/uio.h>
#include <cstdlib>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <string>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""tensorflow/lite/interpreter_builder.h""
#include ""tensorflow/lite/model_builder.h""
#include ""tensorflow/lite/delegates/nnapi/nnapi_delegate.h""
#include ""TFLiteEngine.h""
#include ""Pose3dModel.h""


TFLiteEngine::TFLiteEngine() {
    is_tflite_initialized = false;
}

TFLiteEngine::~TFLiteEngine() {}

void TFLiteEngine::loadModel(AAssetManager *mgr) {
    __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"",
                        ""..................Entering loadModel...................."");

    if (!pose3dModel.is_tflite_initialized) {
        pose3dModel.asset = AAssetManager_open(mgr, MODEL_PATH, AASSET_MODE_BUFFER);
        if (pose3dModel.asset == nullptr) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference asset null----"");
            exit(1);
        }

        // 
        off_t file_length = AAsset_getLength(pose3dModel.asset);
        // 
        std::vector<char> model_data(file_length);
        AAsset_read(pose3dModel.asset, model_data.data(), file_length);

        // 1
        pose3dModel.model = tflite::FlatBufferModel::BuildFromBuffer(model_data.data(),
                                                                     model_data.size());
        if (!pose3dModel.model) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"",
                                ""pose3dInference ----"");
            exit(1);
        }

        __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference ----"");

        tflite::InterpreterBuilder builder(*(pose3dModel.model.get()), pose3dModel.resolver);
        builder(&(pose3dModel.interpreter));

        if (!pose3dModel.interpreter) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"",
                                ""pose3dInference interpreter null----"");
            exit(1);
        }

        if (NUM_THREADS != -1) {
            pose3dModel.interpreter->SetNumThreads(NUM_THREADS);
        }

        // NNAPi
        tflite::StatefulNnApiDelegate::Options options;
        options.execution_preference = tflite::StatefulNnApiDelegate::Options::kSustainedSpeed;
        options.use_burst_computation = true;
        options.allow_fp16 = true;

        if ((pose3dModel.delegate = new tflite::StatefulNnApiDelegate(options)) == NULL) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference NNAPI delegate error----"");
            exit(1);
        }

        if (pose3dModel.delegate != NULL) {
            TfLiteStatus status = pose3dModel.interpreter->ModifyGraphWithDelegate(pose3dModel.delegate);
            if (status != TfLiteStatus::kTfLiteOk)
                __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference Failed to modify graph with delegate!----"");
        }

        if (pose3dModel.interpreter->AllocateTensors() != kTfLiteOk) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"",
                                ""pose3dInference AllocateTensors ERROR----"");
            exit(1);
        }

        pose3dModel.input = pose3dModel.interpreter->typed_input_tensor<float>(0);
        if (pose3dModel.input == nullptr) {
            __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference input null----"");
            exit(1);
        }
        __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"", ""pose3dInference input----"");

        is_tflite_initialized = true;
    }
    __android_log_print(ANDROID_LOG_DEBUG, ""POSE_3D_CPP"",
                        ""..................Exiting loadModel...................."");

    if (pose3dModel.asset != nullptr) {
        AAsset_close(pose3dModel.asset);
    }
}
```

### Standalone code to reproduce the issue

```shell
just ndk build my cpp

ndk-build V=1
```
```


### Relevant log output

_No response_",angelOnly,2024-12-14 09:11:22+00:00,['gaikwadrahul8'],2025-01-10 08:13:05+00:00,2024-12-20 11:18:16+00:00,https://github.com/tensorflow/tensorflow/issues/82973,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2544687090, 'issue_id': 2739719597, 'author': 'angelOnly', 'body': 'I tried the following script to set more NNAPI-related parameters, but it still didn\'t work\r\n\r\nReference link: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/delegates#nnapi-delegate-provider](url)\r\n\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --repo_env=HERMETIC_PYTHON_VERSION=3.12 \\\r\n  --define tflite_with_nnapi=true \\\r\n  --define use_nnapi=true \\\r\n  --define nnapi_accelerator_name="""" \\\r\n  --define nnapi_execution_preference=""low_power"" \\\r\n  --define nnapi_execution_priority=""high"" \\\r\n  --define nnapi_allow_fp16=false \\\r\n  --define nnapi_allow_dynamic_dimensions=false \\\r\n  --define nnapi_use_burst_mode=true \\\r\n  --verbose_failures \\\r\n  tensorflow/lite:libtensorflowlite.so\r\n```', 'created_at': datetime.datetime(2024, 12, 16, 6, 9, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548913243, 'issue_id': 2739719597, 'author': 'yg-dickson', 'body': 'Is there any tutorial for building LiteRT for Android?', 'created_at': datetime.datetime(2024, 12, 17, 16, 14, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550386232, 'issue_id': 2739719597, 'author': 'gaikwadrahul8', 'body': ""Hi, @angelOnly \r\nThank you for trying script to set more NNAPI-related parameters but it didn't work so if you don't mind could you please downgrade the TensorFlow version to either 2.17.* or 2.16.* and see is it working fine or not ?\r\n\r\nWe have updated official documentation w.r.t LiteRT Delegates please refer [here](https://ai.google.dev/edge/litert/performance/delegates) and for GPU delegates for LiteRT and GPU ML operations support please refer this [official documentation](https://ai.google.dev/edge/litert/performance/gpu)\r\n\r\nTo migrate from NNAPI, see the instructions for [TensorFlow Lite in Google Play Services](https://www.tensorflow.org/lite/android/play_services) and optionally [TFLite GPU delegate](https://www.tensorflow.org/lite/android/delegates/gpu) for hardware acceleration please refer this [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide)\r\n\r\nHi, @yg-dickson, Please refer this LiteRT [examples](https://github.com/tensorflow/examples/tree/master/lite/examples) of android and official documentation of [Build LiteRT for Android](https://ai.google.dev/edge/litert/android/lite_build)\r\n\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 18, 5, 37, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556225256, 'issue_id': 2739719597, 'author': 'angelOnly', 'body': ""Hello, thank you very much for your reply! \r\nI have read the example you gave. This is the compilation process of java inference, while I use c++ inference, which needs to compile the.so package instead of.aar. And I hope to support nnapi delegate. I don't know how to fix it.\r\n\r\n> Hi, @angelOnly Thank you for trying script to set more NNAPI-related parameters but it didn't work so if you don't mind could you please downgrade the TensorFlow version to either 2.17.* or 2.16.* and see is it working fine or not ?\r\n> \r\n> Hi, @yg-dickson, Please refer this LiteRT [examples](https://github.com/tensorflow/examples/tree/master/lite/examples) of android and official documentation of [Build LiteRT for Android](https://ai.google.dev/edge/litert/android/lite_build)\r\n> \r\n> Thank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 20, 3, 41, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556225588, 'issue_id': 2739719597, 'author': 'angelOnly', 'body': ""> Is there any tutorial for building LiteRT for Android?\r\n\r\nI can't find any relevant courses. Do you have any recommendations"", 'created_at': datetime.datetime(2024, 12, 20, 3, 42, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556802012, 'issue_id': 2739719597, 'author': 'angelOnly', 'body': 'I have determined where the problem is, because the model does not support some operators of nnapi, resulting in GPU acceleration, but I still failed to compile and open nnapi, I used the gpu delegate to verify this problem.\r\nDownload the official posenet model verification, can be gpu accelerated, and our model does not support gpu acceleration\r\n\r\nnnapi support opthttps://www.tensorflow.org/lite/android/delegates/nnapi?hl=zh-cn#use_supported_models_and_ops\r\ngpu delegate Compiled.so packagehttps://github.com/ValYouW/tflite-dist/releases\r\n![image](https://github.com/user-attachments/assets/7e7068b1-9040-4de8-8c96-9a8978d6c278)\r\n<img width=""1298"" alt=""image"" src=""https://github.com/user-attachments/assets/758adafa-6ba0-45bd-80e5-bd6e53f5f1de"" />\r\n![image](https://github.com/user-attachments/assets/4069972b-2b77-441b-8db5-37035e890fb5)', 'created_at': datetime.datetime(2024, 12, 20, 11, 18, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556802258, 'issue_id': 2739719597, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82973"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82973"">No</a>', 'created_at': datetime.datetime(2024, 12, 20, 11, 18, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559226989, 'issue_id': 2739719597, 'author': 'yg-dickson', 'body': ""> Hi, @angelOnly Thank you for trying script to set more NNAPI-related parameters but it didn't work so if you don't mind could you please downgrade the TensorFlow version to either 2.17.* or 2.16.* and see is it working fine or not ?\r\n> \r\n> Hi, @yg-dickson, Please refer this LiteRT [examples](https://github.com/tensorflow/examples/tree/master/lite/examples) of android and official documentation of [Build LiteRT for Android](https://ai.google.dev/edge/litert/android/lite_build)\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nFollowing this tutorial, I cannot build\r\ncom.google.ai.edge.litert:litert\r\ncom.google.ai.edge.litert:litert-gpu"", 'created_at': datetime.datetime(2024, 12, 23, 8, 49, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580480742, 'issue_id': 2739719597, 'author': 'gaikwadrahul8', 'body': 'Hi, @yg-dickson \r\nApologize for the delayed response, could you please create new issue here :https://github.com/google-ai-edge/LiteRT/issues/new and mention what all steps you followed in that issue ? we will be happy to help you further. \r\n\r\nThank you for your cooperation and understanding.', 'created_at': datetime.datetime(2025, 1, 9, 14, 54, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582019618, 'issue_id': 2739719597, 'author': 'yg-dickson', 'body': 'LiteRT No one answered', 'created_at': datetime.datetime(2025, 1, 10, 8, 13, 4, tzinfo=datetime.timezone.utc)}]","angelOnly (Issue Creator) on (2024-12-16 06:09:09 UTC): I tried the following script to set more NNAPI-related parameters, but it still didn't work

Reference link: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/delegates#nnapi-delegate-provider](url)

```
bazel build -c opt \
  --config=android_arm \
  --repo_env=HERMETIC_PYTHON_VERSION=3.12 \
  --define tflite_with_nnapi=true \
  --define use_nnapi=true \
  --define nnapi_accelerator_name="""" \
  --define nnapi_execution_preference=""low_power"" \
  --define nnapi_execution_priority=""high"" \
  --define nnapi_allow_fp16=false \
  --define nnapi_allow_dynamic_dimensions=false \
  --define nnapi_use_burst_mode=true \
  --verbose_failures \
  tensorflow/lite:libtensorflowlite.so
```

yg-dickson on (2024-12-17 16:14:25 UTC): Is there any tutorial for building LiteRT for Android?

gaikwadrahul8 (Assginee) on (2024-12-18 05:37:26 UTC): Hi, @angelOnly 
Thank you for trying script to set more NNAPI-related parameters but it didn't work so if you don't mind could you please downgrade the TensorFlow version to either 2.17.* or 2.16.* and see is it working fine or not ?

We have updated official documentation w.r.t LiteRT Delegates please refer [here](https://ai.google.dev/edge/litert/performance/delegates) and for GPU delegates for LiteRT and GPU ML operations support please refer this [official documentation](https://ai.google.dev/edge/litert/performance/gpu)

To migrate from NNAPI, see the instructions for [TensorFlow Lite in Google Play Services](https://www.tensorflow.org/lite/android/play_services) and optionally [TFLite GPU delegate](https://www.tensorflow.org/lite/android/delegates/gpu) for hardware acceleration please refer this [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide)

Hi, @yg-dickson, Please refer this LiteRT [examples](https://github.com/tensorflow/examples/tree/master/lite/examples) of android and official documentation of [Build LiteRT for Android](https://ai.google.dev/edge/litert/android/lite_build)


Thank you for your cooperation and patience.

angelOnly (Issue Creator) on (2024-12-20 03:41:38 UTC): Hello, thank you very much for your reply! 
I have read the example you gave. This is the compilation process of java inference, while I use c++ inference, which needs to compile the.so package instead of.aar. And I hope to support nnapi delegate. I don't know how to fix it.

angelOnly (Issue Creator) on (2024-12-20 03:42:06 UTC): I can't find any relevant courses. Do you have any recommendations

angelOnly (Issue Creator) on (2024-12-20 11:18:08 UTC): I have determined where the problem is, because the model does not support some operators of nnapi, resulting in GPU acceleration, but I still failed to compile and open nnapi, I used the gpu delegate to verify this problem.
Download the official posenet model verification, can be gpu accelerated, and our model does not support gpu acceleration

nnapi support opthttps://www.tensorflow.org/lite/android/delegates/nnapi?hl=zh-cn#use_supported_models_and_ops
gpu delegate Compiled.so packagehttps://github.com/ValYouW/tflite-dist/releases
![image](https://github.com/user-attachments/assets/7e7068b1-9040-4de8-8c96-9a8978d6c278)
<img width=""1298"" alt=""image"" src=""https://github.com/user-attachments/assets/758adafa-6ba0-45bd-80e5-bd6e53f5f1de"" />
![image](https://github.com/user-attachments/assets/4069972b-2b77-441b-8db5-37035e890fb5)

google-ml-butler[bot] on (2024-12-20 11:18:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82973"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82973"">No</a>

yg-dickson on (2024-12-23 08:49:16 UTC): Following this tutorial, I cannot build
com.google.ai.edge.litert:litert
com.google.ai.edge.litert:litert-gpu

gaikwadrahul8 (Assginee) on (2025-01-09 14:54:13 UTC): Hi, @yg-dickson 
Apologize for the delayed response, could you please create new issue here :https://github.com/google-ai-edge/LiteRT/issues/new and mention what all steps you followed in that issue ? we will be happy to help you further. 

Thank you for your cooperation and understanding.

yg-dickson on (2025-01-10 08:13:04 UTC): LiteRT No one answered

"
2739610790,issue,closed,completed,Add TensorFlowLite 2.18.0 to Maven Central/CocoaPods Specs.,"Can we please publish the latest (last?) version of TensorFlowLite for Android and iOS before we all move on to LiteRT?
Thanks!",v-hogood,2024-12-14 06:19:25+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-17 16:38:20+00:00,2025-01-17 16:38:18+00:00,https://github.com/tensorflow/tensorflow/issues/82966,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2563373105, 'issue_id': 2739610790, 'author': 'gaikwadrahul8', 'body': 'Hi, @v-hogood \r\nI apologize for the delayed response, I checked and we published last `tensorflow-lite version is 2.16.1` but I need to check with relevant internal team the possibility of publishing the latest **TensorFlow Lite version (2.18.0)** to Maven Central (Android) and CocoaPods (iOS) before transitioning entirely to LiteRT.\r\n\r\nMeanwhile please check this official documentation of [Migrate to LiteRT from TensorFlow Lite](https://ai.google.dev/edge/litert/migration) \r\n\r\nTo migrate an Android application using Tensorflow Lite, replace the dependency from org.tensorflow:tensorflow-lite to com.google.ai.edge.litert. The [LiteRT Maven repository](https://maven.google.com/web/index.html#com.google.ai.edge.litert) includes the following packages:\r\n\r\n[com.google.ai.edge.litert:litert](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert)\r\n[com.google.ai.edge.litert:litert-gpu](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-gpu)\r\n[com.google.ai.edge.litert:litert-metadata](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-metadata)\r\n[com.google.ai.edge.litert:litert-support](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-support)\r\n\r\nYou can make this change in your build.gradle dependencies:\r\n```\r\ndependencies {\r\n  ...\r\n  implementation `com.google.ai.edge.litert:litert:1.0.1`\r\n}\r\n\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 27, 6, 32, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574916222, 'issue_id': 2739610790, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take a look into this issue. Thank you', 'created_at': datetime.datetime(2025, 1, 7, 10, 20, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576060019, 'issue_id': 2739610790, 'author': 'pkgoogle', 'body': 'Hi @v-hogood, is there a reason you cannot transition to litert? Or is there any benefit that releasing TF 2.18.0 to Maven does for you (instead of just transitioning)? Thanks.', 'created_at': datetime.datetime(2025, 1, 7, 19, 23, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476308, 'issue_id': 2739610790, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 15, 1, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594417639, 'issue_id': 2739610790, 'author': 'v-hogood', 'body': ""I will try LiteRT when I get the chance.\nBut is there a reason not to publish every TFLite Release?\nThere hasn't been one in a long time, from before 16kb support:\nhttps://github.com/tensorflow/tensorflow/issues/72157"", 'created_at': datetime.datetime(2025, 1, 16, 3, 46, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596482323, 'issue_id': 2739610790, 'author': 'pkgoogle', 'body': ""Hi @v-hogood, There's quite a bit of work for every release and we want to target/focus on future state unless there's a very strong reason to allocate resources on a previous version."", 'created_at': datetime.datetime(2025, 1, 16, 18, 48, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598751446, 'issue_id': 2739610790, 'author': 'v-hogood', 'body': 'OK I switched to LiteRT 1.0.1', 'created_at': datetime.datetime(2025, 1, 17, 16, 38, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598751494, 'issue_id': 2739610790, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82966"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82966"">No</a>', 'created_at': datetime.datetime(2025, 1, 17, 16, 38, 19, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-27 06:32:15 UTC): Hi, @v-hogood 
I apologize for the delayed response, I checked and we published last `tensorflow-lite version is 2.16.1` but I need to check with relevant internal team the possibility of publishing the latest **TensorFlow Lite version (2.18.0)** to Maven Central (Android) and CocoaPods (iOS) before transitioning entirely to LiteRT.

Meanwhile please check this official documentation of [Migrate to LiteRT from TensorFlow Lite](https://ai.google.dev/edge/litert/migration) 

To migrate an Android application using Tensorflow Lite, replace the dependency from org.tensorflow:tensorflow-lite to com.google.ai.edge.litert. The [LiteRT Maven repository](https://maven.google.com/web/index.html#com.google.ai.edge.litert) includes the following packages:

[com.google.ai.edge.litert:litert](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert)
[com.google.ai.edge.litert:litert-gpu](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-gpu)
[com.google.ai.edge.litert:litert-metadata](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-metadata)
[com.google.ai.edge.litert:litert-support](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert-support)

You can make this change in your build.gradle dependencies:
```
dependencies {
  ...
  implementation `com.google.ai.edge.litert:litert:1.0.1`
}

```

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-01-07 10:20:50 UTC): Hi, @pkgoogle 
Please take a look into this issue. Thank you

pkgoogle (Assginee) on (2025-01-07 19:23:44 UTC): Hi @v-hogood, is there a reason you cannot transition to litert? Or is there any benefit that releasing TF 2.18.0 to Maven does for you (instead of just transitioning)? Thanks.

github-actions[bot] on (2025-01-15 01:59:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

v-hogood (Issue Creator) on (2025-01-16 03:46:07 UTC): I will try LiteRT when I get the chance.
But is there a reason not to publish every TFLite Release?
There hasn't been one in a long time, from before 16kb support:
https://github.com/tensorflow/tensorflow/issues/72157

pkgoogle (Assginee) on (2025-01-16 18:48:34 UTC): Hi @v-hogood, There's quite a bit of work for every release and we want to target/focus on future state unless there's a very strong reason to allocate resources on a previous version.

v-hogood (Issue Creator) on (2025-01-17 16:38:18 UTC): OK I switched to LiteRT 1.0.1

google-ml-butler[bot] on (2025-01-17 16:38:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82966"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82966"">No</a>

"
2738207022,issue,closed,not_planned,Lite Rt Build Cross Compile Toolchain version incompatible,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

8.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to build with cross compile for armv6 raspberry pi 0. In the instruction of the cross compile lite rt, given toolchain link is not compatible with armv6, its not even running simple  cross compiled hello_world.cpp, When we check the attributes of file, the arch of toolchain is armv7, but raspberry pi zero's arch is armv6. 

### Standalone code to reproduce the issue

```shell
#In the host computer:
 ~/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-g++ hello.cpp -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations  -o hello 

#Raspberry Pi Zero:
./hello

readelf -A hello
```


### Relevant log output

```shell
Raspberry Pi Zero output:
Illegal instruction

File attributes
  Tag_CPU_name: ""7-A""
  Tag_CPU_arch: v7
  Tag_CPU_arch_profile: Application
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-2
  Tag_FP_arch: VFPv3
  Tag_Advanced_SIMD_arch: NEONv1
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_rounding: Needed
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_align_preserved: 8-byte, except leaf SP
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_CPU_unaligned_access: v6
```
",YigitKaanBingol,2024-12-13 11:45:50+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-08 18:52:34+00:00,2025-01-08 18:52:31+00:00,https://github.com/tensorflow/tensorflow/issues/82920,"[('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2546882305, 'issue_id': 2738207022, 'author': 'gaikwadrahul8', 'body': ""Hi, @YigitKaanBingol \r\nThank you for bringing this issue to our attention. It appears that you are encountering compatibility problems with the cross-compilation toolchain for ARMv6 on the Raspberry Pi Zero.\r\n\r\nFrom your description it seems that the toolchain you are using `gcc-arm-8.3-2019.03` is targeting ARMv7 architecture, which is indeed incompatible with the ARMv6 architecture of the Raspberry Pi Zero. This mismatch can lead to the `Illegal instruction` error you are experiencing when trying to run your compiled binary. I believe you're referring this [official documentation](https://ai.google.dev/edge/litert/build/cmake_arm) so I'll have a look once from my end and will update you.\r\n\r\nThank you for understanding and patience."", 'created_at': datetime.datetime(2024, 12, 16, 21, 46, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546905896, 'issue_id': 2738207022, 'author': 'YigitKaanBingol', 'body': ""Hi @gaikwadrahul8, thank you for your time and response. \n\nI followed every step from the official documentation as it explains. Unfortunately, I couldn't succeed."", 'created_at': datetime.datetime(2024, 12, 16, 21, 55, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574906267, 'issue_id': 2738207022, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2025, 1, 7, 10, 16, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576049335, 'issue_id': 2738207022, 'author': 'pkgoogle', 'body': 'Hi @YigitKaanBingol, I was able to build successfully on a Debian linux system with the ARMv6 target:\r\n\r\n```sh\r\n$ cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++   -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   -DCMAKE_SYSTEM_NAME=Linux   -DCMAKE_SYSTEM_PROCESSOR=armv6   -DTFLITE_ENABLE_XNNPACK=OFF -DTFLITE_HOST_TOOLS_DIR=/usr/local/bin/  ../tensorflow/lite/\r\n-- Setting build type to Release, for debug builds use\'-DCMAKE_BUILD_TYPE=Debug\'.\r\n-- Pre-built \'flatc\' compiler for cross-compilation purposes found: /usr/local/bin/flatc\r\nCMake Warning at /xxxxxxxxxxxx/git/tensorflow/armv6_build/abseil-cpp/CMakeLists.txt:77 (message):\r\n  A future Abseil release will default ABSL_PROPAGATE_CXX_STD to ON for CMake\r\n  3.8 and up.  We recommend enabling this option to ensure your project still\r\n  builds correctly.\r\n\r\n\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17 - Success\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- \r\n-- Configured Eigen 3.4.90\r\n-- \r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Success\r\nCMake Warning (dev) at /usr/share/cmake-3.29/Modules/FetchContent.cmake:1352 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy\'s OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  tools/cmake/modules/OverridableFetchContent.cmake:398 (FetchContent_Declare)\r\n  tools/cmake/modules/fft2d.cmake:22 (OverridableFetchContent_Declare)\r\n  tools/cmake/modules/Findfft2d.cmake:18 (include)\r\n  CMakeLists.txt:169 (find_package)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Proceeding with version: 24.3.25.4\r\n-- Looking for strtof_l\r\n-- Looking for strtof_l - found\r\n-- Looking for strtoull_l\r\n-- Looking for strtoull_l - found\r\n-- Looking for realpath\r\n-- Looking for realpath - found\r\n-- CMAKE_CXX_FLAGS: -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- \r\n-- 3.21.9.0\r\n-- Performing Test protobuf_HAVE_LD_VERSION_SCRIPT\r\n-- Performing Test protobuf_HAVE_LD_VERSION_SCRIPT - Success\r\n-- Performing Test protobuf_HAVE_BUILTIN_ATOMICS\r\n-- Performing Test protobuf_HAVE_BUILTIN_ATOMICS - Failed\r\n-- Configuring done (55.4s)\r\n-- Generating done (0.6s)\r\n-- Build files have been written to: /xxxxxxx/git/tensorflow/armv6_build\r\n```\r\n\r\nShould work similarly on Ubuntu 22.04, can you tell me what exact commands you do and at which command you fail at?\r\n\r\nThanks.', 'created_at': datetime.datetime(2025, 1, 7, 19, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576099980, 'issue_id': 2738207022, 'author': 'YigitKaanBingol', 'body': 'Thank you for your reply.\nThe issue is not building, it does build. The problem is, the provided toolchain in official documentation in Lite Rt Cross Compile for Armv6 cannot run the basic hello_world example. All the commands and relative outputs given above.\nThanks, Have a nice one.', 'created_at': datetime.datetime(2025, 1, 7, 19, 47, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578388824, 'issue_id': 2738207022, 'author': 'pkgoogle', 'body': 'Hi @YigitKaanBingol, we are transferring this issue to LiteRT. Please use that repo for LiteRT issues moving forward. Thanks.', 'created_at': datetime.datetime(2025, 1, 8, 18, 52, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578388903, 'issue_id': 2738207022, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82920"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82920"">No</a>', 'created_at': datetime.datetime(2025, 1, 8, 18, 52, 33, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-16 21:46:20 UTC): Hi, @YigitKaanBingol 
Thank you for bringing this issue to our attention. It appears that you are encountering compatibility problems with the cross-compilation toolchain for ARMv6 on the Raspberry Pi Zero.

From your description it seems that the toolchain you are using `gcc-arm-8.3-2019.03` is targeting ARMv7 architecture, which is indeed incompatible with the ARMv6 architecture of the Raspberry Pi Zero. This mismatch can lead to the `Illegal instruction` error you are experiencing when trying to run your compiled binary. I believe you're referring this [official documentation](https://ai.google.dev/edge/litert/build/cmake_arm) so I'll have a look once from my end and will update you.

Thank you for understanding and patience.

YigitKaanBingol (Issue Creator) on (2024-12-16 21:55:32 UTC): Hi @gaikwadrahul8, thank you for your time and response. 

I followed every step from the official documentation as it explains. Unfortunately, I couldn't succeed.

gaikwadrahul8 (Assginee) on (2025-01-07 10:16:38 UTC): Hi, @pkgoogle
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2025-01-07 19:18:44 UTC): Hi @YigitKaanBingol, I was able to build successfully on a Debian linux system with the ARMv6 target:

```sh
$ cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++   -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   -DCMAKE_SYSTEM_NAME=Linux   -DCMAKE_SYSTEM_PROCESSOR=armv6   -DTFLITE_ENABLE_XNNPACK=OFF -DTFLITE_HOST_TOOLS_DIR=/usr/local/bin/  ../tensorflow/lite/
-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.
-- Pre-built 'flatc' compiler for cross-compilation purposes found: /usr/local/bin/flatc
CMake Warning at /xxxxxxxxxxxx/git/tensorflow/armv6_build/abseil-cpp/CMakeLists.txt:77 (message):
  A future Abseil release will default ABSL_PROPAGATE_CXX_STD to ON for CMake
  3.8 and up.  We recommend enabling this option to ensure your project still
  builds correctly.


-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17
-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17 - Success
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- 
-- Configured Eigen 3.4.90
-- 
-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT
-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Success
CMake Warning (dev) at /usr/share/cmake-3.29/Modules/FetchContent.cmake:1352 (message):
  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is
  not set.  The policy's OLD behavior will be used.  When using a URL
  download, the timestamps of extracted files should preferably be that of
  the time of extraction, otherwise code that depends on the extracted
  contents might not be rebuilt if the URL changes.  The OLD behavior
  preserves the timestamps from the archive instead, but this is usually not
  what you want.  Update your project to the NEW behavior or specify the
  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this
  robustness issue.
Call Stack (most recent call first):
  tools/cmake/modules/OverridableFetchContent.cmake:398 (FetchContent_Declare)
  tools/cmake/modules/fft2d.cmake:22 (OverridableFetchContent_Declare)
  tools/cmake/modules/Findfft2d.cmake:18 (include)
  CMakeLists.txt:169 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Proceeding with version: 24.3.25.4
-- Looking for strtof_l
-- Looking for strtof_l - found
-- Looking for strtoull_l
-- Looking for strtoull_l - found
-- Looking for realpath
-- Looking for realpath - found
-- CMAKE_CXX_FLAGS: -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations
-- Check if compiler accepts -pthread
-- Check if compiler accepts -pthread - yes
-- 
-- 3.21.9.0
-- Performing Test protobuf_HAVE_LD_VERSION_SCRIPT
-- Performing Test protobuf_HAVE_LD_VERSION_SCRIPT - Success
-- Performing Test protobuf_HAVE_BUILTIN_ATOMICS
-- Performing Test protobuf_HAVE_BUILTIN_ATOMICS - Failed
-- Configuring done (55.4s)
-- Generating done (0.6s)
-- Build files have been written to: /xxxxxxx/git/tensorflow/armv6_build
```

Should work similarly on Ubuntu 22.04, can you tell me what exact commands you do and at which command you fail at?

Thanks.

YigitKaanBingol (Issue Creator) on (2025-01-07 19:47:10 UTC): Thank you for your reply.
The issue is not building, it does build. The problem is, the provided toolchain in official documentation in Lite Rt Cross Compile for Armv6 cannot run the basic hello_world example. All the commands and relative outputs given above.
Thanks, Have a nice one.

pkgoogle (Assginee) on (2025-01-08 18:52:31 UTC): Hi @YigitKaanBingol, we are transferring this issue to LiteRT. Please use that repo for LiteRT issues moving forward. Thanks.

google-ml-butler[bot] on (2025-01-08 18:52:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82920"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82920"">No</a>

"
2735927425,issue,closed,completed,Inconsistency with 'height_shift_range' and 'width_shift_range' parameters in ImageDataGenerator,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.11.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The parameters are exchanged, 'height_shift_range' moves left and right and 'width_shift_range' moves up and bottom.

<br>

```python
from numpy import expand_dims
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot
# load the image
img = load_img('bird.jpg')
# convert to numpy array
data = img_to_array(img)
# expand dimension to one sample
samples = expand_dims(data, 0)
# create image data augmentation generator
datagen = ImageDataGenerator(width_shift_range=[-100,100])# ------- WIDTH_SHIFT_RANGE
# prepare iterator
it = datagen.flow(samples, batch_size=1)
# generate samples and plot
for i in range(9):
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # generate batch of images
    batch = next(it)
    # convert to unsigned integers for viewing
    image = batch[0].astype('uint8')
    # plot raw pixel data
    pyplot.imshow(image)
# show the figure
pyplot.show()
```
![image](https://github.com/user-attachments/assets/0ccd8332-5c37-418e-9182-0d534fdbf1ba)

<br>
<br>

```python
from numpy import expand_dims
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot
# load the image
img = load_img('bird.jpg')
# convert to numpy array
data = img_to_array(img)
# expand dimension to one sample
samples = expand_dims(data, 0)
# create image data augmentation generator
datagen = ImageDataGenerator(height_shift_range=[-100,100]) # ------- HEIGTH_SHIFT_RANGE
# prepare iterator
it = datagen.flow(samples, batch_size=1)
# generate samples and plot
for i in range(9):
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # generate batch of images
    batch = next(it)
    # convert to unsigned integers for viewing
    image = batch[0].astype('uint8')
    # plot raw pixel data
    pyplot.imshow(image)
# show the figure
pyplot.show()
```
![image](https://github.com/user-attachments/assets/e03dc6af-86b6-4b31-ac29-1f9fe4442a89)


### Standalone code to reproduce the issue

```shell
Use the parametes like: ImageDataGenerator(height_shift_range=[-100,100]) and ImageDataGenerator(width_shift_range=[-100,100])
```


### Relevant log output

_No response_",RicardoRobledo,2024-12-12 13:32:27+00:00,['tilakrayal'],2024-12-29 02:06:34+00:00,2024-12-29 02:06:30+00:00,https://github.com/tensorflow/tensorflow/issues/82838,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2540847746, 'issue_id': 2735927425, 'author': 'vaibhavi089', 'body': 'can you elaborate this issue more?', 'created_at': datetime.datetime(2024, 12, 13, 8, 20, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541596767, 'issue_id': 2735927425, 'author': 'tilakrayal', 'body': '@RicardoRobledo,\r\nIn the Keras3.0(default for the tensorflow2.17/2.18) the [ImageDataGenerator ](https://github.com/keras-team/keras/blob/master/keras/legacy/preprocessing/image.py)has been deprecated.\r\n**image_dataset_from_directory** is the replacement of the **ImageDataGenerator**, for augmentation you need to use preprocessing layer separately.\r\nFor more details on API usage, visit https://keras.io/api/data_loading/image/#image_dataset_from_directory-function.\r\n\r\nIf you want to use the keras2.0, try to use the below code for the import and then try to test the code.\r\n\r\n```python\r\n!pip install tf-keras\r\nimport tf_keras as keras\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 13, 14, 35, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557949949, 'issue_id': 2735927425, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 21, 2, 0, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564585250, 'issue_id': 2735927425, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 29, 2, 6, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564585273, 'issue_id': 2735927425, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82838"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82838"">No</a>', 'created_at': datetime.datetime(2024, 12, 29, 2, 6, 32, tzinfo=datetime.timezone.utc)}]","vaibhavi089 on (2024-12-13 08:20:24 UTC): can you elaborate this issue more?

tilakrayal (Assginee) on (2024-12-13 14:35:52 UTC): @RicardoRobledo,
In the Keras3.0(default for the tensorflow2.17/2.18) the [ImageDataGenerator ](https://github.com/keras-team/keras/blob/master/keras/legacy/preprocessing/image.py)has been deprecated.
**image_dataset_from_directory** is the replacement of the **ImageDataGenerator**, for augmentation you need to use preprocessing layer separately.
For more details on API usage, visit https://keras.io/api/data_loading/image/#image_dataset_from_directory-function.

If you want to use the keras2.0, try to use the below code for the import and then try to test the code.

```python
!pip install tf-keras
import tf_keras as keras
```

Thank you!

github-actions[bot] on (2024-12-21 02:00:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-29 02:06:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-29 02:06:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82838"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82838"">No</a>

"
2735873519,issue,open,,error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https://www.tensorflow.org/api_docs/python/tf/raw_ops/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)
grad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)
argmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)
ksize = [1, 2, 2, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""

output_grad = tf.raw_ops.MaxPoolGradWithArgmax(
    input=input_tensor,
    grad=grad_tensor,
    argmax=argmax_indices,
    ksize=ksize,
    strides=strides,
    padding=padding,
    include_batch_in_index=False
)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 1, 1, 1]]
All kernels registered for op MaxPoolGradWithArgmax:
  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]
 [Op:MaxPoolGradWithArgmax] name:
```
",LongZE666,2024-12-12 13:14:53+00:00,['Venkat6871'],2024-12-16 05:23:22+00:00,,https://github.com/tensorflow/tensorflow/issues/82837,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2544624646, 'issue_id': 2735873519, 'author': 'Venkat6871', 'body': 'I tried to run your code on colab using TF v2.17.0, 2.18.0 & nightly and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ce57ea821ced4e2ab2274c4236f08a43/82837_tf-2-18-0-nightly.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 16, 5, 23, 16, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-16 05:23:16 UTC): I tried to run your code on colab using TF v2.17.0, 2.18.0 & nightly and faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ce57ea821ced4e2ab2274c4236f08a43/82837_tf-2-18-0-nightly.ipynb) here for reference.
Thank you!

"
2735583902,issue,closed,completed,"Annoying ""Ignoring Assert operator"" warning in console output","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0 and 2.18.0

### Custom code

Yes

### OS platform and distribution

WSL2 Ubuntu 22.04.3 LTS and Native Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.9.20 and 3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3/8 and 12.5.1/9

### GPU model and memory

_No response_

### Current behavior?

Ever since I switched to Keras 3.x with Tensorflow 2.17+ as backend I am observing the following annoying warning which is printed several times during the `model.fit()` operation: 
`W0000 00:00:1734000173.082846   18720 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert`.

This warning is even printed when I set the environment variable `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""2""` to disable info and warning logs.

The warning is annoying because it interrupts the progress bar which shows the progress during a training epoch and therefore makes the output less readable.
 

### Standalone code to reproduce the issue

```shell
The code is confidential, but basically I am just calling `model.fit()` on a model compiled with the following loss head: `SparseCategoricalCrossentropy(from_logits=False, ignore_class=255)`
```


### Relevant log output

```shell
In the following small log except you can see that the warning already occurs 3 times within the first two training epochs:

W0000 00:00:1733991199.537613    1787 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-12-12 09:13:19.823355: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
I0000 00:00:1733991207.027751    1787 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
18643/18644  0s 32ms/step - loss: 0.4644W0000 00:00:1733991796.147217    1789 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert

Epoch 1: val_loss improved from inf to 0.53242, saving model to /mnt/e/Jonas/tmp_debugging/exp_29/train/20241212-091312_unet2_blue_stain_decayed_knot_weak_encased_knot_sound_knot_pith_resin_pocket_rot_split_wane_moisture_wet_core_infestation_buchs_grain_background_dirt_bark_knothole.weights.h5
18644/18644  715s 38ms/step - loss: 0.4643 - val_loss: 0.5324 - learning_rate: 1.0000e-04
Epoch 2/10
18643/18644  0s 27ms/step - loss: 0.3617W0000 00:00:1733992416.694900    1787 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert

Epoch 2: val_loss improved from 0.53242 to 0.42661, saving model to /mnt/e/Jonas/tmp_debugging/exp_29/train/20241212-091312_unet2_blue_stain_decayed_knot_weak_encased_knot_sound_knot_pith_resin_pocket_rot_split_wane_moisture_wet_core_infestation_buchs_grain_background_dirt_bark_knothole.weights.h5
18644/18644  512s 27ms/step - loss: 0.3617 - val_loss: 0.4266 - learning_rate: 1.0000e-04
```
",RabJon,2024-12-12 11:06:40+00:00,['tilakrayal'],2025-01-10 08:51:56+00:00,2025-01-04 02:00:07+00:00,https://github.com/tensorflow/tensorflow/issues/82832,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2541553204, 'issue_id': 2735583902, 'author': 'tilakrayal', 'body': '@RabJon,\r\nCould you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Also could you please try using tf-nighty/keras-nightly and test the code. \r\n\r\n**pip install tf-nightly or pip install keras-nightly**\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 13, 14, 13, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545322090, 'issue_id': 2735583902, 'author': 'RabJon', 'body': '> @RabJon, Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. \r\n\r\nI created a simple [gist](https://gist.github.com/RabJon/89fbbae4fede25ef87a928436ad4c458) using a mock model with which I was able to reproduce the problem.\r\n\r\n> Also could you please try using tf-nighty/keras-nightly and test the code.\r\n> **pip install tf-nightly or pip install keras-nightly**\r\n\r\nFirst I tried to install only keras-nightly but this was not enough, I immediately got the ""module not found"" error for Tensorflow. Then I also installed tf-nightly and then all the imports worked. \r\nFurthermore, also the problem mentioned in this issue did not appear anymore. \r\nNevertheless, it would be nice to know where this warning comes from, why it can\'t be silenced and if it indicates some kind of mistake from my side?\r\n\r\nThanks and kind regards, \r\nJonas', 'created_at': datetime.datetime(2024, 12, 16, 11, 9, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556376820, 'issue_id': 2735583902, 'author': 'tilakrayal', 'body': '@RabJon,\r\nGlad to know that the warnings disappear with tf-nightly. Could you please try using **os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""3""** which might help to disable the warnings.\r\n\r\n```python\r\nimport os\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""3""\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 20, 6, 31, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137711, 'issue_id': 2735583902, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2570001766, 'issue_id': 2735583902, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 4, 2, 0, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2570001800, 'issue_id': 2735583902, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82832"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82832"">No</a>', 'created_at': datetime.datetime(2025, 1, 4, 2, 0, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582087265, 'issue_id': 2735583902, 'author': 'RabJon', 'body': '> @RabJon, Glad to know that the warnings disappear with tf-nightly. Could you please try using **os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""3""** which might help to disable the warnings.\r\n> \r\n> ```python\r\n> import os\r\n> os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""3""\r\n> ```\r\n> \r\n> Thank you!\r\n\r\nSorry for my late reply. I now tried setting the log level to ""3"" but the warning still occurs!', 'created_at': datetime.datetime(2025, 1, 10, 8, 51, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-13 14:13:59 UTC): @RabJon,
Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Also could you please try using tf-nighty/keras-nightly and test the code. 

**pip install tf-nightly or pip install keras-nightly**

Thank you!

RabJon (Issue Creator) on (2024-12-16 11:09:39 UTC): I created a simple [gist](https://gist.github.com/RabJon/89fbbae4fede25ef87a928436ad4c458) using a mock model with which I was able to reproduce the problem.


First I tried to install only keras-nightly but this was not enough, I immediately got the ""module not found"" error for Tensorflow. Then I also installed tf-nightly and then all the imports worked. 
Furthermore, also the problem mentioned in this issue did not appear anymore. 
Nevertheless, it would be nice to know where this warning comes from, why it can't be silenced and if it indicates some kind of mistake from my side?

Thanks and kind regards, 
Jonas

tilakrayal (Assginee) on (2024-12-20 06:31:58 UTC): @RabJon,
Glad to know that the warnings disappear with tf-nightly. Could you please try using **os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""** which might help to disable the warnings.

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""
```

Thank you!

github-actions[bot] on (2024-12-28 01:59:44 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-04 02:00:06 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-04 02:00:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82832"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82832"">No</a>

RabJon (Issue Creator) on (2025-01-10 08:51:54 UTC): Sorry for my late reply. I now tried setting the log level to ""3"" but the warning still occurs!

"
2732274547,issue,open,,Division by zero error at random places if GPU is used,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux, RedHatEnterprise 8.6

### Mobile device

_No response_

### Python version

N/A

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

Quadro RTX 6000

### Current behavior?

(I do not use Python and found no nightly build for C API)
I have a simple program that builds graphs through the c_api and executes sessions. It works perfectly as long as it is run only on CPU. If GPU is involved then at random places the program generates a Div0 error. The very same program run twice within one minute on the same hardware, etc. behave differently.
The error is somewhere deep in TF/Cuda because in the error dump I see 15 program jumps outside my code (what I can still debug).
I tried it on the different partition of a HPC, but both behave the same way. Card is Quadro RTX 6000, driver 565.57.01 Cuda 12.7 as seen in nvidia-smi, but 12.3 is available as libraries.
I tried with many different settings, etc., but cannot identify any rootcause. I am not even sure if the bug is in the TF binary or in one of the Cuda libraries (or elsewhere).

### Standalone code to reproduce the issue

```shell
I use a Pascal program, called examples, available here: https://github.com/zsoltszakaly/tensorflowforpascal.
It is compiled on the HPC with fpc -MObjFPC -Sh -Fl../tensorflow/lib examples.pas.
The tensorflow/lib directory has
Jan  1  2000  libtensorflow_framework.so -> libtensorflow_framework.so.2
Dec 10 11:04  libtensorflow_framework.so.2 -> libtensorflow_framework.so.2.18.0
Jan  1  2000  libtensorflow_framework.so.2.18.0
Jan  1  2000  libtensorflow.so -> libtensorflow.so.2
Dec 10 11:00  libtensorflow.so.2 -> libtensorflow.so.2.18.0
Jan  1  2000  libtensorflow.so.2.18.0
```


### Relevant log output

```shell
It generates every time a session is run, correctly this:
I0000 00:00:1733834127.549506 1676833 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38484 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0

But in the last one (again, it is random when!):
An unhandled exception occurred at $00007F223F261EA1:
EDivByZero: Division by zero
  $00007F223F261EA1
  $00007F223F1E2E72
  $00007F223F1E1D5D
  $00007F223F1E4183
  $00007F223F1E4AD2
  $00007F223F1E6DAE
  $00007F223F1D842B
  $00007F223F1D4241
  $00007F223EA97604
  $00007F223EA9608F
  $00007F223EA921E6
  $00007F223EA9067A
  $00007F223EA90231
  $00007F222E718923
  $00007F222E72561B
  $00000000004626AA
  $0000000000462931
```
",zsoltszakaly,2024-12-11 08:51:24+00:00,['Venkat6871'],2024-12-27 06:00:38+00:00,,https://github.com/tensorflow/tensorflow/issues/82736,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2538317384, 'issue_id': 2732274547, 'author': 'vaibhavi089', 'body': 'can i contribute to it ? @Venkat6871', 'created_at': datetime.datetime(2024, 12, 12, 9, 18, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539878686, 'issue_id': 2732274547, 'author': 'zsoltszakaly', 'body': ""> can i contribute to it ? @Venkat6871\r\n\r\nI don't know if and how we could work together on it. I know it is a bit of special set-up (Pascal, C-API, TF, Cuda, HPC), so it might be difficult to reproduce. If you (or someone else) has the willingness and capacity, what we can do is a zoom/teams/skype session with screensharing, where I can demonstrate the problem. I am in the Central European Timezone and on the weekend the computer is rather free, so it is easy to work on."", 'created_at': datetime.datetime(2024, 12, 12, 19, 48, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540533170, 'issue_id': 2732274547, 'author': 'Venkat6871', 'body': 'Hi **@zsoltszakaly** ,\r\nApologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 13, 4, 34, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540805381, 'issue_id': 2732274547, 'author': 'vaibhavi089', 'body': 'When we can schedule meet on zoom? Can you please give the timing\r\n\r\n\r\n________________________________\r\nFrom: Venkat6871 ***@***.***>\r\nSent: Friday, December 13, 2024 10:04 AM\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: vaibhavi089 ***@***.***>; Comment ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] Division by zero error at random places if GPU is used (Issue #82736)\r\n\r\n\r\nHi @zsoltszakaly<https://github.com/zsoltszakaly> ,\r\nApologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the documentation<https://www.tensorflow.org/install/source#gpu> for your reference.\r\nThank you!\r\n\r\n\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/82736#issuecomment-2540533170>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BI2V2C55BACRCR6TOOY5WM32FJPWFAVCNFSM6AAAAABTM6L57GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNBQGUZTGMJXGA>.\r\nYou are receiving this because you commented.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 12, 13, 7, 51, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540839937, 'issue_id': 2732274547, 'author': 'zsoltszakaly', 'body': ""This afternoon I can try to change some of the library versions as you suggested earlier to make sure that it is not caused by that and after that later this afternoon or on the weekend any European daytime is good for me. I don't know where you are, so my suggestions: Dec 13, 17:00 GMT, Dec 13 and 14, 9:00-17:00 GMT any time. If it is totally out of your timezone, then I can extend it from 5:00-21:00 GMT on the weekend."", 'created_at': datetime.datetime(2024, 12, 13, 8, 15, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541944642, 'issue_id': 2732274547, 'author': 'zsoltszakaly', 'body': '@vaibhavi089 , I tried to install a brand new cuda-12.5 as suggested, but now it fails with CUDA diagnostics, saying that the libcuda (freshly installed) is 555.42.2 (indeed this is what I see in the installation), but the kernel reported version is 565.57.1.', 'created_at': datetime.datetime(2024, 12, 13, 17, 41, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544852280, 'issue_id': 2732274547, 'author': 'Venkat6871', 'body': 'Hi @vaibhavi089 ,\r\nThank you for being here. Yes, you can contribute. Please follow the [documentation](https://www.tensorflow.org/community/contribute) for contributing and raise a PR for it.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 16, 7, 59, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545400403, 'issue_id': 2732274547, 'author': 'zsoltszakaly', 'body': 'I made many tests and the program still fails at random places. I tried to narrow the place where it occurs, but cannot (as it is random). I tried to include all operations in try..except structures and now I see invalid floating point operations.', 'created_at': datetime.datetime(2024, 12, 16, 11, 44, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558166674, 'issue_id': 2732274547, 'author': 'zsoltszakaly', 'body': 'I don\'t know if it helps, but with gdb I could get out the following error reason and the call hierarchy for one actual run:\r\n`Thread 1 ""examples"" received signal SIGFPE, Arithmetic exception.\r\n0x00007fffec940ea1 in tensorflow::grappler::GenericLayoutOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\nMissing separate debuginfos, use: yum debuginfo-install glibc-2.28-225.el8.x86_64 libgcc-8.5.0-18.el8.x86_64 libstdc++-8.5.0-18.el8.x86_64 nvidia-driver-cuda-libs-565.57.01-1.el8.x86_64\r\n(gdb) where\r\n#0  0x00007fffec940ea1 in tensorflow::grappler::GenericLayoutOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#1  0x00007fffec8c1e72 in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#2  0x00007fffec8c0d5d in tensorflow::grappler::MetaOptimizer::OptimizeGraph(std::vector<std::unique_ptr<tensorflow::grappler::GraphOptimizer, std::default_delete<tensorflow::grappler::GraphOptimizer> >, std::allocator<std::unique_ptr<tensorflow::grappler::GraphOptimizer, std::default_delete<tensorflow::grappler::GraphOptimizer> > > > const&, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#3  0x00007fffec8c3183 in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()\r\n   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#4  0x00007fffec8c3ad2 in tensorflow::grappler::MetaOptimizer::OptimizeConsumeItem(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()\r\n   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#5  0x00007fffec8c5dae in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem&&, tensorflow::ConfigProto const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#6  0x00007fffec8b742b in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, tensorflow::Graph const&, tensorflow::FunctionLibraryDefinition const*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*)\r\n    () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#7  0x00007fffec8b3241 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#8  0x00007fffec176604 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::lts_20230802::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::lts_20230802::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#9  0x00007fffec17508f in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#10 0x00007fffec1711e6 in tensorflow::DirectSession::GetOrCreateExecutors(absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#11 0x00007fffec16f67a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#12 0x00007fffec16f231 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#13 0x00007fffdbdf7923 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, st--Type <RET> for more, q to quit, c to continue without paging--\r\nd::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TSL_Status*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#14 0x00007fffdbe0461b in TF_SessionRun () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2\r\n#15 0x000000000046756a in RUN (this=..., AOPERATIONS=..., highAOPERATIONS=-1, AINPUTS=..., highAINPUTS=1, AINPUTVALUES=..., highAINPUTVALUES=1, AOUTPUTS=..., highAOUTPUTS=0)\r\n    at tf_operations.pas:585\r\n#16 0x00000000004677f1 in RUN (this=..., AINPUTS=..., highAINPUTS=1, AINPUTVALUES=..., highAINPUTVALUES=1, AOUTPUT=0x5864c0 \'finalresult\') at tf_operations.pas:606\r\n#17 0x00000000004043c9 in EXAMPLE7 () at examples.pas:253\r\n#18 0x000000000040bd8c in main () at examples.pas:1208\r\n`', 'created_at': datetime.datetime(2024, 12, 21, 16, 27, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561328681, 'issue_id': 2732274547, 'author': 'dikshantbhangala', 'body': 'Hi @Venkat6871 can i try to contribute on this??', 'created_at': datetime.datetime(2024, 12, 24, 18, 7, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563353107, 'issue_id': 2732274547, 'author': 'Venkat6871', 'body': 'Hi **@dikshantbhangala** ,\r\nThank you for being here. Yes, you can contribute. Please follow the [documentation](https://www.tensorflow.org/community/contribute) for contributing and raise a PR for it.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 27, 6, 0, 31, tzinfo=datetime.timezone.utc)}]","vaibhavi089 on (2024-12-12 09:18:05 UTC): can i contribute to it ? @Venkat6871

zsoltszakaly (Issue Creator) on (2024-12-12 19:48:51 UTC): I don't know if and how we could work together on it. I know it is a bit of special set-up (Pascal, C-API, TF, Cuda, HPC), so it might be difficult to reproduce. If you (or someone else) has the willingness and capacity, what we can do is a zoom/teams/skype session with screensharing, where I can demonstrate the problem. I am in the Central European Timezone and on the weekend the computer is rather free, so it is easy to work on.

Venkat6871 (Assginee) on (2024-12-13 04:34:18 UTC): Hi **@zsoltszakaly** ,
Apologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the [documentation](https://www.tensorflow.org/install/source#gpu) for your reference.
Thank you!

vaibhavi089 on (2024-12-13 07:51:38 UTC): When we can schedule meet on zoom? Can you please give the timing


________________________________
From: Venkat6871 ***@***.***>
Sent: Friday, December 13, 2024 10:04 AM
To: tensorflow/tensorflow ***@***.***>
Cc: vaibhavi089 ***@***.***>; Comment ***@***.***>
Subject: Re: [tensorflow/tensorflow] Division by zero error at random places if GPU is used (Issue #82736)


Hi @zsoltszakaly<https://github.com/zsoltszakaly> ,
Apologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the documentation<https://www.tensorflow.org/install/source#gpu> for your reference.
Thank you!


Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/82736#issuecomment-2540533170>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BI2V2C55BACRCR6TOOY5WM32FJPWFAVCNFSM6AAAAABTM6L57GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNBQGUZTGMJXGA>.
You are receiving this because you commented.Message ID: ***@***.***>

zsoltszakaly (Issue Creator) on (2024-12-13 08:15:06 UTC): This afternoon I can try to change some of the library versions as you suggested earlier to make sure that it is not caused by that and after that later this afternoon or on the weekend any European daytime is good for me. I don't know where you are, so my suggestions: Dec 13, 17:00 GMT, Dec 13 and 14, 9:00-17:00 GMT any time. If it is totally out of your timezone, then I can extend it from 5:00-21:00 GMT on the weekend.

zsoltszakaly (Issue Creator) on (2024-12-13 17:41:35 UTC): @vaibhavi089 , I tried to install a brand new cuda-12.5 as suggested, but now it fails with CUDA diagnostics, saying that the libcuda (freshly installed) is 555.42.2 (indeed this is what I see in the installation), but the kernel reported version is 565.57.1.

Venkat6871 (Assginee) on (2024-12-16 07:59:52 UTC): Hi @vaibhavi089 ,
Thank you for being here. Yes, you can contribute. Please follow the [documentation](https://www.tensorflow.org/community/contribute) for contributing and raise a PR for it.
Thank you!

zsoltszakaly (Issue Creator) on (2024-12-16 11:44:13 UTC): I made many tests and the program still fails at random places. I tried to narrow the place where it occurs, but cannot (as it is random). I tried to include all operations in try..except structures and now I see invalid floating point operations.

zsoltszakaly (Issue Creator) on (2024-12-21 16:27:37 UTC): I don't know if it helps, but with gdb I could get out the following error reason and the call hierarchy for one actual run:
`Thread 1 ""examples"" received signal SIGFPE, Arithmetic exception.
0x00007fffec940ea1 in tensorflow::grappler::GenericLayoutOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-225.el8.x86_64 libgcc-8.5.0-18.el8.x86_64 libstdc++-8.5.0-18.el8.x86_64 nvidia-driver-cuda-libs-565.57.01-1.el8.x86_64
(gdb) where
#0  0x00007fffec940ea1 in tensorflow::grappler::GenericLayoutOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#1  0x00007fffec8c1e72 in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#2  0x00007fffec8c0d5d in tensorflow::grappler::MetaOptimizer::OptimizeGraph(std::vector<std::unique_ptr<tensorflow::grappler::GraphOptimizer, std::default_delete<tensorflow::grappler::GraphOptimizer> >, std::allocator<std::unique_ptr<tensorflow::grappler::GraphOptimizer, std::default_delete<tensorflow::grappler::GraphOptimizer> > > > const&, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#3  0x00007fffec8c3183 in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()
   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#4  0x00007fffec8c3ad2 in tensorflow::grappler::MetaOptimizer::OptimizeConsumeItem(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()
   from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#5  0x00007fffec8c5dae in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem&&, tensorflow::ConfigProto const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#6  0x00007fffec8b742b in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, tensorflow::Graph const&, tensorflow::FunctionLibraryDefinition const*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*)
    () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#7  0x00007fffec8b3241 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#8  0x00007fffec176604 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::lts_20230802::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::lts_20230802::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#9  0x00007fffec17508f in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#10 0x00007fffec1711e6 in tensorflow::DirectSession::GetOrCreateExecutors(absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#11 0x00007fffec16f67a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#12 0x00007fffec16f231 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#13 0x00007fffdbdf7923 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, st--Type <RET> for more, q to quit, c to continue without paging--
d::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TSL_Status*) () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#14 0x00007fffdbe0461b in TF_SessionRun () from /home/p_habsz/p_hab/tensorflow/lib/libtensorflow.so.2
#15 0x000000000046756a in RUN (this=..., AOPERATIONS=..., highAOPERATIONS=-1, AINPUTS=..., highAINPUTS=1, AINPUTVALUES=..., highAINPUTVALUES=1, AOUTPUTS=..., highAOUTPUTS=0)
    at tf_operations.pas:585
#16 0x00000000004677f1 in RUN (this=..., AINPUTS=..., highAINPUTS=1, AINPUTVALUES=..., highAINPUTVALUES=1, AOUTPUT=0x5864c0 'finalresult') at tf_operations.pas:606
#17 0x00000000004043c9 in EXAMPLE7 () at examples.pas:253
#18 0x000000000040bd8c in main () at examples.pas:1208
`

dikshantbhangala on (2024-12-24 18:07:43 UTC): Hi @Venkat6871 can i try to contribute on this??

Venkat6871 (Assginee) on (2024-12-27 06:00:31 UTC): Hi **@dikshantbhangala** ,
Thank you for being here. Yes, you can contribute. Please follow the [documentation](https://www.tensorflow.org/community/contribute) for contributing and raise a PR for it.
Thank you!

"
2730334124,issue,closed,completed,TF 2.18 fails to use GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.6; CUDNN 9.0  

### GPU model and memory

NVIDIA GeForce RTX 3060  12 GBs

### Current behavior?


TF fails to connect to the GPU card on some operations. Specifically, it fails with the code from the TF Basics tutorials, located at   https://www.tensorflow.org/guide/basics : 

""history = new_model.fit(x, y,
                        epochs=100,
                        batch_size=32,
                        verbose=0)
""

The error is 

""
DNN library initialization failed. Look at the errors above for more details.
\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_24212]""
""

2. The code below returns :

print(""Python version: "", sys.version)
print(""TensorFlow version: "", tf.__version__)
print(tf.config.list_physical_devices('GPU'))
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

The result returned is:

Python Version : 3.12.3 [GCC 13.2.0]
TensorFlow version: 2.18.0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Num GPUs Available: 1

If I understand this correctly, TF is telling me that it found ! GPU Device, but it is not able to connect.

Just FYI, PyTorch and Julia can use the GPU with no problem. Therefore I assume the problem is with TF, not with my system.



### Standalone code to reproduce the issue

```shell
import sys

# several messages are displayed when TF is imported in a local system
import tensorflow as tf


x = tf.linspace(-2, 2, 201)
x = tf.cast(x, tf.float32)

def f(x):
  y = x**2 + 2*x - 5
  return y

y = f(x) + tf.random.normal(shape=[201])

plt.plot(x.numpy(), y.numpy(), '.', label='Data')
plt.plot(x, f(x), label='Ground truth')
plt.legend();



class Model(tf.Module):

  def __init__(self):
    # Randomly generate weight and bias terms
    rand_init = tf.random.uniform(shape=[3], minval=0., maxval=5., seed=22)
    # Initialize model parameters
    self.w_q = tf.Variable(rand_init[0])
    self.w_l = tf.Variable(rand_init[1])
    self.b = tf.Variable(rand_init[2])
  
  @tf.function
  def __call__(self, x):
    # Quadratic Model : quadratic_weight * x^2 + linear_weight * x + bias
    return self.w_q * (x**2) + self.w_l * x + self.b


new_model = tf.keras.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.stack([x, x**2], axis=1)),
    tf.keras.layers.Dense(units=1, kernel_initializer=tf.random.normal)])


print(""Python version: "", sys.version)
print(""TensorFlow version: "", tf.__version__)
print(tf.config.list_physical_devices('GPU'))
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))


new_model.compile(
    loss=tf.keras.losses.MSE,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01))

### ERROR HERE 
history = new_model.fit(x, y,
                        epochs=100,
                        batch_size=32,
                        verbose=0)
```


### Relevant log output

```shell
DNN library initialization failed. Look at the errors above for more details.
\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_24212]""
```
",JuanVargas,2024-12-10 14:47:43+00:00,['tilakrayal'],2024-12-10 15:34:49+00:00,2024-12-10 15:34:35+00:00,https://github.com/tensorflow/tensorflow/issues/82669,"[('type:bug', 'Bug')]","[{'comment_id': 2532078760, 'issue_id': 2730334124, 'author': 'JuanVargas', 'body': 'I was able to run the code after I updated the path to LD_LIBRARY_PATH.\r\n\r\nThereore I am closing this issue.', 'created_at': datetime.datetime(2024, 12, 10, 15, 34, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532078853, 'issue_id': 2730334124, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82669"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82669"">No</a>', 'created_at': datetime.datetime(2024, 12, 10, 15, 34, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532079301, 'issue_id': 2730334124, 'author': 'JuanVargas', 'body': 'I was able to run the code after I updated the path to LD_LIBRARY_PATH.\r\n\r\nThereore I am closing this issue.', 'created_at': datetime.datetime(2024, 12, 10, 15, 34, 47, tzinfo=datetime.timezone.utc)}]","JuanVargas (Issue Creator) on (2024-12-10 15:34:35 UTC): I was able to run the code after I updated the path to LD_LIBRARY_PATH.

Thereore I am closing this issue.

google-ml-butler[bot] on (2024-12-10 15:34:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82669"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82669"">No</a>

JuanVargas (Issue Creator) on (2024-12-10 15:34:47 UTC): I was able to run the code after I updated the path to LD_LIBRARY_PATH.

Thereore I am closing this issue.

"
2729411256,issue,open,,ValueError: as_list() is not defined on an unknown TensorShape.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The created dataset's <class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'> shape is unknown.
which when used for model.evaluate results in error:

```
self.model.evaluate(self.data_loader.get_valid_loader(batch_size), verbose=1)
  File ""/home/perfuser/shailesh/9_dec/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/perfuser/shailesh/9_dec/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
    ^^^^^^^^^^^^^^^
ValueError: as_list() is not defined on an unknown TensorShape.
```
Apologies for sharing partial code.
My question is: how can I set the shape of  tensorflow.python.data.ops.prefetch_op._PrefetchDataset dataset, or what is the solution to resolve the issue ""as_list() is not defined on an unknown TensorShape""?

The same dataset with an unknown shape worked with TensorFlow 2.13.

### Standalone code to reproduce the issue

```shell
ds_val = ds_val.map(lambda x: tf.py_function(self.read_nifti_file,
                                                     [x, False], [tf.float32, tf.float32]),
                            num_parallel_calls=tf.data.experimental.AUTOTUNE)
self.model.evaluate(self.data_loader.get_valid_loader(batch_size), verbose=1)
```
```


### Relevant log output

```shell
ds_val 3 <_PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>

type(ds_val) <class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
```
",tanwarsh,2024-12-10 08:40:54+00:00,['Venkat6871'],2024-12-12 06:44:06+00:00,,https://github.com/tensorflow/tensorflow/issues/82637,"[('type:support', 'Support issues'), ('TF 2.18', '')]","[{'comment_id': 2537825257, 'issue_id': 2729411256, 'author': 'Venkat6871', 'body': 'Hi **@tanwarsh** ,\r\n\r\n\r\nApologies for the delay, and thank you for raising your concern here.I tried running your code on Colab using TensorFlow 2.18.0 but encountered a different issue. Could you please provide more code for execution or share a Colab gist? That would help in diagnosing the issue more effectively.\r\nThe issue seems to be related to an unknown TensorShape. While TensorFlow can typically handle shapes with unknown dimensions, it cannot handle shapes with an unknown number of dimensions. When using tf.py_function, the results returned from Python code can be of any shape, which TensorFlow cannot infer automatically. \r\nTo avoid this issue, you should explicitly define the shapes for the tensors. Doing so should allow your code to run smoothly. In older versions, this might have worked differently, but the behavior in newer versions seems to have changed. Please try this solution and let us know if you still face any issues.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 12, 5, 6, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537946141, 'issue_id': 2729411256, 'author': 'tanwarsh', 'body': 'Hi @Venkat6871 , Thank you for the help.\r\nThe code in the link below will not make sense. I tried reproducing the issue but was not able to get exact same issue as it is hard to use exact data and model but the issue is similar and related to shape and my objective is to resolve it by setting the shape only. \r\nhttps://colab.research.google.com/drive/19m8B3FMA9K1i7cZ_kNzw9dufzXsnVpww?usp=sharing', 'created_at': datetime.datetime(2024, 12, 12, 6, 44, 4, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-12 05:06:26 UTC): Hi **@tanwarsh** ,


Apologies for the delay, and thank you for raising your concern here.I tried running your code on Colab using TensorFlow 2.18.0 but encountered a different issue. Could you please provide more code for execution or share a Colab gist? That would help in diagnosing the issue more effectively.
The issue seems to be related to an unknown TensorShape. While TensorFlow can typically handle shapes with unknown dimensions, it cannot handle shapes with an unknown number of dimensions. When using tf.py_function, the results returned from Python code can be of any shape, which TensorFlow cannot infer automatically. 
To avoid this issue, you should explicitly define the shapes for the tensors. Doing so should allow your code to run smoothly. In older versions, this might have worked differently, but the behavior in newer versions seems to have changed. Please try this solution and let us know if you still face any issues.

Thank you!

tanwarsh (Issue Creator) on (2024-12-12 06:44:04 UTC): Hi @Venkat6871 , Thank you for the help.
The code in the link below will not make sense. I tried reproducing the issue but was not able to get exact same issue as it is hard to use exact data and model but the issue is similar and related to shape and my objective is to resolve it by setting the shape only. 
https://colab.research.google.com/drive/19m8B3FMA9K1i7cZ_kNzw9dufzXsnVpww?usp=sharing

"
2728810739,issue,closed,completed,[MSVC] Compile error in external/llvm-project/llvm/lib/Support/TrieRawHashMap.cpp,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest commit

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

Windows 11

### Python version

python 3.12

### Bazel version

bazel 6.5

### GCC/compiler version

MSVC 19.43.34708.97 (inner build)

### CUDA/cuDNN version

12.4

### GPU model and memory

_No response_

### Current behavior?

Recently, as I build Tensorflow from source with internal build of MSVC, I encountered the following errors:
```
external/llvm-project/llvm/lib/Support/TrieRawHashMap.cpp(111): error C2990: 'llvm::TrailingObjects': non-class template has already been declared as a class template
external/llvm-project/llvm/include\llvm/Support/TrailingObjects.h(212): note: see declaration of 'llvm::TrailingObjects'
external/llvm-project/llvm/lib/Support/TrieRawHashMap.cpp(204): error C2990: 'llvm::TrailingObjects': non-class template has already been declared as a class template
external/llvm-project/llvm/include\llvm/Support/TrailingObjects.h(212): note: see declaration of 'llvm::TrailingObjects'
```

Repro steps:

1. git clone https://github.com/tensorflow/tensorflow.git
2. cd /d C:\gitP\tensorflow\tensorflow
3. pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt --upgrade
4. yes """" 2>nul | python ./configure.py 2>&1
5. mkdir build_amd64 & cd build_amd64
6. set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
7. set BAZEL_VC_FULL_VERSION=14.40.33807
8. bazel --output_user_root C:\bazelTemp build --jobs 16 --config=opt --local_ram_resources=16384  --subcommands //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tf_nightly --repo_env=TF_PYTHON_VERSION=3.12

Build log: [tf.txt](https://github.com/user-attachments/files/18070855/tf.txt)
(Search for `Command #` in the log to view the complete list of commands executed.)

### Standalone code to reproduce the issue

```shell
--
```


### Relevant log output

```shell
[8,432 / 22,570] Compiling llvm/lib/Support/SpecialCaseList.cpp [for tool]; 7s local ... (16 actions, 8 running)
ERROR: C:/bazeltemp/yasrpdtv/external/llvm-project/llvm/BUILD.bazel:253:11: Compiling llvm/lib/Support/TrieRawHashMap.cpp [for tool] failed: (Exit 2): cl.exe failed: error executing command (from target @llvm-project//llvm:Support) 
  cd /d C:/bazeltemp/yasrpdtv/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.40.33807\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.40.33807\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.40.33807\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Team Tools\DiagnosticsHub\Collector;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\Extensions\Microsoft\CodeCoverage.Console;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET TEMP=C:\Users\V-NEIL~1\AppData\Local\Temp\2
    SET TMP=C:\Users\V-NEIL~1\AppData\Local\Temp\2
  C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.40.33807\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt-exec-50AE0418/bin/external/llvm-project/llvm/_objs/Support/TrieRawHashMap.obj.params
```
",NEIL-smtg,2024-12-10 02:47:41+00:00,['tilakrayal'],2024-12-28 01:59:49+00:00,2024-12-28 01:59:46+00:00,https://github.com/tensorflow/tensorflow/issues/82574,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues')]","[{'comment_id': 2539031720, 'issue_id': 2728810739, 'author': 'tilakrayal', 'body': '@NEIL-smtg,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the error?\r\n\r\nAlso, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu) and check if you have the correct dependencies installed. Thank you!', 'created_at': datetime.datetime(2024, 12, 12, 13, 58, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556112940, 'issue_id': 2728810739, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137752, 'issue_id': 2728810739, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137773, 'issue_id': 2728810739, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82574"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82574"">No</a>', 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-12 13:58:17 UTC): @NEIL-smtg,
Could you please provide the exact sequence of commands / steps that you executed before running into the error?

Also, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu) and check if you have the correct dependencies installed. Thank you!

github-actions[bot] on (2024-12-20 02:01:44 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-28 01:59:46 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-28 01:59:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82574"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82574"">No</a>

"
2727075821,issue,open,,Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The Python version of the docker images is outdated and should be updated

### Standalone code to reproduce the issue

```shell
docker run -it tensorflow/tensorflow python
```


### Relevant log output

```shell
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
```
",gcuder,2024-12-09 13:35:51+00:00,['Venkat6871'],2025-01-03 19:11:15+00:00,,https://github.com/tensorflow/tensorflow/issues/82529,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('TF 2.18', '')]","[{'comment_id': 2552732468, 'issue_id': 2727075821, 'author': 'Venkat6871', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 12, 19, 4, 9, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569694510, 'issue_id': 2727075821, 'author': 'mihaimaruseac', 'body': 'This might be an artifact of trying to support 3.11 before it was fully released.', 'created_at': datetime.datetime(2025, 1, 3, 19, 11, 12, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-19 04:09:30 UTC): @learning-to-play

mihaimaruseac on (2025-01-03 19:11:12 UTC): This might be an artifact of trying to support 3.11 before it was fully released.

"
2726793015,issue,closed,completed,Linking an Android static library with TFLite GPU using CMake causes undefined symbol errors and can not get the correct install,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Linking an Android library with libtensorflow-lite.a using CMake with GPU delegate enabled causes undefined symbol errors
and can make install to get a complete include envirment.

### Standalone code to reproduce the issue

```shell
before cmake, do some changes:
tensorflow-2.18.0\tensorflow\lite\tools\cmake\modules\ml_dtypes\cmakelists.txt:
target_include_directories(ml_dtypes INTERFACE
  ""
$<BUILD_INTERFACE:$
{ML_DTYPES_SOURCE_DIR}>"" ""
$<INSTALL_INTERFACE:$
{CMAKE_INSTALL_INCLUDEDIR}>""
  ""$<BUILD_INTERFACE:${ML_DTYPES_SOURCE_DIR}/ml_dtypes>"" ""$<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}/ml_dtypes>"")

tensorflow-2.18.0\tensorflow\lite\cmakelists.txt:
change like https://github.com/tensorflow/tensorflow/issues/61312

although do these changes, errors occured like the following log output.

step1:
cmake ../tensorflow-2.18.0/tensorflow/lite -DCMAKE_TOOLCHAIN_FILE=/home/public/tflite/android-ndk-r25c/build/cmake/android.toolchain.cmake \
     -DTFLITE_ENABLE_GPU=ON -DXNNPACK_ENABLE_ARM_BF16=OFF -DANDROID_ABI=arm64-v8a  -DANDROID_PLATFORM=26  \
	 -DTFLITE_HOST_TOOLS_DIR=/home/public/tflite/flatbuffers-master/build/ \
	 -DFLATBUFFERS_FLATC_EXECUTABLE=/home/public/tflite/flatbuffers-master/build/flatc \
     -DTFLITE_ENABLE_INSTALL=ON  -DXNNPACK_ENABLE_ARM_BF16=OFF\
     -DCMAKE_INSTALL_PREFIX=/home/pulic/tflite/local_install
step2: cmake --build . -j
step3: sudo make install
step4: cmake --build . -j -t benchmark_model
```


### Relevant log output

```shell
after step1:
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""eigen"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_flags"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_hash"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_status"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_strings"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_synchronization"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_variant"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""ruy"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_any"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_flat_hash_map"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""xnnpack-delegate"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""XNNPACK"" that is not in any export set.

after step4:
[ 97%] Linking CXX executable benchmark_model
ld: error: undefined symbol: eglGetProcAddress
>>> referenced by async_buffers.cc:35 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:35)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::MapAHardwareBufferToGlBuffer()) in archive ../../libtensorflow-lite.a
>>> referenced by async_buffers.cc:38 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:38)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::MapAHardwareBufferToGlBuffer()) in archive ../../libtensorflow-lite.a
>>> referenced by android_sync.cc:33 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/android_sync.cc:33)
>>>               android_sync.cc.o:((anonymous namespace)::IsGlSupported()::$_0::operator()() const) in archive ../../libtensorflow-lite.a
>>> referenced 3 more times

ld: error: undefined symbol: glGenBuffers
>>> referenced by async_buffers.cc:72 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:72)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::AllocateOpenGlBuffer()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glBindBuffer
>>> referenced by async_buffers.cc:73 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:73)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::AllocateOpenGlBuffer()) in archive ../../libtensorflow-lite.a
>>> referenced by async_buffers.cc:85 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:85)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::AllocateOpenGlBuffer()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glBufferData
>>> referenced by async_buffers.cc:83 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/async_buffers.cc:83)
>>>               async_buffers.cc.o:(tflite::gpu::AsyncBuffer::AllocateOpenGlBuffer()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: eglGetDisplay
>>> referenced by android_sync.cc:59 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/android_sync.cc:59)
>>>               android_sync.cc.o:(tflite::gpu::gl::WaitFdGpu(int)) in archive ../../libtensorflow-lite.a
>>> referenced by android_sync.cc:82 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/android_sync.cc:82)
>>>               android_sync.cc.o:(tflite::gpu::gl::CreateFdGpu()) in archive ../../libtensorflow-lite.a
>>> referenced by gl_call.h:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_call.h:84)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a
>>> referenced 1 more times

ld: error: undefined symbol: glFinish
>>> referenced by android_sync.cc:97 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/android_sync.cc:97)
>>>               android_sync.cc.o:(tflite::gpu::gl::CreateFdGpu()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glGetError
>>> referenced by gl_errors.cc:67 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_errors.cc:67)
>>>               gl_errors.cc.o:(tflite::gpu::gl::GetOpenGlErrors()) in archive ../../libtensorflow-lite.a
>>> referenced by gl_errors.cc:71 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_errors.cc:71)
>>>               gl_errors.cc.o:(tflite::gpu::gl::GetOpenGlErrors()) in archive ../../libtensorflow-lite.a
>>> referenced by gl_errors.cc:76 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_errors.cc:76)
>>>               gl_errors.cc.o:(tflite::gpu::gl::GetOpenGlErrors()) in archive ../../libtensorflow-lite.a
>>> referenced 1 more times

ld: error: undefined symbol: eglGetError
>>> referenced by gl_errors.cc:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_errors.cc:84)
>>>               gl_errors.cc.o:(tflite::gpu::gl::GetEglError()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: eglBindAPI
>>> referenced by gl_call.h:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_call.h:84)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a
>>> referenced by gl_call.h:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_call.h:84)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: eglGetCurrentContext
>>> referenced by egl_environment.cc:75 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:75)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a
>>> referenced by egl_environment.cc:78 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:78)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: eglGetCurrentDisplay
>>> referenced by egl_environment.cc:76 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:76)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: eglInitialize
>>> referenced by gl_call.h:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_call.h:84)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a
>>> referenced by gl_call.h:84 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/gl_call.h:84)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::Init()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glDeleteFramebuffers
>>> referenced by egl_environment.cc:59 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:59)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::~EglEnvironment()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glDeleteTextures
>>> referenced by egl_environment.cc:62 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:62)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::~EglEnvironment()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glGenFramebuffers
>>> referenced by egl_environment.cc:132 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:132)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glBindFramebuffer
>>> referenced by egl_environment.cc:133 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:133)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glGenTextures
>>> referenced by egl_environment.cc:135 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:135)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glBindTexture
>>> referenced by egl_environment.cc:136 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:136)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glFramebufferTexture2D
>>> referenced by egl_environment.cc:138 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:138)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: undefined symbol: glViewport
>>> referenced by egl_environment.cc:144 (/home/public/tflite/tensorflow-2.18.0/tensorflow/lite/delegates/gpu/gl/egl_environment.cc:144)
>>>               egl_environment.cc.o:(tflite::gpu::gl::EglEnvironment::ForceSyncTurning()) in archive ../../libtensorflow-lite.a

ld: error: too many errors emitted, stopping now (use -error-limit=0 to see all errors)
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
make[3]: *** [tools/benchmark/CMakeFiles/benchmark_model.dir/build.make:682: tools/benchmark/benchmark_model] Error 1
make[2]: *** [CMakeFiles/Makefile2:9168: tools/benchmark/CMakeFiles/benchmark_model.dir/all] Error 2
make[1]: *** [CMakeFiles/Makefile2:9175: tools/benchmark/CMakeFiles/benchmark_model.dir/rule] Error 2
make: *** [Makefile:2652: benchmark_model] Error 2
```
",alexliyang,2024-12-09 11:39:32+00:00,['gaikwadrahul8'],2024-12-26 06:30:48+00:00,2024-12-24 08:27:20+00:00,https://github.com/tensorflow/tensorflow/issues/82526,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TF 2.18', '')]","[{'comment_id': 2533740188, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': 'and when set TFLITE_ENABLE_INSTALL=ON, how to get the full tflite envirment for dev other codes, after exec ""sudo make install""?', 'created_at': datetime.datetime(2024, 12, 11, 6, 23, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535582616, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': 'libkleidiai.a  need copy to install lib, or libxnnpack.a will be ld error', 'created_at': datetime.datetime(2024, 12, 11, 11, 18, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545251098, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': 'INFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Loaded OpenCL library with dlopen.\r\nVERBOSE: Replacing 118 out of 118 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\nERROR: TfLiteGpuDelegate Init: PRELU: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 118 (TfLiteGpuDelegateV2) failed to prepare.\r\nERROR: Restored original execution plan after delegate application failure.\r\nCould not setup GPU delegate', 'created_at': datetime.datetime(2024, 12, 16, 10, 53, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546710113, 'issue_id': 2726793015, 'author': 'gaikwadrahul8', 'body': 'Hi, @alexliyang\r\n\r\nI apologize for the delayed response, In Android TensorFlow Lite GPU delegate requires `OpenGL` `ES` and `EGL` for managing the GPU context. To fix the undefined symbol errors you need to make sure that the appropriate `OpenGL` `ES` and `EGL` libraries are linked. This can be done by modifying your `CMakeLists.txt` to include these libraries something like below please refer this similar issue https://github.com/tensorflow/tensorflow/issues/61312 and [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) which may help you to solve your issue.\r\n\r\n\r\n```\r\ntarget_link_libraries (${PROJECT_NAME}\r\n        PUBLIC\r\n          tensorflow-lite\r\n          GLESv3\r\n          EGL\r\n          android\r\n)\r\n```\r\nIf issue still persists or Am I missing something here please let me know ?\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 16, 20, 45, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547313686, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': '@gaikwadrahul8 Thank you for your response, I refer issue #61312 and other repo to modify the CMakeLists.txt, now I can get the libtensorflow-lite.a , and all ld errors have been fixed.\r\nBut I get another error like following , the op PReLu is not supported in tensorflow-lite version 2.18.0?\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Loaded OpenCL library with dlopen.\r\nVERBOSE: Replacing 118 out of 118 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\nERROR: TfLiteGpuDelegate Init: PRELU: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 118 (TfLiteGpuDelegateV2) failed to prepare.\r\nERROR: Restored original execution plan after delegate application failure.\r\nCould not setup GPU delegate\r\n![image](https://github.com/user-attachments/assets/13932146-659f-4a71-9d0a-5994dcebe285)\r\n\r\nI check the model, all PReLu are 4D tensor of shape 1xHxWxC. but get the errors.', 'created_at': datetime.datetime(2024, 12, 17, 1, 36, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547362305, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': 'I find #39749 discuss this problem, but Now version 2.18.0 is ocurred , how to correct it ?', 'created_at': datetime.datetime(2024, 12, 17, 2, 23, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558968657, 'issue_id': 2726793015, 'author': 'gaikwadrahul8', 'body': ""Hi, @alexliyang \r\nI apologize for the delayed response, I checked the [official documentation](https://ai.google.dev/edge/litert/performance/gpu) and it supports `PRELU` Op if you don't mind could you please give it try by downgrading the TensorFlow version to `2.17.1 or 2.16.1` and see is it working as expected or not ?\r\n\r\nIf you've have PyTorch model and want to convert to TensorFlow Lite then you can use [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) which is a python library that supports converting PyTorch models into a .tflite format, which can then be run with TensorFlow Lite and MediaPipe. This enables applications for Android, iOS and IOT that can run models completely on-device. AI Edge Torch offers broad CPU coverage, with initial GPU and NPU support. AI Edge Torch seeks to closely integrate with PyTorch, building on top of torch.export() and providing good coverage of Core ATen operators.\r\n\r\n\r\n\r\nIf issue still persists please let us know with updated error log for further investigation from our end.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 23, 6, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560829383, 'issue_id': 2726793015, 'author': 'alexliyang', 'body': 'I check the model, maybe the torch model convert to tflite, prelu op need to be reshaped to hwc or 1hwc . I use flatc to handle the  model ,then the model can be loaded correctly.\r\nthank you very much!', 'created_at': datetime.datetime(2024, 12, 24, 8, 27, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560829495, 'issue_id': 2726793015, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82526"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82526"">No</a>', 'created_at': datetime.datetime(2024, 12, 24, 8, 27, 22, tzinfo=datetime.timezone.utc)}]","alexliyang (Issue Creator) on (2024-12-11 06:23:32 UTC): and when set TFLITE_ENABLE_INSTALL=ON, how to get the full tflite envirment for dev other codes, after exec ""sudo make install""?

alexliyang (Issue Creator) on (2024-12-11 11:18:57 UTC): libkleidiai.a  need copy to install lib, or libxnnpack.a will be ld error

alexliyang (Issue Creator) on (2024-12-16 10:53:27 UTC): INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Loaded OpenCL library with dlopen.
VERBOSE: Replacing 118 out of 118 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
ERROR: TfLiteGpuDelegate Init: PRELU: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got
INFO: Created 0 GPU delegate kernels.
ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized
ERROR: Node number 118 (TfLiteGpuDelegateV2) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Could not setup GPU delegate

gaikwadrahul8 (Assginee) on (2024-12-16 20:45:31 UTC): Hi, @alexliyang

I apologize for the delayed response, In Android TensorFlow Lite GPU delegate requires `OpenGL` `ES` and `EGL` for managing the GPU context. To fix the undefined symbol errors you need to make sure that the appropriate `OpenGL` `ES` and `EGL` libraries are linked. This can be done by modifying your `CMakeLists.txt` to include these libraries something like below please refer this similar issue https://github.com/tensorflow/tensorflow/issues/61312 and [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) which may help you to solve your issue.


```
target_link_libraries (${PROJECT_NAME}
        PUBLIC
          tensorflow-lite
          GLESv3
          EGL
          android
)
```
If issue still persists or Am I missing something here please let me know ?
Thank you for your cooperation and patience.

alexliyang (Issue Creator) on (2024-12-17 01:36:24 UTC): @gaikwadrahul8 Thank you for your response, I refer issue #61312 and other repo to modify the CMakeLists.txt, now I can get the libtensorflow-lite.a , and all ld errors have been fixed.
But I get another error like following , the op PReLu is not supported in tensorflow-lite version 2.18.0?
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Loaded OpenCL library with dlopen.
VERBOSE: Replacing 118 out of 118 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
ERROR: TfLiteGpuDelegate Init: PRELU: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got
INFO: Created 0 GPU delegate kernels.
ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized
ERROR: Node number 118 (TfLiteGpuDelegateV2) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Could not setup GPU delegate
![image](https://github.com/user-attachments/assets/13932146-659f-4a71-9d0a-5994dcebe285)

I check the model, all PReLu are 4D tensor of shape 1xHxWxC. but get the errors.

alexliyang (Issue Creator) on (2024-12-17 02:23:08 UTC): I find #39749 discuss this problem, but Now version 2.18.0 is ocurred , how to correct it ?

gaikwadrahul8 (Assginee) on (2024-12-23 06:02:07 UTC): Hi, @alexliyang 
I apologize for the delayed response, I checked the [official documentation](https://ai.google.dev/edge/litert/performance/gpu) and it supports `PRELU` Op if you don't mind could you please give it try by downgrading the TensorFlow version to `2.17.1 or 2.16.1` and see is it working as expected or not ?

If you've have PyTorch model and want to convert to TensorFlow Lite then you can use [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) which is a python library that supports converting PyTorch models into a .tflite format, which can then be run with TensorFlow Lite and MediaPipe. This enables applications for Android, iOS and IOT that can run models completely on-device. AI Edge Torch offers broad CPU coverage, with initial GPU and NPU support. AI Edge Torch seeks to closely integrate with PyTorch, building on top of torch.export() and providing good coverage of Core ATen operators.



If issue still persists please let us know with updated error log for further investigation from our end.

Thank you for your cooperation and patience.

alexliyang (Issue Creator) on (2024-12-24 08:27:15 UTC): I check the model, maybe the torch model convert to tflite, prelu op need to be reshaped to hwc or 1hwc . I use flatc to handle the  model ,then the model can be loaded correctly.
thank you very much!

google-ml-butler[bot] on (2024-12-24 08:27:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82526"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82526"">No</a>

"
2725244964,issue,closed,completed,tensorflow error in pycharm in ubuntu 22.04,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

linux ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tensorflow error in pycharm in ubuntu 22.04
![image](https://github.com/user-attachments/assets/1a13794b-9a9d-44b4-b81d-5ad3f585b814)
it's error in pycharm,but the code is correct and can run corretly

### Standalone code to reproduce the issue

```shell
import numpy as np
import matplotlib
import tensorflow.python.keras.models

matplotlib.use('agg')
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs
from tensorflow.python import keras
from tensorflow.keras import backend as K
from tensorflow.keras.layers import (
    Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,
    Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec,
LayerNormalization,Layer, Dense, Dropout
)
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from tensorflow import keras
```


### Relevant log output

_No response_",Chuan1937,2024-12-08 13:34:03+00:00,['Venkat6871'],2025-01-07 02:02:26+00:00,2025-01-07 02:02:23+00:00,https://github.com/tensorflow/tensorflow/issues/82465,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2525914820, 'issue_id': 2725244964, 'author': 'cuixue', 'body': '', 'created_at': datetime.datetime(2024, 12, 8, 13, 37, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530606552, 'issue_id': 2725244964, 'author': 'Venkat6871', 'body': 'Hi **@Chuan1937** ,\r\nCould you please provide details about the error you are facing? I tried running your code on Colab using TensorFlow 2.18.0, and it worked fine for me.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b4532cb16caf48e07ddcb75f1c84e5f2/82465_tf_2-18-0-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 10, 7, 3, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536431974, 'issue_id': 2725244964, 'author': 'Chuan1937', 'body': ""The code is correct, my problem is that on Ubuntu, using the above method to import tensorflow functions will result in an error: there is no such function.Although there may be errors, the code can run normally. I don't know why."", 'created_at': datetime.datetime(2024, 12, 11, 16, 12, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544876631, 'issue_id': 2725244964, 'author': 'Venkat6871', 'body': 'Hi **@Chuan1937** ,\r\nApologies for the delay, and thank you for your patience. I tried running your code on Ubuntu using TensorFlow version 2.18.0, and it is working fine for me. I have attached it below, please check it once. Let me know if I did anything wrong here.\r\n```\r\n(tf) (tf) maayara@venkat-gpu1:~$ python3\r\nPython 3.9.20 (main, Oct  3 2024, 07:27:41) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\n2024-12-16 08:10:24.296033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734336624.317288   45364 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734336624.323644   45364 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-16 08:10:24.346344: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> print(tf.__version__)\r\n2.18.0\r\n>>> import matplotlib\r\n>>> import tensorflow.python.keras.models\r\n>>> from tensorflow.keras.optimizers import Adam\r\n>>> from tensorflow import keras\r\n>>> matplotlib.use(\'agg\')\r\n>>> import os\r\n>>> os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # Suppress TensorFlow logs\r\n>>> from tensorflow.python import keras\r\n>>> from tensorflow.keras import backend as K\r\n>>> from tensorflow.keras.layers import (\r\n...     Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,\r\n...     Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec,\r\n... LayerNormalization,Layer, Dense, Dropout\r\n... )\r\n>>> \r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 16, 8, 12, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545298309, 'issue_id': 2725244964, 'author': 'Chuan1937', 'body': ""I know the code is correct. But the problem is that it will have error report and it's can't popup keras or other api function.\r\nI have defined some deep learning model and run successfully on windows and ubuntu.\r\n\r\n![ 2024-12-16 18-56-58](https://github.com/user-attachments/assets/cb2d9cf4-3288-429b-9210-7c1c6c65ac38)\r\n\r\n![ 2024-12-16 19-01-12](https://github.com/user-attachments/assets/508f4468-ff83-43fa-9d97-42f2ac064a15)"", 'created_at': datetime.datetime(2024, 12, 16, 11, 2, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545303628, 'issue_id': 2725244964, 'author': 'Chuan1937', 'body': 'Thank you for your reply, I hope you can answer my questions', 'created_at': datetime.datetime(2024, 12, 16, 11, 4, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558884642, 'issue_id': 2725244964, 'author': 'Venkat6871', 'body': 'Hi **@Chuan1937** ,\r\nApologies for the delay, and thank you for your patience. Could you please try importing Keras directly instead of importing it from TensorFlow? For example:\r\n`import keras`\r\ninstead of\r\n`from tensorflow import keras`\r\nLet us know if you are still facing the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 4, 27, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566069755, 'issue_id': 2725244964, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 31, 2, 1, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250452, 'issue_id': 2725244964, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250502, 'issue_id': 2725244964, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82465"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82465"">No</a>', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 25, tzinfo=datetime.timezone.utc)}]","cuixue on (2024-12-08 13:37:12 UTC): 

Venkat6871 (Assginee) on (2024-12-10 07:03:45 UTC): Hi **@Chuan1937** ,
Could you please provide details about the error you are facing? I tried running your code on Colab using TensorFlow 2.18.0, and it worked fine for me.
Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b4532cb16caf48e07ddcb75f1c84e5f2/82465_tf_2-18-0-v.ipynb) here for reference.
Thank you!

Chuan1937 (Issue Creator) on (2024-12-11 16:12:05 UTC): The code is correct, my problem is that on Ubuntu, using the above method to import tensorflow functions will result in an error: there is no such function.Although there may be errors, the code can run normally. I don't know why.

Venkat6871 (Assginee) on (2024-12-16 08:12:48 UTC): Hi **@Chuan1937** ,
Apologies for the delay, and thank you for your patience. I tried running your code on Ubuntu using TensorFlow version 2.18.0, and it is working fine for me. I have attached it below, please check it once. Let me know if I did anything wrong here.
```
(tf) (tf) maayara@venkat-gpu1:~$ python3
Python 3.9.20 (main, Oct  3 2024, 07:27:41) 
[GCC 11.2.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
2024-12-16 08:10:24.296033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734336624.317288   45364 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734336624.323644   45364 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-16 08:10:24.346344: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2.18.0
...     Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,
...     Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec,
... LayerNormalization,Layer, Dense, Dropout
... )
```
Thank you!

Chuan1937 (Issue Creator) on (2024-12-16 11:02:43 UTC): I know the code is correct. But the problem is that it will have error report and it's can't popup keras or other api function.
I have defined some deep learning model and run successfully on windows and ubuntu.

![ 2024-12-16 18-56-58](https://github.com/user-attachments/assets/cb2d9cf4-3288-429b-9210-7c1c6c65ac38)

![ 2024-12-16 19-01-12](https://github.com/user-attachments/assets/508f4468-ff83-43fa-9d97-42f2ac064a15)

Chuan1937 (Issue Creator) on (2024-12-16 11:04:27 UTC): Thank you for your reply, I hope you can answer my questions

Venkat6871 (Assginee) on (2024-12-23 04:27:25 UTC): Hi **@Chuan1937** ,
Apologies for the delay, and thank you for your patience. Could you please try importing Keras directly instead of importing it from TensorFlow? For example:
`import keras`
instead of
`from tensorflow import keras`
Let us know if you are still facing the issue.
Thank you!

github-actions[bot] on (2024-12-31 02:01:01 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-07 02:02:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-07 02:02:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82465"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82465"">No</a>

"
2724943694,issue,closed,completed,"Model in Android Studio app, written in Java. Unexpected failure when preparing tensor allocations","Hi, i have problem in my Java app in Android studio, main issue is when model is used. Can someone help?


### 1. System information

- OS Platform: Windows 10
- TensorFlow library: TensorFlow version: 2.12.0

### Reference colab notebooks

https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb

### 2. Code

    private void performObjectDetection() {
        Log.d(""ObjectDetection"", ""Starting object detection process..."");
        String tfliteVersion = TensorFlowLite.runtimeVersion();
        Log.d(""TensorFlowLite"", ""TensorFlow Lite Runtime Version: "" + tfliteVersion);

        if (videoFeedView.getBitmap() != null) {
            Bitmap frameBitmap = videoFeedView.getBitmap();

            Log.d(""ObjectDetection"", ""Frame captured for processing: "" +
                    frameBitmap.getWidth() + ""x"" + frameBitmap.getHeight());

            try {
                // Preprocess the frame
                float[] inputTensor = preprocessFrame(frameBitmap);
                Log.d(""ObjectDetection"", ""Input tensor prepared. Shape: [1, 640, 640, 3]"");
                Log.d(""ObjectDetection"", ""Input tensor size: "" + inputTensor.length);
                Log.d(""ObjectDetection"", ""First 10 tensor values: "" +
                        inputTensor[0] + "", "" + inputTensor[1] + "", "" + inputTensor[2] + ""..."");

                // Initialize output tensor matching the model's output shape
                float[][][] outputTensor = new float[1][5][8400];

                // Allocate tensors and run inference
                try {
                    tfliteInterpreter.allocateTensors(); // Ensure tensors are allocated
                    Log.d(""ObjectDetection"", ""Tensors allocated successfully."");
                    tfliteInterpreter.run(inputTensor, outputTensor);
                    Log.d(""ObjectDetection"", ""Inference completed."");
                } catch (Exception e) {
                    Log.e(""ObjectDetection"", ""Error during inference: "", e);
                    return;
                }

                // Process the model output
                processOutput(outputTensor);

            } catch (Exception e) {
                Log.e(""ObjectDetection"", ""Error during preprocessing or detection: "", e);
            }
        } else {
            Log.w(""ObjectDetection"", ""No frame available for processing."");
        }
    }

    private void processOutput(float[][][] outputTensor) {
        Log.d(""ObjectDetection"", ""Processing output tensor..."");

        // Iterate through detections
        for (int i = 0; i < outputTensor[0][4].length; i++) {
            float confidence = outputTensor[0][4][i]; // Confidence score

            if (confidence > 0.2) { // Adjust confidence threshold as needed
                float xCenter = outputTensor[0][0][i];
                float yCenter = outputTensor[0][1][i];
                float width = outputTensor[0][2][i];
                float height = outputTensor[0][3][i];

                Log.d(""ObjectDetection"", ""Detection "" + i + "": Confidence="" + confidence);

                // Convert YOLO format (x_center, y_center, width, height) to bounding box
                float left = xCenter - width / 2;
                float top = yCenter - height / 2;
                float right = xCenter + width / 2;
                float bottom = yCenter + height / 2;

                Log.d(""ObjectDetection"", ""Bounding box - Left: "" + left + "", Top: "" + top +
                        "", Right: "" + right + "", Bottom: "" + bottom);

                // Update detection overlay (implement this method to draw bounding boxes)
                updateDetectionOverlay(left, top, right, bottom, ""Detected Object"");
            }
        }
    }

    private float[] preprocessFrame(Bitmap frameBitmap) {
        int modelInputSize = 640;

        // Resize the frame to 640x640 (no padding)
        Bitmap resizedBitmap = Bitmap.createScaledBitmap(frameBitmap, modelInputSize, modelInputSize, true);

        // Create a tensor for the input
        float[] inputTensor = new float[modelInputSize * modelInputSize * 3];
        int[] pixelValues = new int[modelInputSize * modelInputSize];
        resizedBitmap.getPixels(pixelValues, 0, modelInputSize, 0, 0, modelInputSize, modelInputSize);

        for (int i = 0; i < pixelValues.length; i++) {
            int pixel = pixelValues[i];

            // Normalize RGB values to [0, 1]
            inputTensor[i * 3] = ((pixel >> 16) & 0xFF) / 255.0f; // Red
            inputTensor[i * 3 + 1] = ((pixel >> 8) & 0xFF) / 255.0f; // Green
            inputTensor[i * 3 + 2] = (pixel & 0xFF) / 255.0f; // Blue
        }
        return inputTensor;
    }

    private void updateDetectionOverlay(float left, float top, float right, float bottom, String label) {
        detectionOverlayView.setBoundingBox(left, top, right, bottom, label);
    }

    private void clearDetectionOverlay() {
        detectionOverlayView.clearBoundingBox();
    }

    private MappedByteBuffer loadModelFile(Context context, String modelPath) throws IOException {
        AssetFileDescriptor fileDescriptor = context.getAssets().openFd(modelPath);
        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

### 3. Logs from Logcat in Android Studio
2024-12-08 05:05:05.366  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Attempting to load TensorFlow Lite model...
2024-12-08 05:05:05.398  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  TensorFlow Lite model loaded successfully.
2024-12-08 05:05:05.401  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Input shape: [1, 640, 640, 3]
2024-12-08 05:05:05.401  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Output shape: [1, 5, 8400]
2024-12-08 05:05:05.457  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Initialization complete.
2024-12-08 05:05:11.316  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Starting object detection...
2024-12-08 05:05:11.322  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Starting object detection process...
2024-12-08 05:05:11.369  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Frame captured for processing: 2292x1080
2024-12-08 05:05:11.422  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Input tensor prepared. Shape: [1, 640, 640, 3]
2024-12-08 05:05:11.422  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Input tensor size: 1228800
2024-12-08 05:05:11.422  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  First 10 tensor values: 0.0, 0.0, 0.0...
2024-12-08 05:05:11.422  9739-9739  ObjectDetection         com.dji.sdk.sample                   D  Tensors allocated successfully.
2024-12-08 05:05:11.427  9739-9739  ObjectDetection         com.dji.sdk.sample                   E  Error during inference: 
                                                                                                    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)
                                                                                                    Node number 0 (PAD) failed to prepare.
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(:308)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.run(:248)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(:101)
                                                                                                    	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(:77)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.run(:94)
                                                                                                    	at org.tensorflow.lite.Interpreter.run(:77)
                                                                                                    	at com.dji.sdk.sample.internal.view.FullScreenVideoViewZPI.performObjectDetection(:521)
                                                                                                    	at com.dji.sdk.sample.internal.view.FullScreenVideoViewZPI.access$900(:56)
                                                                                                    	at com.dji.sdk.sample.internal.view.FullScreenVideoViewZPI$5.run(:480)
                                                                                                    	at android.os.Handler.handleCallback(Handler.java:942)
                                                                                                    	at android.os.Handler.dispatchMessage(Handler.java:99)
                                                                                                    	at android.os.Looper.loopOnce(Looper.java:204)
                                                                                                    	at android.os.Looper.loop(Looper.java:291)
                                                                                                    	at android.app.ActivityThread.main(ActivityThread.java:8134)
                                                                                                    	at java.lang.reflect.Method.invoke(Native Method)
                                                                                                    	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:588)
                                                                                                    	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1019)
",susiecamazotz,2024-12-08 04:19:25+00:00,['gaikwadrahul8'],2024-12-26 02:00:54+00:00,2024-12-26 02:00:53+00:00,https://github.com/tensorflow/tensorflow/issues/82464,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2531795099, 'issue_id': 2724943694, 'author': 'gaikwadrahul8', 'body': ""Hi, @susiecamazotz \r\nI apologize for the delayed response, I see you're using object detection model so I would suggest you to please have a look into this [TensorFlow Lite Object Detection Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) example which may resolve your issue and to run that demo example successfully you'll have to change this line `distributionUrl=https\\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected\r\n\r\nHere is working demo example of object detection screenshot for reference :\r\n\r\n![image](https://github.com/user-attachments/assets/728563ed-262b-4a8f-8b20-116f9ebb78bc)\r\n\r\n\r\nIf your issue still persists please let us know with updated error log for further investigation, if possible could you please help us with your Github repo along with your model to replicate the same behaviour from our end ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 10, 14, 29, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550129604, 'issue_id': 2724943694, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 18, 2, 4, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562073754, 'issue_id': 2724943694, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 26, 2, 0, 52, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-10 14:29:31 UTC): Hi, @susiecamazotz 
I apologize for the delayed response, I see you're using object detection model so I would suggest you to please have a look into this [TensorFlow Lite Object Detection Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) example which may resolve your issue and to run that demo example successfully you'll have to change this line `distributionUrl=https\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected

Here is working demo example of object detection screenshot for reference :

![image](https://github.com/user-attachments/assets/728563ed-262b-4a8f-8b20-116f9ebb78bc)


If your issue still persists please let us know with updated error log for further investigation, if possible could you please help us with your Github repo along with your model to replicate the same behaviour from our end ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-18 02:04:43 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-26 02:00:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2724598337,issue,closed,completed,Dirichlet noise returns Nan under jit_compile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

No

### OS platform and distribution

WSL2

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Function return some Nan values if jit_compile=True

### Standalone code to reproduce the issue

```shell
import tensorflow as tf  # tf.__version__ '2.14.1'
import tensorflow_probability as tfp  # tfp.__version__ '0.22.1'


# @tf.function(jit_compile=True)  # uncomment to trigger bug
def test_exploration_noise(
    prnd: tf.Tensor,
    legal_actions: tf.Tensor,
):
    action_space_size = 9
    batch_size = 2048
    root_dirichlet_alpha = 0.35

    dir_alpha = tf.fill(
            [batch_size, action_space_size], root_dirichlet_alpha
        )
    distribution = tfp.distributions.Dirichlet(dir_alpha * legal_actions)
    noise = distribution.sample(seed=prnd)
    return noise

if __name__ == ""__main__"":
    seed = tf.constant([ 348206018, 1455008836], dtype=tf.int32)
    new_legal_actions = tf.ones([2048, 9]).numpy()
    new_legal_actions[1212] = [1, 0, 0, 0, 0, 0, 1, 1, 0]  # using line 288 won't trigger bug
    new_legal_actions = tf.convert_to_tensor(new_legal_actions)
    noise = test_exploration_noise(
            # root_dirichlet_alpha=config.root_dirichlet_alpha,
            prnd=seed,
            legal_actions=new_legal_actions,
    )
    print(noise[1212].numpy())
    tf.debugging.assert_all_finite(noise, ""invalid noise"")
```


### Relevant log output

```shell
without jit:
[0.00337291 0.         0.         0.         0.         0. 0.94923943 0.04738774 0.        ]

with jit:
[nan nan nan nan nan nan nan nan nan]
```
",cmarlin,2024-12-07 13:33:18+00:00,['Venkat6871'],2024-12-26 02:00:57+00:00,2024-12-26 02:00:54+00:00,https://github.com/tensorflow/tensorflow/issues/82461,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2525117278, 'issue_id': 2724598337, 'author': 'cmarlin', 'body': ""I couldn't run with nightly"", 'created_at': datetime.datetime(2024, 12, 7, 13, 33, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530537983, 'issue_id': 2724598337, 'author': 'Venkat6871', 'body': 'Hi **@cmarlin** ,\r\nApologies for the delay, and thank you for raising your concern. The issue arises because you are multiplying `dir_alpha` by the `legal_action` tensor. If `legal_action` has a value of 0 for a specific action, the corresponding concentration parameter in the Dirichlet distribution becomes 0. This is invalid because the Dirichlet distribution requires all concentration parameters to be strictly positive. Additionally, this issue may occur when using JIT compilation, as XLA might handle zeros differently.\r\nTo resolve this, I added a small epsilon value to avoid zeros, and it worked fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3b920adf58bf3e078bb7b95fb5fe7857/82461_tf_2-18-0-nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 10, 6, 13, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550129630, 'issue_id': 2724598337, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 18, 2, 4, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562073780, 'issue_id': 2724598337, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 26, 2, 0, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562073816, 'issue_id': 2724598337, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82461"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82461"">No</a>', 'created_at': datetime.datetime(2024, 12, 26, 2, 0, 56, tzinfo=datetime.timezone.utc)}]","cmarlin (Issue Creator) on (2024-12-07 13:33:51 UTC): I couldn't run with nightly

Venkat6871 (Assginee) on (2024-12-10 06:13:15 UTC): Hi **@cmarlin** ,
Apologies for the delay, and thank you for raising your concern. The issue arises because you are multiplying `dir_alpha` by the `legal_action` tensor. If `legal_action` has a value of 0 for a specific action, the corresponding concentration parameter in the Dirichlet distribution becomes 0. This is invalid because the Dirichlet distribution requires all concentration parameters to be strictly positive. Additionally, this issue may occur when using JIT compilation, as XLA might handle zeros differently.
To resolve this, I added a small epsilon value to avoid zeros, and it worked fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3b920adf58bf3e078bb7b95fb5fe7857/82461_tf_2-18-0-nightly-v.ipynb) here for reference.

Thank you!

github-actions[bot] on (2024-12-18 02:04:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-26 02:00:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-26 02:00:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82461"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82461"">No</a>

"
2722478099,issue,closed,completed,Kernel crash in optimizer.apply_gradient for complex-valued gradients,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The issue was originally posted as Keras issue keras-team/keras#20581. They decided that this is a TF bug and not coming from Keras:

TensorFlow is able to correctly compute gradients for complex-valued variables. However, the Keras3 optimizers do not seem to be able to correctly apply complex-valued gradients. This worked with Keras 2.

Here is a code snippet that works in TF2.15, but leads to a Kernel crash with Keras 3.7 and TF 2.18 on a GPU.
The crash is caused by the function `optimizer.apply_gradients`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Complex-valued variable (leading to a Kernel crash)
x = tf.Variable(tf.complex(3., 2.), trainable=True)
optimizer = tf.keras.optimizers.SGD()
with tf.GradientTape() as tape:
    loss = tf.abs(x)**2
grads = tape.gradient(loss, tape.watched_variables())
optimizer.apply_gradients(zip(grads, tape.watched_variables()))
print(x)

# Real-valued variable equivalent
x_r = tf.Variable(3., trainable=True)
x_i = tf.Variable(2., trainable=True)
optimizer = tf.keras.optimizers.SGD()
with tf.GradientTape() as tape:
    x = tf.complex(x_r, x_i)
    loss = tf.abs(x)**2
grads = tape.gradient(loss, tape.watched_variables())
optimizer.apply_gradients(zip(grads, tape.watched_variables()))
print(tf.complex(x_r, x_i))
```


### Relevant log output

_No response_",jhoydis,2024-12-06 09:06:49+00:00,['Venkat6871'],2024-12-26 02:01:00+00:00,2024-12-26 02:00:55+00:00,https://github.com/tensorflow/tensorflow/issues/82385,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2528134677, 'issue_id': 2722478099, 'author': 'jhoydis', 'body': 'Hi, \r\n\r\nI am slightly confused. this is exactly the code that I already provided above as ""real-valued"" equivalent. It unfortunately does not solve the issue. The complex-valued variant worked prior to TF 2.17 and now it does not work anymore.', 'created_at': datetime.datetime(2024, 12, 9, 14, 36, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530579611, 'issue_id': 2722478099, 'author': 'Venkat6871', 'body': 'Hi **@jhoydis** ,\r\nApologies for the delay, and thank you for raising your concern. With TensorFlow 2.18.0, Keras 3.x is used by default, which might be cause for the issue. I manually installed Keras 2, and it worked fine for me. To install Keras 2, use the following command:\r\n```\r\n!pip install tf-keras\r\nimport tf_keras as keras\r\n```\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/d8b0105db34a3a5c17acdc46ad603083/82385_tf-2-18-0-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 10, 6, 44, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550129645, 'issue_id': 2722478099, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 18, 2, 4, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562073803, 'issue_id': 2722478099, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 26, 2, 0, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562073863, 'issue_id': 2722478099, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82385"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82385"">No</a>', 'created_at': datetime.datetime(2024, 12, 26, 2, 0, 59, tzinfo=datetime.timezone.utc)}]","jhoydis (Issue Creator) on (2024-12-09 14:36:37 UTC): Hi, 

I am slightly confused. this is exactly the code that I already provided above as ""real-valued"" equivalent. It unfortunately does not solve the issue. The complex-valued variant worked prior to TF 2.17 and now it does not work anymore.

Venkat6871 (Assginee) on (2024-12-10 06:44:18 UTC): Hi **@jhoydis** ,
Apologies for the delay, and thank you for raising your concern. With TensorFlow 2.18.0, Keras 3.x is used by default, which might be cause for the issue. I manually installed Keras 2, and it worked fine for me. To install Keras 2, use the following command:
```
!pip install tf-keras
import tf_keras as keras
```
Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/d8b0105db34a3a5c17acdc46ad603083/82385_tf-2-18-0-v.ipynb) here for reference.
Thank you!

github-actions[bot] on (2024-12-18 02:04:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-26 02:00:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-26 02:00:59 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82385"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82385"">No</a>

"
2721980257,issue,open,,"The warning ""The structure of `inputs` doesn't match the expected structure"" when training a functional model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)

### Custom code

Yes

### OS platform and distribution

Windows 11 23H2 22631.4460

### Mobile device

Windows 11 23H2 22631.4460

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the model is functional, not Sequential, the warning has occured:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*
  warnings.warn(
```

Yes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 10))
  warnings.warn(msg)
```

After the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.

I've traced the source and found that in `Lib\site-packages\keras\src\tree\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:

```
>>> a
<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>
>>> b
[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]
>>> a_structure
PyTreeSpec(*, NoneIsLeaf)
>>> b_structure
PyTreeSpec([*], NoneIsLeaf)
```

The data passed to the `fit` function fully corresponds to the [documentation](https://keras.io/api/models/model_training_apis/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.



### Standalone code to reproduce the issue

```shell
from keras.models import Model
from keras.layers import Dense, Input, Flatten, Concatenate
from keras import utils
import numpy as np
import tensorflow as tf

class SamplesSet(utils.PyDataset):
    
    def __init__(self, batch_size, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))
        y = np.arange(self.batch_size)
        return x1, y
    
train = SamplesSet(100)
x1_train = np.random.uniform(size=10*100).reshape((100, 10))
y_train = np.arange(100)

input1 = Input(shape=(10,))
l1 = Dense(1)(input1)
d2 = Dense(1, activation='sigmoid')(l1)
model = Model(inputs=[input1], outputs=[d2])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(x1_train, y_train, epochs=5, verbose=1)
# In the all cases below warning occures too
# history = Model.fit(train, epochs=5, verbose=1) 
# ret = model.predict(np.arange(10)[np.newaxis,:])
# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))
```


### Relevant log output

_No response_",Alouettesu,2024-12-06 03:47:02+00:00,['tilakrayal'],2025-02-03 21:09:57+00:00,,https://github.com/tensorflow/tensorflow/issues/82372,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2527106268, 'issue_id': 2721980257, 'author': 'Alouettesu', 'body': ""@yuvashrikarunakaran please pay attention this line:\r\n\r\n    model = Model(inputs=[input1], outputs=[d2])\r\n\r\nin your code was turned into:\r\n\r\n    model = Model(inputs=input1, outputs=d2)\r\n\r\nI use lists because this problem had originally came from multi-input model, I've just generalized this model as simple as possible. If we came back to lists, the warning will occure again."", 'created_at': datetime.datetime(2024, 12, 9, 7, 5, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530137865, 'issue_id': 2721980257, 'author': 'Alouettesu', 'body': ""Accordind the [documentation](https://keras.io/api/models/model/):\r\n\r\n> Note: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).\r\n\r\nBut documentations doesn't explain which type in which case should be used. I found that when I use a tuple instead of list, the problem disappears:\r\n\r\n```\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Dense, Input, Flatten, Concatenate\r\nimport numpy as np\r\n\r\nx1_train = np.arange(10*100).reshape((100, 10)).astype(np.float32)\r\nx2_train = np.arange(5*100).reshape((100, 5)).astype(np.float32)\r\ny_train = np.arange(100).astype(np.float32)\r\n\r\ninput1 = Input(shape=(10,), name='keras_tensor')\r\nl1 = Dense(1)(input1)\r\n\r\ninput2 = Input(shape=(5,))\r\nl2 = Dense(1)(input2)\r\n\r\nconc = Concatenate(axis=1)([l1, l2])\r\nfl = Flatten()(conc)\r\nd2 = Dense(1, activation='sigmoid')(fl)\r\nmodel = Model(inputs=(input1, input2), outputs=(d2,))\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = model.fit([x1_train, x2_train], y_train, epochs=5, verbose=1)\r\n```\r\n\r\nBut when I saved the model and loaded it back, the problem occures again:\r\n\r\n```\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Dense, Input, Flatten, Concatenate\r\nimport numpy as np\r\n\r\nx1_train = np.arange(10*100).reshape((100, 10)).astype(np.float32)\r\nx2_train = np.arange(5*100).reshape((100, 5)).astype(np.float32)\r\ny_train = np.arange(100).astype(np.float32)\r\n\r\ninput1 = Input(shape=(10,), name='keras_tensor')\r\nl1 = Dense(1)(input1)\r\n\r\ninput2 = Input(shape=(5,))\r\nl2 = Dense(1)(input2)\r\n\r\nconc = Concatenate(axis=1)([l1, l2])\r\nfl = Flatten()(conc)\r\nd2 = Dense(1, activation='sigmoid')(fl)\r\nmodel = Model(inputs=(input1, input2), outputs=(d2,))\r\n\r\nmodel.save('model.keras')\r\nmodel = load_model('model.keras')\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = model.fit([x1_train, x2_train], y_train, epochs=5, verbose=1)\r\n```\r\n\r\n---\r\n\r\n**Update**\r\n\r\nOn Nightly, after loading model back, there is no problem."", 'created_at': datetime.datetime(2024, 12, 10, 3, 3, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531350143, 'issue_id': 2721980257, 'author': 'RabJon', 'body': ""> @yuvashrikarunakaran please pay attention this line:\r\n> \r\n> ```\r\n> model = Model(inputs=[input1], outputs=[d2])\r\n> ```\r\n> \r\n> in your code was turned into:\r\n> \r\n> ```\r\n> model = Model(inputs=input1, outputs=d2)\r\n> ```\r\n> \r\n> I use lists because this problem had originally came from multi-input model, I've just generalized this model as simple as possible. If we came back to lists, the warning will occure again.\r\n\r\nThank you @Alouettesu, I had the same warning and your suggested fix silenced it for me!"", 'created_at': datetime.datetime(2024, 12, 10, 11, 47, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535228960, 'issue_id': 2721980257, 'author': 'tilakrayal', 'body': ""@Alouettesu,\r\nI tried to execute the mentioned code on tensorflow v2.17 and haven't observed the mentioned warning. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3896b7dfc3895f11824fa6d280d0c7d2/untitled2271.ipynb) and provide if my understanding is different. Thank you!"", 'created_at': datetime.datetime(2024, 12, 11, 8, 57, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537693467, 'issue_id': 2721980257, 'author': 'Alouettesu', 'body': ""@tilakrayal,\r\nI don't know anything about what environment Colab is using, but the warning has occured on my local machine on version 2.13 (Windows 11), version 2.19.0-dev (Nightly, Windows 11) and version 2.18 (WSL). Please get me know if another information about my environment is necessary."", 'created_at': datetime.datetime(2024, 12, 12, 3, 4, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579719732, 'issue_id': 2721980257, 'author': 'tpaul-synaptics', 'body': 'I got the same warning in tf 2.18', 'created_at': datetime.datetime(2025, 1, 9, 10, 21, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587379809, 'issue_id': 2721980257, 'author': 'fegemo', 'body': 'Also happening to me in 2.18, Windows + WSL2, cuda 12.7, driver 566.14', 'created_at': datetime.datetime(2025, 1, 13, 15, 12, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632082730, 'issue_id': 2721980257, 'author': 'Masumekeshavarzi', 'body': ""I am getting the same warning in Tensorflow 2.18 and Cuda 2.12, and the model doesn't start training after this warning."", 'created_at': datetime.datetime(2025, 2, 3, 21, 9, 55, tzinfo=datetime.timezone.utc)}]","Alouettesu (Issue Creator) on (2024-12-09 07:05:42 UTC): @yuvashrikarunakaran please pay attention this line:

    model = Model(inputs=[input1], outputs=[d2])

in your code was turned into:

    model = Model(inputs=input1, outputs=d2)

I use lists because this problem had originally came from multi-input model, I've just generalized this model as simple as possible. If we came back to lists, the warning will occure again.

Alouettesu (Issue Creator) on (2024-12-10 03:03:13 UTC): Accordind the [documentation](https://keras.io/api/models/model/):


But documentations doesn't explain which type in which case should be used. I found that when I use a tuple instead of list, the problem disappears:

```
from keras.models import Model, load_model
from keras.layers import Dense, Input, Flatten, Concatenate
import numpy as np

x1_train = np.arange(10*100).reshape((100, 10)).astype(np.float32)
x2_train = np.arange(5*100).reshape((100, 5)).astype(np.float32)
y_train = np.arange(100).astype(np.float32)

input1 = Input(shape=(10,), name='keras_tensor')
l1 = Dense(1)(input1)

input2 = Input(shape=(5,))
l2 = Dense(1)(input2)

conc = Concatenate(axis=1)([l1, l2])
fl = Flatten()(conc)
d2 = Dense(1, activation='sigmoid')(fl)
model = Model(inputs=(input1, input2), outputs=(d2,))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit([x1_train, x2_train], y_train, epochs=5, verbose=1)
```

But when I saved the model and loaded it back, the problem occures again:

```
from keras.models import Model, load_model
from keras.layers import Dense, Input, Flatten, Concatenate
import numpy as np

x1_train = np.arange(10*100).reshape((100, 10)).astype(np.float32)
x2_train = np.arange(5*100).reshape((100, 5)).astype(np.float32)
y_train = np.arange(100).astype(np.float32)

input1 = Input(shape=(10,), name='keras_tensor')
l1 = Dense(1)(input1)

input2 = Input(shape=(5,))
l2 = Dense(1)(input2)

conc = Concatenate(axis=1)([l1, l2])
fl = Flatten()(conc)
d2 = Dense(1, activation='sigmoid')(fl)
model = Model(inputs=(input1, input2), outputs=(d2,))

model.save('model.keras')
model = load_model('model.keras')

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit([x1_train, x2_train], y_train, epochs=5, verbose=1)
```

---

**Update**

On Nightly, after loading model back, there is no problem.

RabJon on (2024-12-10 11:47:52 UTC): Thank you @Alouettesu, I had the same warning and your suggested fix silenced it for me!

tilakrayal (Assginee) on (2024-12-11 08:57:05 UTC): @Alouettesu,
I tried to execute the mentioned code on tensorflow v2.17 and haven't observed the mentioned warning. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3896b7dfc3895f11824fa6d280d0c7d2/untitled2271.ipynb) and provide if my understanding is different. Thank you!

Alouettesu (Issue Creator) on (2024-12-12 03:04:03 UTC): @tilakrayal,
I don't know anything about what environment Colab is using, but the warning has occured on my local machine on version 2.13 (Windows 11), version 2.19.0-dev (Nightly, Windows 11) and version 2.18 (WSL). Please get me know if another information about my environment is necessary.

tpaul-synaptics on (2025-01-09 10:21:17 UTC): I got the same warning in tf 2.18

fegemo on (2025-01-13 15:12:31 UTC): Also happening to me in 2.18, Windows + WSL2, cuda 12.7, driver 566.14

Masumekeshavarzi on (2025-02-03 21:09:55 UTC): I am getting the same warning in Tensorflow 2.18 and Cuda 2.12, and the model doesn't start training after this warning.

"
2721957174,issue,closed,completed,Convolution and Batch Normalization Layers Fused During TFLite Conversion,"I am working on a research project where I need the convolution layer and batch normalization layer to remain as separate operations after converting my model to TensorFlow Lite (TFLite). However, during the conversion process, these layers are being fused for optimization purposes, and I cannot find a way to prevent this.

Is there an option or method in TensorFlow or TFLite to ensure these layers are preserved separately? Any guidance or suggestions would be greatly appreciated.

System Information:

OS Platform: Windows 11
TensorFlow version: 2.13.0
Installation method: pip
Thank you!",Mohamed-Hazem-Abdo,2024-12-06 03:23:13+00:00,['gaikwadrahul8'],2024-12-22 18:31:06+00:00,2024-12-22 18:31:06+00:00,https://github.com/tensorflow/tensorflow/issues/82371,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2538760911, 'issue_id': 2721957174, 'author': 'gaikwadrahul8', 'body': ""Hi, @Mohamed-Hazem-Abdo\r\nI apologize for the delayed response, As far I know the fusion of convolution and batch normalization layers during TFLite conversion is a common optimization technique. It's designed to improve model performance by reducing the number of operations and memory footprint. However, this fusion can hinder specific research needs such as analyzing the individual contributions of these layers.\r\n\r\nAs far I know there is no direct flag to prevent this fusion during TFLite conversion to prevent the fusion of convolution and batch normalization layers in TFLite. You'll need to modify the TFLite converter's source code to exclude the fusion of specific layers. This requires a deep understanding of the TensorFlow Lite framework and C++ programming. Please refer this similar issue https://github.com/tensorflow/tensorflow/issues/35176 which may help you\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 12, 12, 23, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546280839, 'issue_id': 2721957174, 'author': 'Mohamed-Hazem-Abdo', 'body': 'Hi, @gaikwadrahul8,\r\n\r\nThanks for the response. This solution does work ! I also want to know if there is a way to disable graph optimization during the conversion process.\r\n\r\nLooking forward to your input!', 'created_at': datetime.datetime(2024, 12, 16, 17, 54, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546356076, 'issue_id': 2721957174, 'author': 'gaikwadrahul8', 'body': 'Hi, @Mohamed-Hazem-Abdo \r\nAs far I know in python use the optimizations flag in the TFLiteConverter and ensure that the model is converted without optimization during TensorFlow lite conversion process so you can do something like below so please refer this official documentation [tf.lite.TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) and [tf.lite.Optimize](https://www.tensorflow.org/api_docs/python/tf/lite/Optimize)\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(""path_to_saved_model"")\r\n\r\n# Disable all optimizations\r\nconverter.optimizations = []\r\n\r\ntflite_model = converter.convert()\r\nwith open(""model_no_fusion.tflite"", ""wb"") as f:\r\n    f.write(tflite_model)\r\n\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 16, 18, 32, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558549778, 'issue_id': 2721957174, 'author': 'Mohamed-Hazem-Abdo', 'body': ""hello,@gaikwadrahul8\r\nThanks a lot for your help. this solution doesn't work.only building from the source worked for me. it would be really helpful if some kind of flag is added to turn off graph optimization in the future."", 'created_at': datetime.datetime(2024, 12, 22, 18, 31, 6, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-12 12:23:31 UTC): Hi, @Mohamed-Hazem-Abdo
I apologize for the delayed response, As far I know the fusion of convolution and batch normalization layers during TFLite conversion is a common optimization technique. It's designed to improve model performance by reducing the number of operations and memory footprint. However, this fusion can hinder specific research needs such as analyzing the individual contributions of these layers.

As far I know there is no direct flag to prevent this fusion during TFLite conversion to prevent the fusion of convolution and batch normalization layers in TFLite. You'll need to modify the TFLite converter's source code to exclude the fusion of specific layers. This requires a deep understanding of the TensorFlow Lite framework and C++ programming. Please refer this similar issue https://github.com/tensorflow/tensorflow/issues/35176 which may help you

Thank you for your cooperation and patience.

Mohamed-Hazem-Abdo (Issue Creator) on (2024-12-16 17:54:36 UTC): Hi, @gaikwadrahul8,

Thanks for the response. This solution does work ! I also want to know if there is a way to disable graph optimization during the conversion process.

Looking forward to your input!

gaikwadrahul8 (Assginee) on (2024-12-16 18:32:49 UTC): Hi, @Mohamed-Hazem-Abdo 
As far I know in python use the optimizations flag in the TFLiteConverter and ensure that the model is converted without optimization during TensorFlow lite conversion process so you can do something like below so please refer this official documentation [tf.lite.TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) and [tf.lite.Optimize](https://www.tensorflow.org/api_docs/python/tf/lite/Optimize)

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(""path_to_saved_model"")

# Disable all optimizations
converter.optimizations = []

tflite_model = converter.convert()
with open(""model_no_fusion.tflite"", ""wb"") as f:
    f.write(tflite_model)

```

Thank you for your cooperation and patience.

Mohamed-Hazem-Abdo (Issue Creator) on (2024-12-22 18:31:06 UTC): hello,@gaikwadrahul8
Thanks a lot for your help. this solution doesn't work.only building from the source worked for me. it would be really helpful if some kind of flag is added to turn off graph optimization in the future.

"
2721715034,issue,open,,TensorFlow source code compilation error,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.12.1

### Bazel version

7.3.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My CPU supports AVX512F, I want to build it from source to support my CPU instruction set

### Standalone code to reproduce the issue

```shell
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
```


### Relevant log output

```shell
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
```
",sharkcpt,2024-12-06 00:02:00+00:00,['tilakrayal'],2024-12-27 08:19:39+00:00,,https://github.com/tensorflow/tensorflow/issues/82361,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2521804829, 'issue_id': 2721715034, 'author': 'sharkcpt', 'body': 'Since I successfully compiled this version about tensorflow-2.16.0-cp311-cp311-linux_x86_64.whl , after I have never successfully compiled it again.  Is very sad.', 'created_at': datetime.datetime(2024, 12, 6, 0, 23, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525385403, 'issue_id': 2721715034, 'author': 'sharkcpt', 'body': 'Still incorrect. This is a hint.\r\n\r\nproblem 1\r\n\r\nERROR: Skipping \'//tensorflow/tools/pip_package:build_pip_package\': no such target \'//tensorflow/tools/pip_package:build_pip_package\': target \'build_pip_package\' not declared in package \'tensorflow/tools/pip_package\' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean \'build_pip_package.py\'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target \'//tensorflow/tools/pip_package:build_pip_package\': target \'build_pip_package\' not declared in package \'tensorflow/tools/pip_package\' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean \'build_pip_package.py\'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)\r\nINFO: Elapsed time: 0.062s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nThis is the command what I entered.\r\n\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\nproblem 2.\r\n\r\nwhen  i  was run python3 configure.py  after  i can\'t see any about this \r\n""Additional CPU optimizations like AVX512?""  Tips\r\n\r\nThere must be something wrong with your source code. The path is wrong.', 'created_at': datetime.datetime(2024, 12, 8, 2, 24, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527659077, 'issue_id': 2721715034, 'author': 'Mohamed-Hazem-Abdo', 'body': 'I am also trying to build from source \r\ntry \r\nbazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\r\ni think also bazel version is important could you try with bazel 6.5. \r\ncheck the docs also\r\nhttps://www.tensorflow.org/install/source', 'created_at': datetime.datetime(2024, 12, 9, 11, 28, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533911457, 'issue_id': 2721715034, 'author': 'tilakrayal', 'body': '@sharkcpt,\r\nTensorflow 2.18 is not compatible with the Bazel 7.3.1. Could you please try to install with the compatible versions. Also take a look at the tested build configurations for the [reference](https://www.tensorflow.org/install/source#linux). Thank you!', 'created_at': datetime.datetime(2024, 12, 11, 7, 14, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544858145, 'issue_id': 2721715034, 'author': 'Xephier102', 'body': ""I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, ass-backwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 3-4 days just trying to make this thing function.. That's literally all I've done. \r\n\r\nAll I see everywhere I've gone 'bazel-bin' But this damned thing doesn't create a bazel-bin symlink or folder.. It creates bazel-out and bazel-tensorflow, but no bazel-bin. I just has the epiphany just now to rename the bazel-out to bazel-bin, and adjusted this line to \r\n`` bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg``\r\nadded .py and also went into the folder with that .py file and made the damn thing executable with chmod +x build_pip_package.py cuz it still wouldn't run unless I did. Upon running it, I got the strangest damn thing I ever seen.\r\n\r\n```\r\n``[]  bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 25: Tool to rearrange files and build the wheel.\r\n\r\nIn a nutshell this script does:\r\n1) Takes lists of paths to .h/.py/.so/etc files.\r\n2) Creates a temporary directory.\r\n3) Copies files from #1 to #2 with some exceptions and corrections.\r\n4) A wheel is created from the files in the temp directory.\r\n\r\nMost of the corrections are related to tsl/xla vendoring:\r\nThese files used to be a part of source code but were moved to an external repo.\r\nTo not break the TF API, we pretend that it's still part of the it.\r\n: No such file or directory\r\nimport: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.\r\nimport: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.\r\nimport:  `subprocess' @ error/import.c/ImportImageCommand/1289.\r\nimport: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.\r\nimport: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.\r\nimport:  `sys' @ error/import.c/ImportImageCommand/1289.\r\nimport: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.\r\nimport: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.\r\nimport:  `tempfile' @ error/import.c/ImportImageCommand/1289.\r\nfrom: too many arguments\r\nfrom: too many arguments\r\nfrom: too many arguments\r\nfrom: too many arguments\r\nfrom: too many arguments\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: syntax error near unexpected token `('\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: `def parse_args() -> argparse.Namespace:' ``\r\n```\r\n\r\nIt ran just fine, sorta.. It froze my display and turned my cursor into a crosshair, and when I clicked, my screen unfroze. I had to click several times just to get to the end of that before it un-hyjacked my display. After which, I looked in the folder with the tensorflow folder that I cloned from github, it had turned 8 files in that folder into screenshots.. \r\n![Screenshot_20241216_024823](https://github.com/user-attachments/assets/8691f2f1-82cb-4344-b371-2d8c9464db6a)\r\n\r\nDon't get me wrong, I don't mean to get mad at anyone here(or look like I am), I'm just agitated in general from this whole experience. I mean, how hard does it have to be to flip a few switches on optimization flags. In all honesty, when it comes right down to it, I'm sure this will just boil down to outdated dependencies used by new programs, or permissions nonsense of somesort. It's ALWAYS one of those two things with Linux.\r\n\r\n> I am also trying to build from source try bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu i think also bazel version is important could you try with bazel 6.5. check the docs also https://www.tensorflow.org/install/source\r\n\r\nYea... I looked at that, and I'm about to have an aneurysm/heart attack just trying to process that.. especially due to all the 'apt-get' this and that. I'm on Garuda-Linux. I don't apt, I pacman, and tried getting python3-dev via pacman and nothing, tried yay, even tried flatpak, no python3-dev.. \r\n\r\nWell, I'm gonna go catatonic now while I sulk over my fail of the day.."", 'created_at': datetime.datetime(2024, 12, 16, 8, 3, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549364088, 'issue_id': 2721715034, 'author': 'mihaimaruseac', 'body': ""So, the build targets have changed since 2.16. Now, you basically need to make sure that\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\r\n```\r\n\r\nsucceeds. It might fail if you don't have some dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them).\r\n\r\nIf the above passes, then you get a CPU-only wheel, but at least from this point onwards there won't be many more changes needed to get a GPU one too"", 'created_at': datetime.datetime(2024, 12, 17, 19, 0, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558778593, 'issue_id': 2721715034, 'author': 'Xephier102', 'body': ""> So, the build targets have changed since 2.16. Now, you basically need to make sure that\r\n> \r\n> ```\r\n> bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\r\n> ```\r\n> \r\n> succeeds. It might fail if you don't have some dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them).\r\n> \r\n> If the above passes, then you get a CPU-only wheel, but at least from this point onwards there won't be many more changes needed to get a GPU one too\r\n\r\nWell, I'm in Garuda. Not sure how that would differ in terms of dependencies. As for tensorflow. The idea was to add the CPU optimizations to the GPU tensorflow. Since I'm not filthy stinking rich and only have a 16gb card, I use offloading where I can at all when running more intensive tasks like Flux generation or lora training. So, I figure it'd be handy to have CPU optimizations to maximize the efficiency when offloading."", 'created_at': datetime.datetime(2024, 12, 23, 2, 26, 19, tzinfo=datetime.timezone.utc)}]","sharkcpt (Issue Creator) on (2024-12-06 00:23:57 UTC): Since I successfully compiled this version about tensorflow-2.16.0-cp311-cp311-linux_x86_64.whl , after I have never successfully compiled it again.  Is very sad.

sharkcpt (Issue Creator) on (2024-12-08 02:24:38 UTC): Still incorrect. This is a hint.

problem 1

ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
INFO: Elapsed time: 0.062s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

This is the command what I entered.

bazel build //tensorflow/tools/pip_package:build_pip_package

problem 2.

when  i  was run python3 configure.py  after  i can't see any about this 
""Additional CPU optimizations like AVX512?""  Tips

There must be something wrong with your source code. The path is wrong.

Mohamed-Hazem-Abdo on (2024-12-09 11:28:23 UTC): I am also trying to build from source 
try 
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu
i think also bazel version is important could you try with bazel 6.5. 
check the docs also
https://www.tensorflow.org/install/source

tilakrayal (Assginee) on (2024-12-11 07:14:09 UTC): @sharkcpt,
Tensorflow 2.18 is not compatible with the Bazel 7.3.1. Could you please try to install with the compatible versions. Also take a look at the tested build configurations for the [reference](https://www.tensorflow.org/install/source#linux). Thank you!

Xephier102 on (2024-12-16 08:03:24 UTC): I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, ass-backwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 3-4 days just trying to make this thing function.. That's literally all I've done. 

All I see everywhere I've gone 'bazel-bin' But this damned thing doesn't create a bazel-bin symlink or folder.. It creates bazel-out and bazel-tensorflow, but no bazel-bin. I just has the epiphany just now to rename the bazel-out to bazel-bin, and adjusted this line to 
`` bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg``
added .py and also went into the folder with that .py file and made the damn thing executable with chmod +x build_pip_package.py cuz it still wouldn't run unless I did. Upon running it, I got the strangest damn thing I ever seen.

```
``[]  bazel-bin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 25: Tool to rearrange files and build the wheel.

In a nutshell this script does:
1) Takes lists of paths to .h/.py/.so/etc files.
2) Creates a temporary directory.
3) Copies files from #1 to #2 with some exceptions and corrections.
4) A wheel is created from the files in the temp directory.

Most of the corrections are related to tsl/xla vendoring:
These files used to be a part of source code but were moved to an external repo.
To not break the TF API, we pretend that it's still part of the it.
: No such file or directory
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `subprocess' @ error/import.c/ImportImageCommand/1289.
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `sys' @ error/import.c/ImportImageCommand/1289.
import: unable to grab mouse '': Resource temporarily unavailable @ error/xwindow.c/XSelectWindow/9351.
import: unable to read X window image '': Success @ error/xwindow.c/XImportImage/4961.
import:  `tempfile' @ error/import.c/ImportImageCommand/1289.
from: too many arguments
from: too many arguments
from: too many arguments
from: too many arguments
from: too many arguments
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: syntax error near unexpected token `('
bazel-bin/tensorflow/tools/pip_package/build_pip_package.py: line 43: `def parse_args() -> argparse.Namespace:' ``
```

It ran just fine, sorta.. It froze my display and turned my cursor into a crosshair, and when I clicked, my screen unfroze. I had to click several times just to get to the end of that before it un-hyjacked my display. After which, I looked in the folder with the tensorflow folder that I cloned from github, it had turned 8 files in that folder into screenshots.. 
![Screenshot_20241216_024823](https://github.com/user-attachments/assets/8691f2f1-82cb-4344-b371-2d8c9464db6a)

Don't get me wrong, I don't mean to get mad at anyone here(or look like I am), I'm just agitated in general from this whole experience. I mean, how hard does it have to be to flip a few switches on optimization flags. In all honesty, when it comes right down to it, I'm sure this will just boil down to outdated dependencies used by new programs, or permissions nonsense of somesort. It's ALWAYS one of those two things with Linux.


Yea... I looked at that, and I'm about to have an aneurysm/heart attack just trying to process that.. especially due to all the 'apt-get' this and that. I'm on Garuda-Linux. I don't apt, I pacman, and tried getting python3-dev via pacman and nothing, tried yay, even tried flatpak, no python3-dev.. 

Well, I'm gonna go catatonic now while I sulk over my fail of the day..

mihaimaruseac on (2024-12-17 19:00:28 UTC): So, the build targets have changed since 2.16. Now, you basically need to make sure that

```
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu
```

succeeds. It might fail if you don't have some dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them).

If the above passes, then you get a CPU-only wheel, but at least from this point onwards there won't be many more changes needed to get a GPU one too

Xephier102 on (2024-12-23 02:26:19 UTC): Well, I'm in Garuda. Not sure how that would differ in terms of dependencies. As for tensorflow. The idea was to add the CPU optimizations to the GPU tensorflow. Since I'm not filthy stinking rich and only have a 16gb card, I use offloading where I can at all when running more intensive tasks like Flux generation or lora training. So, I figure it'd be handy to have CPU optimizations to maximize the efficiency when offloading.

"
2721528831,issue,closed,completed,GPU delegate error for Flex models in Android C++ level,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello. I am trying to use GPU delegate on Android C++ level for model that need Flex support. On runtime an error is returned on ModifyGraphWithDelegate. Please, tell if Flex + GPU is possible for Android C++? What corrections should I do to proceed? 

### Standalone code to reproduce the issue

```shell
auto* delegate = TfLiteGpuDelegateV2Create(nullptr);
int res = interpreter->ModifyGraphWithDelegate(delegate)
```


### Relevant log output

```shell
""res"" variable is returned as kTfLiteApplicationError = 3 value
```
",koranten2,2024-12-05 22:05:10+00:00,['gaikwadrahul8'],2025-01-01 02:06:18+00:00,2025-01-01 02:06:16+00:00,https://github.com/tensorflow/tensorflow/issues/82349,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2531482644, 'issue_id': 2721528831, 'author': 'koranten2', 'body': 'Thank you for reply! Do you mean that I need to initialize flex delegate directly, like auto* flex_delegate = tflite::FlexDelegate::Create();? As I could see from documentations only linking is needed for Android. And, please, tell your code is for android or running on android is impossible due to flex unsupporting?', 'created_at': datetime.datetime(2024, 12, 10, 12, 19, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531510579, 'issue_id': 2721528831, 'author': 'koranten2', 'body': 'I used similar code as yours previously on android but not successfully. So my question is it possible to use Flex + GPU on android at all/', 'created_at': datetime.datetime(2024, 12, 10, 12, 31, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546626297, 'issue_id': 2721528831, 'author': 'gaikwadrahul8', 'body': ""Hi, @koranten2\r\nI apologize for the delayed response, As far I know Flex operations (TensorFlow operations that are not natively supported by TensorFlow Lite) can't be run on the GPU delegate. Specifically, when you try to apply the GPU delegate and Flex delegate together the operations required by Flex are not supported by the GPU delegate leading to the error.\r\n\r\nThe GPU delegate in TensorFlow Lite requires the operations to be offloaded to the GPU and it has specific kernels implemented for operations like Conv2D, DepthwiseConv2D, Add, etc. However, Flex operations may involve more complex or custom operations that either dont have GPU kernels or require CPU-based execution. This results in the error you're facing when you attempt to apply both delegates. Please refer official documentation for supported operations of [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu#quantized_models)\r\n\r\nYou'll need to work around this limitation either by splitting the model using CPU for Flex ops or preprocessing the model to remove Flex operations.The best course of action for your use case would be to separate operations that require Flex from those that can run on the GPU. Use the GPU delegate only for supported ops and run Flex operations on the CPU which can be slower. \r\n\r\nIf I have missed something here please let me know. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 16, 20, 9, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560533899, 'issue_id': 2721528831, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 24, 2, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804155, 'issue_id': 2721528831, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566804174, 'issue_id': 2721528831, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82349"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82349"">No</a>', 'created_at': datetime.datetime(2025, 1, 1, 2, 6, 17, tzinfo=datetime.timezone.utc)}]","koranten2 (Issue Creator) on (2024-12-10 12:19:07 UTC): Thank you for reply! Do you mean that I need to initialize flex delegate directly, like auto* flex_delegate = tflite::FlexDelegate::Create();? As I could see from documentations only linking is needed for Android. And, please, tell your code is for android or running on android is impossible due to flex unsupporting?

koranten2 (Issue Creator) on (2024-12-10 12:31:39 UTC): I used similar code as yours previously on android but not successfully. So my question is it possible to use Flex + GPU on android at all/

gaikwadrahul8 (Assginee) on (2024-12-16 20:09:18 UTC): Hi, @koranten2
I apologize for the delayed response, As far I know Flex operations (TensorFlow operations that are not natively supported by TensorFlow Lite) can't be run on the GPU delegate. Specifically, when you try to apply the GPU delegate and Flex delegate together the operations required by Flex are not supported by the GPU delegate leading to the error.

The GPU delegate in TensorFlow Lite requires the operations to be offloaded to the GPU and it has specific kernels implemented for operations like Conv2D, DepthwiseConv2D, Add, etc. However, Flex operations may involve more complex or custom operations that either dont have GPU kernels or require CPU-based execution. This results in the error you're facing when you attempt to apply both delegates. Please refer official documentation for supported operations of [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu#quantized_models)

You'll need to work around this limitation either by splitting the model using CPU for Flex ops or preprocessing the model to remove Flex operations.The best course of action for your use case would be to separate operations that require Flex from those that can run on the GPU. Use the GPU delegate only for supported ops and run Flex operations on the CPU which can be slower. 

If I have missed something here please let me know. 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-24 02:01:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-01 02:06:15 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-01 02:06:17 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82349"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82349"">No</a>

"
2720495929,issue,open,,[XLA] TF XLA outputs abnormal value when compiling `Embedding`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
",shaoyuyoung,2024-12-05 13:58:40+00:00,['tilakrayal'],2024-12-20 06:09:05+00:00,,https://github.com/tensorflow/tensorflow/issues/82317,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:xla', 'XLA'), ('TF 2.18', '')]","[{'comment_id': 2524958947, 'issue_id': 2720495929, 'author': 'shaoyuyoung', 'body': ""hi, @yuvashrikarunakaran, thank you for your kind words and detailed explanation. \n\nActually, in my situation, I just want to discuss the `edge case` (**input_dim=1**). I know it is an `illegal input`. \n\nHowever, WITH and WITHOUT xla behave differently when processing illegal input. Does this reflect a problem with the **robustness** of XLA? Because without xla, the model can return 0 (this is easily understood). I think the abnormal value (in my situation, it is negative) returned by XLA is **Silent Error** ? As the models become more complex, the consequences can be catastrophic. (e.g., by poisoning the model to make wrong decisions).\n\nFinally, I think it's important to align the behavior of the uncompiled and compiled models. I know the community is busy. But we should acknowledge that this is a robustness problem which may cause potential security problem. :)"", 'created_at': datetime.datetime(2024, 12, 7, 6, 7, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533498356, 'issue_id': 2720495929, 'author': 'shaoyuyoung', 'body': 'hi, any updates here?\r\nFeel free to discuss if it is helpful to address this issue. :)', 'created_at': datetime.datetime(2024, 12, 11, 2, 40, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535126140, 'issue_id': 2720495929, 'author': 'tilakrayal', 'body': '@shaoyuyoung,\r\nI was able to reproduce the issue on tensorflow v2.17, v2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3ca252045169f21d7ba1321f3e82e248/untitled2270.ipynb). Please allow to deepdive into the issue and find the root cause for the same. Thank you!', 'created_at': datetime.datetime(2024, 12, 11, 8, 41, 39, tzinfo=datetime.timezone.utc)}]","shaoyuyoung (Issue Creator) on (2024-12-07 06:07:20 UTC): hi, @yuvashrikarunakaran, thank you for your kind words and detailed explanation. 

Actually, in my situation, I just want to discuss the `edge case` (**input_dim=1**). I know it is an `illegal input`. 

However, WITH and WITHOUT xla behave differently when processing illegal input. Does this reflect a problem with the **robustness** of XLA? Because without xla, the model can return 0 (this is easily understood). I think the abnormal value (in my situation, it is negative) returned by XLA is **Silent Error** ? As the models become more complex, the consequences can be catastrophic. (e.g., by poisoning the model to make wrong decisions).

Finally, I think it's important to align the behavior of the uncompiled and compiled models. I know the community is busy. But we should acknowledge that this is a robustness problem which may cause potential security problem. :)

shaoyuyoung (Issue Creator) on (2024-12-11 02:40:28 UTC): hi, any updates here?
Feel free to discuss if it is helpful to address this issue. :)

tilakrayal (Assginee) on (2024-12-11 08:41:39 UTC): @shaoyuyoung,
I was able to reproduce the issue on tensorflow v2.17, v2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3ca252045169f21d7ba1321f3e82e248/untitled2270.ipynb). Please allow to deepdive into the issue and find the root cause for the same. Thank you!

"
2720398284,issue,closed,completed,`TimeDistributed.call()` does not work correctly with `Masking`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Mac OS Sequoia 15.1.1

### Mobile device

Macbook Pro

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to use a `Masking` layer to deal with variable-sized sequences being fed to an LSTM layer. Using `TimeDistributed` with `Masking` gives the following error:

```
ValueError: Exception encountered when calling TimeDistributed.call().

`TimeDistributed` Layer should be passed a `mask` of shape (None, None, ...), received: mask.shape=(None, 8)

Arguments received by TimeDistributed.call():
   inputs=tf.Tensor(shape=(None, None, 8), dtype=float32)
   training=True
   mask=tf.Tensor(shape=(None, 8), dtype=bool)
```

Tensorflow version: 2.16.1
Tensorflow-metal version: 1.1.0
Keras version: 3.0.0


### Standalone code to reproduce the issue

```shell
from keras.layers import Masking, Input, LSTM, Dense, TimeDistributed
from keras.models import Sequential
from keras.utils import pad_sequences

# Create sequences
length = 8
n_batch = length
n_neurons = length
seq = [round(i/float(length), 2) for i in range(length)]

# Outputs same as inputs, just as an example
X = [seq, [xx * 2 for xx in seq[:4]]]
y = [seq, [yy * 2 for yy in seq[:4]]]

# Pad the sequences to be of the same length
maxlen = max([len(xx) for xx in X])
padded_X = pad_sequences(
    X, dtype='float', maxlen=maxlen, padding='post', value=-1
)

padded_y = pad_sequences(
    y, dtype='float', maxlen=maxlen, padding='post', value=-1
)

# Make a sample LSTM model
model = Sequential()
model.add(Input(shape=(length, 1)))
model.add(Masking(mask_value=-1.))
model.add(LSTM(n_neurons, return_sequences=True))
model.add(TimeDistributed(Dense(1)))
model.summary()

# Fit the model
model.compile(loss='mean_squared_error', optimizer='adam')
history = model.fit(padded_X, padded_y, epochs=5, verbose=1)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 35
     33 # Fit the model
     34 model.compile(loss='mean_squared_error', optimizer='adam')
---> 35 history = model.fit(padded_X, padded_y, epochs=500, verbose=1) #, batch_size=2)

File ~/venvs/sat-etl-tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:123, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    120     filtered_tb = _process_traceback_frames(e.__traceback__)
    121     # To get the full stack trace, call:
    122     # `keras.config.disable_traceback_filtering()`
--> 123     raise e.with_traceback(filtered_tb) from None
    124 finally:
    125     del filtered_tb

File ~/venvs/sat-etl-tf/lib/python3.11/site-packages/keras/src/layers/rnn/time_distributed.py:81, in TimeDistributed.call(self, inputs, training, mask)
     78 timesteps = input_shape[1]
     80 if mask_shape is not None and mask_shape[:2] != (batch_size, timesteps):
---> 81     raise ValueError(
     82         ""`TimeDistributed` Layer should be passed a `mask` of shape ""
     83         f""({batch_size}, {timesteps}, ...), ""
     84         f""received: mask.shape={mask_shape}""
     85     )
     87 def time_distributed_transpose(data):
     88     """"""Swaps the timestep and batch dimensions of a tensor.""""""

ValueError: Exception encountered when calling TimeDistributed.call().

`TimeDistributed` Layer should be passed a `mask` of shape (None, None, ...), received: mask.shape=(None, 8)

Arguments received by TimeDistributed.call():
   inputs=tf.Tensor(shape=(None, None, 8), dtype=float32)
   training=True
   mask=tf.Tensor(shape=(None, 8), dtype=bool)
```
",atharva-kelkar,2024-12-05 13:18:48+00:00,['Venkat6871'],2024-12-06 14:56:35+00:00,2024-12-06 14:39:58+00:00,https://github.com/tensorflow/tensorflow/issues/82316,"[('type:bug', 'Bug')]","[{'comment_id': 2522265808, 'issue_id': 2720398284, 'author': 'prempraneethkota', 'body': 'The Masking layer in Keras is designed to work with 3D input tensors that have the shape (batch_size, timesteps, features). This is because the Masking layer is typically used in the context of sequence models like RNNs, LSTMs, and GRUs, which process data sequentially over time. As your input is missing at least one dimension this is expected.', 'created_at': datetime.datetime(2024, 12, 6, 6, 49, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522302709, 'issue_id': 2720398284, 'author': 'atharva-kelkar', 'body': 'My input to the masking layer is 3D, see the model summary below:\r\n\r\n```Model: ""sequential""\r\n\r\n Layer (type)                     Output Shape                  Param # \r\n\r\n masking (Masking)                (None, 8, 1)                        0 \r\n\r\n lstm (LSTM)                      (None, 8, 8)                      320 \r\n\r\n time_distributed                 (None, 8, 1)                        9 \r\n (TimeDistributed)                                                      \r\n\r\n Total params: 329 (1.29 KB)\r\n Trainable params: 329 (1.29 KB)\r\n Non-trainable params: 0 (0.00 B)\r\n```\r\n\r\n`None` is the placeholder for the batchsize dimension, 8 is the number of time steps, and 1 is the feature dimension.', 'created_at': datetime.datetime(2024, 12, 6, 7, 19, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522348694, 'issue_id': 2720398284, 'author': 'prempraneethkota', 'body': 'I see, Let me check.', 'created_at': datetime.datetime(2024, 12, 6, 7, 35, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522496554, 'issue_id': 2720398284, 'author': 'prempraneethkota', 'body': ""Try this : \r\n```python \r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Masking, Input, LSTM, Dense, TimeDistributed\r\nfrom keras.src.models.sequential import Sequential\r\nfrom keras._tf_keras.keras.utils import pad_sequences\r\n\r\n# Create sequences\r\nlength = 8\r\nn_batch = length\r\nn_neurons = length\r\nseq = [round(i / float(length), 2) for i in range(length)]\r\n\r\n# Outputs same as inputs, just as an example\r\nX = [seq, [xx * 2 for xx in seq[:4]]]\r\ny = [seq, [yy * 2 for yy in seq[:4]]]\r\n\r\n# Pad the sequences to be of the same length\r\nmaxlen = max([len(xx) for xx in X])\r\npadded_X = pad_sequences(X, dtype='float', maxlen=maxlen, padding='post', value=-1)\r\npadded_y = pad_sequences(y, dtype='float', maxlen=maxlen, padding='post', value=-1)\r\n\r\n# Expand dimensions to have shape (batch_size, timesteps, features)\r\npadded_X = np.expand_dims(padded_X, axis=-1)\r\npadded_y = np.expand_dims(padded_y, axis=-1)\r\n\r\n# Ensure Eager Execution is enabled\r\ntf.config.run_functions_eagerly(True)\r\n\r\n# Make a sample LSTM model\r\nmodel = Sequential()\r\nmodel.add(Input(shape=(maxlen, 1)))  # Adjust the input shape\r\nmodel.add(Masking(mask_value=-1.))\r\nmodel.add(LSTM(n_neurons, return_sequences=True))\r\nmodel.add(TimeDistributed(Dense(1)))\r\nmodel.summary()\r\n\r\n# Compile the model\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\n# Train the model\r\nhistory = model.fit(padded_X, padded_y, epochs=5, verbose=1)\r\n```"", 'created_at': datetime.datetime(2024, 12, 6, 8, 26, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522510031, 'issue_id': 2720398284, 'author': 'prempraneethkota', 'body': '![image](https://github.com/user-attachments/assets/2fcaa7c4-5ea4-4561-9b2f-263e82f3200e)', 'created_at': datetime.datetime(2024, 12, 6, 8, 34, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523397858, 'issue_id': 2720398284, 'author': 'atharva-kelkar', 'body': 'Thanks! Explicitly switching on eager execution in this line did the trick: `tf.config.run_functions_eagerly(True)`', 'created_at': datetime.datetime(2024, 12, 6, 14, 39, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523397935, 'issue_id': 2720398284, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82316"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82316"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 14, 40, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523431403, 'issue_id': 2720398284, 'author': 'prempraneethkota', 'body': 'Really felt so nice helping you out :)', 'created_at': datetime.datetime(2024, 12, 6, 14, 56, 33, tzinfo=datetime.timezone.utc)}]","prempraneethkota on (2024-12-06 06:49:58 UTC): The Masking layer in Keras is designed to work with 3D input tensors that have the shape (batch_size, timesteps, features). This is because the Masking layer is typically used in the context of sequence models like RNNs, LSTMs, and GRUs, which process data sequentially over time. As your input is missing at least one dimension this is expected.

atharva-kelkar (Issue Creator) on (2024-12-06 07:19:18 UTC): My input to the masking layer is 3D, see the model summary below:

```Model: ""sequential""

 Layer (type)                     Output Shape                  Param # 

 masking (Masking)                (None, 8, 1)                        0 

 lstm (LSTM)                      (None, 8, 8)                      320 

 time_distributed                 (None, 8, 1)                        9 
 (TimeDistributed)                                                      

 Total params: 329 (1.29 KB)
 Trainable params: 329 (1.29 KB)
 Non-trainable params: 0 (0.00 B)
```

`None` is the placeholder for the batchsize dimension, 8 is the number of time steps, and 1 is the feature dimension.

prempraneethkota on (2024-12-06 07:35:04 UTC): I see, Let me check.

prempraneethkota on (2024-12-06 08:26:10 UTC): Try this : 
```python 
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Masking, Input, LSTM, Dense, TimeDistributed
from keras.src.models.sequential import Sequential
from keras._tf_keras.keras.utils import pad_sequences

# Create sequences
length = 8
n_batch = length
n_neurons = length
seq = [round(i / float(length), 2) for i in range(length)]

# Outputs same as inputs, just as an example
X = [seq, [xx * 2 for xx in seq[:4]]]
y = [seq, [yy * 2 for yy in seq[:4]]]

# Pad the sequences to be of the same length
maxlen = max([len(xx) for xx in X])
padded_X = pad_sequences(X, dtype='float', maxlen=maxlen, padding='post', value=-1)
padded_y = pad_sequences(y, dtype='float', maxlen=maxlen, padding='post', value=-1)

# Expand dimensions to have shape (batch_size, timesteps, features)
padded_X = np.expand_dims(padded_X, axis=-1)
padded_y = np.expand_dims(padded_y, axis=-1)

# Ensure Eager Execution is enabled
tf.config.run_functions_eagerly(True)

# Make a sample LSTM model
model = Sequential()
model.add(Input(shape=(maxlen, 1)))  # Adjust the input shape
model.add(Masking(mask_value=-1.))
model.add(LSTM(n_neurons, return_sequences=True))
model.add(TimeDistributed(Dense(1)))
model.summary()

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
history = model.fit(padded_X, padded_y, epochs=5, verbose=1)
```

prempraneethkota on (2024-12-06 08:34:44 UTC): ![image](https://github.com/user-attachments/assets/2fcaa7c4-5ea4-4561-9b2f-263e82f3200e)

atharva-kelkar (Issue Creator) on (2024-12-06 14:39:58 UTC): Thanks! Explicitly switching on eager execution in this line did the trick: `tf.config.run_functions_eagerly(True)`

google-ml-butler[bot] on (2024-12-06 14:40:01 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82316"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82316"">No</a>

prempraneethkota on (2024-12-06 14:56:33 UTC): Really felt so nice helping you out :)

"
2719877399,issue,closed,completed,"when i use Anaconda Python 3.12 import tensorflow , ImportError: DLL load failed while importing _pywrap_tensorflow_internal: (DLL)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\cumtz\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: (DLL)

### Standalone code to reproduce the issue

```shell
the issue happende when ""import tensorflow""
```


### Relevant log output

_No response_",boboAAA,2024-12-05 09:38:38+00:00,['tilakrayal'],2024-12-18 17:57:26+00:00,2024-12-18 17:57:21+00:00,https://github.com/tensorflow/tensorflow/issues/82289,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2523219264, 'issue_id': 2719877399, 'author': 'tilakrayal', 'body': '@boboAAA,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 6, 13, 6, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542662709, 'issue_id': 2719877399, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 14, 2, 5, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551948490, 'issue_id': 2719877399, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 57, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551948550, 'issue_id': 2719877399, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82289"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82289"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 57, 23, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-06 13:06:27 UTC): @boboAAA,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

github-actions[bot] on (2024-12-14 02:05:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mihaimaruseac on (2024-12-18 17:57:21 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:57:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82289"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82289"">No</a>

"
2719667595,issue,closed,completed,Model with dropout in MirroredStrategy not work,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.1

### Custom code

No

### OS platform and distribution

ubuntu22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It is impossible to init model with dropout under any MirroredStrategy using tf API.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
STRATEGY = tf.distribute.MirroredStrategy()
with STRATEGY.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation=""gelu""),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(128),
    ])
```
```


### Relevant log output

```shell
2024-12-05 18:17:02.002493: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-05 18:17:02.016573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-05 18:17:02.030709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-05 18:17:02.034759: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-05 18:17:02.046477: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-05 18:17:05.066676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
2024-12-05 18:17:10.507032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22502 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:3b:00.0, compute capability: 8.6
2024-12-05 18:17:10.507896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22502 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:5e:00.0, compute capability: 8.6
/opt/venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 6
      2 STRATEGY = tf.distribute.MirroredStrategy()
      3 with STRATEGY.scope():
      4     model = tf.keras.Sequential([
      5         tf.keras.layers.Dense(128, activation=""gelu"", input_shape=(28, 28, 1)),
----> 6         tf.keras.layers.Dropout(0.2),
      7         tf.keras.layers.Dense(128),
      8     ])

File /opt/venv/lib/python3.10/site-packages/keras/src/layers/regularization/dropout.py:53, in Dropout.__init__(self, rate, noise_shape, seed, **kwargs)
     51 self.noise_shape = noise_shape
     52 if rate > 0:
---> 53     self.seed_generator = backend.random.SeedGenerator(seed)
     54 self.supports_masking = True
     55 self.built = True

File /opt/venv/lib/python3.10/site-packages/keras/src/random/seed_generator.py:75, in SeedGenerator.__init__(self, seed, name, **kwargs)
     72     return self.backend.convert_to_tensor([seed, 0], dtype=dtype)
     74 with self.backend.name_scope(self.name, caller=self):
---> 75     self.state = self.backend.Variable(
     76         seed_initializer,
     77         shape=(2,),
     78         dtype=self.backend.random_seed_dtype(),
     79         trainable=False,
     80         name=""seed_generator_state"",
     81     )

File /opt/venv/lib/python3.10/site-packages/keras/src/backend/common/variables.py:170, in Variable.__init__(self, initializer, shape, dtype, trainable, autocast, aggregation, name)
    168 if callable(initializer):
    169     self._shape = self._validate_shape(shape)
--> 170     self._initialize_with_initializer(initializer)
    171 else:
    172     self._initialize(initializer)

File /opt/venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:47, in Variable._initialize_with_initializer(self, initializer)
     46 def _initialize_with_initializer(self, initializer):
---> 47     self._value = tf.Variable(
     48         lambda: initializer(self._shape, dtype=self._dtype),
     49         dtype=self._dtype,
     50         trainable=self.trainable,
     51         name=self.name,
     52         aggregation=self._map_aggregation(self.aggregation),
     53     )

File /opt/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File /opt/venv/lib/python3.10/site-packages/tensorflow/python/distribute/values.py:513, in DistributedVariable.__init__(self, strategy, values, aggregation, var_policy)
    510 def __init__(self, strategy, values, aggregation, var_policy=None):
    511   if (aggregation == variables_lib.VariableAggregation.MEAN and
    512       not values[0].dtype.is_floating):
--> 513     raise ValueError(
    514         ""creating distributed tf.Variable with aggregation=MEAN and a ""
    515         ""non-floating dtype is not supported, please use a different ""
    516         ""aggregation or dtype"")
    517   self._distribute_strategy = strategy
    518   self._aggregation = aggregation

ValueError: creating distributed tf.Variable with aggregation=MEAN and a non-floating dtype is not supported, please use a different aggregation or dtype
```
",jamm1985,2024-12-05 08:00:52+00:00,['Venkat6871'],2025-01-24 01:59:53+00:00,2025-01-24 01:59:49+00:00,https://github.com/tensorflow/tensorflow/issues/82281,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2519970094, 'issue_id': 2719667595, 'author': 'jamm1985', 'body': 'It seems that the problem is in the keras version: with `3.5.0` everything is fine, but `3.7.0` gives the error', 'created_at': datetime.datetime(2024, 12, 5, 10, 55, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521726106, 'issue_id': 2719667595, 'author': 'billygrahamram', 'body': ""I've been having the same issue after updating my libraries."", 'created_at': datetime.datetime(2024, 12, 5, 23, 38, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527402162, 'issue_id': 2719667595, 'author': 'Venkat6871', 'body': 'Hi **@jamm1985** ,\r\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.17.0 with Keras version 3.5.0, and it worked fine. However, when using version 3.7.0, I encountered the same issue as you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4cacd4192b9716aca81d5e54984198d6/82281_tf-2-17-0-v.ipynb) here for reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 9, 9, 36, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531488173, 'issue_id': 2719667595, 'author': 'jamm1985', 'body': 'Hi @Venkat6871 !\r\nThe Keras team quickly fixed the bug. At this point, it looks like the working version of keras needs to be fixed in the dependencies for the latest tf 2.16, 2.17, 2.18.', 'created_at': datetime.datetime(2024, 12, 10, 12, 21, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538195893, 'issue_id': 2719667595, 'author': 'Venkat6871', 'body': 'Hi **@jamm1985** ,\r\nI tried running your code on Colab using TensorFlow 2.17.0 with Python 3.7.0, and it is working fine. The issue might be update in the upcoming versions. For your reference, please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/73890cee650887a22ab128b2ff36a8f1/82281_tf-2-17-0-v.ipynb) here.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 12, 8, 37, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538425597, 'issue_id': 2719667595, 'author': 'jamm1985', 'body': '> For your reference, please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/73890cee650887a22ab128b2ff36a8f1/82281_tf-2-17-0-v.ipynb) here. \r\n\r\nThis is because the keras dev version. Note that the dev version is not usually installed by default as a requirement for tf.', 'created_at': datetime.datetime(2024, 12, 12, 10, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547611243, 'issue_id': 2719667595, 'author': 'Venkat6871', 'body': 'Hi @jamm1985,\r\nYeah, as mentioned dev version is not installed by default. If dev version is needed then we need to install explicitly for now. As this issue has been resolved in both tensorflow and keras-nightly it will be updated in the upcoming versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 17, 6, 34, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561542873, 'issue_id': 2719667595, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 25, 2, 0, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562281825, 'issue_id': 2719667595, 'author': 'givikomodebadze', 'body': 'class TPUSafeDropout(layers.Layer):\r\n    def __init__(self, rate, **kwargs):\r\n        super(TPUSafeDropout, self).__init__(**kwargs)\r\n        self.rate = rate\r\n\r\n    def call(self, inputs, training=None):\r\n        if not training or self.rate == 0.0:\r\n            return inputs\r\n        noise_shape = tf.shape(inputs)\r\n        random_tensor = tf.random.uniform(noise_shape, dtype=inputs.dtype)\r\n        dropout_mask = tf.cast(random_tensor >= self.rate, inputs.dtype)\r\n        return inputs * dropout_mask / (1.0 - self.rate)', 'created_at': datetime.datetime(2024, 12, 26, 7, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597274865, 'issue_id': 2719667595, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 17, 1, 58, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611383808, 'issue_id': 2719667595, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 24, 1, 59, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611383851, 'issue_id': 2719667595, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82281"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82281"">No</a>', 'created_at': datetime.datetime(2025, 1, 24, 1, 59, 52, tzinfo=datetime.timezone.utc)}]","jamm1985 (Issue Creator) on (2024-12-05 10:55:31 UTC): It seems that the problem is in the keras version: with `3.5.0` everything is fine, but `3.7.0` gives the error

billygrahamram on (2024-12-05 23:38:35 UTC): I've been having the same issue after updating my libraries.

Venkat6871 (Assginee) on (2024-12-09 09:36:05 UTC): Hi **@jamm1985** ,
Apologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.17.0 with Keras version 3.5.0, and it worked fine. However, when using version 3.7.0, I encountered the same issue as you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4cacd4192b9716aca81d5e54984198d6/82281_tf-2-17-0-v.ipynb) here for reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras
Thank you!

jamm1985 (Issue Creator) on (2024-12-10 12:21:27 UTC): Hi @Venkat6871 !
The Keras team quickly fixed the bug. At this point, it looks like the working version of keras needs to be fixed in the dependencies for the latest tf 2.16, 2.17, 2.18.

Venkat6871 (Assginee) on (2024-12-12 08:37:04 UTC): Hi **@jamm1985** ,
I tried running your code on Colab using TensorFlow 2.17.0 with Python 3.7.0, and it is working fine. The issue might be update in the upcoming versions. For your reference, please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/73890cee650887a22ab128b2ff36a8f1/82281_tf-2-17-0-v.ipynb) here.
Thank you!

jamm1985 (Issue Creator) on (2024-12-12 10:02:00 UTC): This is because the keras dev version. Note that the dev version is not usually installed by default as a requirement for tf.

Venkat6871 (Assginee) on (2024-12-17 06:34:04 UTC): Hi @jamm1985,
Yeah, as mentioned dev version is not installed by default. If dev version is needed then we need to install explicitly for now. As this issue has been resolved in both tensorflow and keras-nightly it will be updated in the upcoming versions.
Thank you!

github-actions[bot] on (2024-12-25 02:00:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

givikomodebadze on (2024-12-26 07:58:32 UTC): class TPUSafeDropout(layers.Layer):
    def __init__(self, rate, **kwargs):
        super(TPUSafeDropout, self).__init__(**kwargs)
        self.rate = rate

    def call(self, inputs, training=None):
        if not training or self.rate == 0.0:
            return inputs
        noise_shape = tf.shape(inputs)
        random_tensor = tf.random.uniform(noise_shape, dtype=inputs.dtype)
        dropout_mask = tf.cast(random_tensor >= self.rate, inputs.dtype)
        return inputs * dropout_mask / (1.0 - self.rate)

github-actions[bot] on (2025-01-17 01:58:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-24 01:59:49 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-24 01:59:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82281"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82281"">No</a>

"
2718208153,issue,open,,Potential Remote Code Execution (RCE) Vulnerability in Custom Layers Handling,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

Kali Linux 2024.1

### Python version

Python 3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.2 

### GPU model and memory

NVIDIA GTX 1080

### Current behavior?

Currently, TensorFlow (primarily through Keras) allows users to create custom layers that can contain malicious code. When a model containing these custom layers is loaded or run, the malicious code in the layer can be executed without any restrictions or filtering. This opens up potential exploits because the executed code could potentially damage the system or steal sensitive data. For example, in this report, the saved model contains a MaliciousLayer layer that calls the os.system() system command to execute malicious code when the model is loaded, which could result in corruption or unauthorized data acquisition.



I hope TensorFlow can provide protection that limits or filters the execution of malicious code in registered custom layers. For example, by blocking malicious functions such as os.system() or prohibiting the use of code that can execute external commands when the model is loaded. In addition, the use of custom layers that contain unwanted external code should be protected by restrictions or sandboxing. This will ensure that the loaded model cannot inject malicious code into the user's system, thereby increasing the security of using TensorFlow models.

The main issue here is that TensorFlow (in this case Keras) allows users to define custom layers that can execute malicious code when the model is loaded and used. This is a potential security hole that can be exploited if malicious code is injected into the model.

The challenge is the ability to include custom layers that execute malicious code. TensorFlow/Keras does not filter or restrict the use of malicious code in registered custom layers, which allows exploits like this.

So, the vulnerability is in the implementation of custom layers in TensorFlow/Keras

Who can exploit the vulnerability?

An attacker or malicious individual who gains access to a TensorFlow model saved in the .keras format and has knowledge of how to create malicious custom layers can exploit this vulnerability. Typically, this would involve actors with an understanding of TensorFlow programming or machine learning who can inject harmful code into the model to be executed when the model is loaded.

What do they gain from it?

An attacker who successfully exploits this vulnerability can execute malicious code on the victim's system when the model is loaded. This allows them to steal sensitive personal data, such as login credentials, financial data, or other private information. Additionally, they can leverage remote code execution to gain control of the system or cause further dama

### Standalone code to reproduce the issue

```shell
1.Create a Python file, for example kontol13.py, containing the following code to create a model with a malicious layer (MaliciousLayer):


import tensorflow as tf
from tensorflow.keras.layers import Layer
import os

# Defines a custom layer containing malicious code
@tf.keras.utils.register_keras_serializable()
class MaliciousLayer(Layer):
    def __init__(self, **kwargs):
        super(MaliciousLayer, self).__init__(**kwargs)

    def call(self, inputs):
        # Code that is executed when the model runs
        os.system('echo ""Malicious Code Executed!""')
        return inputs

# Creating a model with dangerous layers
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(10,)),
    tf.keras.layers.Dense(10, activation='relu'),
    MaliciousLayer(),  # Layer berbahaya
    tf.keras.layers.Dense(1)
])

# Save the model in Keras format (.keras)
model.save('kontol13.keras')

print(""Malicious model created and saved as 'kontol13.keras'."")
```
2.Run the above script to create and save the model containing the malicious layers

I also saved the result in google colab:

https://colab.research.google.com/drive/1IwpwNGOeTPUgYu4Y4bAl7WufOXBvOQy9?authuser=1#scrollTo=EYbfvtj6O6wU&line=1&uniqifier=1
```


### Relevant log output

```shell
I run this code on kali linux this is the output

2024-12-04 17:45:53.327622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() are called are written to STDERR
E0000 00:00:1733309153.640400 154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733309153.730398 154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-04 17:45:54.592724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/funsociety/.local/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
 warnings.warn(
2024-12-04 17:46:03.552814: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
Malicious Code Executed!
```
",samr301,2024-12-04 16:08:09+00:00,['tilakrayal'],2024-12-06 20:37:54+00:00,,https://github.com/tensorflow/tensorflow/issues/82214,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.18', '')]","[{'comment_id': 2523213691, 'issue_id': 2718208153, 'author': 'tilakrayal', 'body': '@samr301,\r\nI tried to execute the mentioned code on both tensorflow v2.17, v2.18 and observed that the code was executed and provided the expected output. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/b09e09ff129773dac9417188166a0c57/untitled2266.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 12, 6, 13, 3, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524134267, 'issue_id': 2718208153, 'author': 'samr301', 'body': 'hi thanks for your response it seems i gave the wrong script i apologize\r\n\r\nthis is the correct script\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Layer\r\nimport os\r\nimport subprocess\r\n\r\n# Defines a custom layer containing malicious code\r\n@tf.keras.utils.register_keras_serializable()\r\nclass MaliciousLayer(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MaliciousLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        # Execute malicious code and capture the output\r\n        result = subprocess.run(\'echo ""Malicious Code Executed!""\', shell=True, capture_output=True, text=True)\r\n        print(result.stdout)  # Output the result of the command\r\n        return inputs\r\n\r\n# Creating a model with dangerous layers\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(10,)),\r\n    tf.keras.layers.Dense(10, activation=\'relu\'),\r\n    MaliciousLayer(),  # Dangerous layer\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\n# Save the model in Keras format (.keras)\r\nmodel.save(\'kontol13.keras\')\r\n\r\n# Print message to confirm model is saved\r\nprint(""Malicious model created and saved as \'kontol13.keras\'."")\r\n\r\n# Run the model on some dummy data to see the malicious output\r\nimport numpy as np\r\ndummy_input = np.random.rand(1, 10)  # Dummy input for model\r\nmodel.predict(dummy_input)  # Running prediction to trigger malicious code\r\n```\r\nafter running this script we can see the malicious code executed \r\nThis script more clearly shows the potential risk because it runs a malicious system command (os.system) that can be executed when the model is loaded and run. This can be dangerous if someone loads the model without understanding the inserted code.', 'created_at': datetime.datetime(2024, 12, 6, 20, 37, 52, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-06 13:03:35 UTC): @samr301,
I tried to execute the mentioned code on both tensorflow v2.17, v2.18 and observed that the code was executed and provided the expected output. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/b09e09ff129773dac9417188166a0c57/untitled2266.ipynb). Thank you!

samr301 (Issue Creator) on (2024-12-06 20:37:52 UTC): hi thanks for your response it seems i gave the wrong script i apologize

this is the correct script
```
import tensorflow as tf
from tensorflow.keras.layers import Layer
import os
import subprocess

# Defines a custom layer containing malicious code
@tf.keras.utils.register_keras_serializable()
class MaliciousLayer(Layer):
    def __init__(self, **kwargs):
        super(MaliciousLayer, self).__init__(**kwargs)

    def call(self, inputs):
        # Execute malicious code and capture the output
        result = subprocess.run('echo ""Malicious Code Executed!""', shell=True, capture_output=True, text=True)
        print(result.stdout)  # Output the result of the command
        return inputs

# Creating a model with dangerous layers
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(10,)),
    tf.keras.layers.Dense(10, activation='relu'),
    MaliciousLayer(),  # Dangerous layer
    tf.keras.layers.Dense(1)
])

# Save the model in Keras format (.keras)
model.save('kontol13.keras')

# Print message to confirm model is saved
print(""Malicious model created and saved as 'kontol13.keras'."")

# Run the model on some dummy data to see the malicious output
import numpy as np
dummy_input = np.random.rand(1, 10)  # Dummy input for model
model.predict(dummy_input)  # Running prediction to trigger malicious code
```
after running this script we can see the malicious code executed 
This script more clearly shows the potential risk because it runs a malicious system command (os.system) that can be executed when the model is loaded and run. This can be dangerous if someone loads the model without understanding the inserted code.

"
2718056857,issue,open,,Are checkpoints broken in >= 2.16?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16, 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The example given in https://www.tensorflow.org/guide/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing
```


### Relevant log output

_No response_",IvanUkhov,2024-12-04 15:11:53+00:00,['Venkat6871'],2024-12-13 08:08:44+00:00,,https://github.com/tensorflow/tensorflow/issues/82209,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2527279266, 'issue_id': 2718056857, 'author': 'Venkat6871', 'body': 'Hi **@IvanUkhov** ,\r\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.17.0 and noticed some numerical loss. However, when using TensorFlow 2.15.0, I observed that the loss value was NaN.\r\nCould you please confirm if my approach is correct?\r\nFor your reference, I have shared the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e25040901a6960adc43906d6b29f4c80/82209_tf-2-17-0-2-15-0.ipynb) here.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 9, 8, 42, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527289579, 'issue_id': 2718056857, 'author': 'IvanUkhov', 'body': 'Thank you for looking into this!\r\n\r\nWell, I simply ran the code from this page https://www.tensorflow.org/guide/checkpoint with three different workbenches under Vertex AI: 2.15, 2.16, and 2.17. I did not observe any NaNs when running. Just that the loss was reset to its original value in version 2.16 and 2.17, indicating that the restoration from the checkpoint did not work out.', 'created_at': datetime.datetime(2024, 12, 9, 8, 47, 7, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-09 08:42:05 UTC): Hi **@IvanUkhov** ,
Apologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow 2.17.0 and noticed some numerical loss. However, when using TensorFlow 2.15.0, I observed that the loss value was NaN.
Could you please confirm if my approach is correct?
For your reference, I have shared the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e25040901a6960adc43906d6b29f4c80/82209_tf-2-17-0-2-15-0.ipynb) here.
Thank you!

IvanUkhov (Issue Creator) on (2024-12-09 08:47:07 UTC): Thank you for looking into this!

Well, I simply ran the code from this page https://www.tensorflow.org/guide/checkpoint with three different workbenches under Vertex AI: 2.15, 2.16, and 2.17. I did not observe any NaNs when running. Just that the loss was reset to its original value in version 2.16 and 2.17, indicating that the restoration from the checkpoint did not work out.

"
2718009665,issue,open,,TPU not support TensorFlow 2.18 and 2.17.1,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18 and tf. 2.17.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`import tensorflow as tf` results `segmentation fault core dumped`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_",edwardyehuang,2024-12-04 14:56:53+00:00,['tilakrayal'],2025-01-08 02:33:50+00:00,,https://github.com/tensorflow/tensorflow/issues/82208,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:tpus', 'tpu, tpuestimator'), ('TF 2.18', '')]","[{'comment_id': 2517674175, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': 'lower version (e.g. <= 2.17.0) is fine', 'created_at': datetime.datetime(2024, 12, 4, 14, 57, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517687052, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': '`tpu-vm-tf-2.18.0-pod-pjrt` and `tpu-vm-tf-2.17.1-pod-pjrt` are not worked', 'created_at': datetime.datetime(2024, 12, 4, 15, 2, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517828107, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': 'I just tested `tpu-vm-tf-2.18.0-pod-se` and `tpu-vm-tf-2.18.0-pod-pjrt-v5p-and-below`, and outputs the same error', 'created_at': datetime.datetime(2024, 12, 4, 15, 46, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523188900, 'issue_id': 2718009665, 'author': 'tilakrayal', 'body': ""@edwardyehuang,\r\nHi, As per the guide here, make sure you setup the TPU correctly https://cloud.google.com/tpu/docs/v4-users-guide.\r\nIn your TPU custer resolver, resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='') for the argument tpu, could you try mentioning different values from below and see if that resolves your issue. \r\nAlso I tried to import in the colab tpu which contains TPU v2-8 and not observed any issue/error.\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 12, 6, 12, 54, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523760398, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': ""> @edwardyehuang, Hi, As per the guide here, make sure you setup the TPU correctly https://cloud.google.com/tpu/docs/v4-users-guide. In your TPU custer resolver, resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='') for the argument tpu, could you try mentioning different values from below and see if that resolves your issue. Also I tried to import in the colab tpu which contains TPU v2-8 and not observed any issue/error.\r\n> \r\n> Thank you!\r\n\r\nHi, please avoid using Colab and instead utilize the cloud TPU-VM Pod (e.g., v4-16).\r\n\r\nYou might find that there is no chance to execute `resolver = tf.distribute.cluster_resolver.TPUClusterResolver` because the error will occur at the import statement `import tensorflow as tf`.\r\n\r\nI have used Cloud TPU for a long time and am somewhat familiar with it.\r\n\r\nWhen using a cloud TPU-VM Pod, ensure you select one of the images listed below:\r\n`tpu-vm-tf-2.18.0-pod-pjrt`\r\n`tpu-vm-tf-2.18.0-pod-se`\r\n`tpu-vm-tf-2.18.0-pod-pjrt-v5p-and-below`\r\n`tpu-vm-tf-2.17.1-pod-pjrt`\r\n`tpu-vm-tf-2.17.1-pod-se`"", 'created_at': datetime.datetime(2024, 12, 6, 17, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568993638, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': 'Any update? @tilakrayal', 'created_at': datetime.datetime(2025, 1, 3, 10, 17, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576585669, 'issue_id': 2718009665, 'author': 'edwardyehuang', 'body': '@tilakrayal \r\n\r\nBelow is more detail output\r\n\r\n```\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007187bc013800 (most recent call first):\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 16 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 14 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 14 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 14 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 33 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/context_managers.py"", line 19 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/__init__.py"", line 17 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/ag_ctx.py"", line 21 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 8 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py"", line 49 in <module>\r\n  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed\r\n  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load\r\n  File ""<stdin>"", line 1 in <module>\r\n\r\nExtension modules: numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, google.protobuf.pyext._message, google3.net.proto2.python.internal.cpp._message (total: 5)\r\nSegmentation fault (core dumped)\r\n\r\n```', 'created_at': datetime.datetime(2025, 1, 8, 2, 33, 49, tzinfo=datetime.timezone.utc)}]","edwardyehuang (Issue Creator) on (2024-12-04 14:57:17 UTC): lower version (e.g. <= 2.17.0) is fine

edwardyehuang (Issue Creator) on (2024-12-04 15:02:18 UTC): `tpu-vm-tf-2.18.0-pod-pjrt` and `tpu-vm-tf-2.17.1-pod-pjrt` are not worked

edwardyehuang (Issue Creator) on (2024-12-04 15:46:11 UTC): I just tested `tpu-vm-tf-2.18.0-pod-se` and `tpu-vm-tf-2.18.0-pod-pjrt-v5p-and-below`, and outputs the same error

tilakrayal (Assginee) on (2024-12-06 12:54:10 UTC): @edwardyehuang,
Hi, As per the guide here, make sure you setup the TPU correctly https://cloud.google.com/tpu/docs/v4-users-guide.
In your TPU custer resolver, resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='') for the argument tpu, could you try mentioning different values from below and see if that resolves your issue. 
Also I tried to import in the colab tpu which contains TPU v2-8 and not observed any issue/error.

Thank you!

edwardyehuang (Issue Creator) on (2024-12-06 17:08:00 UTC): Hi, please avoid using Colab and instead utilize the cloud TPU-VM Pod (e.g., v4-16).

You might find that there is no chance to execute `resolver = tf.distribute.cluster_resolver.TPUClusterResolver` because the error will occur at the import statement `import tensorflow as tf`.

I have used Cloud TPU for a long time and am somewhat familiar with it.

When using a cloud TPU-VM Pod, ensure you select one of the images listed below:
`tpu-vm-tf-2.18.0-pod-pjrt`
`tpu-vm-tf-2.18.0-pod-se`
`tpu-vm-tf-2.18.0-pod-pjrt-v5p-and-below`
`tpu-vm-tf-2.17.1-pod-pjrt`
`tpu-vm-tf-2.17.1-pod-se`

edwardyehuang (Issue Creator) on (2025-01-03 10:17:10 UTC): Any update? @tilakrayal

edwardyehuang (Issue Creator) on (2025-01-08 02:33:49 UTC): @tilakrayal 

Below is more detail output

```
Fatal Python error: Segmentation fault

Current thread 0x00007187bc013800 (most recent call first):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 16 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 14 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 14 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 14 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 33 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/context_managers.py"", line 19 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/__init__.py"", line 17 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/ag_ctx.py"", line 21 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 8 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1078 in _handle_fromlist
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py"", line 49 in <module>
  File ""<frozen importlib._bootstrap>"", line 241 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 883 in exec_module
  File ""<frozen importlib._bootstrap>"", line 688 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1006 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1027 in _find_and_load
  File ""<stdin>"", line 1 in <module>

Extension modules: numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, google.protobuf.pyext._message, google3.net.proto2.python.internal.cpp._message (total: 5)
Segmentation fault (core dumped)

```

"
2717379414,issue,closed,completed,No such file or directory: 'patchelf' while building tensorflow from source,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

CentOS 8

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)

### CUDA/cuDNN version

CUDA 12.5/ CUDNN 9.3

### GPU model and memory

Tesla V100 SXM2 16GB

### Current behavior?

I am getting issue while building tensorflow with this command. 

```
 bazel --output_base=/goutam/bazel_cache build    --repo_env=WHEEL_NAME=tensorflow     --config=cuda     --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_70     --config=cuda_wheel   --verbose_failures     //tensorflow/tools/pip_package:wheel 
```

i am getting same error after installing pathelf.

``` 
(pythontemp) [goutam@Goutam]$ which patchelf
/Goutam/patchelf/bin/patchelf
(pythontemp) [goutam@Goutam]$ patchelf --version
patchelf 0.17.0
(pythontemp) [goutam@Goutam]$ 
```




### Standalone code to reproduce the issue

```shell
FileNotFoundError: [Errno 2] No such file or directory: 'patchelf'
```


### Relevant log output

```shell
: [Errno 2] No such file or directory: 'patchelf'
Constructed file path: /tmp/tensorflow_wheel5el2uqr_/tensorflow/python/_pywrap_tensorflow_internal.so
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 3813.683s, Critical Path: 408.33s
INFO: 32825 processes: 5868 internal, 26957 local.
FAILED: Build did NOT complete successfully
(pythontemp) [goutam@ssl-gpu tensorflow]$ ls /tmp/tensorflow_wheel5el2uqr_/tensorflow/python/_pywrap_tensorflow_internal.so
ls: cannot access '/tmp/tensorflow_wheel5el2uqr_/tensorflow/python/_pywrap_tensorflow_internal.so': No such file or directory
(pythontemp) [goutam@ssl-gpu tensorflow]$ ls /tmp/tensorflow_wheel5el2uqr_/tensorflow/python/
ls: cannot access '/tmp/tensorflow_wheel5el2uqr_/tensorflow/python/': No such file or directory
(pythontemp) [goutam@ssl-gpu tensorflow]$ ls /tmp/tensorflow_wheel5el2uqr_/tensorflow/
ls: cannot access '/tmp/tensorflow_wheel5el2uqr_/tensorflow/': No such file or directory
(pythontemp) [goutam@ssl-gpu tensorflow]$ ls /tmp/tensorflow_wheel5el2uqr_/
ls: cannot access '/tmp/tensorflow_wheel5el2uqr_/': No such file or directory
```
",GoutamAgrawal,2024-12-04 11:10:26+00:00,['Venkat6871'],2025-01-09 09:46:08+00:00,2024-12-18 08:26:11+00:00,https://github.com/tensorflow/tensorflow/issues/82195,"[('type:build/install', 'Build and install issues'), ('TF 2.18', '')]","[{'comment_id': 2527113711, 'issue_id': 2717379414, 'author': 'Venkat6871', 'body': 'Hi **@GoutamAgrawal** ,\r\nApologies for the delay, and thank you for raising your concern. The issue occurs because the directory containing patchelf is not included in the PATH environment variable used by Bazel.Please add patchelf to your PATH. If it still does not work, try specifying the path to patchelf explicitly.\r\nAnd please execute the following installation command and check if it resolves the issue in your case?\r\n```\r\nsudo apt install -y patchelf\r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 9, 7, 10, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538009014, 'issue_id': 2717379414, 'author': 'Venkat6871', 'body': 'Hi **@GoutamAgrawal** ,\r\nApologies for the delay. Could you please check with Python versions 3.10 and 3.11? Kindly let us know if the issue still persists.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 12, 7, 25, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550667629, 'issue_id': 2717379414, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82195"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82195"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 8, 26, 13, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-09 07:10:46 UTC): Hi **@GoutamAgrawal** ,
Apologies for the delay, and thank you for raising your concern. The issue occurs because the directory containing patchelf is not included in the PATH environment variable used by Bazel.Please add patchelf to your PATH. If it still does not work, try specifying the path to patchelf explicitly.
And please execute the following installation command and check if it resolves the issue in your case?
```
sudo apt install -y patchelf
```
Thank you!

Venkat6871 (Assginee) on (2024-12-12 07:25:25 UTC): Hi **@GoutamAgrawal** ,
Apologies for the delay. Could you please check with Python versions 3.10 and 3.11? Kindly let us know if the issue still persists.
Thank you!

google-ml-butler[bot] on (2024-12-18 08:26:13 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82195"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82195"">No</a>

"
2716764782,issue,closed,completed,Unable to Install TensorFlow on Raspberry Pi Zero W (ARMv6),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to install TensorFlow on a Raspberry Pi Zero W with ARMv6 architecture with 32-bit bookworm. However, the installation fails, and I cannot find a version of TensorFlow compatible with ARMv6 architecture. I would appreciate guidance or an official release for ARMv6 support.

raspberrypiwh@raspberrypi:~ $ uname -m
armv6l
raspberrypiwh@raspberrypi:~ $ cat /etc/os-release
PRETTY_NAME=""Raspbian GNU/Linux 12 (bookworm)""
NAME=""Raspbian GNU/Linux""
VERSION_ID=""12""
VERSION=""12 (bookworm)""
VERSION_CODENAME=bookworm
ID=raspbian
ID_LIKE=debian
HOME_URL=""http://www.raspbian.org/""
SUPPORT_URL=""http://www.raspbian.org/RaspbianForums""
BUG_REPORT_URL=""http://www.raspbian.org/RaspbianBugs""

### Standalone code to reproduce the issue

```shell
I'm trying to install TensorFlow on a Raspberry Pi Zero W with ARMv6 architecture with 32-bit bookworm. However, the installation fails, and I cannot find a version of TensorFlow compatible with ARMv6 architecture. I would appreciate guidance or an official release for ARMv6 support.

raspberrypiwh@raspberrypi:~ $ uname -m
armv6l
raspberrypiwh@raspberrypi:~ $ cat /etc/os-release
PRETTY_NAME=""Raspbian GNU/Linux 12 (bookworm)""
NAME=""Raspbian GNU/Linux""
VERSION_ID=""12""
VERSION=""12 (bookworm)""
VERSION_CODENAME=bookworm
ID=raspbian
ID_LIKE=debian
HOME_URL=""http://www.raspbian.org/""
SUPPORT_URL=""http://www.raspbian.org/RaspbianForums""
BUG_REPORT_URL=""http://www.raspbian.org/RaspbianBugs""
```


### Relevant log output

_No response_",chaudhariraj,2024-12-04 07:18:44+00:00,['gaikwadrahul8'],2025-01-11 02:01:34+00:00,2025-01-11 02:01:30+00:00,https://github.com/tensorflow/tensorflow/issues/82146,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2527170637, 'issue_id': 2716764782, 'author': 'gaikwadrahul8', 'body': ""Hi, @chaudhariraj\r\nI apologize for the delayed response, I see community-built TensorFlow binaries which provides an official wheel for `Python3.5` on `Raspberry Pi 1-3 and Pi Zero`. if you're looking for an official wheel for Python3.5 on Raspberry Pi Zero please download it from [here](https://github.com/bitsy-ai/tensorflow-arm-bin?tab=readme-ov-file#community-built-tensorflow-binaries) and install it on your Raspberry Pi Zero W (ARMv6)\r\n\r\nYou can also build from source for the Raspberry Pi Zero (ARMv6) so we recommend cross-compiling the TensorFlow Raspbian package. Cross-compilation is using a different platform to build the package than deploy to. Instead of using the Raspberry Pi's limited RAM and comparatively slow processor, it's easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows. Please refer this [documentation](https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#build-from-source-for-the-raspberry-pi).\r\n\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 9, 7, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544700481, 'issue_id': 2716764782, 'author': 'chaudhariraj', 'body': 'Hi, @gaikwadrahul8 \r\nI recently purchased a Raspberry Pi 4 Model B (2018), installed a 64-bit OS (Bookworm, aarch64 architecture), and the default Python version is 3.11. I installed TensorFlow version 2.17 in a virtual environment (venv). However, I\'m encountering issues accessing the Pi Camera using the picamera2 library. The error states that the libcamera module cannot be found, even though I have installed picamera2 and its dependencies, including libcamera-apps.\r\n\r\nWhen I attempt to install TensorFlow directly in the root environment (outside of the virtual environment), I encounter errors. How can I resolve these issues to successfully access the Pi Camera while using TensorFlow and also how i install tensor flow specific version like 2.10 in root or is possible to install tensor flow with apt command?\r\n\r\n\r\nError:\r\nraspberrypi4b@raspberrypi:~ $ pip install tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\r\nerror: externally-managed-environment\r\n\r\n This environment is externally managed\r\n> To install Python packages system-wide, try apt install\r\n    python3-xyz, where xyz is the package you are trying to\r\n    install.\r\n\r\n    If you wish to install a non-Debian-packaged Python package,\r\n    create a virtual environment using python3 -m venv path/to/venv.\r\n    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n    sure you have python3-full installed.\r\n\r\n    For more information visit http://rptl.io/venv\r\n\r\nnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\nhint: See PEP 668 for the detailed specification.\r\n\r\nOS Details:\r\nraspberrypi4b@raspberrypi:~ $ uname -m\r\naarch64\r\nraspberrypi4b@raspberrypi:~ $ uname -r\r\n6.6.62+rpt-rpi-v8\r\nraspberrypi4b@raspberrypi:~ $  cat /etc/os-release\r\nPRETTY_NAME=""Debian GNU/Linux 12 (bookworm)""\r\nNAME=""Debian GNU/Linux""\r\nVERSION_ID=""12""\r\nVERSION=""12 (bookworm)""\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=""https://www.debian.org/""\r\nSUPPORT_URL=""https://www.debian.org/support""\r\nBUG_REPORT_URL=""https://bugs.debian.org/""\r\nraspberrypi4b@raspberrypi:~ $\r\n\r\nWheel: link - https://www.tensorflow.org/install/pip\r\nhttps://storage.googleapis.com/tensorflow/versions/2.18.0/tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl', 'created_at': datetime.datetime(2024, 12, 16, 6, 19, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558986326, 'issue_id': 2716764782, 'author': 'gaikwadrahul8', 'body': 'Hi, @chaudhariraj \r\nI apologize for the delayed response, could you please follow the below instructions before installing the TensorFlow and see is it resolving your issue or not ?\r\n\r\n```\r\n1. sudo apt install python3-venv\r\n2. python3 -m venv ~/tf\r\n3. source ~/tf/bin/activate\r\n4. pip install tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\r\n```\r\nAfter trying above instructions if issue still persists please let us know with updated error log to investigate this issue further from our end. \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 23, 6, 18, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563352291, 'issue_id': 2716764782, 'author': 'gaikwadrahul8', 'body': 'Hi, @chaudhariraj \r\nTo confirm, have you got chance to try above provided instructions ? If so please let us know is it working as expected or not ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 27, 5, 59, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2570001808, 'issue_id': 2716764782, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 4, 2, 0, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000462, 'issue_id': 2716764782, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000499, 'issue_id': 2716764782, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82146"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82146"">No</a>', 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 32, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-09 07:45:18 UTC): Hi, @chaudhariraj
I apologize for the delayed response, I see community-built TensorFlow binaries which provides an official wheel for `Python3.5` on `Raspberry Pi 1-3 and Pi Zero`. if you're looking for an official wheel for Python3.5 on Raspberry Pi Zero please download it from [here](https://github.com/bitsy-ai/tensorflow-arm-bin?tab=readme-ov-file#community-built-tensorflow-binaries) and install it on your Raspberry Pi Zero W (ARMv6)

You can also build from source for the Raspberry Pi Zero (ARMv6) so we recommend cross-compiling the TensorFlow Raspbian package. Cross-compilation is using a different platform to build the package than deploy to. Instead of using the Raspberry Pi's limited RAM and comparatively slow processor, it's easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows. Please refer this [documentation](https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#build-from-source-for-the-raspberry-pi).

If I have missed something here please let me know.

Thank you for your cooperation and patience.

chaudhariraj (Issue Creator) on (2024-12-16 06:19:13 UTC): Hi, @gaikwadrahul8 
I recently purchased a Raspberry Pi 4 Model B (2018), installed a 64-bit OS (Bookworm, aarch64 architecture), and the default Python version is 3.11. I installed TensorFlow version 2.17 in a virtual environment (venv). However, I'm encountering issues accessing the Pi Camera using the picamera2 library. The error states that the libcamera module cannot be found, even though I have installed picamera2 and its dependencies, including libcamera-apps.

When I attempt to install TensorFlow directly in the root environment (outside of the virtual environment), I encounter errors. How can I resolve these issues to successfully access the Pi Camera while using TensorFlow and also how i install tensor flow specific version like 2.10 in root or is possible to install tensor flow with apt command?


Error:
raspberrypi4b@raspberrypi:~ $ pip install tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
error: externally-managed-environment

 This environment is externally managed
> To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.

    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.

    For more information visit http://rptl.io/venv

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.

OS Details:
raspberrypi4b@raspberrypi:~ $ uname -m
aarch64
raspberrypi4b@raspberrypi:~ $ uname -r
6.6.62+rpt-rpi-v8
raspberrypi4b@raspberrypi:~ $  cat /etc/os-release
PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)""
NAME=""Debian GNU/Linux""
VERSION_ID=""12""
VERSION=""12 (bookworm)""
VERSION_CODENAME=bookworm
ID=debian
HOME_URL=""https://www.debian.org/""
SUPPORT_URL=""https://www.debian.org/support""
BUG_REPORT_URL=""https://bugs.debian.org/""
raspberrypi4b@raspberrypi:~ $

Wheel: link - https://www.tensorflow.org/install/pip
https://storage.googleapis.com/tensorflow/versions/2.18.0/tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl

gaikwadrahul8 (Assginee) on (2024-12-23 06:18:28 UTC): Hi, @chaudhariraj 
I apologize for the delayed response, could you please follow the below instructions before installing the TensorFlow and see is it resolving your issue or not ?

```
1. sudo apt install python3-venv
2. python3 -m venv ~/tf
3. source ~/tf/bin/activate
4. pip install tensorflow-2.18.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
```
After trying above instructions if issue still persists please let us know with updated error log to investigate this issue further from our end. 

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-12-27 05:59:11 UTC): Hi, @chaudhariraj 
To confirm, have you got chance to try above provided instructions ? If so please let us know is it working as expected or not ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2025-01-04 02:00:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-11 02:01:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-11 02:01:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82146"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82146"">No</a>

"
2715523349,issue,open,,CPU performance is questionable,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

None

### Current behavior?

So, I have 2 machines: I-5 5200U and 13700K. Both of them have only iGpu which is ignored; understandable.
I am using DeepFace which internally uses tensorflow. I am running the same python program on both machines with the same datasets. The combination is ArcFace+CenterFace. I am running a single photo vs library of photos.
Dataset 1:
Intel I-5 5200U:
CPU usage ~75%, and it takes 75 seconds.
13700K:
CPU usage ~60% with all cores around 60%, and it takes 23 seconds.

Dataset 2:
Intel I-5 5200U:
CPU usage ~60%, and it takes 99 seconds.
13700K:
CPU usage ~20% with all cores around 20%, and it takes 26 seconds.

We get a speed up of 4x; however, the CPU is not fully utilized in both cases. I was wondering about all sorts of optimizations I can do for tensorflow to squeeze the most of this system.


I also have done benchmark on 13700k, and it is close to everyone's results:
https://browser.geekbench.com/v6/cpu/9219843

### Standalone code to reproduce the issue

```shell
https://sharepad.io/p/a327C3f

Output on I-5 5200U:
Total CPUs available: 4
Available memory: 3978.17 MB
2024-12-03 20:40:07.838943: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
Intra-op threads: 0
Inter-op threads: 0

CPUs detected:
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')

Number of logical CPUs detected by TensorFlow: 1

Output on 13700k:
Total CPUs available: 24
Available memory: 57406.59 MB
2024-12-03 20:40:07.838943: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
Intra-op threads: 0
Inter-op threads: 0

CPUs detected:
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')

Number of logical CPUs detected by TensorFlow: 1
```


### Relevant log output

_No response_",Andruxa0125,2024-12-03 16:48:31+00:00,['tilakrayal'],2024-12-08 09:32:29+00:00,,https://github.com/tensorflow/tensorflow/issues/82099,"[('type:support', 'Support issues'), ('TF 2.18', '')]","[{'comment_id': 2520539075, 'issue_id': 2715523349, 'author': 'tilakrayal', 'body': '@Andruxa0125,\r\nI tried to execute the mentioned  code and observed that there is no issue with the performance on the CPU side. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/3c4bd06804f03bac1b6901f534d21bbe/untitled2264.ipynb) and also looks like this is not related to tensorflow. Kindly provide more information if the issue is related to tensorflow bugs or  features. Thank you!', 'created_at': datetime.datetime(2024, 12, 5, 14, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525558350, 'issue_id': 2715523349, 'author': 'Andruxa0125', 'body': ""I am not sure if it is a bug per say.\r\nMy main 2 concerns are:\r\n1. Why are we not utilizing 100% of CPU? What are the settings to improve efficiency and CPU load?\r\n2. I expected tensorflow to be able to achieve speed up of my new processor compared to the old processor which is 18x; however, I am squeezing 3-4x. Part of it is because tensorflow doesn't fully utilize the CPU.\r\n\r\nWhat should I do to improve the performance?\r\nThanks in advance."", 'created_at': datetime.datetime(2024, 12, 8, 9, 32, 26, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-05 14:53:00 UTC): @Andruxa0125,
I tried to execute the mentioned  code and observed that there is no issue with the performance on the CPU side. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/3c4bd06804f03bac1b6901f534d21bbe/untitled2264.ipynb) and also looks like this is not related to tensorflow. Kindly provide more information if the issue is related to tensorflow bugs or  features. Thank you!

Andruxa0125 (Issue Creator) on (2024-12-08 09:32:26 UTC): I am not sure if it is a bug per say.
My main 2 concerns are:
1. Why are we not utilizing 100% of CPU? What are the settings to improve efficiency and CPU load?
2. I expected tensorflow to be able to achieve speed up of my new processor compared to the old processor which is 18x; however, I am squeezing 3-4x. Part of it is because tensorflow doesn't fully utilize the CPU.

What should I do to improve the performance?
Thanks in advance.

"
2715440796,issue,closed,completed,tensorflow,,snaab64,2024-12-03 16:12:24+00:00,['Venkat6871'],2024-12-04 13:09:46+00:00,2024-12-04 13:09:46+00:00,https://github.com/tensorflow/tensorflow/issues/82094,[],"[{'comment_id': 2517304123, 'issue_id': 2715440796, 'author': 'Venkat6871', 'body': 'Closing this issue as spam.', 'created_at': datetime.datetime(2024, 12, 4, 13, 9, 46, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-04 13:09:46 UTC): Closing this issue as spam.

"
2714885224,issue,closed,completed,How to create Python extension module that uses TensorFlow C API?,,grabonly,2024-12-03 12:21:36+00:00,['Venkat6871'],2024-12-06 11:35:16+00:00,2024-12-06 11:35:13+00:00,https://github.com/tensorflow/tensorflow/issues/82064,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues')]","[{'comment_id': 2519412117, 'issue_id': 2714885224, 'author': 'Venkat6871', 'body': 'Hi **@grabonly** ,\r\nThanks for raising your concern here. There are few steps you need to follow for creating python extension. Download and extract the TensorFlow C API library from [TensorFlow releases](https://github.com/tensorflow/tensorflow/releases). Create a file `my_tf_module.cpp`. Set up a `setup.py` script for building the extension. Run: python setup.py build to generate .so (or .pyd on Windows). Finally text with the following commands:\r\n```\r\nimport my_tf_module\r\nprint(""TF Version:"", my_tf_module.get_tf_version())\r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 5, 7, 6, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522974767, 'issue_id': 2714885224, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82064"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82064"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 11, 35, 15, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-05 07:06:22 UTC): Hi **@grabonly** ,
Thanks for raising your concern here. There are few steps you need to follow for creating python extension. Download and extract the TensorFlow C API library from [TensorFlow releases](https://github.com/tensorflow/tensorflow/releases). Create a file `my_tf_module.cpp`. Set up a `setup.py` script for building the extension. Run: python setup.py build to generate .so (or .pyd on Windows). Finally text with the following commands:
```
import my_tf_module
print(""TF Version:"", my_tf_module.get_tf_version())
```
Thank you!

google-ml-butler[bot] on (2024-12-06 11:35:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82064"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/82064"">No</a>

"
2714130368,issue,open,,How to load partial parameters from subclass model in TF2?,"Hi, all, 
    I have constructed a keras model with subclass mode in TensorFlow2.x, and trained it with a dataset named dataset1(which have 4000 classification, namely the vocabulary size is 4000), and save the checkpoint with tf.train.Checkpoint and tf.train.CheckpointManager. And now I want to finetune it with a new dataset named dataset2(which have 500 classification, the vocabulary size is 500), when directely restore the parameters from the saved checkpoint, it will fail because of the mismatch shape (ie, the embedding layer, the output layer of encoder and decoder etc).

   So, how to restore partial parameter from the checkpoint model, or ignore the shape mismatch parameters when restore from the checkpoint. Here is a simplified example, 

	# custom design layer
	class CustomLayer(tf.keras.layers.Layer):
	    def __init__(self, unit1, unit2, **kwargs):
	        super(CustomLayer, self).__init__(**kwargs)
	        self.dense1 = tf.keras.layers.Dense(unit1, activation='relu')
	        self.dense2 = tf.keras.layers.Dense(unit2, activation='softmax')
	 
	    def call(self, inputs):
	        x = self.dense1(inputs)
	        return self.dense2(x)

	# keras model
	class CustomModel(tf.keras.Model):
	    def __init__(self, unit1, unit2):
	        super(SourceModel, self).__init__()
	        self.layer = tf.keras.layers.Dense(128, activation='relu')
	        self.custom_layer = CustomLayer(64, 10)
	 
	    def call(self, inputs):
	        x = self.layer(inputs)
	        x = self.custom_layer(x)
	        return x

	# original model
        old_model = CustomModel(64, 10)
        optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
        ckpt = tf.train.Checkpoint(step=tf.Variable(0), model=old_model, optimizer=optimizer)
        for sample in dataset1:
                training and save checkpoint ......

        # new model
        new_model = CustomModel(64, 5)
        **how to restore partial parameters from the checkpoint saved before and start a new train in the dastaset2?**

",yjiangling,2024-12-03 06:36:46+00:00,['tilakrayal'],2025-01-21 15:29:43+00:00,,https://github.com/tensorflow/tensorflow/issues/82030,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:apis', 'Highlevel API related issues')]","[{'comment_id': 2520166071, 'issue_id': 2714130368, 'author': 'tilakrayal', 'body': '@yjiangling,\r\nRestores a training checkpoint and this Checkpoint and any objects it depends on. This method is intended to be used to load checkpoints created by save(). For checkpoints created by write() use the read() method which does not expect the save_counter variable added by save().\r\n\r\nRestore() either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added)\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/train/Checkpoint\r\n\r\nAlso try to save the checkpoint of the pre-trained model using tf.train.Checkpoint and use Checkpoint.restore to load the checkpoint. Specify the variables or layers you want to restore. Thank you!', 'created_at': datetime.datetime(2024, 12, 5, 12, 19, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538468213, 'issue_id': 2714130368, 'author': 'yjiangling', 'body': ""> @yjiangling, Restores a training checkpoint and this Checkpoint and any objects it depends on. This method is intended to be used to load checkpoints created by save(). For checkpoints created by write() use the read() method which does not expect the save_counter variable added by save().\r\n> \r\n> Restore() either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added)\r\n> \r\n> https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint\r\n> \r\n> Also try to save the checkpoint of the pre-trained model using tf.train.Checkpoint and use Checkpoint.restore to load the checkpoint. Specify the variables or layers you want to restore. Thank you!\r\n\r\nThank you. I did use save() to ca to create a checkpoint file and use restore() to restore the parameters. But for finetune/transfer training, because the vocabulary size have been changed, the shape of variables in some layers (such as embedding, final output layers, ...) have beem changed, it will give ValueError: Shapes (xxx,) and (yyy,) are incompatible. How to restore from pre_trained checkpoint and skip this variables? I tried this method:\r\n\r\n\tcheckpoint = tf.train.load_checkpoint(checkpoint_path)\r\n\tvar_names = checkpoint.get_variable_to_shape_map().keys()\r\n\ttensor_dict = {}\r\n\tfor var_name in var_names:\r\n\t\tif 'ctc_branch' in var_name:\r\n\t\t\tcontinue\r\n\t\tif 'embedding' in var_name:\r\n\t\t\tcontinue\r\n\t\tif 'final_layer' in var_name:\r\n\t\t\tcontinue\r\n\r\nThe tensor_dict already remove the mismatched variables, but how to restore the dict to the new new checkpoint **ckpt = tf.train.Checkpoint(step=tf.Variable(0), model=model, optimizer=optimizer)**, when use **ckpt._restore_from_tensors(tensor_dict)**, it give **NotImplementedError**"", 'created_at': datetime.datetime(2024, 12, 12, 10, 20, 1, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-05 12:19:30 UTC): @yjiangling,
Restores a training checkpoint and this Checkpoint and any objects it depends on. This method is intended to be used to load checkpoints created by save(). For checkpoints created by write() use the read() method which does not expect the save_counter variable added by save().

Restore() either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added)

https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint

Also try to save the checkpoint of the pre-trained model using tf.train.Checkpoint and use Checkpoint.restore to load the checkpoint. Specify the variables or layers you want to restore. Thank you!

yjiangling (Issue Creator) on (2024-12-12 10:20:01 UTC): Thank you. I did use save() to ca to create a checkpoint file and use restore() to restore the parameters. But for finetune/transfer training, because the vocabulary size have been changed, the shape of variables in some layers (such as embedding, final output layers, ...) have beem changed, it will give ValueError: Shapes (xxx,) and (yyy,) are incompatible. How to restore from pre_trained checkpoint and skip this variables? I tried this method:

	checkpoint = tf.train.load_checkpoint(checkpoint_path)
	var_names = checkpoint.get_variable_to_shape_map().keys()
	tensor_dict = {}
	for var_name in var_names:
		if 'ctc_branch' in var_name:
			continue
		if 'embedding' in var_name:
			continue
		if 'final_layer' in var_name:
			continue

The tensor_dict already remove the mismatched variables, but how to restore the dict to the new new checkpoint **ckpt = tf.train.Checkpoint(step=tf.Variable(0), model=model, optimizer=optimizer)**, when use **ckpt._restore_from_tensors(tensor_dict)**, it give **NotImplementedError**

"
2713630213,issue,closed,completed,[MacOS] TensorFlow Metal Plugin Symbol Not Found Error with TensorFlow 2.18.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

macOS 15.1.1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When trying to import TensorFlow with Metal plugin support on macOS using a virtual environment, an error occurs while loading the Metal plugin. Specifically, TensorFlow fails to locate a required symbol in the shared library libmetal_plugin.dylib. The error indicates a mismatch between the TensorFlow core library and the Metal plugin.


### Standalone code to reproduce the issue

```shell
mkdir test_tf_latest
/opt/homebrew/bin/python3.9 -m venv .venv
source .venv/bin/activate
pip install tensorflow tensorflow-metal

python -c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__); print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/.../.../test_tf_latest/.venv/lib/python3.9/site-packages/tensorflow/__init__.py"", line 437, in <module>
    _ll.load_library(_plugin_dir)
  File ""/.../.../test_tf_latest/.venv/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/.../.../test_tf_latest/.venv/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /.../.../test_tf_latest/.venv/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <2A053E7E-6DBA-37C2-A28E-1D52A5836870> /.../.../test_tf_latest/.venv/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```
",yuchen001,2024-12-02 23:43:46+00:00,['Venkat6871'],2025-01-05 17:19:49+00:00,2024-12-21 02:00:17+00:00,https://github.com/tensorflow/tensorflow/issues/81987,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:macOS', 'macOS Build/Installation issues'), ('TF 2.18', '')]","[{'comment_id': 2519256918, 'issue_id': 2713630213, 'author': 'Venkat6871', 'body': 'Hi **@yuchen001** ,\r\nApologies for the delay, and thank you for raising your concern. Instead of using the command `pip install tensorflow tensorflow-metal` , please use the specific compatible version as [documented](https://pypi.org/project/tensorflow-metal/): `pip install tensorflow==2.14.0 tensorflow-metal`. Additionally, the default NumPy version being installed is 2.02, but the compatible version is 1.24.3. You can install it with:`pip install numpy==1.24.3`.\r\nI have tested this setup, and it is detecting everything as we expected.\r\n```\r\n(myenv) maayara-macbookpro:myenv maayara$ python -c ""import tensorflow as tf; print(\'TensorFlow version:\', tf.__version__); print(\'Num GPUs Available:\', len(tf.config.list_physical_devices(\'GPU\')))""\r\nTensorFlow version: 2.14.0\r\nNum GPUs Available: 1\r\n(myenv) maayara-macbookpro:myenv maayara$ \r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 5, 6, 3, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519745661, 'issue_id': 2713630213, 'author': 'elvisli', 'body': 'I have the same issue.\r\nHave to use the previous version of tensorflow.\r\nFixed by installing tensorflow==2.17.0 and tensorflow-metal==1.1.0', 'created_at': datetime.datetime(2024, 12, 5, 9, 30, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540377832, 'issue_id': 2713630213, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557949992, 'issue_id': 2713630213, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 21, 2, 0, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557950023, 'issue_id': 2713630213, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81987"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81987"">No</a>', 'created_at': datetime.datetime(2024, 12, 21, 2, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564790162, 'issue_id': 2713630213, 'author': 'wal5hy', 'body': 'This finally fixed my set up thankyou!\r\nHere\'s the versions which now can run on a Mac M1 Silicon, and successfully reproduce the Image Classification Notebook here: https://www.tensorflow.org/tutorials/images/classification\r\n```\r\nPython 3.11.9\r\nkeras==2.14.0\r\nnumpy==1.24.3\r\ntensorflow==2.14.1\r\n# tensorflow-metal==1.1.0 <-- update removed this\r\n```\r\nThe final piece was numpy version, I cant see this requirement documented anywhere\r\nUpdate: although tensorflow-metal installs and runs without error, it fails to actually allow the model to be trained! the accurracy hovers around 50% over multiple epochs and never improves. So in the end the best combination was without tensorflow-metal at all\r\n\r\n\r\n\r\n> Hi **@yuchen001** , Apologies for the delay, and thank you for raising your concern. Instead of using the command `pip install tensorflow tensorflow-metal` , please use the specific compatible version as [documented](https://pypi.org/project/tensorflow-metal/): `pip install tensorflow==2.14.0 tensorflow-metal`. Additionally, the default NumPy version being installed is 2.02, but the compatible version is 1.24.3. You can install it with:`pip install numpy==1.24.3`. I have tested this setup, and it is detecting everything as we expected.\r\n> \r\n> ```\r\n> (myenv) maayara-macbookpro:myenv maayara$ python -c ""import tensorflow as tf; print(\'TensorFlow version:\', tf.__version__); print(\'Num GPUs Available:\', len(tf.config.list_physical_devices(\'GPU\')))""\r\n> TensorFlow version: 2.14.0\r\n> Num GPUs Available: 1\r\n> (myenv) maayara-macbookpro:myenv maayara$ \r\n> ```\r\n> \r\n> Thank you!', 'created_at': datetime.datetime(2024, 12, 29, 17, 28, 26, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-05 06:03:28 UTC): Hi **@yuchen001** ,
Apologies for the delay, and thank you for raising your concern. Instead of using the command `pip install tensorflow tensorflow-metal` , please use the specific compatible version as [documented](https://pypi.org/project/tensorflow-metal/): `pip install tensorflow==2.14.0 tensorflow-metal`. Additionally, the default NumPy version being installed is 2.02, but the compatible version is 1.24.3. You can install it with:`pip install numpy==1.24.3`.
I have tested this setup, and it is detecting everything as we expected.
```
(myenv) maayara-macbookpro:myenv maayara$ python -c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__); print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""
TensorFlow version: 2.14.0
Num GPUs Available: 1
(myenv) maayara-macbookpro:myenv maayara$ 
```
Thank you!

elvisli on (2024-12-05 09:30:41 UTC): I have the same issue.
Have to use the previous version of tensorflow.
Fixed by installing tensorflow==2.17.0 and tensorflow-metal==1.1.0

github-actions[bot] on (2024-12-13 02:08:38 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-21 02:00:16 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-21 02:00:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81987"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81987"">No</a>

wal5hy on (2024-12-29 17:28:26 UTC): This finally fixed my set up thankyou!
Here's the versions which now can run on a Mac M1 Silicon, and successfully reproduce the Image Classification Notebook here: https://www.tensorflow.org/tutorials/images/classification
```
Python 3.11.9
keras==2.14.0
numpy==1.24.3
tensorflow==2.14.1
# tensorflow-metal==1.1.0 <-- update removed this
```
The final piece was numpy version, I cant see this requirement documented anywhere
Update: although tensorflow-metal installs and runs without error, it fails to actually allow the model to be trained! the accurracy hovers around 50% over multiple epochs and never improves. So in the end the best combination was without tensorflow-metal at all

"
2713545594,issue,closed,completed,I am usage chrome access ,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 3.8

### Custom code

Yes

### OS platform and distribution

15.03

### Mobile device

15.03

### Python version

9.9

### Bazel version

https://github.com/github/docs.git

### GCC/compiler version

I am usage accessories 

### CUDA/cuDNN version

I am usage chrome access 

### GPU model and memory

Welcome 

### Current behavior?

Behavior is very nice alhamdulillah 
Not a complain I understand 

### Standalone code to reproduce the issue

```shell
Sir hum accessories use krte Hain ko Nikki hoti Hain bilkul bhi kisi ko dara dene ya black mail k liye use nh hota
```


### Relevant log output

```shell
Sir hum accessories use krte Hain ko Nikki hoti Hain bilkul bhi kisi ko dara dene ya black mail k liye use nh hota
```
",muzammilamreen,2024-12-02 22:37:49+00:00,['tilakrayal'],2024-12-15 08:17:29+00:00,2024-12-15 08:17:29+00:00,https://github.com/tensorflow/tensorflow/issues/81980,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2516197663, 'issue_id': 2713545594, 'author': 'tilakrayal', 'body': '@muzammilamreen,\r\nCould you please provide the steps you have followed and also take a look at the official document for the installation of the Tensorflow in the smoother way.\r\n\r\nhttps://www.tensorflow.org/install/pip#step-by-step_instructions\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 4, 4, 58, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594132, 'issue_id': 2713545594, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 12, 2, 7, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543552604, 'issue_id': 2713545594, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2024, 12, 15, 8, 17, 29, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-04 04:58:01 UTC): @muzammilamreen,
Could you please provide the steps you have followed and also take a look at the official document for the installation of the Tensorflow in the smoother way.

https://www.tensorflow.org/install/pip#step-by-step_instructions

Thank you!

github-actions[bot] on (2024-12-12 02:07:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mihaimaruseac on (2024-12-15 08:17:29 UTC): Please don't spam

"
2712879033,issue,open,,Clarify the `constant_op.constant(2)` statement,"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/ops/summary_ops_v2.py#L1062-L1066",Anselmoo,2024-12-02 18:05:41+00:00,['tilakrayal'],2025-02-06 19:24:36+00:00,,https://github.com/tensorflow/tensorflow/issues/81954,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues')]","[{'comment_id': 2516602276, 'issue_id': 2712879033, 'author': 'tilakrayal', 'body': '@Anselmoo,\r\nCould you please provide more context/information about the issue, so that it will be more effective to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 12, 4, 8, 57, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516996288, 'issue_id': 2712879033, 'author': 'Anselmoo', 'body': '@tilakrayal sorry and indeed this is a little bit tough to understand ...  \r\n\r\nAccording to the documentation provided at https://www.tensorflow.org/api_docs/python/tf/summary/graph?hl=en understanding the purpose of `constant_op` can be challenging simply by reading the documentation. To clarify this, I checked the source code, which led me to GitHub. So, I raised an issue to address this confusion.', 'created_at': datetime.datetime(2024, 12, 4, 11, 6, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640553619, 'issue_id': 2712879033, 'author': 'tilakrayal', 'body': '@Anselmoo,\nApologies for the delay. The mentioned `constant_op` details are available in the other source file and also the constant_op is available in tensorflow.python.framework\n**from tensorflow.python.framework import constant_op**\nhttps://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/framework/constant_op.py\n\nRequired details are mentioned in the above file. Thank you!', 'created_at': datetime.datetime(2025, 2, 6, 17, 35, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640794900, 'issue_id': 2712879033, 'author': 'Anselmoo', 'body': '@tilakrayal thank you for feedback. \n\n\nUnfortunately, I encountered a **404** error when attempting to access the provided URL: https://tensorflow.org/api_guides/python/constant_o. Do you get the same error?\n\nhttps://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/framework/constant_op.py#L17\n\n<img width=""894"" alt=""Image"" src=""https://github.com/user-attachments/assets/60e723ba-5c23-424d-81e4-2a9f6f615e12"" />\n\n\nOtherwise, the details in the source code are very detailed and well described.', 'created_at': datetime.datetime(2025, 2, 6, 19, 24, 33, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-04 08:57:03 UTC): @Anselmoo,
Could you please provide more context/information about the issue, so that it will be more effective to debug the issue. Thank you!

Anselmoo (Issue Creator) on (2024-12-04 11:06:14 UTC): @tilakrayal sorry and indeed this is a little bit tough to understand ...  

According to the documentation provided at https://www.tensorflow.org/api_docs/python/tf/summary/graph?hl=en understanding the purpose of `constant_op` can be challenging simply by reading the documentation. To clarify this, I checked the source code, which led me to GitHub. So, I raised an issue to address this confusion.

tilakrayal (Assginee) on (2025-02-06 17:35:07 UTC): @Anselmoo,
Apologies for the delay. The mentioned `constant_op` details are available in the other source file and also the constant_op is available in tensorflow.python.framework
**from tensorflow.python.framework import constant_op**
https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/framework/constant_op.py

Required details are mentioned in the above file. Thank you!

Anselmoo (Issue Creator) on (2025-02-06 19:24:33 UTC): @tilakrayal thank you for feedback. 


Unfortunately, I encountered a **404** error when attempting to access the provided URL: https://tensorflow.org/api_guides/python/constant_o. Do you get the same error?

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/framework/constant_op.py#L17

<img width=""894"" alt=""Image"" src=""https://github.com/user-attachments/assets/60e723ba-5c23-424d-81e4-2a9f6f615e12"" />


Otherwise, the details in the source code are very detailed and well described.

"
2712584227,issue,closed,completed,Strange results for gradient tape : Getting positive gradients for negative response,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I'm working with some gradient based interpretability method ([based on the GradCam code from Keras ](https://keras.io/examples/vision/grad_cam/) ) , and I'm running into a result that seems inconsistent with what would expect from backpropagation.  

I am working with a pertrained VGG16 on imagenet, and I am interested in find the most relevent filters for a given class.

I start by forward propagating an image through the network, and then from the relevant bin, I find the gradients to the layer in question (just like they do in the Keras tutorial).

Then, from the pooled gradients (`pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))`), I find the top-K highest/most pertinent filters. 

From this experiment, I run into 2 strange results. 

1.  For almost any image I pass through (even completely different classes), the network almost always seems to be placing the most importance to the same 1 Filter.

2.  And this result I understand even less; many times, the gradients point ""strongly"" to a filter, even though the filter's output is 0/negative (before relu).  From the backpropagation equation, a negative response should result in a Null gradient, right ?

$$ \frac{dY_{class}}{ dActivation_{in} } = \frac{dY_{class}}{dZ} \cdot \frac{dZ}{dActivation_{in}}$$
 $$ =  Relu'(Activation_{in}\cdot W+b) \cdot W$$

If $Activation_{in}\cdot W+b$ is negative, then $\frac{dY_{class}}{Activation_{in}}$ should be 0, right ? 


I provided 3 images.  

All 3 images point consistently to Filter155 (For observation 1).

And for Img3.JPEG, I find the Top5 most relevant filters: Filter336 has a strong gradient, and yet a completely null output. 

Is there a problem with my code, the gradient computations or just my understanding?

Thanks for your help.

Liam
![img1](https://github.com/user-attachments/assets/e78c1a73-6599-408f-9ba3-e2589cf7e155)
![img2](https://github.com/user-attachments/assets/1fab6726-8ee3-49f8-b668-c19f2a0a0242)
![img3](https://github.com/user-attachments/assets/118ca170-7528-4d95-aa1b-d719d6bc724a)







### Standalone code to reproduce the issue

```shell
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications.vgg16 import decode_predictions
from tensorflow.keras.applications import VGG16
import keras
from keras import backend as K

def get_img_array(img_path, size):
    # `img` is a PIL image of size 299x299
    img = keras.utils.load_img(img_path, target_size=size)
    # `array` is a float32 Numpy array of shape (299, 299, 3)
    array = keras.utils.img_to_array(img)
    # We add a dimension to transform our array into a ""batch""
    # of size (1, 299, 299, 3)
    array = np.expand_dims(array, axis=0)
    return array

img = ""img3.JPEG""
img = keras.applications.vgg16.preprocess_input(get_img_array(img, size=(224,224)))

model = VGG16(weights='imagenet',
				  include_top=True,
				  input_shape=(224, 224, 3))

# Remove last layer's softmax
model.layers[-1].activation = None


#I am interested in finding the most informative filters from this Layer
layer = model.get_layer(""block5_conv3"")

grad_model = keras.models.Model(
    model.inputs,  [layer.output, model.output]
)


pred_idx = None
with tf.GradientTape(persistent=True) as tape:
    last_conv_layer_output, preds = grad_model(img, training=False)

    if pred_idx is None:
        pred_idx = tf.argmax(preds[0])
    print(tf.argmax(preds[0]))
    print(decode_predictions(preds.numpy()))
    class_channel = preds[:, pred_idx]


grads = tape.gradient(class_channel,  last_conv_layer_output) #
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
topFilters = tf.math.top_k(pooled_grads, k=5).indices.numpy()

print(""Top Filters : "", topFilters)
print(""Filter responses: "" , tf.math.reduce_euclidean_norm(last_conv_layer_output, axis=(0,1,2)).numpy()[topFilters])

plt.imshow(last_conv_layer_output[0,:,:,336])
plt.show()
```


### Relevant log output

```shell
For Img3 :

Top Filters      : [155 429 336 272  51]
Filter responses : [ 80.908226 208.93723    0.       232.99017  746.0348  ]
```
",liamaltarac,2024-12-02 16:44:57+00:00,['Venkat6871'],2025-01-03 02:01:43+00:00,2025-01-03 02:01:39+00:00,https://github.com/tensorflow/tensorflow/issues/81944,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('comp:keras', 'Keras related issues'), ('TF 2.11', 'Issues related to TF 2.11')]","[{'comment_id': 2516654878, 'issue_id': 2712584227, 'author': 'Venkat6871', 'body': 'Hi **@liamaltarac** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 4, 9, 18, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517357042, 'issue_id': 2712584227, 'author': 'liamaltarac', 'body': 'Hi @Venkat6871 , Thanks for your reply. I [reposted it. ](https://github.com/keras-team/keras/issues/20591).', 'created_at': datetime.datetime(2024, 12, 4, 13, 27, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552743470, 'issue_id': 2712584227, 'author': 'Venkat6871', 'body': 'Hi **@liamaltarac** ,\r\nCould you please close this issue, as it is already being tracked in another repository?\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 19, 4, 22, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233258, 'issue_id': 2712584227, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568606168, 'issue_id': 2712584227, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 3, 2, 1, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568606209, 'issue_id': 2712584227, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81944"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81944"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 2, 1, 42, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-04 09:18:54 UTC): Hi **@liamaltarac** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

liamaltarac (Issue Creator) on (2024-12-04 13:27:33 UTC): Hi @Venkat6871 , Thanks for your reply. I [reposted it. ](https://github.com/keras-team/keras/issues/20591).

Venkat6871 (Assginee) on (2024-12-19 04:22:43 UTC): Hi **@liamaltarac** ,
Could you please close this issue, as it is already being tracked in another repository?
Thank you!

github-actions[bot] on (2024-12-27 02:01:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-03 02:01:39 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-03 02:01:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81944"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81944"">No</a>

"
2711856840,issue,open,,band width calculation support for ddr5,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The bandwidth is calculated by the following code
// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
  device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 2ULL);


https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/clusters/utils.cc#L132

Similar code also exists in XLA

if (mem_clock_khz && mem_bus_width_bits) {
      // Times 2 because HBM is DDR memory; it gets two data bits per each
      // data lane.
      auto memory_bandwidth =
          uint64_t{2} * (*mem_clock_khz) * 1000 * (*mem_bus_width_bits) / 8;
      device_plane->AddStatValue(
          *device_plane->GetOrCreateStatMetadata(
              GetStatTypeStr(StatType::kDevCapMemoryBandwidth)),
          memory_bandwidth);
    }

https://github.com/openxla/xla/blob/main/xla/backends/profiler/gpu/cupti_collector.cc#L433
https://github.com/openxla/xla/blob/main/xla/backends/profiler/gpu//rocm_collector.cc#L531

but for DDR5 should times 4, Is there any way to distinguish it and fix it?

### Standalone code to reproduce the issue

```shell
// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 2ULL);

to

// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 4ULL);


for ddr5
```


### Relevant log output

_No response_",wangchaochaohu,2024-12-02 13:07:17+00:00,['tilakrayal'],2025-01-21 15:36:38+00:00,,https://github.com/tensorflow/tensorflow/issues/81921,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:grappler', 'Grappler related issues')]","[{'comment_id': 2520171954, 'issue_id': 2711856840, 'author': 'tilakrayal', 'body': '@wangchaochaohu,\r\nCould you please provide more info/context and also the use-case or test case to analyse the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 12, 5, 12, 21, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530132861, 'issue_id': 2711856840, 'author': 'wangchaochaohu', 'body': '@tilakrayal  thanks for your replay. you can test it on AMD GPU MI300 and log the result of bandwith, it is wrong, After consulting the data, I found that the calculation coefficient of ddr5 should be 4', 'created_at': datetime.datetime(2024, 12, 10, 2, 59, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575114798, 'issue_id': 2711856840, 'author': 'dimitar-asenov', 'body': ""I'm a little confused about the mention of MI300. That platform uses HBM3, not DDR5, so not sure why MI300 can be used for testing this. @wangchaochaohu could you please clarify what issue you see? Is the problem that you get the incorrect bandwidth? Perhaps there's another input of the computation that's wrong."", 'created_at': datetime.datetime(2025, 1, 7, 12, 2, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582400045, 'issue_id': 2711856840, 'author': 'wangchaochaohu', 'body': '@dimitar-asenov  thanks for your replay \r\n\r\nHere are some explanations \r\n\r\n```\r\n#include <iostream>\r\n#include <hip/hip_runtime.h>\r\nint main()\r\n{\r\n  hipDeviceProp_t prop;\r\n  auto err1 = hipGetDeviceProperties(&prop, 0);\r\n  std::cout << ""  Clock Rate: "" << prop.memoryClockRate * 1e-3 << "" MHz"" << std::endl;\r\n  std::cout << ""  Memory Bandwidth: "" << 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) * 1e-6 << "" GB/s"" << std::endl;\r\n  return 0;\r\n}\r\n```\r\n\r\nif you using this code will get 2.66TB/s bandwidth (prop.memoryClockRate is 1300Mhz and prop.memoryBusWidth is 8192)\r\n\r\nbut https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html show bandwidth is 5.3 TB/s\r\n\r\nI suspect that the DDR particles used in HBM3 and DDR5 are the same, just the packaging is different\r\nyou can see https://gamersnexus.net/Memory%20Bandwidth%20%28GPU%29 \r\n\r\nso I think HBM3(MI300) use DDR5 and we should x4 \r\n\r\n`device.set_bandwidth(properties.memoryBusWidth / 8 *\r\n                       properties.memoryClockRate * 4ULL);\r\n`', 'created_at': datetime.datetime(2025, 1, 10, 10, 45, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586477344, 'issue_id': 2711856840, 'author': 'dimitar-asenov', 'body': ""Thanks for the details. We have to see what's the best way to compute this and differentiate between when to use 2X and 4X. On an internal chat, someone pointed to https://github.com/ROCm/ROCm/issues/3853 which has a discussion about getting the device version from the hip API."", 'created_at': datetime.datetime(2025, 1, 13, 8, 27, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595761379, 'issue_id': 2711856840, 'author': 'wangchaochaohu', 'body': ""I don't know if nvidia's latest training cards will also have such a difference, Looking forward to related fixes Thanks"", 'created_at': datetime.datetime(2025, 1, 16, 13, 56, 36, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-05 12:21:34 UTC): @wangchaochaohu,
Could you please provide more info/context and also the use-case or test case to analyse the issue in an effective way. Thank you!

wangchaochaohu (Issue Creator) on (2024-12-10 02:59:36 UTC): @tilakrayal  thanks for your replay. you can test it on AMD GPU MI300 and log the result of bandwith, it is wrong, After consulting the data, I found that the calculation coefficient of ddr5 should be 4

dimitar-asenov on (2025-01-07 12:02:54 UTC): I'm a little confused about the mention of MI300. That platform uses HBM3, not DDR5, so not sure why MI300 can be used for testing this. @wangchaochaohu could you please clarify what issue you see? Is the problem that you get the incorrect bandwidth? Perhaps there's another input of the computation that's wrong.

wangchaochaohu (Issue Creator) on (2025-01-10 10:45:06 UTC): @dimitar-asenov  thanks for your replay 

Here are some explanations 

```
#include <iostream>
#include <hip/hip_runtime.h>
int main()
{
  hipDeviceProp_t prop;
  auto err1 = hipGetDeviceProperties(&prop, 0);
  std::cout << ""  Clock Rate: "" << prop.memoryClockRate * 1e-3 << "" MHz"" << std::endl;
  std::cout << ""  Memory Bandwidth: "" << 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) * 1e-6 << "" GB/s"" << std::endl;
  return 0;
}
```

if you using this code will get 2.66TB/s bandwidth (prop.memoryClockRate is 1300Mhz and prop.memoryBusWidth is 8192)

but https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html show bandwidth is 5.3 TB/s

I suspect that the DDR particles used in HBM3 and DDR5 are the same, just the packaging is different
you can see https://gamersnexus.net/Memory%20Bandwidth%20%28GPU%29 

so I think HBM3(MI300) use DDR5 and we should x4 

`device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 4ULL);
`

dimitar-asenov on (2025-01-13 08:27:59 UTC): Thanks for the details. We have to see what's the best way to compute this and differentiate between when to use 2X and 4X. On an internal chat, someone pointed to https://github.com/ROCm/ROCm/issues/3853 which has a discussion about getting the device version from the hip API.

wangchaochaohu (Issue Creator) on (2025-01-16 13:56:36 UTC): I don't know if nvidia's latest training cards will also have such a difference, Looking forward to related fixes Thanks

"
2710977247,issue,closed,completed,Tflite x86 lib and dll for windows,I  tried to build x86 lib in windows. But it does'nt work.  Can anaybody provide me x86 lib and dll for the same ?,apratimankit,2024-12-02 08:15:58+00:00,['gaikwadrahul8'],2024-12-27 02:01:11+00:00,2024-12-27 02:01:09+00:00,https://github.com/tensorflow/tensorflow/issues/81900,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2516156789, 'issue_id': 2710977247, 'author': 'tilakrayal', 'body': '@apratimankit,\r\nCould you please provide the steps which you followed to install the tensorflow lite and also the environment details which helps to debug the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 12, 4, 4, 23, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519546248, 'issue_id': 2710977247, 'author': 'apratimankit', 'body': 'I have followed the same steps to build x86 application. Though it got build it was unable to resolve the issue.  I  followed this [](url)\r\nhttps://github.com/tensorflow/tensorflow/issues/47166 for building tflite in x86. I took the pull from the master branch', 'created_at': datetime.datetime(2024, 12, 5, 8, 18, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531617687, 'issue_id': 2710977247, 'author': 'gaikwadrahul8', 'body': 'Hi, @apratimankit \r\nI apologize for the delayed response, To confirm have you followed [Build LiteRT with CMake](https://ai.google.dev/edge/litert/build/cmake#build_tensorflow_lite_c_library) official documentation? If not please give it try and let us know is it working as expected or not for Windows x86 system?\r\n\r\nPlease try with below two commands :\r\n\r\n```\r\ncmake -A Win32 -DCMAKE_BUILD_TYPE=Release -S ..\\tensorflow\\lite\\c -B .\r\ncmake --build . --config Release\r\n```\r\n\r\nIf you face any issues please let us know with error log to investigate this issue further. \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 10, 13, 19, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2534835825, 'issue_id': 2710977247, 'author': 'apratimankit', 'body': 'nothing was build  and it got stopped here, showing this. I am building it from x64 system.\r\n\r\n```\r\nerror_reporter.cc\r\nmodel_builder_base.cc\r\nmetadata_util.cc\r\nmmap_allocation_disabled.cc\r\nschema_utils.cc\r\nstring_utils.cc\r\nGenerating Code...\r\n```', 'created_at': datetime.datetime(2024, 12, 11, 8, 20, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535072749, 'issue_id': 2710977247, 'author': 'apratimankit', 'body': 'if possible could you share me the lib and dll file ?', 'created_at': datetime.datetime(2024, 12, 11, 8, 38, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535815950, 'issue_id': 2710977247, 'author': 'gaikwadrahul8', 'body': ""Hi, @apratimankit \r\nThank you for trying, As far I know bazel does not support build for x86(32 bits) windows so cmake may work and I see similar issue https://github.com/tensorflow/tensorflow/issues/60253 where user was able to build successfully for x86 windows please refer this comment https://github.com/tensorflow/tensorflow/issues/60253#issuecomment-1541833465 so I'm not sure whether user build from this mentioned commit https://github.com/tensorflow/tensorflow/commit/bf157579cf016bcd00e4d01956a0807a2ff6f608 so could you please give it try from specified commit and see is it working as expected or not ?\r\n\r\nUnfortunately, as far I know we do not have official build for x86(32 bits) windows even I checked unofficial and community build for x86(32 bits) windows so you'll have to build it from source using cmake \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 11, 12, 8, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552621276, 'issue_id': 2710977247, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 19, 2, 5, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233283, 'issue_id': 2710977247, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233310, 'issue_id': 2710977247, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81900"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81900"">No</a>', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 11, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-12-04 04:23:26 UTC): @apratimankit,
Could you please provide the steps which you followed to install the tensorflow lite and also the environment details which helps to debug the issue in an effective way. Thank you!

apratimankit (Issue Creator) on (2024-12-05 08:18:53 UTC): I have followed the same steps to build x86 application. Though it got build it was unable to resolve the issue.  I  followed this [](url)
https://github.com/tensorflow/tensorflow/issues/47166 for building tflite in x86. I took the pull from the master branch

gaikwadrahul8 (Assginee) on (2024-12-10 13:19:12 UTC): Hi, @apratimankit 
I apologize for the delayed response, To confirm have you followed [Build LiteRT with CMake](https://ai.google.dev/edge/litert/build/cmake#build_tensorflow_lite_c_library) official documentation? If not please give it try and let us know is it working as expected or not for Windows x86 system?

Please try with below two commands :

```
cmake -A Win32 -DCMAKE_BUILD_TYPE=Release -S ..\tensorflow\lite\c -B .
cmake --build . --config Release
```

If you face any issues please let us know with error log to investigate this issue further. 

Thank you for your cooperation and patience.

apratimankit (Issue Creator) on (2024-12-11 08:20:55 UTC): nothing was build  and it got stopped here, showing this. I am building it from x64 system.

```
error_reporter.cc
model_builder_base.cc
metadata_util.cc
mmap_allocation_disabled.cc
schema_utils.cc
string_utils.cc
Generating Code...
```

apratimankit (Issue Creator) on (2024-12-11 08:38:14 UTC): if possible could you share me the lib and dll file ?

gaikwadrahul8 (Assginee) on (2024-12-11 12:08:28 UTC): Hi, @apratimankit 
Thank you for trying, As far I know bazel does not support build for x86(32 bits) windows so cmake may work and I see similar issue https://github.com/tensorflow/tensorflow/issues/60253 where user was able to build successfully for x86 windows please refer this comment https://github.com/tensorflow/tensorflow/issues/60253#issuecomment-1541833465 so I'm not sure whether user build from this mentioned commit https://github.com/tensorflow/tensorflow/commit/bf157579cf016bcd00e4d01956a0807a2ff6f608 so could you please give it try from specified commit and see is it working as expected or not ?

Unfortunately, as far I know we do not have official build for x86(32 bits) windows even I checked unofficial and community build for x86(32 bits) windows so you'll have to build it from source using cmake 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-19 02:05:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-27 02:01:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-27 02:01:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81900"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81900"">No</a>

"
2710808183,issue,open,,XLA recompilation every time when shape changed on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.18

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have seen a lot of dynamic dim configurations in the OpenXLA repo. I am using TF GPU-XLA. How should I configure it so that recompilation is not triggered during GPU dynamic dim?

### Standalone code to reproduce the issue

```shell
@tf.function(input_signature=[tf.TensorSpec(shape=[2, None], dtype=tf.float32),
                              tf.TensorSpec(shape=[None, 2], dtype=tf.float32),
                              tf.TensorSpec(shape=[], dtype=tf.float32),],
             jit_compile=True)
def foo(x, y, z):
    exp = tf.add(x, z)
    abs = tf.sin(exp)
    add = tf.add(abs, tf.transpose(y, ))
    mm = tf.matmul(add, y)
    return mm

x = tf.ones([2, 3]).gpu()
y = tf.ones([3, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z) # Compile

x = tf.ones([2, 4]).gpu()
y = tf.ones([4, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z)  # Compile

x = tf.ones([2, 5]).gpu()
y = tf.ones([5, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z)  # Compile
```


### Relevant log output

_No response_",medivh-xp,2024-12-02 07:13:40+00:00,['Venkat6871'],2024-12-20 08:55:40+00:00,,https://github.com/tensorflow/tensorflow/issues/81896,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:xla', 'XLA'), ('TF 2.18', '')]","[{'comment_id': 2526876872, 'issue_id': 2710808183, 'author': 'Venkat6871', 'body': 'Hi **@medivh-xp** ,\r\nApologies for the delay, and thank you for raising your concern. This issue occurs because XLA optimizes the computation graph based on input shapes. A fundamental limitation of XLA is that any change in input shapes requires re-optimization and recompilation of the graph.\r\nTo minimize recompilation, consider using static shapes wherever possible. For example, use fixed dimensions for batch size or feature size to reduce variability in input shapes.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 9, 4, 40, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529985292, 'issue_id': 2710808183, 'author': 'medivh-xp', 'body': '> Hi **@medivh-xp** , Apologies for the delay, and thank you for raising your concern. This issue occurs because XLA optimizes the computation graph based on input shapes. A fundamental limitation of XLA is that any change in input shapes requires re-optimization and recompilation of the graph. To minimize recompilation, consider using static shapes wherever possible. For example, use fixed dimensions for batch size or feature size to reduce variability in input shapes.\r\n> \r\n> Thank you!\r\n\r\nThank you for your reply! Vector computation is memory bound, so fusion can greatly improve overall performance.\r\n. Compilers such as torch inductor will compile a static kernel and then calculate tiling.  is there any way to obtain the benefits of Vector fusion under dynamic shape in TF? The compilation time of XLA is difficult to accept in my application... :(', 'created_at': datetime.datetime(2024, 12, 10, 1, 27, 53, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-09 04:40:54 UTC): Hi **@medivh-xp** ,
Apologies for the delay, and thank you for raising your concern. This issue occurs because XLA optimizes the computation graph based on input shapes. A fundamental limitation of XLA is that any change in input shapes requires re-optimization and recompilation of the graph.
To minimize recompilation, consider using static shapes wherever possible. For example, use fixed dimensions for batch size or feature size to reduce variability in input shapes.

Thank you!

medivh-xp (Issue Creator) on (2024-12-10 01:27:53 UTC): Thank you for your reply! Vector computation is memory bound, so fusion can greatly improve overall performance.
. Compilers such as torch inductor will compile a static kernel and then calculate tiling.  is there any way to obtain the benefits of Vector fusion under dynamic shape in TF? The compilation time of XLA is difficult to accept in my application... :(

"
2708849760,issue,closed,completed,"error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]","I'm new to this.  

Trying to build the Selective Framework for iOS using the below command - 
bash tensorflow/lite/ios/build_frameworks.sh \
  --input_models=model1.tflite,model2.tflite \
  --target_archs=x86_64,armv7,arm64
  
  But getting the error - 
  ERROR: /private/var/tmp/_bazel_tonmoy/d1033cf820cfe9e8569d67cf059cb6df/external/upb/BUILD:57:11: Compiling upb/upb.c [for tool] failed: (Exit 1): wrapped_clang failed: error executing command (from target @upb//:upb) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 32 arguments skipped)

external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]

  192 |   n &= ~(upb_alignof(upb_arena) - 1);

      |          ^~~~~~~~~~~~~~~~~~~~~~

external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'

  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)

      |                                     ^~~~~~

/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/16/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'

   16 | #define offsetof(t, d) __builtin_offsetof(t, d)

      |                                           ^

1 error generated.

Error in child process '/usr/bin/xcrun'. 1

Target //tensorflow/lite/ios/tmp:TensorFlowLiteSelectTfOps_framework failed to build

Use --verbose_failures to see the command lines of failed build steps.

INFO: Elapsed time: 218.346s, Critical Path: 17.92s

INFO: 4162 processes: 747 internal, 3415 local.

FAILED: Build did NOT complete successfully

",d0tb0t71,2024-12-01 09:28:05+00:00,['gaikwadrahul8'],2025-01-11 02:01:36+00:00,2025-01-11 02:01:33+00:00,https://github.com/tensorflow/tensorflow/issues/81724,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2530348545, 'issue_id': 2708849760, 'author': 'd0tb0t71', 'body': 'No response and the issue still happening.', 'created_at': datetime.datetime(2024, 12, 10, 4, 37, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530359263, 'issue_id': 2708849760, 'author': 'tilakrayal', 'body': '@d0tb0t71,\r\nApologies. Could you please provide the exact sequence of commands / steps that you executed before running into the problem. Also provide the tensorflow version and environment details which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 12, 10, 4, 43, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530430615, 'issue_id': 2708849760, 'author': 'd0tb0t71', 'body': ""1. bazel clean --async\r\n2. export HERMETIC_PYTHON_VERSION=3.12\r\n3. bash tensorflow/lite/ios/build_frameworks.sh \\\r\n--input_models=model.tflite \\\r\n--target_archs=arm64\r\n\r\n\r\nI'm on the **master** branch"", 'created_at': datetime.datetime(2024, 12, 10, 5, 21, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533587000, 'issue_id': 2708849760, 'author': 'd0tb0t71', 'body': 'Any update regarding this issue?', 'created_at': datetime.datetime(2024, 12, 11, 4, 10, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538823242, 'issue_id': 2708849760, 'author': 'gaikwadrahul8', 'body': ""Hi, @d0tb0t71 \r\nI apologize for the delayed response, if you're using bazel then could you please try by adding this flag in your bazel command `--copt=-Wno-gnu-offsetof-extensions` and see is it resolving your issue or not ?\r\n\r\nIf the issue persists please help us with complete steps including package versions or if you're following any official documentation to replicate the same behavior on our end.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 12, 12, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543459103, 'issue_id': 2708849760, 'author': 'd0tb0t71', 'body': 'Thank you @gaikwadrahul8 for your response. \r\n\r\nI am new to Tensorflow and all this stuff. Actually, my purpose is to reduce the framework size for iOS. When I use this SpecialTF framework using cocoa pod the app size becomes 200MB+. So now I want to build a reduced-size framework with only the ops used in my model.\r\n\r\nSo can you please give a hint where to use the provided command? I am using **Bazel 6.5.0**.', 'created_at': datetime.datetime(2024, 12, 15, 5, 49, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546398272, 'issue_id': 2708849760, 'author': 'gaikwadrahul8', 'body': 'Hi, @d0tb0t71\r\nAlright, please use the below command with `--copt=-Wno-gnu-offsetof-extensions` flag if it does not work with below command then please modify the `build_frameworks.sh` script manually and You can do this by editing the script and finding the `bazel build` command within the script then adding `--copt=-Wno-gnu-offsetof-extensions\r\n` flag like this :`bazel build //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework --copt=-Wno-gnu-offsetof-extensions` may solve your issue so please give it try and let us know is it working as expected or not ?\r\n\r\n\r\n\r\n```\r\nbash tensorflow/lite/ios/build_frameworks.sh \\\r\n  --input_models=model1.tflite,model2.tflite \\\r\n  --target_archs=x86_64,armv7,arm64 \\\r\n  --copt=-Wno-gnu-offsetof-extensions\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 16, 18, 55, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547682595, 'issue_id': 2708849760, 'author': 'd0tb0t71', 'body': 'Which branch should be used to build the framework? @gaikwadrahul8', 'created_at': datetime.datetime(2024, 12, 17, 7, 24, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563441933, 'issue_id': 2708849760, 'author': 'gaikwadrahul8', 'body': 'Hi, @d0tb0t71 \r\nI apologize for the delayed response, It is recommended to build from master branch after cloning the TensorFlow Github repo by default it will use `master` branch. Thank you', 'created_at': datetime.datetime(2024, 12, 27, 8, 12, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2570001844, 'issue_id': 2708849760, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 4, 2, 0, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000501, 'issue_id': 2708849760, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585000539, 'issue_id': 2708849760, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81724"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81724"">No</a>', 'created_at': datetime.datetime(2025, 1, 11, 2, 1, 35, tzinfo=datetime.timezone.utc)}]","d0tb0t71 (Issue Creator) on (2024-12-10 04:37:52 UTC): No response and the issue still happening.

tilakrayal on (2024-12-10 04:43:24 UTC): @d0tb0t71,
Apologies. Could you please provide the exact sequence of commands / steps that you executed before running into the problem. Also provide the tensorflow version and environment details which helps to debug the issue. Thank you!

d0tb0t71 (Issue Creator) on (2024-12-10 05:21:48 UTC): 1. bazel clean --async
2. export HERMETIC_PYTHON_VERSION=3.12
3. bash tensorflow/lite/ios/build_frameworks.sh \
--input_models=model.tflite \
--target_archs=arm64


I'm on the **master** branch

d0tb0t71 (Issue Creator) on (2024-12-11 04:10:04 UTC): Any update regarding this issue?

gaikwadrahul8 (Assginee) on (2024-12-12 12:52:26 UTC): Hi, @d0tb0t71 
I apologize for the delayed response, if you're using bazel then could you please try by adding this flag in your bazel command `--copt=-Wno-gnu-offsetof-extensions` and see is it resolving your issue or not ?

If the issue persists please help us with complete steps including package versions or if you're following any official documentation to replicate the same behavior on our end.

Thank you for your cooperation and patience.

d0tb0t71 (Issue Creator) on (2024-12-15 05:49:24 UTC): Thank you @gaikwadrahul8 for your response. 

I am new to Tensorflow and all this stuff. Actually, my purpose is to reduce the framework size for iOS. When I use this SpecialTF framework using cocoa pod the app size becomes 200MB+. So now I want to build a reduced-size framework with only the ops used in my model.

So can you please give a hint where to use the provided command? I am using **Bazel 6.5.0**.

gaikwadrahul8 (Assginee) on (2024-12-16 18:55:39 UTC): Hi, @d0tb0t71
Alright, please use the below command with `--copt=-Wno-gnu-offsetof-extensions` flag if it does not work with below command then please modify the `build_frameworks.sh` script manually and You can do this by editing the script and finding the `bazel build` command within the script then adding `--copt=-Wno-gnu-offsetof-extensions
` flag like this :`bazel build //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework --copt=-Wno-gnu-offsetof-extensions` may solve your issue so please give it try and let us know is it working as expected or not ?



```
bash tensorflow/lite/ios/build_frameworks.sh \
  --input_models=model1.tflite,model2.tflite \
  --target_archs=x86_64,armv7,arm64 \
  --copt=-Wno-gnu-offsetof-extensions
```

Thank you for your cooperation and patience.

d0tb0t71 (Issue Creator) on (2024-12-17 07:24:13 UTC): Which branch should be used to build the framework? @gaikwadrahul8

gaikwadrahul8 (Assginee) on (2024-12-27 08:12:32 UTC): Hi, @d0tb0t71 
I apologize for the delayed response, It is recommended to build from master branch after cloning the TensorFlow Github repo by default it will use `master` branch. Thank you

github-actions[bot] on (2025-01-04 02:00:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-11 02:01:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-11 02:01:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81724"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81724"">No</a>

"
2706280936,issue,open,,"Unspecific error message ""None values not supported"" when no labels are provided in dataset","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If I write some tf/keras code that fails to include a label in the dataset, the resulting error message is:  `ValueError: None values not supported.`

The backtrace leads me through `optree/ops.py`.

It's not clear from the message what the `None` values are referring to, nor does the backtrace give me any useful information (as far as I can tell) about the source of the error. I posted this issue here: https://stackoverflow.com/q/79236884/1613983

If the error message could instead make reference to a missing labels set, null `y_true`, etc. that would make it much easier to figure out what's going on.

Somewhat ironically, I ended up solving it after asking GPT o1 about the problem, and it got the answer in one go.

### Standalone code to reproduce the issue

```shell
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import tensorflow as tf

input_data = pd.DataFrame(np.random.rand(1000, 10))

data = tf.data.Dataset.zip({'input_values' : tf.data.Dataset.from_tensor_slices(input_data.values)})

batch_size = 100
train_split = 0.8

train_rows = int(train_split * input_data.shape[0])
train_dataset = data.take(train_rows)
validation_dataset = data.skip(train_rows)

train_data_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
validation_data_batched = validation_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

num_outputs = 10

input_layer = Input(shape=(num_outputs,), name=f'input_values')
output = layers.Dense(num_outputs, activation='sigmoid', name='output')(input_layer)

# Define the model
model = Model(
    inputs=[input_layer],
    outputs=output,
)

max_epochs = 10

def loss(y_true, y_pred):
    return 2.0

model.compile(
    loss=loss,
    optimizer='adam',
)

history = model.fit(
    train_data_batched,
    epochs=max_epochs,
    validation_data=validation_data_batched
)
```


### Relevant log output

```shell
File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/optree/ops.py:752, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    750 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    751 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> 752 return treespec.unflatten(map(func, *flat_args))

ValueError: None values not supported.
```
",armanschwarz,2024-11-29 22:36:45+00:00,['tilakrayal'],2024-12-11 22:40:39+00:00,,https://github.com/tensorflow/tensorflow/issues/81356,"[('type:feature', 'Feature requests'), ('comp:data', 'tf.data related issues'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]","[{'comment_id': 2508718656, 'issue_id': 2706280936, 'author': 'armanschwarz', 'body': 'I\'m not sure if error messages fall under ""documentation"", so I might have labelled this incorrectly.', 'created_at': datetime.datetime(2024, 11, 29, 22, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535995110, 'issue_id': 2706280936, 'author': 'tilakrayal', 'body': '@armanschwarz,\r\nAs the error mentioned, you are trying to feed the None values which were not supported. Could you try to provide the labels and try to execute the mentioned code. THank you!', 'created_at': datetime.datetime(2024, 12, 11, 13, 29, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537347719, 'issue_id': 2706280936, 'author': 'armanschwarz', 'body': ""Just commenting to remove the awaiting response label. @tilakrayal I don't think you've read/understood the question."", 'created_at': datetime.datetime(2024, 12, 11, 22, 39, 42, tzinfo=datetime.timezone.utc)}]","armanschwarz (Issue Creator) on (2024-11-29 22:50:07 UTC): I'm not sure if error messages fall under ""documentation"", so I might have labelled this incorrectly.

tilakrayal (Assginee) on (2024-12-11 13:29:09 UTC): @armanschwarz,
As the error mentioned, you are trying to feed the None values which were not supported. Could you try to provide the labels and try to execute the mentioned code. THank you!

armanschwarz (Issue Creator) on (2024-12-11 22:39:42 UTC): Just commenting to remove the awaiting response label. @tilakrayal I don't think you've read/understood the question.

"
2706280936,issue,open,,"Unspecific error message ""None values not supported"" when no labels are provided in dataset","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If I write some tf/keras code that fails to include a label in the dataset, the resulting error message is:  `ValueError: None values not supported.`

The backtrace leads me through `optree/ops.py`.

It's not clear from the message what the `None` values are referring to, nor does the backtrace give me any useful information (as far as I can tell) about the source of the error. I posted this issue here: https://stackoverflow.com/q/79236884/1613983

If the error message could instead make reference to a missing labels set, null `y_true`, etc. that would make it much easier to figure out what's going on.

Somewhat ironically, I ended up solving it after asking GPT o1 about the problem, and it got the answer in one go.

### Standalone code to reproduce the issue

```shell
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import tensorflow as tf

input_data = pd.DataFrame(np.random.rand(1000, 10))

data = tf.data.Dataset.zip({'input_values' : tf.data.Dataset.from_tensor_slices(input_data.values)})

batch_size = 100
train_split = 0.8

train_rows = int(train_split * input_data.shape[0])
train_dataset = data.take(train_rows)
validation_dataset = data.skip(train_rows)

train_data_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
validation_data_batched = validation_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

num_outputs = 10

input_layer = Input(shape=(num_outputs,), name=f'input_values')
output = layers.Dense(num_outputs, activation='sigmoid', name='output')(input_layer)

# Define the model
model = Model(
    inputs=[input_layer],
    outputs=output,
)

max_epochs = 10

def loss(y_true, y_pred):
    return 2.0

model.compile(
    loss=loss,
    optimizer='adam',
)

history = model.fit(
    train_data_batched,
    epochs=max_epochs,
    validation_data=validation_data_batched
)
```


### Relevant log output

```shell
File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/optree/ops.py:752, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    750 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    751 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> 752 return treespec.unflatten(map(func, *flat_args))

ValueError: None values not supported.
```
",armanschwarz,2024-11-29 22:36:45+00:00,['tilakrayal'],2024-12-11 22:40:39+00:00,,https://github.com/tensorflow/tensorflow/issues/81356,"[('type:feature', 'Feature requests'), ('comp:data', 'tf.data related issues'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]","[{'comment_id': 2508718656, 'issue_id': 2706280936, 'author': 'armanschwarz', 'body': 'I\'m not sure if error messages fall under ""documentation"", so I might have labelled this incorrectly.', 'created_at': datetime.datetime(2024, 11, 29, 22, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535995110, 'issue_id': 2706280936, 'author': 'tilakrayal', 'body': '@armanschwarz,\r\nAs the error mentioned, you are trying to feed the None values which were not supported. Could you try to provide the labels and try to execute the mentioned code. THank you!', 'created_at': datetime.datetime(2024, 12, 11, 13, 29, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537347719, 'issue_id': 2706280936, 'author': 'armanschwarz', 'body': ""Just commenting to remove the awaiting response label. @tilakrayal I don't think you've read/understood the question."", 'created_at': datetime.datetime(2024, 12, 11, 22, 39, 42, tzinfo=datetime.timezone.utc)}]","armanschwarz (Issue Creator) on (2024-11-29 22:50:07 UTC): I'm not sure if error messages fall under ""documentation"", so I might have labelled this incorrectly.

tilakrayal (Assginee) on (2024-12-11 13:29:09 UTC): @armanschwarz,
As the error mentioned, you are trying to feed the None values which were not supported. Could you try to provide the labels and try to execute the mentioned code. THank you!

armanschwarz (Issue Creator) on (2024-12-11 22:39:42 UTC): Just commenting to remove the awaiting response label. @tilakrayal I don't think you've read/understood the question.

"
2705598053,issue,closed,completed,"cannot import flatbuffers, metadata, and metadata_schema_py_generated cause Descriptors cannot be created directly","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Windows 11 Pro

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

can't import the library to add metadata to tflite, I have installed tflite support and the appropriate protobuf.
!pip install -q tflite-support
!pip install -q protobuf==3.20.3

### Standalone code to reproduce the issue

```shell
!pip install -q tflite-support
!pip install -q protobuf==3.20.3
os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""] = ""python""

from tflite_support import flatbuffers
from tflite_support import metadata as _metadata
from tflite_support import metadata_schema_py_generated as _metadata_fb
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-28-37cdba048740> in <cell line: 1>()
----> 1 from tflite_support import flatbuffers
      2 from tflite_support import metadata as _metadata
      3 from tflite_support import metadata_schema_py_generated as _metadata_fb

6 frames
/usr/local/lib/python3.10/dist-packages/tflite_support/__init__.py in <module>
     51 if platform.system() != 'Windows':
     52   # Task Library is not supported on Windows yet.
---> 53   from tflite_support import task

/usr/local/lib/python3.10/dist-packages/tflite_support/task/__init__.py in <module>
     26 """"""
     27 
---> 28 from . import audio
     29 from . import core
     30 from . import processor

/usr/local/lib/python3.10/dist-packages/tflite_support/task/audio/__init__.py in <module>
     18 """"""
     19 
---> 20 from tensorflow_lite_support.python.task.audio import audio_classifier
     21 from tensorflow_lite_support.python.task.audio import audio_embedder
     22 from tensorflow_lite_support.python.task.audio.core import audio_record

/usr/local/lib/python3.10/dist-packages/tensorflow_lite_support/python/task/audio/audio_classifier.py in <module>
     20 from tensorflow_lite_support.python.task.audio.core.pybinds import _pywrap_audio_buffer
     21 from tensorflow_lite_support.python.task.audio.pybinds import _pywrap_audio_classifier
---> 22 from tensorflow_lite_support.python.task.core import base_options as base_options_module
     23 from tensorflow_lite_support.python.task.processor.proto import classification_options_pb2
     24 from tensorflow_lite_support.python.task.processor.proto import classifications_pb2

/usr/local/lib/python3.10/dist-packages/tensorflow_lite_support/python/task/core/base_options.py in <module>
     18 
     19 from tensorflow_lite_support.python.task.core.optional_dependencies import doc_controls
---> 20 from tensorflow_lite_support.python.task.core.proto import base_options_pb2
     21 
     22 _BaseOptionsProto = base_options_pb2.BaseOptions

/usr/local/lib/python3.10/dist-packages/tensorflow_lite_support/python/task/core/proto/base_options_pb2.py in <module>
     34   create_key=_descriptor._internal_create_key,
     35   fields=[
---> 36     _descriptor.FieldDescriptor(
     37       name='file_name', full_name='tflite.python.task.core.BaseOptions.file_name', index=0,
     38       number=1, type=9, cpp_type=9, label=1,

/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py in __new__(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)
    551                 default_value, message_type, enum_type, containing_type,
    552                 is_extension, extension_scope, options=None,
--> 553                 serialized_options=None,
    554                 has_default_value=True, containing_oneof=None, json_name=None,
    555                 file=None, create_key=None):  # pylint: disable=redefined-builtin

TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
",aapiiw,2024-11-29 16:36:30+00:00,['gaikwadrahul8'],2024-12-20 02:01:53+00:00,2024-12-20 02:01:50+00:00,https://github.com/tensorflow/tensorflow/issues/81350,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2516218300, 'issue_id': 2705598053, 'author': 'gaikwadrahul8', 'body': ""Hi, @aapiiw \r\nThank you for bringing this issue to our attention, I'm able to reproduce the same behavior from my end with TensorFlow version 2.17 but when I tried with TensorFlow version 2.13.* it's working as expected so at the moment temporary workaround is downgrading the TensorFlow version to 2.13.*  here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/e2192a5e3e62c917d06554fec55873a6/tflite-issue-81350.ipynb) for reference.\r\n\r\nI think it's better to post this issue here :https://github.com/tensorflow/tflite-support/issues/new to address it quickly \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 4, 5, 19, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594200, 'issue_id': 2705598053, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 12, 2, 7, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556113078, 'issue_id': 2705598053, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556113133, 'issue_id': 2705598053, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81350"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81350"">No</a>', 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 52, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-04 05:19:07 UTC): Hi, @aapiiw 
Thank you for bringing this issue to our attention, I'm able to reproduce the same behavior from my end with TensorFlow version 2.17 but when I tried with TensorFlow version 2.13.* it's working as expected so at the moment temporary workaround is downgrading the TensorFlow version to 2.13.*  here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/e2192a5e3e62c917d06554fec55873a6/tflite-issue-81350.ipynb) for reference.

I think it's better to post this issue here :https://github.com/tensorflow/tflite-support/issues/new to address it quickly 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-12 02:07:55 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-20 02:01:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-20 02:01:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81350"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81350"">No</a>

"
2705574449,issue,closed,not_planned,Request: Add int8 support to Unsorted_Segment_X ops,"(I also raised this request in the new LiteRT repo:  duplicate of https://github.com/google-ai-edge/LiteRT/issues/190)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04.1, but targeting Android
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.18.0


**Provide the text output from tflite_convert**: Used a custom converter that performs post training full quantization using representative datasets.

```python
converter = tf.lite.TFLiteConverter.from_keras_model(base_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = create_representative_data(data_path)
```


Hi! As part of my investigation of deploying a graph neural network (GNN), built from the [TF-GNN ](https://github.com/tensorflow/gnn) library on mobile (Android), I found that the operator [Unsorted_Segment_Sum](https://www.tensorflow.org/mlir/tfl_ops#tflunsorted_segment_sum_tflunsortedsegmentsumop) doesn't support int8. This prevented us from taking advantage of a full quantization of the GNN which relies on the unsorted_segment_x operators for core message passing steps. This resulted in quantized models less performant than the non-quantized ones because of the extra dequantization-quantization layers.


I'd like to request the addition of this data type, since it is a very important operator used by the TF-GNN library itself, and we may see more demand for quantization of GNNs in the future.

Thanks in advance!",sicong-li-arm,2024-11-29 16:21:33+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-22 20:01:42+00:00,2025-01-22 20:01:40+00:00,https://github.com/tensorflow/tensorflow/issues/81348,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.18', '')]","[{'comment_id': 2511288788, 'issue_id': 2705574449, 'author': 'gaikwadrahul8', 'body': 'Hi, @sicong-li-arm \r\nI apologize for the delayed response, I see at the moment we do support `int32` and `float32` data types but not `int8` data type so this issue will be considered as feature request and will update you \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/38f4882ba0a24bcdc2f1ce52be62e1b19242dbee/tensorflow/lite/kernels/unsorted_segment.cc#L205\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 2, 11, 32, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594670946, 'issue_id': 2705574449, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2025, 1, 16, 6, 57, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608152376, 'issue_id': 2705574449, 'author': 'pkgoogle', 'body': 'Please follow progress in LiteRT, thanks.', 'created_at': datetime.datetime(2025, 1, 22, 20, 1, 40, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-02 11:32:58 UTC): Hi, @sicong-li-arm 
I apologize for the delayed response, I see at the moment we do support `int32` and `float32` data types but not `int8` data type so this issue will be considered as feature request and will update you 

https://github.com/tensorflow/tensorflow/blob/38f4882ba0a24bcdc2f1ce52be62e1b19242dbee/tensorflow/lite/kernels/unsorted_segment.cc#L205

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-01-16 06:57:04 UTC): Hi, @pkgoogle 
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2025-01-22 20:01:40 UTC): Please follow progress in LiteRT, thanks.

"
2705514168,issue,closed,not_planned,GPU image 'tensorflow:2.18-gpu-jupyter' does not come with GPU support (missing CUDA libraries),"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When updating from `tensorflow:2.17-gpu-jupyter` to `tensorflow:2.18-gpu-jupyter` we expect GPU support. As per the 2.18 update local drivers are not supported and an install of Hermetic CUDA is needed. We would need to install `tensorflow[and-cuda]` again in the `requirements.txt` file.

As users of the `tensorflow:2.18-gpu-jupyter`, having read ""Optional Features"" at https://hub.docker.com/r/tensorflow/tensorflow, we expect GPU support or the existance of a seperate tag for `[and-cuda]`.

### Standalone code to reproduce the issue

The following is for Run:AI with Kubernetes:
```shell
export job_name=""acceptance-test-${CI_PIPELINE_ID}""
curl -Lsk -o /usr/local/bin/runai <URL>
chmod +x /usr/local/bin/runai
source runai_login
runai config project $runai_project
runai submit $job_name -i $image:$build_number -g 1 -- python3 -c 'import tensorflow as tf; print(len(tf.config.list_physical_devices(""GPU"")))'
while [[ $(runai describe job $job_name | grep ""Status:"" | awk '{print $2}') != ""Succeeded"" ]]; do sleep 10; echo Waiting for pod status to be completed...; done
kubectl logs $pod_name -n ""runai-${runai_project}""
```


### Relevant log output

```shell
root@3bdc05a33062:/tf# python3 -c 'import tensorflow as tf; print(len(tf.config.list_physical_devices(""GPU"")))'
2024-11-29 13:25:40.471862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732886740.493342      11 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732886740.499908      11 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-29 13:25:40.523155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1732886743.034830      11 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
0

root@3bdc05a33062:/tf# nvidia-smi
Fri Nov 29 13:31:56 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:07:00.0 Off |                    0 |
| N/A   28C    P0              65W / 400W |      3MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```
",SandraAnder,2024-11-29 15:45:14+00:00,['Venkat6871'],2025-01-03 13:13:20+00:00,2025-01-03 13:13:16+00:00,https://github.com/tensorflow/tensorflow/issues/81344,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2516295121, 'issue_id': 2705514168, 'author': 'Venkat6871', 'body': 'Hi **@SandraAnder** ,\r\nApologies for the delay, and thank you for raising your concern. Could you please try using a standard GPU instead of tensorflow:2.18-gpu-jupyter? Let us know if you are still unable to use the GPU.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 4, 6, 17, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522724908, 'issue_id': 2705514168, 'author': 'SandraAnder', 'body': 'Hi,\r\nThank you for the answer. I presume that by ""standard GPU"" you mean the `tensorflow:2.18.0-gpu` tag (without `-jupyter`), so I tried the `tensorflow:2.18.0-gpu` tag and we get the same result of no GPU. See error logs below:\r\n\r\n2024-12-06 09:20:48.539413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1733476848.5606[41]    1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1733476848.566966       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-06 09:20:48.589059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nW0000 00:00:1733476850.581204       1 gpu_device.cc:23[44] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n0\r\n\r\nWe have found a workaround for the issue by installing `tensorflow[and-cuda]` in `requirements.txt` when using the tag `tensorflow:2.18-gpu-jupyter`, but in the long run this will cause more manual upkeep of the image when upgrading versions.', 'created_at': datetime.datetime(2024, 12, 6, 10, 7, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527143690, 'issue_id': 2705514168, 'author': 'luksi1', 'body': 'I assume both the ""gpu"" and ""gpu-jupyter"" images are supposed to contain the new CUDA drivers out-of-the-box?', 'created_at': datetime.datetime(2024, 12, 9, 7, 29, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539680347, 'issue_id': 2705514168, 'author': 'chanangaza', 'body': '> Hi, Thank you for the answer. I presume that by ""standard GPU"" you mean the `tensorflow:2.18.0-gpu` tag (without `-jupyter`), so I tried the `tensorflow:2.18.0-gpu` tag and we get the same result of no GPU. See error logs below:\r\n> \r\n> 2024-12-06 09:20:48.539413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1733476848.5606[41] 1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1733476848.566966 1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 2024-12-06 09:20:48.589059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. W0000 00:00:1733476850.581204 1 gpu_device.cc:23[44] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 0\r\n> \r\n> We have found a workaround for the issue by installing `tensorflow[and-cuda]` in `requirements.txt` when using the tag `tensorflow:2.18-gpu-jupyter`, but in the long run this will cause more manual upkeep of the image when upgrading versions.\r\n\r\ncan confirm this works, though the docker image size is then x2...', 'created_at': datetime.datetime(2024, 12, 12, 18, 3, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557433048, 'issue_id': 2705514168, 'author': 'arroyomejias', 'body': 'Same issue with `tensorflow:2.18.0-gpu`.', 'created_at': datetime.datetime(2024, 12, 20, 17, 30, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558902005, 'issue_id': 2705514168, 'author': 'Venkat6871', 'body': 'Hi **@SandraAnder** ,\r\nApologies for the delay, and thank you for your patience. A similar issue is already open. Please follow that issue for further updates.\r\n#80538\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 4, 47, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569204382, 'issue_id': 2705514168, 'author': 'SandraAnder', 'body': ""Thanks @Venkat6871!\r\n\r\nI'll close this as a duplicate."", 'created_at': datetime.datetime(2025, 1, 3, 13, 13, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569204420, 'issue_id': 2705514168, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81344"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81344"">No</a>', 'created_at': datetime.datetime(2025, 1, 3, 13, 13, 18, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-04 06:17:25 UTC): Hi **@SandraAnder** ,
Apologies for the delay, and thank you for raising your concern. Could you please try using a standard GPU instead of tensorflow:2.18-gpu-jupyter? Let us know if you are still unable to use the GPU.
Thank you!

SandraAnder (Issue Creator) on (2024-12-06 10:07:28 UTC): Hi,
Thank you for the answer. I presume that by ""standard GPU"" you mean the `tensorflow:2.18.0-gpu` tag (without `-jupyter`), so I tried the `tensorflow:2.18.0-gpu` tag and we get the same result of no GPU. See error logs below:

2024-12-06 09:20:48.539413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1733476848.5606[41]    1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733476848.566966       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-06 09:20:48.589059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1733476850.581204       1 gpu_device.cc:23[44] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
0

We have found a workaround for the issue by installing `tensorflow[and-cuda]` in `requirements.txt` when using the tag `tensorflow:2.18-gpu-jupyter`, but in the long run this will cause more manual upkeep of the image when upgrading versions.

luksi1 on (2024-12-09 07:29:16 UTC): I assume both the ""gpu"" and ""gpu-jupyter"" images are supposed to contain the new CUDA drivers out-of-the-box?

chanangaza on (2024-12-12 18:03:16 UTC): can confirm this works, though the docker image size is then x2...

arroyomejias on (2024-12-20 17:30:35 UTC): Same issue with `tensorflow:2.18.0-gpu`.

Venkat6871 (Assginee) on (2024-12-23 04:47:49 UTC): Hi **@SandraAnder** ,
Apologies for the delay, and thank you for your patience. A similar issue is already open. Please follow that issue for further updates.
#80538

Thank you!

SandraAnder (Issue Creator) on (2025-01-03 13:13:16 UTC): Thanks @Venkat6871!

I'll close this as a duplicate.

google-ml-butler[bot] on (2025-01-03 13:13:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81344"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/81344"">No</a>

"
2705275379,issue,closed,completed,There is no way to install the gpu version of tensorflow on Windows 11,"There is no way to install the gpu version of tensorflow on Windows 11.
Every time I use pip to install tensorflow, it installs tensorflow_intel for me, which makes it impossible for me to use gpu to train models. I have an nvidia 4060 laptop, and cuda and cudnn are configured.
![ 2024-11-29 212231](https://github.com/user-attachments/assets/4ef3dde0-052d-4174-b57d-4f36fd1a50c5)
And tf cannot recognize the gpu
![ 2024-11-29 223020](https://github.com/user-attachments/assets/3e82d6f0-2b1a-4983-8870-2fe1a5abc72d)
",iwqculrbud,2024-11-29 14:32:26+00:00,['Venkat6871'],2024-11-29 16:40:33+00:00,2024-11-29 16:40:33+00:00,https://github.com/tensorflow/tensorflow/issues/81340,[],[],
2704157852,issue,open,,MixedPrecision + XLA: Seen floating point types of different precisions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.
Without bilinear interpolation or without XLA or without mixed_float16 there is no issue.

In Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-9cc273be2d5a> in <cell line: 28>()
     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)
     27 
---> 28 model.fit(dataset)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-4-9cc273be2d5a>"", line 28, in <cell line: 28>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 368, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 216, in function

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 129, in multi_step_on_iterator

during context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=""Mul"" op_name=""mul_9"" source_file=""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"" source_line=1196}, but mixed precision is disallowed.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]
```
",shkarupa-alex,2024-11-29 07:11:51+00:00,['reedwm'],2024-12-26 12:18:03+00:00,,https://github.com/tensorflow/tensorflow/issues/81273,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:xla', 'XLA'), ('TF 2.18', '')]","[{'comment_id': 2513542248, 'issue_id': 2704157852, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow 2.18.0 version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/7d4ab22c52d5d809c848d6f104219f36/81273_tf-2-18-0-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 3, 4, 45, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562602807, 'issue_id': 2704157852, 'author': 'shkarupa-alex', 'body': 'Any news on this issue?', 'created_at': datetime.datetime(2024, 12, 26, 12, 18, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-12-03 04:45:54 UTC): I tried running your code on Colab using TensorFlow 2.18.0 version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/7d4ab22c52d5d809c848d6f104219f36/81273_tf-2-18-0-v.ipynb) here for reference.

Thank you!

shkarupa-alex (Issue Creator) on (2024-12-26 12:18:02 UTC): Any news on this issue?

"
2702429958,issue,closed,completed,T,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",Tina1L,2024-11-28 14:48:54+00:00,['gaikwadrahul8'],2024-12-15 08:21:09+00:00,2024-12-15 08:21:09+00:00,https://github.com/tensorflow/tensorflow/issues/81213,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2511112709, 'issue_id': 2702429958, 'author': 'gaikwadrahul8', 'body': 'Hi, @Tina1L\r\n\r\nI apologize for the delayed response, if possible could you please explain and elaborate more about your issue it seems like you just created empty issue template with **TensorFlow Lite Converter Issue** option ?\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 2, 10, 14, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047587, 'issue_id': 2702429958, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543553766, 'issue_id': 2702429958, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2024, 12, 15, 8, 21, 9, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-02 10:14:46 UTC): Hi, @Tina1L

I apologize for the delayed response, if possible could you please explain and elaborate more about your issue it seems like you just created empty issue template with **TensorFlow Lite Converter Issue** option ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-10 02:09:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mihaimaruseac on (2024-12-15 08:21:09 UTC): Please don't spam

"
2701137210,issue,closed,completed,ASM ERROR TENSORFLOW LITE DELEGATES FLEX,"ERROR: /mnt/c/Rishik/tensorflow-2.17.0/tensorflow/core/kernels/BUILD:6319:11: Compiling tensorflow/core/kernels/meta_support.cc failed: (Exit 1): arm-none-linux-gnueabihf-gcc failed: error executing command (from target //tensorflow/core/kernels:meta_support) /mnt/c/Users/Rishik/Desktop/bazel_lite/52dfed56af7b723f8fecdd2d8ef8c52a/external/armhf_linux_toolchain/bin/arm-none-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 137 arguments skipped)
In file included from external/gemmlowp/meta/streams.h:307,
                 from external/gemmlowp/meta/quantized_mul_kernels.h:22,
                 from ./tensorflow/core/kernels/meta_support.h:21,
                 from tensorflow/core/kernels/meta_support.cc:18:
external/gemmlowp/meta/streams_arm_32.h: In static member function 'static void gemmlowp::meta::GemmExecutorPackRHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 2; int n = 4; int k = 8; int m_leftovers = 0; int n_leftovers = 0; int k_leftovers = 0]':
external/gemmlowp/meta/streams_arm_32.h:1535:3: error: 'asm' operand has impossible constraints
 1535 |   asm volatile(
      |   ^~~
Target //tensorflow/core/kernels:meta_support failed to build

Above error is shown for the bazel command
 bazel --output_user_root=/mnt/c/Users/Rishik/Desktop/bazel_lite build --config=elinux_armhf -c opt --copt=-march=armv7-a --copt=-mfl
oat-abi=hard --repo_env=TF_PYTHON_VERSION=3.11 //tensorflow/core/kernels:meta_support
",rishik1001,2024-11-28 07:32:53+00:00,['Venkat6871'],2024-12-07 19:47:04+00:00,2024-12-02 07:10:08+00:00,https://github.com/tensorflow/tensorflow/issues/81013,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2510563609, 'issue_id': 2701137210, 'author': 'Venkat6871', 'body': 'Hi **@rishik1001** ,\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 2, 4, 38, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525291456, 'issue_id': 2701137210, 'author': 'ghost', 'body': 'I have the same error. Does anyone have a possible solution?', 'created_at': datetime.datetime(2024, 12, 7, 19, 47, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-12-02 04:38:49 UTC): Hi **@rishik1001** ,
We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

ghost on (2024-12-07 19:47:03 UTC): I have the same error. Does anyone have a possible solution?

"
2700408435,issue,closed,completed," Windows Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\Users\mi\scoop\apps\msys2\current\usr\bin\bash.exe""?","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.6

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

3.7.2

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

errorfail

### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package --config=opt --jobs=12 --local_ram_resources=8192 --local_cpu_resources=12
```


### Relevant log output

```shell
PS D:\project\cTensorflow\tensorflow-1>   bazel build //tensorflow/tools/pip_package:build_pip_package --config=opt --jobs=12 --local_ram_resources=8192 --local_cpu_resources=12
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=154
INFO: Reading rc options for 'build' from d:\project\ctensorflow\tensorflow-1\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=D:/mi/miniforge3/envs/py3.9/python.exe
INFO: Reading rc options for 'build' from d:\project\ctensorflow\tensorflow-1\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from d:\project\ctensorflow\tensorflow-1\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=D:/mi/miniforge3/envs/py3.9/python.exe --action_env PYTHON_LIB_PATH=D:/mi/miniforge3/envs/py3.9/lib/site-packages --python_path=D:/mi/miniforge3/envs/py3.9/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\project\ctensorflow\tensorflow-1\.bazelrc: --output_filter=DONT_MATCH_ANYTHING       
INFO: Found applicable config definition build:v2 in file d:\project\ctensorflow\tensorflow-1\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\project\ctensorflow\tensorflow-1\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\project\ctensorflow\tensorflow-1\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\project\ctensorflow\tensorflow-1\.bazelrc: --define framework_shared_object=false    
DEBUG: C:/users/mi/_bazel_mi/7smei6lj/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary 
software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""DEBUG: Repository io_bazel_rules_docker instantiated at:
  D:/project/ctensorflow/tensorflow-1/WORKSPACE:23:14: in <toplevel>
  D:/project/ctensorflow/tensorflow-1/tensorflow/workspace0.bzl:108:34: in workspace
  C:/users/mi/_bazel_mi/7smei6lj/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  C:/users/mi/_bazel_mi/7smei6lj/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (431 packages loaded, 29424 targets configured).
INFO: Found 1 target...
ERROR: C:/users/mi/_bazel_mi/7smei6lj/external/com_google_protobuf/BUILD:855:26: Executing genrule @com_google_protobuf//:protos_python_genrule failed (Exit -1): bash.exe failed: error executing command
  cd C:/users/mi/_bazel_mi/7smei6lj/execroot/org_tensorflow
  SET PATH=C:\Users\mi\scoop\apps\msys2\current\usr\bin;C:\Users\mi\scoop\apps\msys2\current\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;D:\mi\miniforge3\envs\py3.9;D:\mi\miniforge3\envs\py3.9\Library\mingw-w64\bin;D:\mi\miniforge3\envs\py3.9\Library\usr\bin;D:\mi\miniforge3\envs\py3.9\Library\bin;D:\mi\miniforge3\envs\py3.9\Scripts;D:\mi\miniforge3\envs\py3.9\bin;D:\mi\miniforge3\condabin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\Bandizip;d:\Program Files\Git\cmd;C:\Program 
Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;.;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;.;D:\mi\miniforge3\envs\py3.9;D:\mi\miniforge3\envs\py3.9\Library\mingw-w64\bin;D:\mi\miniforge3\envs\py3.9\Library\usr\bin;D:\mi\miniforge3\envs\py3.9\Library\bin;D:\mi\miniforge3\envs\py3.9\Scripts;D:\mi\miniforge3\envs\py3.9\bin;D:\mi\miniforge3\condabin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\Bandizip;d:\Program Files\Git\cmd;C:\Program Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;.;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;.;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Bandizip\;d:\Program Files\Git\cmd;C:\Program Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY\;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;
    SET PYTHON_BIN_PATH=D:/mi/miniforge3/envs/py3.9/python.exe
    SET PYTHON_LIB_PATH=D:/mi/miniforge3/envs/py3.9/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  C:/Users/mi/scoop/apps/msys2/current/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; cp external/com_google_protobuf/src/google/protobuf/any.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/any.proto && cp external/com_google_protobuf/src/google/protobuf/api.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/api.proto && cp external/com_google_protobuf/src/google/protobuf/compiler/plugin.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/compiler/plugin.proto && cp external/com_google_protobuf/src/google/protobuf/descriptor.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/descriptor.proto && cp external/com_google_protobuf/src/google/protobuf/duration.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/duration.proto && cp external/com_google_protobuf/src/google/protobuf/empty.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/empty.proto && cp external/com_google_protobuf/src/google/protobuf/field_mask.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/field_mask.proto && cp external/com_google_protobuf/src/google/protobuf/source_context.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/source_context.proto && cp external/com_google_protobuf/src/google/protobuf/struct.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/struct.proto && cp external/com_google_protobuf/src/google/protobuf/timestamp.proto 
bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/timestamp.proto && cp external/com_google_protobuf/src/google/protobuf/type.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/type.proto && cp external/com_google_protobuf/src/google/protobuf/wrappers.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/wrappers.proto
Execution platform: @local_execution_config_platform//:platform. Note: Remote connection/protocol failed with: execution failed
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\Users\mi\scoop\apps\msys2\current\usr\bin\bash.exe"" -c ""source external/bazel_tools/tools/genrule/genrule-setup.sh; cp external/com_google_protobuf/src/google/protobuf/any.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/any.proto && cp external/com_google_protobuf/src/google/protobuf/api.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/api.proto && cp external/com_google_protobuf/src/google/protobuf/compiler/(...)):  
?
 (error: 2)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: D:/project/ctensorflow/tensorflow-1/tensorflow/compiler/tf2xla/BUILD:74:17 Executing genrule @com_google_protobuf//:protos_python_genrule failed (Exit -1): bash.exe failed: error executing command
  cd C:/users/mi/_bazel_mi/7smei6lj/execroot/org_tensorflow
  SET PATH=C:\Users\mi\scoop\apps\msys2\current\usr\bin;C:\Users\mi\scoop\apps\msys2\current\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;D:\mi\miniforge3\envs\py3.9;D:\mi\miniforge3\envs\py3.9\Library\mingw-w64\bin;D:\mi\miniforge3\envs\py3.9\Library\usr\bin;D:\mi\miniforge3\envs\py3.9\Library\bin;D:\mi\miniforge3\envs\py3.9\Scripts;D:\mi\miniforge3\envs\py3.9\bin;D:\mi\miniforge3\condabin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\Bandizip;d:\Program Files\Git\cmd;C:\Program 
Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;.;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;.;D:\mi\miniforge3\envs\py3.9;D:\mi\miniforge3\envs\py3.9\Library\mingw-w64\bin;D:\mi\miniforge3\envs\py3.9\Library\usr\bin;D:\mi\miniforge3\envs\py3.9\Library\bin;D:\mi\miniforge3\envs\py3.9\Scripts;D:\mi\miniforge3\envs\py3.9\bin;D:\mi\miniforge3\condabin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\Bandizip;d:\Program Files\Git\cmd;C:\Program Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;.;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;.;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Bandizip\;d:\Program Files\Git\cmd;C:\Program Files\CorpLink\current\module\mdm\x64\policy\bin;D:\Users\mi\miniconda3;D:\mi\miniconda3;D:\mi\miniconda3\Library\include;D:\mi\miniconda3\Scripts;D:\Program Files\PuTTY\;C:\Program Files\Docker\Docker\resources\bin;D:\project\cTensorflow\bazel;D:\project\cTensorflow;D:\project\cTensorflow\linkC\mingw64\bin;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\lib;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0;D:\project\cTensorflow\linkC\libtensorflow-cpu-windows-x86_64-2.6.0\include;C:\Users\mi\scoop\shims;d:\mi\miniforge3;d:\mi\miniforge3\Library\mingw-w64\bin;d:\mi\miniforge3\Library\usr\bin;d:\mi\miniforge3\Library\bin;d:\mi\miniforge3\Scripts;C:\Users\mi\AppData\Local\Microsoft\WindowsApps;D:\Program Files\Microsoft VS Code\bin;D:\Program Files\WitmemStudio;d:\Program Files\JetBrains\CLion 2024.2.2\bin;;d:\Program Files\JetBrains\PyCharm 2024.2.3\bin;
    SET PYTHON_BIN_PATH=D:/mi/miniforge3/envs/py3.9/python.exe
    SET PYTHON_LIB_PATH=D:/mi/miniforge3/envs/py3.9/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  C:/Users/mi/scoop/apps/msys2/current/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; cp external/com_google_protobuf/src/google/protobuf/any.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/any.proto && cp external/com_google_protobuf/src/google/protobuf/api.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/api.proto && cp external/com_google_protobuf/src/google/protobuf/compiler/plugin.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/compiler/plugin.proto && cp external/com_google_protobuf/src/google/protobuf/descriptor.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/descriptor.proto && cp external/com_google_protobuf/src/google/protobuf/duration.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/duration.proto && cp external/com_google_protobuf/src/google/protobuf/empty.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/empty.proto && cp external/com_google_protobuf/src/google/protobuf/field_mask.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/field_mask.proto && cp external/com_google_protobuf/src/google/protobuf/source_context.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/source_context.proto && cp external/com_google_protobuf/src/google/protobuf/struct.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/struct.proto && cp external/com_google_protobuf/src/google/protobuf/timestamp.proto 
bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/timestamp.proto && cp external/com_google_protobuf/src/google/protobuf/type.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/type.proto && cp external/com_google_protobuf/src/google/protobuf/wrappers.proto bazel-out/x64_windows-opt/bin/external/com_google_protobuf/python/google/protobuf/wrappers.proto
Execution platform: @local_execution_config_platform//:platform. Note: Remote connection/protocol failed with: execution failed
INFO: Elapsed time: 154.535s, Critical Path: 9.49s
INFO: 187 processes: 28 internal, 159 local.
FAILED: Build did NOT complete successfully
```
",Kiritor729,2024-11-28 01:38:57+00:00,['tilakrayal'],2024-12-20 02:01:56+00:00,2024-12-20 02:01:52+00:00,https://github.com/tensorflow/tensorflow/issues/80989,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.6.0', '')]","[{'comment_id': 2505103702, 'issue_id': 2700408435, 'author': 'Kiritor729', 'body': '?C:/Users/mi/scoop/apps/msys2/current/usr/bin/bash.exebashmsysmsysscoopmsysmsysbashbashpath', 'created_at': datetime.datetime(2024, 11, 28, 1, 42, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516297944, 'issue_id': 2700408435, 'author': 'tilakrayal', 'body': '@Kiritor729,\r\nTensorflow version 2.6 is not actively supported. Hence, kindly update to the latest stable version 2.18.0 and let us know if you are facing the same issue. Also could you try **bazel clean --expunge** followed by bazel sync.  \r\nhttps://www.tensorflow.org/install/source_windows#cpu\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 4, 6, 19, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594244, 'issue_id': 2700408435, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 12, 2, 7, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556113111, 'issue_id': 2700408435, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556113192, 'issue_id': 2700408435, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80989"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80989"">No</a>', 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 55, tzinfo=datetime.timezone.utc)}]","Kiritor729 (Issue Creator) on (2024-11-28 01:42:31 UTC): ?C:/Users/mi/scoop/apps/msys2/current/usr/bin/bash.exebashmsysmsysscoopmsysmsysbashbashpath

tilakrayal (Assginee) on (2024-12-04 06:19:47 UTC): @Kiritor729,
Tensorflow version 2.6 is not actively supported. Hence, kindly update to the latest stable version 2.18.0 and let us know if you are facing the same issue. Also could you try **bazel clean --expunge** followed by bazel sync.  
https://www.tensorflow.org/install/source_windows#cpu

Thank you!

github-actions[bot] on (2024-12-12 02:07:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-20 02:01:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-20 02:01:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80989"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80989"">No</a>

"
2698915980,issue,closed,completed,InvalidArgumentError when using MirroredStrategy but not with tf.distribute.get_strategy(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

gcc 9.4.0

### CUDA/cuDNN version

12.5.1

### GPU model and memory

2 x NVIDIA Tesla V100 (12 cores, 224 GB RAM, 1474 GB disk)

### Current behavior?

**Problem**: I am encountering an InvalidArgumentError when using MirroredStrategy in TensorFlow. This error does not occur when I use tf.distribute.get_strategy(). Below is the error message I receive and the standalone code to reproduce it.

**Expected Behavior**: The code should run without errors when using MirroredStrategy, similar to when using tf.distribute.get_strategy().

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras

def create_dataset():
    float_data = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)
    string_data = tf.constant([[""foo"", ""bar""], [""baz"", ""qux""]], dtype=tf.string)
    labels = tf.constant([[1], [0]], dtype=tf.float32)
    
    dataset = tf.data.Dataset.from_tensor_slices(((float_data, string_data), labels))
    return dataset

def create_model():
    input_float = keras.Input(shape=(2,), dtype=tf.float32, name='float_input')
    input_string = keras.Input(shape=(2,), dtype=tf.string, name='string_input')
    
    string_lookup = keras.layers.StringLookup(vocabulary=[""foo"", ""bar"", ""baz"", ""qux""], name='string_lookup')
    string_embedding = string_lookup(input_string)
    
    concatenated = keras.layers.Concatenate(name='concatenate')([input_float, string_embedding])
    
    dense = keras.layers.Dense(10, activation='relu', name='dense_1')(concatenated)
    output = keras.layers.Dense(1, activation='sigmoid', name='output')(dense)
    
    model = keras.Model(inputs=[input_float, input_string], outputs=output, name='simple_model')
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def main():
    print(""Single GPU strategy"")
    strategy = tf.distribute.get_strategy()
    
    dataset = create_dataset()
    
    with strategy.scope():
        model = create_model()
        
    model.fit(dataset.batch(2), epochs=5)

    print(""Multiple GPUs strategy"")

    strategy = tf.distribute.MirroredStrategy()
    
    dataset = create_dataset()
    
    with strategy.scope():
        model = create_model()
        
    model.fit(dataset.batch(2), epochs=5)

if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: float, double, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, qint16, quint16, uint16, complex128, half, uint32, uint64, variant
        ; NodeDef: {{node AddN}}; Op<name=AddN; signature=inputs:N*T -> sum:T; attr=N:int,min=1; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_VARIANT]; is_commutative=true; is_aggregate=true> [Op:AddN] name:
```
",ArnauPujantellNavas,2024-11-27 15:25:37+00:00,['gaikwadrahul8'],2024-12-28 01:59:53+00:00,2024-12-28 01:59:51+00:00,https://github.com/tensorflow/tensorflow/issues/80951,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2507501701, 'issue_id': 2698915980, 'author': 'Venkat6871', 'body': 'Hi **@ArnauPujantellNavas** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e75da5abb3d644c36a0927ebccaac89f/80951_tf_2-18-v.ipynb) here for your reference. Let me know if I missed anything or made any mistakes.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 29, 10, 18, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507520758, 'issue_id': 2698915980, 'author': 'ArnauPujantellNavas', 'body': ""Hi @Venkat6871, thank you for your answer.\r\nWhen using one single GPU it works fine, same when using more than one GPU and tf.distribute.get_strategy(). The error shows up when using more than one GPU and MirroredStrategy(). In my case, I am using 2 NVIDIA Tesla V100. \r\nSorry if i didn't explain the issue correctly."", 'created_at': datetime.datetime(2024, 11, 29, 10, 28, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528086312, 'issue_id': 2698915980, 'author': 'gaikwadrahul8', 'body': ""Hi, @ArnauPujantellNavas \r\nI apologize for the delayed response, I have tried your code with 02 T4 GPU's and it seems like working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/d49c8cd6fc06da58cd64486e767b9d7e/tf-issue-80951.ipynb) or Am I missing something here if so please let me know ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 9, 14, 17, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528475343, 'issue_id': 2698915980, 'author': 'ArnauPujantellNavas', 'body': 'Hi @gaikwadrahul8 \r\nThanks for your response, looking at the file I\'ve noticed we are using different versions of the packages. When I try to use yours tensorflow can\'t detect my GPUs properly.\r\n\r\nThis is the command I am using to install the required packages with the latest versions:\r\n`pip install tensorflow[and-cuda]==2.18.0 keras==3.7.0`\r\n\r\nI added some prints to the code to make sure it is using the 2 replicas and not just one of them:\r\n```\r\nimport tensorflow as tf\r\nimport keras\r\n\r\ndef create_dataset():\r\n    float_data = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\r\n    string_data = tf.constant([[""foo"", ""bar""], [""baz"", ""qux""]], dtype=tf.string)\r\n    labels = tf.constant([[1], [0]], dtype=tf.float32)\r\n    \r\n    dataset = tf.data.Dataset.from_tensor_slices(((float_data, string_data), labels))\r\n    return dataset\r\n\r\ndef create_model():\r\n    input_float = keras.Input(shape=(2,), dtype=tf.float32, name=\'float_input\')\r\n    input_string = keras.Input(shape=(2,), dtype=tf.string, name=\'string_input\')\r\n    \r\n    string_lookup = keras.layers.StringLookup(vocabulary=[""foo"", ""bar"", ""baz"", ""qux""], name=\'string_lookup\')\r\n    string_embedding = string_lookup(input_string)\r\n    \r\n    concatenated = keras.layers.Concatenate(name=\'concatenate\')([input_float, string_embedding])\r\n    \r\n    dense = keras.layers.Dense(10, activation=\'relu\', name=\'dense_1\')(concatenated)\r\n    output = keras.layers.Dense(1, activation=\'sigmoid\', name=\'output\')(dense)\r\n    \r\n    model = keras.Model(inputs=[input_float, input_string], outputs=output, name=\'simple_model\')\r\n    model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\r\n    return model\r\n\r\ndef main():\r\n\r\n    print(""Single GPU strategy"")\r\n\r\n    strategy = tf.distribute.get_strategy()\r\n    n_gpu: int = len(tf.config.list_physical_devices(\'GPU\'))\r\n    n_replicas: int = strategy.num_replicas_in_sync\r\n \r\n    print(f\'GPU: {n_gpu}\')\r\n    print(f\'Replicas: {n_replicas}\')\r\n    \r\n    dataset = create_dataset()\r\n    \r\n    with strategy.scope():\r\n        model = create_model()\r\n        \r\n    model.fit(dataset.batch(2), epochs=5)\r\n\r\n    print(""Multiple GPUs strategy"")\r\n\r\n    strategy = tf.distribute.MirroredStrategy()\r\n\r\n    n_gpu: int = len(tf.config.list_physical_devices(\'GPU\'))\r\n    n_replicas: int = strategy.num_replicas_in_sync\r\n \r\n    print(f\'GPU: {n_gpu}\')\r\n    print(f\'Replicas: {n_replicas}\')\r\n    \r\n    dataset = create_dataset()\r\n    \r\n    with strategy.scope():\r\n        model = create_model()\r\n        \r\n    model.fit(dataset.batch(2), epochs=5)\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n\r\n```\r\n\r\nAnd I am recieving the same original error:\r\n```\r\nSingle GPU strategy\r\nGPU: 2\r\nReplicas: 1\r\nI0000 00:00:1733759297.693816   24170 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0\r\nI0000 00:00:1733759297.694337   24170 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14791 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0002:00:00.0, compute capability: 7.0\r\nEpoch 1/5\r\n1/1  2s 2s/step - accuracy: 0.5000 - loss: 0.9865\r\nEpoch 2/5\r\n1/1  0s 11ms/step - accuracy: 0.5000 - loss: 0.9687\r\nEpoch 3/5\r\n1/1  0s 9ms/step - accuracy: 0.5000 - loss: 0.9513\r\nEpoch 4/5\r\n1/1  0s 9ms/step - accuracy: 0.5000 - loss: 0.9342\r\nEpoch 5/5\r\n1/1  0s 10ms/step - accuracy: 0.5000 - loss: 0.9174\r\nMultiple GPUs strategy\r\nGPU: 2\r\nReplicas: 2\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr \'T\' of string is not in the list of allowed values: float, double, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, qint16, quint16, uint16, complex128, half, uint32, uint64, variant\r\n        ; NodeDef: {{node AddN}}; Op<name=AddN; signature=inputs:N*T -> sum:T; attr=N:int,min=1; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_VARIANT]; is_commutative=true; is_aggregate=true> [Op:AddN] name:\r\n```\r\n\r\nCoult you try to execute it with those prints and the same package versions as me?\r\nThank you so much and sorry for the inconvenience.', 'created_at': datetime.datetime(2024, 12, 9, 15, 51, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531536662, 'issue_id': 2698915980, 'author': 'gaikwadrahul8', 'body': ""Hi @ArnauPujantellNavas,\r\n\r\nAfter thorough investigation and version compatibility testing, I've identified a reliable solution for the encountered issue.\r\nInitially, I attempted installation with `pip install tensorflow[and-cuda]==2.18.0 keras==3.7.0` which reproduced the previous error scenario which you mentioned in the issue template. I discovered a compatible combination that resolves the current challenges so please try below recommended temporary workaround at the moment, please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/60a77b7a78c0d95fa107b01defe086e8/tf-mirroredstrategy-issue-80951.ipynb)\r\n\r\n`pip install tensorflow[and-cuda]==2.16.1 keras==3.3.0\r\n`\r\n\r\nIf issue still persists with this `pip install tensorflow[and-cuda]==2.16.1 keras==3.3.0` versions combination please let us know for further investigation.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 10, 12, 43, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538277430, 'issue_id': 2698915980, 'author': 'ArnauPujantellNavas', 'body': ""Hi @gaikwadrahul8,\r\nI've tested it and it works as a workaround. Are you planning to fix it for the next releases?\r\n\r\nThank you so much for your time."", 'created_at': datetime.datetime(2024, 12, 12, 9, 7, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539081049, 'issue_id': 2698915980, 'author': 'gaikwadrahul8', 'body': ""Hi, @ArnauPujantellNavas\r\n\r\nThank you for your confirmation. We're glad to hear that the issue has been resolved by installing TensorFlow and Keras versions `2.16.1` and `3.3.0` respectively using `pip`.\r\n\r\nWe'll investigate this issue further and work on a permanent fix for an upcoming release.\r\n\r\nPlease let us know if you encounter any further problems. If the temporary workaround has successfully resolved the issue, feel free to close this issue for now.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 12, 14, 16, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556113146, 'issue_id': 2698915980, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 20, 2, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137801, 'issue_id': 2698915980, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564137822, 'issue_id': 2698915980, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80951"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80951"">No</a>', 'created_at': datetime.datetime(2024, 12, 28, 1, 59, 53, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-11-29 10:18:08 UTC): Hi **@ArnauPujantellNavas** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0, and I did not encounter any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e75da5abb3d644c36a0927ebccaac89f/80951_tf_2-18-v.ipynb) here for your reference. Let me know if I missed anything or made any mistakes.
Thank you!

ArnauPujantellNavas (Issue Creator) on (2024-11-29 10:28:15 UTC): Hi @Venkat6871, thank you for your answer.
When using one single GPU it works fine, same when using more than one GPU and tf.distribute.get_strategy(). The error shows up when using more than one GPU and MirroredStrategy(). In my case, I am using 2 NVIDIA Tesla V100. 
Sorry if i didn't explain the issue correctly.

gaikwadrahul8 (Assginee) on (2024-12-09 14:17:31 UTC): Hi, @ArnauPujantellNavas 
I apologize for the delayed response, I have tried your code with 02 T4 GPU's and it seems like working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/d49c8cd6fc06da58cd64486e767b9d7e/tf-issue-80951.ipynb) or Am I missing something here if so please let me know ? 

Thank you for your cooperation and patience.

ArnauPujantellNavas (Issue Creator) on (2024-12-09 15:51:49 UTC): Hi @gaikwadrahul8 
Thanks for your response, looking at the file I've noticed we are using different versions of the packages. When I try to use yours tensorflow can't detect my GPUs properly.

This is the command I am using to install the required packages with the latest versions:
`pip install tensorflow[and-cuda]==2.18.0 keras==3.7.0`

I added some prints to the code to make sure it is using the 2 replicas and not just one of them:
```
import tensorflow as tf
import keras

def create_dataset():
    float_data = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)
    string_data = tf.constant([[""foo"", ""bar""], [""baz"", ""qux""]], dtype=tf.string)
    labels = tf.constant([[1], [0]], dtype=tf.float32)
    
    dataset = tf.data.Dataset.from_tensor_slices(((float_data, string_data), labels))
    return dataset

def create_model():
    input_float = keras.Input(shape=(2,), dtype=tf.float32, name='float_input')
    input_string = keras.Input(shape=(2,), dtype=tf.string, name='string_input')
    
    string_lookup = keras.layers.StringLookup(vocabulary=[""foo"", ""bar"", ""baz"", ""qux""], name='string_lookup')
    string_embedding = string_lookup(input_string)
    
    concatenated = keras.layers.Concatenate(name='concatenate')([input_float, string_embedding])
    
    dense = keras.layers.Dense(10, activation='relu', name='dense_1')(concatenated)
    output = keras.layers.Dense(1, activation='sigmoid', name='output')(dense)
    
    model = keras.Model(inputs=[input_float, input_string], outputs=output, name='simple_model')
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def main():

    print(""Single GPU strategy"")

    strategy = tf.distribute.get_strategy()
    n_gpu: int = len(tf.config.list_physical_devices('GPU'))
    n_replicas: int = strategy.num_replicas_in_sync
 
    print(f'GPU: {n_gpu}')
    print(f'Replicas: {n_replicas}')
    
    dataset = create_dataset()
    
    with strategy.scope():
        model = create_model()
        
    model.fit(dataset.batch(2), epochs=5)

    print(""Multiple GPUs strategy"")

    strategy = tf.distribute.MirroredStrategy()

    n_gpu: int = len(tf.config.list_physical_devices('GPU'))
    n_replicas: int = strategy.num_replicas_in_sync
 
    print(f'GPU: {n_gpu}')
    print(f'Replicas: {n_replicas}')
    
    dataset = create_dataset()
    
    with strategy.scope():
        model = create_model()
        
    model.fit(dataset.batch(2), epochs=5)

if __name__ == ""__main__"":
    main()

```

And I am recieving the same original error:
```
Single GPU strategy
GPU: 2
Replicas: 1
I0000 00:00:1733759297.693816   24170 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0
I0000 00:00:1733759297.694337   24170 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14791 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0002:00:00.0, compute capability: 7.0
Epoch 1/5
1/1  2s 2s/step - accuracy: 0.5000 - loss: 0.9865
Epoch 2/5
1/1  0s 11ms/step - accuracy: 0.5000 - loss: 0.9687
Epoch 3/5
1/1  0s 9ms/step - accuracy: 0.5000 - loss: 0.9513
Epoch 4/5
1/1  0s 9ms/step - accuracy: 0.5000 - loss: 0.9342
Epoch 5/5
1/1  0s 10ms/step - accuracy: 0.5000 - loss: 0.9174
Multiple GPUs strategy
GPU: 2
Replicas: 2
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: float, double, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, qint16, quint16, uint16, complex128, half, uint32, uint64, variant
        ; NodeDef: {{node AddN}}; Op<name=AddN; signature=inputs:N*T -> sum:T; attr=N:int,min=1; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_VARIANT]; is_commutative=true; is_aggregate=true> [Op:AddN] name:
```

Coult you try to execute it with those prints and the same package versions as me?
Thank you so much and sorry for the inconvenience.

gaikwadrahul8 (Assginee) on (2024-12-10 12:43:39 UTC): Hi @ArnauPujantellNavas,

After thorough investigation and version compatibility testing, I've identified a reliable solution for the encountered issue.
Initially, I attempted installation with `pip install tensorflow[and-cuda]==2.18.0 keras==3.7.0` which reproduced the previous error scenario which you mentioned in the issue template. I discovered a compatible combination that resolves the current challenges so please try below recommended temporary workaround at the moment, please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/60a77b7a78c0d95fa107b01defe086e8/tf-mirroredstrategy-issue-80951.ipynb)

`pip install tensorflow[and-cuda]==2.16.1 keras==3.3.0
`

If issue still persists with this `pip install tensorflow[and-cuda]==2.16.1 keras==3.3.0` versions combination please let us know for further investigation.

Thank you for your cooperation and patience.

ArnauPujantellNavas (Issue Creator) on (2024-12-12 09:07:31 UTC): Hi @gaikwadrahul8,
I've tested it and it works as a workaround. Are you planning to fix it for the next releases?

Thank you so much for your time.

gaikwadrahul8 (Assginee) on (2024-12-12 14:16:45 UTC): Hi, @ArnauPujantellNavas

Thank you for your confirmation. We're glad to hear that the issue has been resolved by installing TensorFlow and Keras versions `2.16.1` and `3.3.0` respectively using `pip`.

We'll investigate this issue further and work on a permanent fix for an upcoming release.

Please let us know if you encounter any further problems. If the temporary workaround has successfully resolved the issue, feel free to close this issue for now.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-20 02:01:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-28 01:59:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-28 01:59:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80951"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80951"">No</a>

"
2698486981,issue,open,,Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. 

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf

test_inputs = [
    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),
]

test_apis = [
    tf.math.sin, tf.math.cos, tf.math.tan,
    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean
]

for api in test_apis:
    print(f""Testing {api.__name__}"")
    for x in test_inputs:
        try:
            with tf.device('/CPU'):
              cpu_out = api(x)
              print(f""CPU Output: {cpu_out}"")
            with tf.device('/GPU:0'):
              gpu_out = api(x)
              print(f""GPU Output: {gpu_out}"")
        except Exception as e:
            print(f""Error in {api.__name__}: {e}"")
```


### Relevant log output

```shell
Testing sin
CPU Output: [nan +0.j  0.+infj nan+infj]
GPU Output: [nan+nanj nan+infj nan+nanj]
Testing cos
CPU Output: [nan +0.j inf -0.j inf+nanj]
GPU Output: [nan+nanj inf+nanj nan+nanj]
Testing tan
CPU Output: [nan+0.j  0.+1.j  0.+1.j]
GPU Output: [nan+nanj  0. +1.j  0. +1.j]
Testing sinh
CPU Output: [inf +0.j  0.+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing cosh
CPU Output: [inf +0.j nan +0.j inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing exp
CPU Output: [inf +0.j nan+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing reduce_mean
CPU Output: (inf+infj)
GPU Output: (nan+nanj)
```
",rookieLiu2018,2024-11-27 13:13:05+00:00,['tilakrayal'],2025-02-08 01:58:35+00:00,,https://github.com/tensorflow/tensorflow/issues/80947,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2516246074, 'issue_id': 2698486981, 'author': 'tilakrayal', 'body': '@rookieLiu2018,\r\nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/58479\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 4, 5, 45, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516298041, 'issue_id': 2698486981, 'author': 'rookieLiu2018', 'body': 'Thank you for your reply. Is this considered a bug within the TensorFlow framework? Is there any plan to fix it?', 'created_at': datetime.datetime(2024, 12, 4, 6, 19, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626921960, 'issue_id': 2698486981, 'author': 'tilakrayal', 'body': '@rookieLiu2018,\nLooks like this is not the issue with the Tensorflow side. As mentioned, The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others.\n\n\nThank you!', 'created_at': datetime.datetime(2025, 1, 31, 10, 54, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644425526, 'issue_id': 2698486981, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 2, 8, 1, 58, 34, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-12-04 05:45:40 UTC): @rookieLiu2018,
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.

https://github.com/tensorflow/tensorflow/issues/58479

Thank you!

rookieLiu2018 (Issue Creator) on (2024-12-04 06:19:52 UTC): Thank you for your reply. Is this considered a bug within the TensorFlow framework? Is there any plan to fix it?

tilakrayal (Assginee) on (2025-01-31 10:54:58 UTC): @rookieLiu2018,
Looks like this is not the issue with the Tensorflow side. As mentioned, The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others.


Thank you!

github-actions[bot] on (2025-02-08 01:58:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2697277776,issue,open,,"Very serious! Using this method will definitely result in memory leaks, I hope you can provide support","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        print(f""Training model: {model.name}"")
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
",phpYj,2024-11-27 06:27:34+00:00,"['sagunb', 'reedwm', 'tilakrayal']",2024-12-12 17:55:09+00:00,,https://github.com/tensorflow/tensorflow/issues/80895,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:runtime', 'c++ runtime, performance issues (cpu)'), ('type:performance', 'Performance Issue'), ('TF 2.18', '')]","[{'comment_id': 2503682356, 'issue_id': 2697277776, 'author': 'Pacific1223', 'body': 'Hi, I see this issue is already assigned, but I would love to contribute and learn from this task. Could you please assign it to me as well or let me know if I can assist in any way.', 'created_at': datetime.datetime(2024, 11, 27, 11, 53, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503976643, 'issue_id': 2697277776, 'author': 'tilakrayal', 'body': '@Pacific1223,\r\nPlease raise the PR for contributing to the Tensorflow. Also refer to the official doc for the reference. https://www.tensorflow.org/community/contribute\r\n\r\nhttps://github.com/tensorflow/tensorflow/pulls', 'created_at': datetime.datetime(2024, 11, 27, 14, 9, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504285580, 'issue_id': 2697277776, 'author': 'phpYj', 'body': '@tilakrayal @Pacific1223 \r\nFeedback from Keras, this issue has existed for a long time and has been confirmed to originate from TF. \r\nIf it is resolved, please let me know. Thank you very much\r\n\r\nhttps://github.com/keras-team/keras/issues/20245#issuecomment-2342594184\r\nhttps://github.com/keras-team/tf-keras/issues/286\r\nhttps://github.com/keras-team/keras/issues/20552#issuecomment-2503944388', 'created_at': datetime.datetime(2024, 11, 27, 16, 19, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512326153, 'issue_id': 2697277776, 'author': 'MrChike', 'body': ""Hello @phpYj, so it's ok for me to jump on this issue"", 'created_at': datetime.datetime(2024, 12, 2, 18, 11, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512328260, 'issue_id': 2697277776, 'author': 'MrChike', 'body': '@Pacific1223. Can we work together on this?', 'created_at': datetime.datetime(2024, 12, 2, 18, 12, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521492940, 'issue_id': 2697277776, 'author': 'Pacific1223', 'body': '> @MrChike. Can we work together on this?\r\n\r\nYes. Sure.', 'created_at': datetime.datetime(2024, 12, 5, 21, 29, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522230024, 'issue_id': 2697277776, 'author': 'prempraneethkota', 'body': '```python\r\ndef clear_memory():\r\n    tf.keras.backend.clear_session()\r\n    gc.collect()\r\n```\r\nCould u try adding clear memory and garbage collector and let me know what happens? not quite sure if it will work, but something to try.', 'created_at': datetime.datetime(2024, 12, 6, 6, 17, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523971476, 'issue_id': 2697277776, 'author': 'MrChike', 'body': ""> ```python\r\n> def clear_memory():\r\n>     tf.keras.backend.clear_session()\r\n>     gc.collect()\r\n> ```\r\n> \r\n> Could u try adding clear memory and garbage collector and let me know what happens? not quite sure if it will work, but something to try.\r\n\r\nThis was suggested in one of the links shared above. I did try it and it didn't work."", 'created_at': datetime.datetime(2024, 12, 6, 19, 4, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539642624, 'issue_id': 2697277776, 'author': 'phpYj', 'body': 'tf.keras.backend.clear_session()\r\n    gc.collect()\r\nThis method cannot completely solve the problem. As time goes on, the memory still increases, but it only slightly alleviates the issue', 'created_at': datetime.datetime(2024, 12, 12, 17, 55, 8, tzinfo=datetime.timezone.utc)}]","Pacific1223 on (2024-11-27 11:53:53 UTC): Hi, I see this issue is already assigned, but I would love to contribute and learn from this task. Could you please assign it to me as well or let me know if I can assist in any way.

tilakrayal (Assginee) on (2024-11-27 14:09:49 UTC): @Pacific1223,
Please raise the PR for contributing to the Tensorflow. Also refer to the official doc for the reference. https://www.tensorflow.org/community/contribute

https://github.com/tensorflow/tensorflow/pulls

phpYj (Issue Creator) on (2024-11-27 16:19:25 UTC): @tilakrayal @Pacific1223 
Feedback from Keras, this issue has existed for a long time and has been confirmed to originate from TF. 
If it is resolved, please let me know. Thank you very much

https://github.com/keras-team/keras/issues/20245#issuecomment-2342594184
https://github.com/keras-team/tf-keras/issues/286
https://github.com/keras-team/keras/issues/20552#issuecomment-2503944388

MrChike on (2024-12-02 18:11:27 UTC): Hello @phpYj, so it's ok for me to jump on this issue

MrChike on (2024-12-02 18:12:33 UTC): @Pacific1223. Can we work together on this?

Pacific1223 on (2024-12-05 21:29:01 UTC): Yes. Sure.

prempraneethkota on (2024-12-06 06:17:28 UTC): ```python
def clear_memory():
    tf.keras.backend.clear_session()
    gc.collect()
```
Could u try adding clear memory and garbage collector and let me know what happens? not quite sure if it will work, but something to try.

MrChike on (2024-12-06 19:04:10 UTC): This was suggested in one of the links shared above. I did try it and it didn't work.

phpYj (Issue Creator) on (2024-12-12 17:55:08 UTC): tf.keras.backend.clear_session()
    gc.collect()
This method cannot completely solve the problem. As time goes on, the memory still increases, but it only slightly alleviates the issue

"
2694716072,issue,open,,"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf
import numpy as np

test_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)

# TensorFlow computation
with tf.device('/CPU:0'):
    cpu_out = tf.math.log1p(test_input)

# NumPy computation
numpy_out = np.log1p(test_input.numpy())

print(f""CPU Output: {cpu_out}"")
print(f""NumPy Output: {numpy_out}"")
```


### Relevant log output

```shell
CPU Output: [inf +0.j nan+nanj nan+nanj]
NumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]
```
",rookieLiu2018,2024-11-26 13:36:51+00:00,['gaikwadrahul8'],2024-12-09 13:26:17+00:00,,https://github.com/tensorflow/tensorflow/issues/80850,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2507238169, 'issue_id': 2694716072, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow 2.17.0, 2.18.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ff5869d3cd7f4985c5ba062ae5f18b54/80850_tf_2-17-2-18-nightly-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 29, 7, 33, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507341690, 'issue_id': 2694716072, 'author': 'rookieLiu2018', 'body': 'Thank you for your reply. Is there a plan to resolve this?', 'created_at': datetime.datetime(2024, 11, 29, 8, 47, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527949800, 'issue_id': 2694716072, 'author': 'gaikwadrahul8', 'body': ""Hi, @rookieLiu2018\r\nI apologize for the delayed response, I see `tf.math.log1p` equivalent to `scipy.special.xlog1py` and I tried but it's giving the same results as mentioned in the issue template but when I am converting to numpy it's giving expected result please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/665b1370aebd8c775a4389fa6dfa0896/tf-issue-80850.ipynb) even I tried with equivalent api of Pytorch both numpy and pytorch api's are giving same result so we need to discuss internally this issue and will update you, thank you for bringing this issue to our attention.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5bcaafceaf158ebc29b3b9d9fa2053c91ee3cc51/tensorflow/python/ops/math_ops.py#L5542\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 9, 13, 25, 50, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-11-29 07:33:05 UTC): I tried running your code on Colab using TensorFlow 2.17.0, 2.18.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ff5869d3cd7f4985c5ba062ae5f18b54/80850_tf_2-17-2-18-nightly-v.ipynb) here for reference.
Thank you!

rookieLiu2018 (Issue Creator) on (2024-11-29 08:47:42 UTC): Thank you for your reply. Is there a plan to resolve this?

gaikwadrahul8 (Assginee) on (2024-12-09 13:25:50 UTC): Hi, @rookieLiu2018
I apologize for the delayed response, I see `tf.math.log1p` equivalent to `scipy.special.xlog1py` and I tried but it's giving the same results as mentioned in the issue template but when I am converting to numpy it's giving expected result please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/665b1370aebd8c775a4389fa6dfa0896/tf-issue-80850.ipynb) even I tried with equivalent api of Pytorch both numpy and pytorch api's are giving same result so we need to discuss internally this issue and will update you, thank you for bringing this issue to our attention.

https://github.com/tensorflow/tensorflow/blob/5bcaafceaf158ebc29b3b9d9fa2053c91ee3cc51/tensorflow/python/ops/math_ops.py#L5542

Thank you for your cooperation and patience.

"
2694535611,issue,open,,Heap-buffer-overflow in `SparseMatrixSparseCholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)
values = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)
dense_shape = tf.constant([4, 4], dtype=tf.int64)
input = tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices=indices, values=values, dense_shape=dense_shape, name=None
)
permutation = tf.constant([4,1,1,1], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(
  input=input, permutation=permutation, type=tf.float32
)
```


### Relevant log output

```shell
=================================================================
==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0
WRITE of size 4 at 0x60700049d1d0 thread T0
    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86)
    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #20 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #24 0x51ad66  (/usr/bin/python3.11+0x51ad66)
    #25 0x4e75db in _PyObject_MakeTpCall (/usr/bin/python3.11+0x4e75db)
    #26 0x4fb151 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fb151)
    #27 0x531822 in _PyFunction_Vectorcall (/usr/bin/python3.11+0x531822)
    #28 0x541194 in PyObject_Call (/usr/bin/python3.11+0x541194)
    #29 0x4fefe0 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fefe0)
    #30 0x62e1b3  (/usr/bin/python3.11+0x62e1b3)
    #31 0x4f3a66 in PyEval_EvalCode (/usr/bin/python3.11+0x4f3a66)
    #32 0x647c36  (/usr/bin/python3.11+0x647c36)
    #33 0x64534f  (/usr/bin/python3.11+0x64534f)
    #34 0x650d14  (/usr/bin/python3.11+0x650d14)
    #35 0x650a63 in _PyRun_SimpleFileObject (/usr/bin/python3.11+0x650a63)
    #36 0x650832 in _PyRun_AnyFileObject (/usr/bin/python3.11+0x650832)
    #37 0x64f786 in Py_RunMain (/usr/bin/python3.11+0x64f786)
    #38 0x61ee0c in Py_BytesMain (/usr/bin/python3.11+0x61ee0c)
    #39 0x7faaf80d1d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)
    #40 0x7faaf80d1e3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e3f)
    #41 0x61ec94 in _start (/usr/bin/python3.11+0x61ec94)

0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)
allocated by thread T0 here:
    #0 0x7faaf84bf887 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145
    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2803)
    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #21 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #25 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const
Shadow bytes around the buggy address:
  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd
  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa
  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa
  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa
=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa
  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3331846==ABORTING
```
",LongZE666,2024-11-26 12:42:50+00:00,['tilakrayal'],2024-12-04 04:56:07+00:00,,https://github.com/tensorflow/tensorflow/issues/80847,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2505587963, 'issue_id': 2694535611, 'author': 'tilakrayal', 'body': '@LongZE666,\r\nI was able to reproduce the issue on the both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4e585dccd2c9774dbc0ee3cbe7b842b0/untitled2261.ipynb). Also could you please try to contribute with the PR for the changes required for the mentioned issue? Thank you!', 'created_at': datetime.datetime(2024, 11, 28, 9, 0, 41, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-28 09:00:41 UTC): @LongZE666,
I was able to reproduce the issue on the both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4e585dccd2c9774dbc0ee3cbe7b842b0/untitled2261.ipynb). Also could you please try to contribute with the PR for the changes required for the mentioned issue? Thank you!

"
2693837162,issue,closed,completed,[MSVC] Compiler error in external/nsync/platform/c++11/platform.h,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest commit

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

MSVC 19.43.34617.95

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I work on Microsoft Visual C++ testing, where we regularly build popular open-source projects, including yours, with development builds of our compiler and libraries to detect and prevent shipping regressions that would affect you. This also allows us to provide advance notice of breaking changes, which is the case here.

Recently this commit https://github.com/microsoft/STL/pull/5105 is revealing an issue in nysnc.

Compiler error with this STL change:
```
./external/nsync//platform/c++11\platform.h(67): error C2039: 'system_clock': is not a member of 'std::chrono'
C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.42.34433\include\__msvc_chrono.hpp(286): note: see declaration of 'std::chrono'
./external/nsync//platform/c++11\platform.h(67): error C3083: 'system_clock': the symbol to the left of a '::' must be a type
./external/nsync//platform/c++11\platform.h(67): error C2133: 'epoch': unknown size
./external/nsync//platform/c++11\platform.h(67): error C2512: 'std::chrono::time_point': no appropriate default constructor available
C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.42.34433\include\__msvc_chrono.hpp(200): note: see declaration of 'std::chrono::time_point'
./external/nsync//platform/c++11\platform.h(69): error C2676: binary '+': 'std::chrono::time_point' does not define this operator or a conversion to a type acceptable to the predefined operator
```

I have created an upstream PR (https://github.com/google/nsync/pull/25), which is currently awaiting their reponse.

In the meantime, would it possible for you to add this patch ([include_chrono.patch](https://github.com/user-attachments/files/17916641/include_chrono.patch)) to https://github.com/tensorflow/tensorflow/tree/master/third_party 

and update the following line in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L400:  
`patch_file = [""//third_party:nsync.patch"", ""//third_party:include_chrono.patch""],` ?

I have tested this change and confirmed that there are no issues when building Tensorflow from source.

build log: [tf.log](https://github.com/user-attachments/files/17916756/tf.log)


### Standalone code to reproduce the issue

```shell
#include <mutex>
// #include <chrono> this is necessary in next release

int main(){
	std::chrono::system_clock::time_point epoch; 
}
```


### Relevant log output

```shell
C:\Users\v-neilchua\Desktop>cl f.cpp
Microsoft (R) C/C++ Optimizing Compiler Version 19.43.34617.95 for x64
Copyright (C) Microsoft Corporation.  All rights reserved.

f.cpp
f.cpp(4): error C2039: 'chrono': is not a member of 'std'
predefined C++ types (compiler internal)(358): note: see declaration of 'std'
f.cpp(4): error C2039: 'system_clock': is not a member of 'std'
predefined C++ types (compiler internal)(358): note: see declaration of 'std'
f.cpp(4): error C3083: 'chrono': the symbol to the left of a '::' must be a type
f.cpp(4): error C3083: 'system_clock': the symbol to the left of a '::' must be a type
f.cpp(4): error C2039: 'time_point': is not a member of 'std'
predefined C++ types (compiler internal)(358): note: see declaration of 'std'
f.cpp(4): error C2065: 'time_point': undeclared identifier
f.cpp(4): error C2146: syntax error: missing ';' before identifier 'epoch'
f.cpp(4): error C2065: 'epoch': undeclared identifier
```
",NEIL-smtg,2024-11-26 09:15:36+00:00,"['penpornk', 'Venkat6871']",2024-12-09 07:21:19+00:00,2024-12-09 07:21:16+00:00,https://github.com/tensorflow/tensorflow/issues/80836,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2527131608, 'issue_id': 2693837162, 'author': 'NEIL-smtg', 'body': 'this issue is no longer appears in the latest commit, closing this issue.', 'created_at': datetime.datetime(2024, 12, 9, 7, 21, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527131664, 'issue_id': 2693837162, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80836"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80836"">No</a>', 'created_at': datetime.datetime(2024, 12, 9, 7, 21, 18, tzinfo=datetime.timezone.utc)}]","NEIL-smtg (Issue Creator) on (2024-12-09 07:21:16 UTC): this issue is no longer appears in the latest commit, closing this issue.

google-ml-butler[bot] on (2024-12-09 07:21:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80836"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80836"">No</a>

"
2693507537,issue,open,,The Doc of tfl.reverse_v2 need to be updated for supporting axes,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of tfl.reverse_v2](https://www.tensorflow.org/mlir/tfl_ops#tflreverse_v2_tflreversev2op) only supports axis, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/reverse.cc;l=85) can support axes, the implementation seems not be consistent with the doc.

",fujunwei,2024-11-26 07:26:24+00:00,['gaikwadrahul8'],2024-12-02 09:04:07+00:00,,https://github.com/tensorflow/tensorflow/issues/80831,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2510945496, 'issue_id': 2693507537, 'author': 'gaikwadrahul8', 'body': 'Hi, @fujunwei \r\n\r\nI apologize for the delayed response, I see the kernel implementation and it seems like we do support contiguous axes so we need to update our official documentation and for testing contiguous axes support I prepared one sample example in that it supports contiguous axes and not non contiguous axes please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/5fb56c84ba881d3fec8f80df0fa3cacc/tflite-issue-80831.ipynb) if I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 2, 9, 2, 30, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-02 09:02:30 UTC): Hi, @fujunwei 

I apologize for the delayed response, I see the kernel implementation and it seems like we do support contiguous axes so we need to update our official documentation and for testing contiguous axes support I prepared one sample example in that it supports contiguous axes and not non contiguous axes please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/5fb56c84ba881d3fec8f80df0fa3cacc/tflite-issue-80831.ipynb) if I have missed something here please let me know.

Thank you for your cooperation and patience.

"
2693263938,issue,closed,completed,Testing this Issue for LiteRT Migration Task,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",gaikwadrahul8,2024-11-26 05:52:43+00:00,['Venkat6871'],2024-12-02 10:21:59+00:00,2024-12-02 10:21:59+00:00,https://github.com/tensorflow/tensorflow/issues/80822,"[('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2511129912, 'issue_id': 2693263938, 'author': 'gaikwadrahul8', 'body': 'Hi, Team\r\nI am closing this issue now because I created this issue to test transfer option for issue migration task of TFLite issue from TesorFlow core repo to respective [LiteRT](https://github.com/google-ai-edge/LiteRT) and [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) but unfortunately it did not work because **""You can only transfer issues between repositories owned by the same user or organization account. A private repository issue cannot be transferred to a public repository.""** here is [reference](https://docs.github.com/en/issues/tracking-your-work-with-issues/administering-issues/transferring-an-issue-to-another-repository)\r\n\r\nThank you.', 'created_at': datetime.datetime(2024, 12, 2, 10, 21, 57, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Issue Creator) on (2024-12-02 10:21:57 UTC): Hi, Team
I am closing this issue now because I created this issue to test transfer option for issue migration task of TFLite issue from TesorFlow core repo to respective [LiteRT](https://github.com/google-ai-edge/LiteRT) and [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) but unfortunately it did not work because **""You can only transfer issues between repositories owned by the same user or organization account. A private repository issue cannot be transferred to a public repository.""** here is [reference](https://docs.github.com/en/issues/tracking-your-work-with-issues/administering-issues/transferring-an-issue-to-another-repository)

Thank you.

"
2692957167,issue,open,,Aborted (core dumped) in `RaggedBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
",LongZE666,2024-11-26 03:18:31+00:00,['Venkat6871'],2024-12-19 04:15:06+00:00,,https://github.com/tensorflow/tensorflow/issues/80812,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2502914362, 'issue_id': 2692957167, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\r\nThank you for raising your concern here. Is there a specific reason for using such a long value like 6522107765268123892? This value is extremely large, and allocating memory for a tensor of this size would require at least 52 exabytes, which far exceeds current hardware capabilities. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and faced the same issue. However, as an alternative, I used smaller values, and it worked fine for me. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/41cdd58756b0a4f300882ea9fc8aabd8/80812_tf_2-17-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 27, 5, 46, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930614, 'issue_id': 2692957167, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522019962, 'issue_id': 2692957167, 'author': 'LongZE666', 'body': '@Venkat6871 \r\nThanks for your reply, I use this long value to test if there is an error in the operator. And this error seems to be a removal error caused by calculation. The specific location is in RaggedBincountOp::Compute\r\n```C++\r\nOP_REQUIRES_OK(\r\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\r\n```', 'created_at': datetime.datetime(2024, 12, 6, 3, 1, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-27 05:46:02 UTC): Hi **@LongZE666** ,
Thank you for raising your concern here. Is there a specific reason for using such a long value like 6522107765268123892? This value is extremely large, and allocating memory for a tensor of this size would require at least 52 exabytes, which far exceeds current hardware capabilities. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and faced the same issue. However, as an alternative, I used smaller values, and it worked fine for me. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/41cdd58756b0a4f300882ea9fc8aabd8/80812_tf_2-17-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-12-05 02:08:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

LongZE666 (Issue Creator) on (2024-12-06 03:01:02 UTC): @Venkat6871 
Thanks for your reply, I use this long value to test if there is an error in the operator. And this error seems to be a removal error caused by calculation. The specific location is in RaggedBincountOp::Compute
```C++
OP_REQUIRES_OK(
        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));
```

"
2692083007,issue,closed,completed,Custom methods and training configuration in subclass model not saved by ModelCheckpoint callback,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Google Colab / Windows 11

### Mobile device

_No response_

### Python version

3.8.18

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, 

I'm following the tutorial in [https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit](url) to implement a subclassed model (end-to-end GAN). For that:
- I have overridden certain methods (`train_step`, `save`).
- I am using the `ModelCheckpoint` callback to save the model (generator).

The training and model saving run smoothly, but when I load the trained model: 
- The overridden methods are not accessible; instead, the methods from the parent class (tf.keras.Model) are loaded.
![image](https://github.com/user-attachments/assets/34f80ff3-78f9-4d05-a290-e6a214faf55a)
- The training configuration is also not available.
![image](https://github.com/user-attachments/assets/6fc0de9f-5ca3-46c4-92d6-21fa11765111)

I attempted the fix suggested in [https://github.com/tensorflow/tensorflow/issues/62450](url) (and the related suggestions mentioned there), but without success.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/19tc4EY_wI66ZZBeLlhs0rqIyfQIKENhd#scrollTo=OM_MBvzrtm_z
```


### Relevant log output

_No response_",lilianabrandao,2024-11-25 20:12:12+00:00,['tilakrayal'],2024-11-30 11:53:13+00:00,2024-11-30 11:53:10+00:00,https://github.com/tensorflow/tensorflow/issues/80781,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:keras', 'Keras related issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2503969497, 'issue_id': 2692083007, 'author': 'tilakrayal', 'body': '@lilianabrandao,\r\nI tried to execute the mentioned code with the latest TensorFlow v2.17 which contains Keras3.0 and observed that the compile error which you are facing is unavailable. And also the shape mismatch error is occured due to input difference. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/54e58fe537159749e136d7f519e40534/untitled2256.ipynb). Also this issue is more related to Keras please create the issue in Keras-team/keras repo. Thank you!', 'created_at': datetime.datetime(2024, 11, 27, 14, 6, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508936022, 'issue_id': 2692083007, 'author': 'lilianabrandao', 'body': 'Thanks for the gist, @tilakrayal, but I need to address this issue under TensorFlow 2.12.0.\r\n\r\nAfter some investigation, I realized that since the subclassed model is just a wrapper for training two functional models, I need to:\r\n1. Explicitly compile the functional models inside the wrapper to save their training configurations.\r\n2. Override the call function in the wrapper model and explicitly invoke it before saving the subclassed model.\r\n\r\nI hope this helps anyone facing the same issue in the future.', 'created_at': datetime.datetime(2024, 11, 30, 11, 53, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508936030, 'issue_id': 2692083007, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80781"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80781"">No</a>', 'created_at': datetime.datetime(2024, 11, 30, 11, 53, 12, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-27 14:06:45 UTC): @lilianabrandao,
I tried to execute the mentioned code with the latest TensorFlow v2.17 which contains Keras3.0 and observed that the compile error which you are facing is unavailable. And also the shape mismatch error is occured due to input difference. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/54e58fe537159749e136d7f519e40534/untitled2256.ipynb). Also this issue is more related to Keras please create the issue in Keras-team/keras repo. Thank you!

lilianabrandao (Issue Creator) on (2024-11-30 11:53:10 UTC): Thanks for the gist, @tilakrayal, but I need to address this issue under TensorFlow 2.12.0.

After some investigation, I realized that since the subclassed model is just a wrapper for training two functional models, I need to:
1. Explicitly compile the functional models inside the wrapper to save their training configurations.
2. Override the call function in the wrapper model and explicitly invoke it before saving the subclassed model.

I hope this helps anyone facing the same issue in the future.

google-ml-butler[bot] on (2024-11-30 11:53:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80781"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80781"">No</a>

"
2690534026,issue,open,reopened,This method creates a model with a 100% memory leak loop using model. fit(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

ubuntu 2.2 or mac m1

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

best_loss = np.inf
best_model_data = None
for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        current_loss = history.history['loss'][-1]
        print(f""Training model: {model.name}"")
    
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end"")


if best_model_data:
    model_json = best_model_data[""model_architecture""]
    model_weights = json.loads(best_model_data[""model_weights""], object_hook=lambda d: np.array(d))
    model = tf.keras.models.model_from_json(model_json)
    model.set_weights(model_weights)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    print(""ok"")
else:
    print(""not found"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
",phpYj,2024-11-25 12:12:55+00:00,['Venkat6871'],2024-12-23 10:12:16+00:00,,https://github.com/tensorflow/tensorflow/issues/80753,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2497860573, 'issue_id': 2690534026, 'author': 'phpYj', 'body': 'The main issue is the creation method of the model and the code related to the fit loop, which leads to memory leakage. Other codes have little significance\r\nI am running on the CPU', 'created_at': datetime.datetime(2024, 11, 25, 12, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503004311, 'issue_id': 2690534026, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80753"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80753"">No</a>', 'created_at': datetime.datetime(2024, 11, 27, 6, 21, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503014404, 'issue_id': 2690534026, 'author': 'phpYj', 'body': 'I would like to correct that the version number is 2.18, but I accidentally closed the issue', 'created_at': datetime.datetime(2024, 11, 27, 6, 30, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503203801, 'issue_id': 2690534026, 'author': 'Venkat6871', 'body': 'Hi **@phpYj** ,\r\nThank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/30fb12f2188a7826e2c649bbd945dbda/80753_tf_2-18-0-nightly-v.ipynb) here for your reference.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 27, 8, 14, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503237137, 'issue_id': 2690534026, 'author': 'phpYj', 'body': ""Hi @Venkat6871 \r\n\r\nSomeone has raised a similar issue before, as if it was TF's mistake\r\nI've identified the source of the leak in this case, and it's a TF bug. Basically when we create TF functions their resources never get garbage collected. Even if we keep a reference to the functions created and we manually delete them, the memory isn't freed.\r\n\r\nJAX and PyTorch do not appear to have any such problems. I can run the script with no memory accumulation with JAX + XLA compilation for instance.\r\nhttps://github.com/keras-team/keras/issues/20245#issuecomment-2342594184\r\nhttps://github.com/keras-team/tf-keras/issues/286\r\nhttps://github.com/keras-team/keras/issues/20552#issuecomment-2503944388 Answer from the Keras team, this error belongs to TF"", 'created_at': datetime.datetime(2024, 11, 27, 8, 31, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544230335, 'issue_id': 2690534026, 'author': 'edwardyehuang', 'body': 'same issue, and only occur on GPU (not TPU)', 'created_at': datetime.datetime(2024, 12, 16, 0, 23, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559339312, 'issue_id': 2690534026, 'author': 'clement-soules', 'body': 'Dear Tensorflow users and devs. \r\n\r\nI experienced CPU memory leaks during the development of my ML ops framework.\r\nI spent over 50 hours searching for a solution\r\n\r\nSo i will share my research and final solution. \r\n\r\nHow I Identified the RAM Leak : \r\n\r\nInitialy i worked under a Tensorflow NVIDIA at https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow release 24.10 \r\nIn order to find the limit of my hyperparameter search space, i did some stress test to find the limit of my hardware (RTX 4500 ada generation). And i found a RAM leak during keras.fit() loop. \r\n\r\nSorry i don\'t have screenshoot but the RAM curve is the following\r\n\r\nmodel.fit() (with jit_compile=True) \r\nPhase 1 -> compiling training loop -> Sharp drop of RAM (this is the XLA compiling effect)\r\nPhase 2 -> Inverse Log curve -> the first fit steps use more ram than last step and i see a stabilizing effect where the RAM leak become linear)\r\n\r\nDuring my research i expected the following RAM leaks suspect list -> (Nvidia Driver / Cuda, XLA compiler, tf data pipeline, Tensorflow version, keras version, Grappler optimiser config)\r\n\r\ni spend most of the time suspecting XLA, thus i tried many config flags\r\n-> thus XLA suspect removed from list\r\ni spend a lot of time trying different container config with (Nvidia hub and tensorflow official docker hub), \r\nturn out any NVIDIA driver from 535 to 565 and tf 2.16 ,2.17 ,2.8 still have the issue. \r\n-> NVIDIA driver / cuda suspect removed from list\r\ni converted the model back to keras 2 API \r\n-> still have the issue\r\ni run the tf data pipeline without keras.fit\r\n-> no memory leak\r\nafter reading many (maybe all) TF RAM leaks issues, i feel agree with @phpYj \r\n\r\nso i imagined why internal tensorflow dev still having memory leak issue tickers on github since 2.10 and no fix for many users. and i remembered that tcmalloc is a internal google tools. So i decided to try.\r\n\r\nThus under tensorflow/tensorflow/2.17.0-gpu container i installed TCmalloc \r\n\r\nBASH :\r\n\r\napt-get install google-perftools\r\n\r\nfind /usr -name ""libtcmalloc_minimal.so* (for know the path) \r\n\r\nand run my .py script with this env variable LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 (maybe it will be another path for you)\r\n\r\nCPU RAM dataleak removed !!!\r\n\r\nmodel.fit() (with jit_compile=True) \r\nPhase 1 -> Compiling -> Sharp drop of RAM (this is the XLA compiling effect)\r\nPhase 2 -> Training :constant RAM usage with no fluctuation\r\n\r\nI tried again without TCmalloc, and i had RAM leaks again\r\nI tried again with TCmalloc and i had NO RAM leaks\r\n\r\nHope my finding will help some of you\r\n\r\nmy nvidia smi under the container -> Driver Version: 535.183.01   CUDA Version: 12.3', 'created_at': datetime.datetime(2024, 12, 23, 9, 58, 2, tzinfo=datetime.timezone.utc)}]","phpYj (Issue Creator) on (2024-11-25 12:17:18 UTC): The main issue is the creation method of the model and the code related to the fit loop, which leads to memory leakage. Other codes have little significance
I am running on the CPU

google-ml-butler[bot] on (2024-11-27 06:21:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80753"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80753"">No</a>

phpYj (Issue Creator) on (2024-11-27 06:30:28 UTC): I would like to correct that the version number is 2.18, but I accidentally closed the issue

Venkat6871 (Assginee) on (2024-11-27 08:14:54 UTC): Hi **@phpYj** ,
Thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and encountered the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/30fb12f2188a7826e2c649bbd945dbda/80753_tf_2-18-0-nightly-v.ipynb) here for your reference.
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

phpYj (Issue Creator) on (2024-11-27 08:31:49 UTC): Hi @Venkat6871 

Someone has raised a similar issue before, as if it was TF's mistake
I've identified the source of the leak in this case, and it's a TF bug. Basically when we create TF functions their resources never get garbage collected. Even if we keep a reference to the functions created and we manually delete them, the memory isn't freed.

JAX and PyTorch do not appear to have any such problems. I can run the script with no memory accumulation with JAX + XLA compilation for instance.
https://github.com/keras-team/keras/issues/20245#issuecomment-2342594184
https://github.com/keras-team/tf-keras/issues/286
https://github.com/keras-team/keras/issues/20552#issuecomment-2503944388 Answer from the Keras team, this error belongs to TF

edwardyehuang on (2024-12-16 00:23:06 UTC): same issue, and only occur on GPU (not TPU)

clement-soules on (2024-12-23 09:58:02 UTC): Dear Tensorflow users and devs. 

I experienced CPU memory leaks during the development of my ML ops framework.
I spent over 50 hours searching for a solution

So i will share my research and final solution. 

How I Identified the RAM Leak : 

Initialy i worked under a Tensorflow NVIDIA at https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow release 24.10 
In order to find the limit of my hyperparameter search space, i did some stress test to find the limit of my hardware (RTX 4500 ada generation). And i found a RAM leak during keras.fit() loop. 

Sorry i don't have screenshoot but the RAM curve is the following

model.fit() (with jit_compile=True) 
Phase 1 -> compiling training loop -> Sharp drop of RAM (this is the XLA compiling effect)
Phase 2 -> Inverse Log curve -> the first fit steps use more ram than last step and i see a stabilizing effect where the RAM leak become linear)

During my research i expected the following RAM leaks suspect list -> (Nvidia Driver / Cuda, XLA compiler, tf data pipeline, Tensorflow version, keras version, Grappler optimiser config)

i spend most of the time suspecting XLA, thus i tried many config flags
-> thus XLA suspect removed from list
i spend a lot of time trying different container config with (Nvidia hub and tensorflow official docker hub), 
turn out any NVIDIA driver from 535 to 565 and tf 2.16 ,2.17 ,2.8 still have the issue. 
-> NVIDIA driver / cuda suspect removed from list
i converted the model back to keras 2 API 
-> still have the issue
i run the tf data pipeline without keras.fit
-> no memory leak
after reading many (maybe all) TF RAM leaks issues, i feel agree with @phpYj 

so i imagined why internal tensorflow dev still having memory leak issue tickers on github since 2.10 and no fix for many users. and i remembered that tcmalloc is a internal google tools. So i decided to try.

Thus under tensorflow/tensorflow/2.17.0-gpu container i installed TCmalloc 

BASH :

apt-get install google-perftools

find /usr -name ""libtcmalloc_minimal.so* (for know the path) 

and run my .py script with this env variable LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 (maybe it will be another path for you)

CPU RAM dataleak removed !!!

model.fit() (with jit_compile=True) 
Phase 1 -> Compiling -> Sharp drop of RAM (this is the XLA compiling effect)
Phase 2 -> Training :constant RAM usage with no fluctuation

I tried again without TCmalloc, and i had RAM leaks again
I tried again with TCmalloc and i had NO RAM leaks

Hope my finding will help some of you

my nvidia smi under the container -> Driver Version: 535.183.01   CUDA Version: 12.3

"
2690530302,issue,closed,completed,Aborted (core dumped) in `UnBatch`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When `batch_index` is scalar,  `UnBatch` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

batched_tensor = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
batch_index = tf.constant(1, dtype=tf.int64)
id = tf.constant(0, dtype=tf.int64) 

tf.raw_ops.Unbatch(
    batched_tensor=batched_tensor,
    batch_index=batch_index,
    id=id,
    timeout_micros=1000000
)
```


### Relevant log output

```shell
Check failed: d < dims() (1 vs. 0)
```
",LongZE666,2024-11-25 12:11:14+00:00,['tilakrayal'],2024-12-13 02:08:45+00:00,2024-12-13 02:08:42+00:00,https://github.com/tensorflow/tensorflow/issues/80751,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2505528868, 'issue_id': 2690530302, 'author': 'tilakrayal', 'body': '@LongZE666,\r\nI tried to execute the mentioned code on tf-nightly and TensorFlow v2.17, observed that the code was aborted in TF v2.17 and on tf-nigthly the code was executed with the expected error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/36c1807f2037192d3ef3a32d156795ae/untitled2260.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 11, 28, 8, 30, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521929757, 'issue_id': 2690530302, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540377916, 'issue_id': 2690530302, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540377972, 'issue_id': 2690530302, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80751"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80751"">No</a>', 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-28 08:30:26 UTC): @LongZE666,
I tried to execute the mentioned code on tf-nightly and TensorFlow v2.17, observed that the code was aborted in TF v2.17 and on tf-nigthly the code was executed with the expected error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/36c1807f2037192d3ef3a32d156795ae/untitled2260.ipynb). Thank you!

github-actions[bot] on (2024-12-06 02:07:36 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-13 02:08:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-13 02:08:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80751"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80751"">No</a>

"
2689944864,issue,closed,completed,keras model does not learn if using tf.data pipeline,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8907

### GPU model and memory

_No response_

### Current behavior?

A `keras` model *does* learn using Numpy arrays as the input, but fails to make any progress if reading data from a `tf.data` pipeline.  What could be the reason potentially?

In particular, the model consumes batched multidimensional time series (so every point is an N x M tensor) and solves a classification problem.  If the data are prepared in advance, by aggregating the time series in a large Numpy array, then the model successfully learns as indicated by a significant increase in the accuracy.  However, when exactly the same input data is prepared using `tf.data` pipeline, the accuracy remains at the baseline level.

I compared the two sets of data by writing to disk, and they are identical.  Also the types match.

Tried disabling threading (IIUC) by setting

```
options.threading.private_threadpool_size = 1
```

and experimenting with a bunch of `options.experimental_optimization` options.

Could it be the case that the data are read in parallel from the `tf.data` dataset as opposed to being read sequentially from the Numpy array?

Apologies in advance for not providing a 100% reproducible case (cannot share data at the moment). Nevertheless, attaching the pipeline code (`np_array` contains ""raw"" data):

### Standalone code to reproduce the issue

```shell
ds = tf.data.Dataset.from_tensor_slices(np_array.T)
y_ds = (
    ds
   .skip(T - 1)
   .map(lambda s: s[-1] - 1)
   .map(lambda y: to_categorical(y, 3))
)
X_ds = (
    ds
    .map(lambda s: s[:n_features])
    .window(T, shift=1, drop_remainder=True)
    .flat_map(lambda x: x.batch(T, drop_remainder=True))
    .map(lambda x: tf.expand_dims(x, -1))
)
Xy_ds = (
    tf.data.Dataset.zip(X_ds, y_ds)
    .batch(size_batch)
    .repeat(n_epochs * size_batch)
    .prefetch(tf.data.AUTOTUNE)
)
model.fit(
    Xy_train,
    epochs=n_epochs,
    steps_per_epoch=199,  # correct
    verbose=2
)
```


### Relevant log output

_No response_",x1o,2024-11-25 09:28:44+00:00,['tilakrayal'],2024-11-26 16:56:39+00:00,2024-11-26 11:39:09+00:00,https://github.com/tensorflow/tensorflow/issues/80739,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:data', 'tf.data related issues'), ('TF 2.16', '')]","[{'comment_id': 2499830839, 'issue_id': 2689944864, 'author': 'tilakrayal', 'body': '@x1o,\r\nI tried to execute the mentioned code and observed the code was executed with a different error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/71354affa216301557d3f760aa3665fd/untitled2252.ipynb) and please provide the complete code and the dependencies required which helps to debug the issue in an effective way. \r\n\r\nAlso the error might be  tf.function used with AutoGraph disabled. And in keras3 AutoGraph disabled by default. Please take a look for details about [AutoGraph](https://keras.io/guides/migrating_to_keras_3/#tf-autograph).\r\n\r\nYou need to use Eager execution mode model.compile(steps_per_execution=33,run_eagerly=True) to enable AutoGraph for keras3.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 26, 7, 12, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500440314, 'issue_id': 2689944864, 'author': 'x1o', 'body': ""@tilakrayal,\r\n\r\nthank you for your time. I've discovered the problem was due to the implicit shuffling in fit() when working with Numpy array data, contrasting with the absence of such shuffling in the tf.data pipeline. Shuffling is suitable even in time series analysis since the data were pre-aggregated, thus preserving the internal temporal structure."", 'created_at': datetime.datetime(2024, 11, 26, 11, 39, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500440462, 'issue_id': 2689944864, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80739"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80739"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 11, 39, 11, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-26 07:12:55 UTC): @x1o,
I tried to execute the mentioned code and observed the code was executed with a different error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/71354affa216301557d3f760aa3665fd/untitled2252.ipynb) and please provide the complete code and the dependencies required which helps to debug the issue in an effective way. 

Also the error might be  tf.function used with AutoGraph disabled. And in keras3 AutoGraph disabled by default. Please take a look for details about [AutoGraph](https://keras.io/guides/migrating_to_keras_3/#tf-autograph).

You need to use Eager execution mode model.compile(steps_per_execution=33,run_eagerly=True) to enable AutoGraph for keras3.

Thank you!

x1o (Issue Creator) on (2024-11-26 11:39:09 UTC): @tilakrayal,

thank you for your time. I've discovered the problem was due to the implicit shuffling in fit() when working with Numpy array data, contrasting with the absence of such shuffling in the tf.data pipeline. Shuffling is suitable even in time series analysis since the data were pre-aggregated, thus preserving the internal temporal structure.

google-ml-butler[bot] on (2024-11-26 11:39:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80739"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80739"">No</a>

"
2689916404,issue,closed,completed,Android Tensorflow tflite error version 2.17. Didn't find op for builtin opcode 'FULLY_CONNECTED' version '12'. An older version of this builtin might be supported,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 2.17


**Provide the text output from tflite_convert**

**Standalone code to reproduce the issue** 

``@Throws(IOException::class)
    fun loadModelFile(
        context: Context,
        modelFileName: String = ""constant_output_model.tflite""
    ): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFileName)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declareLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declareLength)
    }``
This is the interpreter API initialization
`
InterpreterApi.create(
            TfLiteUtils.loadModelFile(this, ""voice_detection_2_17_0_new_ops.tflite""),
            InterpreterApi.Options().apply {
                numThreads = 1
            }
        )
`
This is the dummy model architecture with same shape and output.

`input_shape = (None, 39, 63, 1)  # Input shape (excluding batch dimension)
inputs = tf.keras.Input(shape=input_shape[1:])
x = tf.keras.layers.Conv2D(filters=128, kernel_size=(8, 8), activation='relu', padding='same')(inputs)
x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)
x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)
x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)
x = tf.keras.layers.Flatten()(x) # if I remove this then model works
x = tf.keras.layers.Dense(units=256)(x)
x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)
x = tf.keras.layers.Dense(units=2, activation='softmax')(x)
outputs = tf.keras.layers.Dense(units=1)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)`

Currently using 2.17 AAR file of tensorflow. It is not present on maven but can be built using official repo

Also, please include a link to a GraphDef or the model if possible.


**Any other info / logs**
Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'FULLY_CONNECTED' version '12'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",therohanchoudhary,2024-11-25 09:18:18+00:00,['gaikwadrahul8'],2024-12-13 02:08:44+00:00,2024-12-13 02:08:43+00:00,https://github.com/tensorflow/tensorflow/issues/80736,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2504233872, 'issue_id': 2689916404, 'author': 'fergushenderson', 'body': ""This looks like it might be due to a mismatch between the version of TensorFlow used to build your model, and the version of TensorFlow Lite used to execute it.  In particular, are you sure that you are using version 2.17 of TF Lite?\r\n\r\nThat version of TF Lite supports versions 1 to 12 (inclusive) of the 'FULLY_CONNECTED'  op\r\nhttps://github.com/tensorflow/tensorflow/blob/ad6d8cc177d0c868982e39e0823d0efbfb95f04c/tensorflow/lite/core/kernels/register.cc#L85\r\nso I would not expect to see that message unless either (a) you are using an older version of TF Lite or (b) you are using an op resolver that doesn't support the 'FULLY_CONNECTED' op at all."", 'created_at': datetime.datetime(2024, 11, 27, 15, 56, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521929796, 'issue_id': 2689916404, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540377954, 'issue_id': 2689916404, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 43, tzinfo=datetime.timezone.utc)}]","fergushenderson on (2024-11-27 15:56:02 UTC): This looks like it might be due to a mismatch between the version of TensorFlow used to build your model, and the version of TensorFlow Lite used to execute it.  In particular, are you sure that you are using version 2.17 of TF Lite?

That version of TF Lite supports versions 1 to 12 (inclusive) of the 'FULLY_CONNECTED'  op
https://github.com/tensorflow/tensorflow/blob/ad6d8cc177d0c868982e39e0823d0efbfb95f04c/tensorflow/lite/core/kernels/register.cc#L85
so I would not expect to see that message unless either (a) you are using an older version of TF Lite or (b) you are using an op resolver that doesn't support the 'FULLY_CONNECTED' op at all.

github-actions[bot] on (2024-12-06 02:07:37 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-13 02:08:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2689651458,issue,closed,completed,Aborted (core dumped) in `ParameterizedTruncatedNormal`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `ParameterizedTruncatedNormal` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant([1610637938, 2], dtype=tf.int32)
means = tf.constant(7.89645e+16, shape=[], dtype=tf.float32)
stdevs = tf.constant(1.251e+12, shape=[], dtype=tf.float32)
minvals = tf.constant(1.23457e+13, shape=[], dtype=tf.float32)
maxvals = tf.constant(1.11111e+15, shape=[], dtype=tf.float32)
seed = 1618
seed2 = 0
tf.raw_ops.ParameterizedTruncatedNormal(shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals, seed=seed, seed2=seed2, name=None)
```


### Relevant log output

```shell
2024-11-25 07:56:01.911210: F tensorflow/core/util/work_sharder.cc:59] Check failed: total >= 0 (0 vs. -10736913)
Aborted (core dumped)
```
",LongZE666,2024-11-25 08:00:02+00:00,['Venkat6871'],2024-12-12 02:08:04+00:00,2024-12-12 02:08:01+00:00,https://github.com/tensorflow/tensorflow/issues/80729,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2499670358, 'issue_id': 2689651458, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\r\nThank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and faced the same issue. However, with TensorFlow 2.18.0 and the nightly versions, it works fine. We always recommend using the latest versions for better results. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/1a2cffe96ec3c88db94255ff598796d1/80729_tf-2-17-2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 26, 5, 6, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004698, 'issue_id': 2689651458, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594324, 'issue_id': 2689651458, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 12, 2, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594375, 'issue_id': 2689651458, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80729"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80729"">No</a>', 'created_at': datetime.datetime(2024, 12, 12, 2, 8, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-26 05:06:59 UTC): Hi **@LongZE666** ,
Thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and faced the same issue. However, with TensorFlow 2.18.0 and the nightly versions, it works fine. We always recommend using the latest versions for better results. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/1a2cffe96ec3c88db94255ff598796d1/80729_tf-2-17-2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-12-04 02:08:16 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-12 02:08:00 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-12 02:08:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80729"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80729"">No</a>

"
2688924412,issue,open,,"Cannot send mail to github-admin@tensorflow.org, the contact address shown on front page of github.com/tensorflow","### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

irrelevant

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The front page of https://github.com/tensorflow lists github-admin@tensorflow.org as a contact address:

<p align=""center"">
<img width=""80%"" alt=""image"" src=""https://github.com/user-attachments/assets/a80e6161-1cde-4d10-b9a2-774f80294d15"">
</p>

However, attempting to send mail to that address results in an error message being mailed back:

```
We're writing to let you know that the group you tried to 
contact (github-admin) may not exist, or you may not have 
permission to post messages to the group. A few more 
details on why you weren't able to post:

 * You might have spelled or formatted the group name incorrectly.
 * The owner of the group may have removed this group.
 * You may need to join the group before receiving permission to post.
 * This group may not be open to posting.

If you have questions related to this or any other Google Group,
visit the Help Center at 
https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.

Thanks,

[tensorflow.org](http://tensorflow.org/) admins
```


### Standalone code to reproduce the issue

```shell
Send an email message to github-admin@tensorflow.org
```


### Relevant log output

```shell
We're writing to let you know that the group you tried to 
contact (github-admin) may not exist, or you may not have 
permission to post messages to the group. A few more 
details on why you weren't able to post:

 * You might have spelled or formatted the group name incorrectly.
 * The owner of the group may have removed this group.
 * You may need to join the group before receiving permission to post.
 * This group may not be open to posting.

If you have questions related to this or any other Google Group,
visit the Help Center at 
https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.

Thanks,

[tensorflow.org](http://tensorflow.org/) admins
```
",mhucka,2024-11-25 03:15:30+00:00,['tilakrayal'],2025-02-06 16:50:33+00:00,,https://github.com/tensorflow/tensorflow/issues/80700,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower')]","[{'comment_id': 2500830658, 'issue_id': 2688924412, 'author': 'tilakrayal', 'body': '@mhucka,\r\nThank you for reporting the issue!', 'created_at': datetime.datetime(2024, 11, 26, 13, 39, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638159783, 'issue_id': 2688924412, 'author': 'JFWooten4', 'body': 'spam removed', 'created_at': datetime.datetime(2025, 2, 5, 22, 13, 58, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-26 13:39:26 UTC): @mhucka,
Thank you for reporting the issue!

JFWooten4 on (2025-02-05 22:13:58 UTC): spam removed

"
2687677977,issue,closed,completed,Cannot find reference 'keras' in '__init__.py' ,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I install tensorflow using `pip install tensorflow`. 
Got 
```
Installing collected packages: keras, tensorflow
Successfully installed keras-3.6.0 tensorflow-2.18.0
```
But when I do 
```
from tensorflow.keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
```
I have error in PyCharm `Cannot find reference 'keras' in '__init__.py' `


### Standalone code to reproduce the issue

```shell
pip install tensorflow
from tensorflow.keras.models import Sequential
```


### Relevant log output

_No response_",KindSpidey,2024-11-24 12:53:14+00:00,['tilakrayal'],2024-11-25 14:13:29+00:00,2024-11-25 14:13:25+00:00,https://github.com/tensorflow/tensorflow/issues/80684,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2497995468, 'issue_id': 2687677977, 'author': 'tilakrayal', 'body': ""@KindSpidey,\r\nKeras has been moved from Keras2.0 to Keras3.0. Keras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch.\r\n```python\r\n- If you were accessing keras as a standalone package, just switch to using the Python package tf_keras instead, which you can install via pip install tf_keras. The code and API are wholly unchanged  it's Keras 2.15 with a different package name. We will keep fixing bugs in tf_keras and we will keep regularly releasing new versions. However, no new features or performance improvements will be added, since the package is now in maintenance mode.\r\n```\r\n\r\n```\r\n- If you were accessing keras via tf.keras, there are no immediate changes until TensorFlow 2.16. TensorFlow 2.16+ will use Keras 3 by default. In TensorFlow 2.16+, to keep using Keras 2, you can first install tf_keras, and then export the environment variable TF_USE_LEGACY_KERAS=1. This will direct TensorFlow 2.16+ to resolve tf.keras to the locally-installed tf_keras package. Note that this may affect more than your own code, however: it will affect any package importing tf.keras in your Python process. To make sure your changes only affect your own code, you should use the tf_keras package.\r\n\r\n```\r\n\r\nhttps://keras.io/keras_3/"", 'created_at': datetime.datetime(2024, 11, 25, 13, 14, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498132308, 'issue_id': 2687677977, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80684"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80684"">No</a>', 'created_at': datetime.datetime(2024, 11, 25, 14, 13, 27, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-25 13:14:58 UTC): @KindSpidey,
Keras has been moved from Keras2.0 to Keras3.0. Keras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch.
```python
- If you were accessing keras as a standalone package, just switch to using the Python package tf_keras instead, which you can install via pip install tf_keras. The code and API are wholly unchanged  it's Keras 2.15 with a different package name. We will keep fixing bugs in tf_keras and we will keep regularly releasing new versions. However, no new features or performance improvements will be added, since the package is now in maintenance mode.
```

```
- If you were accessing keras via tf.keras, there are no immediate changes until TensorFlow 2.16. TensorFlow 2.16+ will use Keras 3 by default. In TensorFlow 2.16+, to keep using Keras 2, you can first install tf_keras, and then export the environment variable TF_USE_LEGACY_KERAS=1. This will direct TensorFlow 2.16+ to resolve tf.keras to the locally-installed tf_keras package. Note that this may affect more than your own code, however: it will affect any package importing tf.keras in your Python process. To make sure your changes only affect your own code, you should use the tf_keras package.

```

https://keras.io/keras_3/

google-ml-butler[bot] on (2024-11-25 14:13:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80684"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80684"">No</a>

"
2687469548,issue,closed,completed,heap-buffer-overflow in `ConjugateTranspose`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,`ConjugateTranspose` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant(0, shape=[1,3], dtype=tf.complex128)
perm = tf.constant([0,-1], dtype=tf.int32)

tf.raw_ops.ConjugateTranspose(x=x, perm=perm)
```


### Relevant log output

```shell
=================================================================
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
==2084475==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000320 at pc 0x7fc342398a73 bp 0x7fc2ad518360 sp 0x7fc2ad518350
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
READ of size 16 at 0x609000000320 thread T143
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
    #0 0x7fc342398a72 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorShufflingOp<Eigen::array<int, 2ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_conjugate_op<std::complex<double> const>, Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::ThreadPoolDevice, false, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorShufflingOp<Eigen::array<int, 2ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_conjugate_op<std::complex<double> const>, Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x482b6a72)
    #1 0x7fc30fb37250 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x15a55250)
    #2 0x7fc30fa9f2f6 in std::_Function_handler<void (), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda()#2}>::_M_invoke(std::_Any_data const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x159bd2f6)
    #3 0x7fc36666b121 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fd121)
    #4 0x7fc366660326 in void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f2326)
    #5 0x7fc364f8625c in tsl::(anonymous namespace)::PThread::ThreadFn(void*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b1825c)
    #6 0x7fc41020eac2  (/lib/x86_64-linux-gnu/libc.so.6+0x94ac2)
    #7 0x7fc41029fa03 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x125a03)

0x609000000320 is located 112 bytes to the right of 48-byte region [0x609000000280,0x6090000002b0)
allocated by thread T0 here:
    #0 0x7fc41059257c in __interceptor_posix_memalign ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:226
    #1 0x7fc3676556de in tsl::port::AlignedMalloc(unsigned long, int) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x51e76de)
    #2 0x7fc36413f0aa in tensorflow::MklCPUAllocator::AllocateRaw(unsigned long, unsigned long) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cd10aa)
    #3 0x7fc364e4f566 in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x29e1566)
    #4 0x7fc364571922 in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2103922)
    #5 0x7fc364574cdb in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2106cdb)
    #6 0x7fc364576a5b in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2108a5b)
    #7 0x7fc33f988f27 in tensorflow::TransposeOp::Compute(tensorflow::OpKernelContext*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x458a6f27)
    #8 0x7fc36414a14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #9 0x7fc363cd0fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #10 0x7fc363bad306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #11 0x7fc363c0af74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #12 0x7fc363c188b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #13 0x7fc32bd3a2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #14 0x7fc32bbcfc2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #15 0x7fc32bbd266a in tensorflow::ExecuteNode::Run() (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #16 0x7fc32bd0c939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #17 0x7fc32bbbfde4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #18 0x7fc32bbc3dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #19 0x7fc32bbcdd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #20 0x7fc31abb6c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #21 0x7fc32bd01e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #22 0x7fc30a73445b in TFE_Execute (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #23 0x7fc3620e2274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #24 0x7fc2f60a2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #25 0x7fc2f61b9899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #26 0x51ad66  (/usr/bin/python3.11+0x51ad66)

Thread T143 created by T0 here:
    #0 0x7fc410535685 in __interceptor_pthread_create ../../../../src/libsanitizer/asan/asan_interceptors.cpp:216
    #1 0x7fc364f93fbf in tsl::(anonymous namespace)::PThread::PThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b25fbf)
    #2 0x7fc364f9450a in tsl::(anonymous namespace)::PosixEnv::StartThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b2650a)
    #3 0x7fc3666693a8 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tsl::thread::EigenEnvironment) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fb3a8)
    #4 0x7fc36666d498 in tsl::thread::ThreadPool::ThreadPool(tsl::Env*, tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, Eigen::Allocator*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41ff498)
    #5 0x7fc36414f278 in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&, int, tsl::Allocator*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce1278)
    #6 0x7fc3641505c5 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce25c5)
    #7 0x7fc364143111 in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tsl::gtl::IntType<tensorflow::Bytes_tag_, long>, tensorflow::DeviceLocality const&, tsl::Allocator*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cd5111)
    #8 0x7fc36413c1dd in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cce1dd)
    #9 0x7fc36430667c in tensorflow::DeviceFactory::AddCpuDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e9867c)
    #10 0x7fc364306bbd in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e98bbd)
    #11 0x7fc30a736db8 in TFE_NewContext (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x10654db8)
    #12 0x7fc2f61a1e70 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}, pybind11::object, TFE_ContextOptions const*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::return_value_policy>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}&&, pybind11::object (*)(TFE_ContextOptions const*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::return_value_policy const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1b6e70)
    #13 0x7fc2f61b9899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #14 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/tensorflow-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x482b6a72) in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorShufflingOp<Eigen::array<int, 2ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_conjugate_op<std::complex<double> const>, Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::ThreadPoolDevice, false, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorShufflingOp<Eigen::array<int, 2ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_conjugate_op<std::complex<double> const>, Eigen::TensorMap<Eigen::Tensor<s
Shadow bytes around the buggy address:
  0x0c127fff8010: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x0c127fff8020: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 fa fa
  0x0c127fff8030: fa fa fa fa fa fa fa fa 00 fa fa fa fa fa fa fa
  0x0c127fff8040: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff8050: 00 00 00 00 00 00 fa fa fa fa fa fa fa fa fa fa
=>0x0c127fff8060: fa fa fa fa[fa]fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff8070: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff8080: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff8090: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff80a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c127fff80b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==2084475==ABORTING
```
",LongZE666,2024-11-24 09:39:44+00:00,['tilakrayal'],2024-12-13 02:08:48+00:00,2024-12-13 02:08:45+00:00,https://github.com/tensorflow/tensorflow/issues/80682,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2500825574, 'issue_id': 2687469548, 'author': 'tilakrayal', 'body': '@LongZE666,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/69213 and https://github.com/tensorflow/tensorflow/issues/63033 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 11, 26, 13, 37, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502518020, 'issue_id': 2687469548, 'author': 'LongZE666', 'body': 'Okay, I will pay attention to these issues, thank you', 'created_at': datetime.datetime(2024, 11, 27, 2, 13, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930642, 'issue_id': 2687469548, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540377984, 'issue_id': 2687469548, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540378039, 'issue_id': 2687469548, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80682"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80682"">No</a>', 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-26 13:37:15 UTC): @LongZE666,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/69213 and https://github.com/tensorflow/tensorflow/issues/63033 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

LongZE666 (Issue Creator) on (2024-11-27 02:13:54 UTC): Okay, I will pay attention to these issues, thank you

github-actions[bot] on (2024-12-05 02:08:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-13 02:08:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-13 02:08:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80682"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80682"">No</a>

"
2687194846,issue,open,,Aborted (core dumped) in `tf.raw_ops.SparseConcat`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseConcat` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)
values1 = tf.constant(""aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac"",
    shape=[3], dtype=tf.string)
shapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)

indices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)
values2 = tf.constant("" "", shape=[4], dtype=tf.string)
shapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)

concat_dim = 1
tf.raw_ops.SparseConcat(
    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None
)
```


### Relevant log output

```shell
2024-11-24 06:36:10.994508: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
",LongZE666,2024-11-24 06:40:09+00:00,['Venkat6871'],2024-12-19 04:13:57+00:00,,https://github.com/tensorflow/tensorflow/issues/80669,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2499658410, 'issue_id': 2687194846, 'author': 'Venkat6871', 'body': 'Hi **@LongZE666** ,\r\nIs there a specific reason for using such long values? I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. However, I tried an alternative approach, and it worked fine for me. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/859a635b37aa7ba023733f3097f954ee/80669_tf_2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 26, 4, 55, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004728, 'issue_id': 2687194846, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522022530, 'issue_id': 2687194846, 'author': 'LongZE666', 'body': '@Venkat6871 \r\nThanks for your reply, I used such long values \u200b\u200bmainly to test for bugs in the operator.', 'created_at': datetime.datetime(2024, 12, 6, 3, 3, 48, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-26 04:55:58 UTC): Hi **@LongZE666** ,
Is there a specific reason for using such long values? I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. However, I tried an alternative approach, and it worked fine for me. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/859a635b37aa7ba023733f3097f954ee/80669_tf_2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-12-04 02:08:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

LongZE666 (Issue Creator) on (2024-12-06 03:03:48 UTC): @Venkat6871 
Thanks for your reply, I used such long values mainly to test for bugs in the operator.

"
2686357423,issue,open,,Issue created for Rollback of PR #80574: Update graph_info.h to avoid warning: multi-line comment [-Wcomment],"Merged PR #80574 is rolled back in f377b15fc677b8ebaaeab3c5156ecf145c7f0631.
    Please follow up with the reviewer and close this issue once its resolved.",github-actions[bot],2024-11-23 18:02:05+00:00,"['gbaned', 'gaikwadrahul8', 'Venkat6871']",2024-11-23 18:02:06+00:00,,https://github.com/tensorflow/tensorflow/issues/80633,[],[],
2685781498,issue,closed,completed,Tensorflow installation issues,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl

### Custom code

Yes

### OS platform and distribution

Windows 11 Home

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

4060 Laptop

### Current behavior?

I originally wanted to install the tensorflow-gpu version, but why does it always download the intel version when I use ""pip install tensorflow""? There are no problems with my cuda and cudnn. How can I solve this problem?
![ 2024-11-23 181957](https://github.com/user-attachments/assets/025ee8ee-4c8c-4bee-9c50-7f6a2fb8886d)


### Standalone code to reproduce the issue

```shell
pip install --upgrade tensorflow
```


### Relevant log output

```shell
ages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)
Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)
Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)
```
",iwqculrbud,2024-11-23 10:26:20+00:00,['tilakrayal'],2024-11-30 16:36:55+00:00,2024-11-29 16:41:03+00:00,https://github.com/tensorflow/tensorflow/issues/80629,"[('type:build/install', 'Build and install issues'), ('subtype:cpu-intel', 'To track windows cpu issues'), ('TF 2.18', '')]","[{'comment_id': 2497987013, 'issue_id': 2685781498, 'author': 'tilakrayal', 'body': '@iwqculrbud,\r\nCould you please provide more information/context and also steps followed to install & the error log which helps to debug the issue in an effective way. \r\n\r\nCould you please install TensorFlow as per the above comment from official Tensorflow PyPi using pip install tensorflow==2.18.0 and close the issue. Thanks!\r\nhttps://pypi.org/project/tensorflow/#files\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 25, 13, 11, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499639662, 'issue_id': 2685781498, 'author': 'iwqculrbud', 'body': '> @iwqculrbud, Could you please provide more information/context and also steps followed to install & the error log which helps to debug the issue in an effective way. Thank you!\r\n\r\n@tilakrayal No problem, I have an intel i5-12500H and an Nvidia 4060 laptop on my device, but when I used pip to install it, I found that it seemed to install the intel optimized version (as shown in the picture), I want to install tensorflow optimized for nvidia.\r\nIs there any tensorflow optimized for nvidia graphics cards?\r\n![ 2024-11-26 123035](https://github.com/user-attachments/assets/d009df0d-0f9e-4fd3-a815-c4b8f7295fdc)', 'created_at': datetime.datetime(2024, 11, 26, 4, 36, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508132833, 'issue_id': 2685781498, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80629"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80629"">No</a>', 'created_at': datetime.datetime(2024, 11, 29, 16, 41, 5, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-25 13:11:18 UTC): @iwqculrbud,
Could you please provide more information/context and also steps followed to install & the error log which helps to debug the issue in an effective way. 

Could you please install TensorFlow as per the above comment from official Tensorflow PyPi using pip install tensorflow==2.18.0 and close the issue. Thanks!
https://pypi.org/project/tensorflow/#files
Thank you!

iwqculrbud (Issue Creator) on (2024-11-26 04:36:21 UTC): @tilakrayal No problem, I have an intel i5-12500H and an Nvidia 4060 laptop on my device, but when I used pip to install it, I found that it seemed to install the intel optimized version (as shown in the picture), I want to install tensorflow optimized for nvidia.
Is there any tensorflow optimized for nvidia graphics cards?
![ 2024-11-26 123035](https://github.com/user-attachments/assets/d009df0d-0f9e-4fd3-a815-c4b8f7295fdc)

google-ml-butler[bot] on (2024-11-29 16:41:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80629"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80629"">No</a>

"
2684374826,issue,closed,completed,PEP8 Violations training.py,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Incorrect spacing, violates PEP8 standards. Not using 4 spaces per indenation level. 
https://peps.python.org/pep-0008/#indentation

### Standalone code to reproduce the issue

```shell
https://peps.python.org/pep-0008/#indentation
```


### Relevant log output

_No response_",kp0119,2024-11-22 19:39:00+00:00,['Venkat6871'],2024-12-10 02:09:18+00:00,2024-12-10 02:09:14+00:00,https://github.com/tensorflow/tensorflow/issues/80590,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.18', '')]","[{'comment_id': 2497302961, 'issue_id': 2684374826, 'author': 'Venkat6871', 'body': 'Hi **@kp0119** ,\r\nApologies for the delay, and thank you for raising your concern here. This issue does not belong to TensorFlow. Please raise your issue in the appropriate repository.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 25, 8, 52, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513383803, 'issue_id': 2684374826, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047697, 'issue_id': 2684374826, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047745, 'issue_id': 2684374826, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80590"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80590"">No</a>', 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 16, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-25 08:52:31 UTC): Hi **@kp0119** ,
Apologies for the delay, and thank you for raising your concern here. This issue does not belong to TensorFlow. Please raise your issue in the appropriate repository.
Thank you!

github-actions[bot] on (2024-12-03 02:08:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-10 02:09:14 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-10 02:09:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80590"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80590"">No</a>

"
2683990651,issue,open,,tf.lite.Interpreter num_threads argument inconsistent documentation and functionality ,"The `tf.lite.Interpreter` [documentation](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) claims:

num_threads:

Sets the number of threads used by the interpreter and available to CPU kernels. If not set, the interpreter will use an implementation-dependent default number of threads. Currently, only a subset of kernels, such as conv, support multi-threading. num_threads should be >= -1. Setting num_threads to 0 has the effect to disable multithreading, which is equivalent to setting num_threads to 1. **If set to the value -1, the number of threads used will be implementation-defined and platform-dependent.** 


However, the [code](https://github.com/tensorflow/tensorflow/blob/857e530ca9cef0dc12ca4aa8fa488a682d189748/tensorflow/lite/python/interpreter.py#L465) is different

```python
    if num_threads is not None:
      if not isinstance(num_threads, int):
        raise ValueError('type of num_threads should be int')
      if num_threads < 1:
        raise ValueError('num_threads should >= 1')
```

Therefore, if the num_threads variable is -1, it raises ValueError.





",aimilefth,2024-11-22 17:32:06+00:00,['gaikwadrahul8'],2024-11-25 10:40:09+00:00,,https://github.com/tensorflow/tensorflow/issues/80579,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2497634700, 'issue_id': 2683990651, 'author': 'gaikwadrahul8', 'body': ""Hi, @aimilefth \r\nThank you for bringing this issue to our attention, I'll have a look into the source code and official documentation of [tf.lite.Interpreter](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) once and will update you. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 25, 10, 39, 52, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-25 10:39:52 UTC): Hi, @aimilefth 
Thank you for bringing this issue to our attention, I'll have a look into the source code and official documentation of [tf.lite.Interpreter](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) once and will update you. 

Thank you for your cooperation and patience.

"
2682430199,issue,open,,Not GPU detected using tensorflow/tensorflow:latest-gpu docker image,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow/tensorflow:latest-gpu

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c ""import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""
2024-11-22 08:20:55.695121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732263655.707089       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732263655.710704       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-22 08:20:55.722395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-22 08:20:57.119693: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW
2024-11-22 08:20:57.119716: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: 81f8d81af78d
2024-11-22 08:20:57.119720: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: 81f8d81af78d
2024-11-22 08:20:57.119789: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 545.23.6
2024-11-22 08:20:57.119804: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 470.256.2
2024-11-22 08:20:57.119808: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:262] kernel version 470.256.2 does not match DSO version 545.23.6 -- cannot find working devices in this configuration
Num GPUs Available: 0

### Standalone code to reproduce the issue

```shell
sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c ""import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""

I've also followed this recommendation. https://stackoverflow.com/questions/79127647/tensorflow-docker-not-using-gpu/79214187#79214187
```


### Relevant log output

```shell
root@35b972e97b30:/# nvidia-smi
Fri Nov 22 08:44:56 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 12.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   38C    P0    22W /  N/A |     10MiB /  5946MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
root@35b972e97b30:/# nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.3, V12.3.107
Build cuda_12.3.r12.3/compiler.33567101_0
```
",raul-parada,2024-11-22 08:46:38+00:00,"['MichaelHudgins', 'Venkat6871']",2025-02-07 17:01:30+00:00,,https://github.com/tensorflow/tensorflow/issues/80538,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2499862274, 'issue_id': 2682430199, 'author': 'Venkat6871', 'body': 'I able to reproduce the same behavior from my end with TensorFlow 2.18.0. I\'ve added output log below for reference\r\nHi, @learning-to-play Please take a look into this issue. Thank you.\r\n\r\n\r\n```\r\n(tf) (base) maayara@venkat-gpu1:~$ sudo docker run -it --rm  tensorflow/tensorflow:latest-gpu python\r\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf\r\n2024-11-26 07:16:02.810759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1732605362.831679       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1732605362.838272       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-26 07:16:02.860261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> tf.__version__\r\n\'2.18.0\'\r\n>>> print(tf.config.list_physical_devices(\'GPU\'))\r\n2024-11-26 07:16:22.228348: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)\r\n[]\r\n>>> \r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 26, 7, 25, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501967462, 'issue_id': 2682430199, 'author': 'learning-to-play', 'body': 'Adding @MichaelHudgins .\r\n@Venkat6871 Did you check with the TensorFlow GPU team?\r\nFYI, nightly CUDA builds are broken and someone is looking into addressing them. After the nightly issue is fixed, could you check if it also resolves this issue here?', 'created_at': datetime.datetime(2024, 11, 26, 21, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521570087, 'issue_id': 2682430199, 'author': 'MichaelHudgins', 'body': '@Venkat6871 what driver version did you replicate under?', 'created_at': datetime.datetime(2024, 12, 5, 22, 9, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529077750, 'issue_id': 2682430199, 'author': 'cantonios', 'body': 'From the error message, I think their local driver is 470.256.2.  They need at least 545.23.6.', 'created_at': datetime.datetime(2024, 12, 9, 18, 45, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530434295, 'issue_id': 2682430199, 'author': 'Venkat6871', 'body': 'Hi **@MichaelHudgins** ,\r\nApologies for the delay, Here i am attaching driver version.\r\n```\r\n(tf) maayara@venkat-gpu1:~$ nvidia-smi\r\nTue Dec 10 05:18:37 2024       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\r\n| N/A   59C    P8              11W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|  No running processes found                                                           |\r\n+---------------------------------------------------------------------------------------+\r\n(tf) maayara@venkat-gpu1:~$ \r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 10, 5, 24, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580738723, 'issue_id': 2682430199, 'author': 'anushkamittal20', 'body': 'Hey folks, I wanted to provide somethings I ran into while running the 2.18.0-gpu/latest-gpu/nightly-gpu version:\r\n```\r\nuser@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.18.0-gpu python -c ""import tensorflow as tf; print(t\r\nf.config.list_physical_devices(\'GPU\'))""\r\nUnable to find image \'tensorflow/tensorflow:2.18.0-gpu\' locally\r\n2.18.0-gpu: Pulling from tensorflow/tensorflow\r\nDigest: sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7\r\nStatus: Downloaded newer image for tensorflow/tensorflow:2.18.0-gpu\r\n2025-01-09 16:07:25.065536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736438845.373101       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1736438845.573143       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-09 16:07:26.230462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nW0000 00:00:1736438866.934009       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\nuser@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:nightly-gpu python -c ""import tensorflow as tf; print(\r\ntf.config.list_physical_devices(\'GPU\'))""\r\nUnable to find image \'tensorflow/tensorflow:nightly-gpu\' locally\r\nnightly-gpu: Pulling from tensorflow/tensorflow\r\naece8493d397: Already exists \r\n03bb9eb021f5: Already exists \r\nd1937dd2edf2: Already exists \r\n89aa5c6f8794: Already exists \r\n7d4f0f8effa7: Already exists \r\n906b440bb5c3: Pull complete \r\n59efbb3557a0: Pull complete \r\n662563cd75b6: Pull complete \r\n7243a54c6955: Pull complete \r\nac1e5c9999c2: Pull complete \r\n3116b6af6cb7: Pull complete \r\n586e9a0ac52e: Pull complete \r\nf9b10396c14f: Pull complete \r\n2b440f334acd: Pull complete \r\n48533fd57cb4: Pull complete \r\n31b964a9fe00: Pull complete \r\n80e34bf604c2: Pull complete \r\n0bd20751bf56: Pull complete \r\nDigest: sha256:62ed11af440e81f0dede0f60fe2e01c45ceaa126a4a3534c23f7368dd1c57645\r\nStatus: Downloaded newer image for tensorflow/tensorflow:nightly-gpu\r\n2025-01-09 16:11:18.795161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736439078.856858       1 cuda_dnn.cc:8597] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1736439078.866924       1 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-09 16:11:19.194013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[]\r\nuser@gpu-node:~$ docker run --rm --gpus all tensorflow/tensorflow:latest-gpu python -c ""import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))""\r\n2025-01-09 14:21:03.803765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736432463.826193       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1736432463.833283       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-09 14:21:03.856870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nW0000 00:00:1736432468.145918       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\n```\r\nvs what I see with 2.17.0 and 2.15.0\r\n```\r\nuser@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.15.0-gpu python -c ""import tensorflow as tf; print(t\r\nf.config.list_physical_devices(\'GPU\'))""\r\nUnable to find image \'tensorflow/tensorflow:2.15.0-gpu\' locally\r\n2.15.0-gpu: Pulling from tensorflow/tensorflow\r\naece8493d397: Already exists \r\n03bb9eb021f5: Already exists \r\nd1937dd2edf2: Already exists \r\n89aa5c6f8794: Already exists \r\n7d4f0f8effa7: Already exists \r\n345716e6b3b7: Pull complete \r\n5cc77b1d99a1: Pull complete \r\nd5ffb1df4a8a: Pull complete \r\nfcb2e2d5ed6e: Pull complete \r\ne9bd5055fa70: Pull complete \r\nbcbc616aad4f: Pull complete \r\n398b2febfc6d: Pull complete \r\nbe5f9db73548: Pull complete \r\nfa267638a1c2: Pull complete \r\ndb08e1d9a0fe: Pull complete \r\n93267ffd0a74: Pull complete \r\n05d2bbef6f77: Pull complete \r\n50f4f00a4047: Pull complete \r\nDigest: sha256:66b44c162783bb92ab6f44c1b38bcdfef70af20937089deb7bc20a4f3d7e5491\r\nStatus: Downloaded newer image for tensorflow/tensorflow:2.15.0-gpu\r\n2025-01-09 16:02:23.236877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2025-01-09 16:02:23.236942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2025-01-09 16:02:23.304111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-09 16:02:24.237674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2025-01-09 16:02:32.776608: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n2025-01-09 16:02:33.515790: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n2025-01-09 16:02:33.520549: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\nuser@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.17.0-gpu python -c ""import tensorflow as tf; print(t\r\nf.config.list_physical_devices(\'GPU\'))""\r\nUnable to find image \'tensorflow/tensorflow:2.17.0-gpu\' locally\r\n2.17.0-gpu: Pulling from tensorflow/tensorflow\r\naece8493d397: Already exists \r\n03bb9eb021f5: Already exists \r\nd1937dd2edf2: Already exists \r\n89aa5c6f8794: Already exists \r\n7d4f0f8effa7: Already exists \r\n2b96df44bf04: Pull complete \r\n98a3a7b98ee2: Pull complete \r\n085bf17cd963: Pull complete \r\n9753a4fc157e: Pull complete \r\n907967daf460: Pull complete \r\nd68083f7ecd4: Pull complete \r\nce45b30a7497: Pull complete \r\nac38ef6a5992: Pull complete \r\n7a44a5335b1e: Pull complete \r\nb6bff34c4cba: Pull complete \r\n502417f80432: Pull complete \r\n586cc130fa2d: Pull complete \r\n8dc063a6a573: Pull complete \r\nDigest: sha256:ebf7ad13136740adeee241fcfe6b14d646e431c6bc5c151d528e31ddafde4623\r\nStatus: Downloaded newer image for tensorflow/tensorflow:2.17.0-gpu\r\n2025-01-09 16:06:37.900711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2025-01-09 16:06:37.925632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2025-01-09 16:06:37.950886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-09 16:06:38.482510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1736438814.084824       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1736438816.060124       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1736438816.063803       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\n```\r\n\r\nI am also sharing the output for `nvidia-smi` to shed light on the driver/cuda version:\r\n```\r\nuser@gpu-node:~$ nvidia-smi\r\nThu Jan  9 16:13:30 2025       \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\r\n| N/A   39C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n                                                                                         \r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|  No running processes found                                                             |\r\n+-----------------------------------------------------------------------------------------+\r\n```', 'created_at': datetime.datetime(2025, 1, 9, 16, 28, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591286240, 'issue_id': 2682430199, 'author': 'ngaywood', 'body': 'The tensorflow 2.18 container does not seem to have libcudnn.so.9 which tensorflow 2.18 tries to load\nThe tensorflow 2.17 container loads libcudnn.so.8 which is installed in both containers.\nThere is no libcudnn.so.9 installed in either container.\n\n`apptainer  pull docker://tensorflow/tensorflow:latest-gpu\napptainer run --nv tensorflow_latest-gpu.sif\n`\n\nIf I start my containers, both 2.17 and 2.18 and turn on debugging for TF:\n`export TF_CPP_MAX_VLOG_LEVEL=3\npython -c ""import tensorflow as tf; tf.config.list_physical_devices(\'GPU\')""\n`\n\nWith 2.17 (that works and finds the GPU), there is this line in the debug output:\n`Successfully opened dynamic library libcudnn.so.8\n`\n\nIn 2.18 (that does not work) we get:\n`Could not load dynamic library \'libcudnn.so.9\'; dlerror: libcudnn.so.9: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs\n`\n\nIn both containers we have:\n```\nApptainer> find /usr -name ""libcudnn*""\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n/usr/share/doc/libcudnn8\n/usr/share/lintian/overrides/libcudnn8\nApptainer>\n```\n\nThat is, no  libcudnn.so.9\ntensorflow 2.18 wants to load libcudnn.so.9', 'created_at': datetime.datetime(2025, 1, 14, 23, 5, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643489555, 'issue_id': 2682430199, 'author': 'jasonleekungfu', 'body': 'Following. I guess no fix of this issue yet?', 'created_at': datetime.datetime(2025, 2, 7, 17, 1, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-26 07:25:19 UTC): I able to reproduce the same behavior from my end with TensorFlow 2.18.0. I've added output log below for reference
Hi, @learning-to-play Please take a look into this issue. Thank you.


```
(tf) (base) maayara@venkat-gpu1:~$ sudo docker run -it --rm  tensorflow/tensorflow:latest-gpu python
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
2024-11-26 07:16:02.810759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732605362.831679       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732605362.838272       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-26 07:16:02.860261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
'2.18.0'
2024-11-26 07:16:22.228348: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)
[]
```
Thank you!

learning-to-play on (2024-11-26 21:28:00 UTC): Adding @MichaelHudgins .
@Venkat6871 Did you check with the TensorFlow GPU team?
FYI, nightly CUDA builds are broken and someone is looking into addressing them. After the nightly issue is fixed, could you check if it also resolves this issue here?

MichaelHudgins (Assginee) on (2024-12-05 22:09:19 UTC): @Venkat6871 what driver version did you replicate under?

cantonios on (2024-12-09 18:45:48 UTC): From the error message, I think their local driver is 470.256.2.  They need at least 545.23.6.

Venkat6871 (Assginee) on (2024-12-10 05:24:51 UTC): Hi **@MichaelHudgins** ,
Apologies for the delay, Here i am attaching driver version.
```
(tf) maayara@venkat-gpu1:~$ nvidia-smi
Tue Dec 10 05:18:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |
| N/A   59C    P8              11W /  70W |      2MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
(tf) maayara@venkat-gpu1:~$ 
```
Thank you!

anushkamittal20 on (2025-01-09 16:28:25 UTC): Hey folks, I wanted to provide somethings I ran into while running the 2.18.0-gpu/latest-gpu/nightly-gpu version:
```
user@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.18.0-gpu python -c ""import tensorflow as tf; print(t
f.config.list_physical_devices('GPU'))""
Unable to find image 'tensorflow/tensorflow:2.18.0-gpu' locally
2.18.0-gpu: Pulling from tensorflow/tensorflow
Digest: sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7
Status: Downloaded newer image for tensorflow/tensorflow:2.18.0-gpu
2025-01-09 16:07:25.065536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736438845.373101       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736438845.573143       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 16:07:26.230462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1736438866.934009       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
user@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:nightly-gpu python -c ""import tensorflow as tf; print(
tf.config.list_physical_devices('GPU'))""
Unable to find image 'tensorflow/tensorflow:nightly-gpu' locally
nightly-gpu: Pulling from tensorflow/tensorflow
aece8493d397: Already exists 
03bb9eb021f5: Already exists 
d1937dd2edf2: Already exists 
89aa5c6f8794: Already exists 
7d4f0f8effa7: Already exists 
906b440bb5c3: Pull complete 
59efbb3557a0: Pull complete 
662563cd75b6: Pull complete 
7243a54c6955: Pull complete 
ac1e5c9999c2: Pull complete 
3116b6af6cb7: Pull complete 
586e9a0ac52e: Pull complete 
f9b10396c14f: Pull complete 
2b440f334acd: Pull complete 
48533fd57cb4: Pull complete 
31b964a9fe00: Pull complete 
80e34bf604c2: Pull complete 
0bd20751bf56: Pull complete 
Digest: sha256:62ed11af440e81f0dede0f60fe2e01c45ceaa126a4a3534c23f7368dd1c57645
Status: Downloaded newer image for tensorflow/tensorflow:nightly-gpu
2025-01-09 16:11:18.795161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736439078.856858       1 cuda_dnn.cc:8597] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736439078.866924       1 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 16:11:19.194013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[]
user@gpu-node:~$ docker run --rm --gpus all tensorflow/tensorflow:latest-gpu python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
2025-01-09 14:21:03.803765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736432463.826193       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736432463.833283       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 14:21:03.856870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1736432468.145918       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```
vs what I see with 2.17.0 and 2.15.0
```
user@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.15.0-gpu python -c ""import tensorflow as tf; print(t
f.config.list_physical_devices('GPU'))""
Unable to find image 'tensorflow/tensorflow:2.15.0-gpu' locally
2.15.0-gpu: Pulling from tensorflow/tensorflow
aece8493d397: Already exists 
03bb9eb021f5: Already exists 
d1937dd2edf2: Already exists 
89aa5c6f8794: Already exists 
7d4f0f8effa7: Already exists 
345716e6b3b7: Pull complete 
5cc77b1d99a1: Pull complete 
d5ffb1df4a8a: Pull complete 
fcb2e2d5ed6e: Pull complete 
e9bd5055fa70: Pull complete 
bcbc616aad4f: Pull complete 
398b2febfc6d: Pull complete 
be5f9db73548: Pull complete 
fa267638a1c2: Pull complete 
db08e1d9a0fe: Pull complete 
93267ffd0a74: Pull complete 
05d2bbef6f77: Pull complete 
50f4f00a4047: Pull complete 
Digest: sha256:66b44c162783bb92ab6f44c1b38bcdfef70af20937089deb7bc20a4f3d7e5491
Status: Downloaded newer image for tensorflow/tensorflow:2.15.0-gpu
2025-01-09 16:02:23.236877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-09 16:02:23.236942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-09 16:02:23.304111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 16:02:24.237674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-09 16:02:32.776608: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-01-09 16:02:33.515790: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-01-09 16:02:33.520549: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
user@gpu-node:~$ docker run --rm --runtime nvidia --user root --gpus all tensorflow/tensorflow:2.17.0-gpu python -c ""import tensorflow as tf; print(t
f.config.list_physical_devices('GPU'))""
Unable to find image 'tensorflow/tensorflow:2.17.0-gpu' locally
2.17.0-gpu: Pulling from tensorflow/tensorflow
aece8493d397: Already exists 
03bb9eb021f5: Already exists 
d1937dd2edf2: Already exists 
89aa5c6f8794: Already exists 
7d4f0f8effa7: Already exists 
2b96df44bf04: Pull complete 
98a3a7b98ee2: Pull complete 
085bf17cd963: Pull complete 
9753a4fc157e: Pull complete 
907967daf460: Pull complete 
d68083f7ecd4: Pull complete 
ce45b30a7497: Pull complete 
ac38ef6a5992: Pull complete 
7a44a5335b1e: Pull complete 
b6bff34c4cba: Pull complete 
502417f80432: Pull complete 
586cc130fa2d: Pull complete 
8dc063a6a573: Pull complete 
Digest: sha256:ebf7ad13136740adeee241fcfe6b14d646e431c6bc5c151d528e31ddafde4623
Status: Downloaded newer image for tensorflow/tensorflow:2.17.0-gpu
2025-01-09 16:06:37.900711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-09 16:06:37.925632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-09 16:06:37.950886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-09 16:06:38.482510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1736438814.084824       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1736438816.060124       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1736438816.063803       1 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

I am also sharing the output for `nvidia-smi` to shed light on the driver/cuda version:
```
user@gpu-node:~$ nvidia-smi
Thu Jan  9 16:13:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |
| N/A   39C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

ngaywood on (2025-01-14 23:05:51 UTC): The tensorflow 2.18 container does not seem to have libcudnn.so.9 which tensorflow 2.18 tries to load
The tensorflow 2.17 container loads libcudnn.so.8 which is installed in both containers.
There is no libcudnn.so.9 installed in either container.

`apptainer  pull docker://tensorflow/tensorflow:latest-gpu
apptainer run --nv tensorflow_latest-gpu.sif
`

If I start my containers, both 2.17 and 2.18 and turn on debugging for TF:
`export TF_CPP_MAX_VLOG_LEVEL=3
python -c ""import tensorflow as tf; tf.config.list_physical_devices('GPU')""
`

With 2.17 (that works and finds the GPU), there is this line in the debug output:
`Successfully opened dynamic library libcudnn.so.8
`

In 2.18 (that does not work) we get:
`Could not load dynamic library 'libcudnn.so.9'; dlerror: libcudnn.so.9: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs
`

In both containers we have:
```
Apptainer> find /usr -name ""libcudnn*""
/usr/lib/x86_64-linux-gnu/libcudnn.so.8
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
/usr/share/doc/libcudnn8
/usr/share/lintian/overrides/libcudnn8
Apptainer>
```

That is, no  libcudnn.so.9
tensorflow 2.18 wants to load libcudnn.so.9

jasonleekungfu on (2025-02-07 17:01:28 UTC): Following. I guess no fix of this issue yet?

"
2681579961,issue,open,,Floating point exception (core dumped) in `tf.raw_ops.Reshape`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,  `tf.raw_ops.Reshape` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)
shape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)

tf.raw_ops.Reshape(tensor=tensor, shape=shape)
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
",LongZE666,2024-11-22 02:58:01+00:00,['tilakrayal'],2024-12-06 13:35:29+00:00,,https://github.com/tensorflow/tensorflow/issues/80529,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2495381636, 'issue_id': 2681579961, 'author': 'savi0tomy', 'body': ""Hi, I'd like to work on this issue. Could you assign this to me?"", 'created_at': datetime.datetime(2024, 11, 23, 7, 27, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497289691, 'issue_id': 2681579961, 'author': 'tilakrayal', 'body': '@savi0tomy,\r\nPlease raise the PR for contributing to the Tensorflow. Also refer to the official doc for the reference. https://www.tensorflow.org/community/contribute\r\n\r\nhttps://github.com/tensorflow/tensorflow/pulls', 'created_at': datetime.datetime(2024, 11, 25, 8, 46, 5, tzinfo=datetime.timezone.utc)}]","savi0tomy on (2024-11-23 07:27:24 UTC): Hi, I'd like to work on this issue. Could you assign this to me?

tilakrayal (Assginee) on (2024-11-25 08:46:05 UTC): @savi0tomy,
Please raise the PR for contributing to the Tensorflow. Also refer to the official doc for the reference. https://www.tensorflow.org/community/contribute

https://github.com/tensorflow/tensorflow/pulls

"
2681571264,issue,closed,completed,Aborted (core dumped) in `SparseTensorDenseMatMul`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseTensorDenseMatMul` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

a_indices = tf.constant(1879048192, shape=[2,2], dtype=tf.int64)
a_values = tf.constant(0, shape=[2], dtype=tf.float32)
a_shape = tf.constant([-1, 5], dtype=tf.int64)
b = tf.constant(0, shape=[5, 5], dtype=tf.float32)

result = tf.raw_ops.SparseTensorDenseMatMul(
    a_indices=a_indices,
    a_values=a_values,
    a_shape=a_shape,
    b=b
)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1
Aborted (core dumped)
```
",LongZE666,2024-11-22 02:49:05+00:00,['Venkat6871'],2024-12-10 02:09:20+00:00,2024-12-10 02:09:16+00:00,https://github.com/tensorflow/tensorflow/issues/80528,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2497177952, 'issue_id': 2681571264, 'author': 'Venkat6871', 'body': 'I request you to take a look at this #65724 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 11, 25, 8, 9, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497277126, 'issue_id': 2681571264, 'author': 'LongZE666', 'body': 'Thank you for your reply, I will pay attention to this issue', 'created_at': datetime.datetime(2024, 11, 25, 8, 39, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513383826, 'issue_id': 2681571264, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047727, 'issue_id': 2681571264, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047797, 'issue_id': 2681571264, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80528"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80528"">No</a>', 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 19, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-25 08:09:39 UTC): I request you to take a look at this #65724 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

LongZE666 (Issue Creator) on (2024-11-25 08:39:37 UTC): Thank you for your reply, I will pay attention to this issue

github-actions[bot] on (2024-12-03 02:08:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-10 02:09:15 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-10 02:09:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80528"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80528"">No</a>

"
2679480897,issue,closed,completed,How to compile and use tfrt_session?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I try to build tfrt_session independently and preload it when running tensorflow, but it failed for lacking of symbol.

### Standalone code to reproduce the issue

```shell
I need a way to build tfrt_session correctly
```


### Relevant log output

_No response_",MoFHeka,2024-11-21 13:27:54+00:00,['tilakrayal'],2024-12-07 02:06:41+00:00,2024-12-07 02:06:38+00:00,https://github.com/tensorflow/tensorflow/issues/80495,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2493886644, 'issue_id': 2679480897, 'author': 'tilakrayal', 'body': '@MoFHeka,\r\nCould you please provide more information/context and the environment details or the error logs which helps to analyse the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 11, 22, 14, 26, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508786253, 'issue_id': 2679480897, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 30, 2, 3, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804294, 'issue_id': 2679480897, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524804309, 'issue_id': 2679480897, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80495"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80495"">No</a>', 'created_at': datetime.datetime(2024, 12, 7, 2, 6, 40, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-22 14:26:24 UTC): @MoFHeka,
Could you please provide more information/context and the environment details or the error logs which helps to analyse the issue in an effective way. Thank you!

github-actions[bot] on (2024-11-30 02:03:51 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-07 02:06:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-07 02:06:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80495"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80495"">No</a>

"
2677022726,issue,closed,completed,Compiler warning in tensorflow/lite/graph_info.h,"This has been experienced using TF 2.18.0 and has been seen in the current master branch.

I have compiled the TensorFlow Lite shared library for use on an embedded device.  In order to compile my application against it, I am using the TF distribution's headers during the compilation process.  When my code includes `tensorflow/lite/interpreter.h`, I see the following warning from the compiler (GCC 11.3.0):

~~~~
In file included from .../tensorflow/include/tensorflow/lite/core/subgraph.h:42,
                 from .../tensorflow/include/tensorflow/lite/core/async/async_subgraph.h:27,
                 from .../tensorflow/include/tensorflow/lite/core/async/async_signature_runner.h:24,
                 from .../tensorflow/include/tensorflow/lite/core/interpreter.h:47,
                 from .../tensorflow/include/tensorflow/lite/interpreter.h:21,
                 from (my code):
.../tensorflow/include/tensorflow/lite/graph_info.h:127:1: warning: multi-line comment [-Wcomment]
  127 | //                    /------------\
      | ^
.../tensorflow/include/tensorflow/lite/graph_info.h:139:1: warning: multi-line comment [-Wcomment]
  139 | //                    /------------\
      | ^
~~~~

The reason for this is because that header has a backslash at the end of a line with a C++-style comment.  To get rid of the warning, someone should either:
* Put another non-whitespace character at the end of those two lines
* Convert the C++-style comment block to a C-style block (`/* ... */`)",Shamino0,2024-11-20 19:56:54+00:00,['gaikwadrahul8'],2024-12-10 13:52:27+00:00,2024-12-10 13:52:27+00:00,https://github.com/tensorflow/tensorflow/issues/80379,"[('comp:lite', 'TF Lite related issues'), ('awaiting PR merge', 'awaiting PR merge'), ('TF 2.18', '')]","[{'comment_id': 2494098568, 'issue_id': 2677022726, 'author': 'gaikwadrahul8', 'body': ""Hi, @Shamino0\r\nThank you for bringing this issue to our attention, I've submitted PR https://github.com/tensorflow/tensorflow/pull/80574 to take care this issue. \r\n\r\nThank you for your cooperation and understanding."", 'created_at': datetime.datetime(2024, 11, 22, 16, 1, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531699910, 'issue_id': 2677022726, 'author': 'gaikwadrahul8', 'body': ""Hi, @Shamino0 \r\nI see this PR has been merged https://github.com/tensorflow/tensorflow/pull/80574 so I'm closing this issue now. Thank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 10, 13, 52, 23, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-22 16:01:17 UTC): Hi, @Shamino0
Thank you for bringing this issue to our attention, I've submitted PR https://github.com/tensorflow/tensorflow/pull/80574 to take care this issue. 

Thank you for your cooperation and understanding.

gaikwadrahul8 (Assginee) on (2024-12-10 13:52:23 UTC): Hi, @Shamino0 
I see this PR has been merged https://github.com/tensorflow/tensorflow/pull/80574 so I'm closing this issue now. Thank you for your cooperation and patience.

"
2676379804,issue,open,,The documentation in `data_performance.ipynb` uses `py_function()` without an explanation,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the [""Better performance with the tf.data API"" guide](https://www.tensorflow.org/guide/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.

Curiously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https://github.com/tensorflow/docs/blob/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405/site/en/guide/data_performance.ipynb#L447-L450):

```python
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```

In the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:

```python
benchmark(
    ArtificialDataset()
    .map(mapped_function)
)
```

And, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:

```python
benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)
```

But, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:

```python
def mapped_function(s):
    # Do some hard pre-processing
    lambda: time.sleep(0.03), [], ()
    return s
```

In fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?

### Standalone code to reproduce the issue

```shell
Mentioned in the above description.
```


### Relevant log output

_No response_",khatchad,2024-11-20 16:01:24+00:00,['tilakrayal'],2025-01-31 10:53:35+00:00,,https://github.com/tensorflow/tensorflow/issues/80365,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug')]","[{'comment_id': 2493244123, 'issue_id': 2676379804, 'author': 'tilakrayal', 'body': '@khatchad,\r\nAs per the document, **tf.py_function** is used to run mapped functions in an eager context. This is important because it allows to use Python functions that are not natively supported by TensorFlow.\r\nCould you please try to contribute from your side with the pull request if the changes are required in the mentioned document. Thank you!', 'created_at': datetime.datetime(2024, 11, 22, 9, 1, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508786273, 'issue_id': 2676379804, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 30, 2, 3, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517965580, 'issue_id': 2676379804, 'author': 'khatchad', 'body': '@tilakrayal. I agree, but I think that the example currently given does not need `tf.py_function` to function correctly. It would seem to me that we are forcing eager execution to see some benefit from the parallelization, whereas hybridizing the mapping function actually seems to demoniate the speedup from parallelization.\r\n\r\nCould there be an example where `tf.py_function()` must be used to function correctly, then, we would *need* the parallelization to see a speedup?', 'created_at': datetime.datetime(2024, 12, 4, 16, 33, 1, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-22 09:01:46 UTC): @khatchad,
As per the document, **tf.py_function** is used to run mapped functions in an eager context. This is important because it allows to use Python functions that are not natively supported by TensorFlow.
Could you please try to contribute from your side with the pull request if the changes are required in the mentioned document. Thank you!

github-actions[bot] on (2024-11-30 02:03:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

khatchad (Issue Creator) on (2024-12-04 16:33:01 UTC): @tilakrayal. I agree, but I think that the example currently given does not need `tf.py_function` to function correctly. It would seem to me that we are forcing eager execution to see some benefit from the parallelization, whereas hybridizing the mapping function actually seems to demoniate the speedup from parallelization.

Could there be an example where `tf.py_function()` must be used to function correctly, then, we would *need* the parallelization to see a speedup?

"
2674935089,issue,open,,Does tfl.quantize support QI4 data type,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of tfl.quantize](https://www.tensorflow.org/mlir/tfl_ops#tflquantize_tflquantizeop) supports QI4 data types, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/quantize.cc;l=121) doesn't support QI4 data type, the implementation seems not be consistent with the doc.

",fujunwei,2024-11-20 08:30:29+00:00,['gaikwadrahul8'],2024-11-22 14:31:35+00:00,,https://github.com/tensorflow/tensorflow/issues/80335,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2493895238, 'issue_id': 2674935089, 'author': 'gaikwadrahul8', 'body': ""Hi, @fujunwei \r\nThank you for bringing this issue to our attention, You've raised an important point regarding the inconsistency between the documented support for QI4 data types in `tfl.quantize` and the actual kernel implementation. This discrepancy can lead to unexpected behavior and limitations in quantization processes. In this [official documentation](https://www.tensorflow.org/model_optimization/guide/quantization/training) in the  **Experiment with quantization and associated hardware** section  it says **TFLite conversion and kernel implementations only support 8-bit quantization.**\r\n\r\nIt seems like we need to update documented support for QI4 data types in `tfl.quantize` part if we do not support yet I'll discuss internally and will update you\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 22, 14, 30, 34, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-22 14:30:34 UTC): Hi, @fujunwei 
Thank you for bringing this issue to our attention, You've raised an important point regarding the inconsistency between the documented support for QI4 data types in `tfl.quantize` and the actual kernel implementation. This discrepancy can lead to unexpected behavior and limitations in quantization processes. In this [official documentation](https://www.tensorflow.org/model_optimization/guide/quantization/training) in the  **Experiment with quantization and associated hardware** section  it says **TFLite conversion and kernel implementations only support 8-bit quantization.**

It seems like we need to update documented support for QI4 data types in `tfl.quantize` part if we do not support yet I'll discuss internally and will update you

Thank you for your cooperation and patience.

"
2674727663,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.TridiagonalSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the diagonals argument is empty and the gpu is available, tf.raw_ops.TridiagonalSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


diagonals = tf.cast(
    tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex64
)
rhs = tf.cast(
    tf.random.uniform([2], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex64
)
partial_pivoting = False
perturb_singular = False

result = tf.raw_ops.TridiagonalSolve(
    diagonals=diagonals,
    rhs=rhs,
    partial_pivoting=partial_pivoting,
    perturb_singular=perturb_singular
)

print(result)
```


### Relevant log output

```shell
2024-11-20 15:07:55.059745: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 15:07:55.070582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 15:07:55.108323: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 15:07:55.132372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 15:07:55.187323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 15:08:02.847006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 15:08:02.849328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 234 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 15:08:03.744404: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-11-20 07:21:35+00:00,['tilakrayal'],2024-12-10 02:09:24+00:00,2024-12-10 02:09:17+00:00,https://github.com/tensorflow/tensorflow/issues/80334,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2497962628, 'issue_id': 2674727663, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nHave you got the chance to check the commit which was created for the tf.raw_ops.TridiagonalSolve has been implemented. \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412\r\n\r\n```python\r\n  OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\r\n                                                          &input_shape));\r\n    input_matrix_shapes->push_back(std::move(input_shape));\r\n```', 'created_at': datetime.datetime(2024, 11, 25, 13, 1, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513383884, 'issue_id': 2674727663, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047748, 'issue_id': 2674727663, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047864, 'issue_id': 2674727663, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80334"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80334"">No</a>', 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 22, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-25 13:01:42 UTC): @x0w3n,
Have you got the chance to check the commit which was created for the tf.raw_ops.TridiagonalSolve has been implemented. 

https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412

```python
  OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},
                                                          &input_shape));
    input_matrix_shapes->push_back(std::move(input_shape));
```

github-actions[bot] on (2024-12-03 02:08:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-10 02:09:16 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-10 02:09:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80334"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80334"">No</a>

"
2674701175,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.RaggedTensorToVariantGradient`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

With a specific input and the gpu available, tf.raw_ops.RaggedTensorToVariantGradient triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


encoded_ragged_grad = tf.data.experimental.to_variant(
    tf.data.Dataset.from_tensor_slices([1, 2, 3])
)
row_splits = tf.random.uniform(
    [2, 2, 2, 1, 8, 8, 2, 1],
    dtype=tf.dtypes.int32,
    minval=-100000,
    maxval=1000000
)
dense_values_shape = tf.random.uniform(
    [0, 0, 4, 1],
    dtype=tf.dtypes.int32,
    minval=-100000,
    maxval=1000000
)
Tvalues = tf.dtypes.int32


result = tf.raw_ops.RaggedTensorToVariantGradient(
    encoded_ragged_grad=encoded_ragged_grad,
    row_splits=row_splits,
    dense_values_shape=dense_values_shape,
    Tvalues=Tvalues
)

print(result)
```


### Relevant log output

```shell
2024-11-20 14:54:41.997416: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:54:42.057982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:54:42.134026: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:54:42.158043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:54:42.178297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:54:49.604912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:54:49.607192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 234 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:54:50.489456: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 4)Asking for tensor of 1 dimensions from a tensor of 4 dimensions
Aborted (core dumped)
```
",x0w3n,2024-11-20 07:05:56+00:00,['Venkat6871'],2024-12-12 02:08:06+00:00,2024-12-12 02:08:03+00:00,https://github.com/tensorflow/tensorflow/issues/80332,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2497119521, 'issue_id': 2674701175, 'author': 'Venkat6871', 'body': 'Hi **@x0w3n** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. However, I tried an alternative approach, and it worked fine for me. I hope this will be helpful to you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/9567e5c2f1e3167ad1f713397432c00a/80332_tf-2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 25, 7, 44, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499324981, 'issue_id': 2674701175, 'author': 'x0w3n', 'body': 'Thank you for the suggestion.', 'created_at': datetime.datetime(2024, 11, 26, 0, 44, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004808, 'issue_id': 2674701175, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594373, 'issue_id': 2674701175, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 12, 2, 8, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594444, 'issue_id': 2674701175, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80332"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80332"">No</a>', 'created_at': datetime.datetime(2024, 12, 12, 2, 8, 5, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-25 07:44:54 UTC): Hi **@x0w3n** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. However, I tried an alternative approach, and it worked fine for me. I hope this will be helpful to you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/9567e5c2f1e3167ad1f713397432c00a/80332_tf-2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

x0w3n (Issue Creator) on (2024-11-26 00:44:53 UTC): Thank you for the suggestion.

github-actions[bot] on (2024-12-04 02:08:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-12 02:08:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-12 02:08:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80332"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80332"">No</a>

"
2674675963,issue,open,,Aborted (core dumped) in `tf.raw_ops.MatrixSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)
```


### Relevant log output

```shell
2024-11-20 14:51:08.714846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:51:08.775383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:51:08.852267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:51:08.876168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:51:08.931206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:51:16.385650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:51:16.387914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:51:16.542383: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-11-20 06:53:21+00:00,['tilakrayal'],2024-12-06 13:36:04+00:00,,https://github.com/tensorflow/tensorflow/issues/80331,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2497344393, 'issue_id': 2674675963, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nHave you tried by providing the input value as a tensor rather than the empty dataset? I tried and it is working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ee9d4ef694a810f7c987da488f072dbc/untitled2248.ipynb). Also the input is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices.', 'created_at': datetime.datetime(2024, 11, 25, 9, 8, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499324246, 'issue_id': 2674675963, 'author': 'x0w3n', 'body': 'I appreciate your response. I tested providing the input as a tensor instead of an empty dataset, and it works as expected. However, we hope the API could handle such corner cases more robustly to prevent crashes. Thanks again for your help.', 'created_at': datetime.datetime(2024, 11, 26, 0, 44, 7, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-25 09:08:03 UTC): @x0w3n,
Have you tried by providing the input value as a tensor rather than the empty dataset? I tried and it is working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ee9d4ef694a810f7c987da488f072dbc/untitled2248.ipynb). Also the input is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices.

x0w3n (Issue Creator) on (2024-11-26 00:44:07 UTC): I appreciate your response. I tested providing the input as a tensor instead of an empty dataset, and it works as expected. However, we hope the API could handle such corner cases more robustly to prevent crashes. Thanks again for your help.

"
2674279471,issue,closed,completed,tflite int8 export is twice as large as saved_model.pb,"### 1. System information

Windows 11

### 2. Code

import tensorflow as tf

saved_model_dir = ""C:/Users/s/Downloads/best (1)_saved_model""

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_dataset_gen():
    for _ in range(100): 
        yield [tf.random.normal([1, 640, 640, 3])]

converter.representative_dataset = representative_dataset_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()

tflite_model_path = ""model_int8.tflite""
with open(tflite_model_path, ""wb"") as f:
    f.write(tflite_model)

![Screenshot 2024-11-19 231208](https://github.com/user-attachments/assets/a0e58c94-6bb3-4335-bc7f-e591efb18e49)

",ssdv1,2024-11-20 04:12:29+00:00,['gaikwadrahul8'],2024-12-21 02:00:21+00:00,2024-12-21 02:00:20+00:00,https://github.com/tensorflow/tensorflow/issues/80319,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2493801519, 'issue_id': 2674279471, 'author': 'gaikwadrahul8', 'body': 'Hi, @ssdv1 \r\nThank you for bringing this issue to our attention, if possible could you please help us with your saved_model along with Google colab notebook which you used to create saved_model also to replicate the similar behavior from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 22, 13, 43, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493839580, 'issue_id': 2674279471, 'author': 'ssdv1', 'body': ""> Hi, @ssdv1 Thank you for bringing this issue to our attention, if possible could you please help us with your saved_model along with Google colab notebook which you used to create saved_model also to replicate the similar behavior from our end ?\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\n[saved_model](https://drive.google.com/drive/folders/1cs9EoFSRrKJYR7uXGam5K5pDX3fu-xu8?usp=drive_link)\r\n\r\n[pytorch file used to create saved_model](https://drive.google.com/file/d/1WC-Uv8bYQfJQu-WNvB2E3TpHuHY4zmwK/view?usp=drive_link)\r\n\r\n`pip install ultralytics \r\nfrom ultralytics import yolo\r\nmodel=YOLO('best (1).pt') \r\nmodel.export(format='tflite')\r\n`\r\nthis code will export .pt file to saved_model"", 'created_at': datetime.datetime(2024, 11, 22, 14, 2, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493904954, 'issue_id': 2674279471, 'author': 'gaikwadrahul8', 'body': 'Hi, @ssdv1 \r\nI do not have access to your shared files please provide me access. Thank you', 'created_at': datetime.datetime(2024, 11, 22, 14, 35, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493910545, 'issue_id': 2674279471, 'author': 'ssdv1', 'body': '> Hi, @ssdv1 I do not have access to your shared files please provide me access. Thank you\r\n\r\nsorry for that. Access set for anyone with link', 'created_at': datetime.datetime(2024, 11, 22, 14, 38, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497959928, 'issue_id': 2674279471, 'author': 'gaikwadrahul8', 'body': ""Hi, @ssdv1 \r\nI apologize for the delayed response, I was trying to replicate the similar behavior from my end while converting **saved_model** to **TFLite** I'm getting this error message` ValueError: Only support at least one signature key.` w.r.t signature so I tried most common default signature but still it's not converting **saved_model** to **TFLite** so please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/5c48e7b77a204557824798277dc0c61a/copy-of-test-80319.ipynb) If I'm doing something wrong please let me know that will be greatly appreciated to replicate same behavior which you mentioned in issue template.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 25, 13, 0, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498451219, 'issue_id': 2674279471, 'author': 'ssdv1', 'body': ""> Hi, @ssdv1 I apologize for the delayed response, I was trying to replicate the similar behavior from my end while converting **saved_model** to **TFLite** I'm getting this error message` ValueError: Only support at least one signature key.` w.r.t signature so I tried most common default signature but still it's not converting **saved_model** to **TFLite** so please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/5c48e7b77a204557824798277dc0c61a/copy-of-test-80319.ipynb) If I'm doing something wrong please let me know that will be greatly appreciated to replicate same behavior which you mentioned in issue template.\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nI dont know about that error. I ran locally not on colab and it worked. Maybe some issue with env. Try doing it locally on your computer. My python version is 3.8 so maybe try with that. Did you try using my saved model directly. [Also check this](https://github.com/ultralytics/ultralytics/issues/17441) check last few comments"", 'created_at': datetime.datetime(2024, 11, 25, 16, 13, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520178764, 'issue_id': 2674279471, 'author': 'gaikwadrahul8', 'body': ""Hi, @ssdv1\r\nI apologize for the delayed response, I tried downgrading the TensorFlow version in Google colab but still I'm getting the same error message `ValueError: Only support at least one signature key.` here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/687927c9ada2721d79741ed46560c4dc/tflite-issue-80319.ipynb) for reference if possible could you please help me with your complete Jupyter notebook along with ultralytics and TensorFlow versions so I'll try on Windows system ? \r\n\r\nIf you're using older version of TensorFlow please give it try with latest TensorFlow version 2.18.0 and see is it resolving your issue or not ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 5, 12, 24, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540378024, 'issue_id': 2674279471, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557950039, 'issue_id': 2674279471, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 21, 2, 0, 20, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-22 13:43:33 UTC): Hi, @ssdv1 
Thank you for bringing this issue to our attention, if possible could you please help us with your saved_model along with Google colab notebook which you used to create saved_model also to replicate the similar behavior from our end ? 

Thank you for your cooperation and patience.

ssdv1 (Issue Creator) on (2024-11-22 14:02:44 UTC): [saved_model](https://drive.google.com/drive/folders/1cs9EoFSRrKJYR7uXGam5K5pDX3fu-xu8?usp=drive_link)

[pytorch file used to create saved_model](https://drive.google.com/file/d/1WC-Uv8bYQfJQu-WNvB2E3TpHuHY4zmwK/view?usp=drive_link)

`pip install ultralytics 
from ultralytics import yolo
model=YOLO('best (1).pt') 
model.export(format='tflite')
`
this code will export .pt file to saved_model

gaikwadrahul8 (Assginee) on (2024-11-22 14:35:21 UTC): Hi, @ssdv1 
I do not have access to your shared files please provide me access. Thank you

ssdv1 (Issue Creator) on (2024-11-22 14:38:04 UTC): sorry for that. Access set for anyone with link

gaikwadrahul8 (Assginee) on (2024-11-25 13:00:25 UTC): Hi, @ssdv1 
I apologize for the delayed response, I was trying to replicate the similar behavior from my end while converting **saved_model** to **TFLite** I'm getting this error message` ValueError: Only support at least one signature key.` w.r.t signature so I tried most common default signature but still it's not converting **saved_model** to **TFLite** so please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/5c48e7b77a204557824798277dc0c61a/copy-of-test-80319.ipynb) If I'm doing something wrong please let me know that will be greatly appreciated to replicate same behavior which you mentioned in issue template.

Thank you for your cooperation and patience.

ssdv1 (Issue Creator) on (2024-11-25 16:13:35 UTC): I dont know about that error. I ran locally not on colab and it worked. Maybe some issue with env. Try doing it locally on your computer. My python version is 3.8 so maybe try with that. Did you try using my saved model directly. [Also check this](https://github.com/ultralytics/ultralytics/issues/17441) check last few comments

gaikwadrahul8 (Assginee) on (2024-12-05 12:24:51 UTC): Hi, @ssdv1
I apologize for the delayed response, I tried downgrading the TensorFlow version in Google colab but still I'm getting the same error message `ValueError: Only support at least one signature key.` here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/687927c9ada2721d79741ed46560c4dc/tflite-issue-80319.ipynb) for reference if possible could you please help me with your complete Jupyter notebook along with ultralytics and TensorFlow versions so I'll try on Windows system ? 

If you're using older version of TensorFlow please give it try with latest TensorFlow version 2.18.0 and see is it resolving your issue or not ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-13 02:08:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-21 02:00:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2674127483,issue,open,,Aborted (core dumped) in `tf.raw_ops.MatrixInverse`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)
```


### Relevant log output

```shell
2024-11-20 10:46:15.940818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:46:16.001155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:46:16.076386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:46:16.100080: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:46:16.154057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:46:23.889652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:46:23.891964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:46:24.756574: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-11-20 02:47:58+00:00,['tilakrayal'],2024-12-06 13:35:57+00:00,,https://github.com/tensorflow/tensorflow/issues/80316,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2493144606, 'issue_id': 2674127483, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nHave you tried by providing the input value as a tensor rather than the empty dataset? I tried and it is working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/5bb5d220d46f49faf4c5478c81bac4f8/untitled2240.ipynb). Also the input is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixInverse\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 22, 8, 19, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499322431, 'issue_id': 2674127483, 'author': 'x0w3n', 'body': 'Thank you for the suggestion. I tried providing the input as a tensor instead of an empty dataset, and it works as expected in that case. However, ideally, we would hope the API could handle such corner cases more properly to prevent it from crashing. Thanks again!', 'created_at': datetime.datetime(2024, 11, 26, 0, 42, 12, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-22 08:19:17 UTC): @x0w3n,
Have you tried by providing the input value as a tensor rather than the empty dataset? I tried and it is working as expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/5bb5d220d46f49faf4c5478c81bac4f8/untitled2240.ipynb). Also the input is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices.

https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixInverse

Thank you!

x0w3n (Issue Creator) on (2024-11-26 00:42:12 UTC): Thank you for the suggestion. I tried providing the input as a tensor instead of an empty dataset, and it works as expected in that case. However, ideally, we would hope the API could handle such corner cases more properly to prevent it from crashing. Thanks again!

"
2674116576,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.MatrixDeterminant/tf.raw_ops.LogMatrixDeterminant`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixDeterminant/ tf.raw_ops.LogMatrixDeterminant triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.LogMatrixDeterminant(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex64),)


import tensorflow as tf
tf.raw_ops.MatrixDeterminant(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex64),)
```


### Relevant log output

```shell
2024-11-20 10:33:29.999480: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:33:30.059256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:33:30.136255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:33:30.160665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:33:30.216533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:33:37.860746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:33:37.863142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:33:38.666859: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-11-20 02:36:50+00:00,['Venkat6871'],2024-12-12 02:08:10+00:00,2024-12-12 02:08:05+00:00,https://github.com/tensorflow/tensorflow/issues/80315,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2497005219, 'issue_id': 2674116576, 'author': 'Venkat6871', 'body': 'Hi **@x0w3n** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. As an alternative, I used a non empty argument and it worked fine. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ad1be2264c5c69dc4c073de9b4824766/80315_tf-2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 25, 6, 52, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499325113, 'issue_id': 2674116576, 'author': 'x0w3n', 'body': 'Thank you for the suggestion.', 'created_at': datetime.datetime(2024, 11, 26, 0, 44, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004852, 'issue_id': 2674116576, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594410, 'issue_id': 2674116576, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 12, 2, 8, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537594508, 'issue_id': 2674116576, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80315"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80315"">No</a>', 'created_at': datetime.datetime(2024, 12, 12, 2, 8, 8, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-25 06:52:36 UTC): Hi **@x0w3n** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. As an alternative, I used a non empty argument and it worked fine. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ad1be2264c5c69dc4c073de9b4824766/80315_tf-2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

x0w3n (Issue Creator) on (2024-11-26 00:44:58 UTC): Thank you for the suggestion.

github-actions[bot] on (2024-12-04 02:08:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-12 02:08:04 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-12 02:08:08 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80315"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80315"">No</a>

"
2674103396,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.CropAndResizeGradBoxes`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

With specific input, tf.raw_ops.CropAndResizeGradBoxes triggers a crash.
It can be reproduced on tf-nightly.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
grads = tf.random.uniform([0, 8, 1, 2], dtype=tf.dtypes.float32, maxval=100000000)
image = tf.random.uniform([0, 8, 1, 2], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)
boxes = tf.random.uniform([0], dtype=tf.dtypes.float32, maxval=100000000)
box_ind = tf.random.uniform([1, 8, 0, 0], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)
tf.raw_ops.CropAndResizeGradBoxes(grads=grads,image=image,boxes=boxes,box_ind=box_ind,method=""bilinear"",)
```


### Relevant log output

```shell
2024-11-20 10:20:02.514545: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:20:02.535316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:20:02.549411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:20:02.553443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:20:02.586202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:20:09.256556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:20:09.256908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:20:10.096248: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 4)Asking for tensor of 1 dimensions from a tensor of 4 dimensions
Aborted (core dumped)
```
",x0w3n,2024-11-20 02:24:56+00:00,['tilakrayal'],2024-11-26 00:37:07+00:00,2024-11-26 00:37:04+00:00,https://github.com/tensorflow/tensorflow/issues/80314,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2491825788, 'issue_id': 2674103396, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/65721 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 11, 21, 17, 15, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499317706, 'issue_id': 2674103396, 'author': 'x0w3n', 'body': ""Thanks for pointing out #65721. Ill go ahead and close this issue since it's covered by the other one. Thanks!"", 'created_at': datetime.datetime(2024, 11, 26, 0, 37, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499317738, 'issue_id': 2674103396, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80314"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80314"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 0, 37, 6, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-21 17:15:49 UTC): @x0w3n,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/65721 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

x0w3n (Issue Creator) on (2024-11-26 00:37:04 UTC): Thanks for pointing out #65721. Ill go ahead and close this issue since it's covered by the other one. Thanks!

google-ml-butler[bot] on (2024-11-26 00:37:06 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80314"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80314"">No</a>

"
2674073773,issue,closed,completed,Aborted (core dumped) in `tf.raw_ops.Cholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.Cholesky triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.Cholesky(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=10), dtype=tf.complex64),)
```


### Relevant log output

```shell
2024-11-20 09:57:25.507816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 09:57:25.568143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 09:57:25.643565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 09:57:25.666794: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 09:57:25.694104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 09:57:33.643380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 09:57:33.645593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 09:57:34.517615: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-11-20 01:59:39+00:00,['Venkat6871'],2024-12-10 02:09:26+00:00,2024-12-10 02:09:19+00:00,https://github.com/tensorflow/tensorflow/issues/80312,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2496924280, 'issue_id': 2674073773, 'author': 'Venkat6871', 'body': 'Hi **@x0w3n** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. As an alternative, I used a Hermitian matrix instead of a scalar matrix, and it worked fine. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5d049835658b4eab15f1eb8788725385/80312_tf-2-18-0-nightly-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 25, 5, 52, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513383908, 'issue_id': 2674073773, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047788, 'issue_id': 2674073773, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530047941, 'issue_id': 2674073773, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80312"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80312"">No</a>', 'created_at': datetime.datetime(2024, 12, 10, 2, 9, 25, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-25 05:52:42 UTC): Hi **@x0w3n** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. As an alternative, I used a Hermitian matrix instead of a scalar matrix, and it worked fine. I hope this helps you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5d049835658b4eab15f1eb8788725385/80312_tf-2-18-0-nightly-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-12-03 02:08:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-10 02:09:19 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-10 02:09:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80312"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80312"">No</a>

"
2673797570,issue,closed,completed,can not import tensorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

jupyter lab does not import tensorflow i brought python down from version 3.13 to 3.11

### Standalone code to reproduce the issue

```shell
import tensorflow
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[35], line 1
----> 1 import tensorflow

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\simil\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",similo01,2024-11-19 22:50:37+00:00,['tilakrayal'],2024-12-18 17:56:37+00:00,2024-12-18 17:56:34+00:00,https://github.com/tensorflow/tensorflow/issues/80293,"[('type:support', 'Support issues'), ('TF 2.8', '')]","[{'comment_id': 2490471809, 'issue_id': 2673797570, 'author': 'tilakrayal', 'body': '@similo01,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 21, 9, 16, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494236844, 'issue_id': 2673797570, 'author': 'similo01', 'body': 'I started with Python 3.13 and what i assumed is the latest version of Tensorflow. I realised when it was not getting installed that there is not Tensorflow for Python 3.13. I un installed and went down to Pythorn 3.11 and even python 3.9 and 3.8 and it would not installed completely though the commands seem to work in the command prompt but they would not work on the Jupyterlab. I went to the tensorrflow website and followed instructions there with no success', 'created_at': datetime.datetime(2024, 11, 22, 16, 46, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551947139, 'issue_id': 2673797570, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551947196, 'issue_id': 2673797570, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80293"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80293"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, 36, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-21 09:16:28 UTC): @similo01,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```
https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

similo01 (Issue Creator) on (2024-11-22 16:46:36 UTC): I started with Python 3.13 and what i assumed is the latest version of Tensorflow. I realised when it was not getting installed that there is not Tensorflow for Python 3.13. I un installed and went down to Pythorn 3.11 and even python 3.9 and 3.8 and it would not installed completely though the commands seem to work in the command prompt but they would not work on the Jupyterlab. I went to the tensorrflow website and followed instructions there with no success

mihaimaruseac on (2024-12-18 17:56:34 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:56:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80293"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80293"">No</a>

"
2673570212,issue,closed,completed,Code Readability in Keras Models,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I examined code in the Kera's model and was wondering if the code can be cleaned up. I noticed that lines 128-213 was very large, and is showing a massive function surrounded by other small functions. Could we break this up into smaller functions?

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/models.py to show the code. Please refer to lines 128-213.
```


### Relevant log output

_No response_",mrqsmar,2024-11-19 21:21:58+00:00,['Venkat6871'],2024-12-06 23:46:20+00:00,2024-12-06 02:07:44+00:00,https://github.com/tensorflow/tensorflow/issues/80282,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.8', '')]","[{'comment_id': 2490344638, 'issue_id': 2673570212, 'author': 'Venkat6871', 'body': 'Hi **@mrqsmar** ,\r\nThank you for raising your concern here. Yes, we can break large functions into smaller ones as needed. Feel free to do so in whatever way works best for you.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 21, 8, 15, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506954505, 'issue_id': 2673570212, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 29, 2, 6, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521929926, 'issue_id': 2673570212, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521929983, 'issue_id': 2673570212, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80282"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80282"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524649295, 'issue_id': 2673570212, 'author': 'mrqsmar', 'body': 'Hello, I have written the code and make a pull request', 'created_at': datetime.datetime(2024, 12, 6, 23, 46, 19, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-21 08:15:59 UTC): Hi **@mrqsmar** ,
Thank you for raising your concern here. Yes, we can break large functions into smaller ones as needed. Feel free to do so in whatever way works best for you.
Thank you!

github-actions[bot] on (2024-11-29 02:06:19 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-06 02:07:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-06 02:07:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80282"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80282"">No</a>

mrqsmar (Issue Creator) on (2024-12-06 23:46:19 UTC): Hello, I have written the code and make a pull request

"
2672797386,issue,open,,tf.gather and workarouds are very slow on TPU,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

TPU VM

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi all,

I am trying to solve a performance bug that occurs during pretraining/fine-tuning a DeBERTa model on TPUs.

In a nutshell, the `tf.gather` implemention is very slow on TPUs (but very fast on GPUs).

I am now looking for a way, that boosts up the `tf.one_hot` + `tf.einsum` trick, which would have massive impact in pretraining DeBERTa models to widespread it's usage.

There are some issues that have also reported this issue:

* https://github.com/huggingface/transformers/issues/18239
* https://github.com/keras-team/keras-hub/issues/606

But with no solution yet. Any help is highly appreciated!

### Standalone code to reproduce the issue

Here's a code snippet with an example:

```python
def take_along_axis_v2(x, indices):
    # Only a valid port of np.take_along_axis when the gather axis is -1

    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239
    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):
        # [B, S, P] -> [B, S, P, D]
        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)

        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)
        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]
        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)

    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls
    else:
        gathered = tf.gather(x, indices, batch_dims=2)

    return gathered
```

Taken from https://github.com/WissamAntoun/CamemBERTa/blob/1a1fb4a658729dfac2bb93842d88261132803ec3/modeling_tf_deberta_v2.py#L734-L750



### Relevant log output

_No response_",stefan-it,2024-11-19 16:43:31+00:00,['tilakrayal'],2024-12-03 04:31:21+00:00,,https://github.com/tensorflow/tensorflow/issues/80253,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:tpus', 'tpu, tpuestimator'), ('type:performance', 'Performance Issue'), ('TF 2.16', '')]","[{'comment_id': 2490463301, 'issue_id': 2672797386, 'author': 'tilakrayal', 'body': '@stefan-it,\r\nCould you please share a reproducible code that supports your statement so that it helps to debug the issue in an effective way and also please try to use the latest tensorflow v2.17 or v2.18 and provide the update. Thank you!', 'created_at': datetime.datetime(2024, 11, 21, 9, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499285687, 'issue_id': 2672797386, 'author': 'stefan-it', 'body': 'Hi @tilakrayal ,\r\n\r\nhere\'s some basic code to reproduce the bahaviour:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nprint(""TensorFlow version:"", tf.__version__)\r\n\r\nstart = time.time()\r\n\r\ndef take_along_axis(x, indices):\r\n    flat_x = tf.reshape(x, (-1, x.shape[-1]))\r\n    flat_indices = tf.reshape(indices, (-1, indices.shape[-1]))\r\n    gathered = tf.gather(flat_x, flat_indices, batch_dims=1)\r\n    gathered = tf.reshape(gathered, indices.shape)\r\n\r\n    return gathered\r\n\r\nfor i in range(0, 5000):\r\n    x_shape = [32, 128, 512]\r\n    indices_shape = [32, 128, 128]\r\n    x = tf.random.uniform(shape=x_shape)\r\n    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n    gathered_original = take_along_axis(x, indices)\r\n\r\nend = time.time()\r\n\r\nprint(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\nOn my RTX 3090 with TF 2.16.1 (on GPU) this code takes ~5.16 seconds. Without GPU-support 9.74 seconds.\r\n\r\nOn a **v4-32** TPU VM Pod (TF 2.16.1) I ran this example:\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\'roberta\')\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nprint(""All devices: "", tf.config.list_logical_devices(\'TPU\'))\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\ndef take_along_axis(x, indices):\r\n    flat_x = tf.reshape(x, (-1, x.shape[-1]))\r\n    flat_indices = tf.reshape(indices, (-1, indices.shape[-1]))\r\n    gathered = tf.gather(flat_x, flat_indices, batch_dims=1)\r\n    gathered = tf.reshape(gathered, indices.shape)\r\n\r\n    return gathered\r\n\r\nwith strategy.scope():\r\n    start = time.time()\r\n    for i in range(0, 5000):\r\n        x_shape = [32, 128, 512]\r\n        indices_shape = [32, 128, 128]\r\n        x = tf.random.uniform(shape=x_shape)\r\n        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n        gathered_original = take_along_axis(x, indices)\r\n\r\n    end = time.time()\r\n    print(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\nAnd it took 13.86 seconds.', 'created_at': datetime.datetime(2024, 11, 26, 0, 7, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500543672, 'issue_id': 2672797386, 'author': 'WissamAntoun', 'body': '@stefan-it How long does it take for `gathered = tf.gather(x, indices, batch_dims=2)` or for the `one_hot+einsum` variant?', 'created_at': datetime.datetime(2024, 11, 26, 12, 3, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502358143, 'issue_id': 2672797386, 'author': 'stefan-it', 'body': 'Thanks @WissamAntoun , I forgot to run experiments with version 2:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nprint(""TensorFlow version:"", tf.__version__)\r\n\r\nstart = time.time()\r\n\r\ndef take_along_axis_v2(x, indices):\r\n    # Only a valid port of np.take_along_axis when the gather axis is -1\r\n\r\n    # TPU + gathers and reshapes don\'t go along well -- see https://github.com/huggingface/transformers/issues/18239\r\n    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):\r\n        # [B, S, P] -> [B, S, P, D]\r\n        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)\r\n\r\n        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)\r\n        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]\r\n        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)\r\n\r\n    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls\r\n    else:\r\n        gathered = tf.gather(x, indices, batch_dims=2)\r\n\r\n    return gathered\r\n\r\nfor i in range(0, 5000):\r\n    x_shape = [32, 128, 512]\r\n    indices_shape = [32, 128, 128]\r\n    x = tf.random.uniform(shape=x_shape)\r\n    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n    gathered_original = take_along_axis_v2(x, indices)\r\n\r\nend = time.time()\r\n\r\nprint(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\nOn my GPU it runs in 1.71 seconds.\r\n\r\nSame code on the TPU VM Pod using:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\'roberta\')\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nprint(""All devices: "", tf.config.list_logical_devices(\'TPU\'))\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\ndef take_along_axis_v2(x, indices):\r\n    # Only a valid port of np.take_along_axis when the gather axis is -1\r\n\r\n    # TPU + gathers and reshapes don\'t go along well -- see https://github.com/huggingface/transformers/issues/18239\r\n    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):\r\n        # [B, S, P] -> [B, S, P, D]\r\n        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)\r\n\r\n        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)\r\n        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]\r\n        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)\r\n\r\n    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls\r\n    else:\r\n        gathered = tf.gather(x, indices, batch_dims=2)\r\n\r\n    return gathered\r\n\r\nwith strategy.scope():\r\n    start = time.time()\r\n    for i in range(0, 5000):\r\n        x_shape = [32, 128, 512]\r\n        indices_shape = [32, 128, 128]\r\n        x = tf.random.uniform(shape=x_shape)\r\n        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n        gathered_original = take_along_axis_v2(x, indices)\r\n\r\n    end = time.time()\r\n    print(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\ntakes 522 seconds!', 'created_at': datetime.datetime(2024, 11, 27, 0, 46, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502360912, 'issue_id': 2672797386, 'author': 'stefan-it', 'body': 'Version 3 of the benchmark just uses `tf.gather`:\r\n\r\nGPU:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nprint(""TensorFlow version:"", tf.__version__)\r\n\r\nstart = time.time()\r\n\r\nfor i in range(0, 5000):\r\n    x_shape = [32, 128, 512]\r\n    indices_shape = [32, 128, 128]\r\n    x = tf.random.uniform(shape=x_shape)\r\n    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n    gathered_original = tf.gather(x, indices, batch_dims=2)\r\n\r\nend = time.time()\r\n\r\nprint(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\nTakes 1.67 seconds.\r\n\r\nTPU code:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\'roberta\')\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nprint(""All devices: "", tf.config.list_logical_devices(\'TPU\'))\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n    start = time.time()\r\n    for i in range(0, 5000):\r\n        x_shape = [32, 128, 512]\r\n        indices_shape = [32, 128, 128]\r\n        x = tf.random.uniform(shape=x_shape)\r\n        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)\r\n        gathered_original = tf.gather(x, indices, batch_dims=2)\r\n\r\n    end = time.time()\r\n    print(""Elapsed time:"", round(end - start, 2), ""seconds"")\r\n```\r\n\r\nTakes 3.41 seconds.', 'created_at': datetime.datetime(2024, 11, 27, 0, 49, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502369812, 'issue_id': 2672797386, 'author': 'stefan-it', 'body': 'Overview:\r\n\r\n| Hardware | Version 1 (`tf.gather` + `tf.reshape`)  | Version 2 (`tf.one_hot` + `tf.einsum`)  | Version (`tf.gather`) |\r\n| ---------------|----------------------------------------------------- | -------------------------------------------------------|------------------------------|\r\n| RTX 3090 | 5.16 s | 1.71 s | 1.67 s |\r\n| v4-32 TPU VM Pod | 13.86s | 522.16 s  | 3.41 s |', 'created_at': datetime.datetime(2024, 11, 27, 0, 56, 39, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-21 09:13:00 UTC): @stefan-it,
Could you please share a reproducible code that supports your statement so that it helps to debug the issue in an effective way and also please try to use the latest tensorflow v2.17 or v2.18 and provide the update. Thank you!

stefan-it (Issue Creator) on (2024-11-26 00:07:11 UTC): Hi @tilakrayal ,

here's some basic code to reproduce the bahaviour:

```python
import time
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)

start = time.time()

def take_along_axis(x, indices):
    flat_x = tf.reshape(x, (-1, x.shape[-1]))
    flat_indices = tf.reshape(indices, (-1, indices.shape[-1]))
    gathered = tf.gather(flat_x, flat_indices, batch_dims=1)
    gathered = tf.reshape(gathered, indices.shape)

    return gathered

for i in range(0, 5000):
    x_shape = [32, 128, 512]
    indices_shape = [32, 128, 128]
    x = tf.random.uniform(shape=x_shape)
    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
    gathered_original = take_along_axis(x, indices)

end = time.time()

print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

On my RTX 3090 with TF 2.16.1 (on GPU) this code takes ~5.16 seconds. Without GPU-support 9.74 seconds.

On a **v4-32** TPU VM Pod (TF 2.16.1) I ran this example:

```
import time
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='roberta')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))
strategy = tf.distribute.TPUStrategy(resolver)

def take_along_axis(x, indices):
    flat_x = tf.reshape(x, (-1, x.shape[-1]))
    flat_indices = tf.reshape(indices, (-1, indices.shape[-1]))
    gathered = tf.gather(flat_x, flat_indices, batch_dims=1)
    gathered = tf.reshape(gathered, indices.shape)

    return gathered

with strategy.scope():
    start = time.time()
    for i in range(0, 5000):
        x_shape = [32, 128, 512]
        indices_shape = [32, 128, 128]
        x = tf.random.uniform(shape=x_shape)
        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
        gathered_original = take_along_axis(x, indices)

    end = time.time()
    print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

And it took 13.86 seconds.

WissamAntoun on (2024-11-26 12:03:21 UTC): @stefan-it How long does it take for `gathered = tf.gather(x, indices, batch_dims=2)` or for the `one_hot+einsum` variant?

stefan-it (Issue Creator) on (2024-11-27 00:46:11 UTC): Thanks @WissamAntoun , I forgot to run experiments with version 2:

```python
import time
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)

start = time.time()

def take_along_axis_v2(x, indices):
    # Only a valid port of np.take_along_axis when the gather axis is -1

    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239
    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):
        # [B, S, P] -> [B, S, P, D]
        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)

        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)
        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]
        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)

    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls
    else:
        gathered = tf.gather(x, indices, batch_dims=2)

    return gathered

for i in range(0, 5000):
    x_shape = [32, 128, 512]
    indices_shape = [32, 128, 128]
    x = tf.random.uniform(shape=x_shape)
    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
    gathered_original = take_along_axis_v2(x, indices)

end = time.time()

print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

On my GPU it runs in 1.71 seconds.

Same code on the TPU VM Pod using:

```python
import time
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='roberta')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))
strategy = tf.distribute.TPUStrategy(resolver)

def take_along_axis_v2(x, indices):
    # Only a valid port of np.take_along_axis when the gather axis is -1

    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239
    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):
        # [B, S, P] -> [B, S, P, D]
        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)

        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)
        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]
        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)

    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls
    else:
        gathered = tf.gather(x, indices, batch_dims=2)

    return gathered

with strategy.scope():
    start = time.time()
    for i in range(0, 5000):
        x_shape = [32, 128, 512]
        indices_shape = [32, 128, 128]
        x = tf.random.uniform(shape=x_shape)
        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
        gathered_original = take_along_axis_v2(x, indices)

    end = time.time()
    print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

takes 522 seconds!

stefan-it (Issue Creator) on (2024-11-27 00:49:06 UTC): Version 3 of the benchmark just uses `tf.gather`:

GPU:

```python
import time
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)

start = time.time()

for i in range(0, 5000):
    x_shape = [32, 128, 512]
    indices_shape = [32, 128, 128]
    x = tf.random.uniform(shape=x_shape)
    indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
    gathered_original = tf.gather(x, indices, batch_dims=2)

end = time.time()

print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

Takes 1.67 seconds.

TPU code:

```python
import time
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='roberta')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))
strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
    start = time.time()
    for i in range(0, 5000):
        x_shape = [32, 128, 512]
        indices_shape = [32, 128, 128]
        x = tf.random.uniform(shape=x_shape)
        indices = tf.random.uniform(shape=indices_shape, minval=1, maxval=128, dtype=tf.int32)
        gathered_original = tf.gather(x, indices, batch_dims=2)

    end = time.time()
    print(""Elapsed time:"", round(end - start, 2), ""seconds"")
```

Takes 3.41 seconds.

stefan-it (Issue Creator) on (2024-11-27 00:56:39 UTC): Overview:

| Hardware | Version 1 (`tf.gather` + `tf.reshape`)  | Version 2 (`tf.one_hot` + `tf.einsum`)  | Version (`tf.gather`) |
| ---------------|----------------------------------------------------- | -------------------------------------------------------|------------------------------|
| RTX 3090 | 5.16 s | 1.71 s | 1.67 s |
| v4-32 TPU VM Pod | 13.86s | 522.16 s  | 3.41 s |

"
2671429357,issue,open,,model.fit fails when the number of rows exceeds Int32.MaxValue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0-dev20241117

### Custom code

Yes

### OS platform and distribution

MacOS 15.1.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would expect model.fit to handle training on extremely large NumPy arrays without limitations.

### Standalone code to reproduce the issue

```shell
import numpy as np
from keras import Sequential
from keras.layers import Dense

n = 2_147_483_648
x = np.zeros(n).astype(np.float32)
y = x

model = Sequential([
    Dense(64, activation=""relu"", input_shape=(1,)),
    Dense(1, activation=""sigmoid"")
])
model.compile(optimizer=""adam"", loss=""binary_crossentropy"")
model.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)
```


### Relevant log output

```shell
ValueError: Invalid value in tensor used for shape: -2147483648
```
",github-clement-schiano,2024-11-19 09:24:51+00:00,['Venkat6871'],2024-11-21 10:47:34+00:00,,https://github.com/tensorflow/tensorflow/issues/80241,"[('type:bug', 'Bug'), ('TF 2.18', '')]","[{'comment_id': 2490758660, 'issue_id': 2671429357, 'author': 'kiransair', 'body': 'Hi @github-clement-schiano, May be tensorFlow uses int32 value for calculating the dimension of the data, as you are passing high dimension ( int32 + 1) causing the error. I tried to use  max of int32 which is 2_147_483_647 but the colab crashes due to high ram utilization. so I tried using a data generator to train the model on batch data and was able to train the model. please refer to this [gist](https://gist.github.com/kiransair/047edbeaf3ed0d2b50c1e7a65dd444d4). Thank You.', 'created_at': datetime.datetime(2024, 11, 21, 10, 40, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490774849, 'issue_id': 2671429357, 'author': 'github-clement-schiano', 'body': ""Hi @kiransair, Im working directly with NumPy arrays and don't want to use data generator. There shouldnt be any size limitations from NumPys perspective. A dataset with 2 billion rows isnt excessively large, and the machine Im using has plenty of RAM to handle it. Do you have any insights into what might be causing this issue?"", 'created_at': datetime.datetime(2024, 11, 21, 10, 47, 31, tzinfo=datetime.timezone.utc)}]","kiransair on (2024-11-21 10:40:10 UTC): Hi @github-clement-schiano, May be tensorFlow uses int32 value for calculating the dimension of the data, as you are passing high dimension ( int32 + 1) causing the error. I tried to use  max of int32 which is 2_147_483_647 but the colab crashes due to high ram utilization. so I tried using a data generator to train the model on batch data and was able to train the model. please refer to this [gist](https://gist.github.com/kiransair/047edbeaf3ed0d2b50c1e7a65dd444d4). Thank You.

github-clement-schiano (Issue Creator) on (2024-11-21 10:47:31 UTC): Hi @kiransair, Im working directly with NumPy arrays and don't want to use data generator. There shouldnt be any size limitations from NumPys perspective. A dataset with 2 billion rows isnt excessively large, and the machine Im using has plenty of RAM to handle it. Do you have any insights into what might be causing this issue?

"
2671052079,issue,closed,completed,Improve Documentation for TensorFlow Setup on Windows,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

Windows 10/11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The TensorFlow installation guide for Windows does not include detailed troubleshooting guidance for:
- Resolving PATH environment variable conflicts.
- Managing mismatches in CUDA and cuDNN versions during GPU setup.
- Debugging pip installation errors in virtual environments.
This creates challenges for new users trying to install TensorFlow on Windows, especially when dealing with GPU setup.



### Standalone code to reproduce the issue

```shell
A bare minimum reproducible test case to identify TensorFlow installation and GPU setup issues on Windows:

import tensorflow as tf

# Check TensorFlow version
print(""TensorFlow version:"", tf.__version__)

# Check GPU availability
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
    print(f""Number of GPUs detected: {len(gpu_devices)}"")
    for gpu in gpu_devices:
        print(f""GPU: {gpu}"")
else:
    print(""No GPUs detected. TensorFlow will run on CPU."")

# Perform a simple TensorFlow operation
try:
    print(""\nRunning a simple computation..."")
    a = tf.constant([[1, 2], [3, 4]])
    b = tf.constant([[5, 6], [7, 8]])
    c = tf.matmul(a, b)
    print(""Matrix multiplication result:\n"", c.numpy())
except Exception as e:
    print(""Error during TensorFlow computation:"", e)
```


### Relevant log output

_No response_",kayladoann,2024-11-19 07:19:44+00:00,['tilakrayal'],2024-12-06 02:07:50+00:00,2024-12-06 02:07:46+00:00,https://github.com/tensorflow/tensorflow/issues/80237,"[('type:docs-bug', 'Document issues'), ('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2490355664, 'issue_id': 2671052079, 'author': 'tilakrayal', 'body': '@kayladoann,\r\nIf you are specifically asking for the Windows, TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-). Instead you can use WSL2 - https://www.tensorflow.org/install/pip#windows-wsl2_1\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 21, 8, 22, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506954531, 'issue_id': 2671052079, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 29, 2, 6, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521929950, 'issue_id': 2671052079, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521930042, 'issue_id': 2671052079, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80237"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80237"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 49, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-21 08:22:15 UTC): @kayladoann,
If you are specifically asking for the Windows, TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-). Instead you can use WSL2 - https://www.tensorflow.org/install/pip#windows-wsl2_1

Thank you!

github-actions[bot] on (2024-11-29 02:06:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-06 02:07:45 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-06 02:07:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80237"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80237"">No</a>

"
2670610852,issue,closed,completed,Why does TensorBoard's Trace Viewer show blank waiting times?,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I ran the program on an x86 machine using oneDNN as the backend library and on an ARM machine using the default library. The TensorBoard profiling data shows blank waiting times on the ARM machine, while the computations on the x86 machine are very continuous. I am currently optimizing performance on the ARM machine and would like to understand the reasons behind the blank times in TensorBoards Trace Viewer on the ARM machine.

The TensorBoard data on the ARM machine shows a waiting gap before the two FusedMatmul operations.
![image](https://github.com/user-attachments/assets/f56d2028-11f5-43e5-ba17-547661f1627a)

computations on the x86 machine are very continuous
![image](https://github.com/user-attachments/assets/c8d6ed2d-0c2c-4d80-b23c-fdbce31d6f9f)


### Standalone code to reproduce the issue

```shell
I would like to know the reason for the waiting gap and how to resolve it. One hypothesis is that oneDNN uses asynchronous computation, while the default uses synchronous
```


### Relevant log output

_No response_",nanzh-19,2024-11-19 03:21:47+00:00,['gaikwadrahul8'],2024-12-25 02:00:37+00:00,2024-12-25 02:00:35+00:00,https://github.com/tensorflow/tensorflow/issues/80231,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2487752483, 'issue_id': 2670610852, 'author': 'Venkat6871', 'body': 'Hi **@nanzh-19** ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thank you!', 'created_at': datetime.datetime(2024, 11, 20, 7, 42, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487779527, 'issue_id': 2670610852, 'author': 'nanzh-19', 'body': ""I am running an inference service on the CPU, and the model is a typical recommendation model. I would like to understand how these blank times appear, and what exactly 'idle' means in TensorFlow."", 'created_at': datetime.datetime(2024, 11, 20, 7, 50, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528564129, 'issue_id': 2670610852, 'author': 'gaikwadrahul8', 'body': ""Hi, @nanzh-19 \r\n\r\nI apologize for the delayed response, As far I know there might be architectural differences ARM's RISC architecture exhibits different computational characteristics compared to x86 CISC architecture and unique instruction processing and cache coherency mechanisms.\r\n\r\nIt may be synchronization overhead like varied memory synchronization strategies, additional latency in thread and resource coordination, differences between oneDNN and default TensorFlow backend implementations\r\n\r\nCould you please try below some performance optimization strategies if you have not tried yet ? I would suggest you to please refer this official documentation of [Optimize TensorFlow performance using the Profiler](https://www.tensorflow.org/guide/profiler#trace_viewer) and [Debug the performance of one GPU](https://www.tensorflow.org/guide/gpu_performance_analysis) \r\n\r\nIn the context of GPU performance and training steps, `idle` refers to a state where the GPU is not actively performing computational work despite being available and powered on.\r\n  \r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\n# Optimize TensorFlow performance for ARM\r\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n\r\n# Threading and parallelism configuration\r\ntf.config.threading.set_intra_op_parallelism_threads(0)\r\ntf.config.threading.set_inter_op_parallelism_threads(0)\r\n\r\n# Enable XLA compilation\r\ntf.config.optimizer.set_jit(True)\r\n```\r\n\r\nIf I have missed something here please let me know. Thank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 9, 16, 26, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547346507, 'issue_id': 2670610852, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 17, 2, 7, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561542928, 'issue_id': 2670610852, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 25, 2, 0, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561542960, 'issue_id': 2670610852, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80231"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80231"">No</a>', 'created_at': datetime.datetime(2024, 12, 25, 2, 0, 37, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-11-20 07:42:41 UTC): Hi **@nanzh-19** ,
In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thank you!

nanzh-19 (Issue Creator) on (2024-11-20 07:50:18 UTC): I am running an inference service on the CPU, and the model is a typical recommendation model. I would like to understand how these blank times appear, and what exactly 'idle' means in TensorFlow.

gaikwadrahul8 (Assginee) on (2024-12-09 16:26:18 UTC): Hi, @nanzh-19 

I apologize for the delayed response, As far I know there might be architectural differences ARM's RISC architecture exhibits different computational characteristics compared to x86 CISC architecture and unique instruction processing and cache coherency mechanisms.

It may be synchronization overhead like varied memory synchronization strategies, additional latency in thread and resource coordination, differences between oneDNN and default TensorFlow backend implementations

Could you please try below some performance optimization strategies if you have not tried yet ? I would suggest you to please refer this official documentation of [Optimize TensorFlow performance using the Profiler](https://www.tensorflow.org/guide/profiler#trace_viewer) and [Debug the performance of one GPU](https://www.tensorflow.org/guide/gpu_performance_analysis) 

In the context of GPU performance and training steps, `idle` refers to a state where the GPU is not actively performing computational work despite being available and powered on.
  
```
import tensorflow as tf
import os

# Optimize TensorFlow performance for ARM
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# Threading and parallelism configuration
tf.config.threading.set_intra_op_parallelism_threads(0)
tf.config.threading.set_inter_op_parallelism_threads(0)

# Enable XLA compilation
tf.config.optimizer.set_jit(True)
```

If I have missed something here please let me know. Thank you for your cooperation and patience.

github-actions[bot] on (2024-12-17 02:07:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-25 02:00:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-25 02:00:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80231"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80231"">No</a>

"
2670266172,issue,closed,completed,Build Failure with nvcc: TensorFlow 2.15.0 with CUDA 12.2 on Jetson Orin Nano ,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

Ubantu 22.04 for jetson orin nano

### Mobile device

Jetson Linux 36.3

### Python version

3.10

### Bazel version

6.1.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

cuda 12.2.140, cuDNN 8.9.4.25

### GPU model and memory

NVIDIA Jetson orin nano devkit

### Current behavior?

I'm attempting to build TensorFlow 2.15.0 with CUDA support using Bazel for . However, the build process fails with multiple errors.
```
nvidia@nvidia-desktop:~/tensorflow_lib_2_15/tensorflow-2.15.0$ sudo ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3.10/dist-packages
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.10/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.10/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: Y
TensorRT support will be enabled for TensorFlow.

Found CUDA 12.2 in:
    /usr/local/cuda-12.2/targets/aarch64-linux/lib
    /usr/local/cuda-12.2/targets/aarch64-linux/include
Found cuDNN 8 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include
Found TensorRT 8.6.2 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include/aarch64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.7


Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

```

Key observations:

Warnings about the cuda config being expanded multiple times.
The issue persists even after trying alternative compilers like Clang 16 and integrating NVCC.
Below are logs with nvcc compiler 
[ERRORLOGS.txt](https://github.com/user-attachments/files/17807292/ERRORLOGS.txt)


### Standalone code to reproduce the issue

```shell
bazel build --config=cuda --jobs=2 //tensorflow:libtensorflow_cc.so
```


### Relevant log output

_No response_",Abhay-2412,2024-11-18 23:27:19+00:00,['tilakrayal'],2024-12-05 02:08:28+00:00,2024-12-05 02:08:25+00:00,https://github.com/tensorflow/tensorflow/issues/80223,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2489028071, 'issue_id': 2670266172, 'author': 'tilakrayal', 'body': '@Abhay-2412,\r\nThanks for reporting the issue. Looks like this is a known issue while building the TensorFlow v2.15 with aarch64.\r\nCould you please try to refer to the issue for the updates on the same. Also please check the version compatibility w.r.t CUDA and cuDNN\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/62490\r\nhttps://github.com/tensorflow/tensorflow/issues/59924\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 20, 16, 21, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505123994, 'issue_id': 2670266172, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930753, 'issue_id': 2670266172, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930801, 'issue_id': 2670266172, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80223"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80223"">No</a>', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 27, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-20 16:21:19 UTC): @Abhay-2412,
Thanks for reporting the issue. Looks like this is a known issue while building the TensorFlow v2.15 with aarch64.
Could you please try to refer to the issue for the updates on the same. Also please check the version compatibility w.r.t CUDA and cuDNN

https://github.com/tensorflow/tensorflow/issues/62490
https://github.com/tensorflow/tensorflow/issues/59924

Thank you!

github-actions[bot] on (2024-11-28 02:06:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-05 02:08:25 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-05 02:08:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80223"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80223"">No</a>

"
2668883772,issue,closed,completed,SavedModelBundle.load call results in Could not find variable sequential/conv2d_1/kernel,"I saved the sequential model in Python and I am using it in Java but keep getting this error,


2024-11-18 10:08:07.698839: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable sequential/conv2d_1/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/sequential/conv2d_1/kernel/class tensorflow::Var does not exist.
	 [[{{function_node __inference_serving_default_8088}}{{node sequential_1/conv2d_1_2/convolution/ReadVariableOp}}]]
Exception in thread ""main"" org.tensorflow.exceptions.TFFailedPreconditionException: Could not find variable sequential/conv2d_1/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/sequential/conv2d_1/kernel/class tensorflow::Var does not exist.
	 [[{{function_node __inference_serving_default_8088}}{{node sequential_1/conv2d_1_2/convolution/ReadVariableOp}}]]
	at org.tensorflow.internal.c_api.AbstractTF_Status.throwExceptionIfNotOK(AbstractTF_Status.java:84)
	at org.tensorflow.Session.run(Session.java:826)
	at org.tensorflow.Session$Runner.runHelper(Session.java:549)


[model.zip](https://github.com/user-attachments/files/17802418/model.zip)

I have tested the model in Python with below code and that works fine


 ```
   prediction_tensors = model.signatures[""serving_default""](tf.cast(new_image, tf.float32))
    for _, values in prediction_tensors.items():
        predictions = values.numpy()[0]
    #print('  Output with input', img_path, ': ', predictions)
    percent = round(np.max(predictions)*100,2)
    result  = CATEGORIES[np.argmax(predictions)]
```

Java code,
```

public static float[] predict3(String modelPath, float[][][][] imageTensorData) {

        System.out.println(TensorFlow.version());
        try(SavedModelBundle model = SavedModelBundle.load(modelPath, ""serve"")) {
        
        Signature sig = model.signatures().get(0);
        Session sess = model.session();
        TensorFunction func =  model.function(""serving_default"");

        TFloat32 rank3Tensor = TFloat32.tensorOf(StdArrays.ndCopyOf(imageTensorData));
        Tensor result = func.call(rank3Tensor);
        }
        return null;
    }

```",avinash10584,2024-11-18 15:10:03+00:00,['Venkat6871'],2024-12-05 02:08:31+00:00,2024-12-05 02:08:27+00:00,https://github.com/tensorflow/tensorflow/issues/80191,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2487583644, 'issue_id': 2668883772, 'author': 'Venkat6871', 'body': 'Hi **@avinash10584** ,\r\nPlease post this issue in the specific repository for faster resolution: https://github.com/tensorflow/java.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 20, 6, 11, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124023, 'issue_id': 2668883772, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930783, 'issue_id': 2668883772, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930870, 'issue_id': 2668883772, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80191"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80191"">No</a>', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 30, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-20 06:11:56 UTC): Hi **@avinash10584** ,
Please post this issue in the specific repository for faster resolution: https://github.com/tensorflow/java.
Thank you!

github-actions[bot] on (2024-11-28 02:06:33 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-05 02:08:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-05 02:08:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80191"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80191"">No</a>

"
2667337997,issue,closed,completed,Regular expression matches also directory in name,"https://github.com/tensorflow/tensorflow/blame/5d1bf95155485aa137a13b72fbf3bd3e83b2f544/tensorflow/lite/CMakeLists.txt#L602

I found a problem tensorflow lite library compilation where I put my project into directory named:

`~/workspace/final_test_6.6.36$`

I found that ""test_.*"" matches also directory name in my directory path
  FILTER ""(.*_test_util_internal|test_.*|.*_ops_wrapper)\\.(cc|h)""

So files in kernels directory are not compiled - and library have a problem with linking",kiszka,2024-11-18 06:43:47+00:00,['gaikwadrahul8'],2025-02-06 11:38:32+00:00,2025-02-06 11:38:29+00:00,https://github.com/tensorflow/tensorflow/issues/80182,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2488680419, 'issue_id': 2667337997, 'author': 'gaikwadrahul8', 'body': ""Hi, @kiszka \r\n\r\nThank you for bringing this issue to our attention, As far I know we can change the regular expression to avoid library compilation where you put your project into directory named `~/workspace/final_test_6.6.36$` to this regular expression `([^/]*_test_util_internal|test_[^/]*|[^/]*_ops_wrapper)\\\\.(cc|h)$` which will do strict filename matching and prevents matching across directories instead of below which allows matching across directories if I'm not wrong\r\nbut at the moment I'm not sure will this change impact in other places of source code so we need to check this and will update you https://github.com/tensorflow/tensorflow/blob/c76ae321772b71207a6f1f64bfb41d034c16ca32/tensorflow/lite/CMakeLists.txt#L602\r\n\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 20, 14, 7, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639583240, 'issue_id': 2667337997, 'author': 'gaikwadrahul8', 'body': ""Hi, @kiszka \nI see this PR has been merged https://github.com/tensorflow/tensorflow/pull/80498 so I'm closing this issue now. Thank you for your cooperation and patience."", 'created_at': datetime.datetime(2025, 2, 6, 11, 38, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639583446, 'issue_id': 2667337997, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80182"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80182"">No</a>', 'created_at': datetime.datetime(2025, 2, 6, 11, 38, 31, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-20 14:07:52 UTC): Hi, @kiszka 

Thank you for bringing this issue to our attention, As far I know we can change the regular expression to avoid library compilation where you put your project into directory named `~/workspace/final_test_6.6.36$` to this regular expression `([^/]*_test_util_internal|test_[^/]*|[^/]*_ops_wrapper)\\.(cc|h)$` which will do strict filename matching and prevents matching across directories instead of below which allows matching across directories if I'm not wrong
but at the moment I'm not sure will this change impact in other places of source code so we need to check this and will update you https://github.com/tensorflow/tensorflow/blob/c76ae321772b71207a6f1f64bfb41d034c16ca32/tensorflow/lite/CMakeLists.txt#L602


Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2025-02-06 11:38:24 UTC): Hi, @kiszka 
I see this PR has been merged https://github.com/tensorflow/tensorflow/pull/80498 so I'm closing this issue now. Thank you for your cooperation and patience.

google-ml-butler[bot] on (2025-02-06 11:38:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80182"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80182"">No</a>

"
2666857528,issue,closed,completed,Unexpected behavior,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Debian Bookworm 12

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

12.2.0

### CUDA/cuDNN version

11.8

### GPU model and memory

Nvidia L4 24Gb

### Current behavior?

I expected c1 to be defined in GPU:1 since a1 and b1 are defined there. I can also understand it being in CPU since the tf.matmul call was outside the GPU:1 context. But why is c1 defined in GPU:0?


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# create logical GPUs
gpus = tf.config.list_physical_devices('GPU')
tf.config.set_logical_device_configuration(
    gpus[0],
    [tf.config.LogicalDeviceConfiguration(memory_limit=5500),
     tf.config.LogicalDeviceConfiguration(memory_limit=5500)])

# create tensors in GPU:1
with tf.device(f'/device:GPU:1'):
  a1 = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b1 = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

# multiply tensors
c1 = tf.matmul(a1, b1)

# show tensor locations
a1.device
b1.device
c1.device
```


### Relevant log output

```shell
a1.device
/job:localhost/replica:0/task:0/device:GPU:1
b1.device
/job:localhost/replica:0/task:0/device:GPU:1
c1.device
/job:localhost/replica:0/task:0/device:GPU:0
```
",veseshan,2024-11-18 02:07:51+00:00,['Venkat6871'],2024-11-18 21:36:57+00:00,2024-11-18 21:36:52+00:00,https://github.com/tensorflow/tensorflow/issues/80180,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2484090982, 'issue_id': 2666857528, 'author': 'ricky-avaneesh', 'body': ""As stated in this section of the tutorial on tensorflow on GPU : https://www.tensorflow.org/guide/gpu#using_a_single_gpu_on_a_multi-gpu_system\r\n\r\nTensorflow automatically assigns the operation to the GPU with lowest ID. Even if you would have initiated it with CPU device, It would have placed the c1 variable in GPU:0. Provided the GPU is available.\r\n\r\nTo get c1 defined in GPU:1 you will have to explicitly define it.\r\n\r\n![TF_issue'](https://github.com/user-attachments/assets/7e6e1dd9-ec88-471d-a909-bd5d013b0eb2)"", 'created_at': datetime.datetime(2024, 11, 18, 20, 43, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484182359, 'issue_id': 2666857528, 'author': 'veseshan', 'body': 'Thanks', 'created_at': datetime.datetime(2024, 11, 18, 21, 36, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484182427, 'issue_id': 2666857528, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80180"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80180"">No</a>', 'created_at': datetime.datetime(2024, 11, 18, 21, 36, 55, tzinfo=datetime.timezone.utc)}]","ricky-avaneesh on (2024-11-18 20:43:30 UTC): As stated in this section of the tutorial on tensorflow on GPU : https://www.tensorflow.org/guide/gpu#using_a_single_gpu_on_a_multi-gpu_system

Tensorflow automatically assigns the operation to the GPU with lowest ID. Even if you would have initiated it with CPU device, It would have placed the c1 variable in GPU:0. Provided the GPU is available.

To get c1 defined in GPU:1 you will have to explicitly define it.

![TF_issue'](https://github.com/user-attachments/assets/7e6e1dd9-ec88-471d-a909-bd5d013b0eb2)

veseshan (Issue Creator) on (2024-11-18 21:36:52 UTC): Thanks

google-ml-butler[bot] on (2024-11-18 21:36:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80180"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80180"">No</a>

"
2666670815,issue,closed,completed,tflite-model-maker TFlite model accurracy ,"Hello,

I was trying to install tflite-model-maker, and it fails in the installation.
But I have converted my BertQa model to tflite using TFlite.Converter.
How do I calculate, the model accuracy (F1) score in this case.
In the examples gives, it uses evaluate from the tflite-model-maker, which I  cannot install.

Thank you",krishnarajk,2024-11-17 22:53:22+00:00,['gaikwadrahul8'],2024-12-05 02:08:34+00:00,2024-12-05 02:08:28+00:00,https://github.com/tensorflow/tensorflow/issues/80179,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TFLiteModelMaker', 'TFLite Model Maker related issues')]","[{'comment_id': 2488329309, 'issue_id': 2666670815, 'author': 'gaikwadrahul8', 'body': ""Hi, @krishnarajk \r\n\r\nThank you for bringing this issue to our attention, I was trying to run TensorFlow Lite Model Maker with a custom dataset and it got installed successfully please refer this [gist-file ](https://colab.research.google.com/gist/gaikwadrahul8/3c5f868e5f1b7e01e703093a715d45cc/gunshot_classification.ipynb) in that I used `Python version 3.9.x and numpy==1.23.4 version ` and it's known issue to us w.r.t `TensorFlow Lite Model Maker` and our relevant team is working on it meanwhile you can give it try with previously mentioned version by creating fresh virtual environment to just avoid package/libraries compatibilities issues\r\n\r\nIf issue still persists after trying above workaround with TensorFlow Lite Model Maker then I would suggest you to please use [MediaPipe Model Maker](https://ai.google.dev/edge/mediapipe/solutions/model_maker) and please refer this [tutorial ](https://ai.google.dev/edge/mediapipe/solutions/customization/image_classifier) \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 20, 11, 28, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124054, 'issue_id': 2666670815, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930811, 'issue_id': 2666670815, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930938, 'issue_id': 2666670815, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80179"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80179"">No</a>', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 33, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-20 11:28:08 UTC): Hi, @krishnarajk 

Thank you for bringing this issue to our attention, I was trying to run TensorFlow Lite Model Maker with a custom dataset and it got installed successfully please refer this [gist-file ](https://colab.research.google.com/gist/gaikwadrahul8/3c5f868e5f1b7e01e703093a715d45cc/gunshot_classification.ipynb) in that I used `Python version 3.9.x and numpy==1.23.4 version ` and it's known issue to us w.r.t `TensorFlow Lite Model Maker` and our relevant team is working on it meanwhile you can give it try with previously mentioned version by creating fresh virtual environment to just avoid package/libraries compatibilities issues

If issue still persists after trying above workaround with TensorFlow Lite Model Maker then I would suggest you to please use [MediaPipe Model Maker](https://ai.google.dev/edge/mediapipe/solutions/model_maker) and please refer this [tutorial ](https://ai.google.dev/edge/mediapipe/solutions/customization/image_classifier) 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-28 02:06:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-05 02:08:27 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-05 02:08:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80179"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80179"">No</a>

"
2665973288,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11,8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I cant run my streamlit file that contains pipline element.

### Standalone code to reproduce the issue

```shell
It is in visual studio code.
```


### Relevant log output

```shell
RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): Traceback (most recent call last): File ""C:\Users\Piroska\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module> from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Nem sikerlt egy dinamikus csatols fggvnytr (DLL) inicializl rutinjt vgrehajtani. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",noteandcode,2024-11-17 14:43:50+00:00,['Venkat6871'],2024-12-04 02:08:29+00:00,2024-12-04 02:08:26+00:00,https://github.com/tensorflow/tensorflow/issues/80177,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.18', '')]","[{'comment_id': 2484812447, 'issue_id': 2665973288, 'author': 'Venkat6871', 'body': 'Hi **@noteandcode** ,\r\nThe error you are facing, `DLL load failed`, is likely due to incompatible versions. Could you please check all the compatible versions and let us know which OS platform you are using? Additionally, please try to fill out all the required templates as it will help us troubleshoot more effectively. And here i am providing [documentation](https://www.tensorflow.org/install/source#linux) for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 19, 6, 32, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502502991, 'issue_id': 2665973288, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004935, 'issue_id': 2665973288, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516004969, 'issue_id': 2665973288, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80177"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80177"">No</a>', 'created_at': datetime.datetime(2024, 12, 4, 2, 8, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-19 06:32:53 UTC): Hi **@noteandcode** ,
The error you are facing, `DLL load failed`, is likely due to incompatible versions. Could you please check all the compatible versions and let us know which OS platform you are using? Additionally, please try to fill out all the required templates as it will help us troubleshoot more effectively. And here i am providing [documentation](https://www.tensorflow.org/install/source#linux) for your reference.
Thank you!

github-actions[bot] on (2024-11-27 02:06:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-04 02:08:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-04 02:08:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80177"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80177"">No</a>

"
2660367486,issue,closed,completed,Issue on inference of converted to tflight Super Resolution model,"### 1. System information

- OS Linux Ubuntu 22.04
- TensorFlow installation from sources
- TensorFlow library version 2.16

### 2. Code

I converted model from tensorflow to tflight. I should use Flex tf ops as not all layers were converted initially
but finally model was converted successfully without errors with **tf ops**.
On inference I have an issue 
**_RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare._**
 and can not use this model. Please, help with this issue!

Initial BasicVSR based model:
[ESWT-12-12_LSR_x4.pth.zip](https://github.com/user-attachments/files/17759174/ESWT-12-12_LSR_x4.pth.zip)

Tf model:
[sr.tf.zip](https://github.com/user-attachments/files/17758924/sr.tf.zip)
Tf light model:
[sr_12-12.tflight.zip](https://github.com/user-attachments/files/17759164/sr_12-12.tflight.zip)

This model initially appeared from Fried-Rice-Lab Super Resolution model based on BaseSVR
[Fried-Rice-Lab](https://github.com/Fried-Rice-Lab/FriedRiceLab?tab=readme-ov-file).
I downloded ESWT-12-12_LSR_x4.pth from their page [Google Drive](https://1drv.ms/u/s!AqKlMh-sml1mw362MfEjdr7orzds?e=budrUU)
This model was converted by scheme pth -> onnx -> tf -> tflight
Conversion script

    _import numpy as np
    import torch
    from basicsr.models import build_model
    from .utils import get_config
    import onnx
    import torchvision
    import onnx_tf
    import tensorflow as tf
    from onnx import helper
    def __init__(self, model_config_path, task_config_path, checkpoint_path):
        self.opt = get_config(model_config_path, task_config_path, checkpoint_path)
        self.device = torch.device('cpu')
        self.model = build_model(self.opt).net_g.to(self.device).to(torch.float32).eval()
        self.saveModel(self.model)_

    _def saveModel(self, model):
        modelName = ""sr""
        input_shape = (1, 3, 256, 256)
        torch.onnx.export(model, torch.randn(input_shape), modelName + '-new.onnx', opset_version=12, input_names=['input'], output_names=['output'])
        onnx_model = onnx.load(modelName + '-new.onnx')
        # Convert ONNX model to TensorFlow format
        tf_model = onnx_tf.backend.prepare(onnx_model)
        # Export  TensorFlow  model
        tf_model.export_graph(modelName + '.tf')
        converter = tf.lite.TFLiteConverter.from_saved_model(modelName + '.tf')
        converter.target_spec.supported_ops = [
          tf.lite.OpsSet.TFLITE_BUILTINS,
          tf.lite.OpsSet.SELECT_TF_OPS
        ]
        tflite_model = converter.convert()
        open(modelName + '.tflite', 'wb').write(tflite_model)_

Attached sr.tf model works fine and generate adequate super resolution result. Inference script


but sr_12-12.tflite inference has crash issue as above.


### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model inference is crashes unexpected.

      _packages/tensorflow/lite/python/interpreter.py"", line 941, in invoke
          self._interpreter.Invoke()
      RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 
      (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare._

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Error log

**2024-11-15 00:07:48.908511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:gpu:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
Traceback (most recent call last):
  File ""/home/me/infer/tflightRun.py"", line 40, in <module>
    res = model.predict(tensorflow_tensor)[0]
  File ""/home/me/infer/tflightRun.py"", line 23, in predict
    self.interpreter.invoke()
  File ""/home/me/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 941, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare.**


Script for tf inference running

        _import tensorflow as tf
        import numpy as np
        from PIL import Image
        import PIL
        
        import torch
        import torchvision
        import torchvision.transforms as T
        
        def swapChannelsInput(input_tensor):
            input_tensor = input_tensor[tf.newaxis, ...]
        
            out = input_tensor.numpy()
            torchTensor = torch.from_numpy(out)
            torchTensor = torchTensor.permute(0, 3, 1, 2)
            np_arr = torchTensor.detach().cpu().numpy()
            tensorflow_tensor = tf.constant(np_arr)
            return tensorflow_tensor
        
        def showOutput(res):
        
            res = tf.squeeze(res)
        
            res = res.numpy()
            torchTensorRes = torch.from_numpy(res)
            torchTensorRes = torchTensorRes.permute(1, 2, 0)
            resFinal = torchTensorRes.detach().cpu().numpy()
            return PIL.Image.fromarray(resFinal.astype(np.uint8))
        
        extraction_path = ""sr.tf/""
        test_image_path = ""frame0.jpg""
        model = tf.saved_model.load(extraction_path)
        
        infer = model.signatures[""serving_default""]
        
        image_np = np.array(Image.open(test_image_path))
        input_tensor = tf.convert_to_tensor(image_np, tf.float32)
        input_tensor = swapChannelsInput(input_tensor)
        
        res = infer(tf.constant(input_tensor))['output']
        showOutput(res).show()_

Script for tflight inference launching

        import tensorflow as tf
        import numpy as np
        import cv2
        
        from PIL import Image
        import PIL
        
        import torch
        import torchvision
        
        class TFLiteModel:
            def __init__(self, model_path: str):
                self.interpreter = tf.lite.Interpreter(model_path)
                self.interpreter.allocate_tensors()
        
                self.input_details = self.interpreter.get_input_details()
                self.output_details = self.interpreter.get_output_details()
        
            def predict(self, *data_args):
                assert len(data_args) == len(self.input_details)
                for data, details in zip(data_args, self.input_details):
                    self.interpreter.set_tensor(details[""index""], data)
                self.interpreter.invoke()
                return self.interpreter.get_tensor(self.output_details[0][""index""])
        
        
        model = TFLiteModel(""sr_12-12.tflite"")
        
        test_image_path = ""frame0.jpg""
        image_np = np.array(Image.open(test_image_path))
        input_tensor = tf.convert_to_tensor(image_np, tf.float32)
        
        input_tensor = input_tensor[tf.newaxis, ...]
        
        out = input_tensor.numpy()
        torchTensor = torch.from_numpy(out)
        torchTensor = torchTensor.permute(0, 3, 1, 2)
        np_arr = torchTensor.detach().cpu().numpy()
        tensorflow_tensor = tf.constant(np_arr)
        res = model.predict(tensorflow_tensor)[0]",koranten2,2024-11-14 23:40:23+00:00,['gaikwadrahul8'],2025-01-31 02:00:03+00:00,2025-01-31 02:00:00+00:00,https://github.com/tensorflow/tensorflow/issues/80069,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.16', '')]","[{'comment_id': 2482983489, 'issue_id': 2660367486, 'author': 'gaikwadrahul8', 'body': 'Hi, @koranten2 \r\n\r\nI apologize for the delayed response, I am able to replicate similar behavior from my end with your provided code snippet and converted TFLite model for reference I\'ve added [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/82aa74c02219252de99e5b90839370cb/tflite-issue-80069.ipynb) file where I\'m getting `"" RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare ""` so we\'ll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 18, 13, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490844554, 'issue_id': 2660367486, 'author': 'koranten2', 'body': 'We resolved ""num_input_elements != num_output_elements (0 != 8)"" issue by converting tf->tflight slightly another way:\r\nconverter.target_spec.supported_ops = [\r\n    #      tf.lite.OpsSet.TFLITE_BUILTINS, // it is commented now\r\n          tf.lite.OpsSet.SELECT_TF_OPS\r\n        ]\r\nAlso, please, answer one additional question to avoid creating new issue.\r\nIf we have Android Studio project that uses flex and tflite in native via shared libraries (not on java level) then is it possible to use GPU + flex? From internet searching I suspect that flex + GPU is not supported but not sure in it. Is flex support only CPU on android now?', 'created_at': datetime.datetime(2024, 11, 21, 11, 19, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595003000, 'issue_id': 2660367486, 'author': 'gaikwadrahul8', 'body': 'Hi, @koranten2\nI apologize for the delayed response and Good to hear that original issue got resolved by including `tf.lite.OpsSet.SELECT_TF_OPS` which convert models that utilize operations not covered by the standard TFLite built-in operations during conversion of TensorFlow model to TFLite format.\n\nAs far I know TensorFlow Lite supports both Flex and GPU acceleration individually, there is currently no support for using them together. The Flex delegate allows the use of TensorFlow operations that are not natively supported by TFLite. However, its implementation does not support GPU acceleration on Android devices. \n\nTFLite provides a separate GPU delegate that accelerates inference for models optimized for GPU execution. However, this delegate does not work with Flex operations. The GPU delegate is designed to work with a specific set of operations that are optimized for performance on the GPU. Please refer this official documentation of [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu) for GPU ML operations support\n\nThank you for your understanding and patience.', 'created_at': datetime.datetime(2025, 1, 16, 9, 30, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611383873, 'issue_id': 2660367486, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 24, 1, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626127874, 'issue_id': 2660367486, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 31, 2, 0, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626128223, 'issue_id': 2660367486, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80069"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80069"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 2, 0, 2, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-18 13:00:46 UTC): Hi, @koranten2 

I apologize for the delayed response, I am able to replicate similar behavior from my end with your provided code snippet and converted TFLite model for reference I've added [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/82aa74c02219252de99e5b90839370cb/tflite-issue-80069.ipynb) file where I'm getting `"" RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare ""` so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.

Thank you for your cooperation and patience.

koranten2 (Issue Creator) on (2024-11-21 11:19:03 UTC): We resolved ""num_input_elements != num_output_elements (0 != 8)"" issue by converting tf->tflight slightly another way:
converter.target_spec.supported_ops = [
    #      tf.lite.OpsSet.TFLITE_BUILTINS, // it is commented now
          tf.lite.OpsSet.SELECT_TF_OPS
        ]
Also, please, answer one additional question to avoid creating new issue.
If we have Android Studio project that uses flex and tflite in native via shared libraries (not on java level) then is it possible to use GPU + flex? From internet searching I suspect that flex + GPU is not supported but not sure in it. Is flex support only CPU on android now?

gaikwadrahul8 (Assginee) on (2025-01-16 09:30:16 UTC): Hi, @koranten2
I apologize for the delayed response and Good to hear that original issue got resolved by including `tf.lite.OpsSet.SELECT_TF_OPS` which convert models that utilize operations not covered by the standard TFLite built-in operations during conversion of TensorFlow model to TFLite format.

As far I know TensorFlow Lite supports both Flex and GPU acceleration individually, there is currently no support for using them together. The Flex delegate allows the use of TensorFlow operations that are not natively supported by TFLite. However, its implementation does not support GPU acceleration on Android devices. 

TFLite provides a separate GPU delegate that accelerates inference for models optimized for GPU execution. However, this delegate does not work with Flex operations. The GPU delegate is designed to work with a specific set of operations that are optimized for performance on the GPU. Please refer this official documentation of [GPU delegates for LiteRT](https://ai.google.dev/edge/litert/performance/gpu) for GPU ML operations support

Thank you for your understanding and patience.

github-actions[bot] on (2025-01-24 01:59:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-31 02:00:00 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-31 02:00:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80069"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80069"">No</a>

"
2659584170,issue,open,,tf.range still miss some dtypes support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

No

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Same issue as in https://github.com/tensorflow/tensorflow/issues/72365 but now with unsigned dtypes

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.range(10, delta=1, dtype='uint8')
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()
      1 import tensorflow as tf
      2 
----> 3 tf.range(10, delta=1, dtype='uint8')

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6000 def raise_from_not_ok_status(e, name) -> NoReturn:
   6001   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6003 
   6004 

InvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32
	; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:
```
",shkarupa-alex,2024-11-14 17:40:39+00:00,['tilakrayal'],2024-11-19 14:45:07+00:00,,https://github.com/tensorflow/tensorflow/issues/80039,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2485915469, 'issue_id': 2659584170, 'author': 'tilakrayal', 'body': '@shkarupa-alex,\r\nI was able to reproduce the issue on tensorflow v2.17, v2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ea738e3131431713fe19ff42a1e70a1c/untitled2235.ipynb). I will try to contribute with the PR for the fix. Thank you!', 'created_at': datetime.datetime(2024, 11, 19, 14, 45, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-19 14:45:00 UTC): @shkarupa-alex,
I was able to reproduce the issue on tensorflow v2.17, v2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ea738e3131431713fe19ff42a1e70a1c/untitled2235.ipynb). I will try to contribute with the PR for the fix. Thank you!

"
2658206097,issue,closed,not_planned,"App Crash with YOLO11n TFLite Model on Android.  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR) on GPU mode","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1 

### Custom code

Yes

### OS platform and distribution

Android

### Mobile device

Some Android Devices

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I converted the YOLO11n model to TensorFlow Lite (TFLite) and used it in my Android app. However, my app crashes on some Android devices, and the following error appears in Logcat:
`Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR)`

To create the TFLite model, I tried various conversion options in YOLO, as shown below:
```
# Load a model
model = YOLO(""yolo11n.pt"")  # load an official model
# model = YOLO(""path/to/best.pt"")  # load a custom trained model

# Export the model
# model.export(format=""tflite"", half = True, batch = 4)

model.export(format=""tflite"", half = True, int8 = True)
```
Despite testing different configurations, the app consistently crashes when using the model in GPU mode.

Interestingly, this issue only occurs with the YOLO11n TFLite model. When I use other TFLite models, such as YOLOv8n or YOLOv9t, everything works fine. This suggests there may be a compatibility issue or a bug in the TFLite Android library specific to the YOLO11n model.




### Standalone code to reproduce the issue

```shell
I'm testing my YOLO TFLite model with the code in this repository:
https://github.com/surendramaran/YOLO/tree/main/YOLOv9-Object-Detector-Android-Tflite
```


### Relevant log output

_No response_",emoo44566,2024-11-14 09:35:00+00:00,"['gaikwadrahul8', 'pkgoogle']",2025-01-22 21:07:38+00:00,2025-01-22 21:07:35+00:00,https://github.com/tensorflow/tensorflow/issues/80019,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2482927436, 'issue_id': 2658206097, 'author': 'gaikwadrahul8', 'body': ""Hi, @emoo44566 \r\n\r\nI apologize for the delayed response, I am able to replicate the same behavior from my end with `YOLO11n` TFLite model on Android and app is getting crashed for reference I've recorded the screen here is [link ](https://drive.google.com/file/d/1odXw7cFyeWvMI1YKwGKphUrA3MPj4gwI/view?usp=sharing), Thank you for bringing this issue to our attention and will have to dig more into this issue. I see one of the user also reported the similar issue https://github.com/tensorflow/tensorflow/issues/78396\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 18, 12, 38, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489064981, 'issue_id': 2658206097, 'author': 'hcdino', 'body': 'Is there any fix yet? I am seeing the same issue', 'created_at': datetime.datetime(2024, 11, 20, 16, 36, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558290907, 'issue_id': 2658206097, 'author': 'hcdino', 'body': 'seems still an issue?', 'created_at': datetime.datetime(2024, 12, 22, 1, 22, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595007634, 'issue_id': 2658206097, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2025, 1, 16, 9, 32, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608271758, 'issue_id': 2658206097, 'author': 'pkgoogle', 'body': 'This does seem similar to that other issue... I will make a new issue for this in LiteRT for now. @hcdino Please open future issues in the LiteRT repo from now on. We will continue investigation to see if it is a duplicate or not.', 'created_at': datetime.datetime(2025, 1, 22, 21, 7, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608271815, 'issue_id': 2658206097, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80019"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80019"">No</a>', 'created_at': datetime.datetime(2025, 1, 22, 21, 7, 37, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-18 12:38:32 UTC): Hi, @emoo44566 

I apologize for the delayed response, I am able to replicate the same behavior from my end with `YOLO11n` TFLite model on Android and app is getting crashed for reference I've recorded the screen here is [link ](https://drive.google.com/file/d/1odXw7cFyeWvMI1YKwGKphUrA3MPj4gwI/view?usp=sharing), Thank you for bringing this issue to our attention and will have to dig more into this issue. I see one of the user also reported the similar issue https://github.com/tensorflow/tensorflow/issues/78396

Thank you for your cooperation and patience.

hcdino on (2024-11-20 16:36:01 UTC): Is there any fix yet? I am seeing the same issue

hcdino on (2024-12-22 01:22:23 UTC): seems still an issue?

gaikwadrahul8 (Assginee) on (2025-01-16 09:32:22 UTC): Hi, @pkgoogle 
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2025-01-22 21:07:35 UTC): This does seem similar to that other issue... I will make a new issue for this in LiteRT for now. @hcdino Please open future issues in the LiteRT repo from now on. We will continue investigation to see if it is a duplicate or not.

google-ml-butler[bot] on (2025-01-22 21:07:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80019"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/80019"">No</a>

"
2656960354,issue,closed,completed,Flex lib not linked on Android for C code on usage Select TensorFlow op(s),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tflight 2.16

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04, Android Studio

### Mobile device

Samsung S21, S10

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I tried to launch my custom tflite model (with select_op) on Android Studio using C API. I tried both libtensorflowlite.so and libtensorflowlite_c.so built from source code using Bazel but got the same error.  I have linked the libtensorflowlite_flex.so in Cmake as below (build follow the https://www.tensorflow.org/lite/guide/ops_select).
But occurred this error:
_E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select_

It looks that flex library was not applied therefore I provided some dubugging inside tflight library. As I could see the main function for Flex delegate initialization is  **AcquireFlexDelegate()**. It can be found both 
\tensorflow\tensorflow\lite\core\interpreter_builder.cc (Weak function version in tflight library) and 
\tensorflow\tensorflow\lite\delegates\flex\delegate_symbol.cc (Strong function version in flex library).

I also found correspondent symbols on **nm** command output
00000000050b53b8 T _ZN6tflite19AcquireFlexDelegateEv@@VERS_1.0  (libtensorflowlite_flex.so)
00000000003b8a4c W _ZN6tflite19AcquireFlexDelegateEv@@VERS_1.0  (libtensorflowlite.so)

As I understand from code for Android case weak linking mechanism should be used and  AcquireFlexDelegate() from libtensorflowlite_flex.so should be called instead of the same function from  libtensorflowlite.so. Correspondent code in \tensorflow\tensorflow\lite\core\interpreter_builder.cc:

___TfLiteStatus InterpreterBuilder::ApplyDelegates(Interpreter* interpreter) {_
  // Apply Flex delegate if applicable.
  if (has_flex_op_) {
    if (Interpreter::TfLiteDelegatePtr flex_delegate = **AcquireFlexDelegate())** {
...._

But on debugging I could see that  **ApplyDelegates()** from libtensorflowlite.so (in my case has_flex_op_ == true) is called and return trivial delegate. This is the reason of further error messages and crash as the function from flex library is not called.

On Ubuntu the same code works well but in this case ApplyDelegates() from libtensorflowlite.so is called and then flex lib is called clear by
_auto acquire_flex_delegate_func = reinterpret_cast<Interpreter::TfLiteDelegatePtr (*)()>(SharedLibrary::GetSymbol(""TF_AcquireFlexDelegate""));_
Unfortunatelly, from code comments I see that this approach is not appropriate for Android as ""TF_AcquireFlexDelegate"" is not defined for this os.

Please, clarify me the write way of using flex lib in native C/C++ on Android using Cmake in Android Studio. Possibly my linking procedure is not fully correct in Cmake, or I wrongly understand the above workflow or some other reason.
I will be very grateful for answer!

### Standalone code to reproduce the issue

```shell
CMakeLists.txt:
...

include_directories(myapp INTERFACE
   ...
   ""${TFLITE_INCLUDE_PATH}""
   ""${TFLITE_THIRDPARTY_DIR}/flatbuffers/include""
   ""${TFLITE_THIRDPARTY_DIR}/absl""
   ...
)

target_compile_options(myapp PRIVATE -Wno-deprecated-declarations)

target_link_libraries(myapp
        ...
        ${TFLITE_LIBRARY_DIR}/libtensorflowlite.so
        ${TFLITE_LIBRARY_DIR}/libtensorflowlite_gpu_delegate.so
        ...                        
)

add_library(TFLITE_FLEX SHARED IMPORTED)
set_target_properties(TFLITE_FLEX PROPERTIES IMPORTED_LOCATION ${TFLITE_LIBRARY_DIR2}/libtensorflowlite_flex.so)
target_link_libraries(myapp TFLITE_FLEX)
```


### Relevant log output

```shell
I/tflite: Created TensorFlow Lite XNNPACK delegate for CPU.
E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite: Node number 12 (FlexTranspose) failed to prepare.
E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite: Node number 12 (FlexTranspose) failed to prepare.
E/: Error at .../cpp/detector.cc, bool seeso::Detector::buildInterpreter(), line 98
```
",koranten2,2024-11-13 22:10:38+00:00,['gaikwadrahul8'],2024-11-20 23:21:01+00:00,2024-11-20 23:20:58+00:00,https://github.com/tensorflow/tensorflow/issues/79984,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2482792858, 'issue_id': 2656960354, 'author': 'gaikwadrahul8', 'body': 'Hi, @koranten2\r\n\r\nI apologize for the delayed response, You just need to make sure that `libtensorflow_flex.so` is linked in. It can be used in the same way as with C++ API. Just build https://www.tensorflow.org/lite/guide/ops_select#c and load it please refer this [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) which may help you to solve your issue and If you\'re doing this in Bazel, adding `""//tensorflow/lite/delegates/flex:delegate""` to `""deps""` of the ""minimal"" cc_binary rule is the right approach that will instruct Bazel to build the flex delegate and then link it into the binary.\r\n\r\nit\'s recommended to use `find_library()` to locate `libtensorflowlite_flex.so` similar to the example in the minimal `CMakeLists.txt` file\r\n\r\nIf I have missed something please let me know. \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 18, 11, 38, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489735798, 'issue_id': 2656960354, 'author': 'koranten2', 'body': 'Thank you alot for advice! Clearing my current cmake to make it minimal like  [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) resolved the issue.', 'created_at': datetime.datetime(2024, 11, 20, 23, 20, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489735965, 'issue_id': 2656960354, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79984"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79984"">No</a>', 'created_at': datetime.datetime(2024, 11, 20, 23, 21, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-18 11:38:14 UTC): Hi, @koranten2

I apologize for the delayed response, You just need to make sure that `libtensorflow_flex.so` is linked in. It can be used in the same way as with C++ API. Just build https://www.tensorflow.org/lite/guide/ops_select#c and load it please refer this [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) which may help you to solve your issue and If you're doing this in Bazel, adding `""//tensorflow/lite/delegates/flex:delegate""` to `""deps""` of the ""minimal"" cc_binary rule is the right approach that will instruct Bazel to build the flex delegate and then link it into the binary.

it's recommended to use `find_library()` to locate `libtensorflowlite_flex.so` similar to the example in the minimal `CMakeLists.txt` file

If I have missed something please let me know. 

Thank you for your cooperation and patience.

koranten2 (Issue Creator) on (2024-11-20 23:20:58 UTC): Thank you alot for advice! Clearing my current cmake to make it minimal like  [TensorFlow Lite C++ minimal example](https://github.com/tensorflow/tensorflow/tree/v2.18.0/tensorflow/lite/examples/minimal) resolved the issue.

google-ml-butler[bot] on (2024-11-20 23:21:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79984"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79984"">No</a>

"
2656913181,issue,closed,completed,Issue created for Rollback of PR #19160: Add tf.regex_match for regex match support,"Merged PR #19160 is rolled back in 285f351d6ced24663332d7aedbfe92c3f63acea8.
    Please follow up with the reviewer and close this issue once its resolved.",github-actions[bot],2024-11-13 21:47:03+00:00,"['yongtang', 'rmlarsen', 'tilakrayal']",2024-12-06 13:39:55+00:00,2024-12-06 13:39:55+00:00,https://github.com/tensorflow/tensorflow/issues/79982,[],[],
2655234754,issue,closed,completed,The post-training quantization results in significant precision loss for internal state model,"The internal state model has assignVariable and varHandle and so on. So I use the following code to convert the internal state model into TFLite. 
```  
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.experimental_new_quantizer = True
converter.experimental_enable_resource_variables = True
converter._experimental_variable_quantization = True#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
```
However, I noticed an accuracy loss of about 10% compare to not converted model. Then, I used a non-stream model which has no assignVariable for Int8 quantization, and there was no significant accuracy loss. How can  I reduce the  accuracy loss for  internal state model ?",ctwillson,2024-11-13 11:50:42+00:00,['gaikwadrahul8'],2024-11-23 06:46:06+00:00,2024-11-23 06:46:06+00:00,https://github.com/tensorflow/tensorflow/issues/79949,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2482307178, 'issue_id': 2655234754, 'author': 'gaikwadrahul8', 'body': 'Hi, @ctwillson \r\n\r\nI apologize for the delayed response, if possible could you please help us with your Google colab notebook along with model to replicate the similar behavior from our end ? Meanwhile please give it try with `tf.float16` instead of `tf.int8` or different quantization technique please refer this [official documentation ](https://ai.google.dev/edge/litert/models/model_optimization)\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 18, 8, 50, 11, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-18 08:50:11 UTC): Hi, @ctwillson 

I apologize for the delayed response, if possible could you please help us with your Google colab notebook along with model to replicate the similar behavior from our end ? Meanwhile please give it try with `tf.float16` instead of `tf.int8` or different quantization technique please refer this [official documentation ](https://ai.google.dev/edge/litert/models/model_optimization)

Thank you for your cooperation and patience.

"
2652398697,issue,closed,completed,Could not create task ':app:processDebugResources'. Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.,"Build/Install

Have you reproduced the bug with TensorFlow Nightly?
No

Source
source

TensorFlow version
donot know

Custom code
No

OS platform and distribution
windows 11

Mobile device
No response

Python version
No response

Bazel version
No response

GCC/compiler version
No response

CUDA/cuDNN version
No response

GPU model and memory
No response


Standalone code to reproduce the issue
Could not create task ':app:processDebugResources'.
Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.

* Try:
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
> Get more help at https://help.gradle.org.

* Exception is:
com.intellij.openapi.externalSystem.model.ExternalSystemException: Could not create task ':app:processDebugResources'.
Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.executeAction(GradleModelFetchAction.java:254)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.doExecute(GradleModelFetchAction.java:143)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$1(GradleModelFetchAction.java:103)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:61)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$2(GradleModelFetchAction.java:102)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.withOpenTelemetry(GradleModelFetchAction.java:113)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$3(GradleModelFetchAction.java:101)
	at com.intellij.gradle.toolingExtension.impl.util.GradleExecutorServiceUtil.withSingleThreadExecutor(GradleExecutorServiceUtil.java:18)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.execute(GradleModelFetchAction.java:100)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.execute(GradleModelFetchAction.java:36)
	at org.gradle.tooling.internal.consumer.connection.InternalBuildActionAdapter.execute(InternalBuildActionAdapter.java:65)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.runAction(AbstractClientProvidedBuildActionRunner.java:131)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.fromBuildModel(AbstractClientProvidedBuildActionRunner.java:104)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.fromBuildModel(AbstractClientProvidedBuildActionRunner.java:84)
	at org.gradle.internal.buildtree.DefaultBuildTreeModelCreator.fromBuildModel(DefaultBuildTreeModelCreator.java:57)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.lambda$fromBuildModel$2(DefaultBuildTreeLifecycleController.java:89)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.lambda$runBuild$4(DefaultBuildTreeLifecycleController.java:119)
	at org.gradle.internal.model.StateTransitionController.lambda$transition$6(StateTransitionController.java:169)
	at org.gradle.internal.model.StateTransitionController.doTransition(StateTransitionController.java:266)
	at org.gradle.internal.model.StateTransitionController.lambda$transition$7(StateTransitionController.java:169)
	at org.gradle.internal.work.DefaultSynchronizer.withLock(DefaultSynchronizer.java:44)
	at org.gradle.internal.model.StateTransitionController.transition(StateTransitionController.java:169)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.runBuild(DefaultBuildTreeLifecycleController.java:116)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.fromBuildModel(DefaultBuildTreeLifecycleController.java:81)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner.runClientAction(AbstractClientProvidedBuildActionRunner.java:43)
	at org.gradle.tooling.internal.provider.runner.ClientProvidedPhasedActionRunner.run(ClientProvidedPhasedActionRunner.java:53)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.internal.buildtree.ProblemReportingBuildActionRunner.run(ProblemReportingBuildActionRunner.java:49)
	at org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:65)
	at org.gradle.tooling.internal.provider.FileSystemWatchingBuildActionRunner.run(FileSystemWatchingBuildActionRunner.java:140)
	at org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:41)
	at org.gradle.launcher.exec.RootBuildLifecycleBuildActionExecutor.lambda$execute$0(RootBuildLifecycleBuildActionExecutor.java:40)
	at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:123)
	at org.gradle.launcher.exec.RootBuildLifecycleBuildActionExecutor.execute(RootBuildLifecycleBuildActionExecutor.java:40)
	at org.gradle.internal.buildtree.InitDeprecationLoggingActionExecutor.execute(InitDeprecationLoggingActionExecutor.java:62)
	at org.gradle.internal.buildtree.InitProblems.execute(InitProblems.java:38)
	at org.gradle.internal.buildtree.DefaultBuildTreeContext.execute(DefaultBuildTreeContext.java:40)
	at org.gradle.launcher.exec.BuildTreeLifecycleBuildActionExecutor.lambda$execute$0(BuildTreeLifecycleBuildActionExecutor.java:58)
	at org.gradle.internal.buildtree.BuildTreeState.run(BuildTreeState.java:58)
	at org.gradle.launcher.exec.BuildTreeLifecycleBuildActionExecutor.execute(BuildTreeLifecycleBuildActionExecutor.java:58)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor$3.call(RunAsBuildOperationBuildActionExecutor.java:61)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor$3.call(RunAsBuildOperationBuildActionExecutor.java:57)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor.execute(RunAsBuildOperationBuildActionExecutor.java:57)
	at org.gradle.launcher.exec.RunAsWorkerThreadBuildActionExecutor.lambda$execute$0(RunAsWorkerThreadBuildActionExecutor.java:36)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:264)
	at org.gradle.internal.work.DefaultWorkerLeaseService.runAsWorkerThread(DefaultWorkerLeaseService.java:128)
	at org.gradle.launcher.exec.RunAsWorkerThreadBuildActionExecutor.execute(RunAsWorkerThreadBuildActionExecutor.java:36)
	at org.gradle.tooling.internal.provider.continuous.ContinuousBuildActionExecutor.execute(ContinuousBuildActionExecutor.java:110)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecutor.execute(SubscribableBuildActionExecutor.java:64)
	at org.gradle.internal.session.DefaultBuildSessionContext.execute(DefaultBuildSessionContext.java:46)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter$ActionImpl.apply(BuildSessionLifecycleBuildActionExecuter.java:92)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter$ActionImpl.apply(BuildSessionLifecycleBuildActionExecuter.java:80)
	at org.gradle.internal.session.BuildSessionState.run(BuildSessionState.java:69)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter.execute(BuildSessionLifecycleBuildActionExecuter.java:62)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter.execute(BuildSessionLifecycleBuildActionExecuter.java:41)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:64)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:51)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:39)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:47)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:31)
	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:65)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:39)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:29)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:35)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:78)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:75)
	at org.gradle.util.internal.Swapper.swap(Swapper.java:38)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:75)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:64)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:63)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:84)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:52)
	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.AbstractManagedExecutor$1.run(AbstractManagedExecutor.java:47)
org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreationException: Could not create task ':app:processDebugResources'.
	at org.gradle.api.internal.tasks.DefaultTaskContainer.taskCreationException(DefaultTaskContainer.java:717)
	at org.gradle.api.internal.tasks.DefaultTaskContainer.access$600(DefaultTaskContainer.java:78)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider.domainObjectCreationException(DefaultTaskContainer.java:709)
	at org.gradle.api.internal.DefaultNamedDomainObjectCollection$AbstractDomainObjectCreatingProvider.tryCreate(DefaultNamedDomainObjectCollection.java:954)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider.access$1401(DefaultTaskContainer.java:656)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider$1.run(DefaultTaskContainer.java:682)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:29)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:26)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.run(DefaultBuildOperationRunner.java:47)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:68)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider.tryCreate(DefaultTaskContainer.java:678)
	at org.gradle.api.internal.DefaultNamedDomainObjectCollection$AbstractDomainObjectCreatingProvider.calculateOwnValue(DefaultNamedDomainObjectCollection.java:935)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateValue(AbstractMinimalProvider.java:115)
	at org.gradle.api.internal.provider.FlatMapProvider.calculateOwnValue(FlatMapProvider.java:46)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateValue(AbstractMinimalProvider.java:115)
	at org.gradle.api.internal.provider.TransformBackedProvider.calculateOwnValue(TransformBackedProvider.java:82)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateValue(AbstractMinimalProvider.java:115)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:128)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:26)
	at org.gradle.api.internal.provider.AbstractProperty.doCalculateValue(AbstractProperty.java:142)
	at org.gradle.api.internal.provider.AbstractProperty.calculateOwnValue(AbstractProperty.java:136)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateValue(AbstractMinimalProvider.java:115)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:128)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:26)
	at org.gradle.api.internal.provider.AbstractProperty.doCalculateValue(AbstractProperty.java:142)
	at org.gradle.api.internal.provider.AbstractProperty.calculateOwnValue(AbstractProperty.java:136)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateValue(AbstractMinimalProvider.java:115)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:128)
	at org.gradle.api.internal.provider.DefaultProperty.calculateValueFrom(DefaultProperty.java:26)
	at org.gradle.api.internal.provider.AbstractProperty.doCalculateValue(AbstractProperty.java:142)
	at org.gradle.api.internal.provider.AbstractProperty.calculateOwnValue(AbstractProperty.java:136)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.calculateOwnPresentValue(AbstractMinimalProvider.java:80)
	at org.gradle.api.internal.provider.AbstractMinimalProvider.get(AbstractMinimalProvider.java:100)
	at org.gradle.api.internal.provider.ProviderResolutionStrategy$2.resolve(ProviderResolutionStrategy.java:33)
	at org.gradle.api.internal.file.collections.ProviderBackedFileCollection.visitChildren(ProviderBackedFileCollection.java:64)
	at org.gradle.api.internal.file.CompositeFileCollection.visitContents(CompositeFileCollection.java:113)
	at org.gradle.api.internal.file.AbstractFileCollection.visitStructure(AbstractFileCollection.java:359)
	at org.gradle.api.internal.file.CompositeFileCollection.lambda$visitContents$0(CompositeFileCollection.java:113)
	at org.gradle.api.internal.file.collections.UnpackingVisitor.add(UnpackingVisitor.java:77)
	at org.gradle.api.internal.file.collections.DefaultConfigurableFileCollection$UnresolvedItemsCollector.visitContents(DefaultConfigurableFileCollection.java:419)
	at org.gradle.api.internal.file.collections.DefaultConfigurableFileCollection.visitChildren(DefaultConfigurableFileCollection.java:328)
	at org.gradle.api.internal.file.CompositeFileCollection.visitContents(CompositeFileCollection.java:113)
	at org.gradle.api.internal.file.AbstractFileCollection.getFiles(AbstractFileCollection.java:123)
	at com.android.build.gradle.internal.ide.ModelBuilder.createAndroidArtifact(ModelBuilder.java:945)
	at com.android.build.gradle.internal.ide.ModelBuilder.createVariant(ModelBuilder.java:651)
	at com.android.build.gradle.internal.ide.ModelBuilder.buildVariant(ModelBuilder.java:606)
	at com.android.build.gradle.internal.ide.ModelBuilder.buildAll(ModelBuilder.java:239)
	at com.android.build.gradle.internal.ide.ModelBuilder.buildAll(ModelBuilder.java:156)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$BuilderWithParameter.build(DefaultToolingModelBuilderRegistry.java:289)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$UserCodeAssigningBuilder.lambda$build$0(DefaultToolingModelBuilderRegistry.java:374)
	at org.gradle.internal.code.DefaultUserCodeApplicationContext$CurrentApplication.reapply(DefaultUserCodeApplicationContext.java:108)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$UserCodeAssigningBuilder.build(DefaultToolingModelBuilderRegistry.java:374)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$LockSingleProjectBuilder.lambda$build$0(DefaultToolingModelBuilderRegistry.java:304)
	at org.gradle.api.internal.project.DefaultProjectStateRegistry$ProjectStateImpl.lambda$fromMutableState$2(DefaultProjectStateRegistry.java:429)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withReplacedLocks(DefaultWorkerLeaseService.java:360)
	at org.gradle.api.internal.project.DefaultProjectStateRegistry$ProjectStateImpl.fromMutableState(DefaultProjectStateRegistry.java:429)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$LockSingleProjectBuilder.build(DefaultToolingModelBuilderRegistry.java:304)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$BuildOperationWrappingBuilder$1.call(DefaultToolingModelBuilderRegistry.java:337)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
	at org.gradle.tooling.provider.model.internal.DefaultToolingModelBuilderRegistry$BuildOperationWrappingBuilder.build(DefaultToolingModelBuilderRegistry.java:334)
	at org.gradle.internal.build.DefaultBuildToolingModelController$AbstractToolingScope.getModel(DefaultBuildToolingModelController.java:85)
	at org.gradle.tooling.internal.provider.runner.DefaultBuildController.getModel(DefaultBuildController.java:108)
	at org.gradle.tooling.internal.consumer.connection.ParameterAwareBuildControllerAdapter.getModel(ParameterAwareBuildControllerAdapter.java:40)
	at org.gradle.tooling.internal.consumer.connection.UnparameterizedBuildController.getModel(UnparameterizedBuildController.java:116)
	at org.gradle.tooling.internal.consumer.connection.NestedActionAwareBuildControllerAdapter.getModel(NestedActionAwareBuildControllerAdapter.java:32)
	at org.gradle.tooling.internal.consumer.connection.UnparameterizedBuildController.findModel(UnparameterizedBuildController.java:100)
	at org.gradle.tooling.internal.consumer.connection.NestedActionAwareBuildControllerAdapter.findModel(NestedActionAwareBuildControllerAdapter.java:32)
	at org.jetbrains.plugins.gradle.model.DefaultBuildController.findModel(DefaultBuildController.java:118)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunnerKt$toMeasuringController$1$findModel$4.invoke(SyncActionRunner.kt:295)
	at com.android.tools.idea.projectsystem.gradle.sync.Counter.invoke(PerformanceMeasurementUtil.kt:108)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunnerKt.measure(SyncActionRunner.kt:340)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunnerKt.access$measure(SyncActionRunner.kt:1)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunnerKt$toMeasuringController$1.findModel(SyncActionRunner.kt:295)
	at com.android.tools.idea.gradle.project.sync.ActionToRun$toSafeController$1.findModel(SyncActionRunner.kt:164)
	at com.android.tools.idea.gradle.project.sync.ModelFetchersKt.findVariantModel(ModelFetchers.kt:83)
	at com.android.tools.idea.gradle.project.sync.AndroidProjectResultKt$v1VariantFetcher$1.invoke(AndroidProjectResult.kt:208)
	at com.android.tools.idea.gradle.project.sync.AndroidProjectResultKt$v1VariantFetcher$1.invoke(AndroidProjectResult.kt:200)
	at com.android.tools.idea.gradle.project.sync.VariantDiscovery$toFetchVariantDependenciesAction$1$1.invoke(VariantDiscovery.kt:213)
	at com.android.tools.idea.gradle.project.sync.VariantDiscovery$toFetchVariantDependenciesAction$1$1.invoke(VariantDiscovery.kt:210)
	at com.android.tools.idea.gradle.project.sync.ModelResult$Companion.create(ModelResult.kt:32)
	at com.android.tools.idea.gradle.project.sync.VariantDiscovery$toFetchVariantDependenciesAction$1.invoke(VariantDiscovery.kt:210)
	at com.android.tools.idea.gradle.project.sync.VariantDiscovery$toFetchVariantDependenciesAction$1.invoke(VariantDiscovery.kt:205)
	at com.android.tools.idea.gradle.project.sync.ActionToRun$map$1.invoke(SyncActionRunner.kt:68)
	at com.android.tools.idea.gradle.project.sync.ActionToRun$map$1.invoke(SyncActionRunner.kt:68)
	at com.android.tools.idea.gradle.project.sync.ActionToRun.run$intellij_android_projectSystem_gradle_sync(SyncActionRunner.kt:79)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunner$runActions$1.invoke(SyncActionRunner.kt:229)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunner$runActions$1.invoke(SyncActionRunner.kt:229)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunner.runAction(SyncActionRunner.kt:256)
	at com.android.tools.idea.gradle.project.sync.SyncActionRunner.runActions(SyncActionRunner.kt:229)
	at com.android.tools.idea.gradle.project.sync.VariantDiscovery.discoverVariantsAndSync(VariantDiscovery.kt:136)
	at com.android.tools.idea.gradle.project.sync.SyncProjectActionWorker$populateAndroidModels$2.invoke(SyncProjectActionWorker.kt:72)
	at com.android.tools.idea.gradle.project.sync.SyncProjectActionWorker$populateAndroidModels$2.invoke(SyncProjectActionWorker.kt:58)
	at com.android.tools.idea.projectsystem.gradle.sync.Counter.invoke(PerformanceMeasurementUtil.kt:108)
	at com.android.tools.idea.gradle.project.sync.SyncProjectActionWorker.populateAndroidModels(SyncProjectActionWorker.kt:58)
	at com.android.tools.idea.gradle.project.sync.AndroidExtraModelProviderWorker.populateBuildModels(AndroidExtraModelProviderWorker.kt:106)
	at com.android.tools.idea.gradle.project.sync.AndroidExtraModelProviderImpl.populateBuildModels(AndroidExtraModelProvider.kt:114)
	at com.android.tools.idea.gradle.project.sync.AndroidExtraModelProvider.populateBuildModels(AndroidExtraModelProvider.kt:56)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$populateModels$13(GradleModelFetchAction.java:281)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.lambda$runWithSpan$1(GradleOpenTelemetry.java:87)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:61)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.runWithSpan(GradleOpenTelemetry.java:86)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$populateModels$15(GradleModelFetchAction.java:278)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.lambda$runWithSpan$1(GradleOpenTelemetry.java:87)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:61)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.runWithSpan(GradleOpenTelemetry.java:86)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.populateModels(GradleModelFetchAction.java:265)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$executeAction$10(GradleModelFetchAction.java:248)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.lambda$runWithSpan$1(GradleOpenTelemetry.java:87)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:61)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.runWithSpan(GradleOpenTelemetry.java:86)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$executeAction$11(GradleModelFetchAction.java:246)
	at java.base/java.lang.Iterable.forEach(Unknown Source)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.executeAction(GradleModelFetchAction.java:245)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.doExecute(GradleModelFetchAction.java:143)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$1(GradleModelFetchAction.java:103)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:61)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$2(GradleModelFetchAction.java:102)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.withOpenTelemetry(GradleModelFetchAction.java:113)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$3(GradleModelFetchAction.java:101)
	at com.intellij.gradle.toolingExtension.impl.util.GradleExecutorServiceUtil.withSingleThreadExecutor(GradleExecutorServiceUtil.java:18)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.execute(GradleModelFetchAction.java:100)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.execute(GradleModelFetchAction.java:36)
	at org.gradle.tooling.internal.consumer.connection.InternalBuildActionAdapter.execute(InternalBuildActionAdapter.java:65)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.runAction(AbstractClientProvidedBuildActionRunner.java:131)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.fromBuildModel(AbstractClientProvidedBuildActionRunner.java:104)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner$ActionAdapter.fromBuildModel(AbstractClientProvidedBuildActionRunner.java:84)
	at org.gradle.internal.buildtree.DefaultBuildTreeModelCreator.fromBuildModel(DefaultBuildTreeModelCreator.java:57)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.lambda$fromBuildModel$2(DefaultBuildTreeLifecycleController.java:89)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.lambda$runBuild$4(DefaultBuildTreeLifecycleController.java:119)
	at org.gradle.internal.model.StateTransitionController.lambda$transition$6(StateTransitionController.java:169)
	at org.gradle.internal.model.StateTransitionController.doTransition(StateTransitionController.java:266)
	at org.gradle.internal.model.StateTransitionController.lambda$transition$7(StateTransitionController.java:169)
	at org.gradle.internal.work.DefaultSynchronizer.withLock(DefaultSynchronizer.java:44)
	at org.gradle.internal.model.StateTransitionController.transition(StateTransitionController.java:169)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.runBuild(DefaultBuildTreeLifecycleController.java:116)
	at org.gradle.internal.buildtree.DefaultBuildTreeLifecycleController.fromBuildModel(DefaultBuildTreeLifecycleController.java:81)
	at org.gradle.tooling.internal.provider.runner.AbstractClientProvidedBuildActionRunner.runClientAction(AbstractClientProvidedBuildActionRunner.java:43)
	at org.gradle.tooling.internal.provider.runner.ClientProvidedPhasedActionRunner.run(ClientProvidedPhasedActionRunner.java:53)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.internal.buildtree.ProblemReportingBuildActionRunner.run(ProblemReportingBuildActionRunner.java:49)
	at org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:65)
	at org.gradle.tooling.internal.provider.FileSystemWatchingBuildActionRunner.run(FileSystemWatchingBuildActionRunner.java:140)
	at org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:41)
	at org.gradle.launcher.exec.RootBuildLifecycleBuildActionExecutor.lambda$execute$0(RootBuildLifecycleBuildActionExecutor.java:40)
	at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:123)
	at org.gradle.launcher.exec.RootBuildLifecycleBuildActionExecutor.execute(RootBuildLifecycleBuildActionExecutor.java:40)
	at org.gradle.internal.buildtree.InitDeprecationLoggingActionExecutor.execute(InitDeprecationLoggingActionExecutor.java:62)
	at org.gradle.internal.buildtree.InitProblems.execute(InitProblems.java:38)
	at org.gradle.internal.buildtree.DefaultBuildTreeContext.execute(DefaultBuildTreeContext.java:40)
	at org.gradle.launcher.exec.BuildTreeLifecycleBuildActionExecutor.lambda$execute$0(BuildTreeLifecycleBuildActionExecutor.java:58)
	at org.gradle.internal.buildtree.BuildTreeState.run(BuildTreeState.java:58)
	at org.gradle.launcher.exec.BuildTreeLifecycleBuildActionExecutor.execute(BuildTreeLifecycleBuildActionExecutor.java:58)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor$3.call(RunAsBuildOperationBuildActionExecutor.java:61)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor$3.call(RunAsBuildOperationBuildActionExecutor.java:57)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
	at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
	at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionExecutor.execute(RunAsBuildOperationBuildActionExecutor.java:57)
	at org.gradle.launcher.exec.RunAsWorkerThreadBuildActionExecutor.lambda$execute$0(RunAsWorkerThreadBuildActionExecutor.java:36)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:264)
	at org.gradle.internal.work.DefaultWorkerLeaseService.runAsWorkerThread(DefaultWorkerLeaseService.java:128)
	at org.gradle.launcher.exec.RunAsWorkerThreadBuildActionExecutor.execute(RunAsWorkerThreadBuildActionExecutor.java:36)
	at org.gradle.tooling.internal.provider.continuous.ContinuousBuildActionExecutor.execute(ContinuousBuildActionExecutor.java:110)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionExecutor.execute(SubscribableBuildActionExecutor.java:64)
	at org.gradle.internal.session.DefaultBuildSessionContext.execute(DefaultBuildSessionContext.java:46)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter$ActionImpl.apply(BuildSessionLifecycleBuildActionExecuter.java:92)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter$ActionImpl.apply(BuildSessionLifecycleBuildActionExecuter.java:80)
	at org.gradle.internal.session.BuildSessionState.run(BuildSessionState.java:69)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter.execute(BuildSessionLifecycleBuildActionExecuter.java:62)
	at org.gradle.tooling.internal.provider.BuildSessionLifecycleBuildActionExecuter.execute(BuildSessionLifecycleBuildActionExecuter.java:41)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:64)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:51)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:39)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:47)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:31)
	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:65)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:39)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:29)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:35)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:78)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:75)
	at org.gradle.util.internal.Swapper.swap(Swapper.java:38)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:75)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:64)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:63)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:84)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)
	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:52)
	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.AbstractManagedExecutor$1.run(AbstractManagedExecutor.java:47)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.gradle.api.GradleException: Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore.createTaskAction(DefaultTaskClassInfoStore.java:126)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore.createTaskClassInfo(DefaultTaskClassInfoStore.java:63)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore.lambda$new$0(DefaultTaskClassInfoStore.java:43)
	at org.gradle.cache.internal.DefaultCrossBuildInMemoryCacheFactory$AbstractCrossBuildInMemoryCache.get(DefaultCrossBuildInMemoryCacheFactory.java:130)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore.getTaskClassInfo(DefaultTaskClassInfoStore.java:51)
	at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory.process(AnnotationProcessingTaskFactory.java:52)
	at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory.create(AnnotationProcessingTaskFactory.java:48)
	at org.gradle.api.internal.tasks.DefaultTaskContainer.createTask(DefaultTaskContainer.java:329)
	at org.gradle.api.internal.tasks.DefaultTaskContainer.access$200(DefaultTaskContainer.java:78)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider.createDomainObject(DefaultTaskContainer.java:699)
	at org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreatingProvider.createDomainObject(DefaultTaskContainer.java:656)
	at org.gradle.api.internal.DefaultNamedDomainObjectCollection$AbstractDomainObjectCreatingProvider.tryCreate(DefaultNamedDomainObjectCollection.java:947)
	... 221 more



Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.

You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.

For more on this, please refer to https://docs.gradle.org/8.5/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.

BUILD FAILED in 21s
Please help!
",piskamenagabygaga,2024-11-12 14:40:36+00:00,['Venkat6871'],2024-12-13 02:08:52+00:00,2024-12-13 02:08:49+00:00,https://github.com/tensorflow/tensorflow/issues/79891,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('type:performance', 'Performance Issue')]","[{'comment_id': 2475505722, 'issue_id': 2652398697, 'author': 'Venkat6871', 'body': 'Hi **@piskamenagabygaga** ,\r\nApologies for the delay, and thank you for raising your concern here. Could you please confirm if you are experiencing any issues with TensorFlow? If so, kindly fill out all the required templates. Additionally, please check the compatibility of all versions involved. I am providing the documentation [link1](https://www.tensorflow.org/install/pip), [link2](https://www.tensorflow.org/install/source_windows) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 14, 6, 19, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492724238, 'issue_id': 2652398697, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 22, 2, 4, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493340240, 'issue_id': 2652398697, 'author': 'zhengtulymGh', 'body': ""I'm also facing this issue, please help.\r\n\r\n// build.gradle\r\n```\r\n// Top-level build file where you can add configuration options common to all sub-projects/modules.\r\nbuildscript {\r\n    dependencies {\r\n        classpath 'androidx.navigation:navigation-safe-args-gradle-plugin:2.4.2'\r\n        classpath 'de.undercouch:gradle-download-task:4.1.2'\r\n    }\r\n}\r\nplugins {\r\n    id 'com.android.application' version '7.2.1' apply false\r\n    id 'com.android.library' version '7.2.1' apply false\r\n    id 'org.jetbrains.kotlin.android' version '1.6.21' apply false\r\n}\r\n\r\ntask clean(type: Delete) {\r\n    delete rootProject.buildDir\r\n}\r\n```\r\n\r\n// gradle-wrapper.properties\r\n```\r\ndistributionBase=GRADLE_USER_HOME\r\ndistributionPath=wrapper/dists\r\ndistributionUrl=https\\://services.gradle.org/distributions/gradle-8.4-bin.zip\r\nnetworkTimeout=10000\r\nvalidateDistributionUrl=true\r\nzipStoreBase=GRADLE_USER_HOME\r\nzipStorePath=wrapper/dists\r\n```\r\n\r\n```\r\nA build operation failed.\r\n    Could not create task ':app:processDebugResources'.\r\nCould not create task ':app:processDebugResources'.\r\nCannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.\r\n\r\n* Try:\r\n> Run with --info or --debug option to get more log output.\r\n> Run with --scan to get full insights.\r\n> Get more help at https://help.gradle.org.\r\n\r\n* Exception is:\r\ncom.intellij.openapi.externalSystem.model.ExternalSystemException: A build operation failed.\r\n    Could not create task ':app:processDebugResources'.\r\nCould not create task ':app:processDebugResources'.\r\nCannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.\r\n\tat com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.addModels(GradleModelFetchAction.java:183)\r\n\tat com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$1(GradleModelFetchAction.java:73)\r\n\tat com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.lambda$runWithSpan$1(GradleOpenTelemetry.java:87)\r\n\tat com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)\r\n        ...\r\n```"", 'created_at': datetime.datetime(2024, 11, 22, 9, 42, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518930856, 'issue_id': 2652398697, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 5, 2, 8, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540378076, 'issue_id': 2652398697, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540378117, 'issue_id': 2652398697, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79891"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79891"">No</a>', 'created_at': datetime.datetime(2024, 12, 13, 2, 8, 51, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-14 06:19:22 UTC): Hi **@piskamenagabygaga** ,
Apologies for the delay, and thank you for raising your concern here. Could you please confirm if you are experiencing any issues with TensorFlow? If so, kindly fill out all the required templates. Additionally, please check the compatibility of all versions involved. I am providing the documentation [link1](https://www.tensorflow.org/install/pip), [link2](https://www.tensorflow.org/install/source_windows) here for your reference.
Thank you!

github-actions[bot] on (2024-11-22 02:04:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

zhengtulymGh on (2024-11-22 09:42:17 UTC): I'm also facing this issue, please help.

// build.gradle
```
// Top-level build file where you can add configuration options common to all sub-projects/modules.
buildscript {
    dependencies {
        classpath 'androidx.navigation:navigation-safe-args-gradle-plugin:2.4.2'
        classpath 'de.undercouch:gradle-download-task:4.1.2'
    }
}
plugins {
    id 'com.android.application' version '7.2.1' apply false
    id 'com.android.library' version '7.2.1' apply false
    id 'org.jetbrains.kotlin.android' version '1.6.21' apply false
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
```

// gradle-wrapper.properties
```
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.4-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists
```

```
A build operation failed.
    Could not create task ':app:processDebugResources'.
Could not create task ':app:processDebugResources'.
Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.

* Try:

* Exception is:
com.intellij.openapi.externalSystem.model.ExternalSystemException: A build operation failed.
    Could not create task ':app:processDebugResources'.
Could not create task ':app:processDebugResources'.
Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.addModels(GradleModelFetchAction.java:183)
	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.lambda$execute$1(GradleModelFetchAction.java:73)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.lambda$runWithSpan$1(GradleOpenTelemetry.java:87)
	at com.intellij.gradle.toolingExtension.impl.telemetry.GradleOpenTelemetry.callWithSpan(GradleOpenTelemetry.java:73)
        ...
```

github-actions[bot] on (2024-12-05 02:08:29 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-13 02:08:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-13 02:08:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79891"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79891"">No</a>

"
2650158836,issue,closed,completed,JIT compliation failed,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Radeon 7900XT

### Current behavior?

I tried to run some tensorflow code to process a few videos in a model I have. When running this code on the CPU, everything works fine. It doesn't work when running it on the GPU.

### Standalone code to reproduce the issue

```shell
# main.py

import numpy as np
import tensorflow as tf
import imageio

from data_processing.data_processing import DatasetPreparer, DataLoader, num_to_char, char_to_num
from data_processing.mouth_detection import MouthDetector
from model.model import LipReadingModel

physical_devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    pass


def main():
    base_dir = ""data/A_U_EE_E/temp/""
    original_video_dir = base_dir + ""videos""
    original_subtitle_dir = base_dir + ""subtitles""
    output_dir = base_dir + ""separated""
    video_dir = output_dir + ""/videos""
    subtitle_dir = output_dir + ""/subtitles""

    mouth_detector = MouthDetector()

    # Instantiate DataLoader (or appropriate class) and DatasetPreparer
    data_loader = DataLoader(detector=mouth_detector)  # Initialize with any necessary parameters

    data_loader.process_all_videos(original_video_dir, original_subtitle_dir, output_dir)

    dataset_preparer = DatasetPreparer(video_directory=video_dir, data_loader=data_loader)  # Provide data_loader here

    dataset = dataset_preparer.prepare_dataset()

    # Fetch a batch of data
    data_iterator = dataset.as_numpy_iterator()
    video_frames, subtitle_tokens = data_iterator.next()

    # # Process frames for saving as a GIF
    # processed_frames = []
    # for frame in video_frames[0]:  # Access the first video in the batch
    #     # Convert normalized frame to uint8
    #     frame = (frame.numpy() * 255).astype(np.uint8)
    #
    #     # Check and reshape if necessary
    #     if frame.shape[-1] == 1:  # Grayscale with singleton dimension
    #         frame = np.squeeze(frame, axis=-1)  # Remove the last dimension for display
    #
    #     processed_frames.append(frame)
    #
    # # Save frames as GIF
    # imageio.mimsave(""./animation.gif"", processed_frames, fps=30)

    # # Decode subtitle tokens to text for verification
    # decoded_subtitles = tf.strings.reduce_join(
    #     [tf.compat.as_str_any(x) for x in num_to_char(subtitle_tokens[0]).numpy()])
    # print(""Decoded Subtitles:"", decoded_subtitles.numpy().decode('utf-8'))


    model = LipReadingModel(char_to_num.vocabulary_size())

    yhat = model.predict(video_frames)
    print(tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[0]]))
    print(model.input_shape)


if __name__ == ""__main__"":
    main()


# data_processing/data_processing.py
import csv
import os
import cv2
import tensorflow as tf
import pandas as pd
from data_processing.mouth_detection import MouthDetector

# Define vocabulary for character mapping
vocab = [""A"", ""U"", ""EE"", ""E"", "" ""]
char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="""")
num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token="""", invert=True)


class DataLoader:
    def __init__(self, detector: MouthDetector):
        self.detector = detector

    def load_video(self, path: str) -> tf.Tensor:
        """"""
        Load video frames, apply mouth detection, convert to grayscale, and normalize.
        """"""
        cap = cv2.VideoCapture(path)
        frames = []

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame = self.detector.detect_and_crop_mouth(frame)  # Crop to mouth region
            if frame is not None:
                frame = tf.image.rgb_to_grayscale(frame)
                frames.append(frame)

        cap.release()
        if len(frames) == 0:
            raise ValueError(f""No valid frames found in video {path}"")

        # Normalize frames
        # mean = tf.math.reduce_mean(frames)
        # std = tf.math.reduce_std(tf.cast(frames, tf.float32))
        # return tf.cast((frames - mean), tf.float32) / std

        # return tf.image.per_image_standardization(frames)

        mean = tf.math.reduce_mean(frames, axis=[0, 1, 2], keepdims=True)
        std = tf.math.reduce_std(tf.cast(frames, tf.float32), axis=[0, 1, 2], keepdims=True)
        frames = tf.cast(frames, tf.float32)
        normalized_frames = (frames - mean) / std
        return normalized_frames

    def load_subtitles(self, path: str) -> tf.Tensor:
        """"""
        Load subtitles and map them to character indices.
        """"""
        df = pd.read_csv(path, header=None, names=['start_time', 'end_time', 'subtitle'])
        tokens = []

        for _, row in df.iterrows():
            subtitle = row['subtitle'].strip().upper()
            if subtitle and subtitle != 'IDLE':
                tokens.extend(list(subtitle) + [' '])

        tokens = tokens[:-1]
        tokenized = tf.strings.unicode_split(tokens, input_encoding='UTF-8')
        return char_to_num(tf.reshape(tokenized, [-1]))

    def split_video_by_frames(self, video_path, subtitles_path, output_dir, max_frames=120):
        """"""
        Split video into chunks of `max_frames` or fewer, keeping word boundaries intact.
        """"""
        # Load video and subtitle data
        cap = cv2.VideoCapture(video_path)
        df = pd.read_csv(subtitles_path, header=None, names=['start_time', 'end_time', 'subtitle'])

        fps = cap.get(cv2.CAP_PROP_FPS)
        part_num = 1
        chunk_frames = []
        chunk_subtitles = []
        current_frame_count = 0
        start_time = 0

        for index, row in df.iterrows():
            start_ms, end_ms, subtitle = row['start_time'], row['end_time'], row['subtitle']

            # Convert start and end times to frame indices
            start_frame = int((start_ms / 1000) * fps)
            end_frame = int((end_ms / 1000) * fps)
            word_frame_count = end_frame - start_frame

            if current_frame_count + word_frame_count > max_frames:
                # Save current chunk if adding this word exceeds the max frame count
                self.save_chunk(chunk_frames, chunk_subtitles, video_path, output_dir, part_num, fps)
                part_num += 1
                chunk_frames = []
                chunk_subtitles = []
                current_frame_count = 0
                start_time = start_ms  # New start time for the new chunk

            # Add word frames and subtitle
            chunk_subtitles.append((start_ms - start_time, end_ms - start_time, subtitle))

            # Extract frames for this word
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            for _ in range(word_frame_count):
                ret, frame = cap.read()
                if not ret:
                    break
                chunk_frames.append(frame)
                current_frame_count += 1

        # Save the last chunk
        if chunk_frames:
            self.save_chunk(chunk_frames, chunk_subtitles, video_path, output_dir, part_num, fps)

        cap.release()

    def save_chunk(self, chunk_frames, chunk_subtitles, video_path, output_dir, part_num, fps):
        """"""
        Save a chunk of frames and its corresponding subtitles.
        """"""
        # Define output video and CSV paths
        file_name = os.path.splitext(os.path.basename(video_path))[0]
        output_video_path = os.path.join(output_dir, ""videos"", f""{file_name}_{part_num}.mp4"")
        output_csv_path = os.path.join(output_dir, ""subtitles"", f""{file_name}_{part_num}.csv"")

        # Save the video chunk
        height, width, _ = chunk_frames[0].shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        for frame in chunk_frames:
            out.write(frame)
        out.release()

        # Save the CSV chunk with adjusted timestamps
        with open(output_csv_path, mode='w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            for start_ms, end_ms, subtitle in chunk_subtitles:
                writer.writerow([start_ms, end_ms, subtitle])

    def process_all_videos(self, video_directory, subtitles_directory, output_directory):
        """"""
        Process each video in the video_directory, split it, and generate output.
        """"""
        for video_file in os.listdir(video_directory):
            if video_file.endswith("".mp4""):
                video_path = os.path.join(video_directory, video_file)
                subtitle_path = os.path.join(subtitles_directory, f""{os.path.splitext(video_file)[0]}.csv"")
                self.split_video_by_frames(video_path, subtitle_path, output_directory)


class PreProcessor:
    @staticmethod
    def prepare_video_and_subtitles(video_path: tf.Tensor, data_loader: DataLoader):
        """"""
        Prepares video and subtitle tensors.
        """"""
        video_path = video_path.numpy().decode('utf-8')
        base_dir = os.path.dirname(os.path.dirname(video_path))
        file_name = os.path.splitext(os.path.basename(video_path))[0]

        subtitles_path = os.path.join(base_dir, 'subtitles', f'{file_name}.csv')
        video_tensor = data_loader.load_video(video_path)
        subtitle_tensor = data_loader.load_subtitles(subtitles_path)

        return video_tensor, subtitle_tensor

    @staticmethod
    def mappable_fn(video_path: tf.Tensor, data_loader: DataLoader):
        """"""
        A wrapper function that maps video path to frames and alignments.
        """"""
        return tf.py_function(lambda x: PreProcessor.prepare_video_and_subtitles(x, data_loader), [video_path], [tf.float32, tf.int64])


class Augmentor:
    @staticmethod
    def augment_video(frames: tf.Tensor) -> tf.Tensor:
        """"""
        Augment video frames by applying transformations such as flipping and concatenating.
        """"""
        if frames.shape.rank == 5:
            # Apply flipping to each frame in the video sequence
            flipped_frames = tf.map_fn(lambda x: tf.image.flip_left_right(x), frames)
        elif frames.shape.rank == 4:
            # Apply flipping directly if frames already have 4D shape
            flipped_frames = tf.image.flip_left_right(frames)
        else:
            raise ValueError(""Expected frames to have 4 or 5 dimensions, got shape: {}"".format(frames.shape))

        # Concatenate original and flipped frames along the batch dimension
        return tf.concat([frames, flipped_frames], axis=0)


class DatasetPreparer:
    def __init__(self, video_directory: str, data_loader: DataLoader):
        self.video_directory = video_directory
        self.data_loader = data_loader

    def prepare_dataset(self) -> tf.data.Dataset:
        """"""
        Prepare a dataset that reads videos and subtitles, applies augmentations, and batches data.
        """"""
        dataset = tf.data.Dataset.list_files(f""{self.video_directory}/*.mp4"")
        dataset = dataset.shuffle(100)
        dataset = dataset.map(lambda path: PreProcessor.mappable_fn(path, self.data_loader))

        # 5400 frames in each training video (assuming 3 minutes 30 fps)
        # 240 tokens in each video (120 letters plus space token to separate)
        frames, alignments = dataset.as_numpy_iterator().next()
        print(frames.shape, alignments.shape)
        dataset = dataset.padded_batch(2, padded_shapes=([120, None, None, None], [12]))
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        dataset = dataset.map(lambda frames, alignments: (Augmentor.augment_video(frames), alignments))

        return dataset


# data_processing/mouth_detection.py

import cv2
import mediapipe as mp
from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np


class MouthDetector:
    def __init__(self, model_path='assets/face_landmarker.task', num_faces=1):
        base_options = python.BaseOptions(model_asset_path=model_path, delegate=""GPU"")
        options = vision.FaceLandmarkerOptions(base_options=base_options,
                                               output_face_blendshapes=True,
                                               output_facial_transformation_matrixes=True,
                                               num_faces=num_faces)
        self.detector = vision.FaceLandmarker.create_from_options(options)

    def expand_bounding_box(self, xmin, ymin, xmax, ymax, padding_ratio=0.4):
        width = xmax - xmin
        height = ymax - ymin
        pad_w = int(width * padding_ratio)
        pad_h = int(height * padding_ratio)
        xmin = max(xmin - pad_w, 0)
        ymin = max(ymin - pad_h, 0)
        xmax = xmax + pad_w
        ymax = ymax + pad_h
        return xmin, ymin, xmax, ymax

    def draw_landmarks_on_image(self, rgb_image, detection_result):
        face_landmarks_list = detection_result.face_landmarks
        annotated_image = np.copy(rgb_image)

        for idx in range(len(face_landmarks_list)):
            face_landmarks = face_landmarks_list[idx]

            face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
            face_landmarks_proto.landmark.extend([
                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks
            ])

            solutions.drawing_utils.draw_landmarks(
                image=annotated_image,
                landmark_list=face_landmarks_proto,
                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp.solutions.drawing_styles
                .get_default_face_mesh_tesselation_style())
            solutions.drawing_utils.draw_landmarks(
                image=annotated_image,
                landmark_list=face_landmarks_proto,
                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp.solutions.drawing_styles
                .get_default_face_mesh_contours_style())
            solutions.drawing_utils.draw_landmarks(
                image=annotated_image,
                landmark_list=face_landmarks_proto,
                connections=mp.solutions.face_mesh.FACEMESH_IRISES,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp.solutions.drawing_styles
                .get_default_face_mesh_iris_connections_style())

        return annotated_image

    def crop_mouth_from_landmarks(self, rgb_image, detection_result, target_size=(250, 100)):
        if detection_result and detection_result.face_landmarks:
            try:
                face_landmarks = detection_result.face_landmarks[0]
                # These are the landmarks for the mouth
                mouth_landmarks = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409,
                                   146, 91, 181, 84, 17, 314, 405, 321, 375, 291,
                                   78, 191, 80, 81, 82, 13, 312, 311, 310, 415,
                                   95, 88, 178, 87, 14, 317, 402, 318, 324, 308]

                x_coords = [face_landmarks[landmark].x for landmark in mouth_landmarks]
                y_coords = [face_landmarks[landmark].y for landmark in mouth_landmarks]
                xmin, xmax = int(min(x_coords) * rgb_image.shape[1]), int(max(x_coords) * rgb_image.shape[1])
                ymin, ymax = int(min(y_coords) * rgb_image.shape[0]), int(max(y_coords) * rgb_image.shape[0])

                xmin, ymin, xmax, ymax = self.expand_bounding_box(xmin, ymin, xmax, ymax)
                cropped_mouth = rgb_image[ymin:ymax, xmin:xmax]
                return cv2.resize(cropped_mouth, target_size, interpolation=cv2.INTER_AREA)
            except cv2.error:
                return None
        return None

    def detect_and_crop_mouth(self, frame, target_size=(250, 100)):
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image_input = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        detection_result = self.detector.detect(mp_image_input)
        return self.crop_mouth_from_landmarks(mp_image_input.numpy_view(), detection_result, target_size=target_size)


# model/model.py

import tensorflow as tf
from tensorflow.keras import layers, models


class LipReadingModel:
    def __init__(self, input_shape=(250, 100), num_classes=5):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = self.create_model()

    def create_model(self):
        """"""
        Creates and compiles the lip-reading model using the Sequential API.
        """"""
        model = models.Sequential()

        model.add(layers.Conv3D(128, 3, input_shape=(120, 100, 250, 1), padding='same'))
        model.add(layers.Activation('relu'))
        model.add(layers.MaxPool3D((1, 2, 2)))

        model.add(layers.Conv3D(256, 3, padding='same'))
        model.add(layers.Activation('relu'))
        model.add(layers.MaxPool3D((1, 2, 2)))

        model.add(layers.Conv3D(120, 3, padding='same'))
        model.add(layers.Activation('relu'))
        model.add(layers.MaxPool3D((1, 2, 2)))

        model.add(layers.TimeDistributed(layers.Flatten()))

        model.add(layers.Bidirectional(layers.LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
        model.add(layers.Dropout(.5))

        model.add(layers.Bidirectional(layers.LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
        model.add(layers.Dropout(.5))

        model.add(layers.Dense(self.num_classes + 1, kernel_initializer='he_normal', activation='softmax'))

        print(model.summary())

        return model

    def load(self, model_path):
        """"""
        Load a pre-trained model from a given path.
        """"""
        self.model = tf.keras.models.load_model(model_path)

    def predict(self, frames):
        """"""
        Predict the sequence of characters from video frames.
        """"""
        # frames = frames / 255.0  # Normalize frames
        return self.model.predict(frames)

    def save(self, model_path):
        """"""
        Save the trained model to the specified path.
        """"""
        self.model.save(model_path)
```


### Relevant log output

```shell
2024-11-11 20:01:52.496107: E external/local_xla/xla/stream_executor/plugin_registry.cc:91] Invalid plugin kind specified: FFT
2024-11-11 20:01:52.522679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-11 20:01:52.765609: E external/local_xla/xla/stream_executor/plugin_registry.cc:91] Invalid plugin kind specified: DNN
2024-11-11 20:01:53.766866: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795159: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795196: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795770: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795818: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795838: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795883: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795905: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795928: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-11-11 20:01:53.795938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19406 MB memory:  -> device: 0, name: Radeon RX 7900 XT, pci bus id: 0000:03:00.0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1731348114.182076   76546 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5
I0000 00:00:1731348114.183730   76671 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.0-devel), renderer: Radeon RX 7900 XT (radeonsi, navi31, LLVM 18.1.7, DRM 3.57, 6.8.0-48-generic)
W0000 00:00:1731348114.184010   76546 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1731348114.187960   76674 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1731348114.199152   76685 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
2024-11-11 20:02:36.433486: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433500: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433510: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433519: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433533: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433548: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433562: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
2024-11-11 20:02:36.433574: E external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:243] bitcode module is required by this HLO module but was not found at ./opencl.bc
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
2024-11-11 20:02:36.433878: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:207] INTERNAL: Generating device code failed.
2024-11-11 20:02:36.434390: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.
2024-11-11 20:02:36.434402: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.
Traceback (most recent call last):
  File ""/home/yoav/PycharmProjects/Lip-C/main.py"", line 70, in <module>
    main()
  File ""/home/yoav/PycharmProjects/Lip-C/main.py"", line 35, in main
    dataset = dataset_preparer.prepare_dataset()
  File ""/home/yoav/PycharmProjects/Lip-C/data_processing/data_processing.py"", line 205, in prepare_dataset
    dataset = tf.data.Dataset.list_files(f""{self.video_directory}/*.mp4"")
  File ""/home/yoav/PycharmProjects/Lip-C/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1338, in list_files
    buffer_size = math_ops.maximum(
  File ""/home/yoav/PycharmProjects/Lip-C/venv/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
  File ""/home/yoav/PycharmProjects/Lip-C/venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 6419, in maximum
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/yoav/PycharmProjects/Lip-C/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__Maximum_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Maximum] name:
```
",MrYoavon,2024-11-11 19:13:11+00:00,['gaikwadrahul8'],2024-12-16 12:04:11+00:00,2024-12-16 12:04:08+00:00,https://github.com/tensorflow/tensorflow/issues/79798,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2474442663, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': ""By the way, I'm using ROCm version 6.2.2"", 'created_at': datetime.datetime(2024, 11, 13, 18, 38, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475985337, 'issue_id': 2650158836, 'author': 'Venkat6871', 'body': 'Hi **@MrYoavon** ,\r\nApologies for the delay, and thank you for raising your concern here. I am trying to replicate your code on Colab but am encountering a different issue. Could you please provide your Colab gist? It would make troubleshooting your issue easier.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 14, 10, 32, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476001832, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': 'I could but I encounter the same issue when running a simple code from ROCm tensorflow configuration guide.\n\nimport tensorflow as tf\nprint(""TensorFlow version:"", tf.__version__)\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation=\'relu\'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\npredictions = model(x_train[:1]).numpy()\ntf.nn.softmax(predictions).numpy()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_fn(y_train[:1], predictions).numpy()\nmodel.compile(optimizer=\'adam\',\n              loss=loss_fn,\n              metrics=[\'accuracy\'])\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n\n\nThis code is meant to help you check if you can run Tensorflow on your AMD GPU, and this too doesn\'t work with the same error message (the line that crashes is a different one but the error code is still the same)', 'created_at': datetime.datetime(2024, 11, 14, 10, 40, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476874243, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': 'Okay... I have made some odd progress. I can run the code perfectly fine if I use the terminal to run the main.py file. The ""Run File"" button in Pycharm Professional 2024.2.3 somehow causes it to not work properly.\r\nI would like to be able to use the button but if it\'s an issue with Pycharm then I guess I\'ll just use the terminal. Maybe it\'s something with the run configuration of Pycharm?', 'created_at': datetime.datetime(2024, 11, 14, 16, 28, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482353524, 'issue_id': 2650158836, 'author': 'Venkat6871', 'body': 'Hi **@MrYoavon** ,\r\nApologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and it worked fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ce2bb22feb799d2f54ea4a782ba1b85f/79798_tf_2-17-0-nightly-v.ipynb) here for your reference. Let me know if I missed anything.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 18, 9, 8, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482811723, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': ""This wouldn't cause an issue on Colab since the problem is with the use of Tensorflow with ROCm. The code itself isn't problematic. According to ROCm documentation, my ROCm version is only compatible with Tensorflow versions 2.14.1, 2.15.1, 2.16.1. I'm using Tensorflow 2.16.1 and ROCm 6.2.4."", 'created_at': datetime.datetime(2024, 11, 18, 11, 46, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484130621, 'issue_id': 2650158836, 'author': 'Gika-0', 'body': 'Hi, \r\nI follow the topic, I have the same problem with 7900XTX ubuntu 24.04 rocm 6.2.4 and tensorflow-rocm 2.16.2 and python 3.10 in a venv.', 'created_at': datetime.datetime(2024, 11, 18, 21, 6, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487811900, 'issue_id': 2650158836, 'author': 'Venkat6871', 'body': 'Hi **@MrYoavon** ,\r\nCould you please provide the ROCm documentation you are following? Also, try using the latest TensorFlow versions (2.17.0 or 2.18.0), as they may help you run more smoothly.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 20, 7, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487825292, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': ""Hi,\nThis is the ROCm documentation I followed: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html\n\nUsing Tensorflow 2.17 or 2.18 isn't possible, according to the documentation."", 'created_at': datetime.datetime(2024, 11, 20, 8, 5, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523137448, 'issue_id': 2650158836, 'author': 'patrick-praher', 'body': 'Hi,\r\ni had the same issue and fixed it. The hint came from: https://github.com/ROCm/ROCm/issues/3835\r\nI just set the following environment variable: `ROCM_PATH=/opt/rocm` before executing the script', 'created_at': datetime.datetime(2024, 12, 6, 12, 42, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528695016, 'issue_id': 2650158836, 'author': 'gaikwadrahul8', 'body': ""Hi, @MrYoavon, @Gika-0\r\nI apologize for the delayed response, Hi, @patrick-praher thank you for your pointers, it is known issue please refer this original issue https://github.com/ROCm/ROCm/issues/1796#issuecomment-1447710413 and setting `ROCM_PATH` to `/opt/rocm` should fix this issue and also make sure that you're using correct supported versions of `ROCm` and `TensorFlow` mentioned [here](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html#installing-tensorflow-for-rocm)\r\n\r\n**Note :** As of **ROCm 6.1**, tensorflow-rocm packages are found at https://repo.radeon.com/rocm/manylinux. Prior to ROCm 6.1, packages were found at https://pypi.org/project/tensorflow-rocm.\r\n\r\nPlease let us know after setting `ROCM_PATH` to `/opt/rocm` resolving your issue or not ? if issue still persists please let us know. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 12, 9, 16, 51, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545448377, 'issue_id': 2650158836, 'author': 'MrYoavon', 'body': 'This worked. Thanks to everyone who has taken their time to help :)', 'created_at': datetime.datetime(2024, 12, 16, 12, 4, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545448465, 'issue_id': 2650158836, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79798"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79798"">No</a>', 'created_at': datetime.datetime(2024, 12, 16, 12, 4, 10, tzinfo=datetime.timezone.utc)}]","MrYoavon (Issue Creator) on (2024-11-13 18:38:09 UTC): By the way, I'm using ROCm version 6.2.2

Venkat6871 on (2024-11-14 10:32:42 UTC): Hi **@MrYoavon** ,
Apologies for the delay, and thank you for raising your concern here. I am trying to replicate your code on Colab but am encountering a different issue. Could you please provide your Colab gist? It would make troubleshooting your issue easier.
Thank you!

MrYoavon (Issue Creator) on (2024-11-14 10:40:42 UTC): I could but I encounter the same issue when running a simple code from ROCm tensorflow configuration guide.

import tensorflow as tf
print(""TensorFlow version:"", tf.__version__)
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
predictions = model(x_train[:1]).numpy()
tf.nn.softmax(predictions).numpy()
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test,  y_test, verbose=2)



This code is meant to help you check if you can run Tensorflow on your AMD GPU, and this too doesn't work with the same error message (the line that crashes is a different one but the error code is still the same)

MrYoavon (Issue Creator) on (2024-11-14 16:28:56 UTC): Okay... I have made some odd progress. I can run the code perfectly fine if I use the terminal to run the main.py file. The ""Run File"" button in Pycharm Professional 2024.2.3 somehow causes it to not work properly.
I would like to be able to use the button but if it's an issue with Pycharm then I guess I'll just use the terminal. Maybe it's something with the run configuration of Pycharm?

Venkat6871 on (2024-11-18 09:08:39 UTC): Hi **@MrYoavon** ,
Apologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and it worked fine for me. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/ce2bb22feb799d2f54ea4a782ba1b85f/79798_tf_2-17-0-nightly-v.ipynb) here for your reference. Let me know if I missed anything.
Thank you!

MrYoavon (Issue Creator) on (2024-11-18 11:46:50 UTC): This wouldn't cause an issue on Colab since the problem is with the use of Tensorflow with ROCm. The code itself isn't problematic. According to ROCm documentation, my ROCm version is only compatible with Tensorflow versions 2.14.1, 2.15.1, 2.16.1. I'm using Tensorflow 2.16.1 and ROCm 6.2.4.

Gika-0 on (2024-11-18 21:06:35 UTC): Hi, 
I follow the topic, I have the same problem with 7900XTX ubuntu 24.04 rocm 6.2.4 and tensorflow-rocm 2.16.2 and python 3.10 in a venv.

Venkat6871 on (2024-11-20 07:58:35 UTC): Hi **@MrYoavon** ,
Could you please provide the ROCm documentation you are following? Also, try using the latest TensorFlow versions (2.17.0 or 2.18.0), as they may help you run more smoothly.
Thank you!

MrYoavon (Issue Creator) on (2024-11-20 08:05:58 UTC): Hi,
This is the ROCm documentation I followed: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html

Using Tensorflow 2.17 or 2.18 isn't possible, according to the documentation.

patrick-praher on (2024-12-06 12:42:50 UTC): Hi,
i had the same issue and fixed it. The hint came from: https://github.com/ROCm/ROCm/issues/3835
I just set the following environment variable: `ROCM_PATH=/opt/rocm` before executing the script

gaikwadrahul8 (Assginee) on (2024-12-09 16:51:49 UTC): Hi, @MrYoavon, @Gika-0
I apologize for the delayed response, Hi, @patrick-praher thank you for your pointers, it is known issue please refer this original issue https://github.com/ROCm/ROCm/issues/1796#issuecomment-1447710413 and setting `ROCM_PATH` to `/opt/rocm` should fix this issue and also make sure that you're using correct supported versions of `ROCm` and `TensorFlow` mentioned [here](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html#installing-tensorflow-for-rocm)

**Note :** As of **ROCm 6.1**, tensorflow-rocm packages are found at https://repo.radeon.com/rocm/manylinux. Prior to ROCm 6.1, packages were found at https://pypi.org/project/tensorflow-rocm.

Please let us know after setting `ROCM_PATH` to `/opt/rocm` resolving your issue or not ? if issue still persists please let us know. 

Thank you for your cooperation and patience.

MrYoavon (Issue Creator) on (2024-12-16 12:04:08 UTC): This worked. Thanks to everyone who has taken their time to help :)

google-ml-butler[bot] on (2024-12-16 12:04:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79798"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79798"">No</a>

"
2646030310,issue,closed,completed,Configuration Error: --define PYTHON_BIN_PATH='C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is not executable. Is it the python binary?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.6.0

### Custom code

Yes

### OS platform and distribution

Windows11 x64

### Mobile device

_No response_

### Python version

3.13.0

### Bazel version

3.7.2

### GCC/compiler version

14.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
bazel build --config=opt //tensorflow:tensorflow_cc.dll
INFO: Reading 'startup' options from d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --output_user_root=D:/Users/LENOVO/Desktop/tensorflow-master/build-output
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe --action_env PYTHON_LIB_PATH=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/Lib/site-packages --python_path=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\users\lenovo\desktop\tensorflow-master\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --define framework_shared_object=false
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/b570a1921c9e55ac53c8972bd2bfd37cd0eb510d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Repository local_execution_config_python instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:15:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:1088:19: in workspace
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:85:27: in _tf_toolchains
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl:275:41: in <toplevel>
INFO: Repository local_config_python instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:15:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:1088:19: in workspace
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:95:21: in _tf_toolchains
Repository rule python_configure defined at:
  D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl:294:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl"", line 209, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is not executable. Is it the python binary?
INFO: Repository go_sdk instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:23:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
INFO: Repository rules_java instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:23:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps
Repository rule http_archive defined at:
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
ERROR: Analysis of target '//tensorflow:tensorflow_cc.dll' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is not executable. Is it the python binary?
INFO: Elapsed time: 1.363s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    currently loading: @bazel_tools//tools/jdk
    Fetching ...docker; Cloning 251f6a68b439744094faff800cd029798edf9faa of https://github.com/bazelbuild/rules_docker\
.git
make: *** [makefile:4: all] Error 1
```
'C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is a valid executable file


### Standalone code to reproduce the issue

```shell
python ./configure.py
bazel build --config=opt //tensorflow:tensorflow_cc.dll
```


### Relevant log output

```shell
bazel build --config=opt //tensorflow:tensorflow_cc.dll
INFO: Reading 'startup' options from d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --output_user_root=D:/Users/LENOVO/Desktop/tensorflow-master/build-output
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from d:\users\lenovo\desktop\tensorflow-master\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe --action_env PYTHON_LIB_PATH=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/Lib/site-packages --python_path=C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\users\lenovo\desktop\tensorflow-master\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\users\lenovo\desktop\tensorflow-master\.bazelrc: --define framework_shared_object=false
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/b570a1921c9e55ac53c8972bd2bfd37cd0eb510d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Repository local_execution_config_python instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:15:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:1088:19: in workspace
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:85:27: in _tf_toolchains
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl:275:41: in <toplevel>
INFO: Repository local_config_python instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:15:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:1088:19: in workspace
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace2.bzl:95:21: in _tf_toolchains
Repository rule python_configure defined at:
  D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl:294:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl"", line 209, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""D:/users/lenovo/desktop/tensorflow-master/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is not executable. Is it the python binary?
INFO: Repository go_sdk instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:23:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
INFO: Repository rules_java instantiated at:
  D:/users/lenovo/desktop/tensorflow-master/WORKSPACE:23:14: in <toplevel>
  D:/users/lenovo/desktop/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps
Repository rule http_archive defined at:
  D:/users/lenovo/desktop/tensorflow-master/build-output/ijcwdbxz/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
ERROR: Analysis of target '//tensorflow:tensorflow_cc.dll' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/LENOVO/AppData/Local/Programs/Python/Python313/python.exe' is not executable. Is it the python binary?
INFO: Elapsed time: 1.363s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    currently loading: @bazel_tools//tools/jdk
    Fetching ...docker; Cloning 251f6a68b439744094faff800cd029798edf9faa of https://github.com/bazelbuild/rules_docker\
.git
make: *** [makefile:4: all] Error 1
```
",6VastUniverse,2024-11-09 13:13:57+00:00,['tilakrayal'],2024-11-26 02:05:47+00:00,2024-11-26 02:05:44+00:00,https://github.com/tensorflow/tensorflow/issues/79757,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.6.0', '')]","[{'comment_id': 2467688831, 'issue_id': 2646030310, 'author': 'tilakrayal', 'body': '@6VastUniverse,\r\nPlease do the following steps to fix it manually.\r\n\r\n**Step 1.** Open the `python_configure.bzl `file, in my example the file path from [here](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L128)\r\n\r\n`cmd += ""system(\\""%s script.py\\"");"" % python_bin.replace(""\\\\"",""/"")`\r\n\r\n**Step 2.** Rerun the build command. Then, you can go further and see another error ""invalid escape sequence"".\r\n\r\nERROR\r\nC:/users/username/_bazel_username/external/local_execution_config_python/BUILD:11:24: invalid escape sequence: \\U     \r\nERROR: C:/users/username/_bazel_username/external/local_execution_config_python/BUILD:17:24: invalid escape sequence: \\U \r\n\r\n**Step 3**. Open the BUILD file, in my example the file path is C:/external/local_execution_config_python/BUILD, and replace every \\ with \\\\ at line 11 and 17.\r\n\r\n**Step 4.** Continue the build process.\r\n\r\ncould you please try adding python definitions in the {project}/WORKSPACE file, then it might work. and Project can be built successfully.  Also please set the Java path as mentioned below. Add JAVA_HOME in the environmental variables\r\n\r\nset JAVA_HOME = C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.4.101-hotspot\r\n\r\nIf you still get an issue, run bazel clean --expunge command to clear bazel cache and re-rerun the build command\r\n\r\nRefer similar issue [here](https://github.com/google/mediapipe/issues/724#issue-622686030). Thank you!', 'created_at': datetime.datetime(2024, 11, 11, 9, 47, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553249, 'issue_id': 2646030310, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499452593, 'issue_id': 2646030310, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499452812, 'issue_id': 2646030310, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79757"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79757"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 45, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-11 09:47:57 UTC): @6VastUniverse,
Please do the following steps to fix it manually.

**Step 1.** Open the `python_configure.bzl `file, in my example the file path from [here](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L128)

`cmd += ""system(\""%s script.py\"");"" % python_bin.replace(""\\"",""/"")`

**Step 2.** Rerun the build command. Then, you can go further and see another error ""invalid escape sequence"".

ERROR
C:/users/username/_bazel_username/external/local_execution_config_python/BUILD:11:24: invalid escape sequence: \U     
ERROR: C:/users/username/_bazel_username/external/local_execution_config_python/BUILD:17:24: invalid escape sequence: \U 

**Step 3**. Open the BUILD file, in my example the file path is C:/external/local_execution_config_python/BUILD, and replace every \ with \\ at line 11 and 17.

**Step 4.** Continue the build process.

could you please try adding python definitions in the {project}/WORKSPACE file, then it might work. and Project can be built successfully.  Also please set the Java path as mentioned below. Add JAVA_HOME in the environmental variables

set JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-17.0.4.101-hotspot

If you still get an issue, run bazel clean --expunge command to clear bazel cache and re-rerun the build command

Refer similar issue [here](https://github.com/google/mediapipe/issues/724#issue-622686030). Thank you!

github-actions[bot] on (2024-11-19 02:05:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-26 02:05:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-26 02:05:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79757"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79757"">No</a>

"
2645989294,issue,open,,Bug: `Compiling upb/upb.c failed` due to `Clang` Version Mismatch in Tensorflow's Docker Build Image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

CUDA 12.5.1 / cuDNN 9.3.0

### GPU model and memory

NVIDIA RTX 3090 24GB DDR6

### Current behavior?

I expected the `tensorflow-gpu` wheel to compile successfully. However I received the error detailed below. I believe it's because the required `Clang` version (`18.1.8`) installed in the docker image is incorrect. [According to the documentation](https://www.tensorflow.org/install/source#gpu), the required version of `Clang` is `17.0.6` for `tf2.18`. I pulled [`tensorflow/build:2.18-python3.9`](https://hub.docker.com/layers/tensorflow/build/2.18-python3.9/images/sha256-843f48fe24727cdef4d76ae2724edc8385b2b2b44e3e4da0752e84b9ca142a81?context=explore) from DockerHub.

Do I need to manually roll back to `Clang 17.0.6` within the container, or should I be pulling a different image?
```bash
ERROR: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing command (from target @upb//:upb) /usr/lib/llvm-18/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""upb""' -iquote ... (remaining 44 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-18/lib/clang/18/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 524.930s, Critical Path: 5.67s
INFO: 1437 processes: 820 internal, 617 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
# Pull the docker image to build tf.
docker pull tensorflow/build:2.18-python3.9

# Pull the tensorflow repo.
git pull https://github.com/tensorflow/tensorflow.git

# Change to the tensorflow directory.
cd tensorflow

# Checkout and switch to the r2.18 branch.
git fetch origin && git checkout -b r2.18 origin/r2.18

# Run a container to build tensorflow.
docker run \
        -it \
	--name tf-build \
	-h tf-build \
	-u root \
	-e HOST_PERMS=""$(id -u):$(id -g)"" \
	--runtime=nvidia \
	--gpus=all \
	--rm \
	--shm-size=2g \
	--ulimit memlock=-1 \
	--ulimit stack=67108864 \
	-v $PWD:/mnt \
	-w /mnt \
	tensorflow/build:2.18-python3.9 \
	bash

# From within the container, configure the build.
./configure

# Compile the wheel.
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
```


### Relevant log output

```shell
tf-docker /mnt > ./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.9/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.5.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 9.3.0


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-18/bin/clang]:


You have Clang 18.1.8 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
tf-docker /mnt > bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=173
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1726964088092166976 --host_action_env=DOCKER_HOST_CACHEBUSTER=1726964088140283903
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-18/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /mnt/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /mnt/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:opt in file /mnt/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native
INFO: Found applicable config definition build:linux in file /mnt/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:96:14:
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.9 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.9
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (759 packages loaded, 56246 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing command (from target @upb//:upb) /usr/lib/llvm-18/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""upb""' -iquote ... (remaining 44 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-18/lib/clang/18/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 524.930s, Critical Path: 5.67s
INFO: 1437 processes: 820 internal, 617 local.
FAILED: Build did NOT complete successfully
```
",NeilPandya,2024-11-09 12:24:00+00:00,['Venkat6871'],2025-01-19 18:26:25+00:00,,https://github.com/tensorflow/tensorflow/issues/79756,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2466196190, 'issue_id': 2645989294, 'author': 'NeilPandya', 'body': 'My host specifications:\r\n```bash\r\n        a8888b.           Host        -  neil@FractalNorth                       \r\n       d888888b.          Machine     -  ASUS System Product Name Version        \r\n       8P""YP""Y88          Kernel      -  6.8.0-48-generic                        \r\n       8|o||o|88          Distro      -  Ubuntu 24.04.1 LTS (Noble Numbat)       \r\n       8\'    .88          DE          -  KDE                                     \r\n       8`._.\' Y8.         WM          -  KWin (X11)                              \r\n      d/      `8b.        Packages    -  2463 (dpkg), 14 (cargo), 21 (flatpak)   \r\n     dP        Y8b.       Shell       -  bash                                    \r\n    d8:       ::88b.      Terminal    -  konsole                                 \r\n   d8""         \'Y88b      Resolution  -  1920x1080, 1920x1080                    \r\n  :8P           :888      Uptime      -  5h 10m                                  \r\n   8a.         _a88P      CPU         -  AMD Ryzen 9 5950X 16-Core Processor (32)\r\n ._/""Yaa     .| 88P|      CPU Load    -  1%                                      \r\n \\    YP""    `|     `.    Memory      -  9.3 GB / 32.8 GB                        \r\n /     \\.___.d|    .\'     GPU         -  GA102 [GeForce RTX 3090]                \r\n `--..__)     `._.\'', 'created_at': datetime.datetime(2024, 11, 9, 12, 27, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466756708, 'issue_id': 2645989294, 'author': 'NeilPandya', 'body': 'Built `tf2.18` successfully with a container from the [`tensorflow/build:2.17-python3.9`](https://hub.docker.com/layers/tensorflow/build/2.17-python3.9/images/sha256-2e37aac354d0c76a0384df89e55a1cfd0a29ca993bb000bf75d516eb2d763d4b?context=explore) image from DockerHub. I remember that image had the corresponding required `Clang` version installed. The `Clang` version in the `tensorflow/build:2.18-python3.9` image needs to be rolled back.\r\n```bash\r\ntf-docker /mnt > ./configure\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.9/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.5.1\r\n\r\n\r\nPlease specify the hermetic cuDNN version you want to use or leave empty to use the default version. 9.3.0\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6\r\n\r\n\r\nPlease specify the local CUDA path you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify the local CUDNN path you want to use or leave empty to use the default version.\r\n\r\n\r\nPlease specify the local NCCL path you want to use or leave empty to use the default version.\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [Y/n]: y\r\nClang will be used as CUDA compiler.\r\n\r\nPlease specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]:\r\n\r\n\r\nYou have Clang 17.0.6 installed.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v1          \t# Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\ntf-docker /mnt > bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Reading \'startup\' options from /mnt/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited \'common\' options: --isatty=1 --terminal_columns=173\r\nINFO: Reading rc options for \'build\' from /mnt/.bazelrc:\r\n  Inherited \'common\' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for \'build\' from /etc/bazel.bazelrc:\r\n  \'build\' options: --action_env=DOCKER_CACHEBUSTER=1717287186963126151 --host_action_env=DOCKER_HOST_CACHEBUSTER=1717287187020135198\r\nINFO: Reading rc options for \'build\' from /mnt/.bazelrc:\r\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for \'build\' from /mnt/.tf_configure.bazelrc:\r\n  \'build\' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /mnt/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /mnt/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm\r\nINFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm\r\nINFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true\r\nINFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6\r\nINFO: Found applicable config definition build:cuda_wheel in file /mnt/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false\r\nINFO: Found applicable config definition build:opt in file /mnt/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native\r\nINFO: Found applicable config definition build:linux in file /mnt/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /mnt/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:96:14:\r\nHERMETIC_PYTHON_VERSION variable was not set correctly, using default version.\r\nPython 3.9 will be used.\r\nTo select Python version, either set HERMETIC_PYTHON_VERSION env variable in\r\nyour shell:\r\n  export HERMETIC_PYTHON_VERSION=3.12\r\nOR pass it as an argument to bazel command directly or inside your .bazelrc\r\nfile:\r\n  --repo_env=HERMETIC_PYTHON_VERSION=3.12\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.9\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl\r\nDEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz\r\nINFO: Analyzed target //tensorflow/tools/pip_package:wheel (759 packages loaded, 56246 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:wheel up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/wheel_house\r\nINFO: Elapsed time: 4493.264s, Critical Path: 339.64s\r\nINFO: 32817 processes: 5867 internal, 26950 local.\r\nINFO: Build completed successfully, 32817 total actions\r\n```', 'created_at': datetime.datetime(2024, 11, 10, 14, 26, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466763263, 'issue_id': 2645989294, 'author': 'NeilPandya', 'body': ""When attempting to install the wheel within the container, I got the following error:\r\n```bash\r\ntf-docker /mnt > pip install bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow*.whl\r\nProcessing ./bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.18.0-cp39-cp39-linux_x86_64.whl\r\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.0.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.6.3)\r\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (24.3.25)\r\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (0.4.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (0.2.0)\r\nCollecting libclang>=13.0.0 (from tensorflow==2.18.0)\r\n  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (3.3.0)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (23.2)\r\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (3.20.3)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (2.32.3)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (70.0.0)\r\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.18.0) (1.16.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (2.1.1)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (4.8.0)\r\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.14.1)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.59.3)\r\nCollecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\r\n  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting keras>=3.5.0 (from tensorflow==2.18.0)\r\n  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting numpy<2.1.0,>=1.26.0 (from tensorflow==2.18.0)\r\n  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n      60.9/60.9 kB 2.6 MB/s eta 0:00:00\r\nCollecting h5py>=3.11.0 (from tensorflow==2.18.0)\r\n  Downloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\nCollecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0)\r\n  Downloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\nCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.18.0)\r\n  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.41.3)\r\nCollecting rich (from keras>=3.5.0->tensorflow==2.18.0)\r\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\r\nCollecting namex (from keras>=3.5.0->tensorflow==2.18.0)\r\n  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\r\nCollecting optree (from keras>=3.5.0->tensorflow==2.18.0)\r\n  Downloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\r\n      47.8/47.8 kB 864.9 kB/s eta 0:00:00\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.2.1)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2024.2.2)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.6)\r\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\r\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.3)\r\nRequirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (4.6.4)\r\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (2.1.5)\r\nCollecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.18.0)\r\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\r\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0)\r\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nDownloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n    5.4/5.4 MB 2.4 MB/s eta 0:00:00\r\nDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\r\n    1.2/1.2 MB 2.9 MB/s eta 0:00:00\r\nDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\r\n    24.5/24.5 MB 2.7 MB/s eta 0:00:00\r\nDownloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n    2.2/2.2 MB 3.3 MB/s eta 0:00:00\r\nUsing cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\r\nUsing cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\r\nDownloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\r\n    5.1/5.1 MB 2.7 MB/s eta 0:00:00\r\nUsing cached namex-0.0.8-py3-none-any.whl (5.8 kB)\r\nDownloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\r\n    358.8/358.8 kB 3.3 MB/s eta 0:00:00\r\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\r\n    242.4/242.4 kB 2.2 MB/s eta 0:00:00\r\nUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nInstalling collected packages: namex, libclang, tensorflow-io-gcs-filesystem, optree, numpy, mdurl, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.22.4\r\n    Uninstalling numpy-1.22.4:\r\n      Successfully uninstalled numpy-1.22.4\r\n  Attempting uninstall: ml-dtypes\r\n    Found existing installation: ml-dtypes 0.3.2\r\n    Uninstalling ml-dtypes-0.3.2:\r\n      Successfully uninstalled ml-dtypes-0.3.2\r\n  Attempting uninstall: h5py\r\n    Found existing installation: h5py 3.10.0\r\n    Uninstalling h5py-3.10.0:\r\n      Successfully uninstalled h5py-3.10.0\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 2.0.2 which is incompatible.\r\nSuccessfully installed h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 optree-0.13.0 rich-13.9.4 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1\r\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.3.1\r\n[notice] To update, run: python3 -m pip install --upgrade pip\r\n```"", 'created_at': datetime.datetime(2024, 11, 10, 14, 46, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467135098, 'issue_id': 2645989294, 'author': 'fuhailin', 'body': ""I know add `--copt=-Wno-gnu-offsetof-extensions` compile option can ignore this error, but I don't know why this option not add in the document."", 'created_at': datetime.datetime(2024, 11, 11, 2, 58, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467621449, 'issue_id': 2645989294, 'author': 'NeilPandya', 'body': '> I know add `--copt=-Wno-gnu-offsetof-extensions` compile option can ignore this error, but I don\'t know why this option not add in the document.\r\n\r\n@fuhailin, thank you for the tip!\r\n\r\nA question: should I be adding that compilation flag in the configuration options like this...\r\n```bash\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-gnu-offsetof-extensions -march=native\r\n```\r\n...or in my `bazel build` command, after running `./configure`, like this?\r\n```bash\r\nbazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt --copt=-Wno-gnu-offsetof-extensions\r\n```\r\nDoes it matter?', 'created_at': datetime.datetime(2024, 11, 11, 9, 15, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482307398, 'issue_id': 2645989294, 'author': 'fuhailin', 'body': '> > I know add `--copt=-Wno-gnu-offsetof-extensions` compile option can ignore this error, but I don\'t know why this option not add in the document.\r\n> \r\n> @fuhailin, thank you for the tip!\r\n> \r\n> A question: should I be adding that compilation flag in the configuration options like this...\r\n> \r\n> ```shell\r\n> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-gnu-offsetof-extensions -march=native\r\n> ```\r\n> \r\n> ...or in my `bazel build` command, after running `./configure`, like this?\r\n> \r\n> ```shell\r\n> bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt --copt=-Wno-gnu-offsetof-extensions\r\n> ```\r\n> \r\n> Does it matter?\r\n\r\nI think here is no difference with your two options, but I just try the second method, and it works for me :)', 'created_at': datetime.datetime(2024, 11, 18, 8, 50, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600971094, 'issue_id': 2645989294, 'author': 'Reyadeyat', 'body': 'Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-sign-compare -march=native -mavx2 -mavx512f -mfma\n#To build tensorflow CPU package: \nbazelisk build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --local_ram_resources=2048 --config=monolithic --verbose_failures --config=opt --copt=-Wno-gnu-offsetof-extensions --copt=-march=native --copt=-mavx2 --copt=-mavx512f --copt=-mfma', 'created_at': datetime.datetime(2025, 1, 19, 18, 26, 24, tzinfo=datetime.timezone.utc)}]","NeilPandya (Issue Creator) on (2024-11-09 12:27:02 UTC): My host specifications:
```bash
        a8888b.           Host        -  neil@FractalNorth                       
       d888888b.          Machine     -  ASUS System Product Name Version        
       8P""YP""Y88          Kernel      -  6.8.0-48-generic                        
       8|o||o|88          Distro      -  Ubuntu 24.04.1 LTS (Noble Numbat)       
       8'    .88          DE          -  KDE                                     
       8`._.' Y8.         WM          -  KWin (X11)                              
      d/      `8b.        Packages    -  2463 (dpkg), 14 (cargo), 21 (flatpak)   
     dP        Y8b.       Shell       -  bash                                    
    d8:       ::88b.      Terminal    -  konsole                                 
   d8""         'Y88b      Resolution  -  1920x1080, 1920x1080                    
  :8P           :888      Uptime      -  5h 10m                                  
   8a.         _a88P      CPU         -  AMD Ryzen 9 5950X 16-Core Processor (32)
 ._/""Yaa     .| 88P|      CPU Load    -  1%                                      
 \    YP""    `|     `.    Memory      -  9.3 GB / 32.8 GB                        
 /     \.___.d|    .'     GPU         -  GA102 [GeForce RTX 3090]                
 `--..__)     `._.'

NeilPandya (Issue Creator) on (2024-11-10 14:26:07 UTC): Built `tf2.18` successfully with a container from the [`tensorflow/build:2.17-python3.9`](https://hub.docker.com/layers/tensorflow/build/2.17-python3.9/images/sha256-2e37aac354d0c76a0384df89e55a1cfd0a29ca993bb000bf75d516eb2d763d4b?context=explore) image from DockerHub. I remember that image had the corresponding required `Clang` version installed. The `Clang` version in the `tensorflow/build:2.18-python3.9` image needs to be rolled back.
```bash
tf-docker /mnt > ./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.9/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.5.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 9.3.0


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]:


You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
tf-docker /mnt > bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=173
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1717287186963126151 --host_action_env=DOCKER_HOST_CACHEBUSTER=1717287187020135198
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /mnt/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /mnt/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:opt in file /mnt/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native
INFO: Found applicable config definition build:linux in file /mnt/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:96:14:
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.9 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.9
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (759 packages loaded, 56246 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:wheel up-to-date:
  bazel-bin/tensorflow/tools/pip_package/wheel_house
INFO: Elapsed time: 4493.264s, Critical Path: 339.64s
INFO: 32817 processes: 5867 internal, 26950 local.
INFO: Build completed successfully, 32817 total actions
```

NeilPandya (Issue Creator) on (2024-11-10 14:46:07 UTC): When attempting to install the wheel within the container, I got the following error:
```bash
tf-docker /mnt > pip install bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow*.whl
Processing ./bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.18.0-cp39-cp39-linux_x86_64.whl
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.0.0)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (0.4.0)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (0.2.0)
Collecting libclang>=13.0.0 (from tensorflow==2.18.0)
  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (3.3.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (23.2)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (3.20.3)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (2.32.3)
Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (70.0.0)
Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.18.0) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (2.1.1)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (4.8.0)
Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.14.1)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.18.0) (1.59.3)
Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)
  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)
Collecting keras>=3.5.0 (from tensorflow==2.18.0)
  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)
Collecting numpy<2.1.0,>=1.26.0 (from tensorflow==2.18.0)
  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
      60.9/60.9 kB 2.6 MB/s eta 0:00:00
Collecting h5py>=3.11.0 (from tensorflow==2.18.0)
  Downloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0)
  Downloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.18.0)
  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.41.3)
Collecting rich (from keras>=3.5.0->tensorflow==2.18.0)
  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)
Collecting namex (from keras>=3.5.0->tensorflow==2.18.0)
  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)
Collecting optree (from keras>=3.5.0->tensorflow==2.18.0)
  Downloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)
      47.8/47.8 kB 864.9 kB/s eta 0:00:00
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2024.2.2)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.6)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.3)
Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (4.6.4)
Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (2.1.5)
Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.18.0)
  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0)
  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Downloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)
    5.4/5.4 MB 2.4 MB/s eta 0:00:00
Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)
    1.2/1.2 MB 2.9 MB/s eta 0:00:00
Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)
    24.5/24.5 MB 2.7 MB/s eta 0:00:00
Downloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)
    2.2/2.2 MB 3.3 MB/s eta 0:00:00
Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)
Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)
Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)
    5.1/5.1 MB 2.7 MB/s eta 0:00:00
Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)
Downloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)
    358.8/358.8 kB 3.3 MB/s eta 0:00:00
Downloading rich-13.9.4-py3-none-any.whl (242 kB)
    242.4/242.4 kB 2.2 MB/s eta 0:00:00
Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, optree, numpy, mdurl, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow
  Attempting uninstall: numpy
    Found existing installation: numpy 1.22.4
    Uninstalling numpy-1.22.4:
      Successfully uninstalled numpy-1.22.4
  Attempting uninstall: ml-dtypes
    Found existing installation: ml-dtypes 0.3.2
    Uninstalling ml-dtypes-0.3.2:
      Successfully uninstalled ml-dtypes-0.3.2
  Attempting uninstall: h5py
    Found existing installation: h5py 3.10.0
    Uninstalling h5py-3.10.0:
      Successfully uninstalled h5py-3.10.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 2.0.2 which is incompatible.
Successfully installed h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 optree-0.13.0 rich-13.9.4 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

[notice] A new release of pip is available: 24.0 -> 24.3.1
[notice] To update, run: python3 -m pip install --upgrade pip
```

fuhailin on (2024-11-11 02:58:05 UTC): I know add `--copt=-Wno-gnu-offsetof-extensions` compile option can ignore this error, but I don't know why this option not add in the document.

NeilPandya (Issue Creator) on (2024-11-11 09:15:06 UTC): @fuhailin, thank you for the tip!

A question: should I be adding that compilation flag in the configuration options like this...
```bash
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-gnu-offsetof-extensions -march=native
```
...or in my `bazel build` command, after running `./configure`, like this?
```bash
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt --copt=-Wno-gnu-offsetof-extensions
```
Does it matter?

fuhailin on (2024-11-18 08:50:17 UTC): I think here is no difference with your two options, but I just try the second method, and it works for me :)

Reyadeyat on (2025-01-19 18:26:24 UTC): Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-sign-compare -march=native -mavx2 -mavx512f -mfma
#To build tensorflow CPU package: 
bazelisk build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --local_ram_resources=2048 --config=monolithic --verbose_failures --config=opt --copt=-Wno-gnu-offsetof-extensions --copt=-march=native --copt=-mavx2 --copt=-mavx512f --copt=-mfma

"
2645889516,issue,closed,completed, ModuleNotFoundError ,"No module named 'tensorflow' when using from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
Environment: Describe your environment, including:

TensorFlow version ( 2.17.0, 2.18.0)
Python version (3.8.10)

ModuleNotFoundError: No module named 'tensorflow'
Expected Behavior: Should've imported the KerasClassifier from tensorflow.keras.wrappers.scikit_learn",Harsha7226,2024-11-09 10:16:53+00:00,['tilakrayal'],2024-11-26 02:05:49+00:00,2024-11-26 02:05:45+00:00,https://github.com/tensorflow/tensorflow/issues/79750,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2467524138, 'issue_id': 2645889516, 'author': 'tilakrayal', 'body': '@Harsha7226,\r\nCould please try to install scikeras using **pip install scikeras** and then import **from scikeras.wrappers import KerasClassifier**.  Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4097395867f620c2b959411a613a413c/untitled2224.ipynb). Thank you!', 'created_at': datetime.datetime(2024, 11, 11, 8, 28, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553282, 'issue_id': 2645889516, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499452735, 'issue_id': 2645889516, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499453154, 'issue_id': 2645889516, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79750"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79750"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-11 08:28:53 UTC): @Harsha7226,
Could please try to install scikeras using **pip install scikeras** and then import **from scikeras.wrappers import KerasClassifier**.  Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4097395867f620c2b959411a613a413c/untitled2224.ipynb). Thank you!

github-actions[bot] on (2024-11-19 02:05:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-26 02:05:45 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-26 02:05:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79750"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79750"">No</a>

"
2645784206,issue,closed,completed,Error when inferencing on a tflite model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am facing this error when trying to test inference of a tflite model on an image. Could you please help take a look and give your appropriate support?
Error invoking model: output size must be non-negativeNode number 14 (TfLiteFlexDelegate) failed to invoke.Node number 7 (WHILE) failed to invoke.Node number 590 (WHILE) failed to invoke.

### Standalone code to reproduce the issue

```shell
My pipeline converting: Pytorch model -> ONNX -> TF -> TF lite
My script to test the .tflite model:
import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Function to preprocess the image (resizing, normalization, etc.)
def preprocess_image(image_path, target_size=(320, 320)):
    # Load the image and resize
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, target_size)
    
    # Normalize the image: you can adjust this step depending on your model's requirements
    image = image / 255.0
    
    # Ensure the shape is [height, width, channels]
    return image

# Post-process the YOLOv5 Lite output
def postprocess_yolov5lite_output(output):
    # Assuming output shape is (num_boxes, 6) [x1, y1, x2, y2, score, class_id]
    boxes = output[:, :4]  # First 4 are the bounding box coordinates
    scores = output[:, 4]  # 5th is the score
    class_ids = output[:, 5].astype(int)  # 6th is the class id

    # Apply some thresholding to remove low-confidence boxes (if needed)
    confidence_threshold = 0.5
    mask = scores > confidence_threshold
    boxes = boxes[mask]
    scores = scores[mask]
    class_ids = class_ids[mask]

    return {'boxes': boxes, 'scores': scores, 'class_ids': class_ids}

# Function to run the TensorFlow Lite model without using delegates
def run_tflite_model_no_delegate(tflite_file, test_image):
    # Initialize the interpreter without using delegates
    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))
    
    # Allocate tensors
    interpreter.allocate_tensors()
    
    # Get input and output details
    input_details = interpreter.get_input_details()[0]
    output_details = interpreter.get_output_details()[0]

    # Check if input is quantized (for uint8 input), rescale to uint8 if needed
    if input_details['dtype'] == np.uint8:
        input_scale, input_zero_point = input_details[""quantization""]
        test_image = test_image / input_scale + input_zero_point

    # Add the batch dimension
    test_image = np.expand_dims(test_image, axis=0).astype(input_details[""dtype""])  # Shape becomes [1, 320, 320, 3]
    
    # Permute the image to [1, 3, 320, 320] if needed
    test_image = np.transpose(test_image, (0, 3, 1, 2))

    # Set the input tensor and invoke the interpreter
    interpreter.set_tensor(input_details[""index""], test_image)
    
    try:
        interpreter.invoke()
    except Exception as e:
        print(f""Error invoking model: {e}"")
        return None

    # Get the output (bounding boxes, scores, and class IDs)
    output = interpreter.get_tensor(output_details[""index""])[0]

    # Post-processing step for YOLOv5 Lite output
    predictions = postprocess_yolov5lite_output(output)

    return predictions

# Function to test the model and visualize the output
def test_model_no_delegate(tflite_file, test_image, model_type):
    # Run the model without delegates
    predictions = run_tflite_model_no_delegate(tflite_file, test_image)

    if predictions is None:
        print(""Model inference failed."")
        return

    # Plot the image with predictions
    plt.imshow(test_image[0])  # test_image has batch dimension, so use test_image[0]
    ax = plt.gca()

    predicted_boxes = predictions['boxes']
    predicted_scores = predictions['scores']
    predicted_class_ids = predictions['class_ids']

    for i in range(len(predicted_boxes)):
        box = predicted_boxes[i]
        score = predicted_scores[i]
        class_id = predicted_class_ids[i]

        rect = patches.Rectangle(
            (box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='r', facecolor='none'
        )
        ax.add_patch(rect)

        label = f""Class: {class_id} Score: {score:.2f}""
        plt.text(box[0], box[1], label, color='red', fontsize=10, verticalalignment='top', horizontalalignment='left')

    plt.title(f""{model_type} Model"")
    plt.grid(False)
    plt.show()

# Load and preprocess the image
test_image_path = '/home/ngerr/Workspace/Thesis/YOLOv5-Lite/cpp_demo/ort/images/000000001000.jpg'
test_image = preprocess_image(test_image_path)

# Set the TensorFlow Lite model path
tflite_model_path = ""/home/ngerr/Workspace/Thesis/YOLOv5-Lite/model_zoo/tflite_model/v5lite_quant_model.tflite""

# Testing the model without delegates
test_model_no_delegate(tflite_model_path, test_image, model_type=""Quantized"")
```


### Relevant log output

_No response_",trieu1162000,2024-11-09 08:04:07+00:00,['gaikwadrahul8'],2024-11-27 02:06:58+00:00,2024-11-27 02:06:54+00:00,https://github.com/tensorflow/tensorflow/issues/79736,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2470689693, 'issue_id': 2645784206, 'author': 'gaikwadrahul8', 'body': ""Hi, @trieu1162000 \r\n\r\nThank you for bringing this issue to our attention, if possible could you please help us with your model and image to replicate the similar behavior from our end ? If you're okay could you please give a try with AI Edge Torch package to convert a PyTorch model to the LiteRT format because there is no need to go with this conversion process **Pytorch model -> ONNX -> TF -> TF lite** please refer this [official documentation ](https://ai.google.dev/edge/litert/models/pytorch_to_tflite)\r\n\r\nThank you for your cooperation and understanding"", 'created_at': datetime.datetime(2024, 11, 12, 14, 30, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487166484, 'issue_id': 2645784206, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 20, 2, 4, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503039, 'issue_id': 2645784206, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503087, 'issue_id': 2645784206, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79736"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79736"">No</a>', 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 56, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-12 14:30:10 UTC): Hi, @trieu1162000 

Thank you for bringing this issue to our attention, if possible could you please help us with your model and image to replicate the similar behavior from our end ? If you're okay could you please give a try with AI Edge Torch package to convert a PyTorch model to the LiteRT format because there is no need to go with this conversion process **Pytorch model -> ONNX -> TF -> TF lite** please refer this [official documentation ](https://ai.google.dev/edge/litert/models/pytorch_to_tflite)

Thank you for your cooperation and understanding

github-actions[bot] on (2024-11-20 02:04:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-27 02:06:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-27 02:06:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79736"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79736"">No</a>

"
2645405006,issue,closed,completed,Support custom cell/RNN layers with extension types,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to write a Keras-like model with [keras.layers.RNN](https://www.tensorflow.org/guide/keras/working_with_rnns#built-in_rnn_layers_a_simple_example) that supports [Extension types](https://www.tensorflow.org/guide/extension_type), both for inputs and states. 

### Standalone code to reproduce the issue

```shell
import keras
import tensorflow as tf


class MaskedTensor(tf.experimental.ExtensionType):
    """"""A tensor paired with a boolean mask, indicating which values are valid.""""""

    values: tf.Tensor
    mask: tf.Tensor
    shape: tf.TensorShape
    dtype: tf.DType

    def __init__(self, values, mask):
        self.values = values
        self.mask = mask
        self.shape = values.shape
        self.dtype = values.dtype


@tf.experimental.dispatch_for_api(tf.compat.v1.transpose)
def transpose(a: MaskedTensor, perm=None, name=""transpose"", conjugate=False):
    values = tf.transpose(a.values, perm, conjugate, name)
    mask = tf.transpose(a.mask, perm, conjugate, name)
    return MaskedTensor(values, mask)


@tf.experimental.dispatch_for_api(tf.shape)
def shape(input: MaskedTensor, out_type=None, name=None):
    return tf.shape(input.values, out_type, name)


@tf.experimental.dispatch_for_api(tf.unstack)
def unstack(value: MaskedTensor, num=None, axis=0, name=""unstack""):
    values = tf.unstack(value.values, num, axis, name)
    mask = tf.unstack(value.mask, num, axis, name)
    return [MaskedTensor(x, m) for x, m in zip(values, mask)]


@keras.saving.register_keras_serializable()
class Cell(tf.keras.layers.Layer):
    @property
    def state_size(self):
        return tf.TensorShape([5])

    def call(self, inputs, states):
        assert isinstance(inputs, MaskedTensor)
        assert isinstance(states, MaskedTensor)
        return inputs, states

    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        return MaskedTensor(tf.zeros((batch_size, 5)), tf.ones((batch_size, 5), tf.bool))


if __name__ == ""__main__"":
    input_spec = MaskedTensor.Spec(
        values=tf.TensorSpec(shape=[2, 10, 5]),
        mask=tf.TensorSpec(shape=[2, 10, 5]),
        shape=[2, 10, 5],
        dtype=tf.float32,
    )
    x = tf.keras.layers.Input(type_spec=input_spec)
    y = tf.keras.layers.RNN(Cell(), return_sequences=True, stateful=True, unroll=True)(x)
    model = tf.keras.models.Model(x, y)
    model.summary()
```


### Relevant log output

```shell
File ""D:\projects\testing\proof_of_concept.py"", line 62, in <module>
    y = tf.keras.layers.RNN(Cell(), return_sequences=True, stateful=True, unroll=True)(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\.envs\tensorflow\Lib\site-packages\keras\src\layers\rnn\base_rnn.py"", line 557, in __call__
    return super().__call__(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\.envs\tensorflow\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""D:\.envs\tensorflow\Lib\site-packages\tensorflow\python\framework\constant_op.py"", line 103, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (MaskedTensor(values=<tf.Tensor: shape=(2, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>, mask=<tf.Tensor: shape=(2, 5), dtype=bool, numpy=
array([[ True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True]])>, shape=TensorShape([2, 5]), dtype=tf.float32)) with an unsupported type (<class '__main__.MaskedTensor'>) to a Tensor.
```
",Johansmm,2024-11-09 00:03:31+00:00,['tilakrayal'],2024-11-15 09:31:00+00:00,2024-11-15 09:30:59+00:00,https://github.com/tensorflow/tensorflow/issues/79709,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2467499331, 'issue_id': 2645405006, 'author': 'tilakrayal', 'body': '@Johansmm,\r\nLooks like this issue is more related to Keras. Could you please raise the issue in Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 11, 11, 8, 16, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469984797, 'issue_id': 2645405006, 'author': 'Johansmm', 'body': '@tilakrayal thanks for the response. I created an issue in https://github.com/keras-team/keras/issues/20485', 'created_at': datetime.datetime(2024, 11, 12, 9, 14, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477946010, 'issue_id': 2645405006, 'author': 'tilakrayal', 'body': '@Johansmm,\r\nCould you please try to close this issue, since it has been tracking in the Keras repo. Thank you!', 'created_at': datetime.datetime(2024, 11, 15, 4, 37, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478346714, 'issue_id': 2645405006, 'author': 'Johansmm', 'body': 'Closing the issue :)', 'created_at': datetime.datetime(2024, 11, 15, 9, 31, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-11 08:16:31 UTC): @Johansmm,
Looks like this issue is more related to Keras. Could you please raise the issue in Keras-team/keras repo for the quick resolution. Thank you!

Johansmm (Issue Creator) on (2024-11-12 09:14:30 UTC): @tilakrayal thanks for the response. I created an issue in https://github.com/keras-team/keras/issues/20485

tilakrayal (Assginee) on (2024-11-15 04:37:59 UTC): @Johansmm,
Could you please try to close this issue, since it has been tracking in the Keras repo. Thank you!

Johansmm (Issue Creator) on (2024-11-15 09:31:00 UTC): Closing the issue :)

"
2644304363,issue,closed,completed,error while converting to tflite,"Hello,
I'm trying to convert a tensorflow ""saved model"" to tflite format on a W11 PC with Python 3.12 and  tf v2.18.0.
The model I'm working with is this one:
https://github.com/tonytw1/squirrel-detector/tree/main/models/squirrelnet_ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model

I just get a barrage of incomprehensible error messages (after about 30s), see below.

Thanks for any help.

`**cvt = tf.lite.TFLiteConverter.from_saved_model('model_directory')
lm = cvt.convert()**

Traceback (most recent call last):
  File ""<pyshell#11>"", line 1, in <module>
    lm = cvt.convert()
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1238, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1190, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1572, in convert
    return self._convert_from_saved_model(graph_def)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1430, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert.py"", line 1045, in convert_saved_model
    data = convert(
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert.py"", line 376, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): 'tf.StridedSlice' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): called from
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): see current operation: %28 = ""tf.StridedSlice""(%27, %14, %14, %13) <{begin_mask = 14 : i64, ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}> {device = """"} : (tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x?x?x3xf32>
<unknown>:0: note: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): failed while converting: 'map/while_body': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: StridedSlice
Details:
	tf.StridedSlice(tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<1x?x?x3xf32>) : {begin_mask = 14 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}

<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): see current operation: 
""func.func""() <{function_type = (tensor<i32>, tensor<i32>, tensor<?x320x320x3xf32>, tensor<?x3xi32>, tensor<1x?x?x3xui8>) -> (tensor<i32>, tensor<i32>, tensor<?x320x320x3xf32>, tensor<?x3xi32>, tensor<1x?x?x3xui8>), sym_name = ""map/while_body"", sym_visibility = ""private""}> ({
^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<?x320x320x3xf32>, %arg3: tensor<?x3xi32>, %arg4: tensor<1x?x?x3xui8>):
  %0 = ""arith.constant""() <{value = dense<-1> : tensor<3xi32>}> : () -> tensor<3xi32>
  %1 = ""arith.constant""() <{value = dense<0> : tensor<i32>}> : () -> tensor<i32>
  %2 = ""arith.constant""() <{value = dense<1.000000e+00> : tensor<f32>}> : () -> tensor<f32>
  %3 = ""arith.constant""() <{value = dense<0> : tensor<1xi32>}> : () -> tensor<1xi32>
  %4 = ""arith.constant""() <{value = dense<0> : tensor<2xi32>}> : () -> tensor<2xi32>
  %5 = ""arith.constant""() <{value = dense<-1> : tensor<1xi32>}> : () -> tensor<1xi32>
  %6 = ""arith.constant""() <{value = dense<1> : tensor<1xi32>}> : () -> tensor<1xi32>
  %7 = ""arith.constant""() <{value = dense<1> : tensor<i32>}> : () -> tensor<i32>
  %8 = ""arith.constant""() <{value = dense<-1> : tensor<4xi32>}> : () -> tensor<4xi32>
  %9 = ""arith.constant""() <{value = dense<0> : tensor<3xi32>}> : () -> tensor<3xi32>
  %10 = ""arith.constant""() <{value = dense<[[320, 320, 3]]> : tensor<1x3xi32>}> : () -> tensor<1x3xi32>
  %11 = ""arith.constant""() <{value = dense<-1> : tensor<2xi32>}> : () -> tensor<2xi32>
  %12 = ""arith.constant""() <{value = dense<[1, 320, 320, 3]> : tensor<4xi32>}> : () -> tensor<4xi32>
  %13 = ""arith.constant""() <{value = dense<1> : tensor<4xi32>}> : () -> tensor<4xi32>
  %14 = ""arith.constant""() <{value = dense<0> : tensor<4xi32>}> : () -> tensor<4xi32>
  %15 = ""arith.constant""() <{value = dense<0.00784313772> : tensor<f32>}> : () -> tensor<f32>
  %16 = ""arith.constant""() <{value = dense<320> : tensor<2xi32>}> : () -> tensor<2xi32>
  %17 = ""tfl.add""(%arg0, %7) <{fused_activation_function = ""NONE""}> : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %18 = ""tfl.add""(%arg1, %7) <{fused_activation_function = ""NONE""}> : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %19 = ""tfl.reshape""(%arg1, %6) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %20 = ""tfl.concatenation""(%19, %5) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %21 = ""tfl.slice""(%arg3, %4, %20) : (tensor<?x3xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xi32>
  %22 = ""tfl.reshape""(%18, %6) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %23 = ""tfl.concatenation""(%22, %3) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %24 = ""tfl.slice""(%arg3, %23, %11) : (tensor<?x3xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x3xi32>
  %25 = ""tfl.concatenation""(%21, %10, %24) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<?x?xi32>, tensor<1x3xi32>, tensor<?x3xi32>) -> tensor<?x3xi32>
  %26 = ""tfl.gather""(%arg4, %arg1) <{axis = 0 : i32, batch_dims = 0 : i32}> : (tensor<1x?x?x3xui8>, tensor<i32>) -> tensor<?x?x3xui8>
  %27 = ""tfl.cast""(%26) : (tensor<?x?x3xui8>) -> tensor<?x?x3xf32>
  %28 = ""tf.StridedSlice""(%27, %14, %14, %13) <{begin_mask = 14 : i64, ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}> {device = """"} : (tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x?x?x3xf32>
  %29 = ""tfl.mul""(%28, %15) <{fused_activation_function = ""NONE""}> : (tensor<1x?x?x3xf32>, tensor<f32>) -> tensor<1x?x?x3xf32>
  %30 = ""tfl.sub""(%29, %2) <{fused_activation_function = ""NONE""}> : (tensor<1x?x?x3xf32>, tensor<f32>) -> tensor<1x?x?x3xf32>
  %31 = ""tfl.unpack""(%30) <{axis = 0 : i32, num = 1 : i32}> : (tensor<1x?x?x3xf32>) -> tensor<?x?x3xf32>
  %32 = ""tfl.expand_dims""(%31, %1) : (tensor<?x?x3xf32>, tensor<i32>) -> tensor<1x?x?x3xf32>
  %33 = ""tfl.resize_bilinear""(%32, %16) <{align_corners = false, half_pixel_centers = false}> : (tensor<1x?x?x3xf32>, tensor<2xi32>) -> tensor<1x320x320x3xf32>
  %34 = ""tfl.strided_slice""(%33, %14, %12, %13) <{begin_mask = 14 : i32, ellipsis_mask = 0 : i32, end_mask = 14 : i32, new_axis_mask = 0 : i32, offset = false, shrink_axis_mask = 1 : i32}> : (tensor<1x320x320x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<320x320x3xf32>
  %35 = ""tfl.concatenation""(%19, %0) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<1xi32>, tensor<3xi32>) -> tensor<4xi32>
  %36 = ""tfl.slice""(%arg2, %14, %35) : (tensor<?x320x320x3xf32>, tensor<4xi32>, tensor<4xi32>) -> tensor<?x?x?x?xf32>
  %37 = ""tfl.concatenation""(%22, %9) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<1xi32>, tensor<3xi32>) -> tensor<4xi32>
  %38 = ""tfl.slice""(%arg2, %37, %8) : (tensor<?x320x320x3xf32>, tensor<4xi32>, tensor<4xi32>) -> tensor<?x320x320x3xf32>
  %39 = ""tfl.reshape""(%34, %12) : (tensor<320x320x3xf32>, tensor<4xi32>) -> tensor<1x320x320x3xf32>
  %40 = ""tfl.concatenation""(%36, %39, %38) <{axis = 0 : i32, fused_activation_function = ""NONE""}> : (tensor<?x?x?x?xf32>, tensor<1x320x320x3xf32>, tensor<?x320x320x3xf32>) -> tensor<?x320x320x3xf32>
  ""func.return""(%17, %18, %40, %25, %arg4) : (tensor<i32>, tensor<i32>, tensor<?x320x320x3xf32>, tensor<?x3xi32>, tensor<1x?x?x3xui8>) -> ()
}) : () -> ()
`",mcl-uk,2024-11-08 15:05:03+00:00,['gaikwadrahul8'],2024-11-12 15:15:54+00:00,2024-11-12 15:15:51+00:00,https://github.com/tensorflow/tensorflow/issues/79695,"[('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.18', '')]","[{'comment_id': 2470188989, 'issue_id': 2644304363, 'author': 'gaikwadrahul8', 'body': 'Hi, @mcl-uk \r\n\r\nThank you for bringing this issue to our attention, I am able to replicate the same behavior from my end, Here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/dde5d27324d524bd7053340ef60d5429/tflite-issue-79695.ipynb) for reference\r\n\r\n**Here is output error log for reference :**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n[<ipython-input-6-3c97b26abbb0>](https://localhost:8080/#) in <cell line: 2>()\r\n      1 cvt = tf.lite.TFLiteConverter.from_saved_model(MODEL_PATH)\r\n----> 2 lm = cvt.convert()\r\n\r\n7 frames\r\n[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\r\n    374               enable_mlir_converter,\r\n    375           )\r\n--> 376       raise converter_error\r\n    377 \r\n    378   return _run_deprecated_conversion_binary(\r\n\r\nConverterError: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): \'tf.StridedSlice\' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): called from\r\n<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from\r\n<unknown>:0: note: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): failed while converting: \'map/while_body\': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: StridedSlice\r\nDetails:\r\n\ttf.StridedSlice(tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<1x?x?x3xf32>) : {begin_mask = 14 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}\r\n\r\n<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from\r\n```\r\nTo fix this you need to tell the converter to include TensorFlow ops or Select TF Ops. This can be achieved by setting the target_spec.supported_ops attribute of the converter to include tf.lite.OpsSet.SELECT_TF_OPS along with tf.lite.OpsSet.TFLITE_BUILTINS.\r\n\r\n1. `tf.lite.OpsSet.TFLITE_BUILTINS`: This includes the standard set of TensorFlow Lite operations.\r\n\r\n2. `tf.lite.OpsSet.SELECT_TF_OPS`: This allows the converter to include TensorFlow operations that are not directly supported in TensorFlow Lite and in this case `tf.StridedSlice` Op does not have direct support in TensorFlow Lite ops so to enable the use of `tf.StridedSlice` Op so we need to add `tf.lite.OpsSet.SELECT_TF_OPS` during conversion\r\n\r\nI have tried from my end and it seems like working as expected please refer second section of this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/75d334b77c0c50a4fc848d5b1067c500/tflite-issue-79695.ipynb) where I converted model to TFLite successfully.\r\n\r\nPlease give it try from your end and let me know is it working as expected or not ? if not please help me with error log to investigate this issue further from our end. \r\n\r\nThank you for you cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 12, 10, 42, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470739033, 'issue_id': 2644304363, 'author': 'mcl-uk', 'body': 'Thanks for responding.\r\nTaking on board your suggestions, my convert script now looks like:\r\n`\r\nimport tensorflow as tf\r\ncvt = tf.lite.TFLiteConverter.from_saved_model(\'model_dir\')\r\ncvt.target_spec.supported_ops.add(tf.lite.OpsSet.TFLITE_BUILTINS)\r\ncvt.target_spec.supported_ops.add(tf.lite.OpsSet.SELECT_TF_OPS)\r\nlm = cvt.convert()\r\n`\r\nHowever this again fails, now reporting:\r\n`\r\nTraceback (most recent call last):\r\n  File ""<pyshell#27>"", line 1, in <module>\r\n    lm = cvt.convert()\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1238, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1190, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1572, in convert\r\n    return self._convert_from_saved_model(graph_def)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py"", line 1430, in _convert_from_saved_model\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py"", line 215, in wrapper\r\n    raise error from None  # Re-throws the exception.\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py"", line 205, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py"", line 1044, in convert_saved_model\r\n    conversion_flags = build_conversion_flags(**kwargs)\r\n  File ""C:\\Users\\Steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py"", line 785, in build_conversion_flags\r\n    conversion_flags.select_user_tf_ops.extend(select_user_tf_ops)\r\nTypeError: bad argument type for built-in operation\r\n`\r\nI\'d be most gratefull for any further assistance.', 'created_at': datetime.datetime(2024, 11, 12, 14, 49, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470807944, 'issue_id': 2644304363, 'author': 'mcl-uk', 'body': 'Appologies, I have re-tried the script afresh and can confirm that it _does_ now work.\r\nThanks again for your help.', 'created_at': datetime.datetime(2024, 11, 12, 15, 15, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470808033, 'issue_id': 2644304363, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79695"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79695"">No</a>', 'created_at': datetime.datetime(2024, 11, 12, 15, 15, 53, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-12 10:42:36 UTC): Hi, @mcl-uk 

Thank you for bringing this issue to our attention, I am able to replicate the same behavior from my end, Here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/dde5d27324d524bd7053340ef60d5429/tflite-issue-79695.ipynb) for reference

**Here is output error log for reference :**

```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
[<ipython-input-6-3c97b26abbb0>](https://localhost:8080/#) in <cell line: 2>()
      1 cvt = tf.lite.TFLiteConverter.from_saved_model(MODEL_PATH)
----> 2 lm = cvt.convert()

7 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)
    374               enable_mlir_converter,
    375           )
--> 376       raise converter_error
    377 
    378   return _run_deprecated_conversion_binary(

ConverterError: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): 'tf.StridedSlice' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): called from
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(fused[""StridedSlice:"", ""map/while/strided_slice@map_while_body_8032""] at callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]))): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""StatelessWhile:"", ""map/while@__inference_call_func_12016""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_14389""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): failed while converting: 'map/while_body': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: StridedSlice
Details:
	tf.StridedSlice(tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<1x?x?x3xf32>) : {begin_mask = 14 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}

<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
```
To fix this you need to tell the converter to include TensorFlow ops or Select TF Ops. This can be achieved by setting the target_spec.supported_ops attribute of the converter to include tf.lite.OpsSet.SELECT_TF_OPS along with tf.lite.OpsSet.TFLITE_BUILTINS.

1. `tf.lite.OpsSet.TFLITE_BUILTINS`: This includes the standard set of TensorFlow Lite operations.

2. `tf.lite.OpsSet.SELECT_TF_OPS`: This allows the converter to include TensorFlow operations that are not directly supported in TensorFlow Lite and in this case `tf.StridedSlice` Op does not have direct support in TensorFlow Lite ops so to enable the use of `tf.StridedSlice` Op so we need to add `tf.lite.OpsSet.SELECT_TF_OPS` during conversion

I have tried from my end and it seems like working as expected please refer second section of this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/75d334b77c0c50a4fc848d5b1067c500/tflite-issue-79695.ipynb) where I converted model to TFLite successfully.

Please give it try from your end and let me know is it working as expected or not ? if not please help me with error log to investigate this issue further from our end. 

Thank you for you cooperation and patience.

mcl-uk (Issue Creator) on (2024-11-12 14:49:17 UTC): Thanks for responding.
Taking on board your suggestions, my convert script now looks like:
`
import tensorflow as tf
cvt = tf.lite.TFLiteConverter.from_saved_model('model_dir')
cvt.target_spec.supported_ops.add(tf.lite.OpsSet.TFLITE_BUILTINS)
cvt.target_spec.supported_ops.add(tf.lite.OpsSet.SELECT_TF_OPS)
lm = cvt.convert()
`
However this again fails, now reporting:
`
Traceback (most recent call last):
  File ""<pyshell#27>"", line 1, in <module>
    lm = cvt.convert()
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1238, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1190, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1572, in convert
    return self._convert_from_saved_model(graph_def)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1430, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert.py"", line 1044, in convert_saved_model
    conversion_flags = build_conversion_flags(**kwargs)
  File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\lite\python\convert.py"", line 785, in build_conversion_flags
    conversion_flags.select_user_tf_ops.extend(select_user_tf_ops)
TypeError: bad argument type for built-in operation
`
I'd be most gratefull for any further assistance.

mcl-uk (Issue Creator) on (2024-11-12 15:15:51 UTC): Appologies, I have re-tried the script afresh and can confirm that it _does_ now work.
Thanks again for your help.

google-ml-butler[bot] on (2024-11-12 15:15:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79695"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79695"">No</a>

"
2643775371,issue,open,,tf.cast to int8 produce wrong number,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.15 - 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

input is a long list include random numbers.
tf.cast to int8 output is  different from numpy cast.

The strange thing is if you truncate the long input list to a short list then things works fine.

The code is straight forward, you can reproduce it in the colab

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing
```


### Relevant log output

```shell
NumPy Casted values (int8): 
 [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21
 -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36
  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63
   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54
   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7
  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94
  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15
  -11]
TensorFlow Casted values (int8): 
 [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15
  -11]
```
```
",JonOberry4662,2024-11-08 11:33:35+00:00,['tilakrayal'],2024-11-11 04:36:10+00:00,,https://github.com/tensorflow/tensorflow/issues/79689,"[('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2464606712, 'issue_id': 2643775371, 'author': 'VadisettyRahul', 'body': ""Hi @tilakrayal @JonOberry4662 \r\n\r\nIf it's feasible, I'd like to investigate the issue, it seems to me that there are bugs in the casting operations in tensorflow/core/kernels/.\r\n\r\nI can try to review the logic that defines the casting behavior for int8. You can add additional checks or adjust the way the values \u200b\u200bare handled.\r\n\r\nWhat do you think?"", 'created_at': datetime.datetime(2024, 11, 8, 12, 12, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2464687968, 'issue_id': 2643775371, 'author': 'JonOberry4662', 'body': '@VadisettyRahul \r\n\r\nBtw it works fine on tf2.12\r\n\r\nhttps://colab.research.google.com/drive/1M0EcrsktHufkVZQomN8Z9eHZ20I35nCl?usp=sharing', 'created_at': datetime.datetime(2024, 11, 8, 12, 54, 15, tzinfo=datetime.timezone.utc)}]","VadisettyRahul on (2024-11-08 12:12:04 UTC): Hi @tilakrayal @JonOberry4662 

If it's feasible, I'd like to investigate the issue, it seems to me that there are bugs in the casting operations in tensorflow/core/kernels/.

I can try to review the logic that defines the casting behavior for int8. You can add additional checks or adjust the way the values are handled.

What do you think?

JonOberry4662 (Issue Creator) on (2024-11-08 12:54:15 UTC): @VadisettyRahul 

Btw it works fine on tf2.12

https://colab.research.google.com/drive/1M0EcrsktHufkVZQomN8Z9eHZ20I35nCl?usp=sharing

"
2643245338,issue,closed,completed,"Gradle project sync failed. Basic functionality (e.g. editing, debugging) will not work properly.","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow:tensorflow-lite:2.9.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I just downloaded current example-master zip file. and extract it.
And then I open the file path (tensorflow-lite/examples/style_transfer/android) with android studio.
When I try to click sync button, it keeps saying that as the title.
""Gradle project sync failed. Basic functionality (e.g. editing, debugging) will not work properly.""
<img width=""1440"" alt=""Screenshot 2024-11-08 at 15 11 38"" src=""https://github.com/user-attachments/assets/1835b31c-097c-4eb1-9d1a-e500ab776a1c"">


### Standalone code to reproduce the issue

```shell
This is the error message:
Cannot use @TaskAction annotation on method DataBindingGenBaseClassesTask.writeBaseClasses() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method

Description of error:

Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.
You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.
For more on this, please refer to https://docs.gradle.org/8.4/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.
```


### Relevant log output

_No response_",minncode,2024-11-08 08:14:17+00:00,['gaikwadrahul8'],2024-11-27 02:07:00+00:00,2024-11-27 02:06:56+00:00,https://github.com/tensorflow/tensorflow/issues/79657,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2470775857, 'issue_id': 2643245338, 'author': 'gaikwadrahul8', 'body': ""Hi, @minncode \r\n\r\nI apologize for the delayed response, I'm able to replicate the same behavior from my end for reference I've added output log below so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention\r\n\r\n**Here is output log for reference :**\r\n\r\n![image](https://github.com/user-attachments/assets/3f334043-143e-4c73-91a0-44b30b9b2347)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 12, 15, 3, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470881815, 'issue_id': 2643245338, 'author': 'gaikwadrahul8', 'body': ""Hi, @minncode \r\n\r\nI tried with gradle version `7.6.2` instead of `8.4` so I changed this line `distributionUrl=https\\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle `version 7.6.2` and our relevant team will fix this issue with `gradle version 8.4` soon\r\n\r\n**Here is output screenshot for reference :**\r\n\r\n![image](https://github.com/user-attachments/assets/2528e445-d219-49a2-9f5c-ed12e53ffa7e)\r\n\r\nThank you for your cooperation and understanding."", 'created_at': datetime.datetime(2024, 11, 12, 15, 44, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487166517, 'issue_id': 2643245338, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 20, 2, 4, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503069, 'issue_id': 2643245338, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503153, 'issue_id': 2643245338, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79657"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79657"">No</a>', 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 59, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-12 15:03:29 UTC): Hi, @minncode 

I apologize for the delayed response, I'm able to replicate the same behavior from my end for reference I've added output log below so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention

**Here is output log for reference :**

![image](https://github.com/user-attachments/assets/3f334043-143e-4c73-91a0-44b30b9b2347)

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-11-12 15:44:36 UTC): Hi, @minncode 

I tried with gradle version `7.6.2` instead of `8.4` so I changed this line `distributionUrl=https\://services.gradle.org/distributions/gradle-8.4-bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle-7.6.2-bin.zip` in `gradle-wrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle `version 7.6.2` and our relevant team will fix this issue with `gradle version 8.4` soon

**Here is output screenshot for reference :**

![image](https://github.com/user-attachments/assets/2528e445-d219-49a2-9f5c-ed12e53ffa7e)

Thank you for your cooperation and understanding.

github-actions[bot] on (2024-11-20 02:04:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-27 02:06:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-27 02:06:59 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79657"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79657"">No</a>

"
2641990224,issue,closed,completed,tf.keras.Sequential model can't use tf.data.Dataset.from_generator,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

after creating dataset from tf.data.Dataset.from_generator, model of tf.keras.sequential always failed due to shape not match for the input. Even just the simple github copilot generated sample can't run. not sure if anyone succeeded. thanks.

### Standalone code to reproduce the issue

```shell
#code & sample data

import tensorflow as tf
import pandas as pd

# test.csv 
# x,y,z,target
# 0.7042368571200515,0.7007563770917052,0.9923080382725273,0
# 0.11426662913904129,0.8766193882299516,0.7525734407311002,0
# 0.3990706480213546,0.7509927893745748,0.08310617899165762,0
# 0.6541820169804226,0.8728205248135915,0.009735857788017,0
# 0.9824663638939366,0.8929642732916638,0.42717481372577704,0
# 0.05016847688897663,0.8640977215284672,0.00015648051633876392,1
# 0.9746878269143237,0.4675049082702597,0.8701887846733452,0
# 0.6351460617236527,0.41715847867753963,0.7187310540710531,0
# 0.48783818394438205,0.7666787407271458,0.27013849266804313,0
# 0.32031329934121744,0.17932919492349586,0.6898206330541312,0
# 0.9543041190067443,0.591335844278636,0.6588428533365475,0
# 0.19313525739712412,0.9852300738375201,0.6948888181361819,0
# 0.19820267930950564,0.933211841142697,0.903656352513663,0
# 0.8150172410867663,0.8880582276213321,0.5061326797194212,0
# 0.596347151996661,0.7352080480185654,0.7880475513257801,0
# 0.6134560868023209,0.3485123047276638,0.22781550361885472,0
# 0.8044922456384954,0.45120831616370516,0.5767554455960054,0
# 0.6715578431234355,0.7646054358158448,0.9451860531031546,0
# 0.7686609033200247,0.6114036496260894,0.7650105537257866,0
# 0.05197577933003528,0.28496109714833917,0.41306518543162885,0
# 0.344460901937348,0.766332305545744,0.5144459764257473,0
# 0.6599678166641048,0.5292402310805339,0.5094529642981013,0
# 0.10673926773965803,0.5238891179909103,0.9817150442751443,0
# 0.7036732515429891,0.23654285159967436,0.8762269476492692,0
# 0.8781094838240854,0.506176060331502,0.9067167580705571,0
# 0.3374843921398276,0.8600866154828248,0.2973216448787409,0
# 0.943770089167269,0.0686808858245227,0.48951596198556235,1
# 0.6765152574791524,0.1375712100211337,0.1737266892058592,0
# 0.7273752026856982,0.9533380344200385,0.4924386036510685,1
# 0.4658204645836098,0.2500965060050161,0.48105252784504837,0
# 0.2880634095162119,0.6276728155035326,0.19165303472399087,0
# 0.11083669998863499,0.21704265767720732,0.6676057357044906,0
# 0.12851954218455197,0.20802495693235157,0.667663085267044,0
# 0.8727789507757944,0.3265873016685742,0.1886650498978053,0
# 0.8461403050364225,0.43490654451648725,0.31975559963755273,0
# 0.5077604100733044,0.4655673281242404,0.2802123251669665,0
# 0.2233222755592028,0.04915222505078809,0.8972617683363415,0
# 0.2770966381433091,0.6911062101812422,0.35029445120157965,0
# 0.06505403740430493,0.5549924736882712,0.1512830697345361,0
# 0.633287065526996,0.6726877553668914,0.7480470622224006,0
# 0.15276758287427195,0.09551409131836819,0.7330651843012955,0
# 0.8177575478572151,0.3118379196659643,0.7115535780280724,0
# 0.4034709361948867,0.5915301572051304,0.8315961740558816,0
# 0.2521911664746448,0.48834451689396763,0.7968736310010842,0
# 0.17204367637440232,0.9044065209258801,0.46848650028550876,1
# 0.2730952384015819,0.6654793002791546,0.6148138694973475,0
# 0.8689420382367301,0.8348391041594503,0.05993433393586789,0
# 0.5976464192216739,0.4190036279235926,0.07710971075225881,0
# 0.9703383518752555,0.7117974134043004,0.984298292889044,0


# Define a generator function to read the CSV file in chunks
def csv_generator(file_path, chunksize=1000):
    for chunk in pd.read_csv(file_path, chunksize=chunksize):
        for row in chunk.itertuples(index=False):
            yield row

# Define the feature and label extraction function
def parse_csv_row(*row):
    features = row[:-1]  # Assuming the last column is the label
    label = row[-1]
    return tf.convert_to_tensor(features, dtype=tf.float32), tf.convert_to_tensor(label, dtype=tf.float32)

# Create a TensorFlow Dataset from the generator
file_path = 'test.csv'
dataset = tf.data.Dataset.from_generator(
    lambda: csv_generator(file_path),
    output_signature=(
        tf.TensorSpec(shape=(3,), dtype=tf.float32),  # Adjust shape to match the number of features
        tf.TensorSpec(shape=(), dtype=tf.float32)     # Adjust shape to match the label
    )
)

# Map the parsing function to the dataset
dataset = dataset.map(parse_csv_row)

# Batch the dataset
batch_size = 32
dataset = dataset.batch(batch_size)

# Define a simple logistic regression model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(3,)),  # Adjust shape to match the number of features
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(dataset, epochs=10)
```


### Relevant log output

```shell
Epoch 1/10
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/litn/pyvenv/py3.12.5/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/litn/pyvenv/py3.12.5/lib/python3.12/site-packages/keras/src/models/functional.py"", line 244, in _adjust_input_rank
    raise ValueError(
ValueError: Exception encountered when calling Sequential.call().

Invalid input shape for input Tensor(""data:0"", shape=(None, 1, 3), dtype=float32). Expected shape (None, 3), but input has incompatible shape (None, 1, 3)

Arguments received by Sequential.call():
   inputs=tf.Tensor(shape=(None, 1, 3), dtype=float32)
   training=True
   mask=None
```
",litn2018,2024-11-07 19:35:34+00:00,['Venkat6871'],2024-11-27 02:07:04+00:00,2024-11-27 02:06:57+00:00,https://github.com/tensorflow/tensorflow/issues/79624,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2469650967, 'issue_id': 2641990224, 'author': 'Venkat6871', 'body': 'Hi **@litn2018** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and I faced the same issue. However, after making some changes, it is now asking for the CSV file, which you did not provide. Could you please share the file? It will help us troubleshoot more easily. For your reference, I have provided the [gist](https://colab.sandbox.google.com/gist/Venkat6871/eaab361aaeb4638c6e3db974dd40f4a1/79624_tf_2-17-0-nightly-v.ipynb) here.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 12, 5, 36, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487166558, 'issue_id': 2641990224, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 20, 2, 4, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503118, 'issue_id': 2641990224, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 27, 2, 6, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502503230, 'issue_id': 2641990224, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79624"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79624"">No</a>', 'created_at': datetime.datetime(2024, 11, 27, 2, 7, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-12 05:36:25 UTC): Hi **@litn2018** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and I faced the same issue. However, after making some changes, it is now asking for the CSV file, which you did not provide. Could you please share the file? It will help us troubleshoot more easily. For your reference, I have provided the [gist](https://colab.sandbox.google.com/gist/Venkat6871/eaab361aaeb4638c6e3db974dd40f4a1/79624_tf_2-17-0-nightly-v.ipynb) here.
Thank you!

github-actions[bot] on (2024-11-20 02:04:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-27 02:06:57 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-27 02:07:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79624"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79624"">No</a>

"
2641421019,issue,closed,completed,TensorFlow program run problem,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

window 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

please help me solve this problem its a lil bit a headache for me right now

### Standalone code to reproduce the issue

```shell
C:\Users\uneeb\PycharmProjects\pythonProject5\venv\Scripts\python.exe C:\Users\uneeb\PycharmProjects\pythonProject5\qwal.py 
Traceback (most recent call last):
  File ""C:\Users\uneeb\PycharmProjects\pythonProject5\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\uneeb\PycharmProjects\pythonProject5\qwal.py"", line 2, in <module>
    import tensorflow
  File ""C:\Users\uneeb\PycharmProjects\pythonProject5\venv\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\uneeb\PycharmProjects\pythonProject5\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\uneeb\PycharmProjects\pythonProject5\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

Process finished with exit code 1
```


### Relevant log output

```shell
none
```
",Unaib12-ux,2024-11-07 15:41:47+00:00,['tilakrayal'],2024-12-18 17:56:45+00:00,2024-12-18 17:56:42+00:00,https://github.com/tensorflow/tensorflow/issues/79609,"[('type:support', 'Support issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2467485318, 'issue_id': 2641421019, 'author': 'tilakrayal', 'body': '@Unaib12-ux,\r\n\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 11, 8, 8, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467493381, 'issue_id': 2641421019, 'author': 'Unaib12-ux', 'body': '[image: image.png]\r\ncurrently these are available on my pc\r\n\r\nOn Mon, Nov 11, 2024 at 1:08\u202fPM tilakrayal ***@***.***> wrote:\r\n\r\n> @Unaib12-ux <https://github.com/Unaib12-ux>,\r\n>\r\n> Could you please provide the tensorflow and the compatible version which\r\n> you are trying to install. Also there are at least 3 possible scenarios:\r\n>\r\n> - You need to install the MSVC 2019 redistributable- Your CPU does not support AVX2 instructions- Your CPU/Python is on 32 bits- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n>\r\n> #61887 <https://github.com/tensorflow/tensorflow/issues/61887>\r\n> Thank you!\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/79609#issuecomment-2467485318>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BMXWKHFWWPXCIDDOGRV2N3D2ABQZBAVCNFSM6AAAAABRLQAYVOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINRXGQ4DKMZRHA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 11, 11, 8, 13, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551947353, 'issue_id': 2641421019, 'author': 'mihaimaruseac', 'body': 'Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551947401, 'issue_id': 2641421019, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79609"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79609"">No</a>', 'created_at': datetime.datetime(2024, 12, 18, 17, 56, 43, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-11 08:08:25 UTC): @Unaib12-ux,

Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

Unaib12-ux (Issue Creator) on (2024-11-11 08:13:04 UTC): [image: image.png]
currently these are available on my pc

On Mon, Nov 11, 2024 at 1:08PM tilakrayal ***@***.***> wrote:

mihaimaruseac on (2024-12-18 17:56:42 UTC): Duplicate of #19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

google-ml-butler[bot] on (2024-12-18 17:56:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79609"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79609"">No</a>

"
2640267968,issue,closed,completed,Windows Wheel file doesn't exist for tensorflow_io_gcs_filesystem library for the latest version pip install tensorflow-io-gcs-filesystem==0.37.1.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

if we install -> pip install tensorflow-io-gcs-filesystem==0.37.1 it will always download the wheel file of older version of [tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl ](https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl)(1.5 MB [view hashes](https://pypi.org/project/tensorflow-io-gcs-filesystem/0.31.0/#copy-hash-modal-5a9e9ff6-6e95-4cec-940e-ae66d32efe0c))

It should be able to get a version corresponding to the latest version for windows also.


### Standalone code to reproduce the issue

```shell
if we install -> pip install tensorflow-io-gcs-filesystem==0.37.1 it will always download the wheel file of older version of [tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl ](https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl)(1.5 MB [view hashes](https://pypi.org/project/tensorflow-io-gcs-filesystem/0.31.0/#copy-hash-modal-5a9e9ff6-6e95-4cec-940e-ae66d32efe0c))
```


### Relevant log output

_No response_",adarshbilimagga,2024-11-07 08:37:19+00:00,['tilakrayal'],2024-12-22 02:05:45+00:00,2024-12-22 02:05:42+00:00,https://github.com/tensorflow/tensorflow/issues/79581,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.18', '')]","[{'comment_id': 2464934598, 'issue_id': 2640267968, 'author': 'tilakrayal', 'body': '@adarshbilimagga,\r\nLooks like this issue is more related to Tensorflow-io. Could you please raise the issue in tensorflow-io from [here](https://github.com/tensorflow/io/issues) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 11, 8, 14, 43, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466013312, 'issue_id': 2640267968, 'author': 'adarshbilimagga', 'body': 'Yes i had reported there but we didnt get any response - https://github.com/tensorflow/io/issues/2048.\r\nNow again i have created another ticket - https://github.com/tensorflow/io/issues/2087.\r\n\r\nIs there any other way out where we can get a support for this apart from creating issue.', 'created_at': datetime.datetime(2024, 11, 9, 3, 16, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508988668, 'issue_id': 2640267968, 'author': 'isuruwe', 'body': 'same here', 'created_at': datetime.datetime(2024, 11, 30, 15, 3, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523307522, 'issue_id': 2640267968, 'author': 'tilakrayal', 'body': ""@adarshbilimagga,\r\nAs the issue is related to tensorflow-io, please check the update in the respective repo for the quick resolution and also tensorflow doesn't have control on the tf-io repo. Thank you!"", 'created_at': datetime.datetime(2024, 12, 6, 13, 56, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542662774, 'issue_id': 2640267968, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 14, 2, 5, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300032, 'issue_id': 2640267968, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558300052, 'issue_id': 2640267968, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79581"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79581"">No</a>', 'created_at': datetime.datetime(2024, 12, 22, 2, 5, 44, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-08 14:43:38 UTC): @adarshbilimagga,
Looks like this issue is more related to Tensorflow-io. Could you please raise the issue in tensorflow-io from [here](https://github.com/tensorflow/io/issues) for the quick resolution. Thank you!

adarshbilimagga (Issue Creator) on (2024-11-09 03:16:12 UTC): Yes i had reported there but we didnt get any response - https://github.com/tensorflow/io/issues/2048.
Now again i have created another ticket - https://github.com/tensorflow/io/issues/2087.

Is there any other way out where we can get a support for this apart from creating issue.

isuruwe on (2024-11-30 15:03:29 UTC): same here

tilakrayal (Assginee) on (2024-12-06 13:56:09 UTC): @adarshbilimagga,
As the issue is related to tensorflow-io, please check the update in the respective repo for the quick resolution and also tensorflow doesn't have control on the tf-io repo. Thank you!

github-actions[bot] on (2024-12-14 02:05:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-22 02:05:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-22 02:05:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79581"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79581"">No</a>

"
2637872767,issue,closed,completed,"tf.autodiff.ForwardAccumulator._watch(primal, tangent) erroneously refers to dtype.is_floating which does not exist for a Keras layer.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Linux ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The bug is that in tf.autodiff.ForwardAccumulator._watch(primal, tangent), it refers to the attribute primal.dtype.is_floating, but this causes a crash, as primal.dtype is now a string type variable, and so does not have the attribute ""is_floating"".

Here is the error message I see, from the standalone code below.

  File ""/home/me/.local/lib/python3.12/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch
    if not primal.dtype.is_floating:
           ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'is_floating'


### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
x = tf.constant([[2.0, 3.0], [1.0, 4.0]])
targets = tf.constant([[1.], [-1.]])
dense = tf.keras.layers.Dense(1)
dense.build([None, 2])
with tf.autodiff.ForwardAccumulator(
   primals=dense.kernel,
   tangents=tf.constant([[1.], [0.]])) as acc:
  loss = tf.reduce_sum((dense(x) - targets) ** 2.)
print(acc.jvp(loss))
```


### Relevant log output

_No response_",mikefairbank,2024-11-06 11:49:23+00:00,['gaikwadrahul8'],2025-01-31 02:00:06+00:00,2025-01-31 02:00:03+00:00,https://github.com/tensorflow/tensorflow/issues/79523,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2459555743, 'issue_id': 2637872767, 'author': 'mikefairbank', 'body': 'The standalone code I gave to reproduce this issue is the example code given at https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator , so that code really should work.', 'created_at': datetime.datetime(2024, 11, 6, 11, 56, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467573866, 'issue_id': 2637872767, 'author': 'Venkat6871', 'body': 'Hi **@mikefairbank** ,\r\nApologies for the delay, and thank you for raising your concern here. By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.\r\n```\r\n!pip install tf-keras\r\n\r\nimport tf_keras as keras\r\n```\r\nAlso I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/7444fac2775c239a2c07edf7faae4cea/79523_tf-2-18-0-v.ipynb).\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 11, 8, 53, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467715050, 'issue_id': 2637872767, 'author': 'mikefairbank', 'body': 'Thank you for looking into this.  The fix you suggested works well for me.  I\'ve switched to tf-keras from keras, and pip upgraded my tensorflow versions to 2.18 now (for both tensorflow and tf-keras).\r\n\r\nThere still seems to be the same bug though affecting tf.keras:\r\n\r\n  File ""/home/me/.local/lib/python3.12/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch\r\n    if not primal.dtype.is_floating:\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'str\' object has no attribute \'is_floating\'\r\n\r\nIs that something that can/should be fixed?  Should the user just _know_ to use tf_keras and not tf.keras?  That doesn\'t seem right!', 'created_at': datetime.datetime(2024, 11, 11, 9, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467913178, 'issue_id': 2637872767, 'author': 'mikefairbank', 'body': 'Also, the example code in https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator will need changing to use tf-keras instead of tf.keras.  I think the needed solution is to fix forwardprop.py, line 411 so that it can handle string dtypes.', 'created_at': datetime.datetime(2024, 11, 11, 11, 12, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512300631, 'issue_id': 2637872767, 'author': 'MrChike', 'body': 'So this task has been closed then.', 'created_at': datetime.datetime(2024, 12, 2, 17, 59, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512327401, 'issue_id': 2637872767, 'author': 'mikefairbank', 'body': 'Please don\'t close it.  I think:\r\n1.  the example code in https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator will need changing to use tf-keras instead of tf.keras\r\n2. The code in tf.keras has this bug still: \r\n\r\nFile ""/home/me/.local/lib/python3.12/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch\r\nif not primal.dtype.is_floating:\r\n^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'str\' object has no attribute \'is_floating\'\r\n\r\nWe can work around this by switching tf-keras for tf.keras, but I\'m not sure that is the permanent solution to this problem.', 'created_at': datetime.datetime(2024, 12, 2, 18, 12, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552941745, 'issue_id': 2637872767, 'author': 'gaikwadrahul8', 'body': 'Hi, @mikefairbank\r\nI apologize for the delayed response, It seems like this issue is happening because of [TensorFlow + Keras 2 backwards compatibility](https://keras.io/getting_started/) From TensorFlow 2.0 to TensorFlow 2.15 (included), doing `pip install tensorflow` will also install the corresponding version of Keras 2  for instance, `pip install tensorflow==2.14.0` will install `keras==2.14.0`. That version of Keras is then available via both import keras and from tensorflow import keras (the [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) namespace).\r\n\r\nStarting with `TensorFlow 2.16`, doing `pip install tensorflow` will install `Keras 3`. When you have TensorFlow >= 2.16 and Keras 3, then by default from tensorflow import keras ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) will be Keras 3.\r\n\r\nMeanwhile, the legacy Keras 2 package is still being released regularly and is available on PyPI as tf_keras (or equivalently tf-keras  note that - and _ are equivalent in PyPI package names). To use it, you can install it via pip install tf_keras then import it via import tf_keras as keras.\r\n\r\nShould you want [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) to stay on Keras 2 after upgrading to TensorFlow 2.16+, you can configure your TensorFlow installation so that [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) points to tf_keras. To achieve this:\r\n\r\n1. Make sure to install `tf_keras`. Note that TensorFlow does not install it by default.\r\n2. Export the environment variable `TF_USE_LEGACY_KERAS=1`.\r\n\r\nI have submitted PR to update example https://github.com/tensorflow/tensorflow/pull/83318 and also please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/d1a6b2b563eb63a0ceb8c1fa260196a0/testing-tf-core-issue-79523.ipynb)\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 19, 7, 10, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553968510, 'issue_id': 2637872767, 'author': 'mihaimaruseac', 'body': 'From my understanding of the comments posted here, the same error occurs when using either `keras` or `tf-keras`, right?', 'created_at': datetime.datetime(2024, 12, 19, 13, 26, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556565325, 'issue_id': 2637872767, 'author': 'gaikwadrahul8', 'body': 'Hi, @mihaimaruseac \r\nToday, I have tried different combinations of TensorFlow versions and Keras versions along with standalone installing Keras without installing the `tf_keras` and setting up environment variable `TF_USE_LEGACY_KERAS=1 ` it is throwing this error message `AttributeError: \'str\' object has no attribute \'is_floating\'` for examples mentioned for [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) but when I install `tf_keras` manually and setting up environment variable `TF_USE_LEGACY_KERAS=1 ` the examples are working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/9848c39ad054300864f080226d7a1079/testing-tf-core-issue-79523.ipynb) where I did all the experiments with different versions of TensorFlow and Keras even I checked by installing Keras package standalone but examples did not work as expected\r\n\r\nIf I am not wrong examples mentioned for [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) are working with `Keras 2` but it is not working with `Keras 3 ` so should we proceed with making the [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) API compatible with Keras 3 or are the below proposed changes sufficient for the time being? \r\n\r\nI have missed something here please let me know. Thank you\r\n\r\n```\r\n>>> # Please install tf_keras using this command pip install tf_keras before executing below examples\r\n>>> import os\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> os.environ[""TF_USE_LEGACY_KERAS""] = ""1""\r\n```', 'created_at': datetime.datetime(2024, 12, 20, 9, 2, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560177128, 'issue_id': 2637872767, 'author': 'mihaimaruseac', 'body': 'I tried it myself. First, setup:\r\n\r\n```bash\r\n[...]$ pip list\r\nPackage                      Version\r\n---------------------------- -------------------\r\nabsl-py                      2.1.0\r\nastunparse                   1.6.3\r\ncertifi                      2024.12.14\r\ncharset-normalizer           3.4.0\r\nflatbuffers                  24.3.25\r\ngast                         0.6.0\r\ngoogle-pasta                 0.2.0\r\ngrpcio                       1.68.1\r\nh5py                         3.12.1\r\nidna                         3.10\r\nkeras-nightly                3.7.0.dev2024122303\r\nlibclang                     18.1.1\r\nMarkdown                     3.7\r\nmarkdown-it-py               3.0.0\r\nMarkupSafe                   3.0.2\r\nmdurl                        0.1.2\r\nml_dtypes                    0.5.0\r\nnamex                        0.0.8\r\nnumpy                        2.1.3\r\nopt_einsum                   3.4.0\r\noptree                       0.13.1\r\npackaging                    24.2\r\npip                          24.0\r\nprotobuf                     5.29.2\r\nPygments                     2.18.0\r\nrequests                     2.32.3\r\nrich                         13.9.4\r\nsetuptools                   70.3.0\r\nsix                          1.17.0\r\ntb-nightly                   2.19.0a20241223\r\ntensorboard-data-server      0.7.2\r\ntensorflow-io-gcs-filesystem 0.37.1\r\ntermcolor                    2.5.0\r\ntf_nightly                   2.19.0.dev20241219\r\ntyping_extensions            4.12.2\r\nurllib3                      2.3.0\r\nWerkzeug                     3.1.3\r\nwheel                        0.45.1\r\nwrapt                        1.17.0\r\n[...]$ echo ${TF_USE_LEGACY_KERAS:-unset}\r\nunset\r\n```\r\n\r\nThen, reproduction\r\n\r\n```python\r\n>>> import tensorflow as tf \r\n2024-12-23 11:00:56.974298: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:00:56.978166: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:00:56.986610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734980457.003693 2169296 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734980457.007630 2169296 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-23 11:00:57.021934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> x = tf.constant([[2.0, 3.0], [1.0, 4.0]])\r\nE0000 00:00:1734980461.384182 2169296 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\r\nW0000 00:00:1734980461.386100 2169296 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n>>> targets = tf.constant([[1.], [-1.]])\r\n>>> dense = tf.keras.layers.Dense(1)\r\n>>> dense.build([None, 2])\r\n>>> with tf.autodiff.ForwardAccumulator(primals=dense.kernel, tangents=tf.constant([[1.], [0.]])) as acc:\r\n...   loss = tf.reduce_sum((dense(x) - targets) ** 2.)\r\n... \r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 372, in __init__\r\n    self._watch(primals, tangents)\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 423, in _watch\r\n    nest.map_structure(_watch, primals, tangents)\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py"", line 628, in map_structure\r\n    return nest_util.map_structure(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1065, in map_structure\r\n    return _tf_core_map_structure(func, *structure, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in _tf_core_map_structure\r\n    [func(*x) for x in entries],\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in <listcomp>\r\n    [func(*x) for x in entries],\r\n     ^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch\r\n    if not primal.dtype.is_floating:\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'str\' object has no attribute \'is_floating\'\r\n```\r\n\r\nMinimizing:\r\n\r\n```python\r\n>>> tf.autodiff.ForwardAccumulator(primals=dense.kernel, tangents=tf.constant([[1.], [0.]]))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 372, in __init__\r\n    self._watch(primals, tangents)\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 423, in _watch\r\n    nest.map_structure(_watch, primals, tangents)\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py"", line 628, in map_structure\r\n    return nest_util.map_structure(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1065, in map_structure\r\n    return _tf_core_map_structure(func, *structure, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in _tf_core_map_structure\r\n    [func(*x) for x in entries],\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in <listcomp>\r\n    [func(*x) for x in entries],\r\n     ^^^^^^^^\r\n  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch\r\n    if not primal.dtype.is_floating:\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'str\' object has no attribute \'is_floating\'\r\n```\r\n\r\nUsing only constants:\r\n\r\n```python\r\n>>> tf.autodiff.ForwardAccumulator(primals=tf.constant([[0.], [1.]]), tangents=tf.constant([[1.], [0.]]))\r\n<tensorflow.python.eager.forwardprop.ForwardAccumulator object at 0x7faeb00ba150>\r\n```\r\n\r\nSo, the issue is on the keras layer:\r\n\r\n```python\r\n>>> tf.constant([[0.], [1.]]).dtype\r\ntf.float32\r\n>>> dense.kernel.dtype\r\n\'float32\'\r\n```\r\n\r\nWe should make the code compatible with both TF-Keras and Keras 3.', 'created_at': datetime.datetime(2024, 12, 23, 19, 7, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560184408, 'issue_id': 2637872767, 'author': 'mihaimaruseac', 'body': 'Now that the issue is isolated, installing `tf_keras-nightly` directly results in both keras versions being installed:\r\n\r\n```console\r\n[...]$ pip list | grep keras\r\nkeras-nightly                3.7.0.dev2024122303\r\ntf_keras-nightly             2.19.0.dev2024122310\r\n```\r\n\r\nTesting\r\n\r\n```python\r\n[...]$ python\r\nPython 3.11.9 (main, Jun 19 2024, 00:38:48) [GCC 13.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf \r\n2024-12-23 11:10:20.719619: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:10:20.722685: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:10:20.731190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734981020.746389 2173405 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734981020.750512 2173405 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-23 11:10:20.766911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> l = tf.keras.layers.Dense(1)\r\n>>> l.build([None, 2])\r\nE0000 00:00:1734981044.048778 2173405 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\r\nW0000 00:00:1734981044.050537 2173405 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n>>> l.kernel.dtype\r\n\'float32\'\r\n```\r\n\r\nThis is not good. Let\'s set the environment variable:\r\n\r\n```python\r\n[...]$ export TF_USE_LEGACY_KERAS=1\r\n[...]$ python\r\nPython 3.11.9 (main, Jun 19 2024, 00:38:48) [GCC 13.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import tensorflow as tf \r\n2024-12-23 11:11:13.884706: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:11:13.887806: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-23 11:11:13.896277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734981073.910177 2173913 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734981073.914621 2173913 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-23 11:11:13.928734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> l = tf.keras.layers.Dense(1)\r\n>>> l.build([None, 2])\r\nE0000 00:00:1734981083.268596 2173913 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\r\nW0000 00:00:1734981083.270463 2173913 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n>>> l.kernel.dtype\r\ntf.float32\r\n```\r\n\r\nSo, this should likely be a bug reported on Keras and what we can do for TF is to suggest to set the environment variable (in tutorial, I\'m not sure we should do that in code since the code works perfect when not using Keras at all)', 'created_at': datetime.datetime(2024, 12, 23, 19, 14, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595113186, 'issue_id': 2637872767, 'author': 'gaikwadrahul8', 'body': 'Hi, @mihaimaruseac\nI apologize for the delayed response and Thank you for taking the time to provide such a thorough analysis\n\nHi, @mikefairbank, Please refer above [comment](https://github.com/tensorflow/tensorflow/issues/79523#issuecomment-2560184408), I have tried from my end by setting up environment variable to `os.environ[""TF_USE_LEGACY_KERAS""] = ""1""` in Google colab where `keras 3.5.0` and `tf_keras 2.17.0` versions are already available and it\'s working as expected please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/9dab56f9acbf9baf463acac1b814aa0c/tflite-issue-79523.ipynb)\n\nThank you for your understanding and patience.', 'created_at': datetime.datetime(2025, 1, 16, 10, 14, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611383923, 'issue_id': 2637872767, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 24, 1, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626128275, 'issue_id': 2637872767, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 31, 2, 0, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626128422, 'issue_id': 2637872767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79523"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79523"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 2, 0, 5, tzinfo=datetime.timezone.utc)}]","mikefairbank (Issue Creator) on (2024-11-06 11:56:24 UTC): The standalone code I gave to reproduce this issue is the example code given at https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator , so that code really should work.

Venkat6871 on (2024-11-11 08:53:16 UTC): Hi **@mikefairbank** ,
Apologies for the delay, and thank you for raising your concern here. By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.
```
!pip install tf-keras

import tf_keras as keras
```
Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/7444fac2775c239a2c07edf7faae4cea/79523_tf-2-18-0-v.ipynb).
Thank you!

mikefairbank (Issue Creator) on (2024-11-11 09:58:32 UTC): Thank you for looking into this.  The fix you suggested works well for me.  I've switched to tf-keras from keras, and pip upgraded my tensorflow versions to 2.18 now (for both tensorflow and tf-keras).

There still seems to be the same bug though affecting tf.keras:

  File ""/home/me/.local/lib/python3.12/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch
    if not primal.dtype.is_floating:
           ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'is_floating'

Is that something that can/should be fixed?  Should the user just _know_ to use tf_keras and not tf.keras?  That doesn't seem right!

mikefairbank (Issue Creator) on (2024-11-11 11:12:09 UTC): Also, the example code in https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator will need changing to use tf-keras instead of tf.keras.  I think the needed solution is to fix forwardprop.py, line 411 so that it can handle string dtypes.

MrChike on (2024-12-02 17:59:49 UTC): So this task has been closed then.

mikefairbank (Issue Creator) on (2024-12-02 18:12:06 UTC): Please don't close it.  I think:
1.  the example code in https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator will need changing to use tf-keras instead of tf.keras
2. The code in tf.keras has this bug still: 

File ""/home/me/.local/lib/python3.12/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch
if not primal.dtype.is_floating:
^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'is_floating'

We can work around this by switching tf-keras for tf.keras, but I'm not sure that is the permanent solution to this problem.

gaikwadrahul8 (Assginee) on (2024-12-19 07:10:53 UTC): Hi, @mikefairbank
I apologize for the delayed response, It seems like this issue is happening because of [TensorFlow + Keras 2 backwards compatibility](https://keras.io/getting_started/) From TensorFlow 2.0 to TensorFlow 2.15 (included), doing `pip install tensorflow` will also install the corresponding version of Keras 2  for instance, `pip install tensorflow==2.14.0` will install `keras==2.14.0`. That version of Keras is then available via both import keras and from tensorflow import keras (the [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) namespace).

Starting with `TensorFlow 2.16`, doing `pip install tensorflow` will install `Keras 3`. When you have TensorFlow >= 2.16 and Keras 3, then by default from tensorflow import keras ([tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)) will be Keras 3.

Meanwhile, the legacy Keras 2 package is still being released regularly and is available on PyPI as tf_keras (or equivalently tf-keras  note that - and _ are equivalent in PyPI package names). To use it, you can install it via pip install tf_keras then import it via import tf_keras as keras.

Should you want [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) to stay on Keras 2 after upgrading to TensorFlow 2.16+, you can configure your TensorFlow installation so that [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) points to tf_keras. To achieve this:

1. Make sure to install `tf_keras`. Note that TensorFlow does not install it by default.
2. Export the environment variable `TF_USE_LEGACY_KERAS=1`.

I have submitted PR to update example https://github.com/tensorflow/tensorflow/pull/83318 and also please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/d1a6b2b563eb63a0ceb8c1fa260196a0/testing-tf-core-issue-79523.ipynb)

Thank you for your cooperation and patience.

mihaimaruseac on (2024-12-19 13:26:48 UTC): From my understanding of the comments posted here, the same error occurs when using either `keras` or `tf-keras`, right?

gaikwadrahul8 (Assginee) on (2024-12-20 09:02:28 UTC): Hi, @mihaimaruseac 
Today, I have tried different combinations of TensorFlow versions and Keras versions along with standalone installing Keras without installing the `tf_keras` and setting up environment variable `TF_USE_LEGACY_KERAS=1 ` it is throwing this error message `AttributeError: 'str' object has no attribute 'is_floating'` for examples mentioned for [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) but when I install `tf_keras` manually and setting up environment variable `TF_USE_LEGACY_KERAS=1 ` the examples are working as expected please refer this [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/9848c39ad054300864f080226d7a1079/testing-tf-core-issue-79523.ipynb) where I did all the experiments with different versions of TensorFlow and Keras even I checked by installing Keras package standalone but examples did not work as expected

If I am not wrong examples mentioned for [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) are working with `Keras 2` but it is not working with `Keras 3 ` so should we proceed with making the [tf.autodiff.ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) API compatible with Keras 3 or are the below proposed changes sufficient for the time being? 

I have missed something here please let me know. Thank you

```
```

mihaimaruseac on (2024-12-23 19:07:37 UTC): I tried it myself. First, setup:

```bash
[...]$ pip list
Package                      Version
---------------------------- -------------------
absl-py                      2.1.0
astunparse                   1.6.3
certifi                      2024.12.14
charset-normalizer           3.4.0
flatbuffers                  24.3.25
gast                         0.6.0
google-pasta                 0.2.0
grpcio                       1.68.1
h5py                         3.12.1
idna                         3.10
keras-nightly                3.7.0.dev2024122303
libclang                     18.1.1
Markdown                     3.7
markdown-it-py               3.0.0
MarkupSafe                   3.0.2
mdurl                        0.1.2
ml_dtypes                    0.5.0
namex                        0.0.8
numpy                        2.1.3
opt_einsum                   3.4.0
optree                       0.13.1
packaging                    24.2
pip                          24.0
protobuf                     5.29.2
Pygments                     2.18.0
requests                     2.32.3
rich                         13.9.4
setuptools                   70.3.0
six                          1.17.0
tb-nightly                   2.19.0a20241223
tensorboard-data-server      0.7.2
tensorflow-io-gcs-filesystem 0.37.1
termcolor                    2.5.0
tf_nightly                   2.19.0.dev20241219
typing_extensions            4.12.2
urllib3                      2.3.0
Werkzeug                     3.1.3
wheel                        0.45.1
wrapt                        1.17.0
[...]$ echo ${TF_USE_LEGACY_KERAS:-unset}
unset
```

Then, reproduction

```python
2024-12-23 11:00:56.974298: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:00:56.978166: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:00:56.986610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734980457.003693 2169296 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734980457.007630 2169296 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-23 11:00:57.021934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E0000 00:00:1734980461.384182 2169296 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.
W0000 00:00:1734980461.386100 2169296 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
...   loss = tf.reduce_sum((dense(x) - targets) ** 2.)
... 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 372, in __init__
    self._watch(primals, tangents)
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 423, in _watch
    nest.map_structure(_watch, primals, tangents)
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py"", line 628, in map_structure
    return nest_util.map_structure(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1065, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in _tf_core_map_structure
    [func(*x) for x in entries],
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in <listcomp>
    [func(*x) for x in entries],
     ^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch
    if not primal.dtype.is_floating:
           ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'is_floating'
```

Minimizing:

```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 372, in __init__
    self._watch(primals, tangents)
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 423, in _watch
    nest.map_structure(_watch, primals, tangents)
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py"", line 628, in map_structure
    return nest_util.map_structure(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1065, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in _tf_core_map_structure
    [func(*x) for x in entries],
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py"", line 1105, in <listcomp>
    [func(*x) for x in entries],
     ^^^^^^^^
  File ""/tmp/tf/venv/lib/python3.11/site-packages/tensorflow/python/eager/forwardprop.py"", line 411, in _watch
    if not primal.dtype.is_floating:
           ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'is_floating'
```

Using only constants:

```python
<tensorflow.python.eager.forwardprop.ForwardAccumulator object at 0x7faeb00ba150>
```

So, the issue is on the keras layer:

```python
tf.float32
'float32'
```

We should make the code compatible with both TF-Keras and Keras 3.

mihaimaruseac on (2024-12-23 19:14:32 UTC): Now that the issue is isolated, installing `tf_keras-nightly` directly results in both keras versions being installed:

```console
[...]$ pip list | grep keras
keras-nightly                3.7.0.dev2024122303
tf_keras-nightly             2.19.0.dev2024122310
```

Testing

```python
[...]$ python
Python 3.11.9 (main, Jun 19 2024, 00:38:48) [GCC 13.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
2024-12-23 11:10:20.719619: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:10:20.722685: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:10:20.731190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734981020.746389 2173405 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734981020.750512 2173405 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-23 11:10:20.766911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E0000 00:00:1734981044.048778 2173405 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.
W0000 00:00:1734981044.050537 2173405 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
'float32'
```

This is not good. Let's set the environment variable:

```python
[...]$ export TF_USE_LEGACY_KERAS=1
[...]$ python
Python 3.11.9 (main, Jun 19 2024, 00:38:48) [GCC 13.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
2024-12-23 11:11:13.884706: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:11:13.887806: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-23 11:11:13.896277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734981073.910177 2173913 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734981073.914621 2173913 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-23 11:11:13.928734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E0000 00:00:1734981083.268596 2173913 cuda_executor.cc:1189] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.
W0000 00:00:1734981083.270463 2173913 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
tf.float32
```

So, this should likely be a bug reported on Keras and what we can do for TF is to suggest to set the environment variable (in tutorial, I'm not sure we should do that in code since the code works perfect when not using Keras at all)

gaikwadrahul8 (Assginee) on (2025-01-16 10:14:19 UTC): Hi, @mihaimaruseac
I apologize for the delayed response and Thank you for taking the time to provide such a thorough analysis

Hi, @mikefairbank, Please refer above [comment](https://github.com/tensorflow/tensorflow/issues/79523#issuecomment-2560184408), I have tried from my end by setting up environment variable to `os.environ[""TF_USE_LEGACY_KERAS""] = ""1""` in Google colab where `keras 3.5.0` and `tf_keras 2.17.0` versions are already available and it's working as expected please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/9dab56f9acbf9baf463acac1b814aa0c/tflite-issue-79523.ipynb)

Thank you for your understanding and patience.

github-actions[bot] on (2025-01-24 01:59:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-31 02:00:02 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-31 02:00:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79523"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79523"">No</a>

"
2637789446,issue,open,,Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

docker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0

### Mobile device

None

### Python version

Python 3.11.0rc1 (provided by tensorflow docker image)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.7

### GPU model and memory

NVIDIA GeForce RTX 4060 Ti 16G

### Current behavior?

I Strictly followed the instructions provided in:
https://www.tensorflow.org/install/docker 
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
Got correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)

![image](https://github.com/user-attachments/assets/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)

Downloaded tensorflow/tensorflow latest-gpu-jupyter image and ran the container.
Opend a new jupyter notebook (http://127.0.0.1:8888/tree?token=...)
Importing tensorflow I wanted to check the GPU support.
Got error messages and empy available gpu list.
`2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.__version__

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

```shell
2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
'2.18.0'

Num GPUs Available:  0
2024-11-06 10:31:54.748888: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)
```
",VsevolodLvov,2024-11-06 11:15:27+00:00,['tilakrayal'],2024-11-11 11:55:32+00:00,,https://github.com/tensorflow/tensorflow/issues/79520,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues'), ('TF 2.18', '')]","[{'comment_id': 2459475213, 'issue_id': 2637789446, 'author': 'VsevolodLvov', 'body': 'I run Windows: 10.0.19045.5011', 'created_at': datetime.datetime(2024, 11, 6, 11, 17, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460498087, 'issue_id': 2637789446, 'author': 'chanangaza', 'body': 'same as above. running 2.17.1-gpu results in all OK.', 'created_at': datetime.datetime(2024, 11, 6, 18, 31, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461498739, 'issue_id': 2637789446, 'author': 'VsevolodLvov', 'body': 'Did not find 2.17.1 docker image, used 2.17.0 instead - unfortunally the same result: the same errors and no GPU found.\r\n![image](https://github.com/user-attachments/assets/15b36eb6-4570-4b6c-93fc-ede5e3d5f714)', 'created_at': datetime.datetime(2024, 11, 7, 7, 23, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462701132, 'issue_id': 2637789446, 'author': 'chanangaza', 'body': '> Did not find 2.17.1 docker image, used 2.17.0 instead - unfortunatally the same result: the same errors and no GPU found. ![image](https://private-user-images.githubusercontent.com/106977073/383855149-15b36eb6-4570-4b6c-93fc-ede5e3d5f714.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA5OTM5MzUsIm5iZiI6MTczMDk5MzYzNSwicGF0aCI6Ii8xMDY5NzcwNzMvMzgzODU1MTQ5LTE1YjM2ZWI2LTQ1NzAtNGI2Yy05M2ZjLWVkZTVlM2Q1ZjcxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwN1QxNTMzNTVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01MGQxNWE2MmM0ODVhYmEwZDZjMDE2NzgxM2IyNWE1ZGE0M2Q2MzlkMzM2YmIxMzAxMjk0ZDEzMjU3MDkwNjUwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fjL6CXFmcqppGzc6J_87DVtPXfdNW7pGXc4beFgqwyE)\r\n\r\nsorry, my bad meant `2.17.0-gpu`.\r\n\r\nbelow is what I get with `2.17.0-gpu`:\r\n![image](https://github.com/user-attachments/assets/42f52e58-b12c-4e50-a7d2-bec8be6f17f3)\r\n\r\noutput of same but with `2.18.0-gpu`:\r\n![image](https://github.com/user-attachments/assets/1b761b75-5f10-4183-9932-67ee137cea58)', 'created_at': datetime.datetime(2024, 11, 7, 16, 37, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465029223, 'issue_id': 2637789446, 'author': 'psavarmattas', 'body': 'I was stuck with it also the way I solved it was to use this command:\r\n` docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:2.17.0-gpu-jupyter` \r\nThis allowed the docker container to use the gpus. Hope this helps!', 'created_at': datetime.datetime(2024, 11, 8, 15, 23, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467691513, 'issue_id': 2637789446, 'author': 'tilakrayal', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 11, 11, 9, 49, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467997131, 'issue_id': 2637789446, 'author': 'VsevolodLvov', 'body': '> I was stuck with it also the way I solved it was to use this command: ` docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:2.17.0-gpu-jupyter` This allowed the docker container to use the gpus. Hope this helps!\r\n\r\nYes! It seems to work. Though error messages and warnings still persist. Anyhow, I was expecting TF 2.18 to work. Sadly enough, it would not.', 'created_at': datetime.datetime(2024, 11, 11, 11, 55, 31, tzinfo=datetime.timezone.utc)}]","VsevolodLvov (Issue Creator) on (2024-11-06 11:17:06 UTC): I run Windows: 10.0.19045.5011

chanangaza on (2024-11-06 18:31:52 UTC): same as above. running 2.17.1-gpu results in all OK.

VsevolodLvov (Issue Creator) on (2024-11-07 07:23:52 UTC): Did not find 2.17.1 docker image, used 2.17.0 instead - unfortunally the same result: the same errors and no GPU found.
![image](https://github.com/user-attachments/assets/15b36eb6-4570-4b6c-93fc-ede5e3d5f714)

chanangaza on (2024-11-07 16:37:12 UTC): sorry, my bad meant `2.17.0-gpu`.

below is what I get with `2.17.0-gpu`:
![image](https://github.com/user-attachments/assets/42f52e58-b12c-4e50-a7d2-bec8be6f17f3)

output of same but with `2.18.0-gpu`:
![image](https://github.com/user-attachments/assets/1b761b75-5f10-4183-9932-67ee137cea58)

psavarmattas on (2024-11-08 15:23:46 UTC): I was stuck with it also the way I solved it was to use this command:
` docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:2.17.0-gpu-jupyter` 
This allowed the docker container to use the gpus. Hope this helps!

tilakrayal (Assginee) on (2024-11-11 09:49:14 UTC): @learning-to-play

VsevolodLvov (Issue Creator) on (2024-11-11 11:55:31 UTC): Yes! It seems to work. Though error messages and warnings still persist. Anyhow, I was expecting TF 2.18 to work. Sadly enough, it would not.

"
2636903976,issue,closed,completed,How build libtensorflow-lite.a(or libxnnpack-delegate.a) or libtensorflowlite.so with KleidiAI(SVE),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

latest(Compiled using the latest cmake code)

### Custom code

No

### OS platform and distribution

Mac OS

### Mobile device

_No response_

### Python version

3.13.0(bazel build used 3.9.6)

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,
I compiled libtensorflowlite with Bazel and CMake, however, without SVE (from KleidiAI) in the library. How can I compile libtensorflowlite with SVE support (to enable KleidiAI)?

My compilation commands are as follows:

For CMake:

cmake -DCMAKE_TOOLCHAIN_FILE=/Users/mi/Library/Android/sdk/ndk/28.0.12433566/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a ../tensorflow_src/tensorflow/lite
cmake --build . -j

I did not check for SVE in the libtensorflow-lite.a and libxnnpack-delegate.a libraries built with CMake.

For Bazel:

bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so

I did not check for SVE in the libtensorflowlite.so library built with Bazel.

Thanks very much.

### Standalone code to reproduce the issue

```shell
cmake -DCMAKE_TOOLCHAIN_FILE=/Users/mi/Library/Android/sdk/ndk/28.0.12433566/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a ../tensorflow_src/tensorflow/lite
cmake --build . -j

and

bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so
```


### Relevant log output

_No response_",ytmeg,2024-11-06 02:50:56+00:00,['gaikwadrahul8'],2024-11-26 02:05:53+00:00,2024-11-26 02:05:48+00:00,https://github.com/tensorflow/tensorflow/issues/79479,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues')]","[{'comment_id': 2463658093, 'issue_id': 2636903976, 'author': 'ytmeg', 'body': 'I\'ve noticed that CMake is building for the XNNPACK target processor as x86_64, instead of the desired arm64.\r\n`-- Building for XNNPACK_TARGET_PROCESSOR: x86_64`\r\n\r\nI attempted to address this by setting CMAKE_SYSTEM_PROCESSOR to aarch64, but it didn\'t have the desired effect. \r\n`cmake -DCMAKE_TOOLCHAIN_FILE=/Users/mi/Library/Android/sdk/ndk/28.0.12433566/build/cmake/android.toolchain.cmake \\\r\n  -DANDROID_ABI=arm64-v8a -DCMAKE_SYSTEM_PROCESSOR=aarch64 ../tensorflow_src/tensorflow/lite`\r\n\r\n Furthermore, I tried to make changes to the CMakeLists.txt file located in tensorflow/lite/CMakeLists.txt, but this resulted in a build error.\r\n`set(CMAKE_SYSTEM_PROCESSOR aarch64)\r\nset(ANDROID_ABI arm64-v8a )\r\n`\r\n`Undefined symbols for architecture x86_64:\r\n  ""_cpuinfo_x86_mach_init"", referenced from:\r\n      _cpuinfo_initialize in init.c.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[2]: *** [_deps/cpuinfo-build/libcpuinfo.dylib] Error 1\r\nmake[1]: *** [_deps/cpuinfo-build/CMakeFiles/cpuinfo.dir/all] Error 2`\r\n\r\nHow can I build a libtensorflowlite.so with KleidiAI?', 'created_at': datetime.datetime(2024, 11, 8, 2, 55, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467434769, 'issue_id': 2636903976, 'author': 'gaikwadrahul8', 'body': ""Hi, @ytmeg\r\n\r\nI apologize for the delayed response, I was trying to replicate the similar behavior from my end with bazel on `Apple M1 Pro Macbook` and after running this command `bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so` I got below error message and **Build did NOT complete successfully**  so did you encounter the same error message or different ? or Am I missing something here if so please let me know ?\r\n\r\nHere is output error log for reference :\r\n```\r\ngaikwadrahul-macbookpro2:tensorflow_src gaikwadrahul$ bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so\r\nINFO: Reading 'startup' options from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=214\r\nINFO: Reading rc options for 'build' from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Found applicable config definition build:short_logs in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm64 in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --define=xnn_enable_avxvnniint8=false --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt\r\nINFO: Found applicable config definition build:no_tfrt in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\r\nDEBUG: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_tsl/third_party/py/python_repo.bzl:119:10: Using hermetic Python 3.12\r\nERROR: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_config_cc/BUILD:44:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nERROR: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_config_cc/BUILD:44:19: Analysis of target '@local_config_cc//:toolchain' failed\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: \r\nINFO: Elapsed time: 3.013s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (35 packages loaded, 594 targets configured)\r\n```\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 11, 7, 37, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553327, 'issue_id': 2636903976, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499453120, 'issue_id': 2636903976, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499453413, 'issue_id': 2636903976, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79479"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79479"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 51, tzinfo=datetime.timezone.utc)}]","ytmeg (Issue Creator) on (2024-11-08 02:55:20 UTC): I've noticed that CMake is building for the XNNPACK target processor as x86_64, instead of the desired arm64.
`-- Building for XNNPACK_TARGET_PROCESSOR: x86_64`

I attempted to address this by setting CMAKE_SYSTEM_PROCESSOR to aarch64, but it didn't have the desired effect. 
`cmake -DCMAKE_TOOLCHAIN_FILE=/Users/mi/Library/Android/sdk/ndk/28.0.12433566/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a -DCMAKE_SYSTEM_PROCESSOR=aarch64 ../tensorflow_src/tensorflow/lite`

 Furthermore, I tried to make changes to the CMakeLists.txt file located in tensorflow/lite/CMakeLists.txt, but this resulted in a build error.
`set(CMAKE_SYSTEM_PROCESSOR aarch64)
set(ANDROID_ABI arm64-v8a )
`
`Undefined symbols for architecture x86_64:
  ""_cpuinfo_x86_mach_init"", referenced from:
      _cpuinfo_initialize in init.c.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [_deps/cpuinfo-build/libcpuinfo.dylib] Error 1
make[1]: *** [_deps/cpuinfo-build/CMakeFiles/cpuinfo.dir/all] Error 2`

How can I build a libtensorflowlite.so with KleidiAI?

gaikwadrahul8 (Assginee) on (2024-11-11 07:37:54 UTC): Hi, @ytmeg

I apologize for the delayed response, I was trying to replicate the similar behavior from my end with bazel on `Apple M1 Pro Macbook` and after running this command `bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so` I got below error message and **Build did NOT complete successfully**  so did you encounter the same error message or different ? or Am I missing something here if so please let me know ?

Here is output error log for reference :
```
gaikwadrahul-macbookpro2:tensorflow_src gaikwadrahul$ bazel build -c opt --config=android_arm64 --cpu=arm64-v8a tensorflow/lite:libtensorflowlite.so
INFO: Reading 'startup' options from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=214
INFO: Reading rc options for 'build' from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Found applicable config definition build:short_logs in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --define=xnn_enable_avxvnniint8=false --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/gaikwadrahul/tflite-issue-77264/tensorflow/tensorflow/TFLite-Issue-#79479/tensorflow_src/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_tsl/third_party/py/python_repo.bzl:119:10: Using hermetic Python 3.12
ERROR: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_config_cc/BUILD:44:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
ERROR: /private/var/tmp/_bazel_gaikwadrahul/384282bf6fffff5596fd87bd08fcf5e2/external/local_config_cc/BUILD:44:19: Analysis of target '@local_config_cc//:toolchain' failed
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: 
INFO: Elapsed time: 3.013s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (35 packages loaded, 594 targets configured)
```
Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-19 02:05:13 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-26 02:05:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-26 02:05:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79479"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79479"">No</a>

"
2635966422,issue,open,,[TFLite/LiteRT] Conv3D does not support acceleration,"**System information**
- TensorFlow Python version: 2.17.
- TensorFlow Android versions:
```
    implementation 'org.tensorflow:tensorflow-lite:2.16.1'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.16.1'
    implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.16.1'
```

**Summary**

I am trying to speed up a model which is mostly made up of `Conv3D` operators on Android (for video classification). As far as I understand, neither the GPU delegate nor the quantization mechanism support `Conv3D`, so I am stuck with just running the full-precision model on CPU.

Am I missing some other ways I could use to accelerate this? If not, could this be a feature request for supporting a way?

Some performance numbers from my tests (one model pass):
* LiteRT, CPU-only, Android (Galaxy S10+): ~1400ms
* LiteRT, CPU-only, iOS (iPhone 13 Pro): ~700ms
* CoreML, accelerated, iOS, `float32` (same phone): ~160ms
* CoreML, accelerated, iOS, `float16`) (same phone): ~15ms",kodek16,2024-11-05 16:43:25+00:00,['gaikwadrahul8'],2024-11-11 05:40:30+00:00,,https://github.com/tensorflow/tensorflow/issues/79439,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2467281587, 'issue_id': 2635966422, 'author': 'gaikwadrahul8', 'body': 'Hi, @kodek16 \r\n\r\nI apologize for the delayed response, I see at the moment we do not support Conv3D Op with GPU delegates for LiteRT and There are some limitations to what TensorFlow ML operations, or ops, can be accelerated by the LiteRT GPU delegate. The delegate supports the mentioned ops in 16-bit and 32-bit float precision in this [official documentation](https://ai.google.dev/edge/litert/performance/gpu) so this issue will be considered as feature request, thank you for showing your interest to support Conv3D Op\r\n\r\nThank you for you cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 11, 5, 40, 23, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-11 05:40:23 UTC): Hi, @kodek16 

I apologize for the delayed response, I see at the moment we do not support Conv3D Op with GPU delegates for LiteRT and There are some limitations to what TensorFlow ML operations, or ops, can be accelerated by the LiteRT GPU delegate. The delegate supports the mentioned ops in 16-bit and 32-bit float precision in this [official documentation](https://ai.google.dev/edge/litert/performance/gpu) so this issue will be considered as feature request, thank you for showing your interest to support Conv3D Op

Thank you for you cooperation and patience.

"
2635872003,issue,open,,Cannot Convert 51 to a shape - Movenet pose classification tutorial,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Win11, colab notebook

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the notebook pose_classification.ipynb tutorial works fine until the following cell: 

`# Define the model
inputs = tf.keras.Input(shape=(51))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(len(class_names), activation=""softmax"")(layer)

model = keras.Model(inputs, outputs)
model.summary()`

I've run this both locally and within colab and get the same results. Any advice or insight would be helpful. 

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=1Pte6b1bgWKv
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)

<ipython-input-28-f905a19927b3> in <cell line: 2>()
      1 # Define the model
----> 2 inputs = tf.keras.Input(shape=(51))
      3 embedding = landmarks_to_embedding(inputs)
      4 
      5 layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)

2 frames

/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py in standardize_shape(shape)
    528             raise ValueError(""Undefined shapes are not supported."")
    529         if not hasattr(shape, ""__iter__""):
--> 530             raise ValueError(f""Cannot convert '{shape}' to a shape."")
    531         if config.backend() == ""tensorflow"":
    532             if isinstance(shape, tf.TensorShape):

ValueError: Cannot convert '51' to a shape.
```
",delray,2024-11-05 16:02:00+00:00,['gaikwadrahul8'],2024-12-25 17:56:19+00:00,,https://github.com/tensorflow/tensorflow/issues/79435,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2458818165, 'issue_id': 2635872003, 'author': 'tilakrayal', 'body': '@delray,\r\nI tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/72202#issuecomment-2245024521\r\nhttps://github.com/tensorflow/tensorflow/issues/70841\r\nhttps://github.com/tensorflow/tensorflow/issues/72802\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 6, 29, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459976050, 'issue_id': 2635872003, 'author': 'delray', 'body': ""@tilakrayal \r\nI don't have an issue running inference from Movenet - my problem is building the model embedding using `tf.kerasInput(shape=()51)` \r\nthe code and examples you linked to in the gist [here ](https://github.com/tensorflow/tensorflow/issues/79435#issuecomment-2458818165) doesn't address my issue.  Are you able to run the notebook in my original request at the top?\r\n[https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=1Pte6b1bgWKv](url)"", 'created_at': datetime.datetime(2024, 11, 6, 14, 57, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462717190, 'issue_id': 2635872003, 'author': 'gaikwadrahul8', 'body': ""Hi, @delray \r\n\r\nThank you for bringing this issue to our attention, I was trying to replicate similar behavior from my end but I'm getting different error message `ValueError: Mmap of '41' at offset '0' failed with error '22'.` because model is downloading successfully but with `0 bytes` even I cross verified by downloading the `movenet_thunder.tflite` model file and size is showing 0 bytes so at the moment it's seems like temporary issue due to internal server and it will fix soon once it got resolved I'll try to replicate the same issue which you mentioned in the issue template\r\n\r\nFor reference I've added screenshot below :\r\n\r\n![image](https://github.com/user-attachments/assets/23322711-b9c1-447d-b2f4-841980be8568)\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 7, 16, 41, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462779625, 'issue_id': 2635872003, 'author': 'delray', 'body': ""> Hi, @delray\r\n> \r\n> Thank you for bringing this issue to our attention, I was trying to replicate similar behavior from my end but I'm getting different error message `ValueError: Mmap of '41' at offset '0' failed with error '22'.` because model is downloading successfully but with `0 bytes` even I cross verified by downloading the `movenet_thunder.tflite` model file and size is showing 0 bytes so at the moment it's seems like temporary issue due to internal server and it will fix soon once it got resolved I'll try to replicate the same issue which you mentioned in the issue template\r\n> \r\n> For reference I've added screenshot below :\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/115997457/384055500-23322711-b9c1-447d-b2f4-841980be8568.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA5OTg4ODIsIm5iZiI6MTczMDk5ODU4MiwicGF0aCI6Ii8xMTU5OTc0NTcvMzg0MDU1NTAwLTIzMzIyNzExLWI5YzEtNDQ3ZC1iMmY0LTg0MTk4MGJlODU2OC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwN1QxNjU2MjJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hYmI1Y2IzM2M5MDM1MTEyNzJhNzYxYTA4NTJiYTY2ZjE5ZTVkYjcyMTQ1NDEwOGIxMTYyZDI3YWEwYjM4NDdmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.IjVXLwQOQHznNwIv-66GimQ9TYtU1KAP4ol6Cob7jms)\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nHi @gaikwadrahul8 , \r\n\r\nThe movenet_thunderfile has been moved. \r\n\r\nI solved this problem by downloading the movenet_thunder.tflite manually from here: https://www.kaggle.com/models/google/movenet/tensorFlow2/singlepose-thunder and renaming it appropriately as in the code. After doing that I could do the inference on a simple image but couldnt build the model with the tf.keras as mentioned above."", 'created_at': datetime.datetime(2024, 11, 7, 17, 4, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462936594, 'issue_id': 2635872003, 'author': 'gaikwadrahul8', 'body': 'Hi, @delray \r\n\r\nThank you for guiding me, now I\'m able to replicate the same behavior from my end after downloading the model manually from Kaggle platform so we\'ll have to dig more into this issue and will update you, thank you for bringing this issue to our attention\r\n\r\n**Error log for reference :**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-29-f905a19927b3>](https://localhost:8080/#) in <cell line: 2>()\r\n      1 # Define the model\r\n----> 2 inputs = tf.keras.Input(shape=(51))\r\n      3 embedding = landmarks_to_embedding(inputs)\r\n      4 \r\n      5 layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py](https://localhost:8080/#) in standardize_shape(shape)\r\n    528             raise ValueError(""Undefined shapes are not supported."")\r\n    529         if not hasattr(shape, ""__iter__""):\r\n--> 530             raise ValueError(f""Cannot convert \'{shape}\' to a shape."")\r\n    531         if config.backend() == ""tensorflow"":\r\n    532             if isinstance(shape, tf.TensorShape):\r\n\r\nValueError: Cannot convert \'51\' to a shape.\r\n```\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 7, 18, 23, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504030348, 'issue_id': 2635872003, 'author': 'delray', 'body': 'Bumping this up - has there been any progress in finding this issue?', 'created_at': datetime.datetime(2024, 11, 27, 14, 32, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549676946, 'issue_id': 2635872003, 'author': 'delray', 'body': 'bumping again, are there any ideas regarding this?', 'created_at': datetime.datetime(2024, 12, 17, 21, 30, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561942795, 'issue_id': 2635872003, 'author': 'Mxk123456', 'body': '> Hi, @delray\r\n> \r\n> Thank you for guiding me, now I\'m able to replicate the same behavior from my end after downloading the model manually from Kaggle platform so we\'ll have to dig more into this issue and will update you, thank you for bringing this issue to our attention\r\n> \r\n> **Error log for reference :**\r\n> \r\n> ```\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> [<ipython-input-29-f905a19927b3>](https://localhost:8080/#) in <cell line: 2>()\r\n>       1 # Define the model\r\n> ----> 2 inputs = tf.keras.Input(shape=(51))\r\n>       3 embedding = landmarks_to_embedding(inputs)\r\n>       4 \r\n>       5 layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\r\n> \r\n> 2 frames\r\n> [/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py](https://localhost:8080/#) in standardize_shape(shape)\r\n>     528             raise ValueError(""Undefined shapes are not supported."")\r\n>     529         if not hasattr(shape, ""__iter__""):\r\n> --> 530             raise ValueError(f""Cannot convert \'{shape}\' to a shape."")\r\n>     531         if config.backend() == ""tensorflow"":\r\n>     532             if isinstance(shape, tf.TensorShape):\r\n> \r\n> ValueError: Cannot convert \'51\' to a shape.\r\n> ```\r\n> \r\n> Thank you for your cooperation and patience.\r\n\r\nHow to avoid this problem', 'created_at': datetime.datetime(2024, 12, 25, 16, 19, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561964411, 'issue_id': 2635872003, 'author': 'delray', 'body': 'The shape error? I am hoping to find the solution as well. If so I will post it here. \n\nIf you are talking about the model downloading - see the post above for the correct link.', 'created_at': datetime.datetime(2024, 12, 25, 17, 56, 18, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-11-06 06:29:46 UTC): @delray,
I tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.

https://github.com/tensorflow/tensorflow/issues/72202#issuecomment-2245024521
https://github.com/tensorflow/tensorflow/issues/70841
https://github.com/tensorflow/tensorflow/issues/72802

Thank you!

delray (Issue Creator) on (2024-11-06 14:57:54 UTC): @tilakrayal 
I don't have an issue running inference from Movenet - my problem is building the model embedding using `tf.kerasInput(shape=()51)` 
the code and examples you linked to in the gist [here ](https://github.com/tensorflow/tensorflow/issues/79435#issuecomment-2458818165) doesn't address my issue.  Are you able to run the notebook in my original request at the top?
[https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=1Pte6b1bgWKv](url)

gaikwadrahul8 (Assginee) on (2024-11-07 16:41:48 UTC): Hi, @delray 

Thank you for bringing this issue to our attention, I was trying to replicate similar behavior from my end but I'm getting different error message `ValueError: Mmap of '41' at offset '0' failed with error '22'.` because model is downloading successfully but with `0 bytes` even I cross verified by downloading the `movenet_thunder.tflite` model file and size is showing 0 bytes so at the moment it's seems like temporary issue due to internal server and it will fix soon once it got resolved I'll try to replicate the same issue which you mentioned in the issue template

For reference I've added screenshot below :

![image](https://github.com/user-attachments/assets/23322711-b9c1-447d-b2f4-841980be8568)

Thank you for your cooperation and patience.

delray (Issue Creator) on (2024-11-07 17:04:23 UTC): Hi @gaikwadrahul8 , 

The movenet_thunderfile has been moved. 

I solved this problem by downloading the movenet_thunder.tflite manually from here: https://www.kaggle.com/models/google/movenet/tensorFlow2/singlepose-thunder and renaming it appropriately as in the code. After doing that I could do the inference on a simple image but couldnt build the model with the tf.keras as mentioned above.

gaikwadrahul8 (Assginee) on (2024-11-07 18:23:07 UTC): Hi, @delray 

Thank you for guiding me, now I'm able to replicate the same behavior from my end after downloading the model manually from Kaggle platform so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention

**Error log for reference :**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-29-f905a19927b3>](https://localhost:8080/#) in <cell line: 2>()
      1 # Define the model
----> 2 inputs = tf.keras.Input(shape=(51))
      3 embedding = landmarks_to_embedding(inputs)
      4 
      5 layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)

2 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py](https://localhost:8080/#) in standardize_shape(shape)
    528             raise ValueError(""Undefined shapes are not supported."")
    529         if not hasattr(shape, ""__iter__""):
--> 530             raise ValueError(f""Cannot convert '{shape}' to a shape."")
    531         if config.backend() == ""tensorflow"":
    532             if isinstance(shape, tf.TensorShape):

ValueError: Cannot convert '51' to a shape.
```

Thank you for your cooperation and patience.

delray (Issue Creator) on (2024-11-27 14:32:19 UTC): Bumping this up - has there been any progress in finding this issue?

delray (Issue Creator) on (2024-12-17 21:30:40 UTC): bumping again, are there any ideas regarding this?

Mxk123456 on (2024-12-25 16:19:21 UTC): How to avoid this problem

delray (Issue Creator) on (2024-12-25 17:56:18 UTC): The shape error? I am hoping to find the solution as well. If so I will post it here. 

If you are talking about the model downloading - see the post above for the correct link.

"
2635208762,issue,closed,completed,Problems with EfficientNet v2 b0 inference in tf lite format,"### 1. System information

- OS Platform and Distribution: Windows 11
- TensorFlow installation : pip 
- TensorFlow library: 2.18.0

### 2. Code

import tensorflow as tf

input_shape = (224, 224, 3)
inputs = tf.keras.Input(shape=input_shape)

model = tf.keras.applications.EfficientNetv2b0(
    include_top=True,
    weights='imagenet',
    input_tensor=inputs
)

full_model = tf.keras.Model(inputs=inputs, outputs=model.outputs)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(full_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True)

with open('EfficientNet_float32.tflite', 'wb') as f:
    f.write(tflite_model)

print(""TFLite model saved successfully."")

- When I run inference in python, the model outputs the correct solutions. But model is much slower than in the onnx format . Results for TF lite: 0.145 for onnx: 0.0108   CPU: 6-core AMD Risen 5 7500F
- When I run inference in Kotlin, the model outputs are incorrect. Model always gives the same wrong classes with less than 0.05 probs like nematode, microphone, matchstick . Also i can`t run inference on GPU or with using NNAPI on mobile phone. 
I switched one of default model in this example from tf repo: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android

I used tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True) and got it:  Your model looks compatible with GPU delegate on TFLite runtime version 2.18.0.
This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.

",feff2,2024-11-05 11:36:03+00:00,['gaikwadrahul8'],2024-11-28 02:06:38+00:00,2024-11-28 02:06:38+00:00,https://github.com/tensorflow/tensorflow/issues/79422,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.18', '')]","[{'comment_id': 2462466531, 'issue_id': 2635208762, 'author': 'gaikwadrahul8', 'body': 'Hi, @feff2 \r\n\r\nThank you for bringing this issue to our attention, I see this PR https://github.com/tensorflow/tensorflow/pull/74830 was merged on September 12, 2024 for Deprecating GPU compatibility experimental feature `from both tf/compiler/mlir/lite:flatbuffer_export and tf/lite/python/analyzer_wrapper:model_analyzer` but still we are getting below messages \r\n\r\n```\r\nYour model looks compatible with GPU delegate on TFLite runtime version 2.18.0-rc0.\r\nThis does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.\r\n```\r\nYou mentioned that the inference time for the TFLite model is significantly slower (0.145 seconds) compared to the ONNX model (0.0108 seconds). This could be due to several factors like model optimization so ensure that you are using optimizations effectively. You can try using quantization techniques to reduce the model size and improve inference speed. Please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) and also check if your CPU is being fully utilized. Sometimes the model may not be optimized for the specific hardware you are using.\r\n\r\nThe model outputs incorrect classes with low probabilities when run in Kotlin. This could be due to input preprocessing so ensure that the input images are preprocessed in the same way as during training. This includes resizing, normalization and any other transformations.\r\n\r\nIf possible could you please help us with your Github repo replicate similar behavior along with complete steps which will be great to investigate this issue further ?\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 7, 15, 4, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2464476322, 'issue_id': 2635208762, 'author': 'feff2', 'body': 'Hi @gaikwadrahul8 \r\nI use this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\nI changed ImageClassifierHelper\r\n[ImageClassifierHelper.zip](https://github.com/user-attachments/files/17677754/ImageClassifierHelper.zip)\r\nI use the model that was created by the following code:\r\nimport tensorflow as tf\r\n\r\ninput_shape = (224, 224, 3)\r\ninputs = tf.keras.Input(shape=input_shape)\r\n\r\nmodel = tf.keras.applications.EfficientNetv2b0(\r\ninclude_top=True,\r\nweights=\'imagenet\',\r\ninput_tensor=inputs\r\n)\r\n\r\nfull_model = tf.keras.Model(inputs=inputs, outputs=model.outputs)\r\n\r\nmodel.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(full_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\ntf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True)\r\n\r\nwith open(\'EfficientNet_float32.tflite\', \'wb\') as f:\r\nf.write(tflite_model)\r\n\r\nprint(""TFLite model saved successfully."")\r\nand  added metadata according to this documentation: https://ai.google.dev/edge/litert/models/metadata?hl=en', 'created_at': datetime.datetime(2024, 11, 8, 11, 29, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469924237, 'issue_id': 2635208762, 'author': 'gaikwadrahul8', 'body': ""Hi, @feff2\r\n\r\nI'm able to create model successfully with above provided code snippet for reference here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/88a1e4f2dc4ad41c6e945e171ee15c1b/tflite-issue-79422.ipynb), if possible could you please help us with your Github repo (along with complete steps) to replicate similar behavior from our end which will help us to investigate this issue further ?\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 12, 8, 47, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913594, 'issue_id': 2635208762, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124113, 'issue_id': 2635208762, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 37, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-07 15:04:43 UTC): Hi, @feff2 

Thank you for bringing this issue to our attention, I see this PR https://github.com/tensorflow/tensorflow/pull/74830 was merged on September 12, 2024 for Deprecating GPU compatibility experimental feature `from both tf/compiler/mlir/lite:flatbuffer_export and tf/lite/python/analyzer_wrapper:model_analyzer` but still we are getting below messages 

```
Your model looks compatible with GPU delegate on TFLite runtime version 2.18.0-rc0.
This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.
```
You mentioned that the inference time for the TFLite model is significantly slower (0.145 seconds) compared to the ONNX model (0.0108 seconds). This could be due to several factors like model optimization so ensure that you are using optimizations effectively. You can try using quantization techniques to reduce the model size and improve inference speed. Please refer this [official documentation](https://ai.google.dev/edge/litert/models/model_optimization) and also check if your CPU is being fully utilized. Sometimes the model may not be optimized for the specific hardware you are using.

The model outputs incorrect classes with low probabilities when run in Kotlin. This could be due to input preprocessing so ensure that the input images are preprocessed in the same way as during training. This includes resizing, normalization and any other transformations.

If possible could you please help us with your Github repo replicate similar behavior along with complete steps which will be great to investigate this issue further ?

Thank you for your cooperation and patience.

feff2 (Issue Creator) on (2024-11-08 11:29:35 UTC): Hi @gaikwadrahul8 
I use this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android
I changed ImageClassifierHelper
[ImageClassifierHelper.zip](https://github.com/user-attachments/files/17677754/ImageClassifierHelper.zip)
I use the model that was created by the following code:
import tensorflow as tf

input_shape = (224, 224, 3)
inputs = tf.keras.Input(shape=input_shape)

model = tf.keras.applications.EfficientNetv2b0(
include_top=True,
weights='imagenet',
input_tensor=inputs
)

full_model = tf.keras.Model(inputs=inputs, outputs=model.outputs)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(full_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True)

with open('EfficientNet_float32.tflite', 'wb') as f:
f.write(tflite_model)

print(""TFLite model saved successfully."")
and  added metadata according to this documentation: https://ai.google.dev/edge/litert/models/metadata?hl=en

gaikwadrahul8 (Assginee) on (2024-11-12 08:47:07 UTC): Hi, @feff2

I'm able to create model successfully with above provided code snippet for reference here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/88a1e4f2dc4ad41c6e945e171ee15c1b/tflite-issue-79422.ipynb), if possible could you please help us with your Github repo (along with complete steps) to replicate similar behavior from our end which will help us to investigate this issue further ?

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-21 02:04:03 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-28 02:06:37 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2635195653,issue,open,,Current LiteRT Android dependencies in documentation look broken,"I think after the recent TensorflowLite rename to LiteRT some pages in documentation where renamed incorrectly and are currently very confusing.
For a particular example, see this: https://ai.google.dev/edge/litert/android/gpu
- The docs say to add `com.google.ai.edge.litert:litert-gpu` and `com.google.ai.edge.litert:litert-gpu-api` with versions `2.X.Y`, which do not exist (current latest version is `1.0.1`), to a toml version catalog. 
- In the next paragraph it also switches into using gradle files instead of toml to declare other dependencies, which I found somewhat confusing.
- Later, in [standalone setup](https://ai.google.dev/edge/litert/android/gpu#add_project_dependencies_2), it says to include `com.google.ai.edge.litert:litert-gpu-delegate-plugin` dependency, which does not exist and also follows with a code snipped supposedly showing how to include it, but it shows other dependencies.
- [Other places](https://ai.google.dev/edge/litert/android/metadata/codegen#acceleration) too include dependencies with incorrect versions, like `com.google.ai.edge.litert:litert-gpu:2.3.0`

I just happend to start working with LiteRT for Android right now and found it very difficult to distinguish which parts of documentation are outdated and which aren't.",vykintazo,2024-11-05 11:29:22+00:00,['gaikwadrahul8'],2024-11-12 07:24:17+00:00,,https://github.com/tensorflow/tensorflow/issues/79421,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue')]","[{'comment_id': 2469778779, 'issue_id': 2635195653, 'author': 'gaikwadrahul8', 'body': ""Hi, @vykintazo \r\n\r\nI apologize for the delayed response, thank you for bringing this issue to our attention I'll have look into this issue and will update you. \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 12, 7, 24, 2, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-12 07:24:02 UTC): Hi, @vykintazo 

I apologize for the delayed response, thank you for bringing this issue to our attention I'll have look into this issue and will update you. 

Thank you for your cooperation and patience.

"
2634522436,issue,closed,completed,h5py module not found ,"ModuleNotFoundError                       Traceback (most recent call last)
/tmp/ipykernel_292199/3905199583.py in <module>
      2 import os
      3 import numpy as np
----> 4 import h5py
      5 # import opencv as cv2
      6 # verify mintpy install

ModuleNotFoundError: No module named 'h5py'
this error is faced while using different versions of python3 and mintpy for sbas processing using Mintpy",Pavithra-sl,2024-11-05 06:13:23+00:00,['Venkat6871'],2024-11-23 02:02:40+00:00,2024-11-23 02:02:37+00:00,https://github.com/tensorflow/tensorflow/issues/79381,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2461406070, 'issue_id': 2634522436, 'author': 'Venkat6871', 'body': 'Hi **@Pavithra-sl** ,\r\nApologies for the delay, and thank you for raising your concern here. The error you are facing is likely because Python cannot locate the h5py library. Could you please check everything is installed properly? Let us know if the issue persists.\r\n\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 9, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461417167, 'issue_id': 2634522436, 'author': 'Pavithra-sl', 'body': 'Yes, it is working.\r\n\r\n\r\nOn Thu, 7 Nov 2024 at 11:39, Venkat6871 ***@***.***> wrote:\r\n\r\n> Hi ***@***.*** <https://github.com/Pavithra-sl>* ,\r\n> Apologies for the delay, and thank you for raising your concern here. The\r\n> error you are facing is likely because Python cannot locate the h5py\r\n> library. Could you please check everything is installed properly? Let us\r\n> know if the issue persists.\r\n>\r\n> We see that the issue template\r\n> <https://github.com/tensorflow/tensorflow/issues/new/choose> has not been\r\n> filled, could you please do so as it helps us analyze the issue [tf\r\n> version, steps followed before you ran into this error or stand alone\r\n> code/colab gist to reproduce the issue faced.\r\n> Thank you!\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/79381#issuecomment-2461406070>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BMVPXU24CPGIK5ICJJZ5IX3Z7L73NAVCNFSM6AAAAABRFY5SCWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINRRGQYDMMBXGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 11, 7, 6, 19, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461424100, 'issue_id': 2634522436, 'author': 'Venkat6871', 'body': 'Hi **@Pavithra-sl** ,\r\nGlad to see your issue is resolved! Please feel free to close the issue if everything is working as expected.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 25, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461440797, 'issue_id': 2634522436, 'author': 'Pavithra-sl', 'body': 'Is there any specific module for doing PSInSAR for deformation monitoring\r\n\r\n\r\n\r\nOn Thu, 7 Nov 2024 at 11:56, Venkat6871 ***@***.***> wrote:\r\n\r\n> Hi ***@***.*** <https://github.com/Pavithra-sl>* ,\r\n> Glad to see your issue is resolved! Please feel free to close the issue if\r\n> everything is working as expected.\r\n> Thank you!\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/79381#issuecomment-2461424100>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BMVPXU3QBOVF6Z5LAF77ABTZ7MBYZAVCNFSM6AAAAABRFY5SCWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINRRGQZDIMJQGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 11, 7, 6, 40, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461459379, 'issue_id': 2634522436, 'author': 'Venkat6871', 'body': 'Hi **@Pavithra-sl** ,\r\nSorry, it looks like this issue is not related to TensorFlow. Please raise your query in the relevant repository. If everything is working as expected based on the information above, feel free to close the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 55, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805290, 'issue_id': 2634522436, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212105, 'issue_id': 2634522436, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212144, 'issue_id': 2634522436, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79381"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79381"">No</a>', 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 39, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-07 06:09:35 UTC): Hi **@Pavithra-sl** ,
Apologies for the delay, and thank you for raising your concern here. The error you are facing is likely because Python cannot locate the h5py library. Could you please check everything is installed properly? Let us know if the issue persists.

We see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.
Thank you!

Pavithra-sl (Issue Creator) on (2024-11-07 06:19:56 UTC): Yes, it is working.


On Thu, 7 Nov 2024 at 11:39, Venkat6871 ***@***.***> wrote:

Venkat6871 (Assginee) on (2024-11-07 06:25:56 UTC): Hi **@Pavithra-sl** ,
Glad to see your issue is resolved! Please feel free to close the issue if everything is working as expected.
Thank you!

Pavithra-sl (Issue Creator) on (2024-11-07 06:40:10 UTC): Is there any specific module for doing PSInSAR for deformation monitoring



On Thu, 7 Nov 2024 at 11:56, Venkat6871 ***@***.***> wrote:

Venkat6871 (Assginee) on (2024-11-07 06:55:35 UTC): Hi **@Pavithra-sl** ,
Sorry, it looks like this issue is not related to TensorFlow. Please raise your query in the relevant repository. If everything is working as expected based on the information above, feel free to close the issue.
Thank you!

github-actions[bot] on (2024-11-15 02:05:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:36 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-23 02:02:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79381"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79381"">No</a>

"
2634374985,issue,open,,Create a trainable tensorflow or LiteRT (with signatures) graph from a frozen tensorflow model,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am working with a frozen TensorFlow model (saved as a .pb file) and am exploring ways to make it trainable again, in its original TensorFlow format and eventually a LiteRT model with [signatures to train](https://ai.google.dev/edge/litert/models/ondevice_training).

The goal is to restore training capabilities (such as fine-tuning or continued training) from a model that has already been frozen. I would appreciate guidance on how to approach this scenario.

### Key Points:
#### Frozen TensorFlow Model:
I have a TensorFlow model that was previously trained and saved in its frozen state (as a .pb file). This model no longer contains trainable variables, as they have been converted into constants. However, I would like to continue training this model on new data or fine-tune it for a different task.

**Question 1**: What is the recommended approach to restore a frozen TensorFlow model to a state where it can be trained or fine-tuned again? Specifically, how do I extract the original model architecture and variables from the frozen graph to make it trainable once more?

**Question 2**: If the frozen model has layers that are not needed for retraining, how can I unfreeze and selectively fine-tune only some parts of the model while keeping others frozen?

#### LiteRT Model:
I am also exploring the possibility of taking a LiteRT model that was previously trained & converted to LiteRT, and then restoring its training capability. 

**Question 3**: Is it possible to convert a TensorFlow Lite model back into TensorFlow (with the appropriate weights) and add appropriate training signatures and then convert it back to a trainable LiteRT model? If so, what are the best practices, and any potential challenges involved in such a conversion?

**Question 4**: If not, and I want to fine-tune a model that has been converted to LiteRT, would it be better to work with the original TensorFlow model and re-convert it to LiteRT after training, rather than trying to restore training from the LiteRT version?

#### Expected Outcome:
Any guidance on the best approach to make a frozen model trainable again (whether in TensorFlow or TensorFlow Lite) would be greatly appreciated. Specifically, Im seeking clarity on:

- How to restore training capability from a frozen .pb file in TensorFlow.
- How to manage layer freezing or unfreezing during fine-tuning.
- Whether a TensorFlow Lite model can be retrained, or if it's more efficient to work with the TensorFlow version instead.

### Standalone code to reproduce the issue

```shell
_
```


### Relevant log output

_No response_",AdwaithAnand,2024-11-05 04:20:28+00:00,['gaikwadrahul8'],2024-11-25 14:10:12+00:00,,https://github.com/tensorflow/tensorflow/issues/79377,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2462399848, 'issue_id': 2634374985, 'author': 'gaikwadrahul8', 'body': 'Hi, @AdwaithAnand \r\n\r\nI apologize for the delayed response, To confirm did you try something like mentioned in this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/94b16a1f4c1675d7e374f3cb27d551e4/tflite-issue-79377.ipynb) for restoring frozen TensorFlow model for training and selectively unfreezing and fine-tuning parts of the model ? \r\n\r\nJust to confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version please ?\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 7, 14, 37, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472605450, 'issue_id': 2634374985, 'author': 'AdwaithAnand', 'body': 'Hi @gaikwadrahul8,\r\nI tried what you\'ve mentioned in the gist-file, but am facing an issue in the step to create the keras model from the frozen model in the line `model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)`.\r\n\r\nThe following is the error I\'m facing\r\n```\r\nValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(""input_1:0"", shape=(None, 3, 224, 224), dtype=float32)\r\n```\r\nCan you please help with this?\r\n\r\nSorry for the delayed response, I was trying out a few things to confirm that the issue was not due to the frozengraph I\'m working with. I\'m working with a resnet18 frozen model.', 'created_at': datetime.datetime(2024, 11, 13, 6, 50, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482146379, 'issue_id': 2634374985, 'author': 'gaikwadrahul8', 'body': ""Hi, @AdwaithAnand \r\n\r\nI apologize for the delayed response, if possible could you please check the the input tensor shape matches the model's expected input and also verify the tensor names are correct and also check if any preprocessing is needed ? you can see model architecture by loading your model here https://netron.app/ \r\n\r\nCould you please confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 18, 7, 26, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485422464, 'issue_id': 2634374985, 'author': 'AdwaithAnand', 'body': 'Hi @gaikwadrahul8,\r\nThe input tensor shape does match the model\'s expected input. The tensor names also seems to be correct.\r\n\r\nThe model was previously trained and saved using Tensorflow 2.x.\r\n\r\nAs a sample for testing, the following is the code I used to create the frozen model for mobilenetV2.\r\n\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.applications import MobileNetV2\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\ndef get_mobilenet_model():\r\n    base_model = MobileNetV2(weights=\'imagenet\', include_top=True,\r\n                input_shape=(224, 224, 3))\r\n    base_model.trainable = False\r\n    inputs = Input(shape=(224, 224, 3))\r\n    x = experimental.preprocessing.Rescaling(1./127.5, offset=-1)(inputs)\r\n    outputs=base_model(x, training=False)\r\n    model = tf.keras.Model(inputs, outputs)\r\n    return model\r\n\r\nmodel = get_mobilenet_model()\r\n\r\nfull_model = tf.function(lambda x: model(x))\r\nfull_model = full_model.get_concrete_function(\r\n    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=""input_2""))\r\n# Get frozen ConcreteFunction\r\nfrozen_func = convert_variables_to_constants_v2(full_model)\r\nfrozen_func.graph.as_graph_def()\r\nlayers = [op.name for op in frozen_func.graph.get_operations()]\r\n\r\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph,\r\n                  logdir=""./frozen_models"",\r\n                  name=""mobilenet_frozen_graph.pb"",\r\n                  as_text=False)\r\n```\r\n\r\nFollowing is the code I used to make it trainable, based on the reference in gist-file provided\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef load_frozen_model_for_training(frozen_graph_path):\r\n \r\n    with tf.io.gfile.GFile(frozen_graph_path, ""rb"") as f:\r\n        graph_def = tf.compat.v1.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n \r\n    def _imports_graph_def():\r\n        tf.graph_util.import_graph_def(graph_def, name="""")\r\n \r\n    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\r\n    graph = wrapped_import.graph\r\n \r\n    input_tensor = graph.get_tensor_by_name(""input_2:0"") \r\n    output_tensor = graph.get_tensor_by_name(""Identity:0"") \r\n\r\n    # Create a Keras model\r\n    model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\r\n \r\n    # Make the layers trainable\r\n    for layer in model.layers:\r\n        layer.trainable = True\r\n \r\n    return model\r\n\r\n\r\nfrozen_model_path = ""./frozen_models/mobilenet_frozen_graph.pb""\r\ntrainable_model = load_frozen_model_for_training(frozen_model_path)\r\n\r\ntrainable_model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\n\r\nSAVED_MODEL_DIR = ""mobilenet_from_frozen_graph""\r\n\r\ntf.saved_model.save(\r\n    trainable_model,\r\n    SAVED_MODEL_DIR\r\n    )\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter._enable_tflite_resource_variables = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\nopen(""mobilenet_from_frozen.tflite"", \'wb\').write(tflite_model)\r\n```\r\n\r\n\r\nError being faced:\r\n```\r\nValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(""input_2:0"", shape=(None, 224, 224, 3), dtype=float32)\r\n```', 'created_at': datetime.datetime(2024, 11, 19, 11, 13, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498124303, 'issue_id': 2634374985, 'author': 'gaikwadrahul8', 'body': ""Hi, @AdwaithAnand \r\nI apologize for the delayed response, I am able to replicate the same behavior from my end for reference here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/a01794436465e0b54519d2d13b17b780/test-79377.ipynb) so we'll have to dig more into this issue and wil update you\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 25, 14, 10, 2, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-07 14:37:49 UTC): Hi, @AdwaithAnand 

I apologize for the delayed response, To confirm did you try something like mentioned in this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/94b16a1f4c1675d7e374f3cb27d551e4/tflite-issue-79377.ipynb) for restoring frozen TensorFlow model for training and selectively unfreezing and fine-tuning parts of the model ? 

Just to confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version please ?

Thank you for your cooperation and patience.

AdwaithAnand (Issue Creator) on (2024-11-13 06:50:20 UTC): Hi @gaikwadrahul8,
I tried what you've mentioned in the gist-file, but am facing an issue in the step to create the keras model from the frozen model in the line `model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)`.

The following is the error I'm facing
```
ValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(""input_1:0"", shape=(None, 3, 224, 224), dtype=float32)
```
Can you please help with this?

Sorry for the delayed response, I was trying out a few things to confirm that the issue was not due to the frozengraph I'm working with. I'm working with a resnet18 frozen model.

gaikwadrahul8 (Assginee) on (2024-11-18 07:26:30 UTC): Hi, @AdwaithAnand 

I apologize for the delayed response, if possible could you please check the the input tensor shape matches the model's expected input and also verify the tensor names are correct and also check if any preprocessing is needed ? you can see model architecture by loading your model here https://netron.app/ 

Could you please confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version ? 

Thank you for your cooperation and patience.

AdwaithAnand (Issue Creator) on (2024-11-19 11:13:35 UTC): Hi @gaikwadrahul8,
The input tensor shape does match the model's expected input. The tensor names also seems to be correct.

The model was previously trained and saved using Tensorflow 2.x.

As a sample for testing, the following is the code I used to create the frozen model for mobilenetV2.

``` python
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import *
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.tools import freeze_graph

def get_mobilenet_model():
    base_model = MobileNetV2(weights='imagenet', include_top=True,
                input_shape=(224, 224, 3))
    base_model.trainable = False
    inputs = Input(shape=(224, 224, 3))
    x = experimental.preprocessing.Rescaling(1./127.5, offset=-1)(inputs)
    outputs=base_model(x, training=False)
    model = tf.keras.Model(inputs, outputs)
    return model

model = get_mobilenet_model()

full_model = tf.function(lambda x: model(x))
full_model = full_model.get_concrete_function(
    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=""input_2""))
# Get frozen ConcreteFunction
frozen_func = convert_variables_to_constants_v2(full_model)
frozen_func.graph.as_graph_def()
layers = [op.name for op in frozen_func.graph.get_operations()]

tf.io.write_graph(graph_or_graph_def=frozen_func.graph,
                  logdir=""./frozen_models"",
                  name=""mobilenet_frozen_graph.pb"",
                  as_text=False)
```

Following is the code I used to make it trainable, based on the reference in gist-file provided

```python
import tensorflow as tf

def load_frozen_model_for_training(frozen_graph_path):
 
    with tf.io.gfile.GFile(frozen_graph_path, ""rb"") as f:
        graph_def = tf.compat.v1.GraphDef()
        graph_def.ParseFromString(f.read())
 
    def _imports_graph_def():
        tf.graph_util.import_graph_def(graph_def, name="""")
 
    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])
    graph = wrapped_import.graph
 
    input_tensor = graph.get_tensor_by_name(""input_2:0"") 
    output_tensor = graph.get_tensor_by_name(""Identity:0"") 

    # Create a Keras model
    model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)
 
    # Make the layers trainable
    for layer in model.layers:
        layer.trainable = True
 
    return model


frozen_model_path = ""./frozen_models/mobilenet_frozen_graph.pb""
trainable_model = load_frozen_model_for_training(frozen_model_path)

trainable_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

SAVED_MODEL_DIR = ""mobilenet_from_frozen_graph""

tf.saved_model.save(
    trainable_model,
    SAVED_MODEL_DIR
    )

converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter._enable_tflite_resource_variables = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open(""mobilenet_from_frozen.tflite"", 'wb').write(tflite_model)
```


Error being faced:
```
ValueError: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: Tensor(""input_2:0"", shape=(None, 224, 224, 3), dtype=float32)
```

gaikwadrahul8 (Assginee) on (2024-11-25 14:10:02 UTC): Hi, @AdwaithAnand 
I apologize for the delayed response, I am able to replicate the same behavior from my end for reference here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/a01794436465e0b54519d2d13b17b780/test-79377.ipynb) so we'll have to dig more into this issue and wil update you

Thank you for your cooperation and patience.

"
2634166972,issue,open,,WSL instruction outdated,"Page URL: https://www.tensorflow.org/install/pip#windows-wsl2

At this point in time Ubuntu 24.04 requires (or, recommends) using a virtual environment to install Python packages. As a result, the following command will no longer work in WSL:

```
python3 -m pip install tensorflow[and-cuda]
```

Below is a summary of commands needed to set up a virtual environment and install Tensorflow with the CUDA libraries.

```
sudo apt install python3-venv

python3 -m venv tf

source ~/tf/bin/activate

pip install tensorflow[and-cuda]
```",bibhas2,2024-11-05 00:44:38+00:00,['gaikwadrahul8'],2024-12-19 12:21:50+00:00,,https://github.com/tensorflow/tensorflow/issues/79370,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('wsl2', 'Windows Subsystem for Linux')]","[{'comment_id': 2461449722, 'issue_id': 2634166972, 'author': 'Venkat6871', 'body': 'Hi **@bibhas2** ,\r\nApologies for the delay. Could you please confirm if you are facing any errors while installation? You can try using either ```python3 -m pip install tensorflow[and-cuda]``` to get CUDA support or ```pip install tensorflow``` . In the latest version, CUDA is detected automatically with the following command: ```pip install tensorflow```.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 47, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470986064, 'issue_id': 2634166972, 'author': 'bibhas2', 'body': 'Hi @Venkat6871 as I have noted in my original bug report ""python3 -m pip"" is no longer the preferred way to install packages in Ubuntu 24.04. I recommend you create a fresh WSL instance in Windoiws 11 and try it out. But, here is more details.\r\n\r\nIf you run:\r\n\r\n```\r\npython3 -m pip install tensorflow[and-cuda]\r\n```\r\n\r\nYou get:\r\n\r\n```\r\n/usr/bin/python3: No module named pip\r\n```\r\n\r\nThis is because ``pip`` is not installed. You can try installing ``pip``.\r\n\r\n```\r\nsudo apt-get install python3-pip\r\n```\r\n\r\nThen try again:\r\n\r\n```\r\npython3 -m pip install tensorflow[and-cuda]\r\n```\r\n\r\nNow you get:\r\n\r\n```\r\nerror: externally-managed-environment\r\n\r\n This environment is externally managed\r\n> To install Python packages system-wide, try apt install\r\n    python3-xyz, where xyz is the package you are trying to\r\n    install.\r\n\r\n    If you wish to install a non-Debian-packaged Python package,\r\n    create a virtual environment using python3 -m venv path/to/venv.\r\n    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n    sure you have python3-full installed.\r\n\r\n    If you wish to install a non-Debian packaged Python application,\r\n    it may be easiest to use pipx install xyz, which will manage a\r\n    virtual environment for you. Make sure you have pipx installed.\r\n\r\n    See /usr/share/doc/python3.12/README.venv for more information.\r\n\r\nnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\nhint: See PEP 668 for the detailed specification.\r\n```\r\n\r\nThis boils down to the fact that global installation of pip packages is deprecated. One should create a virtual environment and install packages there. Here are the steps to do so:\r\n\r\n```\r\nsudo apt install python3-venv\r\n\r\npython3 -m venv tf\r\n\r\nsource ~/tf/bin/activate\r\n\r\npip install tensorflow[and-cuda]\r\n```\r\n\r\nLet me know if this is clear or if you need more information.', 'created_at': datetime.datetime(2024, 11, 12, 16, 26, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553333075, 'issue_id': 2634166972, 'author': 'gaikwadrahul8', 'body': 'Hi, @bibhas2 \r\nI apologize for the delayed response, I was able to replicate the similar behavior from my end on `Ubuntu 24.04` and Your analysis is correct global installation of Python packages with `pip` in system-managed environments is now discouraged due to the potential for breaking system dependencies. `Ubuntu 24.04` adhering to `PEP 668` treats the Python environment as externally managed. This issue is specific to `Ubuntu 24.04` due to its stricter adherence to `PEP 668` and the new policy of treating Python environments as externally managed. In `Ubuntu 22.04` you can still globally install Python packages with pip without encountering the externally managed environment error. \r\n\r\nI have tried on `Ubuntu 22.04` and it\'s working without encountering the externally managed environment error\r\n\r\nI have followed below steps on `Ubuntu 24.04 ` to work as expected\r\n\r\n```\r\n1. sudo apt install python3-venv\r\n2. python3 -m venv ~/tf\r\n3. source ~/tf/bin/activate\r\n4. python3 -m pip install tensorflow[and-cuda]\r\n```\r\n\r\nHere is output log on Ubuntu 22.04 for reference :\r\n\r\n```\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -m pip install tensorflow[and-cuda]\r\nRequirement already satisfied: tensorflow[and-cuda] in ./miniconda3/lib/python3.12/site-packages (2.18.0)\r\nRequirement already satisfied: absl-py>=1.0.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.1.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.6.3)\r\nRequirement already satisfied: flatbuffers>=24.3.25 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.3.25)\r\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.6.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.2.0)\r\nRequirement already satisfied: libclang>=13.0.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (18.1.1)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.4.0)\r\nRequirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.1)\r\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (5.29.2)\r\nRequirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.32.3)\r\nRequirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (72.1.0)\r\nRequirement already satisfied: six>=1.12.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.5.0)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (4.12.2)\r\nRequirement already satisfied: wrapt>=1.11.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.68.1)\r\nRequirement already satisfied: tensorboard<2.19,>=2.18 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.18.0)\r\nRequirement already satisfied: keras>=3.5.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.7.0)\r\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.0.2)\r\nRequirement already satisfied: h5py>=3.11.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.12.1)\r\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.4.1)\r\nRequirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.3.2)\r\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\r\nRequirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\r\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\r\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\r\nRequirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (9.3.0.75)\r\nRequirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.2.3.61)\r\nRequirement already satisfied: nvidia-curand-cu12==10.3.6.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (10.3.6.82)\r\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.6.3.83)\r\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.1.3)\r\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.21.5)\r\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\r\nRequirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\r\nRequirement already satisfied: rich in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (13.9.4)\r\nRequirement already satisfied: namex in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.0.8)\r\nRequirement already satisfied: optree in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.13.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.7.4)\r\nRequirement already satisfied: markdown>=2.6.8 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.7)\r\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (0.7.2)\r\nRequirement already satisfied: werkzeug>=1.0.1 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.1.3)\r\nRequirement already satisfied: MarkupSafe>=2.1.1 in ./miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.0.2)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (2.18.0)\r\nRequirement already satisfied: mdurl~=0.1 in ./miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda]) (0.1.2)\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))""\r\n2024-12-19 10:07:02.036714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734602822.059238   41184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734602822.066361   41184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-19 10:07:02.093021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -c ""import tensorflow as tf; print(tf.__version__)""\r\n2024-12-19 10:08:56.195567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1734602936.216505   41197 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1734602936.222966   41197 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-19 10:08:56.245212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2.18.0\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ \r\n```\r\n**For Ubuntu 24.04** I got exactly same output log which you mentioned in this issue template so we need to update our official instructions documentation specifically for `Ubuntu 24.04` for [Windows WSL2](https://www.tensorflow.org/install/pip#windows-wsl2)\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 19, 10, 18, 53, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-11-07 06:47:53 UTC): Hi **@bibhas2** ,
Apologies for the delay. Could you please confirm if you are facing any errors while installation? You can try using either ```python3 -m pip install tensorflow[and-cuda]``` to get CUDA support or ```pip install tensorflow``` . In the latest version, CUDA is detected automatically with the following command: ```pip install tensorflow```.
Thank you!

bibhas2 (Issue Creator) on (2024-11-12 16:26:18 UTC): Hi @Venkat6871 as I have noted in my original bug report ""python3 -m pip"" is no longer the preferred way to install packages in Ubuntu 24.04. I recommend you create a fresh WSL instance in Windoiws 11 and try it out. But, here is more details.

If you run:

```
python3 -m pip install tensorflow[and-cuda]
```

You get:

```
/usr/bin/python3: No module named pip
```

This is because ``pip`` is not installed. You can try installing ``pip``.

```
sudo apt-get install python3-pip
```

Then try again:

```
python3 -m pip install tensorflow[and-cuda]
```

Now you get:

```
error: externally-managed-environment

 This environment is externally managed
> To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.

    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.

    If you wish to install a non-Debian packaged Python application,
    it may be easiest to use pipx install xyz, which will manage a
    virtual environment for you. Make sure you have pipx installed.

    See /usr/share/doc/python3.12/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
```

This boils down to the fact that global installation of pip packages is deprecated. One should create a virtual environment and install packages there. Here are the steps to do so:

```
sudo apt install python3-venv

python3 -m venv tf

source ~/tf/bin/activate

pip install tensorflow[and-cuda]
```

Let me know if this is clear or if you need more information.

gaikwadrahul8 (Assginee) on (2024-12-19 10:18:53 UTC): Hi, @bibhas2 
I apologize for the delayed response, I was able to replicate the similar behavior from my end on `Ubuntu 24.04` and Your analysis is correct global installation of Python packages with `pip` in system-managed environments is now discouraged due to the potential for breaking system dependencies. `Ubuntu 24.04` adhering to `PEP 668` treats the Python environment as externally managed. This issue is specific to `Ubuntu 24.04` due to its stricter adherence to `PEP 668` and the new policy of treating Python environments as externally managed. In `Ubuntu 22.04` you can still globally install Python packages with pip without encountering the externally managed environment error. 

I have tried on `Ubuntu 22.04` and it's working without encountering the externally managed environment error

I have followed below steps on `Ubuntu 24.04 ` to work as expected

```
1. sudo apt install python3-venv
2. python3 -m venv ~/tf
3. source ~/tf/bin/activate
4. python3 -m pip install tensorflow[and-cuda]
```

Here is output log on Ubuntu 22.04 for reference :

```
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -m pip install tensorflow[and-cuda]
Requirement already satisfied: tensorflow[and-cuda] in ./miniconda3/lib/python3.12/site-packages (2.18.0)
Requirement already satisfied: absl-py>=1.0.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.4.0)
Requirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (5.29.2)
Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.32.3)
Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (72.1.0)
Requirement already satisfied: six>=1.12.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.0)
Requirement already satisfied: termcolor>=1.1.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.5.0)
Requirement already satisfied: typing-extensions>=3.6.6 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.68.1)
Requirement already satisfied: tensorboard<2.19,>=2.18 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.18.0)
Requirement already satisfied: keras>=3.5.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.7.0)
Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.0.2)
Requirement already satisfied: h5py>=3.11.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.12.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.4.1)
Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.3.2)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)
Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)
Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (9.3.0.75)
Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.2.3.61)
Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (10.3.6.82)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.6.3.83)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.1.3)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.21.5)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in ./miniconda3/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)
Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)
Requirement already satisfied: rich in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (13.9.4)
Requirement already satisfied: namex in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.0.8)
Requirement already satisfied: optree in ./miniconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.13.1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.7.4)
Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in ./miniconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.1.3)
Requirement already satisfied: MarkupSafe>=2.1.1 in ./miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.0.2)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in ./miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda]) (0.1.2)
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
2024-12-19 10:07:02.036714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734602822.059238   41184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734602822.066361   41184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-19 10:07:02.093021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ python3 -c ""import tensorflow as tf; print(tf.__version__)""
2024-12-19 10:08:56.195567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734602936.216505   41197 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734602936.222966   41197 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-19 10:08:56.245212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2.18.0
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~$ 
```
**For Ubuntu 24.04** I got exactly same output log which you mentioned in this issue template so we need to update our official instructions documentation specifically for `Ubuntu 24.04` for [Windows WSL2](https://www.tensorflow.org/install/pip#windows-wsl2)

Thank you for your cooperation and patience.

"
2634024575,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"My OS is Windows server which but this is a VM on a VDS - big server.

```
soft Windows [Version 10.0.20348.2762]
(c) Microsoft Corporation. All rights reserved.

C:\Users\Administrator>cd Desktop

C:\Users\Administrator\Desktop>cd max

C:\Users\Administrator\Desktop\MAX>dir
 Volume in drive C has no label.
 Volume Serial Number is BE46-6828

 Directory of C:\Users\Administrator\Desktop\MAX

11/04/2024  02:49 PM    <DIR>          .
11/04/2024  02:15 PM    <DIR>          ..
07/19/2024  04:17 PM             8,187 api.py
07/19/2024  02:49 PM         5,282,219 epoch-100-new-model.keras
07/19/2024  04:30 PM               191 gunicorn_config.py
10/02/2024  11:05 AM    <DIR>          myenv
11/04/2024  02:49 PM    <DIR>          old
10/30/2024  09:33 AM                 0 python
07/19/2024  04:28 PM                23 README.md
10/03/2024  12:26 AM               787 requirements.txt
10/03/2024  01:24 AM               509 run_server.py
11/04/2024  02:03 PM    <DIR>          temp_images
10/03/2024  12:24 AM    <DIR>          tensorflow-env
10/03/2024  12:24 AM    <DIR>          tensorflow-envsource
11/04/2024  02:03 PM    <DIR>          __pycache__
               7 File(s)      5,291,916 bytes
               8 Dir(s)  35,836,018,688 bytes free

C:\Users\Administrator\Desktop\MAX>python api.py
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\Desktop\MAX\api.py"", line 2, in <module>
    import keras
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py"", line 4, in <module>
    from keras.api import DTypePolicy
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\__init__.py"", line 8, in <module>
    from keras.api import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\activations\__init__.py"", line 7, in <module>
    from keras.src.activations import deserialize
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\__init__.py"", line 1, in <module>
    from keras.src import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\__init__.py"", line 3, in <module>
    from keras.src.activations.activations import elu
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\activations.py"", line 1, in <module>
    from keras.src import backend
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\__init__.py"", line 9, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\__init__.py"", line 2, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\dtypes.py"", line 5, in <module>
    from keras.src.backend.common.variables import standardize_dtype
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\variables.py"", line 11, in <module>
    from keras.src.utils.module_utils import tensorflow as tf
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\__init__.py"", line 1, in <module>
    from keras.src.utils.audio_dataset_utils import audio_dataset_from_directory
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\audio_dataset_utils.py"", line 4, in <module>
    from keras.src.utils import dataset_utils
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\dataset_utils.py"", line 9, in <module>
    from keras.src import tree
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\__init__.py"", line 1, in <module>
    from keras.src.tree.tree_api import assert_same_structure
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\tree_api.py"", line 6, in <module>
    from keras.src.tree import optree_impl as tree_impl
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\optree_impl.py"", line 17, in <module>
    from tensorflow.python.trackable.data_structures import ListWrapper
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

C:\Users\Administrator\Desktop\MAX>myenv\Scripts\activate

(myenv) C:\Users\Administrator\Desktop\MAX>python api.py
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\Desktop\MAX\api.py"", line 2, in <module>
    import keras
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py"", line 4, in <module>
    from keras.api import DTypePolicy
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\__init__.py"", line 8, in <module>
    from keras.api import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\activations\__init__.py"", line 7, in <module>
    from keras.src.activations import deserialize
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\__init__.py"", line 1, in <module>
    from keras.src import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\__init__.py"", line 3, in <module>
    from keras.src.activations.activations import elu
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\activations.py"", line 1, in <module>
    from keras.src import backend
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\__init__.py"", line 9, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\__init__.py"", line 2, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\dtypes.py"", line 5, in <module>
    from keras.src.backend.common.variables import standardize_dtype
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\variables.py"", line 11, in <module>
    from keras.src.utils.module_utils import tensorflow as tf
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\__init__.py"", line 1, in <module>
    from keras.src.utils.audio_dataset_utils import audio_dataset_from_directory
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\audio_dataset_utils.py"", line 4, in <module>
    from keras.src.utils import dataset_utils
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\dataset_utils.py"", line 9, in <module>
    from keras.src import tree
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\__init__.py"", line 1, in <module>
    from keras.src.tree.tree_api import assert_same_structure
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\tree_api.py"", line 6, in <module>
    from keras.src.tree import optree_impl as tree_impl
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\optree_impl.py"", line 17, in <module>
    from tensorflow.python.trackable.data_structures import ListWrapper
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

(myenv) C:\Users\Administrator\Desktop\MAX>
(myenv) C:\Users\Administrator\Desktop\MAX>
(myenv) C:\Users\Administrator\Desktop\MAX>python run_server.py
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\Desktop\MAX\run_server.py"", line 1, in <module>
    import api
  File ""C:\Users\Administrator\Desktop\MAX\api.py"", line 2, in <module>
    import keras
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py"", line 4, in <module>
    from keras.api import DTypePolicy
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\__init__.py"", line 8, in <module>
    from keras.api import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\api\activations\__init__.py"", line 7, in <module>
    from keras.src.activations import deserialize
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\__init__.py"", line 1, in <module>
    from keras.src import activations
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\__init__.py"", line 3, in <module>
    from keras.src.activations.activations import elu
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\activations\activations.py"", line 1, in <module>
    from keras.src import backend
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\__init__.py"", line 9, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\__init__.py"", line 2, in <module>
    from keras.src.backend.common.dtypes import result_type
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\dtypes.py"", line 5, in <module>
    from keras.src.backend.common.variables import standardize_dtype
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\backend\common\variables.py"", line 11, in <module>
    from keras.src.utils.module_utils import tensorflow as tf
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\__init__.py"", line 1, in <module>
    from keras.src.utils.audio_dataset_utils import audio_dataset_from_directory
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\audio_dataset_utils.py"", line 4, in <module>
    from keras.src.utils import dataset_utils
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\utils\dataset_utils.py"", line 9, in <module>
    from keras.src import tree
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\__init__.py"", line 1, in <module>
    from keras.src.tree.tree_api import assert_same_structure
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\tree_api.py"", line 6, in <module>
    from keras.src.tree import optree_impl as tree_impl
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\src\tree\optree_impl.py"", line 17, in <module>
    from tensorflow.python.trackable.data_structures import ListWrapper
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```

```
(myenv) C:\Users\Administrator\Desktop\MAX>python --version
Python 3.9.0
```",BaseMax,2024-11-04 22:51:12+00:00,['tilakrayal'],2025-01-03 17:00:56+00:00,2024-12-06 02:07:50+00:00,https://github.com/tensorflow/tensorflow/issues/79362,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2458831344, 'issue_id': 2634024575, 'author': 'tilakrayal', 'body': '@BaseMax,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 6, 41, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206575, 'issue_id': 2634024575, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506954610, 'issue_id': 2634024575, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 29, 2, 6, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521930045, 'issue_id': 2634024575, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521930121, 'issue_id': 2634024575, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79362"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79362"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 2, 7, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569544124, 'issue_id': 2634024575, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 17, 0, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-06 06:41:24 UTC): @BaseMax,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```
https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-11-14 02:01:13 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-29 02:06:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-06 02:07:49 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-06 02:07:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79362"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79362"">No</a>

mihaimaruseac on (2025-01-03 17:00:54 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

"
2633861856,issue,open,,TFLite Interpreter `experimental_preserve_all_tensors` yields different output,"### 1. System information

- OS Platform and Distribution: Pop!_OS 22.04 LTS
- TensorFlow installation: pip package
- TensorFlow library: 2.17.0

### 2. Code



```
import tensorflow as tf
import numpy as np

from ai_edge_litert.interpreter import Interpreter

IMAGE_SHAPE = (10, 10, 1)
IMAGE_SIZE = IMAGE_SHAPE[0]*IMAGE_SHAPE[1]*IMAGE_SHAPE[2]

tf.random.set_seed(0)
np.random.seed(0)
tf.keras.utils.set_random_seed(0)

inp_layer = tf.keras.layers.InputLayer(shape=IMAGE_SHAPE, batch_size=1)
conv_layer = tf.keras.layers.Conv2D(4, (3, 3), strides=(2, 2), padding=""same"", use_bias=True, activation=""relu"")

model = tf.keras.models.Sequential([
  inp_layer,
  conv_layer,
])

w = conv_layer.get_weights()
k = w[0]
b = w[1]

k[:, :, :, 0] = 0
k[:, :, :, 2] = 0
b = np.zeros(shape=w[1].shape)

print(""kernel:\n"", k)
print(""bias:\n"", b)

w[0] = k
w[1] = b
conv_layer.set_weights(w)

def representative_dataset():
  for _ in range(10):
    data = np.random.rand(1, *IMAGE_SHAPE)
    yield [data.astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

#filename = ""example.tflite""
#with open(filename, 'wb') as f:
#  f.write(tflite_quant_model)

#interp = Interpreter(filename)
#exper = Interpreter(filename, experimental_preserve_all_tensors=True)
interp = Interpreter(model_content=tflite_quant_model)
exper = Interpreter(model_content=tflite_quant_model, experimental_preserve_all_tensors=True)


inp=np.zeros(IMAGE_SIZE, dtype=np.int8)
arr = [62, 63, 64, 72, 73, 74, 82, 83, 84]
for i in arr:
  inp[i] = i
t = tf.constant(inp, shape=IMAGE_SHAPE, dtype=tf.int8)

print(""input:\n"", t, ""\n"")

def alloc_and_run(interpreter):
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  interpreter.set_tensor(input_details[0]['index'], [t])
  interpreter.invoke()

  return interpreter.get_tensor(output_details[0]['index'])

out_interp = alloc_and_run(interp)
out_exper = alloc_and_run(exper)

print(""out_interp:\n"", out_interp, ""\n"")
print(""out_exper:\n"", out_exper, ""\n"")

assert(tf.reduce_all(tf.equal(out_interp, out_exper)))
```

### 3. Failure after conversion
Conversion is successful, but interpreter and experimental interpreter yield a different result:
```
out_interp:
 [[[[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -23 -128  -47]
   [-128   28 -128  -61]
   [-128   15 -128  -78]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -12 -128   -5]
   [-128   57 -128  -34]
   [-128   10 -128  -98]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128 -123 -128  -75]
   [-128 -128 -128  -66]
   [-128 -128 -128 -120]
   [-128 -117 -128  -83]
   [-128 -128 -128 -128]]]] 

out_exper:
 [[[[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -23 -128  -47]
   [-128   28 -128  -61]
   [-128   15 -128  -78]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -12 -128   -5]
   [-128   57 -128  -33]
   [-128   10 -128  -98]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128 -123 -128  -75]
   [-128 -128 -128  -66]
   [-128 -128 -128 -120]
   [-128 -117 -128  -83]
   [-128 -128 -128 -128]]]] 

Traceback (most recent call last):
  File ""/home/sri/riptools/scratch5/example.py"", line 79, in <module>
    assert(tf.reduce_all(tf.equal(out_interp, out_exper)))
AssertionError
```
The difference here is:
`out_interp[0][3][1][3] = -33`
`out_exper[0][3][1][3] = -34`
This ""off-by-one"" may occur with other sized input tensors and larger non-zero inputs/weights as well.",sri-cherukuri,2024-11-04 21:14:51+00:00,['gaikwadrahul8'],2024-11-11 05:06:51+00:00,,https://github.com/tensorflow/tensorflow/issues/79355,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2461388062, 'issue_id': 2633861856, 'author': 'Venkat6871', 'body': '@gaikwadrahul8 the issue was able to reproduce the issue on Colab using TF v2.18.0, Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/0def21ab28205a59ee6d1b4647a0c2c9/79355_tf_2-18-0-v.ipynb) here for reference \r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 5, 54, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467246380, 'issue_id': 2633861856, 'author': 'gaikwadrahul8', 'body': ""Hi, @sri-cherukuri \r\n\r\nI apologize for the delayed response, I'm also able to reproduce the same behavior from my end here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/0fa9f4e0aa83cc0eddc992d64146a273/tflite-issue-79355.ipynb) for reference with TensorFlow version **2.17.x** and **2.18.x** so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 11, 5, 6, 19, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-11-07 05:54:42 UTC): @gaikwadrahul8 the issue was able to reproduce the issue on Colab using TF v2.18.0, Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/0def21ab28205a59ee6d1b4647a0c2c9/79355_tf_2-18-0-v.ipynb) here for reference 
Thank you!

gaikwadrahul8 (Assginee) on (2024-11-11 05:06:19 UTC): Hi, @sri-cherukuri 

I apologize for the delayed response, I'm also able to reproduce the same behavior from my end here is [gist-file](https://colab.research.google.com/gist/gaikwadrahul8/0fa9f4e0aa83cc0eddc992d64146a273/tflite-issue-79355.ipynb) for reference with TensorFlow version **2.17.x** and **2.18.x** so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.

Thank you for your cooperation and patience.

"
2633473882,issue,open,,Unable to install TensorFlow: No matching distribution found for TensorFlow!,"When trying to install TensorFlow via pip, I encounter an error stating that no matching distribution can be found. The command I used and the error message are as follows:

```
C:\Users\Enes> pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
Operating System: Windows 10
Python Version: (Python 3.13.0)

I am currently using Python 3.13. Could this be related to compatibility with this specific Python version?

Could you provide guidance on how to resolve this issue, or suggest any compatible alternatives?",bajramienes,2024-11-04 17:54:37+00:00,['tilakrayal'],2025-02-07 06:50:29+00:00,,https://github.com/tensorflow/tensorflow/issues/79349,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues')]","[{'comment_id': 2461396703, 'issue_id': 2633473882, 'author': 'tilakrayal', 'body': '@bajramienes,\r\nThe error which you are facing is due to the mismatch of tensorflow compatible versions. I request the latest tensorflow v2.18 is compatible with the python 3.9-312. Kindly take a look at the official document for the compatible versions.\r\nhttps://www.tensorflow.org/install/source_windows#cpu\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 2, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462189005, 'issue_id': 2633473882, 'author': 'bajramienes', 'body': '@tilakrayal, the latest version, TensorFlow 2.18.0, supports Python versions 3.93.12, while mine is 3.13.0 with pip 24.3.1. I need to wait for the next release of TensorFlow that supports Python 3.13.0.', 'created_at': datetime.datetime(2024, 11, 7, 13, 4, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466730533, 'issue_id': 2633473882, 'author': 'nalkat', 'body': 'All brand new Fedora 41 installations will face this. Please update TF to work with 3.13 default installs where we have no easy choice. Why is this even a problem?', 'created_at': datetime.datetime(2024, 11, 10, 13, 11, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470974864, 'issue_id': 2633473882, 'author': 'atrigupta', 'body': 'the latest version, TensorFlow 2.18.0, supports Python versions 3.93.12, We have to wait to supports Python 3.13.0.', 'created_at': datetime.datetime(2024, 11, 12, 16, 21, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495971756, 'issue_id': 2633473882, 'author': 'bajramienes', 'body': 'Hello @atrigupta, any news about this issue?', 'created_at': datetime.datetime(2024, 11, 24, 12, 21, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503769017, 'issue_id': 2633473882, 'author': 'sridvany', 'body': 'Has anyone solved the problem?', 'created_at': datetime.datetime(2024, 11, 27, 12, 36, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503801389, 'issue_id': 2633473882, 'author': 'thebaptiste', 'body': ""I'm afraid we'll have to wait next year for a new tensorflow release supporting Python 3.13... See [this comment](https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2498148533)\r\nI think the only alternative for the time being is to use Python 3.12.\r\nPython 3.13.0 release is less than 2 months old, we can't ask every package to support it in such a short time... \r\nBy the way, several tensorflow dependencies are not supporting Python 3.13 either.\r\nAs far as I'm concerned, after a new Python major release is out, I wait about 6 months before starting to use it, except for testing purpose."", 'created_at': datetime.datetime(2024, 11, 27, 12, 51, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634958851, 'issue_id': 2633473882, 'author': 'bajramienes', 'body': 'Hello everyone, any updates on this issue?', 'created_at': datetime.datetime(2025, 2, 4, 20, 10, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642079210, 'issue_id': 2633473882, 'author': 'nalkat', 'body': ""> I'm afraid we'll have to wait next year for a new tensorflow release supporting Python 3.13... See [this comment](https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2498148533)\n> I think the only alternative for the time being is to use Python 3.12.\n> Python 3.13.0 release is less than 2 months old, we can't ask every package to support it in such a short time... \n> By the way, several tensorflow dependencies are not supporting Python 3.13 either.\n> As far as I'm concerned, after a new Python major release is out, I wait about 6 months before starting to use it, except for testing purpose.\n\nFor a package as important as this one, it would make more sense if it was released as soon as possible to accommodate a breaking point release. So, unless a major code refactoring was required to move forward with the industry, then the work should have already been completed by now. If not, then the door gets opened pretty wide to have someone else shore up that deficiency and render the package obsolete. \n\nOSes releasing newer versions of major dependencies are not really just choosing to do so without real reason and without fully testing it out. That means the vendor libraries had just as much time to do the same. Those OS vendors certainly don't want to be held back from pushing the evolution of software development forward because one library vendor, even one as important as this, didn't upgrade their offering. If they did that, nothing would ever move forward. \n\nIt's far too easy to stagnate, but by doing so, that action creates the risk of losing market share to those who can't wait an additional 6 months or longer. They might decide that it's in everyone's best interest to just make a new solution and not be forced to use older, less secure OSes. Depending on the actual difficulty to make it compatible , even if no features are added, It's really in everyone's best interest, especially their own, just to do that at minimum and open the blocked gate to continued innovation.\n\nIf the vendor were to keep up in a reasonable time frame, or at least explain why it isn't possible while providing a release map and set a reasonable time frame for opening this massive gate, then everyone wins. 6 months in software development, ESPECIALLY these days, is an eternity. The current climate in which everyone feels compelled to jump in to stay viable demands that kind of company lead us into the future. So, as I said, if one company drops the ball too long, they will likely lose their edge and be replaced. Then regaining the throne will be impossible without some massive breakthrough discovery or featureset to regain the trust they have lost.\n\nOne only has to look at Intel as a prime example for proof of what I am saying, and because of this, I'm guessing NVidia before too long, too. Mr. Huang is pretty smart, though, so I would also guess that the competition will really need to up their game, and they will, even if that includes rewriting a sovereign dependency should it be required. The lack of movement here since I first brought it up is starting to seem like it is."", 'created_at': datetime.datetime(2025, 2, 7, 6, 50, 27, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-07 06:02:36 UTC): @bajramienes,
The error which you are facing is due to the mismatch of tensorflow compatible versions. I request the latest tensorflow v2.18 is compatible with the python 3.9-312. Kindly take a look at the official document for the compatible versions.
https://www.tensorflow.org/install/source_windows#cpu

Thank you!

bajramienes (Issue Creator) on (2024-11-07 13:04:05 UTC): @tilakrayal, the latest version, TensorFlow 2.18.0, supports Python versions 3.93.12, while mine is 3.13.0 with pip 24.3.1. I need to wait for the next release of TensorFlow that supports Python 3.13.0.

nalkat on (2024-11-10 13:11:41 UTC): All brand new Fedora 41 installations will face this. Please update TF to work with 3.13 default installs where we have no easy choice. Why is this even a problem?

atrigupta on (2024-11-12 16:21:49 UTC): the latest version, TensorFlow 2.18.0, supports Python versions 3.93.12, We have to wait to supports Python 3.13.0.

bajramienes (Issue Creator) on (2024-11-24 12:21:22 UTC): Hello @atrigupta, any news about this issue?

sridvany on (2024-11-27 12:36:29 UTC): Has anyone solved the problem?

thebaptiste on (2024-11-27 12:51:32 UTC): I'm afraid we'll have to wait next year for a new tensorflow release supporting Python 3.13... See [this comment](https://github.com/tensorflow/tensorflow/issues/78774#issuecomment-2498148533)
I think the only alternative for the time being is to use Python 3.12.
Python 3.13.0 release is less than 2 months old, we can't ask every package to support it in such a short time... 
By the way, several tensorflow dependencies are not supporting Python 3.13 either.
As far as I'm concerned, after a new Python major release is out, I wait about 6 months before starting to use it, except for testing purpose.

bajramienes (Issue Creator) on (2025-02-04 20:10:23 UTC): Hello everyone, any updates on this issue?

nalkat on (2025-02-07 06:50:27 UTC): For a package as important as this one, it would make more sense if it was released as soon as possible to accommodate a breaking point release. So, unless a major code refactoring was required to move forward with the industry, then the work should have already been completed by now. If not, then the door gets opened pretty wide to have someone else shore up that deficiency and render the package obsolete. 

OSes releasing newer versions of major dependencies are not really just choosing to do so without real reason and without fully testing it out. That means the vendor libraries had just as much time to do the same. Those OS vendors certainly don't want to be held back from pushing the evolution of software development forward because one library vendor, even one as important as this, didn't upgrade their offering. If they did that, nothing would ever move forward. 

It's far too easy to stagnate, but by doing so, that action creates the risk of losing market share to those who can't wait an additional 6 months or longer. They might decide that it's in everyone's best interest to just make a new solution and not be forced to use older, less secure OSes. Depending on the actual difficulty to make it compatible , even if no features are added, It's really in everyone's best interest, especially their own, just to do that at minimum and open the blocked gate to continued innovation.

If the vendor were to keep up in a reasonable time frame, or at least explain why it isn't possible while providing a release map and set a reasonable time frame for opening this massive gate, then everyone wins. 6 months in software development, ESPECIALLY these days, is an eternity. The current climate in which everyone feels compelled to jump in to stay viable demands that kind of company lead us into the future. So, as I said, if one company drops the ball too long, they will likely lose their edge and be replaced. Then regaining the throne will be impossible without some massive breakthrough discovery or featureset to regain the trust they have lost.

One only has to look at Intel as a prime example for proof of what I am saying, and because of this, I'm guessing NVidia before too long, too. Mr. Huang is pretty smart, though, so I would also guess that the competition will really need to up their game, and they will, even if that includes rewriting a sovereign dependency should it be required. The lack of movement here since I first brought it up is starting to seem like it is.

"
2632822275,issue,closed,completed,Installing tensorflow-text triggers a rollback of current's tensorflow version,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

> pip install --upgrade tensorflow
Successfully installed flatbuffers-24.3.25 h5py-3.12.1 ml-dtypes-0.4.1 numpy-2.0.2 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 **tensorflow-2.18.0** tensorflow-intel-2.18.0

> pip install --upgrade tensorflow-text
Successfully installed keras-2.10.0 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 **tensorflow-2.10.1**

### Standalone code to reproduce the issue

```shell
Just type in sequence the 2 command lines written above.
```


### Relevant log output

_No response_",Mark531,2024-11-04 13:28:56+00:00,['Venkat6871'],2024-11-21 02:04:08+00:00,2024-11-21 02:04:06+00:00,https://github.com/tensorflow/tensorflow/issues/79331,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2454926627, 'issue_id': 2632822275, 'author': 'VadisettyRahul', 'body': 'Hi @Mark531 \r\n\r\nThis issue occurs because tensorflow-text has restrictive version dependencies, which leads to a downgrade of tensorflow to version 2.10.1 when it is installed. The documentation says that when installing, note the version of tensorflow you are running, as you must specify the corresponding minor version of TF Text.', 'created_at': datetime.datetime(2024, 11, 4, 14, 53, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455123248, 'issue_id': 2632822275, 'author': 'Mark531', 'body': 'Is this lib deprecated?', 'created_at': datetime.datetime(2024, 11, 4, 16, 11, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455242922, 'issue_id': 2632822275, 'author': 'VadisettyRahul', 'body': '@Mark531 I searched and saw that the library is active and in use, especially for cases where text processing is essential for machine learning models with tensorflow., but it is delayed to maintain alignment with newer versions of tensorflow. That is, updates to the tensorflow-text library are not updated immediately.', 'created_at': datetime.datetime(2024, 11, 4, 17, 3, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455358070, 'issue_id': 2632822275, 'author': 'Mark531', 'body': 'Ok, so it is currently usable with tensorflow 2.10.1.', 'created_at': datetime.datetime(2024, 11, 4, 17, 55, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457453793, 'issue_id': 2632822275, 'author': 'VadisettyRahul', 'body': '@Venkat6871 need help?', 'created_at': datetime.datetime(2024, 11, 5, 15, 19, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458811473, 'issue_id': 2632822275, 'author': 'Venkat6871', 'body': 'Hi **@Mark531** ,\r\nApologies for the delay, and thank you for raising your concern here. Could you please confirm where exactly you are facing the issuewith TensorFlow or with TensorFlow-Text triggers? If it is with TensorFlow-Text triggers, please raise the issue in that particular [repository](https://github.com/tensorflow/text/issues).\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 6, 23, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206608, 'issue_id': 2632822275, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913649, 'issue_id': 2632822275, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913702, 'issue_id': 2632822275, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79331"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79331"">No</a>', 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 7, tzinfo=datetime.timezone.utc)}]","VadisettyRahul on (2024-11-04 14:53:54 UTC): Hi @Mark531 

This issue occurs because tensorflow-text has restrictive version dependencies, which leads to a downgrade of tensorflow to version 2.10.1 when it is installed. The documentation says that when installing, note the version of tensorflow you are running, as you must specify the corresponding minor version of TF Text.

Mark531 (Issue Creator) on (2024-11-04 16:11:35 UTC): Is this lib deprecated?

VadisettyRahul on (2024-11-04 17:03:12 UTC): @Mark531 I searched and saw that the library is active and in use, especially for cases where text processing is essential for machine learning models with tensorflow., but it is delayed to maintain alignment with newer versions of tensorflow. That is, updates to the tensorflow-text library are not updated immediately.

Mark531 (Issue Creator) on (2024-11-04 17:55:48 UTC): Ok, so it is currently usable with tensorflow 2.10.1.

VadisettyRahul on (2024-11-05 15:19:52 UTC): @Venkat6871 need help?

Venkat6871 (Assginee) on (2024-11-06 06:23:51 UTC): Hi **@Mark531** ,
Apologies for the delay, and thank you for raising your concern here. Could you please confirm where exactly you are facing the issuewith TensorFlow or with TensorFlow-Text triggers? If it is with TensorFlow-Text triggers, please raise the issue in that particular [repository](https://github.com/tensorflow/text/issues).
Thank you!

github-actions[bot] on (2024-11-14 02:01:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-21 02:04:05 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-21 02:04:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79331"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79331"">No</a>

"
2632672762,issue,closed,completed,Tensorflow Build for Alpine on Multiple Architectures,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.5.0

### Custom code

No

### OS platform and distribution

Linux Alpine 3.18

### Mobile device

_No response_

### Python version

3.11.10-r1

### Bazel version

6.5.0

### GCC/compiler version

12.2.1_git20220924-r10

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expected behavior: Successful build.
Actual behavior: A python related error is printed to the console and the build fails.

I'm trying to build Tensorflow 2.15.0 on Alpine Linux 3.18. My objective is to build Alpine compatible wheels for x86 and Arm64.

To do this I plan to dow a build inside docker:
1/ Install Python 3.11.10-r1, along with any dependencies,
2/ Download Bazel 6.5.0 , do a bootstrap build, using scripts/bootstrap/compile.sh,
3/ Trigger the Tensorflow build using Bazel,
4/ Export the wheel that has been built.

The first two steps are working fine, however I'm getting an error that I don't understand from Bazel when building Tensorflow.

From what I understand Bazel bunks down to system processes for the python components, so I have verified that the packages that it appears to be failing on are available on Alpine by downloading and installing them outside of Bazel.

I would be grateful if anyone could help me with the following questions:
a) Increase the debug output from Bazel,
b) Explain how Bazel and Tensorflow specifically uses pipy,
c) And hence explain the error above?

Any help would be gratefully received.
Bryan

### Standalone code to reproduce the issue

```shell
FROM docker-default-virtual.health.artifactory.tio.systems/core-container-alpine318:latest

RUN cat > /etc/apk/repositories <<EOF
https://dl-cdn.alpinelinux.org/alpine/v$(cut -d'.' -f1,2 /etc/alpine-release)/main/
https://dl-cdn.alpinelinux.org/alpine/v$(cut -d'.' -f1,2 /etc/alpine-release)/community/
EOF

ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk
ENV LOCAL_RESOURCES=2048,.5,1.0
ENV BAZEL_VERSION=6.5.0
RUN apk add --no-cache python3 python3-tkinter py3-numpy py3-numpy-f2py freetype libpng libjpeg-turbo imagemagick graphviz git

RUN apk add --no-cache --virtual=.build-deps \
        bash \
        cmake \
        curl \
        freetype-dev \
        g++ \
        libjpeg-turbo-dev \
        libpng-dev \
        linux-headers \
        make \
        musl-dev \
        openblas-dev \
        openjdk17 \
        patch \
        perl \
        python3-dev \
        py3-numpy-dev \
        rsync \
        sed \
        swig \
        zip \
        py3-pip \
        clang \
    && cd /tmp \
    && pip3 install --no-cache-dir wheel

RUN ls /usr/lib/jvm/

ENV UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE

# Bazel download
RUN curl -SLO https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-dist.zip \
    && mkdir bazel-${BAZEL_VERSION} \
    && unzip -qd bazel-${BAZEL_VERSION} bazel-${BAZEL_VERSION}-dist.zip

ENV BAZEL_DEBUG_JAVA_COMPILATION=true
ENV BAZEL_BOOTSTRAP_STARTUP_OPTIONS=""""
ENV EXTRA_BAZEL_ARGS=""--tool_java_runtime_version=local_jdk""

# Bazel install
RUN cd bazel-${BAZEL_VERSION} \
    && sed -i -e 's/-classpath/-J-Xmx8192m -J-Xms128m -classpath/g' scripts/bootstrap/compile.sh \
    && sed -i -e 's/-jar/--add-opens java.base\/java.lang=ALL-UNNAMED -jar/g' scripts/bootstrap/compile.sh \
    && bash compile.sh \
    && cp -p output/bazel /usr/bin/

# Download Tensorflow
ENV TENSORFLOW_VERSION=2.15.0
RUN cd /tmp \
    && curl -SL https://github.com/tensorflow/tensorflow/archive/v${TENSORFLOW_VERSION}.tar.gz \
        | tar xzf -
        
RUN apk add --no-cache hdf5-dev \
    && pip install --upgrade certifi packaging opt-einsum keras tensorflow_estimator \
    && which python \
    && which python3 \
    && ls -R /root/.cache/

RUN pip install astunparse

# Build Tensorflow
RUN cd /tmp/tensorflow-${TENSORFLOW_VERSION} \
    && : musl-libc does not have ""secure_getenv"" function \
    && export PYTHON_BIN_PATH=""$(which python)"" \
    && export TF_PYTHON_VERSION=""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')"" \
    #&& sed -i -e '/JEMALLOC_HAVE_SECURE_GETENV/d' third_party/jemalloc.BUILD \
    #&& sed -i -e '/define TF_GENERATE_BACKTRACE/d' tensorflow/core/platform/default/stacktrace.h \
    #&& sed -i -e '/define TF_GENERATE_STACKTRACE/d' tensorflow/core/platform/stacktrace_handler.cc \
    #&& sed -i -e 's/workspace(name = ""org_tensorflow"")/workspace(name = ""org_tensorflow"")\n\nload(""@bazel_tools\/\/tools\/build_defs\/repo:http.bzl"", ""http_archive"")/g' WORKSPACE \
    && PYTHON_LIB_PATH=/usr/lib/python${TF_PYTHON_VERSION}/site-packages \
        CC_OPT_FLAGS=""-march=native"" \
        TF_NEED_JEMALLOC=1 \
        TF_NEED_GCP=0 \
        TF_NEED_HDFS=0 \
        TF_NEED_S3=0 \
        TF_ENABLE_XLA=0 \
        TF_NEED_GDR=0 \
        TF_NEED_VERBS=0 \
        TF_NEED_OPENCL=0 \
        TF_NEED_CUDA=0 \
        TF_NEED_MPI=0 \
        bash configure

RUN export PYTHON_BIN_PATH=""$(which python)"" \
    && export TF_PYTHON_VERSION=""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')"" \
    &&cd /tmp/tensorflow-${TENSORFLOW_VERSION} \
    && bazel clean --expunge \
    && bazel build -c opt --define=no_tensorflow_py_deps=true --verbose_failures //tensorflow/tools/pip_package:build_pip_package

RUN cd /tmp/tensorflow-${TENSORFLOW_VERSION} \
    && ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

RUN cp /tmp/tensorflow_pkg/tensorflow-${TENSORFLOW_VERSION}-cp36-cp36m-linux_x86_64.whl /root

# Make sure it's built properly
RUN pip3 install --no-cache-dir /root/tensorflow-${TENSORFLOW_VERSION}-cp36-cp36m-linux_x86_64.whl \
    && python3 -c 'import tensorflow'

#RUN python3 -c 'import tensorflow as tf; print(""TensorFlow version:"", tf.__version__)'
```


### Relevant log output

```shell
00:21:54  #15 54.60 Analyzing: target //tensorflow/tools/pip_package:build_pip_package (129 packages loaded, 431 targets configured)
00:21:54  #15 55.31 INFO: Repository pypi_packaging instantiated at:
00:21:54  #15 55.31   /tmp/tensorflow-2.15.0/WORKSPACE:70:13: in <toplevel>
00:21:54  #15 55.31   /root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/pypi/requirements.bzl:49:20: in install_deps
00:21:54  #15 55.31 Repository rule whl_library defined at:
00:21:54  #15 55.31   /root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/rules_python/python/pip_install/pip_repository.bzl:697:30: in <toplevel>
00:21:54  #15 55.33 ERROR: An error occurred during the fetch of repository 'pypi_packaging':
00:21:54  #15 55.33    Traceback (most recent call last):
00:21:54  #15 55.33 	File ""/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/rules_python/python/pip_install/pip_repository.bzl"", line 596, column 13, in _whl_library_impl
00:21:54  #15 55.33 		fail(""whl_library %s failed: %s (%s) error code: '%s'"" % (rctx.attr.name, result.stdout, result.stderr, result.return_code))
00:21:54  #15 55.33 Error in fail: whl_library pypi_packaging failed:  (src/main/tools/process-wrapper-legacy.cc:80: ""execvp(/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/python_x86_64-unknown-linux-gnu/bin/python3, ...)"": No such file or directory
00:21:54  #15 55.33 ) error code: '1'
00:21:54  #15 55.33 ERROR: /tmp/tensorflow-2.15.0/WORKSPACE:70:13: fetching whl_library rule //external:pypi_packaging: Traceback (most recent call last):
00:21:54  #15 55.33 	File ""/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/rules_python/python/pip_install/pip_repository.bzl"", line 596, column 13, in _whl_library_impl
00:21:54  #15 55.33 		fail(""whl_library %s failed: %s (%s) error code: '%s'"" % (rctx.attr.name, result.stdout, result.stderr, result.return_code))
00:21:54  #15 55.33 Error in fail: whl_library pypi_packaging failed:  (src/main/tools/process-wrapper-legacy.cc:80: ""execvp(/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/python_x86_64-unknown-linux-gnu/bin/python3, ...)"": No such file or directory
00:21:54  #15 55.33 ) error code: '1'
00:21:54  #15 55.33 INFO: Repository org_brotli instantiated at:
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/WORKSPACE:84:14: in <toplevel>
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:928:21: in workspace
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:874:20: in _tf_repositories
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/third_party/repo.bzl:136:21: in tf_http_archive
00:21:54  #15 55.33 Repository rule _tf_http_archive defined at:
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/third_party/repo.bzl:89:35: in <toplevel>
00:21:54  #15 55.33 INFO: Repository stablehlo instantiated at:
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/WORKSPACE:84:14: in <toplevel>
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:921:28: in workspace
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:88:14: in _initialize_third_party
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/third_party/stablehlo/workspace.bzl:11:20: in repo
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/third_party/repo.bzl:136:21: in tf_http_archive
00:21:54  #15 55.33 Repository rule _tf_http_archive defined at:
00:21:54  #15 55.33   /tmp/tensorflow-2.15.0/third_party/repo.bzl:89:35: in <toplevel>
00:21:54  #15 55.38 INFO: Repository com_google_absl instantiated at:
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/WORKSPACE:84:14: in <toplevel>
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:921:28: in workspace
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/tensorflow/workspace2.bzl:63:9: in _initialize_third_party
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/third_party/absl/workspace.bzl:39:20: in repo
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/third_party/repo.bzl:136:21: in tf_http_archive
00:21:54  #15 55.38 Repository rule _tf_http_archive defined at:
00:21:54  #15 55.38   /tmp/tensorflow-2.15.0/third_party/repo.bzl:89:35: in <toplevel>
00:21:54  #15 55.40 ERROR: /tmp/tensorflow-2.15.0/tensorflow/python/framework/BUILD:299:18: //tensorflow/python/framework:framework depends on @pypi_packaging//:pkg in repository @pypi_packaging which failed to fetch. no such package '@pypi_packaging//': whl_library pypi_packaging failed:  (src/main/tools/process-wrapper-legacy.cc:80: ""execvp(/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/python_x86_64-unknown-linux-gnu/bin/python3, ...)"": No such file or directory
00:21:54  #15 55.40 ) error code: '1'
00:21:54  #15 55.43 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
00:21:55  #15 55.46 INFO: Elapsed time: 51.759s
00:21:55  #15 55.46 INFO: 0 processes.
00:21:55  #15 55.46 FAILED: Build did NOT complete successfully (147 packages loaded, 515 targets configured)
00:22:03  #15 ERROR: process ""/bin/sh -c export PYTHON_BIN_PATH=\""$(which python)\""     && export TF_PYTHON_VERSION=\""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')\""     &&cd /tmp/tensorflow-${TENSORFLOW_VERSION}     && bazel clean --expunge     && bazel build -c opt --define=no_tensorflow_py_deps=true --verbose_failures //tensorflow/tools/pip_package:build_pip_package"" did not complete successfully: exit code: 1
00:22:03  ------
00:22:03   > [12/15] RUN export PYTHON_BIN_PATH=""$(which python)""     && export TF_PYTHON_VERSION=""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')""     &&cd /tmp/tensorflow-2.15.0     && bazel clean --expunge     && bazel build -c opt --define=no_tensorflow_py_deps=true --verbose_failures //tensorflow/tools/pip_package:build_pip_package:
00:22:03  55.38   /tmp/tensorflow-2.15.0/third_party/absl/workspace.bzl:39:20: in repo
00:22:03  55.38   /tmp/tensorflow-2.15.0/third_party/repo.bzl:136:21: in tf_http_archive
00:22:03  55.38 Repository rule _tf_http_archive defined at:
00:22:03  55.38   /tmp/tensorflow-2.15.0/third_party/repo.bzl:89:35: in <toplevel>
00:22:03  55.40 ERROR: /tmp/tensorflow-2.15.0/tensorflow/python/framework/BUILD:299:18: //tensorflow/python/framework:framework depends on @pypi_packaging//:pkg in repository @pypi_packaging which failed to fetch. no such package '@pypi_packaging//': whl_library pypi_packaging failed:  (src/main/tools/process-wrapper-legacy.cc:80: ""execvp(/root/.cache/bazel/_bazel_root/96fc12a8fd07b4bb0604d61746db3a5b/external/python_x86_64-unknown-linux-gnu/bin/python3, ...)"": No such file or directory
00:22:03  55.40 ) error code: '1'
00:22:03  55.43 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
00:22:03  55.46 INFO: Elapsed time: 51.759s
00:22:03  55.46 INFO: 0 processes.
00:22:03  55.46 FAILED: Build did NOT complete successfully (147 packages loaded, 515 targets configured)
00:22:03  ------
00:22:03  WARNING: No output specified with docker-container driver. Build result will only remain in the build cache. To push result image into registry use --push or to load image into docker use --load
00:22:03  Dockerfile:109
00:22:03  --------------------
00:22:03   108 |     # --local_resources ${LOCAL_RESOURCES}
00:22:03   109 | >>> RUN export PYTHON_BIN_PATH=""$(which python)"" \
00:22:03   110 | >>>     && export TF_PYTHON_VERSION=""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')"" \
00:22:03   111 | >>>     &&cd /tmp/tensorflow-${TENSORFLOW_VERSION} \
00:22:03   112 | >>>     && bazel clean --expunge \
00:22:03   113 | >>>     && bazel build -c opt --define=no_tensorflow_py_deps=true --verbose_failures //tensorflow/tools/pip_package:build_pip_package
00:22:03   114 |     
00:22:03  --------------------
00:22:03  ERROR: failed to solve: process ""/bin/sh -c export PYTHON_BIN_PATH=\""$(which python)\""     && export TF_PYTHON_VERSION=\""$(${PYTHON_BIN_PATH} --version | sed -E -e 's/Python //' -e 's/.[0-9]+//')\""     &&cd /tmp/tensorflow-${TENSORFLOW_VERSION}     && bazel clean --expunge     && bazel build -c opt --define=no_tensorflow_py_deps=true --verbose_failures //tensorflow/tools/pip_package:build_pip_package"" did not complete successfully: exit code: 1
```
",els8482,2024-11-04 12:20:52+00:00,['tilakrayal'],2024-11-26 02:05:55+00:00,2024-11-26 02:05:51+00:00,https://github.com/tensorflow/tensorflow/issues/79327,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2468399505, 'issue_id': 2632672762, 'author': 'tilakrayal', 'body': '@els8482,\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62899 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!', 'created_at': datetime.datetime(2024, 11, 11, 15, 8, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553382, 'issue_id': 2632672762, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499453291, 'issue_id': 2632672762, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499453643, 'issue_id': 2632672762, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79327"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79327"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 2, 5, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-11 15:08:08 UTC): @els8482,
I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62899 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!

github-actions[bot] on (2024-11-19 02:05:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-26 02:05:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-26 02:05:54 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79327"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79327"">No</a>

"
2632401813,issue,closed,completed,`unresolved external symbol TfLiteGpuDelegateV2Create` linker error with Visual Studio,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TfLite nightly, 2.16, 2.10

### Custom code

No

### OS platform and distribution

Windows 10 Pro & Home

### Mobile device

_No response_

### Python version

Irrelevant, C++ API is used

### Bazel version

_No response_

### GCC/compiler version

Microsoft Visual Studio 2022 C++ compiler

### CUDA/cuDNN version

_No response_

### GPU model and memory

Geforce RTX 2080 (8 GB), Geforce GT 1030 (2 GB GDDR5), Intel HD Graphics 520

### Current behavior?

The function `TfLiteGpuDelegateV2Create` in the minimal-working example below yields the linker error `unresolved external symbol __imp_TfLiteGpuDelegateV2Create` with Visual Studio.

Why?
Short answer:
Function `TfLiteGpuDelegateV2Create` is prefixed with preprocessor macro `TFL_CAPI_EXPORT` as `TFL_CAPI_EXPORT TfLiteDelegate* TfLiteGpuDelegateV2Create(const TfLiteGpuDelegateOptionsV2* options);` (see `tensorflow/lite/delegates/gpu/delegate.h`) When *building* TfLite, `TFL_CAPI_EXPORT` is empty. When *using* `delegate.h` in my application, `TFL_CAPI_EXPORT` contains `__declspec(dllimport)`. This `__declspec(dllimport)` causes the linker error.

Long answer:
Function `TfLiteGpuDelegateV2Create` is prefixed with preprocessor macro `TFL_CAPI_EXPORT`. `TFL_CAPI_EXPORT` is defined in `tensorflow\lite\core\c\c_api_types.h` as
```
// Define TFL_CAPI_EXPORT macro to export a function properly with a shared
// library.
#ifdef SWIG
    #define TFL_CAPI_EXPORT
#elif defined(TFL_STATIC_LIBRARY_BUILD)
    #define TFL_CAPI_EXPORT
#else  // not definded TFL_STATIC_LIBRARY_BUILD
    #if defined(_WIN32)
        #ifdef TFL_COMPILE_LIBRARY
            #define TFL_CAPI_EXPORT __declspec(dllexport)
        #else
            #define TFL_CAPI_EXPORT __declspec(dllimport)
        #endif  // TFL_COMPILE_LIBRARY
    #else
        #define TFL_CAPI_EXPORT __attribute__((visibility(""default"")))
    #endif  // _WIN32
#endif  // SWIG
```
`TFL_CAPI_EXPORT` is empty when building TfLite with the defaults via CMake. `TFL_CAPI_EXPORT` is empty because `TFL_STATIC_LIBRARY_BUILD` is defined in TfLite's `CMakeLists.txt`. `TFL_CAPI_EXPORT` is not empty when `tensorflow\lite\core\c\c_api_types.h` is being included in my application, e.g. indirectly in the minimum-working example by including `tensorflow/lite/delegates/gpu/delegate.h`. `TFL_CAPI_EXPORT` then expands to `__declspec(dllimport)` because `TFL_STATIC_LIBRARY_BUILD` is not defined. This leads to the linker error `unresolved external symbol __imp_TfLiteGpuDelegateV2Create`.

Work-around:
Define `TFL_STATIC_LIBRARY_BUILD` before including the TfLite headers in my application. Then `TFL_CAPI_EXPORT` is empty when `tensorflow\lite\core\c\c_api_types.h` is included in my application. But `TFL_STATIC_LIBRARY_BUILD` is an internal macro, the user should not be aware of this macro.

Proposed solution:
Remove `__declspec(dllimport)` from `TFL_CAPI_EXPORT`. It is optional for functions, it is merely a small optimization when calling functions and not of significant impact in compute-intensive applications as DNN processing.
Alternative: remove prefix `TFL_CAPI_EXPORT` from function declarations that are part not of the C API. Because I believe the TfLite C API by default builds to a .dll and the regular TfLite by default builds to a static lib.
Alternative2: Rethink this design with `TFL_CAPI_EXPORT`.

N.B. I build the regular TfLite, not the TfLite C API.

(Possibly) Related:
https://github.com/tensorflow/tensorflow/issues/61442
https://github.com/tensorflow/flutter-tflite/issues/83
https://github.com/tensorflow/flutter-tflite/issues/82
https://github.com/am15h/tflite_flutter_plugin/issues/60
https://groups.google.com/g/angleproject/c/xKmUgKZFpgY/m/65FHXZJjBwAJ

### Standalone code to reproduce the issue

```shell
Minimal working example:
Build commands (in Windows Command Prompt):
git clone --single-branch --branch nightly https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_x64_release
cd tflite_x64_release
cmake -G ""Visual Studio 17 2022"" -A x64 -DCMAKE_MSVC_RUNTIME_LIBRARY=MultiThreaded -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=release -DTFLITE_ENABLE_GPU=ON ..\tensorflow_src\tensorflow\lite
cmake --build . -j 8 --config release


//#define TFL_STATIC_LIBRARY_BUILD  //Define this macro to avoid unresolved external symbol of TfLiteGpuDelegateV2Create
#include ""tensorflow/lite/logger.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""
#include <iostream>

int main() {
    tflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_VERBOSE);

    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""./lite-model_deeplabv3_1_metadata_2.tflite"");  //Model from https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_segmentation/rpi/lite-model_deeplabv3_1_metadata_2.tflite
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder interpreter_builder(*model, resolver);
    interpreter_builder.SetNumThreads(1);
    TfLiteDelegate* gpu_delegate = TfLiteGpuDelegateV2Create(nullptr);
    interpreter_builder.AddDelegate(gpu_delegate);

    std::cout << ""Done\n"";
    return EXIT_SUCCESS;
}
```


### Relevant log output

_No response_",misterBart,2024-11-04 10:20:25+00:00,['pkgoogle'],2025-01-13 09:14:18+00:00,2025-01-13 09:14:15+00:00,https://github.com/tensorflow/tensorflow/issues/79317,"[('awaiting review', 'Pull request awaiting review'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2470703870, 'issue_id': 2632401813, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take a look into this issue. Thank you', 'created_at': datetime.datetime(2024, 11, 12, 14, 35, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471364214, 'issue_id': 2632401813, 'author': 'pkgoogle', 'body': ""Hi @misterBart, I noticed you are using GeForce GPU's ... CUDA isn't supported for Windows past TF 2.11, please review this note: https://www.tensorflow.org/install/source_windows#gpu to ensure your setup is supported. That being said I see you are trying in nightly, 2.16, and 2.10 ... which version do you wish to try? (If you want to try with WSL2 or tensorflow-cpu with TensorFlow-DirectML-Plugin). Alternatively, you can make a PR with your suggestions and we can see how the review goes."", 'created_at': datetime.datetime(2024, 11, 12, 19, 17, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475642365, 'issue_id': 2632401813, 'author': 'misterBart', 'body': 'I\'m not using CUDA. This issue is completely unrelated to CUDA and Nvidia GPUs. And aside from testing with Geforce GPUs I also tested with an Intel HD 520 GPU, as I wrote down under ""GPU model and memory"" in my opening post. This issue is about a linker error with the Visual Studio compiler, and I already posted a PR to solve this issue (see above).\r\n\r\nThis issue is present in TfLite 2.10, 2.16 and in Nightly.', 'created_at': datetime.datetime(2024, 11, 14, 7, 50, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586565626, 'issue_id': 2632401813, 'author': 'misterBart', 'body': 'My PR to solve this issue has been accepted, therefore I close this issue.', 'created_at': datetime.datetime(2025, 1, 13, 9, 14, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586565689, 'issue_id': 2632401813, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79317"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79317"">No</a>', 'created_at': datetime.datetime(2025, 1, 13, 9, 14, 17, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 on (2024-11-12 14:35:39 UTC): Hi, @pkgoogle 
Please take a look into this issue. Thank you

pkgoogle (Assginee) on (2024-11-12 19:17:24 UTC): Hi @misterBart, I noticed you are using GeForce GPU's ... CUDA isn't supported for Windows past TF 2.11, please review this note: https://www.tensorflow.org/install/source_windows#gpu to ensure your setup is supported. That being said I see you are trying in nightly, 2.16, and 2.10 ... which version do you wish to try? (If you want to try with WSL2 or tensorflow-cpu with TensorFlow-DirectML-Plugin). Alternatively, you can make a PR with your suggestions and we can see how the review goes.

misterBart (Issue Creator) on (2024-11-14 07:50:12 UTC): I'm not using CUDA. This issue is completely unrelated to CUDA and Nvidia GPUs. And aside from testing with Geforce GPUs I also tested with an Intel HD 520 GPU, as I wrote down under ""GPU model and memory"" in my opening post. This issue is about a linker error with the Visual Studio compiler, and I already posted a PR to solve this issue (see above).

This issue is present in TfLite 2.10, 2.16 and in Nightly.

misterBart (Issue Creator) on (2025-01-13 09:14:15 UTC): My PR to solve this issue has been accepted, therefore I close this issue.

google-ml-butler[bot] on (2025-01-13 09:14:17 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79317"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79317"">No</a>

"
2631322197,issue,closed,completed,Can't build tensorflow,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Linux Gentoo 

### Mobile device

_No response_

### Python version

3.11

### Bazel version

Bazelisk 1.22.1 (bazel 6.5.0)

### GCC/compiler version

llvm-17 clang17

### CUDA/cuDNN version

12..6.1 / 9.5.0

### GPU model and memory

RTX 3050 laptop 8GB, but compiling for RTX 6000 ADA

### Current behavior?

Hello everyone. 
I am new to tensorflow building and it seems that I can't manage to do it correctly. Whatever version of CUDA or CUDNN I choose the builds always fail. Currently, I am trying to compile from source using the command 
```
bazel build -j 14 --local_ram_resources=HOST_RAM*.8 \
  --config=cuda_wheel \
  //tensorflow/tools/pip_package:wheel \
  --repo_env=HERMETIC_CUDA_VERSION=""12.6.1"" \
  --repo_env=HERMETIC_CUDNN_VERSION=""9.5.0"" \
  --repo_env=HERMETIC_CUDA_DRIVER_VERSION=""550"" \
  --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=""sm_89"" \
  --repo_env=WHEEL_NAME=tensorflow_test \
  --verbose_failures
```

However, it returns the error ""clang-17: error: cannot find libdevice for sm_89; provide path to different CUDA installation via '--cuda-path', or pass '-nocudalib' to build without linking with libdevice
clang-17: error: cannot find CUDA installation; provide its path via '--cuda-path', or pass '-nocudainc' to build without CUDA includes
clang-17: error: cannot find CUDA installation; provide its path via '--cuda-path', or pass '-nocudainc' to build without CUDA includes
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 893.679s, Critical Path: 71.04s
INFO: 11895 processes: 5385 internal, 6510 local.
FAILED: Build did NOT complete successfully""

I must admit that don't understand why it fails. As I understand the recent updates to TF, it is supposed to download the necessary CUDA and CUDNN versions itself, so I don't see which path I am supposed to provide here. I have already tried to use the recommended CUDA and CUDNN versions, but it didn't help with my problem. I have read throughout the documentation in both https://github.com/openxla/xla/blob/main/docs/hermetic_cuda.md#environment-variables-controlling-the-hermetic-cudacudnn-versions and https://www.tensorflow.org/install/source?hl=en but it doesn't help much. Is there any tutorial I could follow for compiling tensorflow, or anyone that could help me with that? 

Thank you very much.

### Standalone code to reproduce the issue

```shell
Standalone code:

bazel build -j 14 --local_ram_resources=HOST_RAM*.8 \
  --config=cuda_wheel \
  //tensorflow/tools/pip_package:wheel \
  --repo_env=HERMETIC_CUDA_VERSION=""12.6.1"" \
  --repo_env=HERMETIC_CUDNN_VERSION=""9.5.0"" \
  --repo_env=HERMETIC_CUDA_DRIVER_VERSION=""550"" \
  --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=""sm_89"" \
  --repo_env=WHEEL_NAME=tensorflow_test \
  --verbose_failures

Or 

bazel build -j 14 --local_ram_resources=HOST_RAM*.8 \
  --config=cuda_wheel \
  //tensorflow/tools/pip_package:wheel \
  --repo_env=HERMETIC_CUDA_VERSION=""12.5.1"" \
  --repo_env=HERMETIC_CUDNN_VERSION=""9.3.0"" \
  --repo_env=HERMETIC_CUDA_DRIVER_VERSION=""550"" \
  --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=""sm_89"" \
  --repo_env=WHEEL_NAME=tensorflow_test \
  --verbose_failures
```


### Relevant log output

_No response_",Alphxt,2024-11-03 16:45:48+00:00,['tilakrayal'],2024-11-23 02:02:43+00:00,2024-11-23 02:02:38+00:00,https://github.com/tensorflow/tensorflow/issues/79299,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2462373816, 'issue_id': 2631322197, 'author': 'tilakrayal', 'body': '@Alphxt,\r\nCould you please try to use --config=cuda_clang after configuring to build using clang. (I also think that using bazel build without --config flags should work too)\r\n\r\nThe build currently also fails because of an error in `configure` script. The fix should be upstream soon, the workaround before it lands is to change the following lines of `.tf_configure.bazelrc` from:\r\n```python\r\nbuild --config=cuda\r\ntest --config=cuda\r\n```\r\nto:\r\n```python\r\nbuild --config=cuda_clang\r\ntest --config=cuda_clang\r\n```\r\nThis should be done after running `configure`. Thank you!', 'created_at': datetime.datetime(2024, 11, 7, 14, 26, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805342, 'issue_id': 2631322197, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212131, 'issue_id': 2631322197, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212180, 'issue_id': 2631322197, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79299"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79299"">No</a>', 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 42, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-07 14:26:46 UTC): @Alphxt,
Could you please try to use --config=cuda_clang after configuring to build using clang. (I also think that using bazel build without --config flags should work too)

The build currently also fails because of an error in `configure` script. The fix should be upstream soon, the workaround before it lands is to change the following lines of `.tf_configure.bazelrc` from:
```python
build --config=cuda
test --config=cuda
```
to:
```python
build --config=cuda_clang
test --config=cuda_clang
```
This should be done after running `configure`. Thank you!

github-actions[bot] on (2024-11-15 02:05:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-23 02:02:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79299"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79299"">No</a>

"
2630279219,issue,closed,completed,Unable to register error after installing tensorflow 2.18.0,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

pip package

### TensorFlow version

v2.18.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia 1060 6Gb

### Current behavior?

Hi, I have installed Tensorflow 2.18.0 with ""python3 -m pip install tensorflow[and-cuda]"" on my newly installed Ubuntu 24.04. It installed successfully but I got error when try to import it with python. My driver version is 550 and I don't installed CUDA and cuDNN separetaly because TF2.18 uses Hermetic CUDA.

### Standalone code to reproduce the issue

```shell
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
2024-11-02 01:08:00.536542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730497080.560232   55183 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730497080.565334   55183 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-02 01:08:00.584775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
",AminA193,2024-11-02 07:16:21+00:00,['Venkat6871'],2024-12-22 07:07:30+00:00,2024-11-06 10:22:22+00:00,https://github.com/tensorflow/tensorflow/issues/79268,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2458735174, 'issue_id': 2630279219, 'author': 'Venkat6871', 'body': 'Hi **@AminA193** ,\r\nApologies for the delay, and thank you for raising your concern here. In your issue, there is not an erroronly a warning, which should not significantly impact running the code. I tried a sample code, and it is working fine. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/66aae380a4762774e7d2bab8c5639e1b/79268_2-18-0.ipynb) here for reference. If you would prefer to remove the warning, downgrading to a previous version might help.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 5, 7, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458834824, 'issue_id': 2630279219, 'author': 'AminA193', 'body': 'Hi dear [Venkat6871](https://github.com/Venkat6871),\r\nThank you so much. I installed it again and the tried with the codes provided in your gist for mnist dataset training and it worked for me.', 'created_at': datetime.datetime(2024, 11, 6, 6, 44, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459230215, 'issue_id': 2630279219, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79268"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79268"">No</a>', 'created_at': datetime.datetime(2024, 11, 6, 10, 22, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558356095, 'issue_id': 2630279219, 'author': 'GhostDog98', 'body': '> Hi **AminA193** , Apologies for the delay, and thank you for raising your concern here. In your issue, there is not an erroronly a warning, which should not significantly impact running the code. I tried a sample code, and it is working fine. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/66aae380a4762774e7d2bab8c5639e1b/79268_2-18-0.ipynb) here for reference. If you would prefer to remove the warning, downgrading to a previous version might help. Thank you!\r\n\r\nSorry to slightly necro this thread @AminA193 (it was the only one i found that addressed this issue), but while I am aware that this is ""only"" a warning, what does that warning actually mean? What can be done to prevent it, and does it impact speed of CUDA operations with tensorflow?', 'created_at': datetime.datetime(2024, 12, 22, 7, 7, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-06 05:07:30 UTC): Hi **@AminA193** ,
Apologies for the delay, and thank you for raising your concern here. In your issue, there is not an erroronly a warning, which should not significantly impact running the code. I tried a sample code, and it is working fine. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/66aae380a4762774e7d2bab8c5639e1b/79268_2-18-0.ipynb) here for reference. If you would prefer to remove the warning, downgrading to a previous version might help.
Thank you!

AminA193 (Issue Creator) on (2024-11-06 06:44:31 UTC): Hi dear [Venkat6871](https://github.com/Venkat6871),
Thank you so much. I installed it again and the tried with the codes provided in your gist for mnist dataset training and it worked for me.

google-ml-butler[bot] on (2024-11-06 10:22:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79268"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79268"">No</a>

GhostDog98 on (2024-12-22 07:07:28 UTC): Sorry to slightly necro this thread @AminA193 (it was the only one i found that addressed this issue), but while I am aware that this is ""only"" a warning, what does that warning actually mean? What can be done to prevent it, and does it impact speed of CUDA operations with tensorflow?

"
2628073575,issue,open,,`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Running `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.

[colab](https://colab.research.google.com/drive/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
A = tf.constant([[2,2],[2,2]], dtype=tf.int64)
B = tf.constant([[0,0],[0,0]], dtype=tf.int64)

with tf.device(""/gpu:0""):
    output_gpu = tf.math.floormod(A, B) # No error
    print(f""\nGPU: {output_gpu}\n"") # GPU: [[2 2] [2 2]]

with tf.device(""/cpu:0""):
    output_cpu = tf.math.floormod(A, B) 
    # InvalidArgumentError: Integer division by zero
```


### Relevant log output

```shell
GPU: [[2 2]
 [2 2]]

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-ba17e88e7095> in <cell line: 9>()
      8 
      9 with tf.device(""/cpu:0""):
---> 10     output_cpu = tf.math.floormod(A, B)
     11     # InvalidArgumentError: Integer division by zero

2 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5984 
   5985 

InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorMod] name:
```
",jiren-the-gray,2024-11-01 00:42:37+00:00,['tilakrayal'],2024-11-06 08:33:40+00:00,,https://github.com/tensorflow/tensorflow/issues/79162,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2456590929, 'issue_id': 2628073575, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/079bc746a9e2c6f880ea3c0312e935d7/untitled2218.ipynb).', 'created_at': datetime.datetime(2024, 11, 5, 8, 56, 57, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-05 08:56:57 UTC): I was able to reproduce the issue on tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/079bc746a9e2c6f880ea3c0312e935d7/untitled2218.ipynb).

"
2627962667,issue,open,,`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Passing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.

### Standalone code to reproduce the issue
[colab](https://colab.research.google.com/drive/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)
```shell
import tensorflow as tf
import numpy as np

A = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,
                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)

B = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [
                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)

with tf.device(""/gpu:0""):
    output_gpu = tf.linalg.lstsq(A, B)
    
with tf.device( ""/cpu:0""):
    output_cpu = tf.linalg.lstsq(A, B)

np.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=100

Mismatched elements: 2 / 10 (20%)
Max absolute difference: 112.95715
Max relative difference: 0.2963447
 x: array([[[[-170.69518 ,   28.379017],
         [ 164.85085 ,  -28.04222 ],
         [-273.4264  ,   46.952198],...
 y: array([[[[-241.07059 ,   40.25023 ],
         [ 232.80626 ,  -39.505226],
         [-386.38354 ,   66.00629 ],...
```
",jiren-the-gray,2024-10-31 22:52:23+00:00,['Venkat6871'],2024-11-05 05:04:50+00:00,,https://github.com/tensorflow/tensorflow/issues/79157,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2456243048, 'issue_id': 2627962667, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow v2.18.0 with both CPU and GPU, as well as the nightly version. I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/2d95dc99e7e79e57042db06f748e3ea9/79157_tf_2-18-0-cpu-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/312e4d161262d160956f8871a92e1b27/79157_tf_2-18-0-gpu-nightly-v.ipynb) here for reference.', 'created_at': datetime.datetime(2024, 11, 5, 5, 4, 48, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-05 05:04:48 UTC): I tried running your code on Colab using TensorFlow v2.18.0 with both CPU and GPU, as well as the nightly version. I faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/2d95dc99e7e79e57042db06f748e3ea9/79157_tf_2-18-0-cpu-v.ipynb), [gist2](https://colab.sandbox.google.com/gist/Venkat6871/312e4d161262d160956f8871a92e1b27/79157_tf_2-18-0-gpu-nightly-v.ipynb) here for reference.

"
2627822232,issue,closed,completed,Tensorflow Lite C++ ,"Is there any C++ API to enable flex delegates, after generating the tensorflowlite_flex.dll file ?",rishik1001,2024-10-31 21:03:33+00:00,['gaikwadrahul8'],2024-11-19 02:05:21+00:00,2024-11-19 02:05:18+00:00,https://github.com/tensorflow/tensorflow/issues/79153,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2453882454, 'issue_id': 2627822232, 'author': 'gaikwadrahul8', 'body': 'Hi, @rishik1001\r\n\r\nI couldn\'t find a direct C++ API to explicitly enable the Flex delegate after the `tensorflowlite_flex.dll` is generated but as far I know you can enable it by doing something like below if I\'m not wrong please refer to this [source file](https://github.com/tensorflow/tensorflow/blob/56d35d4d34acfbb2c2b91a88b2e250888d429fa7/tensorflow/lite/delegates/flex/delegate.cc)\r\n\r\n```\r\n#include ""tensorflow/lite/delegates/flex/delegate.h""\r\n#include ""tensorflow/lite/kernels/register.h""\r\n#include ""tensorflow/lite/model.h""\r\n#include ""tensorflow/lite/interpreter.h""\r\n\r\n// Load your TFLite model\r\nstd::unique_ptr<tflite::FlatBufferModel> model = \r\n    tflite::FlatBufferModel::BuildFromFile(""model.tflite"");\r\n\r\n// Create interpreter builder\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder builder(*model, resolver);\r\n\r\n// Create the Flex delegate\r\nTfLiteDelegateUniquePtr flex_delegate = tflite::FlexDelegate::Create();\r\n\r\n// Add the delegate to interpreter options\r\nbuilder.AddDelegate(flex_delegate.get());\r\n\r\n// Build the interpreter\r\nbuilder(&interpreter);\r\n\r\n// Allocate tensors\r\ninterpreter->AllocateTensors();\r\n\r\n// Set input tensor data and run inference\r\ninterpreter->Invoke();\r\n```\r\nIf I have missed something here please let me know.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 11, 4, 6, 2, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469437989, 'issue_id': 2627822232, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 12, 1, 59, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553412, 'issue_id': 2627822232, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553453, 'issue_id': 2627822232, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79153"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79153"">No</a>', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 19, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-04 06:02:06 UTC): Hi, @rishik1001

I couldn't find a direct C++ API to explicitly enable the Flex delegate after the `tensorflowlite_flex.dll` is generated but as far I know you can enable it by doing something like below if I'm not wrong please refer to this [source file](https://github.com/tensorflow/tensorflow/blob/56d35d4d34acfbb2c2b91a88b2e250888d429fa7/tensorflow/lite/delegates/flex/delegate.cc)

```
#include ""tensorflow/lite/delegates/flex/delegate.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/interpreter.h""

// Load your TFLite model
std::unique_ptr<tflite::FlatBufferModel> model = 
    tflite::FlatBufferModel::BuildFromFile(""model.tflite"");

// Create interpreter builder
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder builder(*model, resolver);

// Create the Flex delegate
TfLiteDelegateUniquePtr flex_delegate = tflite::FlexDelegate::Create();

// Add the delegate to interpreter options
builder.AddDelegate(flex_delegate.get());

// Build the interpreter
builder(&interpreter);

// Allocate tensors
interpreter->AllocateTensors();

// Set input tensor data and run inference
interpreter->Invoke();
```
If I have missed something here please let me know.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-12 01:59:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-19 02:05:17 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-19 02:05:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79153"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79153"">No</a>

"
2626881492,issue,open,,Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17

### Custom code

No

### OS platform and distribution

Linux Mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

clang 14.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute/-1030865605`, `tf_data_private_threadpool/-102013...`, etc., have negative thread IDs.

![image](https://github.com/user-attachments/assets/71d32241-d617-4e52-bcbf-d3140ebef440)

My log file is also attached.

[20241030-161104.zip](https://github.com/user-attachments/files/17587809/20241030-161104.zip)

### Standalone code to reproduce the issue

```shell
(Just the profiler demo code) https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
```


### Relevant log output

_No response_",wokron,2024-10-31 13:36:36+00:00,['Venkat6871'],2024-11-05 04:10:21+00:00,,https://github.com/tensorflow/tensorflow/issues/79128,"[('type:bug', 'Bug'), ('awaiting PR merge', 'awaiting PR merge'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2449864657, 'issue_id': 2626881492, 'author': 'wokron', 'body': 'Actually, I found the cause of this issue and I will create a PR for this. In the `tsl/platform/default/env.cc` file of the tsl library, the return type of the `GetCurrentThreadId()` function is `int32`. It should be cast to `uint32`.\r\n\r\nHowever, I am still curious why the return type of `GetCurrentThreadId()` is a signed integer. It is quite strange for thread IDs to be negative, especially now that we are seeing an ""overflow"" issue. Can someone explain this?', 'created_at': datetime.datetime(2024, 10, 31, 13, 37, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456192128, 'issue_id': 2626881492, 'author': 'Venkat6871', 'body': 'Hi **@wokron** ,\r\nApologies for the delay, and thank you for raising your concern here. You have already submitted a [PR](https://github.com/tensorflow/tensorflow/pull/79131), and once it is merged, your issue will be resolved.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 5, 4, 10, 11, tzinfo=datetime.timezone.utc)}]","wokron (Issue Creator) on (2024-10-31 13:37:35 UTC): Actually, I found the cause of this issue and I will create a PR for this. In the `tsl/platform/default/env.cc` file of the tsl library, the return type of the `GetCurrentThreadId()` function is `int32`. It should be cast to `uint32`.

However, I am still curious why the return type of `GetCurrentThreadId()` is a signed integer. It is quite strange for thread IDs to be negative, especially now that we are seeing an ""overflow"" issue. Can someone explain this?

Venkat6871 (Assginee) on (2024-11-05 04:10:11 UTC): Hi **@wokron** ,
Apologies for the delay, and thank you for raising your concern here. You have already submitted a [PR](https://github.com/tensorflow/tensorflow/pull/79131), and once it is merged, your issue will be resolved.
Thank you!

"
2626818285,issue,closed,completed,int8 quantization,Does tflite support int8 quantization? Does int8 quantization support quantizing only the specified layer?,JinXiaozhao,2024-10-31 13:07:48+00:00,['tilakrayal'],2024-11-19 02:05:24+00:00,2024-11-19 02:05:19+00:00,https://github.com/tensorflow/tensorflow/issues/79127,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2454074274, 'issue_id': 2626818285, 'author': 'tilakrayal', 'body': '@JinXiaozhao,\r\nHave you gone through the official document where Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers.\r\n\r\nhttps://ai.google.dev/edge/litert/models/post_training_integer_quant\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 4, 8, 23, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469438015, 'issue_id': 2626818285, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 12, 1, 59, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553436, 'issue_id': 2626818285, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553507, 'issue_id': 2626818285, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79127"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79127"">No</a>', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 22, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-04 08:23:25 UTC): @JinXiaozhao,
Have you gone through the official document where Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers.

https://ai.google.dev/edge/litert/models/post_training_integer_quant

Thank you!

github-actions[bot] on (2024-11-12 01:59:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-19 02:05:18 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-19 02:05:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79127"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79127"">No</a>

"
2626806338,issue,closed,completed,Sequential not working,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

Manjaro

### Python version

3.12.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

help me please to figure out why this error occur. I tried to update drivers for cpu and other stuff but it doesn't help

### Standalone code to reproduce the issue

```shell
model = Sequential([
    Conv2D(46, kernel_size=(3,3), activation='relu', input_shape=(224,224,3)),
    Conv2D(46, kernel_size=(3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.15),
    
    Conv2D(128, kernel_size=(3,3), activation='relu'),
    Conv2D(128, kernel_size=(3,3), activation='relu'),
    Conv2D(128, kernel_size=(3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.3),
    
    Conv2D(256, kernel_size=(3,3), activation='relu'),
    Conv2D(256, kernel_size=(3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.5),
    
    Conv2D(32, kernel_size=(3,3), activation='relu'),
    Conv2D(32, kernel_size=(3,3), activation='relu'),
    Conv2D(32, kernel_size=(3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),
    Dropout(0.1),
    
    Flatten(),
    Dense(15, activation='softmax', kernel_regularizer=l2(0.16))
])

model.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
```


### Relevant log output

```shell
InternalError                             Traceback (most recent call last)
Cell In[31], line 1
----> 1 model = Sequential([
      2     Conv2D(46, kernel_size=(3,3), activation='relu', input_shape=(224,224,3)),
      3     Conv2D(46, kernel_size=(3,3), activation='relu'),
      4     BatchNormalization(),
      5     MaxPooling2D((2,2)),
      6     Dropout(0.15),
      7     
      8     Conv2D(128, kernel_size=(3,3), activation='relu'),
      9     Conv2D(128, kernel_size=(3,3), activation='relu'),
     10     Conv2D(128, kernel_size=(3,3), activation='relu'),
     11     BatchNormalization(),
     12     MaxPooling2D((2,2)),
     13     Dropout(0.3),
     14     
     15     Conv2D(256, kernel_size=(3,3), activation='relu'),
     16     Conv2D(256, kernel_size=(3,3), activation='relu'),
     17     BatchNormalization(),
     18     MaxPooling2D((2,2)),
     19     Dropout(0.5),
     20     
     21     Conv2D(32, kernel_size=(3,3), activation='relu'),
     22     Conv2D(32, kernel_size=(3,3), activation='relu'),
     23     Conv2D(32, kernel_size=(3,3), activation='relu'),
     24     BatchNormalization(),
     25     MaxPooling2D((2,2)),
     26     Dropout(0.1),
     27     
     28     Flatten(),
     29     Dense(15, activation='softmax', kernel_regularizer=l2(0.16))
     30 ])
     32 model.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

File ~/.venv/lib/python3.12/site-packages/keras/src/models/sequential.py:76, in Sequential.__init__(self, layers, trainable, name)
     74 for layer in layers:
     75     self.add(layer, rebuild=False)
---> 76 self._maybe_rebuild()

File ~/.venv/lib/python3.12/site-packages/keras/src/models/sequential.py:141, in Sequential._maybe_rebuild(self)
    139 if isinstance(self._layers[0], InputLayer) and len(self._layers) > 1:
    140     input_shape = self._layers[0].batch_shape
--> 141     self.build(input_shape)
    142 elif hasattr(self._layers[0], ""input_shape"") and len(self._layers) > 1:
    143     # We can build the Sequential model if the first layer has the
    144     # `input_shape` property. This is most commonly found in Functional
    145     # model.
    146     input_shape = self._layers[0].input_shape

File ~/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:226, in Layer.__new__.<locals>.build_wrapper(*args, **kwargs)
    224 with obj._open_name_scope():
    225     obj._path = current_path()
--> 226     original_build_method(*args, **kwargs)
    227 # Record build config.
    228 signature = inspect.signature(original_build_method)

File ~/.venv/lib/python3.12/site-packages/keras/src/models/sequential.py:187, in Sequential.build(self, input_shape)
    185 for layer in self._layers[1:]:
    186     try:
--> 187         x = layer(x)
    188     except NotImplementedError:
    189         # Can happen if shape inference is not implemented.
    190         # TODO: consider reverting inbound nodes on layers processed.
    191         return

File ~/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/random.py:19, in _cast_seed(seed)
     17     return seed
     18 else:
---> 19     seed = tf.cast(tf.math.floormod(seed, tf.int32.max - 1), dtype=""int32"")
     20     return seed

InternalError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name:
```
",Zxcbuster,2024-10-31 13:03:26+00:00,['Venkat6871'],2024-11-20 02:04:20+00:00,2024-11-20 02:04:17+00:00,https://github.com/tensorflow/tensorflow/issues/79126,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2456948022, 'issue_id': 2626806338, 'author': 'Venkat6871', 'body': 'Hi **@Zxcbuster** ,\r\nApologies for the delay, and thank you for raising your concern here. Could you please provide a code snippet to help troubleshoot your issue? Currently, only the model is provided. Also, please include the versions of CUDA and cuDNN you are using. I tried running your provided code on Colab with TensorFlow versions 2.17.0 and 2.18.0, and it is compiling without any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4af70fb6649a9b132858371d2608887d/79126_tf_2-17-0-2-18-0-v.ipynb) here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 5, 11, 41, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472173157, 'issue_id': 2626806338, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 13, 2, 1, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487166642, 'issue_id': 2626806338, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 20, 2, 4, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487166688, 'issue_id': 2626806338, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79126"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79126"">No</a>', 'created_at': datetime.datetime(2024, 11, 20, 2, 4, 18, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-05 11:41:36 UTC): Hi **@Zxcbuster** ,
Apologies for the delay, and thank you for raising your concern here. Could you please provide a code snippet to help troubleshoot your issue? Currently, only the model is provided. Also, please include the versions of CUDA and cuDNN you are using. I tried running your provided code on Colab with TensorFlow versions 2.17.0 and 2.18.0, and it is compiling without any issues. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/4af70fb6649a9b132858371d2608887d/79126_tf_2-17-0-2-18-0-v.ipynb) here for your reference.
Thank you!

github-actions[bot] on (2024-11-13 02:01:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-20 02:04:16 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-20 02:04:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79126"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79126"">No</a>

"
2626215053,issue,open,,TensorFlow Stable Delegate Python API,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow Python API currently doesn't support running stable delegates. Are there any workarounds? If not, are there any plans to support this in the future?

### Standalone code to reproduce the issue

```shell
self._model = tf.lite.Interpreter(
    model_path=stryolov5.tflite,
    experimental_delegates=[delegate.so]
)
```


### Relevant log output

_No response_",KozAAAAA,2024-10-31 07:52:45+00:00,"['gaikwadrahul8', 'pkgoogle']",2024-11-21 18:21:21+00:00,,https://github.com/tensorflow/tensorflow/issues/79111,"[('stat:contribution welcome', 'Status - Contributions welcome'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2467789766, 'issue_id': 2626215053, 'author': 'gaikwadrahul8', 'body': ""Hi, @KozAAAAA \r\n\r\nI apologize for the delayed response, As far I know currently TensorFlow python API does not support running stable delegates and If I'm not wrong we do support C++ API please refer this [official documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate),  regarding stable delegate python API I'll confirm with relevant team if there is any future plan to support that and will update you here if I got any update from that relevant team\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 11, 11, 10, 23, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469613864, 'issue_id': 2626215053, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\nPlease take a look into this issue. Thank you.', 'created_at': datetime.datetime(2024, 11, 12, 4, 59, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471348811, 'issue_id': 2626215053, 'author': 'pkgoogle', 'body': 'Hi @KozAAAAA, You are correct it does not seem supported. Hi @qukhan, perhaps you know if there are any workarounds or plans to support this. Thanks.', 'created_at': datetime.datetime(2024, 11, 12, 19, 8, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488630136, 'issue_id': 2626215053, 'author': 'KozAAAAA', 'body': '@qukhan Any updates on this?', 'created_at': datetime.datetime(2024, 11, 20, 13, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488698879, 'issue_id': 2626215053, 'author': 'qukhan', 'body': ""@fergushenderson You may know more than me about this... AFAIK this interface is experimental and isn't currently fully supported. Is that still the case?"", 'created_at': datetime.datetime(2024, 11, 20, 14, 15, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489513987, 'issue_id': 2626215053, 'author': 'fergushenderson', 'body': ""The stable delegate API is no longer experimental. \r\n\r\nBut currently stable delegates are only supported for the C, C++, and Java APIs, not for the Python API (and I think also not for the C#, Objective-C, and Swift APIs).\r\n\r\nIt does IMHO make good sense to add support for stable delegates to the Python API. So I suspect the LiteRT team may be supportive of adding such an API. I do meet with the LiteRT team on a regular basis and I will discuss this with them. But I'm sure it would be a lot easier for them to say yes if there was already a patch for that ready to be reviewed (hint, hint :-)."", 'created_at': datetime.datetime(2024, 11, 20, 20, 47, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489597353, 'issue_id': 2626215053, 'author': 'KozAAAAA', 'body': 'Well, actually I would love to contribute! I will take a look at it this weekend', 'created_at': datetime.datetime(2024, 11, 20, 21, 42, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-11-11 10:23:41 UTC): Hi, @KozAAAAA 

I apologize for the delayed response, As far I know currently TensorFlow python API does not support running stable delegates and If I'm not wrong we do support C++ API please refer this [official documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate),  regarding stable delegate python API I'll confirm with relevant team if there is any future plan to support that and will update you here if I got any update from that relevant team

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-11-12 04:59:42 UTC): Hi, @pkgoogle
Please take a look into this issue. Thank you.

pkgoogle (Assginee) on (2024-11-12 19:08:46 UTC): Hi @KozAAAAA, You are correct it does not seem supported. Hi @qukhan, perhaps you know if there are any workarounds or plans to support this. Thanks.

KozAAAAA (Issue Creator) on (2024-11-20 13:46:35 UTC): @qukhan Any updates on this?

qukhan on (2024-11-20 14:15:27 UTC): @fergushenderson You may know more than me about this... AFAIK this interface is experimental and isn't currently fully supported. Is that still the case?

fergushenderson on (2024-11-20 20:47:47 UTC): The stable delegate API is no longer experimental. 

But currently stable delegates are only supported for the C, C++, and Java APIs, not for the Python API (and I think also not for the C#, Objective-C, and Swift APIs).

It does IMHO make good sense to add support for stable delegates to the Python API. So I suspect the LiteRT team may be supportive of adding such an API. I do meet with the LiteRT team on a regular basis and I will discuss this with them. But I'm sure it would be a lot easier for them to say yes if there was already a patch for that ready to be reviewed (hint, hint :-).

KozAAAAA (Issue Creator) on (2024-11-20 21:42:00 UTC): Well, actually I would love to contribute! I will take a look at it this weekend

"
2625949151,issue,open,,Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS x86_64

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. 

The discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.

### Standalone code to reproduce the issue

[colab](https://colab.research.google.com/drive/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)
[Safe Tensors](https://drive.google.com/file/d/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6/view?usp=sharing)

```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file

def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)

def tensorflow_version(input, cpu=True):
    set_seed()
    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        b_tensor = tf.constant(input[""b""])
        A_tensor = tf.constant(input[""A""])

        upper = input.get(""upper"", True)
        transpose = input.get(""transpose"", False)
        unitriangular = input.get(""unitriangular"", False)

        solution = tf.linalg.triangular_solve(
            A_tensor, b_tensor, lower=not upper, adjoint=transpose
        )

        return {""triangular_solve_solution"": solution.numpy()}

def load_safe_tensor(file_path):
    safe_tensor_data = load_file(file_path)
    A_tensor = safe_tensor_data[""A""]
    b_tensor = safe_tensor_data[""b""]

    return {
        ""A"": tf.convert_to_tensor(A_tensor.numpy()),
        ""b"": tf.convert_to_tensor(b_tensor.numpy()),
    }

def calculate_differences(cpu_result, gpu_result):
    diff = np.abs(cpu_result - gpu_result)
    mae = np.mean(diff)
    mse = np.mean(diff**2)
    rmse = np.sqrt(mse)
    max_diff = np.max(diff)
    mean_relative_diff = np.mean(diff / (np.abs(cpu_result) + 1e-10))

    return {
        ""Mean Absolute Error"": mae,
        ""Mean Squared Error"": mse,
        ""Root Mean Squared Error"": rmse,
        ""Maximum Absolute Difference"": max_diff,
        ""Mean Relative Difference"": mean_relative_diff,
    }

def main():
    file_path = ""tensorflow_triangular_solve_3.safetensors""
    input_data = load_safe_tensor(file_path)
    input_data[""upper""] = True
    input_data[""transpose""] = True
    input_data[""unitriangular""] = True

    result_cpu = tensorflow_version(input_data, cpu=True)
    print(""CPU Result:"")
    print(result_cpu)

    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_version(input_data, cpu=False)
        print(""GPU Result:"")
        print(result_gpu)

        if result_gpu:
            cpu_solution = result_cpu[""triangular_solve_solution""]
            gpu_solution = result_gpu[""triangular_solve_solution""]
            differences = calculate_differences(cpu_solution, gpu_solution)
            for key, value in differences.items():
                print(f""{key}: {value}"")
    else:
        print(""GPU not available."")

if __name__ == ""__main__"":
    main()
```

#### **Issue Summary:**
- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.
- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.
- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlows triangular solve functionality.


### Relevant log output

```shell
Mean Absolute Error: 1.566790629150662e+21
Mean Squared Error: inf
Root Mean Squared Error: inf
Maximum Absolute Difference: 1.4265324671452624e+26
```
",Cirno-2000,2024-10-31 04:34:40+00:00,['Venkat6871'],2024-11-07 14:59:52+00:00,,https://github.com/tensorflow/tensorflow/issues/79090,"[('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2462454559, 'issue_id': 2625949151, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow 2.18.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/db3a781cb05b8bb95038ad79817c238e/79090_tf_2-18-0-cpu-_nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 14, 59, 50, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-07 14:59:50 UTC): I tried running your code on Colab using TensorFlow 2.18.0 and the nightly version, and I faced the same issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/db3a781cb05b8bb95038ad79817c238e/79090_tf_2-18-0-cpu-_nightly-v.ipynb) here for reference.

Thank you!

"
2625939517,issue,closed,completed,Discrepancy in `tf.experimental.numpy.nansum` with `dtype=float16` on CPU vs GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS x86_64

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

#### **Bug Description:**

When using `tf.experimental.numpy.nansum` with a specified `dtype` of `float16`, there is a significant discrepancy between the CPU and GPU results. On the CPU, the operation produces a normal float16 value, while on the GPU, it results in an `inf` value, indicating a potential issue in handling float16 precision on the GPU.

When switching to `float32` for the dtype, the results between the CPU and GPU align closely, which suggests that the discrepancy is specifically related to the handling of float16 values.



### Standalone code to reproduce the issue

[colab](https://colab.research.google.com/drive/14b31pm1EDgivd-xcIxugFVuJc8D7_4a2?usp=sharing)
[safe tensors](https://drive.google.com/file/d/1sg_bCRlhow5VYenkFKARRe2Matuw7owq/view?usp=sharing)


```shell
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file

def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)

def tensorflow_version(input, cpu=True):
    set_seed()
    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        # Enable NumPy behavior in TensorFlow
        tf.experimental.numpy.experimental_enable_numpy_behavior()

        # Unpack input dictionary
        x1 = tf.constant(input[""input""])
        x2 = input[""dim""]
        keepdim = input.get(""keepdim"", False)
        dtype = input.get(""dtype"", None)

        if dtype is not None:
            dtype = tf.as_dtype(dtype)

        # Perform the nansum operation
        y = tf.experimental.numpy.nansum(x1, axis=x2, keepdims=keepdim, dtype=dtype)

        return {""nansum"": y.numpy()}

def load_safe_tensor(file_path):
    """"""Loads the safe tensor from the specified file path using safetensors library.""""""
    safe_tensor_data = load_file(file_path)

    # Assuming the input tensor is stored with key ""input""
    input_tensor = safe_tensor_data[""input""]

    # Convert the loaded tensor to TensorFlow tensor
    return {""input"": tf.convert_to_tensor(input_tensor.numpy())}

def main():
    # File location
    file_path = (
        ""tensorflow_nansum_2.safetensors""
    )

    # Load the safe tensor
    input_data = load_safe_tensor(file_path)

    # Add non-tensor arguments to the input data
    input_data[""dim""] = (1,)
    input_data[""keepdim""] = True
    input_data[""dtype""] = np.float16  # np.float32 will work

    # Run on CPU
    result_cpu = tensorflow_version(input_data, cpu=True)
    print(""CPU Result:"")
    print(result_cpu)

    # Check if GPU is available
    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_version(input_data, cpu=False)
        print(""GPU Result:"")
        print(result_gpu)

        # Calculate and print the differences
        if result_gpu:
            diff = np.abs(result_cpu[""nansum""] - result_gpu[""nansum""])
            print(
                ""Maximum Absolute Difference between CPU and GPU results:"", np.max(diff)
            )
    else:
        print(""GPU not available."")

if __name__ == ""__main__"":
    main()
```

#### **Issue Summary:**
- When `dtype` is set to `float16`, the GPU calculation produces an `inf` value, while the CPU results in a proper float16 output. This discrepancy is not observed with `float32`.
- The unexpected `inf` value suggests that the GPU implementation for `tf.experimental.numpy.nansum` with `float16` has numerical precision or overflow issues.
- Switching to `float32` resolves the issue, indicating that this is specific to float16 precision handling.



### Relevant log output

```shell
CPU Result:
{'nansum': array([[[2048. , -473.5]]], dtype=float16)}
GPU Result:
{'nansum': array([[[   inf, -471.5]]], dtype=float16)}
Maximum Absolute Difference between CPU and GPU results: inf
```
",Cirno-2000,2024-10-31 04:22:27+00:00,['tilakrayal'],2024-11-08 20:39:24+00:00,2024-11-08 20:39:21+00:00,https://github.com/tensorflow/tensorflow/issues/79083,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2461408447, 'issue_id': 2625939517, 'author': 'tilakrayal', 'body': '@Cirno-2000,\r\nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 7, 6, 11, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465712142, 'issue_id': 2625939517, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79083"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79083"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 20, 39, 23, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-07 06:11:46 UTC): @Cirno-2000,
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.

Thank you!

google-ml-butler[bot] on (2024-11-08 20:39:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79083"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79083"">No</a>

"
2625410751,issue,closed,completed,Discrepancy in Error Handling for `tf.gather` on CPU vs GPU Leading to Undefined Behavior,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04 jammy

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a custom implementation of `tf_as_strided` on TensorFlow using `tf.gather` to simulate the behavior of creating a strided tensor, a significant discrepancy in error handling between CPU and GPU is observed. The CPU raises an `INVALID_ARGUMENT` error when indices are out of range, whereas the GPU produces unexpected outputs without raising an error. This lack of consistent error handling on the GPU could lead to silent data corruption or undefined behavior.

### Reproduction Steps:

  - Create a tensor with some data and define a size and stride pattern that leads to out-of-bounds index access.
  - Implement the `tf_as_strided` function using tf.gather to simulate a custom striding mechanism.
  - Run the function on both CPU and GPU using TensorFlow's `tf.device` context.

### Standalone code to reproduce the issue


Colab: [link](https://colab.research.google.com/drive/1vQkXo3T-2cROEd31yySVsOTIq4Rw2isC?usp=sharing)

Safe Tensors: [Google Drive](https://drive.google.com/file/d/1SmfHggjjPF2fEyVMCtBWB_NFE-MVRhb-/view?usp=sharing)

```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file


def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)


def tensorflow_version(input, cpu=True):
    # Set seed for reproducibility
    set_seed()

    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        input_tensor = input[""input""]
        size = input[""size""]
        stride = input[""stride""]
        storage_offset = input.get(""storage_offset"", 0)

        def tf_as_strided(x, size, stride, storage_offset):
            flat_tensor = tf.reshape(x, [-1])
            start_idx = storage_offset
            idxs = []
            for i in range(size[0]):
                for j in range(size[1]):
                    idxs.append(start_idx + i * stride[0] + j * stride[1])

            strided_tensor = tf.gather(flat_tensor, idxs)
            strided_tensor = tf.reshape(strided_tensor, size)
            return strided_tensor

        strided_tensor = tf_as_strided(input_tensor, size, stride, storage_offset)

        return {""as_strided_result"": strided_tensor.numpy()}


def load_safe_tensor(file_path):
    """"""Loads the safe tensor from the specified file path using safetensors library.""""""
    safe_tensor_data = load_file(file_path)

    # Assuming the input tensor is stored with key ""input""
    input_tensor = safe_tensor_data[""input""]

    # Convert the loaded tensor to TensorFlow tensor
    return {""input"": tf.convert_to_tensor(input_tensor.numpy())}


def main():
    # File location
    file_path = ""tensorflow_as_strided_1.safetensors""

    # Load the safe tensor
    input_data = load_safe_tensor(file_path)

    # Add non-tensor arguments to the input data
    input_data[""size""] = (2, 3)
    input_data[""stride""] = (0, 1)
    input_data[""storage_offset""] = 0

    # Run on CPU
    try:
        result_cpu = tensorflow_version(input_data, cpu=True)
        print(""CPU Result:"")
        print(result_cpu)
    except Exception as e:
        print(""CPU Error:"")
        print(e)

    # Check if GPU is available
    if tf.config.list_physical_devices(""GPU""):
        try:
            result_gpu = tensorflow_version(input_data, cpu=False)
            print(""GPU Result:"")
            print(result_gpu)
            
        except Exception as e:
            print(""GPU Error:"")
            print(e)
    else:
        print(""GPU not available."")


if __name__ == ""__main__"":
    main()
```

### Observed Behavior:

  - CPU Output: Local rendezvous is aborting with status: `INVALID_ARGUMENT: indices[1] = 1 is not in [0, 1)`
  - GPU Output: Produces an output tensor with unexpected values without raising any exception, e.g., `{'as_strided_result': array([[4468, 0, 0], [4468, 0, 0]])}.`
  - Additional Details: The resulting tensor contains repeated elements of 4468 along with zeros, indicating that the GPU did not properly handle out-of-bounds indices. The input data provided during the GPU run included a tensor that was incorrectly created due to this discrepancy: `{'input': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4468])>}`.

### Expected Behavior:

Both CPU and GPU should raise an INVALID_ARGUMENT error if indices are out of range to ensure consistent behavior and avoid silent failures or undefined values. Specifically, no tensor should be created with values like [4468, 0, 0] without any validation error.


### Relevant log output

```shell
2024-10-30 17:39:27.333559: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: indices[1] = 1 is not in [0, 1)
CPU Error:
{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = 1 is not in [0, 1) [Op:GatherV2]
GPU Result:
{'as_strided_result': array([[4468,    0,    0],
       [4468,    0,    0]])}
```",Cirno-2000,2024-10-30 21:42:38+00:00,['Venkat6871'],2024-11-28 02:06:43+00:00,2024-11-28 02:06:40+00:00,https://github.com/tensorflow/tensorflow/issues/79063,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2453926794, 'issue_id': 2625410751, 'author': 'Venkat6871', 'body': 'Hi **@Cirno-2000** ,\r\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab with TensorFlow version 2.17.0 and faced a different issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/51bf6c259d927c91c52b2bbdade20b97/79063_tf_2-18-cpu-v.ipynb) here for reference. Let me know if I made any mistakes. Thank you!', 'created_at': datetime.datetime(2024, 11, 4, 6, 44, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465733757, 'issue_id': 2625410751, 'author': 'Cirno-2000', 'body': 'Hi @Venkat6871, sorry for the delay. I tried your [gist](https://colab.research.google.com/gist/Venkat6871/51bf6c259d927c91c52b2bbdade20b97/79063_tf_2-18-cpu-v.ipynb) and it returned results successfully.\r\n\r\nHave you run the code with a GPU runtime, based on the error message?', 'created_at': datetime.datetime(2024, 11, 8, 20, 54, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472472732, 'issue_id': 2625410751, 'author': 'Venkat6871', 'body': 'Hi **@Cirno-2000** ,\r\nApologies for the delay. I tried using the GPU runtime also, but I am facing the same issue as with the CPU. Please refer to the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b69e4558c53ba2e8d56096a2560135ec/79063_tf-nightly-2-19-0-v.ipynb) here for more details. Let me know if I made any mistakes.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 13, 6, 2, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913697, 'issue_id': 2625410751, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124164, 'issue_id': 2625410751, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505124218, 'issue_id': 2625410751, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79063"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79063"">No</a>', 'created_at': datetime.datetime(2024, 11, 28, 2, 6, 41, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-04 06:44:31 UTC): Hi **@Cirno-2000** ,
Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab with TensorFlow version 2.17.0 and faced a different issue. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/51bf6c259d927c91c52b2bbdade20b97/79063_tf_2-18-cpu-v.ipynb) here for reference. Let me know if I made any mistakes. Thank you!

Cirno-2000 (Issue Creator) on (2024-11-08 20:54:13 UTC): Hi @Venkat6871, sorry for the delay. I tried your [gist](https://colab.research.google.com/gist/Venkat6871/51bf6c259d927c91c52b2bbdade20b97/79063_tf_2-18-cpu-v.ipynb) and it returned results successfully.

Have you run the code with a GPU runtime, based on the error message?

Venkat6871 (Assginee) on (2024-11-13 06:02:26 UTC): Hi **@Cirno-2000** ,
Apologies for the delay. I tried using the GPU runtime also, but I am facing the same issue as with the CPU. Please refer to the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b69e4558c53ba2e8d56096a2560135ec/79063_tf-nightly-2-19-0-v.ipynb) here for more details. Let me know if I made any mistakes.
Thank you!

github-actions[bot] on (2024-11-21 02:04:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-28 02:06:39 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-28 02:06:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79063"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79063"">No</a>

"
2625343517,issue,closed,completed,Inconsistent Results Between CPU and GPU for `tf.math.pow`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04 jammy

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

When running the `tf.math.pow` function with specific large exponent values on the CPU and GPU, the results are significantly different. Specifically, for the same input, the CPU returns zero, while the GPU produces an extremely large result.

### Standalone code to reproduce the issue

#### Code:
```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file


def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)


def tensorflow_pow_version(input, cpu=True):
    # Set seed for reproducibility
    set_seed()

    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        # Unpack input dictionary
        input_tensor = tf.constant(input[""input""])
        exponent_tensor = tf.constant(input[""exponent""])

        # Apply TensorFlow equivalent
        result = tf.math.pow(input_tensor, exponent_tensor)

        return {""float_power_result"": result.numpy()}


def load_safe_tensor(file_path):
    """"""Loads the safe tensor from the specified file path using safetensors library.""""""
    safe_tensor_data = load_file(file_path)

    # Identify the correct keys for the tensors
    key_input = ""input""  # Assuming ""input"" is the key for the base tensor
    key_exponent = ""exponent""  # Assuming ""exponent"" is the key for the exponent tensor

    # Extract the tensors using the keys
    input_tensor = safe_tensor_data[key_input]
    exponent_tensor = safe_tensor_data[key_exponent]

    # Convert the loaded tensors to TensorFlow tensors
    return {
        ""input"": tf.convert_to_tensor(input_tensor.numpy()),
        ""exponent"": tf.convert_to_tensor(exponent_tensor.numpy()),
    }


def main():
    # Load the safe tensor file
    file_path = ""tensorflow_float_power_9.safetensors""  # Path to your safetensors file

    # Load the safe tensor
    input_data = load_safe_tensor(file_path)

    # Run on CPU
    result_cpu = tensorflow_pow_version(input_data, cpu=True)
    print(""CPU Result:"", result_cpu)

    # Run on GPU
    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_pow_version(input_data, cpu=False)
        print(""GPU Result:"", result_gpu)
    else:
        print(""GPU not available."")


if __name__ == ""__main__"":
    main()

```

#### Colab:
```shell
https://colab.research.google.com/drive/1GCUvc_FXk5p9nbFD2G_OOxhDx6Usp5uN?usp=sharing
```

The safetensors can be found here at: [Google Drive](https://drive.google.com/file/d/19k9IJAijb0wAOkYPKmlm44zTzXBWEPMS/view?usp=sharing)

### Relevant log output

```shell
CPU Result: {'float_power_result': array([0])}
GPU Result: {'float_power_result': array([402509216696238080])}
```
```
",Cirno-2000,2024-10-30 21:06:26+00:00,['tilakrayal'],2024-11-08 20:39:11+00:00,2024-11-08 20:39:08+00:00,https://github.com/tensorflow/tensorflow/issues/79060,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2454068949, 'issue_id': 2625343517, 'author': 'tilakrayal', 'body': '@Cirno-2000,\r\nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/58479\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 4, 8, 20, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465711813, 'issue_id': 2625343517, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79060"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79060"">No</a>', 'created_at': datetime.datetime(2024, 11, 8, 20, 39, 10, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-04 08:20:26 UTC): @Cirno-2000,
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU.

https://github.com/tensorflow/tensorflow/issues/58479

Thank you!

google-ml-butler[bot] on (2024-11-08 20:39:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79060"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79060"">No</a>

"
2624392887,issue,closed,completed,Tensorflow DLL (Dynamic Link Library) Load Failure,"Here's the error: 

_ImportError:_ DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
Failed to load the native TensorFlow runtime.

_Major Action:_ Multiple versions of tensorflow and python have already been executed yet the error pertains. 

_Tools Used:_ Django, Conda (for virtual env)",HappyStackCoder,2024-10-30 14:59:48+00:00,['Venkat6871'],2025-01-03 17:01:03+00:00,2024-11-23 02:02:41+00:00,https://github.com/tensorflow/tensorflow/issues/79041,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2453857225, 'issue_id': 2624392887, 'author': 'Venkat6871', 'body': 'Hi **@HappyStackCoder** ,\r\nApologies for the delay, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 4, 5, 33, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456868627, 'issue_id': 2624392887, 'author': 'Fahadxx', 'body': 'anyone found a proper solution for the CPU not supporting AVX2?', 'created_at': datetime.datetime(2024, 11, 5, 11, 0, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805419, 'issue_id': 2624392887, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212161, 'issue_id': 2624392887, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569544293, 'issue_id': 2624392887, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 17, 1, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-04 05:33:11 UTC): Hi **@HappyStackCoder** ,
Apologies for the delay, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.
https://github.com/tensorflow/tensorflow/issues/61887
Thank you!

Fahadxx on (2024-11-05 11:00:59 UTC): anyone found a proper solution for the CPU not supporting AVX2?

github-actions[bot] on (2024-11-15 02:05:16 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

mihaimaruseac on (2025-01-03 17:01:02 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

"
2623495311,issue,closed,completed,TensorFlow Ignores Logging Configurations and Outputs Unwanted Warnings,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

(Colab runtime) Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm encountering an issue where TensorFlow outputs runtime warnings messages despite configuring logging to suppress them. Messages are logged even though `TF_CPP_MIN_LOG_LEVEL` is set to ""3"" and `absl.logging` is configured to show only errors. Using a custom context manager to suppress the output doesn't work either, which suggests that certain TensorFlow modules might override these logging settings.

### Standalone code to reproduce the issue

You can try this snippet also on [this](https://colab.research.google.com/drive/1PRrDJo-K3n5U2zgTrH9_XeujVq3EySTX?usp=sharing) colab notebook. Please check the runtime warnings.

```python
import os
import sys
import io
from contextlib import contextmanager
from absl import logging as absl_logging

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""

absl_logging.set_verbosity(absl_logging.ERROR)

import tensorflow as tf
from tensorflow.python.framework.convert_to_constants import (
    convert_variables_to_constants_v2,
)
from keras import Sequential
from keras.layers import Dense, Input


# Define a context manager to suppress both stdout and stderr
@contextmanager
def suppress_output():
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    # Create temporary string buffers for both
    temp_stdout = io.StringIO()
    temp_stderr = io.StringIO()
    try:
        # Redirect stdout and stderr to the temporary buffers
        sys.stdout = temp_stdout
        sys.stderr = temp_stderr
        yield
    finally:
        # Restore the original stdout and stderr
        sys.stdout = old_stdout
        sys.stderr = old_stderr


# Define the model
input_shape = (1,)
keras_model = Sequential([Input(shape=input_shape), Dense(1)])
full_model = tf.function(lambda x: keras_model(x))
full_model = full_model.get_concrete_function(
    x=tf.TensorSpec(shape=input_shape, dtype=tf.float32)
)

# Even this suppression does not work
with suppress_output():
    frozen_func = convert_variables_to_constants_v2(
        full_model, lower_control_flow=False
    )
    # Output to STDERR here:
    # WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
    # I0000 00:00:1730280536.054103    6179 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0

print(f""TensorFlow Version: {tf.__version__}"")
```


### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1730280536.054103    6179 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
```
",arianmaghsoudnia,2024-10-30 09:58:53+00:00,['tilakrayal'],2024-11-23 02:02:46+00:00,2024-11-23 02:02:42+00:00,https://github.com/tensorflow/tensorflow/issues/79029,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2462361527, 'issue_id': 2623495311, 'author': 'tilakrayal', 'body': '@arianmaghsoudnia,\r\nApologies for the delay. Could you please check whether you are facing with the latest tensorflow v2.18 and also please create a virtual environment and test your code again. Thank you!', 'created_at': datetime.datetime(2024, 11, 7, 14, 21, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477805449, 'issue_id': 2623495311, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 15, 2, 5, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212183, 'issue_id': 2623495311, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495212222, 'issue_id': 2623495311, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79029"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79029"">No</a>', 'created_at': datetime.datetime(2024, 11, 23, 2, 2, 45, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-07 14:21:26 UTC): @arianmaghsoudnia,
Apologies for the delay. Could you please check whether you are facing with the latest tensorflow v2.18 and also please create a virtual environment and test your code again. Thank you!

github-actions[bot] on (2024-11-15 02:05:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-23 02:02:42 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-23 02:02:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79029"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/79029"">No</a>

"
2622686734,issue,open,,[TF-2.18] Protoc-related Segmentation Fault on GH200 when Building from Source ,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

Rocky Linux 9.3 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

17.0.6 (LLVM)

### CUDA/cuDNN version

CUDA-12.4.1/cuDNN-8.9.6

### GPU model and memory

Grace Hopper GH200 

### Current behavior?

Since there is no pip package for GH200, and Amazon only provides CPU wheel for `aarch64`, we are building `tf-2.18` from src. 

```
$ git branch 
master 
* r2.18
```
Both LLVM and Bazel versions are according to recommendation
```
$ clang --version 
clang version 17.0.6
Target: aarch64-unknown-linux-gnu
Thread model: posix
``` 
``` 
$ bazel --version 
bazel 6.5.0- (@non-git)
```

Bazel build failed with protoc-related segmentation fault. 

### Standalone code to reproduce the issue

```shell
./configure 
Please specify the location of python. [Default is /scratch/optpar01/.conda/tf2_src/bin/python3]: 

Found possible Python library paths:
  /scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.4.1

Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 8.9.6

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.0,8.0,9.0

Please specify the local CUDA path you want to use or leave empty to use the default version. 

Please specify the local CUDNN path you want to use or leave empty to use the default version. 

Please specify the local NCCL path you want to use or leave empty to use the default version. 


Do you want to use clang as CUDA compiler? [Y/n]: 
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin/clang]: 

You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: --copt=-Wno-error=unused-command-line-argument

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
```


### Relevant log output

```shell
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --verbose_failures
ERROR: /scratch/optpar01/.cache/_bazel_optpar01/1b4d76cef27e0cbebd5791dfb99ed3a8/external/local_tsl/tsl/profiler/protobuf/BUILD:36:17: Action external/local_tsl/tsl/profiler/protobuf/profiler_service.grpc.pb.h failed: (Segmentation fault): protoc failed: error executing command (from target @local_tsl//tsl/profiler/protobuf:_profiler_service_cc_grpc_proto_grpc_codegen) 
  (cd /scratch/optpar01/.cache/_bazel_optpar01/1b4d76cef27e0cbebd5791dfb99ed3a8/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin/clang-17 \
    LD_LIBRARY_PATH=/apps/ARM_node/ARM_applications/python/3.12.4/lib:/apps/ARM_node/ARM_applications/python/3.12.4/lib/python3.12:/usr/lib64:/usr/lib64/slurm:/usr/local/lib \
    PATH=/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/bazel-6.5.0-es6l5xjprjwauy22iuwlxoj4uztdcxsu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/zip-3.0-pxcrfxbv4w274tczkoz7t4a54rv3r5de/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/clang-17.0.6/openjdk-11.0.23_9-s6cgcclfuzjttl5cdh3evsp3rrmuuvo6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/bazel-6.5.0-es6l5xjprjwauy22iuwlxoj4uztdcxsu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/zip-3.0-pxcrfxbv4w274tczkoz7t4a54rv3r5de/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/bzip2-1.0.8-jvd2b77w6emrwhj2g7mquhr7oteeb2uk/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/clang-17.0.6/openjdk-11.0.23_9-s6cgcclfuzjttl5cdh3evsp3rrmuuvo6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/unzip-6.0-fpkt734eot7dakwydys5mgo5yomxwac3/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/curl-8.7.1-ca7uqkpgoorf7yzvajfyab2dfty7eitr/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/swig-4.1.1-gpwxntk2voqulme22h55ngp5ztgzyg6x/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/pcre2-10.43-bzcntkq3ul5umao7fcke5w4vrlyssbk7/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/lua-5.3.6-lec5qldurljnfjg4jo2fvsekfughzwru/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/unzip-6.0-fpkt734eot7dakwydys5mgo5yomxwac3/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/readline-8.2-47up4u4jjwf4io45zeioeyrh3wwwg4tg/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/curl-8.7.1-ca7uqkpgoorf7yzvajfyab2dfty7eitr/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/openssl-3.3.1-7uxchckf5qrcojfgntn77hvtoxjjkrq6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/nghttp2-1.62.0-x5y5gbq2qybl4iiqvjv7bflc2nlx35zu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/hwloc-2.9.3-onlxonz6ibms2b6fxhk7wqcdpo5l5myn/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/ncurses-6.5-fvbf5zfdj2bcwylse6cv6mztrwadhgyv/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/binutils-2.43.1-t3kxibwizkzbb7e3boxm3qkpngjuzzns/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/zstd-1.5.6-miqeogpdx7z7lcovzh3bsfwr2tzihxw2/bin:/scratch/optpar01/.conda/tf2_src/bin:/apps/ARM_node/ARM_applications/Miniconda/24.5.0/condabin:/apps/ARM_node/ARM_applications/python/3.12.4/bin:/home01/optpar01/.cargo/bin:/scratch/optpar01/spack/bin:/home01/optpar01/apps/build/gv/3.7.4/bin:/apps/applications/htop/3.0.5:/apps/applications/nvtop/1.1.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home01/optpar01/bin \
    PYTHON_BIN_PATH=/scratch/optpar01/.conda/tf2_src/bin/python3 \
    PYTHON_LIB_PATH=/scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages \
    SPACK_LOADED_HASHES=es6l5xjprjwauy22iuwlxoj4uztdcxsu:52ysnwfyyhc6tchulc5wlen7h6yc3fju \
    SPACK_PYTHON=/usr/bin/python3 \
    SPACK_ROOT=/scratch/optpar01/spack \
    TF2_BEHAVIOR=1 \
  bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_google_protobuf/protoc '--plugin=protoc-gen-PLUGIN=bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_github_grpc_grpc/src/compiler/grpc_cpp_plugin' '--PLUGIN_out=services_namespace=grpc,generate_mock_code=true:bazel-out/aarch64-opt/bin/external/local_tsl' '--proto_path=external/local_tsl' '--proto_path=external/local_tsl' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto' '--proto_path=external/local_tsl' '--proto_path=bazel-out/aarch64-opt/bin/external/local_tsl/external/local_tsl' external/local_tsl/tsl/profiler/protobuf/profiler_service.proto)
```
```
",vitduck,2024-10-30 02:15:04+00:00,['Venkat6871'],2024-11-13 06:51:36+00:00,,https://github.com/tensorflow/tensorflow/issues/78994,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2460180805, 'issue_id': 2622686734, 'author': 'Venkat6871', 'body': 'Hi **@vitduck** ,\r\nApologies for the delay, thank you for raising your issue here. I tried running your command on the CPU and I faced the following issue: ```FAILED: Build did NOT complete successfully```. Could you please confirm whether you ran it with a GPU or CPU? If the issue is GPU-related, I will attempt to replicate it using a GPU. Here i am attaching screenshot for your reference.\r\n<img width=""1728"" alt=""Screenshot 2024-11-06 at 9 21 31\u202fPM"" src=""https://github.com/user-attachments/assets/545d2b70-b869-45a5-b82d-9cfc580e0ba5"">\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 6, 16, 5, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461074713, 'issue_id': 2622686734, 'author': 'vitduck', 'body': ""Hi @Venkat6871 \r\nThanks for replying. \r\n\r\nDuring configure state, I have selected CUDA. as shown the original post. \r\n```\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n```\r\nI am using a bare metal GH200 node. \r\nThe compilation is done by the ARM CPU. So I don't think it matters whether the command is run on CPU or GPU. \r\nBut please correct me if I have misunderstood. \r\n\r\nMy guess is that the instruction sets used to build `protoc` is incorrect for `aarch64`, generating segmentation fault. \r\n`bazel` is too heavy-handed and cryptic, so I could not figure out what is wrong."", 'created_at': datetime.datetime(2024, 11, 7, 0, 26, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472606848, 'issue_id': 2622686734, 'author': 'Venkat6871', 'body': '@learning-to-play ,', 'created_at': datetime.datetime(2024, 11, 13, 6, 51, 29, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-11-06 16:05:39 UTC): Hi **@vitduck** ,
Apologies for the delay, thank you for raising your issue here. I tried running your command on the CPU and I faced the following issue: ```FAILED: Build did NOT complete successfully```. Could you please confirm whether you ran it with a GPU or CPU? If the issue is GPU-related, I will attempt to replicate it using a GPU. Here i am attaching screenshot for your reference.
<img width=""1728"" alt=""Screenshot 2024-11-06 at 9 21 31PM"" src=""https://github.com/user-attachments/assets/545d2b70-b869-45a5-b82d-9cfc580e0ba5"">

Thank you!

vitduck (Issue Creator) on (2024-11-07 00:26:22 UTC): Hi @Venkat6871 
Thanks for replying. 

During configure state, I have selected CUDA. as shown the original post. 
```
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
```
I am using a bare metal GH200 node. 
The compilation is done by the ARM CPU. So I don't think it matters whether the command is run on CPU or GPU. 
But please correct me if I have misunderstood. 

My guess is that the instruction sets used to build `protoc` is incorrect for `aarch64`, generating segmentation fault. 
`bazel` is too heavy-handed and cryptic, so I could not figure out what is wrong.

Venkat6871 (Assginee) on (2024-11-13 06:51:29 UTC): @learning-to-play ,

"
2622361161,issue,closed,completed,Grouped Conv3D convolutions don't work with some of the number of input filters,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.9.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA version 11.2, cuDNN version 8.1.0

### GPU model and memory

NVIDIA A100 80GB

### Current behavior?

As the title says, grouped 3D convolutions don't work if we have some specific combinations of input / output filters and batch size.  If I try to execute the standalone code I provided, the script crashes with the log output I provided. If I reduce the number of groups  to 1, the calculations proceed as expected.
If I reduce the `batch_size` to 8, the forward pass of the layers is performed successfully, but the following warning is shown:
```
2024-10-29 21:25:47.628087: W tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:727] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[8,64,64,64,128]{3,2,1,4,0}, u8[0]{0}) custom-call(f32[8,64,64,64,512]{3,2,1,4,0}, f32[1,1,1,256,128]{2,1,0,3,4}), window={size=1x1x1}, dim_labels=b012f_012io->b012f, feature_group_count=2, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers

conv_layer = layers.Conv3D(
    kernel_size=1,
    filters=128,
    groups=2,
)

batch_size = 16
layer_input = tf.random.normal((batch_size, 64, 64, 64, 512))
output = conv_layer(layer_input)
print(output.shape)
```


### Relevant log output

```shell
2024-10-29 21:18:03.441000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-29 21:18:04.710425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78924 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:35:00.0, compute capability: 8.0
2024-10-29 21:18:04.712408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78924 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:36:00.0, compute capability: 8.0
2024-10-29 21:18:06.075880: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x47fb3520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-10-29 21:18:06.075966: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2024-10-29 21:18:06.075979: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2024-10-29 21:18:06.742974: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2024-10-29 21:18:08.492106: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-10-29 21:18:08.492559: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:296 : UNKNOWN: Failed to determine best cudnn convolution algorithm for:
%cudnn-conv = (f32[16,64,64,64,128]{3,2,1,4,0}, u8[0]{0}) custom-call(f32[16,64,64,64,512]{3,2,1,4,0} %copy, f32[1,1,1,256,128]{2,1,0,3,4} %copy.1), window={size=1x1x1}, dim_labels=b012f_012io->b012f, feature_group_count=2, custom_call_target=""__cudnn$convForward"", metadata={op_type=""Conv3D"" op_name=""Conv3D"" source_file=""/home/ssh/.local/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py"" source_line=225}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""

Original error: UNKNOWN: CUDNN_STATUS_NOT_SUPPORTED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(3520): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed

To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.
Traceback (most recent call last):
  File ""xla_test.py"", line 12, in <module>
    output = conv_layer(layer_input)
  File ""/home/ssh/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ssh/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError: Exception encountered when calling layer ""conv3d"" (type Conv3D).

Failed to determine best cudnn convolution algorithm for:
%cudnn-conv = (f32[16,64,64,64,128]{3,2,1,4,0}, u8[0]{0}) custom-call(f32[16,64,64,64,512]{3,2,1,4,0} %copy, f32[1,1,1,256,128]{2,1,0,3,4} %copy.1), window={size=1x1x1}, dim_labels=b012f_012io->b012f, feature_group_count=2, custom_call_target=""__cudnn$convForward"", metadata={op_type=""Conv3D"" op_name=""Conv3D"" source_file=""/home/ssh/.local/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py"" source_line=225}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""

Original error: UNKNOWN: CUDNN_STATUS_NOT_SUPPORTED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(3520): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed

To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning. [Op:__inference__jit_compiled_convolution_op_28]

Call arguments received by layer ""conv3d"" (type Conv3D):
   inputs=tf.Tensor(shape=(16, 64, 64, 64, 512), dtype=float32)
```
",dan-kazbek,2024-10-29 21:28:29+00:00,['tilakrayal'],2024-11-21 02:04:12+00:00,2024-11-21 02:04:09+00:00,https://github.com/tensorflow/tensorflow/issues/78984,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2458902151, 'issue_id': 2622361161, 'author': 'tilakrayal', 'body': '@dan-kazbek,\r\nLooks like this is an issue which is related to Keras. So, could you please raise the issue in the [Keras-team/Keras](https://github.com/keras-team/keras/issues) repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 11, 6, 7, 36, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206668, 'issue_id': 2622361161, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913741, 'issue_id': 2622361161, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489913780, 'issue_id': 2622361161, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78984"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78984"">No</a>', 'created_at': datetime.datetime(2024, 11, 21, 2, 4, 11, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-11-06 07:36:24 UTC): @dan-kazbek,
Looks like this is an issue which is related to Keras. So, could you please raise the issue in the [Keras-team/Keras](https://github.com/keras-team/keras/issues) repo for the quick resolution. Thank you!

github-actions[bot] on (2024-11-14 02:01:17 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-21 02:04:09 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-21 02:04:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78984"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78984"">No</a>

"
2622238653,issue,closed,completed,GPU support tensorflow NVIDIA JETSON ORIN NX 8GB UBUNTU 22.04,"https://github.com/tensorflow/tensorflow/issues/62711 - came across this and we can install but trying to run on python3.12 Jetpack 6.0 and having trouble getting the gpu recognized. nvcc --version 12.2 and LB_LIBRARY_PATH is fine. Still can't get this running. No and-cuda apparently. Anything special to get it to recognize the gpu?
Tried the link in the post as well as 12.16.1 python 3.10/2 without success
Wheel file not supporting the format although uname -a agrees with arch64 in releases
Please help",kevqcpp,2024-10-29 20:16:33+00:00,['Venkat6871'],2024-11-01 17:39:12+00:00,2024-11-01 17:39:09+00:00,https://github.com/tensorflow/tensorflow/issues/78983,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2446676098, 'issue_id': 2622238653, 'author': 'Venkat6871', 'body': 'Hi **@kevqcpp** ,\r\nPlease check all compatibility versions for that specific TensorFlow version. Here, I am providing [documentation](https://www.tensorflow.org/install/source) for your reference. Also, please let us know where you are facing the issue and which versions you are using. This will make it easier for us to troubleshoot.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 30, 11, 18, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447186192, 'issue_id': 2622238653, 'author': 'kevqcpp', 'body': ""Advertised as compatible 12.16.1 - python 3.10\r\nMy understanding was that this installation uses a different version from those listed in the tensorflow datasheet\r\nhttps://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html\r\n\r\nThe user who mentioned that they could get gpu working on a jetson used the dp distro and idk what that is or why I should use that but it doesn't appear that I can install it for gpu use although I can install the library. How to enable gpu on jetson?\r\nJetpack 6.0\r\nnvcc --version 12.2"", 'created_at': datetime.datetime(2024, 10, 30, 13, 35, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448318146, 'issue_id': 2622238653, 'author': 'kevqcpp', 'body': 'Updated to jetpack 6.1 -comes with cuda 12.6\r\nchanged cudnn path links LD_LIBRARY and PATH to include 12.6.1 Python 3.10/12 (both advertise support)\r\nTensorflow 12.16.1\r\nStill no GPU', 'created_at': datetime.datetime(2024, 10, 30, 20, 33, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452301647, 'issue_id': 2622238653, 'author': 'kevqcpp', 'body': 'Reflashed the device with Jetpack version and only one python and it worked fine', 'created_at': datetime.datetime(2024, 11, 1, 17, 39, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452301727, 'issue_id': 2622238653, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78983"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78983"">No</a>', 'created_at': datetime.datetime(2024, 11, 1, 17, 39, 11, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-30 11:18:03 UTC): Hi **@kevqcpp** ,
Please check all compatibility versions for that specific TensorFlow version. Here, I am providing [documentation](https://www.tensorflow.org/install/source) for your reference. Also, please let us know where you are facing the issue and which versions you are using. This will make it easier for us to troubleshoot.
Thank you!

kevqcpp (Issue Creator) on (2024-10-30 13:35:17 UTC): Advertised as compatible 12.16.1 - python 3.10
My understanding was that this installation uses a different version from those listed in the tensorflow datasheet
https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html

The user who mentioned that they could get gpu working on a jetson used the dp distro and idk what that is or why I should use that but it doesn't appear that I can install it for gpu use although I can install the library. How to enable gpu on jetson?
Jetpack 6.0
nvcc --version 12.2

kevqcpp (Issue Creator) on (2024-10-30 20:33:29 UTC): Updated to jetpack 6.1 -comes with cuda 12.6
changed cudnn path links LD_LIBRARY and PATH to include 12.6.1 Python 3.10/12 (both advertise support)
Tensorflow 12.16.1
Still no GPU

kevqcpp (Issue Creator) on (2024-11-01 17:39:08 UTC): Reflashed the device with Jetpack version and only one python and it worked fine

google-ml-butler[bot] on (2024-11-01 17:39:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78983"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78983"">No</a>

"
2621322207,issue,closed,completed,tf.keras.Concatenate fails with 5 dimensional tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5.1 / 9

### GPU model and memory

A100 80G

### Current behavior?

When trying to use a keras Concatenate layer with two 5 dimensional tensors with all but the last dimension to 1, there is an ""IndexError"". In the provided example the result should be a tensor of shape (1, 1, 1, 1, 10). It works ok with <=4 dimensional tensors or when any of the first four dimension is not one.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.keras.layers.Concatenate()([
    np.arange(5).reshape(1, 1, 1, 1, 5),
    np.arange(5).reshape(1, 1, 1, 1, 5)
])
```


### Relevant log output

```shell
import numpy as np
      3 import tensorflow as tf
----> 4 tf.keras.layers.Concatenate()([
      5     np.arange(5).reshape(1, 1, 1, 1, 5),
      6     np.arange(5).reshape(1, 1, 1, 1, 5)
      7 ])

File ~/.conda/pa311tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.conda/pa311tf/lib/python3.11/site-packages/keras/src/layers/merging/concatenate.py:70, in Concatenate.build(self, input_shape)
     60 for axis, axis_value in enumerate(
     61     reduced_inputs_shapes[i][1:], start=1
     62 ):
   (...)
     67     # but if tensor shapes are not the same when
     68     # calling, an exception will be raised.
     69     if axis != concat_axis and axis_value == 1:
---> 70         del reduced_inputs_shapes[i][axis]
     72 if len(reduced_inputs_shapes[i]) > self.axis:
     73     del reduced_inputs_shapes[i][self.axis]

IndexError: list assignment index out of range
```
",pboutinaud,2024-10-29 13:48:27+00:00,['tilakrayal'],2024-10-31 08:53:08+00:00,2024-10-31 08:53:05+00:00,https://github.com/tensorflow/tensorflow/issues/78957,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:keras', 'Keras related issues'), ('TF 2.18', '')]","[{'comment_id': 2449305804, 'issue_id': 2621322207, 'author': 'tilakrayal', 'body': '@pboutinaud,\r\nI guess the problem might be due to Keras3.0 which contains by default with the tensorflow v2.17 and v2.18. When I tried to execute the same code with the Keras2.0(tf-keras), it was executed without fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/512b07148f1c838e09f4cbe03537b07f/untitled2212.ipynb).\r\n\r\n```python\r\n!pip install tf-keras==2.18.0\r\nimport tf_keras as keras\r\n```\r\n\r\nFor Keras3.0, please raise the request in the Keras-team/Keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 10, 31, 8, 28, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449347913, 'issue_id': 2621322207, 'author': 'pboutinaud', 'body': 'Ok, thank you', 'created_at': datetime.datetime(2024, 10, 31, 8, 53, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449347968, 'issue_id': 2621322207, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78957"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78957"">No</a>', 'created_at': datetime.datetime(2024, 10, 31, 8, 53, 7, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-31 08:28:16 UTC): @pboutinaud,
I guess the problem might be due to Keras3.0 which contains by default with the tensorflow v2.17 and v2.18. When I tried to execute the same code with the Keras2.0(tf-keras), it was executed without fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/512b07148f1c838e09f4cbe03537b07f/untitled2212.ipynb).

```python
!pip install tf-keras==2.18.0
import tf_keras as keras
```

For Keras3.0, please raise the request in the Keras-team/Keras repo for the quick resolution. Thank you!

pboutinaud (Issue Creator) on (2024-10-31 08:53:05 UTC): Ok, thank you

google-ml-butler[bot] on (2024-10-31 08:53:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78957"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78957"">No</a>

"
2621235290,issue,closed,completed,Build from source v2.18.0 - countless warnings e.g. Do not pass an `input_shape`/`input_dim` argument to a layer.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

13.2

### Bazel version

bazilisk v1.22.1

### GCC/compiler version

gcc (Ubuntu 13.2.0-23ubuntu4) 13.2.0

### CUDA/cuDNN version

12.6.1 / 9.4.0

### GPU model and memory

 NVIDIA GeForce RTX 4060 Ti

### Current behavior?

I'm building from source, tag v2.18.0. Compilation succeeds with the **configuration below.** When used I get countless warnings. GPU seems to be used.

I'm running the basic example from the startpage of the web site and get the following output:

2024-10-29 13:56:32.470690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730206592.497706   98446 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730206592.505972   98446 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-29 13:56:32.530518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/user/source/tf-test/venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
I0000 00:00:1730206595.503757   98446 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14163 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9
2024-10-29 13:56:36.992391: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.
2024-10-29 13:56:37.185922: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.
Epoch 1/5
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1730206598.144274   98498 service.cc:148] XLA service 0x76ccf8002630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1730206598.144353   98498 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9
2024-10-29 13:56:38.176299: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1730206598.266340   98498 cuda_dnn.cc:529] Loaded cuDNN version 90400
I0000 00:00:1730206600.512484   98498 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1875/1875  7s 2ms/step - accuracy: 0.8608 - loss: 0.4798    
Epoch 2/5
1875/1875  4s 2ms/step - accuracy: 0.9552 - loss: 0.1478   
Epoch 3/5
1875/1875  4s 2ms/step - accuracy: 0.9674 - loss: 0.1076   
Epoch 4/5
1875/1875  4s 2ms/step - accuracy: 0.9746 - loss: 0.0843   
Epoch 5/5
1875/1875  4s 2ms/step - accuracy: 0.9777 - loss: 0.0713   
2024-10-29 13:56:59.770413: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 4 bytes spill stores, 4 bytes spill loads

313/313  3s 5ms/step - accuracy: 0.9724 - loss: 0.0882

Is my build corrupt or what did I do wrong? I'm very thankful for any hint and help. Thank you!

### Standalone code to reproduce the issue

```shell
Compiling from code (checkout tag v2.18.0):

./configure 
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.12/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.6.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 9.4.0

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.9


Please specify the local CUDA path you want to use or leave empty to use the default version. /usr/local/cuda-12.6/


Please specify the local CUDNN path you want to use or leave empty to use the default version. /usr/include/


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: Y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]:


You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

host:~/source/tensorflow$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=nogcp --config=nonccl
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /home/user/source/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=181
INFO: Reading rc options for 'build' from /home/user/source/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/user/source/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --defi    ine=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visib    bility
INFO: Reading rc options for 'build' from /home/user/source/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --    -config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/user/source/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/user/source/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/user/source/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm        
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.6.1 --repo_env HERMETIC_CUDNN_VERSION=9.4.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.9 --repo_env LOCAL_CUDA_PATH=/usr/local/cuda-12.6/ --repo_env LOCAL_CUDNN_PATH=/usr/include/
INFO: Found applicable config definition build:cuda_clang in file /home/user/source/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm        
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.6.1 --repo_env HERMETIC_CUDNN_VERSION=9.4.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.9 --repo_env LOCAL_CUDA_PATH=/usr/local/cuda-12.6/ --repo_env LOCAL_CUDNN_PATH=/usr/include/
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /home/user/source/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.6.1 --repo_env HERMETIC_CUDNN_VERSION=9.4.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.9 --repo_env LOCAL_CUDA_PATH=/usr/local/cuda-12.6/ --repo_env LOCAL_CUDNN_PATH=/usr/include/
INFO: Found applicable config definition build:cuda_wheel in file /home/user/source/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:nogcp in file /home/user/source/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nonccl in file /home/user/source/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /home/user/source/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt    t=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxop    pt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/user/source/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/user/.cache/bazel/_bazel_user/60b540d7b9200554d741de3bca139495/external/local_xla/third_party/py/python_repo.bzl:96:14:
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.12 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /home/user/.cache/bazel/_bazel_user/60b540d7b9200554d741de3bca139495/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.12                 
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /home/user/.cache/bazel/_bazel_user/60b540d7b9200554d741de3bca139495/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl       
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (757 packages loaded, 55850 targets configured).
INFO: Found 1 target...
```


### Relevant log output

_No response_",LeeABarron,2024-10-29 13:18:25+00:00,['Venkat6871'],2024-11-29 02:06:31+00:00,2024-11-29 02:06:28+00:00,https://github.com/tensorflow/tensorflow/issues/78955,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2445842144, 'issue_id': 2621235290, 'author': 'Venkat6871', 'body': 'Hi **@LeeABarron** ,\r\nThank you for raising your issue here. I noticed there may be a version compatibility problem. Although everything compiled successfully, any compatibility issues could lead to performance problems. It is best to install compatible versions. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) here for your reference, please review it and check all compatibility versions. Let us know if the issue persists.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 30, 4, 50, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446005539, 'issue_id': 2621235290, 'author': 'LeeABarron', 'body': 'Hi @Venkat6871 \r\nThank you for your response and your time.\r\n\r\nI did 2.17 before and dealt with another issue that\'s why I went to 2.18.\r\nAt time of build this table was not updated yet for 2.18 - so I went with the max versions of third_party\\xla\\third_party\\tsl\\third_party\\gpus\\cuda\\hermetic\\cuda_redist_versions.bzl at the time of the tag v2.18.0\r\nI guess this mean these max versions (12.6.1 and 9.4) are ""experimental"" or for development only. Did I miss another way to find the max recommended versions from the code, when the table is not updated yet?\r\n\r\nI will happily give it a whirl with 2.5.1 and 9.3 and post my results. Thank you very much.', 'created_at': datetime.datetime(2024, 10, 30, 6, 52, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452372767, 'issue_id': 2621235290, 'author': 'LeeABarron', 'body': ""Unfortunately exactly the same - build successful (should I post configuration?) but warnings remain:\r\n\r\n2024-11-01 19:16:54.296179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730485014.331975   26232 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730485014.342833   26232 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-01 19:16:54.382102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11490434/11490434  2s 0us/step \r\n/home/user/source/tf-eval/venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\r\n  super().__init__(**kwargs)\r\nI0000 00:00:1730485023.031375   26232 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14242 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\r\nEpoch 1/5\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1730485027.010202   26313 service.cc:148] XLA service 0x7804e00083c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\nI0000 00:00:1730485027.010277   26313 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\r\n2024-11-01 19:17:07.141875: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nI0000 00:00:1730485027.408144   26313 cuda_dnn.cc:529] Loaded cuDNN version 90300\r\nI0000 00:00:1730485029.842869   26313 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n1875/1875  8s 2ms/step - accuracy: 0.8568 - loss: 0.4844    \r\nEpoch 2/5\r\n1875/1875  4s 2ms/step - accuracy: 0.9567 - loss: 0.1485   \r\nEpoch 3/5\r\n1875/1875  4s 2ms/step - accuracy: 0.9688 - loss: 0.1038   \r\nEpoch 4/5\r\n1875/1875  4s 2ms/step - accuracy: 0.9734 - loss: 0.0850   \r\nEpoch 5/5\r\n1875/1875  4s 2ms/step - accuracy: 0.9755 - loss: 0.0749   \r\n2024-11-01 19:17:46.978835: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 4 bytes spill stores, 4 bytes spill loads\r\n\r\n313/313  3s 5ms/step - accuracy: 0.9720 - loss: 0.0944\r\n\r\nWhat could I try? Thank you very much."", 'created_at': datetime.datetime(2024, 11, 1, 18, 27, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453420757, 'issue_id': 2621235290, 'author': 'Senantq', 'body': 'The warning ""To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags"" is CPU-related and has nothing to do with the CUDA and CUDNN versions. Using -march=native as a CPU flag when you use the configure.py would do the job. For the recommended CUDA and CUDNN versions, there are in the file .bazelrc of the repo, search for the line ""build:cuda_clang_official --repo_env=HERMETIC_CUDA_VERSION"" and ""build:cuda_clang_official --repo_env=HERMETIC_CUDNN_VERSION"" for your rc. The other warnings are just there and don\'t seem to impact training', 'created_at': datetime.datetime(2024, 11, 3, 13, 3, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475546982, 'issue_id': 2621235290, 'author': 'Venkat6871', 'body': 'Hi **@LeeABarron** ,\r\nApologies for the delay. For the similar issue one more issue was raised and still it is in open state.  please follow them for further updates. I am providing the [link](https://github.com/tensorflow/tensorflow/issues/62075) to that issue here for your reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 14, 6, 49, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492724382, 'issue_id': 2621235290, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 22, 2, 4, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506954668, 'issue_id': 2621235290, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 29, 2, 6, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506954705, 'issue_id': 2621235290, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78955"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78955"">No</a>', 'created_at': datetime.datetime(2024, 11, 29, 2, 6, 30, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-30 04:50:57 UTC): Hi **@LeeABarron** ,
Thank you for raising your issue here. I noticed there may be a version compatibility problem. Although everything compiled successfully, any compatibility issues could lead to performance problems. It is best to install compatible versions. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) here for your reference, please review it and check all compatibility versions. Let us know if the issue persists.

Thank you!

LeeABarron (Issue Creator) on (2024-10-30 06:52:57 UTC): Hi @Venkat6871 
Thank you for your response and your time.

I did 2.17 before and dealt with another issue that's why I went to 2.18.
At time of build this table was not updated yet for 2.18 - so I went with the max versions of third_party\xla\third_party\tsl\third_party\gpus\cuda\hermetic\cuda_redist_versions.bzl at the time of the tag v2.18.0
I guess this mean these max versions (12.6.1 and 9.4) are ""experimental"" or for development only. Did I miss another way to find the max recommended versions from the code, when the table is not updated yet?

I will happily give it a whirl with 2.5.1 and 9.3 and post my results. Thank you very much.

LeeABarron (Issue Creator) on (2024-11-01 18:27:14 UTC): Unfortunately exactly the same - build successful (should I post configuration?) but warnings remain:

2024-11-01 19:16:54.296179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730485014.331975   26232 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730485014.342833   26232 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-01 19:16:54.382102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434  2s 0us/step 
/home/user/source/tf-eval/venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
I0000 00:00:1730485023.031375   26232 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14242 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9
Epoch 1/5
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1730485027.010202   26313 service.cc:148] XLA service 0x7804e00083c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1730485027.010277   26313 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9
2024-11-01 19:17:07.141875: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1730485027.408144   26313 cuda_dnn.cc:529] Loaded cuDNN version 90300
I0000 00:00:1730485029.842869   26313 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1875/1875  8s 2ms/step - accuracy: 0.8568 - loss: 0.4844    
Epoch 2/5
1875/1875  4s 2ms/step - accuracy: 0.9567 - loss: 0.1485   
Epoch 3/5
1875/1875  4s 2ms/step - accuracy: 0.9688 - loss: 0.1038   
Epoch 4/5
1875/1875  4s 2ms/step - accuracy: 0.9734 - loss: 0.0850   
Epoch 5/5
1875/1875  4s 2ms/step - accuracy: 0.9755 - loss: 0.0749   
2024-11-01 19:17:46.978835: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 4 bytes spill stores, 4 bytes spill loads

313/313  3s 5ms/step - accuracy: 0.9720 - loss: 0.0944

What could I try? Thank you very much.

Senantq on (2024-11-03 13:03:09 UTC): The warning ""To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags"" is CPU-related and has nothing to do with the CUDA and CUDNN versions. Using -march=native as a CPU flag when you use the configure.py would do the job. For the recommended CUDA and CUDNN versions, there are in the file .bazelrc of the repo, search for the line ""build:cuda_clang_official --repo_env=HERMETIC_CUDA_VERSION"" and ""build:cuda_clang_official --repo_env=HERMETIC_CUDNN_VERSION"" for your rc. The other warnings are just there and don't seem to impact training

Venkat6871 (Assginee) on (2024-11-14 06:49:04 UTC): Hi **@LeeABarron** ,
Apologies for the delay. For the similar issue one more issue was raised and still it is in open state.  please follow them for further updates. I am providing the [link](https://github.com/tensorflow/tensorflow/issues/62075) to that issue here for your reference.
Thank you!

github-actions[bot] on (2024-11-22 02:04:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-29 02:06:27 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-29 02:06:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78955"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78955"">No</a>

"
2619999753,issue,closed,completed,tensorflow importation error after installation,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow as tf

File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\elish\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_",Alliswell1234,2024-10-29 03:00:44+00:00,['tilakrayal'],2025-01-03 17:01:11+00:00,2024-11-14 02:01:20+00:00,https://github.com/tensorflow/tensorflow/issues/78903,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.10', '')]","[{'comment_id': 2446018540, 'issue_id': 2619999753, 'author': 'tilakrayal', 'body': '@Alliswell1234,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 30, 7, 3, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461161969, 'issue_id': 2619999753, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 7, 2, 0, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206730, 'issue_id': 2619999753, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206812, 'issue_id': 2619999753, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78903"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78903"">No</a>', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569544487, 'issue_id': 2619999753, 'author': 'mihaimaruseac', 'body': 'Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.', 'created_at': datetime.datetime(2025, 1, 3, 17, 1, 10, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-30 07:03:26 UTC): @Alliswell1234,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-11-07 02:00:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-14 02:01:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-14 02:01:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78903"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78903"">No</a>

mihaimaruseac on (2025-01-03 17:01:10 UTC): Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.

"
2619957667,issue,closed,completed,[v2.18] Compiling llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:RISCVCodeGen',"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

Ubuntu clang version 18.1.8

### CUDA/cuDNN version

CUDA 12.2/cuDNN 8

### GPU model and memory

NVIDIA A30 24GiB * 4

### Current behavior?

I am trying to build Tensorflow v2.18.0 from source, before compiling I choose the CUDA and clang option enabled and other options is the default setting, but it shows an error:
```
ERROR: /workspaces/cache/bazel/external/llvm-project/llvm/BUILD.bazel:2535:16: Compiling llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:RISCVCodeGen':
this rule is missing dependency declarations for the following files included by 'llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/RISCVCommonTableGen/RISCVGenGlobalISel.inc'
Target //tensorflow/tools/pip_package:wheel failed to build
```

### Standalone code to reproduce the issue

```shell
I use the following command to build:

bazel --output_base=/workspaces/cache/bazel     build //tensorflow/tools/pip_package:wheel     --repo_env=WHEEL_NAME=tensorflow     --config=cuda     --config=cuda_wheel     --action_env=HTTP_PROXY=http://127.0.0.1:7890     --action_env=HTTPS_PROXY
=http://127.0.0.1:7890 --copt=-Wno-gnu-offsetof-extensions --verbose_failures
```



### Relevant log output

```shell
root@op-arsenaldevk8s-gpu02:/workspaces/com_github/tensorflow# bazel --output_base=/workspaces/cache/bazel     build //tensorflow/tools/pip_package:wheel     --repo_env=WHEEL_NAME=tensorflow     --config=cuda     --config=cuda_wheel     --action_env=HTTP_PROXY=http://127.0.0.1:7890     --action_env=HTTPS_PROXY=http://127.0.0.1:7890 --copt=-Wno-gnu-offsetof-extensions --verbose_failures
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: 170d549b-a095-4437-bd4c-b61483e541a1
INFO: Reading 'startup' options from /workspaces/com_github/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=318
INFO: Reading rc options for 'build' from /workspaces/com_github/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --remote_cache=http://localhost:7070
INFO: Reading rc options for 'build' from /workspaces/com_github/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /workspaces/com_github/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/bin/python3 --action_env PYTHON_LIB_PATH=/opt/conda/lib/python3.10/site-packages --python_path=/bin/python3 --action_env LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/:/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-18/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /workspaces/com_github/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /workspaces/com_github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /workspaces/com_github/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.0
INFO: Found applicable config definition build:cuda_clang in file /workspaces/com_github/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.0
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /workspaces/com_github/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.0
INFO: Found applicable config definition build:cuda_wheel in file /workspaces/com_github/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:linux in file /workspaces/com_github/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /workspaces/com_github/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /workspaces/cache/bazel/external/llvm-project/llvm/BUILD.bazel:2535:16: Compiling llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:RISCVCodeGen':
this rule is missing dependency declarations for the following files included by 'llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/RISCVCommonTableGen/RISCVGenGlobalISel.inc'
Target //tensorflow/tools/pip_package:wheel failed to build
ERROR: /workspaces/com_github/tensorflow/tensorflow/tools/pip_package/BUILD:271:9 Action tensorflow/tools/pip_package/wheel_house failed: undeclared inclusion(s) in rule '@llvm-project//llvm:RISCVCodeGen':
this rule is missing dependency declarations for the following files included by 'llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/RISCVCommonTableGen/RISCVGenGlobalISel.inc'
INFO: Elapsed time: 433.437s, Critical Path: 194.62s
INFO: 3371 processes: 2 remote cache hit, 225 internal, 3144 local.
FAILED: Build did NOT complete successfully
```
",fuhailin,2024-10-29 02:26:54+00:00,['Venkat6871'],2024-11-01 08:53:32+00:00,2024-11-01 08:53:29+00:00,https://github.com/tensorflow/tensorflow/issues/78902,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2445809467, 'issue_id': 2619957667, 'author': 'Venkat6871', 'body': 'Hi **@fuhailin** ,\r\nThank you for raising your issue here. I noticed that there is a version compatibility problem. Before building, please verify that all versions are compatible as specified in the documentation. This will help ensure a smooth build process without issues. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) here for your reference, please review it and check all compatibility versions. Let us know if the issue still persists.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 30, 4, 20, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451536038, 'issue_id': 2619957667, 'author': 'fuhailin', 'body': '> Hi **@fuhailin** , Thank you for raising your issue here. I noticed that there is a version compatibility problem. Before building, please verify that all versions are compatible as specified in the documentation. This will help ensure a smooth build process without issues. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) here for your reference, please review it and check all compatibility versions. Let us know if the issue still persists.\r\n> \r\n> Thank you!\r\n\r\nThanks for your reply, I followed the document you provided here, and change my cuda version to the same with the document, the compiling error still existed.\r\n But I try the method here https://github.com/tensorflow/tensorflow/issues/55563#issuecomment-1312393701 , add `--spawn_strategy=sandboxed` solved this problem for me too.', 'created_at': datetime.datetime(2024, 11, 1, 8, 52, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451536843, 'issue_id': 2619957667, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78902"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78902"">No</a>', 'created_at': datetime.datetime(2024, 11, 1, 8, 53, 31, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-30 04:20:40 UTC): Hi **@fuhailin** ,
Thank you for raising your issue here. I noticed that there is a version compatibility problem. Before building, please verify that all versions are compatible as specified in the documentation. This will help ensure a smooth build process without issues. I am providing the [documentation](https://www.tensorflow.org/install/source#gpu) here for your reference, please review it and check all compatibility versions. Let us know if the issue still persists.

Thank you!

fuhailin (Issue Creator) on (2024-11-01 08:52:53 UTC): Thanks for your reply, I followed the document you provided here, and change my cuda version to the same with the document, the compiling error still existed.
 But I try the method here https://github.com/tensorflow/tensorflow/issues/55563#issuecomment-1312393701 , add `--spawn_strategy=sandboxed` solved this problem for me too.

google-ml-butler[bot] on (2024-11-01 08:53:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78902"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78902"">No</a>

"
2619475744,issue,closed,completed,Tensorflow Lite Reduce binary size,"This is BUILD File as given in the documentation,
load(
    ""//tensorflow/lite:build_def.bzl"",
    ""tflite_custom_cc_library"",
    ""tflite_cc_shared_object"",
)

tflite_custom_cc_library(
    name = ""selectively_built_cc_lib"",
    models = [
        "":auto.tflite"",
    ],
)

# Shared lib target for convenience, pulls in the core runtime and builtin ops.
# Note: This target is not yet finalized, and the exact set of exported (C/C++)
# APIs is subject to change. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite.so`
#   - Mac: `libtensorflowlite.dylib`
#   - Windows: `tensorflowlite.dll`
tflite_cc_shared_object(
    name = ""tensorflowlite"",
    # Until we have more granular symbol export for the C++ API on Windows,
    # export all symbols.
    features = [""windows_export_all_symbols""],
    linkopts = select({
        ""//tensorflow:macos"": [
            ""-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)"",
        ],
        ""//tensorflow:windows"": [],
        ""//conditions:default"": [
            ""-Wl,-z,defs"",
            ""-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)"",
        ],
    }),
    per_os_targets = True,
    deps = [
        "":selectively_built_cc_lib"",
        ""//tensorflow/lite:tflite_exported_symbols.lds"",
        ""//tensorflow/lite:tflite_version_script.lds"",
    ],
)

ERROR: C:/rishik/tensorflow-2.17.0/tmp/BUILD:7:25: gen_selected_ops failed: (Exit -1073741819): generate_op_registrations.exe failed: error executing command (from target //tmp:selectively_built_cc_lib_registration)
  cd /d C:/rishik/bazel4/5lnkvv5w/execroot/org_tensorflow
bazel-out\x64_windows-opt-exec-BB41B15F\bin\tensorflow\lite\tools\generate_op_registrations.exe --namespace= --output_registration=bazel-out/x64_windows-opt/bin/tmp/selectively_built_cc_lib_registration_registration.cc --tflite_path=tensorflow/lite --input_models=tmp/auto.tflite
# Configuration: d8b6abda1087700c56031d915b13c37cfb693d4da0543a50c2b5e6165b4e15e1
# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl
Target //tmp:tensorflowlite failed to build
INFO: Elapsed time: 268.072s, Critical Path: 46.35s
INFO: 1278 processes: 489 internal, 789 local.
FAILED: Build did NOT complete successfully

This error is shown, how do I resolve it

",rishik1001,2024-10-28 20:31:24+00:00,['gaikwadrahul8'],2024-11-14 02:01:26+00:00,2024-11-14 02:01:21+00:00,https://github.com/tensorflow/tensorflow/issues/78894,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2447331675, 'issue_id': 2619475744, 'author': 'gaikwadrahul8', 'body': 'Hi, @rishik1001 \r\n\r\nThank you for bringing this issue to our attention, if possible could you please help us with complete steps or code snippets with Google colab to replicate the same behavior from our end to investigate this issue further ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 10, 30, 14, 23, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461162014, 'issue_id': 2619475744, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 7, 2, 0, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206776, 'issue_id': 2619475744, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475206874, 'issue_id': 2619475744, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78894"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78894"">No</a>', 'created_at': datetime.datetime(2024, 11, 14, 2, 1, 25, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-10-30 14:23:37 UTC): Hi, @rishik1001 

Thank you for bringing this issue to our attention, if possible could you please help us with complete steps or code snippets with Google colab to replicate the same behavior from our end to investigate this issue further ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-11-07 02:00:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-14 02:01:21 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-14 02:01:25 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78894"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78894"">No</a>

"
2617981709,issue,closed,completed,libtensorflow libraries for >2.15,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Windows Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Is libtensorflow still being built for every version?
According to the documentation [here](https://www.tensorflow.org/install/lang_c) it is, but according to the GCS bucket in that page which links to [the bucket](https://storage.googleapis.com/libtensorflow-nightly), there hasn't been any new builds for two years.

The last official version available is 2.15

### Standalone code to reproduce the issue

```shell
No code involved to reproduce
```


### Relevant log output

_No response_",MattiasDC,2024-10-28 10:24:12+00:00,['gaikwadrahul8'],2024-12-06 10:19:59+00:00,2024-12-06 10:19:56+00:00,https://github.com/tensorflow/tensorflow/issues/78869,"[('type:docs-bug', 'Document issues'), ('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('TF 2.16', '')]","[{'comment_id': 2520624395, 'issue_id': 2617981709, 'author': 'gaikwadrahul8', 'body': 'Hi, @MattiasDC \r\nI apologize for the delayed response, Thank you for reporting the issue regarding Nightly libtensorflow C packages availability. I see in this [official documentation](https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds) the status of Nightly Binary Official GCS marked as **Status Temporarily Unavailable** but we do have libtensorflow packages for stable version of TensorFlow available so at the moment I would suggest you to please go with stable libtensorflow C packages which you can download and extract it for more information please refer this [official documentation](https://www.tensorflow.org/install/lang_c)\r\n\r\nFor future update regarding Nightly libtensorflow C packages availability you can see the status here : https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 12, 5, 15, 26, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520794951, 'issue_id': 2617981709, 'author': 'gaikwadrahul8', 'body': 'Hi, @MattiasDC\r\nCould you please confirm if this issue is resolved for you now, For future update regarding Nightly libtensorflow C packages availability you can see the status here : https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds ? \r\n\r\nPlease feel free to close the issue if it is resolved ? Thank you', 'created_at': datetime.datetime(2024, 12, 5, 16, 11, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522750923, 'issue_id': 2617981709, 'author': 'MattiasDC', 'body': 'Thanks for all the info!', 'created_at': datetime.datetime(2024, 12, 6, 10, 19, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522750970, 'issue_id': 2617981709, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78869"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78869"">No</a>', 'created_at': datetime.datetime(2024, 12, 6, 10, 19, 58, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-12-05 15:26:16 UTC): Hi, @MattiasDC 
I apologize for the delayed response, Thank you for reporting the issue regarding Nightly libtensorflow C packages availability. I see in this [official documentation](https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds) the status of Nightly Binary Official GCS marked as **Status Temporarily Unavailable** but we do have libtensorflow packages for stable version of TensorFlow available so at the moment I would suggest you to please go with stable libtensorflow C packages which you can download and extract it for more information please refer this [official documentation](https://www.tensorflow.org/install/lang_c)

For future update regarding Nightly libtensorflow C packages availability you can see the status here : https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds

Thank you for your cooperation and patience.

gaikwadrahul8 (Assginee) on (2024-12-05 16:11:09 UTC): Hi, @MattiasDC
Could you please confirm if this issue is resolved for you now, For future update regarding Nightly libtensorflow C packages availability you can see the status here : https://github.com/tensorflow/tensorflow?tab=readme-ov-file#official-builds ? 

Please feel free to close the issue if it is resolved ? Thank you

MattiasDC (Issue Creator) on (2024-12-06 10:19:56 UTC): Thanks for all the info!

google-ml-butler[bot] on (2024-12-06 10:19:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78869"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78869"">No</a>

"
2616487197,issue,closed,not_planned,Unable to build TensorFlow 2.18 with GCC,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

GCC 13.2.0

### CUDA/cuDNN version

CUDA 12.6.2, cuDNN 8.9.7.29

### GPU model and memory

_No response_

### Current behavior?

When compiling TF 2.18 with GCC, the build passes many CUDA flags to the compiler:
```
ERROR: /tmp/spack6u_rodz5/1b99beed63669ad175919c6b77052d81/external/local_xla/xla/service/gpu/BUILD:2647:13: Compiling xla/service/gpu/make_batch_pointers.cu.cc failed: (Exit 1): gcc failed: error executing command (from target @local_xla//xla/service/gpu:make_batch_pointers_kernel)
...
gcc: error: unrecognized command-line option '--cuda-gpu-arch=sm_35'
gcc: error: unrecognized command-line option '--cuda-include-ptx=sm_70'
gcc: error: unrecognized command-line option '--cuda-gpu-arch=sm_70'
gcc: error: unrecognized command-line option '-Xcuda-fatbinary=--compress-all'
gcc: error: unrecognized command-line option '-nvcc_options=expt-relaxed-constexpr'
```
I would expect these flags to be passed to nvcc instead of GCC.

P.S. Yes, I know that Clang 17.0.6 is the only supported compiler, but I am nevertheless trying to build with GCC. I'm hoping another community member can help me figure out how to build with GCC.

I experienced a similar issue when building JAX with GCC: https://github.com/jax-ml/jax/issues/23689

### Standalone code to reproduce the issue

I'm using the following build script: https://github.com/spack/spack/pull/47211

Then I run:
```console
> spack install py-tensorflow
```

### Relevant log output

* [build log](https://github.com/user-attachments/files/17533526/spack-build-out.txt)
* [build env](https://github.com/user-attachments/files/17533525/spack-build-env-mods.txt)
",adamjstewart,2024-10-27 10:25:18+00:00,"['learning-to-play', 'tilakrayal']",2024-11-05 10:25:48+00:00,2024-10-29 14:28:59+00:00,https://github.com/tensorflow/tensorflow/issues/78846,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.18', '')]","[{'comment_id': 2443467201, 'issue_id': 2616487197, 'author': 'tilakrayal', 'body': 'cc @learning-to-play', 'created_at': datetime.datetime(2024, 10, 29, 7, 39, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444012060, 'issue_id': 2616487197, 'author': 'learning-to-play', 'body': 'TensorFlow supports building with Clang.', 'created_at': datetime.datetime(2024, 10, 29, 11, 58, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444012186, 'issue_id': 2616487197, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 11, 58, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444419095, 'issue_id': 2616487197, 'author': 'robertu94', 'body': ""Strictly speaking, the [docs](https://www.tensorflow.org/install/source) say:\r\n\r\n```\r\nClang is a C/C++/Objective-C compiler that is compiled in C++ based on LLVM. It is the default compiler to build TensorFlow starting with TensorFlow 2.13. The current supported version is LLVM/Clang 17.\r\n```\r\n\r\nThat does not mean that GCC+NVCC is unsupported or categorically forbidden.  It just says that it isn't default.  I understand that Google may not want to shoulder the burden of maintaining multiple compilers, especially given the complexities of GPU programming toolchains, but they should just say so if that is the case.\r\n\r\nResponding that\r\n\r\n```\r\nTensorFlow supports building with Clang.\r\n```\r\n\r\nIs unresponsive to the bug being reported, and it certainly isn't `completed` in that there is a fix or work around that exists for this issue.  A resolution like `closed wontfix` would be more appropriate."", 'created_at': datetime.datetime(2024, 10, 29, 14, 22, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444436776, 'issue_id': 2616487197, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 14, 29, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444444669, 'issue_id': 2616487197, 'author': 'learning-to-play', 'body': ""Changed to closed won't fix.\r\nPlease let me know if you prefer me to reopen this ticket, as other users may have comments about the issues for GCC build."", 'created_at': datetime.datetime(2024, 10, 29, 14, 31, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444484837, 'issue_id': 2616487197, 'author': 'adamjstewart', 'body': 'I think you\'re going to find that it is difficult to simply ""not support"" the world\'s most common family of compilers. It would be more realistic to support a good default compiler family on each OS:\r\n\r\n* Linux: GCC\r\n* macOS: Xcode/Apple Clang\r\n* Windows: Visual Studio\r\n\r\nEven if you close this issue, other people will open new issues to report the same problem. If Google is not willing or able to support GCC, the rest of the community will.\r\n\r\nAll of this is to say that Google doesn\'t have to support anything they don\'t want to, but I would greatly appreciate it if standard compilers like GCC were still supported by someone (either Google or the greater TF community).', 'created_at': datetime.datetime(2024, 10, 29, 14, 46, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456792730, 'issue_id': 2616487197, 'author': 'adamjstewart', 'body': 'FYI, we managed to solve the GCC build issue. In addition to:\r\n```bash\r\nTF_NEED_CUDA=1\r\nTF_CUDA_COMPUTE_CAPABILITIES=...\r\n```\r\nwe also needed to set:\r\n```bash\r\nCUDA_NVCC=1\r\nHERMETIC_CUDA_COMPUTE_CAPABILITIES=...\r\n```\r\nThe Spack TF recipe now correctly builds on Linux x86_64 and aarch64 for GCC and Clang.', 'created_at': datetime.datetime(2024, 11, 5, 10, 25, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-29 07:39:51 UTC): cc @learning-to-play

learning-to-play (Assginee) on (2024-10-29 11:58:41 UTC): TensorFlow supports building with Clang.

google-ml-butler[bot] on (2024-10-29 11:58:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">No</a>

robertu94 on (2024-10-29 14:22:57 UTC): Strictly speaking, the [docs](https://www.tensorflow.org/install/source) say:

```
Clang is a C/C++/Objective-C compiler that is compiled in C++ based on LLVM. It is the default compiler to build TensorFlow starting with TensorFlow 2.13. The current supported version is LLVM/Clang 17.
```

That does not mean that GCC+NVCC is unsupported or categorically forbidden.  It just says that it isn't default.  I understand that Google may not want to shoulder the burden of maintaining multiple compilers, especially given the complexities of GPU programming toolchains, but they should just say so if that is the case.

Responding that

```
TensorFlow supports building with Clang.
```

Is unresponsive to the bug being reported, and it certainly isn't `completed` in that there is a fix or work around that exists for this issue.  A resolution like `closed wontfix` would be more appropriate.

google-ml-butler[bot] on (2024-10-29 14:29:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78846"">No</a>

learning-to-play (Assginee) on (2024-10-29 14:31:54 UTC): Changed to closed won't fix.
Please let me know if you prefer me to reopen this ticket, as other users may have comments about the issues for GCC build.

adamjstewart (Issue Creator) on (2024-10-29 14:46:57 UTC): I think you're going to find that it is difficult to simply ""not support"" the world's most common family of compilers. It would be more realistic to support a good default compiler family on each OS:

* Linux: GCC
* macOS: Xcode/Apple Clang
* Windows: Visual Studio

Even if you close this issue, other people will open new issues to report the same problem. If Google is not willing or able to support GCC, the rest of the community will.

All of this is to say that Google doesn't have to support anything they don't want to, but I would greatly appreciate it if standard compilers like GCC were still supported by someone (either Google or the greater TF community).

adamjstewart (Issue Creator) on (2024-11-05 10:25:47 UTC): FYI, we managed to solve the GCC build issue. In addition to:
```bash
TF_NEED_CUDA=1
TF_CUDA_COMPUTE_CAPABILITIES=...
```
we also needed to set:
```bash
CUDA_NVCC=1
HERMETIC_CUDA_COMPUTE_CAPABILITIES=...
```
The Spack TF recipe now correctly builds on Linux x86_64 and aarch64 for GCC and Clang.

"
2615849864,issue,closed,completed, Why no mac-intel-x86 version tensorflow after tensorflow 2.16.2?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

mac 15.0.1

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't install tensorflow 2.17 2.18 version because there is no relase version for my sytem.
does no support mac intel chip anymore after 2.16.2 version?
<img width=""477"" alt=""image"" src=""https://github.com/user-attachments/assets/dbe20bf6-c6cd-4f0e-982d-3b80f42543c9"">


### Standalone code to reproduce the issue

```shell
no code
I can pip_search tensroflow 2.18
but only 2.16.2 can been downloaded.
and no wheel file can found .
```


### Relevant log output

_No response_",sexjun,2024-10-26 12:59:34+00:00,['tilakrayal'],2024-10-29 00:44:03+00:00,2024-10-29 00:43:59+00:00,https://github.com/tensorflow/tensorflow/issues/78836,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('subtype:macOS', 'macOS Build/Installation issues')]","[{'comment_id': 2439794362, 'issue_id': 2615849864, 'author': 'sexjun', 'body': 'mac intel chip anaconda found 2.17 version tensroflow. \r\nit sloved my problem. iree work expect.\r\n\r\ndoes google will not support mac with intel chip after tensorflow 2.17 but community does?\r\n```\r\n(tf)   offcial_guide_tf git:(20241017)  conda  list | grep tensorflow\r\ntensorflow                2.17.0          cpu_py310hb3d9b59_0\r\ntensorflow-base           2.17.0          cpu_py310ha37eea7_0\r\ntensorflow-estimator      2.17.0          cpu_py310h7aa49ff_0\r\n```', 'created_at': datetime.datetime(2024, 10, 27, 1, 30, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441626562, 'issue_id': 2615849864, 'author': 'tilakrayal', 'body': '@sexjun,\r\nHave you got the chance to have a look at the official release document where it was mentioned that Mac x86 builds are being deprecated and will no longer be released as a Pip package from TF 2.17 onwards.\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.16.1\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 28, 13, 42, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442939164, 'issue_id': 2615849864, 'author': 'sexjun', 'body': ""@tilakrayal \r\nThanks very much , That's what i need."", 'created_at': datetime.datetime(2024, 10, 29, 0, 43, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442939201, 'issue_id': 2615849864, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78836"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78836"">No</a>', 'created_at': datetime.datetime(2024, 10, 29, 0, 44, 1, tzinfo=datetime.timezone.utc)}]","sexjun (Issue Creator) on (2024-10-27 01:30:12 UTC): mac intel chip anaconda found 2.17 version tensroflow. 
it sloved my problem. iree work expect.

does google will not support mac with intel chip after tensorflow 2.17 but community does?
```
(tf)   offcial_guide_tf git:(20241017)  conda  list | grep tensorflow
tensorflow                2.17.0          cpu_py310hb3d9b59_0
tensorflow-base           2.17.0          cpu_py310ha37eea7_0
tensorflow-estimator      2.17.0          cpu_py310h7aa49ff_0
```

tilakrayal (Assginee) on (2024-10-28 13:42:04 UTC): @sexjun,
Have you got the chance to have a look at the official release document where it was mentioned that Mac x86 builds are being deprecated and will no longer be released as a Pip package from TF 2.17 onwards.
https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1

Thank you!

sexjun (Issue Creator) on (2024-10-29 00:43:59 UTC): @tilakrayal 
Thanks very much , That's what i need.

google-ml-butler[bot] on (2024-10-29 00:44:01 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78836"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78836"">No</a>

"
2615620522,issue,open,,`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import os
from tensorflow.python.ops import lookup_ops

def _createVocabFile(basename, values=('brain', 'salad', 'surgery')):
    vocabulary_file = os.path.join(""/tmp"", basename)
    with open(vocabulary_file, 'w') as f:
        f.write('\n'.join(values) + '\n')
    return vocabulary_file

vocab_file = _createVocabFile('feat_to_id_1.txt')
vocab_size = 9223372036854775807
oov_buckets = 1
table = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",cybersupersoap,2024-10-26 07:19:51+00:00,['Venkat6871'],2024-11-07 15:30:41+00:00,,https://github.com/tensorflow/tensorflow/issues/78831,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2443295064, 'issue_id': 2615620522, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on tensorflow 2.17, 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/33422c16b92fe89e80dc0577f8762a6d/78831_tf_2-17-0-nightly-v.ipynb)', 'created_at': datetime.datetime(2024, 10, 29, 6, 7, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454728249, 'issue_id': 2615620522, 'author': 'aroun-coumar', 'body': 'Why is the vocab_size so large?\r\nThis could be the cause for the segmentation fault\r\nTry using some normal range\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 11, 4, 13, 32, 42, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-29 06:07:59 UTC): I was able to reproduce the issue on tensorflow 2.17, 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/33422c16b92fe89e80dc0577f8762a6d/78831_tf_2-17-0-nightly-v.ipynb)

aroun-coumar on (2024-11-04 13:32:42 UTC): Why is the vocab_size so large?
This could be the cause for the segmentation fault
Try using some normal range

Thanks!

"
2615618759,issue,open,,"`gen_list_ops.tensor_list_concat_v2` aborts with ""Check failed: size >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [[gist](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_list_ops
from tensorflow.python.ops import list_ops

l = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)
t = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)
Aborted (core dumped)
```
",cybersupersoap,2024-10-26 07:15:16+00:00,['tilakrayal'],2024-10-30 06:50:07+00:00,,https://github.com/tensorflow/tensorflow/issues/78830,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2446001971, 'issue_id': 2615618759, 'author': 'tilakrayal', 'body': '@cybersupersoap,\r\nI was able to reproduce the tensorflow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/38b8a31d8452ae19ad55ceae1223cafb/untitled2208.ipynb).\r\n\r\n![image](https://github.com/user-attachments/assets/9887a62f-d227-4b3a-85e2-bccdc70b505a)', 'created_at': datetime.datetime(2024, 10, 30, 6, 49, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-10-30 06:49:54 UTC): @cybersupersoap,
I was able to reproduce the tensorflow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/38b8a31d8452ae19ad55ceae1223cafb/untitled2208.ipynb).

![image](https://github.com/user-attachments/assets/9887a62f-d227-4b3a-85e2-bccdc70b505a)

"
2615617721,issue,closed,completed,"data_flow_ops.FIFOQueue aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1AlRfxK85FE1U5NB4xkSHdliDvG7IRC2_?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.eager import context
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.eager import context
import tensorflow as tf
with context.graph_mode():
    q = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())
    q.dequeue_many([1, 2, 3, 4])
```


### Relevant log output

```shell
Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```
",cybersupersoap,2024-10-26 07:12:18+00:00,['Venkat6871'],2025-01-15 02:00:08+00:00,2025-01-15 02:00:05+00:00,https://github.com/tensorflow/tensorflow/issues/78829,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.18', '')]","[{'comment_id': 2443211428, 'issue_id': 2615617721, 'author': 'Venkat6871', 'body': 'I was able to reproduce the issue on tensorflow 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/b1149bd7e0620adf1a8549483f109525/78829_tf-2-18-0-nightly-v.ipynb)', 'created_at': datetime.datetime(2024, 10, 29, 4, 55, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481858141, 'issue_id': 2615617721, 'author': 'SanjaySG', 'body': 'The issue here is in the usage of FIFOQueue.dequeue_many(). The function expects a scalar or a scalar Tensor n corresponding to the number of elements to dequeue. Both dequeue_many() and dequeue_up_to() also expect all the queue elements to have a specific shape, which is set when the queue is initialized.\r\n\r\nFor example: \r\n```\r\n    >>> q = tf.queue.FIFOQueue(10, tf.int32, shapes=tf.TensorShape(2))\r\n    >>> q.enqueue(tf.constant([1, 2], dtype=tf.int32, shape=(2)))\r\n    >>> q.enqueue(tf.constant([3, 4], dtype=tf.int32, shape=(2)))\r\n    >>> q.enqueue(tf.constant([5, 6], dtype=tf.int32, shape=(2)))\r\n    >>> q.enqueue(tf.constant([7, 8], dtype=tf.int32, shape=(2)))\r\n    >>> q.dequeue_many(3)\r\n    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\r\n    array([[1, 2],\r\n       [3, 4],\r\n       [5, 6]], dtype=int32)>\r\n```\r\n\r\nI can send out a PR to add this example, along with examples for the other functions of FIFOQueue, to the code.', 'created_at': datetime.datetime(2024, 11, 18, 3, 15, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563272706, 'issue_id': 2615617721, 'author': 'SanjaySG', 'body': 'The examples for FIFOQueue usage have been added to data_flow_ops.py through: https://github.com/tensorflow/tensorflow/commit/6337f9fb09a2b1b3da9a81d0aa7d65bea298d934. The code changes were merged to the internal Google fork and copied over by the copybara-service. \r\n\r\nThis should help clarify the usage of the FIFOQueue functions.', 'created_at': datetime.datetime(2024, 12, 27, 3, 20, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564988839, 'issue_id': 2615617721, 'author': 'SanjaySG', 'body': '@Venkat6871 Can you please mark this as resolved?', 'created_at': datetime.datetime(2024, 12, 30, 3, 28, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565081703, 'issue_id': 2615617721, 'author': 'Venkat6871', 'body': 'Hi **@cybersupersoap** ,\r\nPlease take a look at the PR which was raised has been merged https://github.com/tensorflow/tensorflow/pull/83570. Also the examples were added for the FIFOQueue API.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L504\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 30, 6, 37, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250626, 'issue_id': 2615617721, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476525, 'issue_id': 2615617721, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 15, 2, 0, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591476587, 'issue_id': 2615617721, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78829"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78829"">No</a>', 'created_at': datetime.datetime(2025, 1, 15, 2, 0, 7, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-10-29 04:55:10 UTC): I was able to reproduce the issue on tensorflow 2.18 and tf-nightly. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/Venkat6871/b1149bd7e0620adf1a8549483f109525/78829_tf-2-18-0-nightly-v.ipynb)

SanjaySG on (2024-11-18 03:15:15 UTC): The issue here is in the usage of FIFOQueue.dequeue_many(). The function expects a scalar or a scalar Tensor n corresponding to the number of elements to dequeue. Both dequeue_many() and dequeue_up_to() also expect all the queue elements to have a specific shape, which is set when the queue is initialized.

For example: 
```
    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=
    array([[1, 2],
       [3, 4],
       [5, 6]], dtype=int32)>
```

I can send out a PR to add this example, along with examples for the other functions of FIFOQueue, to the code.

SanjaySG on (2024-12-27 03:20:38 UTC): The examples for FIFOQueue usage have been added to data_flow_ops.py through: https://github.com/tensorflow/tensorflow/commit/6337f9fb09a2b1b3da9a81d0aa7d65bea298d934. The code changes were merged to the internal Google fork and copied over by the copybara-service. 

This should help clarify the usage of the FIFOQueue functions.

SanjaySG on (2024-12-30 03:28:53 UTC): @Venkat6871 Can you please mark this as resolved?

Venkat6871 (Assginee) on (2024-12-30 06:37:38 UTC): Hi **@cybersupersoap** ,
Please take a look at the PR which was raised has been merged https://github.com/tensorflow/tensorflow/pull/83570. Also the examples were added for the FIFOQueue API.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L504
Thank you!

github-actions[bot] on (2025-01-07 02:02:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-15 02:00:04 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-15 02:00:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78829"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/78829"">No</a>

"
