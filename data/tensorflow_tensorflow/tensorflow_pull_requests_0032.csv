id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2526723054,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 03:48:09+00:00,[],2024-09-15 03:48:09+00:00,,https://github.com/tensorflow/tensorflow/pull/75804,[],[],
2526722896,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 03:47:23+00:00,[],2024-09-15 03:47:23+00:00,,https://github.com/tensorflow/tensorflow/pull/75803,[],[],
2526722887,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 03:47:20+00:00,[],2024-09-17 07:45:11+00:00,2024-09-17 07:45:11+00:00,https://github.com/tensorflow/tensorflow/pull/75802,[],[],
2526722140,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 03:43:51+00:00,[],2024-09-15 03:43:51+00:00,,https://github.com/tensorflow/tensorflow/pull/75801,[],[],
2526716144,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 03:14:03+00:00,[],2024-09-15 03:14:03+00:00,,https://github.com/tensorflow/tensorflow/pull/75800,[],[],
2526711850,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:56:14+00:00,[],2024-09-17 05:51:13+00:00,2024-09-17 05:51:12+00:00,https://github.com/tensorflow/tensorflow/pull/75799,[],[],
2526711669,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:55:25+00:00,[],2024-09-15 02:55:25+00:00,,https://github.com/tensorflow/tensorflow/pull/75798,[],[],
2526711665,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:55:24+00:00,[],2024-09-15 02:55:24+00:00,,https://github.com/tensorflow/tensorflow/pull/75797,[],[],
2526710545,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:50:52+00:00,[],2024-09-15 02:50:52+00:00,,https://github.com/tensorflow/tensorflow/pull/75796,[],[],
2526710365,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:50:07+00:00,[],2024-09-15 02:50:07+00:00,,https://github.com/tensorflow/tensorflow/pull/75795,[],[],
2526709066,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:44:23+00:00,[],2024-09-18 09:23:14+00:00,2024-09-18 09:23:13+00:00,https://github.com/tensorflow/tensorflow/pull/75794,[],[],
2526703625,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-15 02:34:34+00:00,[],2024-09-17 07:31:31+00:00,2024-09-17 07:31:30+00:00,https://github.com/tensorflow/tensorflow/pull/75793,[],[],
2526641327,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 22:30:26+00:00,[],2024-09-14 22:30:26+00:00,,https://github.com/tensorflow/tensorflow/pull/75792,[],[],
2526604207,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 20:55:20+00:00,[],2024-09-14 20:55:20+00:00,,https://github.com/tensorflow/tensorflow/pull/75791,[],[],
2526572741,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 19:13:16+00:00,[],2024-09-14 19:13:16+00:00,,https://github.com/tensorflow/tensorflow/pull/75790,[],[],
2526541421,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 17:53:21+00:00,[],2024-09-14 17:53:21+00:00,,https://github.com/tensorflow/tensorflow/pull/75789,[],[],
2526500958,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 16:28:05+00:00,[],2024-09-14 16:28:05+00:00,,https://github.com/tensorflow/tensorflow/pull/75788,[],[],
2526466001,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 15:07:56+00:00,[],2024-09-14 15:07:56+00:00,,https://github.com/tensorflow/tensorflow/pull/75787,[],[],
2526442944,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 14:10:21+00:00,[],2024-09-14 14:10:21+00:00,,https://github.com/tensorflow/tensorflow/pull/75786,[],[],
2526414894,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 13:30:48+00:00,[],2024-09-14 13:30:48+00:00,,https://github.com/tensorflow/tensorflow/pull/75784,[],[],
2526402504,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 13:19:12+00:00,[],2024-09-17 06:16:54+00:00,,https://github.com/tensorflow/tensorflow/pull/75783,[],[],
2526333099,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 12:13:54+00:00,[],2024-09-14 12:13:54+00:00,,https://github.com/tensorflow/tensorflow/pull/75782,[],[],
2526233843,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 10:42:54+00:00,[],2024-09-14 10:42:54+00:00,,https://github.com/tensorflow/tensorflow/pull/75781,[],[],
2526196022,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 10:09:27+00:00,[],2024-09-14 10:09:27+00:00,,https://github.com/tensorflow/tensorflow/pull/75780,[],[],
2526128913,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 09:07:59+00:00,[],2024-09-14 09:07:59+00:00,,https://github.com/tensorflow/tensorflow/pull/75779,[],[],
2526077293,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 08:20:23+00:00,[],2024-09-14 09:27:40+00:00,2024-09-14 09:27:39+00:00,https://github.com/tensorflow/tensorflow/pull/75778,[],[],
2526050606,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 07:29:34+00:00,[],2024-09-14 07:29:34+00:00,,https://github.com/tensorflow/tensorflow/pull/75777,[],[],
2526019354,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 06:09:31+00:00,[],2024-09-14 06:09:31+00:00,,https://github.com/tensorflow/tensorflow/pull/75776,[],[],
2526011902,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 05:50:27+00:00,[],2024-09-14 09:01:49+00:00,2024-09-14 09:01:48+00:00,https://github.com/tensorflow/tensorflow/pull/75775,[],[],
2526004051,pull_request,open,,cleanup: remove cc_stubby_versions from BUILD and bazel files,"cleanup: remove cc_stubby_versions from BUILD and bazel files
",copybara-service[bot],2024-09-14 05:26:28+00:00,[],2024-09-14 05:26:28+00:00,,https://github.com/tensorflow/tensorflow/pull/75774,[],[],
2525996970,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 05:04:50+00:00,[],2024-09-17 04:52:58+00:00,2024-09-17 04:52:57+00:00,https://github.com/tensorflow/tensorflow/pull/75772,[],[],
2525992356,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:50:11+00:00,[],2024-09-14 04:50:11+00:00,,https://github.com/tensorflow/tensorflow/pull/75771,[],[],
2525991893,pull_request,closed,,[XLA] Propagate the layout of layout constrained custom calls with higher,"[XLA] Propagate the layout of layout constrained custom calls with higher
priority because they have no ability to accept another layout.
",copybara-service[bot],2024-09-14 04:48:33+00:00,['blakehechtman'],2024-09-15 02:46:16+00:00,2024-09-15 02:46:15+00:00,https://github.com/tensorflow/tensorflow/pull/75770,[],[],
2525991793,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:48:13+00:00,[],2024-09-14 04:48:13+00:00,,https://github.com/tensorflow/tensorflow/pull/75769,[],[],
2525990523,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:43:45+00:00,[],2024-09-14 04:43:45+00:00,,https://github.com/tensorflow/tensorflow/pull/75768,[],[],
2525989564,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:40:32+00:00,[],2024-09-16 07:25:31+00:00,2024-09-16 07:25:30+00:00,https://github.com/tensorflow/tensorflow/pull/75767,[],[],
2525988037,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:35:43+00:00,[],2024-09-14 04:35:43+00:00,,https://github.com/tensorflow/tensorflow/pull/75766,[],[],
2525987066,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:32:46+00:00,[],2024-09-14 04:32:46+00:00,,https://github.com/tensorflow/tensorflow/pull/75765,[],[],
2525986706,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:31:37+00:00,[],2024-09-14 04:31:37+00:00,,https://github.com/tensorflow/tensorflow/pull/75764,[],[],
2525986074,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:29:39+00:00,[],2024-09-14 04:29:39+00:00,,https://github.com/tensorflow/tensorflow/pull/75763,[],[],
2525985154,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 04:26:45+00:00,[],2024-09-14 04:26:45+00:00,,https://github.com/tensorflow/tensorflow/pull/75762,[],[],
2525967741,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 03:31:43+00:00,[],2024-09-14 03:31:43+00:00,,https://github.com/tensorflow/tensorflow/pull/75761,[],[],
2525967294,pull_request,closed,,Add DT_INT8 to _VarHandlesOp,"Add DT_INT8 to _VarHandlesOp
",copybara-service[bot],2024-09-14 03:30:23+00:00,[],2024-09-18 19:45:54+00:00,2024-09-18 19:45:54+00:00,https://github.com/tensorflow/tensorflow/pull/75760,[],[],
2525964109,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-14 03:20:56+00:00,[],2024-09-14 03:20:56+00:00,,https://github.com/tensorflow/tensorflow/pull/75759,[],[],
2525902544,pull_request,open,,Fix the bug when using threshold as default cost for some ops,"Fix the bug when using threshold as default cost for some ops
",copybara-service[bot],2024-09-14 00:51:57+00:00,[],2024-09-14 00:51:57+00:00,,https://github.com/tensorflow/tensorflow/pull/75758,[],[],
2525901856,pull_request,closed,,PR #17182: Parametrize ConstantsFloatTest OneCellFloat,"PR #17182: Parametrize ConstantsFloatTest OneCellFloat

Imported from GitHub PR https://github.com/openxla/xla/pull/17182

Parametrize ConstantsFloatTest OneCellFloat


Copybara import of the project:

--
ad01a3eadc77385968af54f27685133caca5e8f9 by Alexander Pivovarov <pivovaa@amazon.com>:

Parametrize ConstantsFloatTest OneCellFloat

Merging this change closes #17182

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17182 from apivovarov:param_constants_test ad01a3eadc77385968af54f27685133caca5e8f9
",copybara-service[bot],2024-09-14 00:50:12+00:00,[],2024-09-16 11:14:11+00:00,2024-09-16 11:14:10+00:00,https://github.com/tensorflow/tensorflow/pull/75757,[],[],
2525901057,pull_request,closed,,"PR #17058: Replace ""Navi"" with corresponding public product names","PR #17058: Replace ""Navi"" with corresponding public product names

Imported from GitHub PR https://github.com/openxla/xla/pull/17058

The term 'Navi' is an internal product name used exclusively within AMD and should not appear in public projects. This PR replaces those 'Navi' names with the corresponding public product names.
Copybara import of the project:

--
47248a044861ffe4d2f129674bab11916626e5a3 by scxfjiang <sc.xfjiang@gmail.com>:

scrub navi

Merging this change closes #17058

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17058 from ROCm:ci_scrub_navi_symbol 47248a044861ffe4d2f129674bab11916626e5a3
",copybara-service[bot],2024-09-14 00:48:14+00:00,[],2024-09-16 11:40:45+00:00,2024-09-16 11:40:44+00:00,https://github.com/tensorflow/tensorflow/pull/75756,[],[],
2525895195,pull_request,closed,,PR #17177: Parametrize FloatNormalizationF8Test ResolveIfUnsupportedF8,"PR #17177: Parametrize FloatNormalizationF8Test ResolveIfUnsupportedF8

Imported from GitHub PR https://github.com/openxla/xla/pull/17177

Parametrize FloatNormalizationF8Test ResolveIfUnsupportedF8

other f8 types such as F8E4M3 will reuse this test
Copybara import of the project:

--
d4d6825352fcef93abd4bcb93582c54c75329d1a by Alexander Pivovarov <pivovaa@amazon.com>:

Parametrize FloatNormalizationF8Test ResolveIfUnsupportedF8

Merging this change closes #17177

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17177 from apivovarov:param_float_normalization_test d4d6825352fcef93abd4bcb93582c54c75329d1a
",copybara-service[bot],2024-09-14 00:34:00+00:00,[],2024-09-16 12:05:58+00:00,2024-09-16 12:05:57+00:00,https://github.com/tensorflow/tensorflow/pull/75755,[],[],
2525889195,pull_request,closed,,Add a pass to remove redundant check (select) feeding into a collective permute with cycle.,"Add a pass to remove redundant check (select) feeding into a collective permute with cycle.
When collective-permute operates on a comparison to a device id and the senders match the condition's branch we can link collective-permute to the original data skipping the comparison.
For example
   condition = broadcast(compare(replica_id, X), direction=EQ
   data_snd = select(condition, compare_true_data, compare_false_data)
   rcv = collective-permute(data_snd compare_true_data), pairs={{X,0}}
 can be transformed to
   rcv = collective-permute(compare_true_data), pairs={{X,0}}

 The pass is *only* handling compare direction={EQ,NE}.
 The pass handles Compare with and without preceding Broadcast.
",copybara-service[bot],2024-09-14 00:21:05+00:00,[],2024-09-18 23:56:25+00:00,2024-09-18 23:56:24+00:00,https://github.com/tensorflow/tensorflow/pull/75754,[],[],
2525886205,pull_request,open,,Integrate LLVM at llvm/llvm-project@fbf0a8015389,"Integrate LLVM at llvm/llvm-project@fbf0a8015389

Updates LLVM usage to match
[fbf0a8015389](https://github.com/llvm/llvm-project/commit/fbf0a8015389)
",copybara-service[bot],2024-09-14 00:15:01+00:00,[],2024-09-14 00:15:01+00:00,,https://github.com/tensorflow/tensorflow/pull/75753,[],[],
2525883996,pull_request,closed,,Update `cuda_clang_official` config with CUDA 12.5.1.,"Update `cuda_clang_official` config with CUDA 12.5.1.
",copybara-service[bot],2024-09-14 00:10:46+00:00,[],2024-09-16 15:06:48+00:00,2024-09-16 15:06:47+00:00,https://github.com/tensorflow/tensorflow/pull/75752,[],[],
2525866727,pull_request,closed,,PR #17135: Add TypeParam to FP8E4M3DistanceTest,"PR #17135: Add TypeParam to FP8E4M3DistanceTest

Imported from GitHub PR https://github.com/openxla/xla/pull/17135

Add TypeParam to FP8E4M3DistanceTest

This test suite will be extended with tsl::float8_e4m3 type in future
Copybara import of the project:

--
8afe5a96ad239f9bd546b0575d9c85a60ad8af80 by Alexander Pivovarov <pivovaa@amazon.com>:

Add TypeParam to FP8E4M3DistanceTest

Merging this change closes #17135

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17135 from apivovarov:dedup_fp_util_test 8afe5a96ad239f9bd546b0575d9c85a60ad8af80
",copybara-service[bot],2024-09-13 23:49:38+00:00,[],2024-09-16 11:57:11+00:00,2024-09-16 11:57:10+00:00,https://github.com/tensorflow/tensorflow/pull/75751,[],[],
2525834776,pull_request,closed,,HostOffloading: Properly handle XLA tuple shapes in host compute output copy removal optimization.,"HostOffloading: Properly handle XLA tuple shapes in host compute output copy removal optimization.

- Before, we assumed that the output was a tuple with single level, this missed the case when the output is a single array.
- Use ShapeTree and ForEachLeafSubshape to account for any tuple output shape.
",copybara-service[bot],2024-09-13 23:04:45+00:00,[],2024-09-14 02:52:03+00:00,2024-09-14 02:52:02+00:00,https://github.com/tensorflow/tensorflow/pull/75750,[],[],
2525794082,pull_request,closed,,Add cuda c++ for command buffer kernels,"Add cuda c++ for command buffer kernels

Reverts ea3e1b98aa89f9b10902c76b05ee07ea0e4c7d42
",copybara-service[bot],2024-09-13 22:30:53+00:00,[],2024-09-16 17:51:34+00:00,2024-09-16 17:51:32+00:00,https://github.com/tensorflow/tensorflow/pull/75749,[],[],
2525786523,pull_request,closed,,[TOCO Removal] Remove enable_mlir_converter from Python API.,"[TOCO Removal] Remove enable_mlir_converter from Python API.
",copybara-service[bot],2024-09-13 22:23:37+00:00,['arfaian'],2024-09-16 03:40:28+00:00,2024-09-16 03:40:27+00:00,https://github.com/tensorflow/tensorflow/pull/75748,[],[],
2525767686,pull_request,closed,,[HLO Componentization] Create pass sub-component,"[HLO Componentization] Create pass sub-component
",copybara-service[bot],2024-09-13 22:07:44+00:00,['sdasgup3'],2024-09-14 07:23:41+00:00,2024-09-14 07:23:40+00:00,https://github.com/tensorflow/tensorflow/pull/75747,[],[],
2525762127,pull_request,closed,,PR #15403: Handle multiple users in all-gather dynamic-slice simplification. Add AllGatherDynamicSliceSimplifier pass,"PR #15403: Handle multiple users in all-gather dynamic-slice simplification. Add AllGatherDynamicSliceSimplifier pass

Imported from GitHub PR https://github.com/openxla/xla/pull/15403

I have found in some models that have poor SPMD partitioning the below pattern. 

```
all-gather.1 = all-gather(x)
dot.1 = dot(all-gather.1, y) 
dynamic-slice.1 = dynamic-slice(all-gather.1) // can be cancelled
```

In this case, the all-gather has multiple users but the dynamic-slice can be cancelled. This is applicable to all-reduce and reduce-scatter also. My changes now support multiple users, but it also depends how this utility is used by internal TPU compiler and the GPU ReduceScatterCreator pass. My changes assume the cancellation is run like this --

1. Find a dynamic-slice
2. Check if dynamic-slice can be cancelled
3. Delete dynamic-slice but do not delete the collective
4. The collective is deleted by the DCE pass if it has no users

The above workflow then supports removing dynamic-slices even if the collective has multiple users. The above is what we are using in our internal Neuron workflow. 
Interested to hear thoughts on this. 
Copybara import of the project:

--
f518bd6e3164aa10b60b4689f2aa2ee8d8faa7ae by ptoulme-aws <ptoulme@amazon.com>:

Handle multiple users in all-gather dynamic-slice simplification. Add AllGatherDynamicSliceSimplifier pass

Merging this change closes #15403

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15403 from ptoulme-aws:multiple_user_collectives f518bd6e3164aa10b60b4689f2aa2ee8d8faa7ae
",copybara-service[bot],2024-09-13 22:02:11+00:00,[],2024-09-17 02:03:17+00:00,2024-09-17 02:03:16+00:00,https://github.com/tensorflow/tensorflow/pull/75746,[],[],
2525747861,pull_request,closed,,Add enable FC keep num dims pass,"Add enable FC keep num dims pass
",copybara-service[bot],2024-09-13 21:47:14+00:00,['chunnienc'],2024-09-17 06:26:24+00:00,2024-09-17 06:26:23+00:00,https://github.com/tensorflow/tensorflow/pull/75745,[],[],
2525710801,pull_request,closed,,Allow closures for ErrorSpecGen in exhasutive test,"Allow closures for ErrorSpecGen in exhasutive test

Swaps the fn ptr type for `std::function` that allows closures at the cost of a heap allocation. The better ergonomics are worth the slightly higher cost since this is just test infrastructure.
",copybara-service[bot],2024-09-13 21:13:34+00:00,[],2024-09-16 21:00:04+00:00,2024-09-16 21:00:03+00:00,https://github.com/tensorflow/tensorflow/pull/75744,[],[],
2525708670,pull_request,closed,,Support multiple TF platforms at the same time,"Support multiple TF platforms at the same time
",copybara-service[bot],2024-09-13 21:11:55+00:00,[],2024-09-13 22:16:25+00:00,2024-09-13 22:16:24+00:00,https://github.com/tensorflow/tensorflow/pull/75743,[],[],
2525688022,pull_request,closed,,Add move reshape after fc pass,"Add move reshape after fc pass
",copybara-service[bot],2024-09-13 20:57:41+00:00,['chunnienc'],2024-09-17 05:24:25+00:00,2024-09-17 05:24:25+00:00,https://github.com/tensorflow/tensorflow/pull/75742,[],[],
2525685762,pull_request,closed,,Add Python 3.13.0rc2 support to rules_python in a form of a patch.,"Add Python 3.13.0rc2 support to rules_python in a form of a patch.
TODO: once rules_python has 3.13 out of the box, the patch can be safely removed.

rules_python does not depend on rc versions of Python (seems to be against its philosophy), while JAX wants to support it, that is why adding RC support in a form of a patch instead of adding it directly to rules_python.
",copybara-service[bot],2024-09-13 20:56:19+00:00,['vam-google'],2024-09-16 20:14:31+00:00,2024-09-16 20:14:30+00:00,https://github.com/tensorflow/tensorflow/pull/75741,[],[],
2525668561,pull_request,closed,,"Update to use a static global variable for the devices retrieved from the fallback state, so that kernel executions do not need to do the same device retrieval work repeatedly.","Update to use a static global variable for the devices retrieved from the fallback state, so that kernel executions do not need to do the same device retrieval work repeatedly.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/75822 from syzygial:fix_typo d17fe711aa6481234b70e6d66c4346332ac02d67
",copybara-service[bot],2024-09-13 20:44:57+00:00,['changhuilin'],2024-09-16 19:06:31+00:00,2024-09-16 19:06:30+00:00,https://github.com/tensorflow/tensorflow/pull/75740,[],[],
2525608682,pull_request,open,,[lrt-compiler-plugin] dummy example plugin impl,"[lrt-compiler-plugin] dummy example plugin impl
",copybara-service[bot],2024-09-13 20:10:14+00:00,['LukeBoyer'],2024-09-13 20:10:15+00:00,,https://github.com/tensorflow/tensorflow/pull/75739,[],[],
2525602067,pull_request,closed,,PR #17130: Add default upcasting behavior to DoWithUpcastToF32,"PR #17130: Add default upcasting behavior to DoWithUpcastToF32

Imported from GitHub PR https://github.com/openxla/xla/pull/17130


### Issue Description:
@[reedwm](https://github.com/reedwm) reedwm
Since the type list here is growing, I would avoid hardcoding the list of FP8 types. One way to avoid this is to have DoWithUpcastToF32 take a should_upcast bool instead of the existing upcast_types list. Then you can pass something like should_upcast = BitWidth(b.GetShape(x).element_type) <= 16.

There are a lot of places where we list out all FP8 types, but every place we can remove listing these out will help when more types are added :)

### Solution Description:
Returns operation(operand), except if `operand` is one of the types in `upcast_types`. In such cases, it is first converted to F32, then the result is converted back to the original type.
If `upcast_types` is empty, the default upcasting behavior is applied: upcast if BitWidth <= 16.

DoWithUpcastToF32 usage stats:
- with default upcasting - 5 times
- with `{BF16, F16}` - 5 times
- with `{BF16}` - 1 time
Copybara import of the project:

--
1cec176d7ec13881095e755faf48ccea6d053f74 by Alexander Pivovarov <pivovaa@amazon.com>:

Add default upcasting behavior to DoWithUpcastToF32

Returns operation(operand), except if `operand` is one of the types in
`upcast_types`. In such cases, it is first converted to F32, then the result
is converted back to the original type.
If `upcast_types` is empty, the default upcasting behavior is applied:
upcast if BitWidth <= 16.

Merging this change closes #17130

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17130 from apivovarov:DoWithUpcastToF32 1cec176d7ec13881095e755faf48ccea6d053f74
",copybara-service[bot],2024-09-13 20:06:08+00:00,[],2024-09-16 11:28:22+00:00,2024-09-16 11:28:21+00:00,https://github.com/tensorflow/tensorflow/pull/75738,[],[],
2525588029,pull_request,closed,,Rollback of change breaking downstream tests.,"Rollback of change breaking downstream tests.

Reverts b59f12404934d878e0742478c8df37617b22b976
",copybara-service[bot],2024-09-13 19:56:46+00:00,[],2024-09-13 20:43:09+00:00,2024-09-13 20:43:08+00:00,https://github.com/tensorflow/tensorflow/pull/75737,[],[],
2525584720,pull_request,closed,,Add support for keep dims in current sum folding,"Add support for keep dims in current sum folding
",copybara-service[bot],2024-09-13 19:54:47+00:00,['LukeBoyer'],2024-09-13 21:17:09+00:00,2024-09-13 21:17:09+00:00,https://github.com/tensorflow/tensorflow/pull/75736,[],[],
2525579286,pull_request,closed,,"[TSL] Bump ml_dtypes. Add float8_e4m3, float8_e3m4","[TSL] Bump ml_dtypes. Add float8_e4m3, float8_e3m4

ml_dtypes Updates:
Add float8_e4m3 and float8_e3m4 types support
Fix float divmod with zero denominator
Add int2 and uint2 types
ml_dtypes/commits

Related PRs
ml_dtypes PR Add float8_e4m3 jax-ml/ml_dtypes#161 Add float8_e4m3 (Merged)
XLA PR Add support for float8_e4m3 #16585 (In Review)

This closes https://github.com/openxla/xla/pull/17075
",copybara-service[bot],2024-09-13 19:52:32+00:00,[],2024-09-17 21:12:22+00:00,2024-09-17 21:12:21+00:00,https://github.com/tensorflow/tensorflow/pull/75735,[],[],
2525534445,pull_request,closed,,Reverts f975479fb985a23b3cf1d1289dee8e09931d5d57,"Reverts f975479fb985a23b3cf1d1289dee8e09931d5d57
",copybara-service[bot],2024-09-13 19:30:13+00:00,['LukeBoyer'],2024-09-17 21:56:20+00:00,2024-09-17 21:56:19+00:00,https://github.com/tensorflow/tensorflow/pull/75734,[],[],
2525534329,pull_request,closed,,Cleanup in BFloat16Propagation.,"Cleanup in BFloat16Propagation.
",copybara-service[bot],2024-09-13 19:30:08+00:00,['SandSnip3r'],2024-09-16 22:37:58+00:00,2024-09-16 22:37:57+00:00,https://github.com/tensorflow/tensorflow/pull/75733,[],[],
2525469089,pull_request,closed,,Add manual tag to lrt_model_test,"Add manual tag to lrt_model_test
",copybara-service[bot],2024-09-13 18:56:08+00:00,['LukeBoyer'],2024-09-13 19:23:00+00:00,2024-09-13 19:22:58+00:00,https://github.com/tensorflow/tensorflow/pull/75732,[],[],
2525459444,pull_request,closed,,PR #17112: Clean up for legacy comment and misleading error message.,"PR #17112: Clean up for legacy comment and misleading error message.

Imported from GitHub PR https://github.com/openxla/xla/pull/17112

Clean up for legacy comment since triton fp8 gemm is in place. Also clean up misleading error message in cublasLT runtime.
Copybara import of the project:

--
ef22252dade7ccca270c5680cfa01d17e1bb850b by Elfie Guo <elfieg@nvidia.com>:

Clean up for legacy comment and weird error message.

Merging this change closes #17112

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17112 from elfiegg:clean ef22252dade7ccca270c5680cfa01d17e1bb850b
",copybara-service[bot],2024-09-13 18:50:42+00:00,[],2024-09-16 11:34:01+00:00,2024-09-16 11:34:00+00:00,https://github.com/tensorflow/tensorflow/pull/75731,[],[],
2525457191,pull_request,closed,,[Take 2] Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"[Take 2] Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
After:

jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit

Reverts e8c735905747d5173ad51ece93bfc957afdcd5f7
",copybara-service[bot],2024-09-13 18:49:43+00:00,['pschuh'],2024-09-17 23:29:26+00:00,2024-09-17 23:29:25+00:00,https://github.com/tensorflow/tensorflow/pull/75730,[],[],
2525456445,pull_request,closed,,[xla:ffi] Optimize returning tsl::AsyncValueRef from FFI handler,"[xla:ffi] Optimize returning tsl::AsyncValueRef from FFI handler

BEFORE: BM_AsyncAnyBufferArgX1       55.2 ns         55.2 ns     12514333
AFTER:  BM_AsyncAnyBufferArgX1       31.6 ns         31.6 ns     21924178
",copybara-service[bot],2024-09-13 18:49:11+00:00,['ezhulenev'],2024-09-14 03:08:08+00:00,2024-09-14 03:08:07+00:00,https://github.com/tensorflow/tensorflow/pull/75729,[],[],
2525454721,pull_request,closed,,Add a command line flag to disable XLA GPU passes based on binary libraries.,"Add a command line flag to disable XLA GPU passes based on binary libraries.

Start using the new pass to optionally disable many cuDNN-specific passes.
",copybara-service[bot],2024-09-13 18:47:57+00:00,[],2024-09-17 19:40:25+00:00,2024-09-17 19:40:24+00:00,https://github.com/tensorflow/tensorflow/pull/75728,[],[],
2525443719,pull_request,open,,Integrate LLVM at llvm/llvm-project@b9d85b1263ef,"Integrate LLVM at llvm/llvm-project@b9d85b1263ef

Updates LLVM usage to match
[b9d85b1263ef](https://github.com/llvm/llvm-project/commit/b9d85b1263ef)
",copybara-service[bot],2024-09-13 18:40:08+00:00,[],2024-09-13 18:40:08+00:00,,https://github.com/tensorflow/tensorflow/pull/75727,[],[],
2525438395,pull_request,closed,,Add coming changes related to LiteRT repo to release.md,"Add coming changes related to LiteRT repo to release.md
",copybara-service[bot],2024-09-13 18:36:31+00:00,[],2024-09-13 18:52:26+00:00,2024-09-13 18:52:25+00:00,https://github.com/tensorflow/tensorflow/pull/75726,[],[],
2525427654,pull_request,closed,,#tf-data Allows `tf.data.experiemental.get_model_proto` to accept `NumpyIterator` as input,"#tf-data Allows `tf.data.experiemental.get_model_proto` to accept `NumpyIterator` as input
",copybara-service[bot],2024-09-13 18:29:22+00:00,['jimlinntu'],2024-09-17 20:33:33+00:00,2024-09-17 20:33:32+00:00,https://github.com/tensorflow/tensorflow/pull/75725,[],[],
2525399709,pull_request,closed,,We should skip async-start for a call to host computation instead of returning an,"We should skip async-start for a call to host computation instead of returning an
error in host_offload_legalize, during the copy collection phase.
",copybara-service[bot],2024-09-13 18:12:09+00:00,[],2024-09-26 18:07:34+00:00,2024-09-26 18:07:32+00:00,https://github.com/tensorflow/tensorflow/pull/75724,[],[],
2525397736,pull_request,closed,,Add support for kConvolution to FusionWrapper.,"Add support for kConvolution to FusionWrapper.
",copybara-service[bot],2024-09-13 18:11:26+00:00,[],2024-09-13 20:54:40+00:00,2024-09-13 20:54:39+00:00,https://github.com/tensorflow/tensorflow/pull/75723,[],[],
2525380609,pull_request,closed,,"Bump 4w compat to v1.5.0 release Aug 1, 2024","Bump 4w compat to v1.5.0 release Aug 1, 2024
",copybara-service[bot],2024-09-13 18:03:02+00:00,['GleasonK'],2024-09-14 05:18:40+00:00,2024-09-14 05:18:39+00:00,https://github.com/tensorflow/tensorflow/pull/75722,[],[],
2525333114,pull_request,closed,,Fix a typo in the code for enumerating sharding strategies for kScatter ops. Also remove a debug print which inadvertently got in as part of the same CL.,"Fix a typo in the code for enumerating sharding strategies for kScatter ops. Also remove a debug print which inadvertently got in as part of the same CL.
",copybara-service[bot],2024-09-13 17:30:03+00:00,[],2024-09-14 03:35:39+00:00,2024-09-14 03:35:38+00:00,https://github.com/tensorflow/tensorflow/pull/75721,[],[],
2525330525,pull_request,closed,,Update np_test.py cuda config to be compatible with numpy 2.0.0.,"Update np_test.py cuda config to be compatible with numpy 2.0.0.
",copybara-service[bot],2024-09-13 17:28:13+00:00,[],2024-09-16 17:38:00+00:00,2024-09-16 17:37:58+00:00,https://github.com/tensorflow/tensorflow/pull/75720,[],[],
2525320678,pull_request,closed,,Remove unnecessary dependency from se_gpu_pjrt_compiler.,"Remove unnecessary dependency from se_gpu_pjrt_compiler.
",copybara-service[bot],2024-09-13 17:20:58+00:00,[],2024-09-17 20:50:05+00:00,2024-09-17 20:50:04+00:00,https://github.com/tensorflow/tensorflow/pull/75719,[],[],
2525315488,pull_request,closed,,Remove unused cuda_activation target.,"Remove unused cuda_activation target.
",copybara-service[bot],2024-09-13 17:17:23+00:00,[],2024-09-17 15:55:55+00:00,2024-09-17 15:55:54+00:00,https://github.com/tensorflow/tensorflow/pull/75718,[],[],
2525302759,pull_request,closed,,#tf-data Fixes `data_service_ops.py` broken doc test in NumPy 2.0 upgrade by adding `np_array.item()`,"#tf-data Fixes `data_service_ops.py` broken doc test in NumPy 2.0 upgrade by adding `np_array.item()`
",copybara-service[bot],2024-09-13 17:11:59+00:00,['jimlinntu'],2024-09-13 18:07:33+00:00,2024-09-13 18:07:32+00:00,https://github.com/tensorflow/tensorflow/pull/75717,[],[],
2525297983,pull_request,closed,,Remove the use of gpu_activation.h in rocm code.,"Remove the use of gpu_activation.h in rocm code.
",copybara-service[bot],2024-09-13 17:10:03+00:00,[],2024-09-13 17:43:57+00:00,2024-09-13 17:43:56+00:00,https://github.com/tensorflow/tensorflow/pull/75716,[],[],
2525211473,pull_request,closed,,Delete unused rocm_activation.h.,"Delete unused rocm_activation.h.
",copybara-service[bot],2024-09-13 16:27:59+00:00,[],2024-09-13 16:53:42+00:00,2024-09-13 16:53:41+00:00,https://github.com/tensorflow/tensorflow/pull/75715,[],[],
2525185439,pull_request,closed,,[xla:ffi] Add CallAsync for invoking potentially asyncrhronous FFI handlers,"[xla:ffi] Add CallAsync for invoking potentially asyncrhronous FFI handlers
",copybara-service[bot],2024-09-13 16:12:36+00:00,['ezhulenev'],2024-09-13 18:23:47+00:00,2024-09-13 18:23:47+00:00,https://github.com/tensorflow/tensorflow/pull/75714,[],[],
2525108545,pull_request,closed,,[xla:cpu] Temporarily disable fast F64 Tanh due to incorrect results.,"[xla:cpu] Temporarily disable fast F64 Tanh due to incorrect results.
https://github.com/google/jax/issues/23590
",copybara-service[bot],2024-09-13 15:28:19+00:00,['penpornk'],2024-09-13 17:06:35+00:00,2024-09-13 17:06:34+00:00,https://github.com/tensorflow/tensorflow/pull/75713,[],[],
2525062688,pull_request,closed,,Add support for Tegra chips in hermetic CUDA rules.,"Add support for Tegra chips in hermetic CUDA rules.

See [Github issue](https://github.com/tensorflow/tensorflow/issues/75353).
",copybara-service[bot],2024-09-13 15:04:51+00:00,[],2024-09-13 17:53:59+00:00,2024-09-13 17:53:59+00:00,https://github.com/tensorflow/tensorflow/pull/75712,[],[],
2525034871,pull_request,closed,,[PJRT] Use `typedef struct X {} X;` idiom for PJRT_Api.,"[PJRT] Use `typedef struct X {} X;` idiom for PJRT_Api.

This allows forward declaration (`struct PJRT_Api;`) in C++, i.e. if you want to carry `PJRT_Api` as an opaque pointer to pass further without need to depend on the actual header.

In C, `struct X` and (typedef) `X` are in different namespaces so they can coexist.

In C++ this idiom is allowed by § 9.2.3.3 of the standard for compatibility with C.
",copybara-service[bot],2024-09-13 14:51:59+00:00,[],2024-09-13 16:31:09+00:00,2024-09-13 16:31:08+00:00,https://github.com/tensorflow/tensorflow/pull/75711,[],[],
2524974469,pull_request,closed,,#sdy Add sdy round trip shard map import.,"#sdy Add sdy round trip shard map import.
",copybara-service[bot],2024-09-13 14:23:38+00:00,[],2024-09-16 18:00:26+00:00,2024-09-16 18:00:24+00:00,https://github.com/tensorflow/tensorflow/pull/75710,[],[],
2524949613,pull_request,closed,,Lower transpose to materialize & insert op,"Lower transpose to materialize & insert op
",copybara-service[bot],2024-09-13 14:11:35+00:00,[],2024-09-25 08:53:25+00:00,2024-09-25 08:53:23+00:00,https://github.com/tensorflow/tensorflow/pull/75709,[],[],
2524935370,pull_request,closed,,[XLA:GPU] Support complex numbers in materialize & insert op lowering,"[XLA:GPU] Support complex numbers in materialize & insert op lowering
",copybara-service[bot],2024-09-13 14:04:49+00:00,[],2024-09-16 13:40:34+00:00,2024-09-16 13:40:33+00:00,https://github.com/tensorflow/tensorflow/pull/75708,[],[],
2524918226,pull_request,closed,,Integrate LLVM at llvm/llvm-project@f0b3287297ae,"Integrate LLVM at llvm/llvm-project@f0b3287297ae

Updates LLVM usage to match
[f0b3287297ae](https://github.com/llvm/llvm-project/commit/f0b3287297ae)
",copybara-service[bot],2024-09-13 13:57:01+00:00,[],2024-09-13 17:31:47+00:00,2024-09-13 17:31:47+00:00,https://github.com/tensorflow/tensorflow/pull/75707,[],[],
2524844564,pull_request,closed,,[XLA:GPU] Fix InsertOp's lowering when applying indices,"[XLA:GPU] Fix InsertOp's lowering when applying indices

InsertOp is supposed to take the map results of the vector as inputs into its map and have no symbols as it doesn't create a loop for this.  Add a verifier to check that InsertOp is lowerable given these constraints.
",copybara-service[bot],2024-09-13 13:22:09+00:00,[],2024-09-16 13:09:29+00:00,2024-09-16 13:09:28+00:00,https://github.com/tensorflow/tensorflow/pull/75706,[],[],
2524747026,pull_request,closed,,[XLA:GPU] Use padded tile size to estimate FLOPs and choose the number of warps.,"[XLA:GPU] Use padded tile size to estimate FLOPs and choose the number of warps.

Triton has a requirement that all tiles should be padded to the next power of 2 for each dimensions. The emitted kernel will perform computation on the padded value, so it affect the amount of computation and the number of threads in a block. However, padded values are set directly in registed, so it doesn't affect memory access time.
",copybara-service[bot],2024-09-13 12:39:09+00:00,[],2024-09-13 13:35:39+00:00,2024-09-13 13:35:39+00:00,https://github.com/tensorflow/tensorflow/pull/75705,[],[],
2524563346,pull_request,closed,,[XLA:GPU] Make sure that xla_gpu_enable_pgle_accuracy_checker is false explicitly.,"[XLA:GPU] Make sure that xla_gpu_enable_pgle_accuracy_checker is false explicitly.

It got lost when https://github.com/openxla/xla/pull/15317 was merged internally.
",copybara-service[bot],2024-09-13 11:07:16+00:00,[],2024-09-13 12:04:57+00:00,2024-09-13 12:04:56+00:00,https://github.com/tensorflow/tensorflow/pull/75704,[],[],
2524530165,pull_request,closed,,[XLA:GPU] Run post-scheduling verification under the `xla_gpu_enable_pgle_accuracy_checker` flag.,"[XLA:GPU] Run post-scheduling verification under the `xla_gpu_enable_pgle_accuracy_checker` flag.
",copybara-service[bot],2024-09-13 10:48:45+00:00,[],2024-09-16 10:20:22+00:00,2024-09-16 10:20:21+00:00,https://github.com/tensorflow/tensorflow/pull/75703,[],[],
2524386530,pull_request,open,,Validate requantization scale in Conv2D/FullyConnected operator,Avoid errors later-on when delegate attempts to create XNNPACK operators with unsupported requantization scales.,soniravi31,2024-09-13 09:41:34+00:00,['gbaned'],2025-01-16 06:18:34+00:00,,https://github.com/tensorflow/tensorflow/pull/75702,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:S', 'CL Change Size: Small')]","[{'comment_id': 2367632664, 'issue_id': 2524386530, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau ,Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 23, 9, 9, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384863636, 'issue_id': 2524386530, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau ,Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 1, 5, 55, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399563082, 'issue_id': 2524386530, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau ,Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 8, 11, 16, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434263838, 'issue_id': 2524386530, 'author': 'soniravi31', 'body': 'Hi @talumbau , Please help to take a look on this.\r\nThanks.', 'created_at': datetime.datetime(2024, 10, 24, 4, 31, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469825653, 'issue_id': 2524386530, 'author': 'keerthanakadiri', 'body': 'Hi @majiddadashi , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 12, 7, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530552517, 'issue_id': 2524386530, 'author': 'keerthanakadiri', 'body': 'Hi @junjiang-lab, Can you please take a look at this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 10, 6, 24, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573069248, 'issue_id': 2524386530, 'author': 'soniravi31', 'body': 'Hi @mattsoulanille and @junjiang-lab, \r\ncould you please review this PR.\r\nThanks.', 'created_at': datetime.datetime(2025, 1, 6, 13, 3, 19, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-09-23 09:09:17 UTC): Hi @talumbau ,Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 05:55:41 UTC): Hi @talumbau ,Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-08 11:16:42 UTC): Hi @talumbau ,Can you please review this PR? Thank you !

soniravi31 (Issue Creator) on (2024-10-24 04:31:05 UTC): Hi @talumbau , Please help to take a look on this.
Thanks.

keerthanakadiri on (2024-11-12 07:55:00 UTC): Hi @majiddadashi , Can you please review this PR? Thank you !

keerthanakadiri on (2024-12-10 06:24:15 UTC): Hi @junjiang-lab, Can you please take a look at this PR? Thank you !

soniravi31 (Issue Creator) on (2025-01-06 13:03:19 UTC): Hi @mattsoulanille and @junjiang-lab, 
could you please review this PR.
Thanks.

"
2524358265,pull_request,closed,,"[XLA:GPU][NFC] Move ""tile size fits in registers"" check to a separate function.","[XLA:GPU][NFC] Move ""tile size fits in registers"" check to a separate function.

A huge comment in the middle of the loop doesn't improve readability.
",copybara-service[bot],2024-09-13 09:30:57+00:00,[],2024-09-13 10:20:42+00:00,2024-09-13 10:20:41+00:00,https://github.com/tensorflow/tensorflow/pull/75701,[],[],
2524285522,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 09:00:28+00:00,[],2024-09-13 09:00:28+00:00,,https://github.com/tensorflow/tensorflow/pull/75700,[],[],
2524154636,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 08:01:58+00:00,[],2024-09-13 08:01:58+00:00,,https://github.com/tensorflow/tensorflow/pull/75699,[],[],
2524108155,pull_request,closed,,Fix typos in documentation strings,"Hi, Team
I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",Venkat6871,2024-09-13 07:40:33+00:00,['gbaned'],2024-09-20 10:18:54+00:00,2024-09-20 10:18:54+00:00,https://github.com/tensorflow/tensorflow/pull/75698,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:S', 'CL Change Size: Small')]","[{'comment_id': 2354437889, 'issue_id': 2524108155, 'author': 'keerthanakadiri', 'body': 'Hi @Venkat6871 , Can you please resolve the conflicts? Thank you!', 'created_at': datetime.datetime(2024, 9, 17, 3, 33, 59, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-09-17 03:33:59 UTC): Hi @Venkat6871 , Can you please resolve the conflicts? Thank you!

"
2524011502,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 06:50:31+00:00,[],2024-09-13 06:50:31+00:00,,https://github.com/tensorflow/tensorflow/pull/75697,[],[],
2523980766,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 06:29:29+00:00,[],2024-09-13 06:29:29+00:00,,https://github.com/tensorflow/tensorflow/pull/75696,[],[],
2523948413,pull_request,closed,,Allow propagations on reduce to occur,"Allow propagations on reduce to occur
",copybara-service[bot],2024-09-13 06:05:16+00:00,[],2024-09-14 14:09:04+00:00,2024-09-14 14:09:04+00:00,https://github.com/tensorflow/tensorflow/pull/75695,[],[],
2523937170,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 05:56:23+00:00,[],2024-09-13 05:56:23+00:00,,https://github.com/tensorflow/tensorflow/pull/75694,[],[],
2523922280,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 05:43:23+00:00,[],2024-09-13 05:43:23+00:00,,https://github.com/tensorflow/tensorflow/pull/75693,[],[],
2523921727,pull_request,closed,,[lrt-compiler-plugin] dummy example plugin impl,"[lrt-compiler-plugin] dummy example plugin impl
",copybara-service[bot],2024-09-13 05:43:03+00:00,['LukeBoyer'],2024-09-13 20:17:10+00:00,2024-09-13 20:17:10+00:00,https://github.com/tensorflow/tensorflow/pull/75692,[],[],
2523917254,pull_request,closed,,[lrt-compiler-plugin] plugin header,"[lrt-compiler-plugin] plugin header
",copybara-service[bot],2024-09-13 05:38:55+00:00,['LukeBoyer'],2024-09-13 06:34:51+00:00,2024-09-13 06:34:50+00:00,https://github.com/tensorflow/tensorflow/pull/75691,[],[],
2523916353,pull_request,closed,,[lite-rt-compiler-plugin] Basic algos needed to plugin application.,"[lite-rt-compiler-plugin] Basic algos needed to plugin application.
",copybara-service[bot],2024-09-13 05:38:09+00:00,['LukeBoyer'],2024-09-13 20:26:55+00:00,2024-09-13 20:26:54+00:00,https://github.com/tensorflow/tensorflow/pull/75690,[],[],
2523865770,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:46:13+00:00,[],2024-09-17 06:35:46+00:00,2024-09-17 06:35:45+00:00,https://github.com/tensorflow/tensorflow/pull/75689,[],[],
2523864132,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:44:20+00:00,[],2024-09-13 04:44:20+00:00,,https://github.com/tensorflow/tensorflow/pull/75688,[],[],
2523862181,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:42:20+00:00,[],2024-09-13 04:42:20+00:00,,https://github.com/tensorflow/tensorflow/pull/75687,[],[],
2523855910,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:35:08+00:00,[],2024-09-13 05:36:00+00:00,2024-09-13 05:35:59+00:00,https://github.com/tensorflow/tensorflow/pull/75686,[],[],
2523855582,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:34:48+00:00,[],2024-09-13 04:34:48+00:00,,https://github.com/tensorflow/tensorflow/pull/75685,[],[],
2523853686,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:32:40+00:00,[],2024-09-13 04:32:40+00:00,,https://github.com/tensorflow/tensorflow/pull/75684,[],[],
2523852777,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:31:40+00:00,[],2024-09-14 07:37:16+00:00,,https://github.com/tensorflow/tensorflow/pull/75683,[],[],
2523849106,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 04:27:50+00:00,[],2024-09-13 04:27:50+00:00,,https://github.com/tensorflow/tensorflow/pull/75682,[],[],
2523824573,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 03:59:25+00:00,[],2024-09-14 09:37:58+00:00,2024-09-14 09:37:57+00:00,https://github.com/tensorflow/tensorflow/pull/75681,[],[],
2523821626,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-13 03:55:34+00:00,[],2024-09-13 03:55:34+00:00,,https://github.com/tensorflow/tensorflow/pull/75680,[],[],
2523813348,pull_request,open,,fix d2h issue,"fix d2h issue
",copybara-service[bot],2024-09-13 03:44:56+00:00,['changhuilin'],2024-09-13 17:35:31+00:00,,https://github.com/tensorflow/tensorflow/pull/75679,[],[],
2523770842,pull_request,open,,[Take 2] Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.,"[Take 2] Generalize global jit cpp cache keys so we can add more keys than the current donate_argnums.

This allows us to get more cache hits globally. For example:

Before:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache miss
```

After:

```
jax.jit(f, out_shardings=s)(arr)
jax.jit(f, out_shardings=s)(arr)  # cpp cache hit
```

Also, we can remove the hack (which I didn't like) in multihost_utils.py.

Reverts e8c735905747d5173ad51ece93bfc957afdcd5f7
",copybara-service[bot],2024-09-13 03:00:12+00:00,['yashk2810'],2024-09-13 03:16:25+00:00,,https://github.com/tensorflow/tensorflow/pull/75678,[],[],
2523669551,pull_request,open,,Reverts 5adafde02706dfe24e3a56744d0baecfdc3df90b,"Reverts 5adafde02706dfe24e3a56744d0baecfdc3df90b
",copybara-service[bot],2024-09-13 01:23:40+00:00,[],2024-09-13 01:23:40+00:00,,https://github.com/tensorflow/tensorflow/pull/75677,[],[],
2523623899,pull_request,open,,Internal breakage.,"Internal breakage.

Reverts 5adafde02706dfe24e3a56744d0baecfdc3df90b

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16921 from zhenying-liu:nccl-buffer-output b5e43d6455adc49f5ac99a9a9e95cf495eb46170
",copybara-service[bot],2024-09-13 00:53:56+00:00,[],2024-09-13 00:53:56+00:00,,https://github.com/tensorflow/tensorflow/pull/75676,[],[],
2523551160,pull_request,open,,Repurpose `HloHostCopyRewriter` for any late-compilation cleanup related to host offloading; renamed to `HostOffloadCleanup`. Use `HostOffloadCleanup` to move any `Convert`s that happen during host memory offload to once the tensor is on the device.,"Repurpose `HloHostCopyRewriter` for any late-compilation cleanup related to host offloading; renamed to `HostOffloadCleanup`. Use `HostOffloadCleanup` to move any `Convert`s that happen during host memory offload to once the tensor is on the device.

Also change float support to prevent host offloading copies from also being used to change `PrimitiveType`.

This all solves a bug where originally float propagation tried to reuse a host offloading copy to convert from f32 to bf16. Once float support was changed to disallow this, float normalization inserted a convert before the host offloading copy. However, if this is a host-to-device copy, the convert would be happening on the host. `HostOffloadCleanup` moves this convert.

Reverts 6b5ab9f54bfa159977b35ed347066bc53b3ca15d
",copybara-service[bot],2024-09-13 00:05:13+00:00,['SandSnip3r'],2024-09-16 19:20:27+00:00,,https://github.com/tensorflow/tensorflow/pull/75675,[],[],
2523547981,pull_request,closed,,Internal only change.,"Internal only change.

Reverts e4a35b40a8d3251fa871c1a3400b8596c2ee3b86
",copybara-service[bot],2024-09-13 00:01:23+00:00,['yangustc07'],2024-09-13 00:24:47+00:00,2024-09-13 00:24:46+00:00,https://github.com/tensorflow/tensorflow/pull/75674,[],[],
2523543691,pull_request,closed,,Pull out broadcasts from splat const after phase 1.,"Pull out broadcasts from splat const after phase 1.
",copybara-service[bot],2024-09-12 23:55:54+00:00,['LukeBoyer'],2024-09-15 16:58:07+00:00,2024-09-15 16:58:05+00:00,https://github.com/tensorflow/tensorflow/pull/75673,[],[],
2523536109,pull_request,open,,Add support for downloading clang from llvm-project repo.,"Add support for downloading clang from llvm-project repo.
",copybara-service[bot],2024-09-12 23:45:21+00:00,[],2024-09-13 18:19:58+00:00,,https://github.com/tensorflow/tensorflow/pull/75672,[],[],
2523531800,pull_request,closed,,Fix some layout test failures on gpu backend,"Fix some layout test failures on gpu backend
",copybara-service[bot],2024-09-12 23:39:27+00:00,['kanglant'],2024-09-13 17:17:59+00:00,2024-09-13 17:17:58+00:00,https://github.com/tensorflow/tensorflow/pull/75671,[],[],
2523516117,pull_request,open,,N/A (needed to silence presubmit errors),"N/A (needed to silence presubmit errors)
",copybara-service[bot],2024-09-12 23:18:55+00:00,[],2024-09-12 23:18:55+00:00,,https://github.com/tensorflow/tensorflow/pull/75670,[],[],
2523509885,pull_request,closed,,Add transmission/delay budgets to XProf MXLA trace/graph viewer.,"Add transmission/delay budgets to XProf MXLA trace/graph viewer.
",copybara-service[bot],2024-09-12 23:11:13+00:00,[],2024-09-17 00:01:14+00:00,2024-09-17 00:01:14+00:00,https://github.com/tensorflow/tensorflow/pull/75669,[],[],
2523497364,pull_request,closed,,[lrt-model] model implementation,"[lrt-model] model implementation
",copybara-service[bot],2024-09-12 22:57:36+00:00,['LukeBoyer'],2024-09-13 06:12:30+00:00,2024-09-13 06:12:29+00:00,https://github.com/tensorflow/tensorflow/pull/75668,[],[],
2523494803,pull_request,closed,,[lrt-model] Basic model re-serialization.,"[lrt-model] Basic model re-serialization.

* Cleanup load model.
* error message param for some macros.
",copybara-service[bot],2024-09-12 22:54:40+00:00,['LukeBoyer'],2024-09-13 20:35:15+00:00,2024-09-13 20:35:15+00:00,https://github.com/tensorflow/tensorflow/pull/75667,[],[],
2523475959,pull_request,closed,,Add missing field when initialize the CuptiTracerEvent.,"Add missing field when initialize the CuptiTracerEvent.
Add test to prevent future changes break this.
",copybara-service[bot],2024-09-12 22:36:22+00:00,[],2024-09-14 06:27:42+00:00,2024-09-14 06:27:42+00:00,https://github.com/tensorflow/tensorflow/pull/75665,[],[],
2523475519,pull_request,open,,caused by diff base,"caused by diff base
",copybara-service[bot],2024-09-12 22:35:53+00:00,[],2024-09-12 22:35:53+00:00,,https://github.com/tensorflow/tensorflow/pull/75664,[],[],
2523475282,pull_request,closed,,Stop using the `ScopedActivateExecutorContext` wrapper class in favor of the `ScopedActivateContext` class in preparation for `ScopedActivateExecutorContext`'s deletion.,"Stop using the `ScopedActivateExecutorContext` wrapper class in favor of the `ScopedActivateContext` class in preparation for `ScopedActivateExecutorContext`'s deletion.
",copybara-service[bot],2024-09-12 22:35:36+00:00,[],2024-09-16 20:30:39+00:00,2024-09-16 20:30:38+00:00,https://github.com/tensorflow/tensorflow/pull/75663,[],[],
2523474172,pull_request,closed,,Add a test.,"Add a test.
",copybara-service[bot],2024-09-12 22:34:26+00:00,[],2024-09-14 02:43:22+00:00,2024-09-14 02:43:21+00:00,https://github.com/tensorflow/tensorflow/pull/75662,[],[],
2523463828,pull_request,closed,,Fix space-to-batch propagation bug on reduce.,"Fix space-to-batch propagation bug on reduce.
",copybara-service[bot],2024-09-12 22:24:22+00:00,[],2024-09-13 03:49:25+00:00,2024-09-13 03:49:24+00:00,https://github.com/tensorflow/tensorflow/pull/75661,[],[],
2523445710,pull_request,closed,,device_manager_ creation must be guarded by mutex so that concurrent session creations won't have race.,"device_manager_ creation must be guarded by mutex so that concurrent session creations won't have race.
",copybara-service[bot],2024-09-12 22:10:13+00:00,[],2024-09-12 23:58:02+00:00,2024-09-12 23:58:01+00:00,https://github.com/tensorflow/tensorflow/pull/75660,[],[],
2523420470,pull_request,closed,,"We do not want to prevent _all_ cublas fallback (even those not involving int4) if int4 Triton fusion is enabled. Further, this regresses v2 MPP models because there","We do not want to prevent _all_ cublas fallback (even those not involving int4) if int4 Triton fusion is enabled. Further, this regresses v2 MPP models because there
are cases e.g. prefill with fat activations where unfused cublas wins over fused Triton.

Reverts f59b46bb8fb749ef3aac65fcd1145f2d95bf6b78
",copybara-service[bot],2024-09-12 21:51:33+00:00,[],2024-09-12 22:40:43+00:00,2024-09-12 22:40:43+00:00,https://github.com/tensorflow/tensorflow/pull/75659,[],[],
2523405814,pull_request,open,,test - remove sync,"test - remove sync
",copybara-service[bot],2024-09-12 21:39:26+00:00,['changhuilin'],2024-09-12 21:39:27+00:00,,https://github.com/tensorflow/tensorflow/pull/75658,[],[],
2523317612,pull_request,closed,,Remove unused cuda_dnn_headers target.,"Remove unused cuda_dnn_headers target.
",copybara-service[bot],2024-09-12 20:44:15+00:00,[],2024-09-13 16:59:33+00:00,2024-09-13 16:59:32+00:00,https://github.com/tensorflow/tensorflow/pull/75656,[],[],
2523316545,pull_request,closed,,Remove unnecessary dependency on gpu_activation.h.,"Remove unnecessary dependency on gpu_activation.h.
",copybara-service[bot],2024-09-12 20:43:28+00:00,[],2024-09-12 23:02:37+00:00,2024-09-12 23:02:36+00:00,https://github.com/tensorflow/tensorflow/pull/75655,[],[],
2523297701,pull_request,closed,,Turn ScopedActivateExecutorContext into an alias to ScopedActivateContext to ease its deprecation.,"Turn ScopedActivateExecutorContext into an alias to ScopedActivateContext to ease its deprecation.
",copybara-service[bot],2024-09-12 20:30:33+00:00,[],2024-09-12 21:14:40+00:00,2024-09-12 21:14:39+00:00,https://github.com/tensorflow/tensorflow/pull/75654,[],[],
2523261119,pull_request,closed,,[XLA:GPU] Remove the forced cast to `f32` when generating Triton reductions.,"[XLA:GPU] Remove the forced cast to `f32` when generating Triton reductions.

Triton can now handle reductions of types other than `f32`. Removing the cast makes a lot of the code simpler and also yields more ""correct"" numerics - in some cases this means less precise. I had to relax the error tolerance in a couple of `f16` tests because the calculations are now actually done in `f16` unlike the previous `f32`.

Simplifications enabled by this:
- No more casts in the generated code.
- Removed the need for the `is_within_reduction_computation` parameter in `triton_support.cc`.
- Removed a lot of cases that needed `skip_failure_branch_to_avoid_crash`.
",copybara-service[bot],2024-09-12 20:08:25+00:00,[],2024-09-12 22:05:05+00:00,2024-09-12 22:05:05+00:00,https://github.com/tensorflow/tensorflow/pull/75653,[],[],
2523127435,pull_request,closed,,Insert default value attributes when converting dialect ops to node defs.,"Insert default value attributes when converting dialect ops to node defs.
",copybara-service[bot],2024-09-12 18:58:22+00:00,[],2024-09-12 19:34:20+00:00,2024-09-12 19:34:19+00:00,https://github.com/tensorflow/tensorflow/pull/75652,[],[],
2523094877,pull_request,closed,,[MHLO] Fix typo in mhlo.dynamic_broadcast_in_dim op argument name,"[MHLO] Fix typo in mhlo.dynamic_broadcast_in_dim op argument name
",copybara-service[bot],2024-09-12 18:41:00+00:00,[],2024-09-12 23:11:57+00:00,2024-09-12 23:11:56+00:00,https://github.com/tensorflow/tensorflow/pull/75651,[],[],
2523088435,pull_request,closed,,[xla:ffi] Add benchmarks for async FFI calls,"[xla:ffi] Add benchmarks for async FFI calls

-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
BM_AnyBufferArgX1            18.2 ns         18.2 ns     37983521
BM_AnyBufferArgX4            22.7 ns         22.7 ns     30682729
BM_AnyBufferArgX8            27.9 ns         27.9 ns     24817074
BM_AsyncAnyBufferArgX1       55.2 ns         55.2 ns     12514333
",copybara-service[bot],2024-09-12 18:37:06+00:00,['ezhulenev'],2024-09-14 02:58:17+00:00,2024-09-14 02:58:17+00:00,https://github.com/tensorflow/tensorflow/pull/75650,[],[],
2523083950,pull_request,closed,,Internal only change.,"Internal only change.
",copybara-service[bot],2024-09-12 18:34:17+00:00,[],2024-09-12 22:14:58+00:00,2024-09-12 22:14:56+00:00,https://github.com/tensorflow/tensorflow/pull/75649,[],[],
2523079490,pull_request,closed,,Fail fast if required folders under CUDA/CUDNN/NCCL local paths are not available.,"Fail fast if required folders under CUDA/CUDNN/NCCL local paths are not available.

Failure should help to catch the issues before the actual compilation phase.

The instructions for local paths are [here](https://github.com/openxla/xla/blob/main/docs/hermetic_cuda.md?plain=1#L164).
",copybara-service[bot],2024-09-12 18:31:35+00:00,[],2024-09-12 19:43:49+00:00,2024-09-12 19:43:48+00:00,https://github.com/tensorflow/tensorflow/pull/75648,[],[],
2523077193,pull_request,open,,Integrate LLVM at llvm/llvm-project@a16164d0c258,"Integrate LLVM at llvm/llvm-project@a16164d0c258

Updates LLVM usage to match
[a16164d0c258](https://github.com/llvm/llvm-project/commit/a16164d0c258)
",copybara-service[bot],2024-09-12 18:30:12+00:00,[],2024-09-12 18:30:12+00:00,,https://github.com/tensorflow/tensorflow/pull/75647,[],[],
2523076986,pull_request,closed,,[Numpy] Upgrade TF wheels to NumPy 2.0,"[Numpy] Upgrade TF wheels to NumPy 2.0

To ensure compatibility with numpy>=2.0.0, the following dependencies have been upgraded as well: ml-dtype>=0.4.0, h5py>=3.11.0, and scipy>=1.13.0.

Maintain lock files and an env file for follow up numpy1 jobs as planned.
",copybara-service[bot],2024-09-12 18:30:04+00:00,['kanglant'],2024-09-18 20:47:46+00:00,2024-09-18 20:47:45+00:00,https://github.com/tensorflow/tensorflow/pull/75646,[],[],
2523076333,pull_request,closed,,Reverts ba46bf55c91773be94f5983dad8bd61a970c5c05,"Reverts ba46bf55c91773be94f5983dad8bd61a970c5c05
",copybara-service[bot],2024-09-12 18:29:41+00:00,[],2024-09-12 19:23:17+00:00,2024-09-12 19:23:16+00:00,https://github.com/tensorflow/tensorflow/pull/75645,[],[],
2523044328,pull_request,closed,,Remove the usage of internal bridge passes from prepare_tf.cc,"Remove the usage of internal bridge passes from prepare_tf.cc
",copybara-service[bot],2024-09-12 18:11:04+00:00,[],2024-09-16 18:57:01+00:00,2024-09-16 18:57:01+00:00,https://github.com/tensorflow/tensorflow/pull/75644,[],[],
2522912742,pull_request,closed,,[XLA:GPU] Build a workaround to fix tiling propagation for Triton when tile sizes,"[XLA:GPU] Build a workaround to fix tiling propagation for Triton when tile sizes
are not set to be a power of two.

Because we currently have cases when tile sizes are not set to be a power of two
(e.g. when capturing the full dimension at the output, or when introducing a
non-power-of-two contracting dimension in the middle of tiling propagation), we
can get into issues with `reshape`s. Previously to this change, we would
verify that constraints coming out of a `reshape` are satisfied by a
tuple of exact tile sizes, but later pad these tile sizes at code generation
time---making the lowering of the `reshape` incorrect.

The proper fix for this issue would be to always propagate tile sizes that are
a power of 2 in fusions that will be code generated using Triton. Alas, this
is easier said than done, and requires introducing a principled way of tiling
along newly introduced contracting dimensions.

We already know how we intend
to solve this, but it'll take a while. For this reason, we instead introduce
here Triton-specific constraints aimed at weeding out improper tile sizes as
they arise.
",copybara-service[bot],2024-09-12 16:59:12+00:00,[],2024-09-12 20:31:26+00:00,2024-09-12 20:31:25+00:00,https://github.com/tensorflow/tensorflow/pull/75643,[],[],
2522883427,pull_request,closed,,GPU TopK custom call needs the same layout for operand and output.,"GPU TopK custom call needs the same layout for operand and output.

We need to enforce this during layout assignment. Also, the output of TopK
custom call needs to have the default layout.
",copybara-service[bot],2024-09-12 16:44:40+00:00,['akuegel'],2024-09-13 09:49:18+00:00,2024-09-13 09:49:18+00:00,https://github.com/tensorflow/tensorflow/pull/75642,[],[],
2522858228,pull_request,closed,,Reverts 65811e53f11472c8be120b269a8eac741137649b,"Reverts 65811e53f11472c8be120b269a8eac741137649b
",copybara-service[bot],2024-09-12 16:30:39+00:00,[],2024-09-12 17:19:43+00:00,2024-09-12 17:19:43+00:00,https://github.com/tensorflow/tensorflow/pull/75641,[],[],
2522844463,pull_request,closed,,Move CUDA configuration paragraph up in the document.,"Move CUDA configuration paragraph up in the document.
",copybara-service[bot],2024-09-12 16:24:49+00:00,[],2024-09-12 17:39:08+00:00,2024-09-12 17:39:08+00:00,https://github.com/tensorflow/tensorflow/pull/75640,[],[],
2522705685,pull_request,closed,,[xla:ffi] Add support for returning xla::ffi::Future from FFI handler,"[xla:ffi] Add support for returning xla::ffi::Future from FFI handler
",copybara-service[bot],2024-09-12 15:19:54+00:00,['ezhulenev'],2024-09-12 16:06:21+00:00,2024-09-12 16:06:21+00:00,https://github.com/tensorflow/tensorflow/pull/75639,[],[],
2522653583,pull_request,closed,,[XLA:GPU] Remove temporary workaround.,"[XLA:GPU] Remove temporary workaround.

We have added the flag slp-max-vf=4 to workaround a PTX backend bug that was
triggered by a recent SLP vectorizer change. The PTX backend bug is fixed, so
we can remove the workaround.
",copybara-service[bot],2024-09-12 14:59:34+00:00,['akuegel'],2024-09-12 17:05:12+00:00,2024-09-12 17:05:12+00:00,https://github.com/tensorflow/tensorflow/pull/75638,[],[],
2522573720,pull_request,open,,Add additional methods to ClientLibraryTestBase,"Add additional methods to ClientLibraryTestBase
",copybara-service[bot],2024-09-12 14:32:04+00:00,[],2024-09-12 16:36:34+00:00,,https://github.com/tensorflow/tensorflow/pull/75637,[],[],
2522553236,pull_request,closed,,Integrate LLVM at llvm/llvm-project@36adf8ecedb6,"Integrate LLVM at llvm/llvm-project@36adf8ecedb6

Updates LLVM usage to match
[36adf8ecedb6](https://github.com/llvm/llvm-project/commit/36adf8ecedb6)
",copybara-service[bot],2024-09-12 14:23:54+00:00,[],2024-09-12 16:54:39+00:00,2024-09-12 16:54:38+00:00,https://github.com/tensorflow/tensorflow/pull/75636,[],[],
2522550452,pull_request,closed,,Integrate Triton up to [50d803cd](https://github.com/openai/triton/commits/50d803cdb4e68910ed663251100e168ea4d2519d,"Integrate Triton up to [50d803cd](https://github.com/openai/triton/commits/50d803cdb4e68910ed663251100e168ea4d2519d
",copybara-service[bot],2024-09-12 14:23:10+00:00,['chsigg'],2024-09-16 21:06:19+00:00,2024-09-16 21:06:18+00:00,https://github.com/tensorflow/tensorflow/pull/75635,[],[],
2522539371,pull_request,closed,,Remove calls to xnnpack from TFLite kernels.,"Remove calls to xnnpack from TFLite kernels.

This is no longer necessary since all models now support XNNPack.
",copybara-service[bot],2024-09-12 14:19:24+00:00,['alankelly'],2024-09-17 12:50:02+00:00,2024-09-17 12:50:01+00:00,https://github.com/tensorflow/tensorflow/pull/75634,[],[],
2522518019,pull_request,closed,,Expand the set of strategies generated for kScatter HLO ops.,"Expand the set of strategies generated for kScatter HLO ops.

As part of this CL creates a significantly simplified copy of the handling of scatter ops in  ShardingPropagation::InferShardingFromOperands. The implementation currently does not support tuple-shaped scatter ops (nor did the original implementation), but it should be easy to generalize if needed.
",copybara-service[bot],2024-09-12 14:10:29+00:00,[],2024-09-12 15:57:09+00:00,2024-09-12 15:57:08+00:00,https://github.com/tensorflow/tensorflow/pull/75633,[],[],
2522435718,pull_request,closed,,PR #16913: [PJRT:GPU] Enable creating topology without a GPU device,"PR #16913: [PJRT:GPU] Enable creating topology without a GPU device

Imported from GitHub PR https://github.com/openxla/xla/pull/16913

Currently PJRT_TopologyDescription_Create always creates topology from the local client. This requires having a local GPU device.

This patch allows explicitly specifying topology shape and device config in PJRT_TopologyDescription_Create call, without querying local client. This enables deviceless compilation.
Copybara import of the project:

--
bc85038cbdfbed4b43b5859037515ef9049ecfec by Jaroslav Sevcik <jsevcik@nvidia.com>:

[PJRT:GPU] Enable creating topology without a GPU device

--
e4eb44ecf356f0c7d859c969ff38b69e52d569c5 by Jaroslav Sevcik <jsevcik@nvidia.com>:

Enable overlaying topology on local device

--
ff25a5790bfba35262f9649cb6df234fc2af5a3d by Jaroslav Sevcik <jsevcik@nvidia.com>:

Address reviewer comments

--
8bfb7a251f1f687b65e6db3744dc7c76d60a321e by Jaroslav Sevcik <jsevcik@nvidia.com>:

Cleanup

Merging this change closes #16913

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16913 from jaro-sevcik:deviceless-topology-creation 8bfb7a251f1f687b65e6db3744dc7c76d60a321e
",copybara-service[bot],2024-09-12 13:39:39+00:00,[],2024-09-26 09:04:16+00:00,2024-09-26 09:04:14+00:00,https://github.com/tensorflow/tensorflow/pull/75632,[],[],
2522361506,pull_request,closed,,[XLA:GPU] Move part of `SymbolicTileAnalysis`'s constraint-checking logic into a new method `ConstraintExpression::IsSatisfiedBy`.,"[XLA:GPU] Move part of `SymbolicTileAnalysis`'s constraint-checking logic into a new method `ConstraintExpression::IsSatisfiedBy`.

This resolves an outstanding `TODO`, but more importantly, makes it easier
to reuse this logic in an upcoming change.
",copybara-service[bot],2024-09-12 13:11:35+00:00,[],2024-09-12 13:35:36+00:00,2024-09-12 13:35:35+00:00,https://github.com/tensorflow/tensorflow/pull/75631,[],[],
2522115685,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-12 11:21:35+00:00,[],2024-09-13 03:38:07+00:00,2024-09-13 03:38:07+00:00,https://github.com/tensorflow/tensorflow/pull/75630,[],[],
2521879300,pull_request,closed,,Integrate LLVM at llvm/llvm-project@3cd01371e007,"Integrate LLVM at llvm/llvm-project@3cd01371e007

Updates LLVM usage to match
[3cd01371e007](https://github.com/llvm/llvm-project/commit/3cd01371e007)
",copybara-service[bot],2024-09-12 09:38:12+00:00,[],2024-09-12 12:55:36+00:00,2024-09-12 12:55:34+00:00,https://github.com/tensorflow/tensorflow/pull/75629,[],[],
2521864387,pull_request,closed,,Workaround a potential compile breakage with Clang HEAD and header modules.,"Workaround a potential compile breakage with Clang HEAD and header modules.
Top-level lambdas may cause issues that we are observing internally.

Using a struct instead to workaround the issue.
",copybara-service[bot],2024-09-12 09:31:39+00:00,[],2024-09-13 15:04:45+00:00,2024-09-13 15:04:45+00:00,https://github.com/tensorflow/tensorflow/pull/75628,[],[],
2521845319,pull_request,closed,,Rename no_rocm tag to cuda-only,"Rename no_rocm tag to cuda-only

This renames the tag `no_rocm` to `cuda-only` and prepares for the introduction of a new `rocm-only` tag.

This part of a bigger effort to unify the build graph and get rid of too many compilation modes than we can't all test.
",copybara-service[bot],2024-09-12 09:23:50+00:00,[],2024-09-23 07:23:27+00:00,2024-09-23 07:23:26+00:00,https://github.com/tensorflow/tensorflow/pull/75627,[],[],
2521777584,pull_request,closed,,Introduce rocm-only tag and remove if_rocm_is_configured,"Introduce rocm-only tag and remove if_rocm_is_configured

This replaces all `if_rocm_is_configured` guards in `stream_executor/rocm/...` with a filtering tag `rocm-only`. The CUDA build on the CI gets adjusted to skip those targets.

This uncovered some additional problems that get fixed as well:

- A wrong library name for the hipfft library in the Bazel CUDA configuration
- A wrong test case in the RocmVersionParser test that so far has not been running anywhere.
- Missing tags for the platform alias targets in `stream_executor/BUILD`
",copybara-service[bot],2024-09-12 08:54:51+00:00,[],2024-09-25 06:27:42+00:00,2024-09-25 06:27:41+00:00,https://github.com/tensorflow/tensorflow/pull/75626,[],[],
2521504382,pull_request,closed,,PR #17044: Explain the different PTX compilation and linking methods.,"PR #17044: Explain the different PTX compilation and linking methods.

Imported from GitHub PR https://github.com/openxla/xla/pull/17044


Copybara import of the project:

--
171bdae12d408cde3b7929dae3734eeca646a930 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Explain the different PTX compilation and linking methods.

Merging this change closes #17044

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17044 from dimvar:document-compilation-linking-methods 171bdae12d408cde3b7929dae3734eeca646a930
",copybara-service[bot],2024-09-12 06:50:08+00:00,[],2024-09-12 07:24:48+00:00,2024-09-12 07:24:47+00:00,https://github.com/tensorflow/tensorflow/pull/75620,[],[],
2521480189,pull_request,closed,,Update Documentation for New FeaturesFeature/update docs,This pull request updates the README.md file to include new instructions for the recently added functionality,yashraj-45,2024-09-12 06:35:57+00:00,['gbaned'],2024-09-12 07:00:22+00:00,2024-09-12 07:00:21+00:00,https://github.com/tensorflow/tensorflow/pull/75619,"[('size:S', 'CL Change Size: Small')]","[{'comment_id': 2345388011, 'issue_id': 2521480189, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/75619/checks?check_run_id=30034554759) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 9, 12, 6, 36, 1, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-09-12 06:36:01 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/75619/checks?check_run_id=30034554759) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2521443888,pull_request,open,,[TEST] Internal change only,"[TEST] Internal change only

This is not getting submitted.
",copybara-service[bot],2024-09-12 06:13:01+00:00,[],2024-09-12 06:13:01+00:00,,https://github.com/tensorflow/tensorflow/pull/75618,[],[],
2521218354,pull_request,closed,,internal BUILD rule visibility,"internal BUILD rule visibility
",copybara-service[bot],2024-09-12 02:54:38+00:00,[],2024-09-13 16:45:35+00:00,2024-09-13 16:45:34+00:00,https://github.com/tensorflow/tensorflow/pull/75617,[],[],
2521170344,pull_request,closed,,Add noop kernel on empty conditional graph as a workaround.,"Add noop kernel on empty conditional graph as a workaround.

Reverts 47076501556176d7cc2eb773122d7e709f00a54b
",copybara-service[bot],2024-09-12 02:12:43+00:00,[],2024-09-12 17:46:37+00:00,2024-09-12 17:46:36+00:00,https://github.com/tensorflow/tensorflow/pull/75616,[],[],
2521131640,pull_request,closed,,Integrate LLVM at llvm/llvm-project@e55d6f5ea265,"Integrate LLVM at llvm/llvm-project@e55d6f5ea265

Updates LLVM usage to match
[e55d6f5ea265](https://github.com/llvm/llvm-project/commit/e55d6f5ea265)
",copybara-service[bot],2024-09-12 01:40:12+00:00,[],2024-09-12 02:44:42+00:00,2024-09-12 02:44:41+00:00,https://github.com/tensorflow/tensorflow/pull/75615,[],[],
2521048095,pull_request,open,,Integrate LLVM at llvm/llvm-project@e55d6f5ea265,"Integrate LLVM at llvm/llvm-project@e55d6f5ea265

Updates LLVM usage to match
[e55d6f5ea265](https://github.com/llvm/llvm-project/commit/e55d6f5ea265)
",copybara-service[bot],2024-09-12 00:16:59+00:00,[],2024-09-12 02:47:53+00:00,,https://github.com/tensorflow/tensorflow/pull/75614,[],[],
2521040465,pull_request,closed,,mlir_hlo_to_hlo: Propagate diagnostics from PrepareForExport.,"mlir_hlo_to_hlo: Propagate diagnostics from PrepareForExport.

Propagating diagnostics back to the resulting Status helps make it clear what
has gone wrong.

A new unit test for interfaces above the MLIR level (like this) is added to
allow broader test coverage (and avoid linking Shape and StableHLO dialects
to xla-translate).
",copybara-service[bot],2024-09-12 00:08:56+00:00,[],2024-09-12 19:12:44+00:00,2024-09-12 19:12:43+00:00,https://github.com/tensorflow/tensorflow/pull/75613,[],[],
2521010113,pull_request,closed,,Update the comment of why we want to delete the ops called between RestoreV2Op to AssignVariableOp.,"Update the comment of why we want to delete the ops called between RestoreV2Op to AssignVariableOp.
",copybara-service[bot],2024-09-11 23:42:27+00:00,[],2024-09-12 00:12:26+00:00,2024-09-12 00:12:26+00:00,https://github.com/tensorflow/tensorflow/pull/75612,[],[],
2520985430,pull_request,closed,,Simplifies the result of sharding propagation to map names to shardings instead of pointers (which may or may not reference instructions that are still in memory).,"Simplifies the result of sharding propagation to map names to shardings instead of pointers (which may or may not reference instructions that are still in memory).
",copybara-service[bot],2024-09-11 23:26:06+00:00,[],2024-09-12 00:54:37+00:00,2024-09-12 00:54:37+00:00,https://github.com/tensorflow/tensorflow/pull/75611,[],[],
2520963918,pull_request,closed,,Remove unused dependencies on cuda_activation.,"Remove unused dependencies on cuda_activation.
",copybara-service[bot],2024-09-11 23:17:35+00:00,[],2024-09-12 15:22:43+00:00,2024-09-12 15:22:41+00:00,https://github.com/tensorflow/tensorflow/pull/75610,[],[],
2520948647,pull_request,closed,,Wait for events in a different thread if they are not defined yet.,"Wait for events in a different thread if they are not defined yet.
",copybara-service[bot],2024-09-11 23:07:42+00:00,[],2024-09-23 17:22:47+00:00,2024-09-23 17:22:45+00:00,https://github.com/tensorflow/tensorflow/pull/75609,[],[],
2520941097,pull_request,closed,,Add TFLite flatbuffer debug metadata serialization logic.,"Add TFLite flatbuffer debug metadata serialization logic.
",copybara-service[bot],2024-09-11 23:00:07+00:00,[],2024-09-16 05:39:44+00:00,2024-09-16 05:39:43+00:00,https://github.com/tensorflow/tensorflow/pull/75608,[],[],
2520928134,pull_request,closed,,[xla:ffi] Add support for returning tsl::AsyncValueRef from FFI handler,"[xla:ffi] Add support for returning tsl::AsyncValueRef from FFI handler
",copybara-service[bot],2024-09-11 22:49:20+00:00,['ezhulenev'],2024-09-12 18:32:27+00:00,2024-09-12 18:32:26+00:00,https://github.com/tensorflow/tensorflow/pull/75607,[],[],
2520903825,pull_request,closed,,Stop using the ScopedActivateExecutorContext wrapper class in favor of ScopedActivateContext class in preparation for ScopedActivateExecutorContext's deletion.,"Stop using the ScopedActivateExecutorContext wrapper class in favor of ScopedActivateContext class in preparation for ScopedActivateExecutorContext's deletion.
",copybara-service[bot],2024-09-11 22:30:36+00:00,[],2024-09-12 15:34:29+00:00,2024-09-12 15:34:28+00:00,https://github.com/tensorflow/tensorflow/pull/75606,[],[],
2520898789,pull_request,closed,,add more graph dumps,"add more graph dumps
",copybara-service[bot],2024-09-11 22:25:37+00:00,[],2024-09-11 23:53:18+00:00,2024-09-11 23:53:18+00:00,https://github.com/tensorflow/tensorflow/pull/75605,[],[],
2520879265,pull_request,closed,,Check for dynamism when fusing reshapes.,"Check for dynamism when fusing reshapes.
",copybara-service[bot],2024-09-11 22:12:37+00:00,['vamsimanchala'],2024-09-12 03:43:35+00:00,2024-09-12 03:43:35+00:00,https://github.com/tensorflow/tensorflow/pull/75604,[],[],
2520860714,pull_request,closed,,Add GraphGetNodeCount to CudaDriver implementation.,"Add GraphGetNodeCount to CudaDriver implementation.
",copybara-service[bot],2024-09-11 21:57:11+00:00,[],2024-09-12 00:23:47+00:00,2024-09-12 00:23:47+00:00,https://github.com/tensorflow/tensorflow/pull/75603,[],[],
2520850855,pull_request,closed,,Move `BatchedGatherScatterNormalizer` to `PreOptimizationPipeline` because `ScatterExpander` is applied there.,"Move `BatchedGatherScatterNormalizer` to `PreOptimizationPipeline` because `ScatterExpander` is applied there.

Also add a test with a zero sized batch dim, since jax can emit this.
",copybara-service[bot],2024-09-11 21:51:12+00:00,[],2024-09-12 18:13:03+00:00,2024-09-12 18:13:02+00:00,https://github.com/tensorflow/tensorflow/pull/75602,[],[],
2520842112,pull_request,open,,Introduce an option to use XLA Split concat ops in XLA sharding util.,"Introduce an option to use XLA Split concat ops in XLA sharding util.
",copybara-service[bot],2024-09-11 21:45:10+00:00,['ishark'],2024-09-11 21:45:11+00:00,,https://github.com/tensorflow/tensorflow/pull/75601,[],[],
2520826368,pull_request,open,,Introduce an option to use XLA Split concat ops in XLA sharding util.,"Introduce an option to use XLA Split concat ops in XLA sharding util.
",copybara-service[bot],2024-09-11 21:35:56+00:00,['ishark'],2024-09-11 21:35:57+00:00,,https://github.com/tensorflow/tensorflow/pull/75600,[],[],
2520818423,pull_request,open,,Add TPUValidateInputsPass to the MLIR bridge session pipeline,"Add TPUValidateInputsPass to the MLIR bridge session pipeline
",copybara-service[bot],2024-09-11 21:32:41+00:00,[],2024-09-11 21:32:41+00:00,,https://github.com/tensorflow/tensorflow/pull/75599,[],[],
2520815761,pull_request,closed,,Add a new ShouldFuse method that takes a function to decide whether to fuse in-place ops.,"Add a new ShouldFuse method that takes a function to decide whether to fuse in-place ops.
This allows for fusion decisions related to in-place ops to be more target dependent.
",copybara-service[bot],2024-09-11 21:31:32+00:00,[],2024-09-12 01:15:14+00:00,2024-09-12 01:15:14+00:00,https://github.com/tensorflow/tensorflow/pull/75598,[],[],
2520797616,pull_request,closed,,Remove unused KernelCacheConfig.,"Remove unused KernelCacheConfig.
",copybara-service[bot],2024-09-11 21:20:31+00:00,[],2024-09-11 23:42:39+00:00,2024-09-11 23:42:39+00:00,https://github.com/tensorflow/tensorflow/pull/75597,[],[],
2520785222,pull_request,open,,Integrate LLVM at llvm/llvm-project@e55d6f5ea265,"Integrate LLVM at llvm/llvm-project@e55d6f5ea265

Updates LLVM usage to match
[e55d6f5ea265](https://github.com/llvm/llvm-project/commit/e55d6f5ea265)
",copybara-service[bot],2024-09-11 21:11:37+00:00,[],2024-09-11 21:11:37+00:00,,https://github.com/tensorflow/tensorflow/pull/75596,[],[],
2520780843,pull_request,open,,Check the MLIR pattern when removing a lot graph analysis check.,"Check the MLIR pattern when removing a lot graph analysis check.
",copybara-service[bot],2024-09-11 21:09:30+00:00,[],2024-09-11 21:09:30+00:00,,https://github.com/tensorflow/tensorflow/pull/75595,[],[],
2520779184,pull_request,open,,Deprecate fallback mode in mlir_graph_optimization_pass.,"Deprecate fallback mode in mlir_graph_optimization_pass.
",copybara-service[bot],2024-09-11 21:09:06+00:00,[],2024-09-11 21:09:06+00:00,,https://github.com/tensorflow/tensorflow/pull/75594,[],[],
2520769498,pull_request,closed,,Introduce an option to use XLA Split concat ops in XLA sharding util.,"Introduce an option to use XLA Split concat ops in XLA sharding util.
",copybara-service[bot],2024-09-11 21:04:28+00:00,['ishark'],2024-09-11 21:39:40+00:00,2024-09-11 21:39:39+00:00,https://github.com/tensorflow/tensorflow/pull/75593,[],[],
2520759381,pull_request,closed,,Move InfeedManager and OutfeedManager creation into GpuTransferManager.,"Move InfeedManager and OutfeedManager creation into GpuTransferManager.
",copybara-service[bot],2024-09-11 20:59:04+00:00,[],2024-09-11 22:20:57+00:00,2024-09-11 22:20:56+00:00,https://github.com/tensorflow/tensorflow/pull/75592,[],[],
2520740944,pull_request,open,,Use ScopedActivateContext instead of the ScopedActivateExecutorContext wrapper in common_runtime code.,"Use ScopedActivateContext instead of the ScopedActivateExecutorContext wrapper in common_runtime code.
",copybara-service[bot],2024-09-11 20:49:07+00:00,[],2024-09-11 20:49:07+00:00,,https://github.com/tensorflow/tensorflow/pull/75591,[],[],
2520710068,pull_request,closed,,Expose LocalDeviceManager in LiteSessionWrapper,"Expose LocalDeviceManager in LiteSessionWrapper
",copybara-service[bot],2024-09-11 20:33:40+00:00,[],2024-09-12 18:43:52+00:00,2024-09-12 18:43:51+00:00,https://github.com/tensorflow/tensorflow/pull/75590,[],[],
2520699391,pull_request,closed,,Check if weak lru cach entry exists before accessing it.,"Check if weak lru cach entry exists before accessing it.
",copybara-service[bot],2024-09-11 20:28:08+00:00,[],2024-09-12 00:43:23+00:00,2024-09-12 00:43:23+00:00,https://github.com/tensorflow/tensorflow/pull/75589,[],[],
2520686591,pull_request,closed,,"[XLA:GPU] Compute a block-specific base ptr, and residual shapes when calling Triton's `MakeTensorPtrOp`.","[XLA:GPU] Compute a block-specific base ptr, and residual shapes when calling Triton's `MakeTensorPtrOp`.

When using `MakeTensorPtrOp` the addresses Triton reads are a multiple of the stride and offset. This makes it impossible to read blocks from offsets that are not aligned with the stride. To avoid this issue we can bake the offset directly into the base-pointer and provide a ""residual shape"" which is used for masking. The residual shape is simply `parent_tensor_shape - offsets`.

This change enables arbitrary slices to work properly - e.g. unaligned ones.
",copybara-service[bot],2024-09-11 20:20:51+00:00,[],2024-09-12 11:14:33+00:00,2024-09-12 11:14:32+00:00,https://github.com/tensorflow/tensorflow/pull/75588,[],[],
2520646185,pull_request,closed,,[EagerTensor] Inline and specialize `set_shape()` method.,"[EagerTensor] Inline and specialize `set_shape()` method.

This method is called heavily when accessing elements of a tf.data.Iterator in eager mode. To reduce the overhead, we specialize for the internal implementation of `TensorShape` and use the fact that the eager tensor always has a fully-defined shape.
",copybara-service[bot],2024-09-11 19:56:25+00:00,[],2024-09-11 20:31:40+00:00,2024-09-11 20:31:40+00:00,https://github.com/tensorflow/tensorflow/pull/75587,[],[],
2520608984,pull_request,closed,,Use `try_cast` in `SafeNumDevices` now that python shardings are always initialized and cannot be in an uninitialized state.,"Use `try_cast` in `SafeNumDevices` now that python shardings are always initialized and cannot be in an uninitialized state.

If that happens, it's a bug in the python land.
",copybara-service[bot],2024-09-11 19:34:13+00:00,['yashk2810'],2024-09-11 21:29:29+00:00,2024-09-11 21:29:29+00:00,https://github.com/tensorflow/tensorflow/pull/75586,[],[],
2520596847,pull_request,closed,,Better error message in HloComputation::ReplaceInstruction,"Better error message in HloComputation::ReplaceInstruction
",copybara-service[bot],2024-09-11 19:26:57+00:00,['SandSnip3r'],2024-09-12 00:01:33+00:00,2024-09-12 00:01:32+00:00,https://github.com/tensorflow/tensorflow/pull/75584,[],[],
2520595173,pull_request,open,,Add python 3.13 to aarch64 image for JAX,"Add python 3.13 to aarch64 image for JAX
",copybara-service[bot],2024-09-11 19:25:54+00:00,['MichaelHudgins'],2024-09-11 19:25:55+00:00,,https://github.com/tensorflow/tensorflow/pull/75583,[],[],
2520577875,pull_request,closed,,Add pre simulation device assignment to `HloModuleConfig` to use in case of simulating a large topology on an smaller one.,"Add pre simulation device assignment to `HloModuleConfig` to use in case of simulating a large topology on an smaller one.
",copybara-service[bot],2024-09-11 19:18:17+00:00,['subhankarshah'],2024-09-16 06:40:24+00:00,2024-09-16 06:40:23+00:00,https://github.com/tensorflow/tensorflow/pull/75582,[],[],
2520483501,pull_request,closed,,Use ScopedActivateContext instead of wrapper class ScopedActivateExecutorContext.,"Use ScopedActivateContext instead of wrapper class ScopedActivateExecutorContext.
",copybara-service[bot],2024-09-11 18:42:45+00:00,[],2024-09-11 22:30:15+00:00,2024-09-11 22:30:15+00:00,https://github.com/tensorflow/tensorflow/pull/75581,[],[],
2520459634,pull_request,closed,,Make `ExhaustiveOpTestTraits::BuildFromInputs` `static_assert` condition more explicit,"Make `ExhaustiveOpTestTraits::BuildFromInputs` `static_assert` condition more explicit

Try to fix macOS build by being more explicit about the `static_assert` condition.
",copybara-service[bot],2024-09-11 18:33:10+00:00,[],2024-09-11 20:45:07+00:00,2024-09-11 20:45:06+00:00,https://github.com/tensorflow/tensorflow/pull/75580,[],[],
2520455137,pull_request,closed,,Internal only changes,"Internal only changes
",copybara-service[bot],2024-09-11 18:30:34+00:00,['zzzaries'],2024-09-12 20:42:37+00:00,2024-09-12 20:42:36+00:00,https://github.com/tensorflow/tensorflow/pull/75579,[],[],
2520340842,pull_request,closed,,[Distributed Eager] Fix reference counting of handle objects in the SendAsProtosWhenPossible fast path.,"[Distributed Eager] Fix reference counting of handle objects in the SendAsProtosWhenPossible fast path.

Previously, this would lead to handle objects being created on each call to `EagerRemoteExecute()` and leaked.
",copybara-service[bot],2024-09-11 17:32:00+00:00,[],2024-09-16 03:29:29+00:00,2024-09-16 03:29:28+00:00,https://github.com/tensorflow/tensorflow/pull/75577,[],[],
2520288862,pull_request,closed,,#sdy Add sdy-round-trip-shard-map-export Pass.,"#sdy Add sdy-round-trip-shard-map-export Pass.
",copybara-service[bot],2024-09-11 17:13:38+00:00,[],2024-09-13 15:20:43+00:00,2024-09-13 15:20:42+00:00,https://github.com/tensorflow/tensorflow/pull/75576,[],[],
2520283277,pull_request,closed,,Promote testing::tflite::SimpleConstTensor,"Promote testing::tflite::SimpleConstTensor

This is a struct that simplifies the creation and management of constant
`TfLiteTensor` objects, automatically deallocating the memory (including
dims) at destruction time.

Example:
  float data[] = {2.71828f, 3.14159f};
  SimpleConstTensor a(TfLiteType::kTfLiteFloat32, {1, 2}, absl::MakeSpan(data));
",copybara-service[bot],2024-09-11 17:11:06+00:00,['terryheo'],2024-09-11 18:40:35+00:00,2024-09-11 18:40:34+00:00,https://github.com/tensorflow/tensorflow/pull/75575,[],[],
2520268780,pull_request,closed,,Add TF wheel API test.,"Add TF wheel API test.

This test verifies whether the API v2 packages can be imported from the
current build. It utilizes the `_api/v2/api_packages.txt` list of packages from
the local wheel file specified in the `requirements_lock_<python_version>.txt`.

The test should be executed after the TF wheel was built and put into `dist` dir inside Tensorflow repository.
",copybara-service[bot],2024-09-11 17:03:02+00:00,[],2024-09-20 17:43:27+00:00,2024-09-20 17:43:26+00:00,https://github.com/tensorflow/tensorflow/pull/75574,[],[],
2520256982,pull_request,closed,,[xla:ffi] NFC: Clean up fetching XLA FFI metadata,"[xla:ffi] NFC: Clean up fetching XLA FFI metadata

1. Move internal implementation details to .cc file
2. Struct aggregate initialization is guaranteed to value initialize members missing from the initializer list => no need to initialize members to 0 and nullptr
3. Fake struct inheritance using XLA_FFI_Extension_Base data member
",copybara-service[bot],2024-09-11 16:56:16+00:00,['ezhulenev'],2024-09-11 22:08:28+00:00,2024-09-11 22:08:27+00:00,https://github.com/tensorflow/tensorflow/pull/75573,[],[],
2520221289,pull_request,closed,,Use `xla::GetDefaultStablehloVersion` with ~12w compatibility requirement in third_party/tensorflow/compiler/xla/python/ifrt/hlo/hlo_program_serdes.cc,"Use `xla::GetDefaultStablehloVersion` with ~12w compatibility requirement in third_party/tensorflow/compiler/xla/python/ifrt/hlo/hlo_program_serdes.cc
",copybara-service[bot],2024-09-11 16:39:54+00:00,[],2024-09-14 09:54:55+00:00,2024-09-14 09:54:54+00:00,https://github.com/tensorflow/tensorflow/pull/75572,[],[],
2520180617,pull_request,open,,Check for exact error explanations.,"Check for exact error explanations.

This CL updates NodeMatchers.CheckControlDependence to check for matcher messages. This is a followup to a prior CL which changed matcher messages. It is now possible for a more exact check.
",copybara-service[bot],2024-09-11 16:18:06+00:00,[],2024-09-11 16:18:06+00:00,,https://github.com/tensorflow/tensorflow/pull/75571,[],[],
2520176505,pull_request,closed,,"[xla:ffi] Add XLA_FFI_Future and xla::ffi::{Promise,Future} for signalling async events","[xla:ffi] Add XLA_FFI_Future and xla::ffi::{Promise,Future} for signalling async events

In preparation for supporting asynchronous XLA FFI handlers add a Pomise/Future primitives that can be used to signal completion of async computations.
",copybara-service[bot],2024-09-11 16:15:50+00:00,['ezhulenev'],2024-09-11 18:05:33+00:00,2024-09-11 18:05:33+00:00,https://github.com/tensorflow/tensorflow/pull/75570,[],[],
2520121371,pull_request,closed,,Use ScopedActivateContext instead of wrapper ScopedActivateExecutorContext in tensorflow/core.,"Use ScopedActivateContext instead of wrapper ScopedActivateExecutorContext in tensorflow/core.
",copybara-service[bot],2024-09-11 15:50:48+00:00,[],2024-09-12 22:23:34+00:00,2024-09-12 22:23:34+00:00,https://github.com/tensorflow/tensorflow/pull/75569,[],[],
2519867654,pull_request,closed,,[XLA:GPU] Fix an issue where Triton incorrectly creates two replace rewrites when lowering `IndexCastOp`.,"[XLA:GPU] Fix an issue where Triton incorrectly creates two replace rewrites when lowering `IndexCastOp`.

`createDestOps` should only create the replacement ops, but not actually do the replacement. This replacement happens in the caller of `createDestOps` which in this case is `ElementwiseOpConversionBase::matchAndRewrite`.
",copybara-service[bot],2024-09-11 14:10:34+00:00,[],2024-09-11 14:56:39+00:00,2024-09-11 14:56:39+00:00,https://github.com/tensorflow/tensorflow/pull/75568,[],[],
2519752238,pull_request,closed,,Updating the error message in flatbuffer_translate.cc,Updating the error message in flatbuffer_translate.cc and tf_tfl_translate_cl.cc,tilakrayal,2024-09-11 13:26:29+00:00,['gbaned'],2024-09-16 14:04:33+00:00,2024-09-16 14:04:32+00:00,https://github.com/tensorflow/tensorflow/pull/75567,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]",[],
2519705534,pull_request,closed,,Remove unused BF16xBF16ToBF16 Ampere and Hopper cutlass kernels.,"Remove unused BF16xBF16ToBF16 Ampere and Hopper cutlass kernels.
",copybara-service[bot],2024-09-11 13:07:25+00:00,[],2024-09-11 14:17:12+00:00,2024-09-11 14:17:11+00:00,https://github.com/tensorflow/tensorflow/pull/75566,[],[],
2519691418,pull_request,closed,,Make ptx_compilation_test less strict and more robust,"Make ptx_compilation_test less strict and more robust

Comparing binary artifacts of different linking methods in the PTX
compilation pipeline turned out to be too brittle. Differences
occur especially with different driver versions which we not
always control.

The differences seem to be minimal though. Single bytes in the
GPU binaries differ which is probably related to different register
assignments and other things that don't affect the actual program.

So this change replace the check for binary equivalence by a
check that only compares the binary sizes. That's not ideal but
better than nothing. If it turns out this is still too brittle
we might need to remove this requirement as well.

Ideally we would disassemble the GPU programs and do some
smarter equivalence comparisons, but for now that's out of
reach.
",copybara-service[bot],2024-09-11 13:01:46+00:00,[],2024-09-13 16:03:35+00:00,2024-09-13 16:03:35+00:00,https://github.com/tensorflow/tensorflow/pull/75565,[],[],
2519582760,pull_request,closed,,Add explicit includes to rocm_executor.h,"Add explicit includes to rocm_executor.h

A recent refactoring broke the ROCm build as it was relying on transitive includes.

This change adds explicit includes for all the symbols used in the header.
",copybara-service[bot],2024-09-11 12:14:25+00:00,[],2024-09-11 14:45:57+00:00,2024-09-11 14:45:56+00:00,https://github.com/tensorflow/tensorflow/pull/75564,[],[],
2519554512,pull_request,closed,,PR #17053: [GPU] cuDNN GEMM: handle more tensors with 1-sized dimensions.,"PR #17053: [GPU] cuDNN GEMM: handle more tensors with 1-sized dimensions.

Imported from GitHub PR https://github.com/openxla/xla/pull/17053

cuDNN relies on strides to determine layouts and gets confused when both strides of non-batch dimensions are equal to 1 - this is the case for tensors with 1-sized dimension like [A,1]. The stride of the 1-sized dimension does not matter for correctness because there is no iteration along this dimension, but setting it to A and representing the tensor as its  equivalent [1,A] helps cuDNN.
Copybara import of the project:

--
48cd1b242b5841fab539dc53e3e8ae781fe148b6 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] cuDNN GEMM: handle more tensors with 1-sized dimensions.

Merging this change closes #17053

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17053 from openxla:cudnn_1_sized_dims 48cd1b242b5841fab539dc53e3e8ae781fe148b6
",copybara-service[bot],2024-09-11 12:01:23+00:00,[],2024-09-12 07:56:13+00:00,2024-09-12 07:56:11+00:00,https://github.com/tensorflow/tensorflow/pull/75563,[],[],
2519495528,pull_request,closed,,Use `XNNPACK_RETURN_CHECK` in `weight_cache.cc`,"Use `XNNPACK_RETURN_CHECK` in `weight_cache.cc`

Replace `WriteData` function with `FilDescriptor::Write`.
",copybara-service[bot],2024-09-11 11:35:04+00:00,['qukhan'],2024-09-11 12:08:04+00:00,2024-09-11 12:08:03+00:00,https://github.com/tensorflow/tensorflow/pull/75561,[],[],
2519465609,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 11:25:15+00:00,[],2024-09-11 12:47:43+00:00,2024-09-11 12:47:42+00:00,https://github.com/tensorflow/tensorflow/pull/75560,[],[],
2519431989,pull_request,closed,,Remove unused DeviceDescription::runtime_version_string and driver_version_string,"Remove unused DeviceDescription::runtime_version_string and driver_version_string

These methods have been replaced by `DeviceDescription::{runtime_version|driver_version}` and are no longer needed.
",copybara-service[bot],2024-09-11 11:08:49+00:00,[],2024-09-12 08:36:58+00:00,2024-09-12 08:36:57+00:00,https://github.com/tensorflow/tensorflow/pull/75559,[],[],
2519350052,pull_request,open,,Integrate LLVM at llvm/llvm-project@d1cad2290c10,"Integrate LLVM at llvm/llvm-project@d1cad2290c10

Updates LLVM usage to match
[d1cad2290c10](https://github.com/llvm/llvm-project/commit/d1cad2290c10)
",copybara-service[bot],2024-09-11 10:31:13+00:00,[],2024-09-11 10:31:13+00:00,,https://github.com/tensorflow/tensorflow/pull/75557,[],[],
2519305343,pull_request,closed,,Add lookup table for Cutlass upcast gemm kernel selection.,"Add lookup table for Cutlass upcast gemm kernel selection.
",copybara-service[bot],2024-09-11 10:15:25+00:00,[],2024-09-11 12:35:52+00:00,2024-09-11 12:35:51+00:00,https://github.com/tensorflow/tensorflow/pull/75556,[],[],
2519140784,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 09:20:34+00:00,[],2024-09-18 17:02:26+00:00,,https://github.com/tensorflow/tensorflow/pull/75554,[],[],
2519044557,pull_request,closed,,Fix various tag assignments in XLA,"Fix various tag assignments in XLA

I'm trying to make the `gpu` and `no_rocm` tags consistent in the code base
in the sence that a not-`gpu`-tagged target doesn't depend on a `gpu` tagged
target unconditionally.

This change fixes the last remaining inconsistencies.
",copybara-service[bot],2024-09-11 08:48:04+00:00,[],2024-09-11 10:06:38+00:00,2024-09-11 10:06:37+00:00,https://github.com/tensorflow/tensorflow/pull/75553,[],[],
2519019136,pull_request,closed,,Remove if_cuda from bandwidth benchmark,"Remove if_cuda from bandwidth benchmark

This is CUDA-only code, so a `if_cuda` is not really needed. Tagging the targets as
`no_rocm` is the better alternative.
",copybara-service[bot],2024-09-11 08:39:34+00:00,[],2024-09-11 10:17:58+00:00,2024-09-11 10:17:57+00:00,https://github.com/tensorflow/tensorflow/pull/75552,[],[],
2518997404,pull_request,open,,Remove call to xnn_run_mmaximum/minimum from tflite kernel.,"Remove call to xnn_run_mmaximum/minimum from tflite kernel.

The performance gains are minimal and this is a potential source of OOB reads. 

If performance is necessary, then use the XNNPack delegate.
",copybara-service[bot],2024-09-11 08:32:04+00:00,['alankelly'],2024-09-11 08:32:05+00:00,,https://github.com/tensorflow/tensorflow/pull/75551,[],[],
2518771044,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 07:14:27+00:00,[],2024-09-11 08:10:03+00:00,2024-09-11 08:10:02+00:00,https://github.com/tensorflow/tensorflow/pull/75550,[],[],
2518669968,pull_request,closed,,"PR #17021: [NV] Use FP8 conversion intrinsics, when available","PR #17021: [NV] Use FP8 conversion intrinsics, when available

Imported from GitHub PR https://github.com/openxla/xla/pull/17021

The previous #16734 was rolled back. This PR addresses some comments from it.

PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in compute-bound FP8 kernels).

The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions.

Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version < 12.1, the ptxas will complain..

Reference:
https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt (see ""PTX ISA Notes"" and ""Target ISA Notes"").
Copybara import of the project:

--
c7d86e0e1ee5a7e1db6c0828009c992639b48af0 by Sergey Kozub <skozub@nvidia.com>:

[NV] Use FP8 conversion intrinsics, when available

The previous https://github.com/openxla/xla/pull/16734 was rolled back. This PR addresses some comments from #16734.

PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types.
This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in compute-bound FP8 kernels).

The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions.

Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version < 12.1, the ptxas will complain..

Reference:
https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt (see ""PTX ISA Notes"" and ""Target ISA Notes"").

Merging this change closes #17021

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17021 from openxla:skozub/f8-cvt-intrinsics-v2 c7d86e0e1ee5a7e1db6c0828009c992639b48af0
",copybara-service[bot],2024-09-11 06:35:21+00:00,[],2024-09-11 06:59:26+00:00,2024-09-11 06:59:25+00:00,https://github.com/tensorflow/tensorflow/pull/75549,[],[],
2518501629,pull_request,closed,,Integrate LLVM at llvm/llvm-project@d1cad2290c10,"Integrate LLVM at llvm/llvm-project@d1cad2290c10

Updates LLVM usage to match
[d1cad2290c10](https://github.com/llvm/llvm-project/commit/d1cad2290c10)
",copybara-service[bot],2024-09-11 05:16:04+00:00,[],2024-09-11 16:10:43+00:00,2024-09-11 16:10:42+00:00,https://github.com/tensorflow/tensorflow/pull/75548,[],[],
2518492124,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 05:10:56+00:00,[],2024-09-11 05:10:56+00:00,,https://github.com/tensorflow/tensorflow/pull/75547,[],[],
2518484388,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 05:06:36+00:00,[],2024-09-11 06:02:57+00:00,,https://github.com/tensorflow/tensorflow/pull/75546,[],[],
2518471831,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 04:59:41+00:00,[],2024-09-11 06:43:34+00:00,2024-09-11 06:43:33+00:00,https://github.com/tensorflow/tensorflow/pull/75545,[],[],
2518404884,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 04:03:23+00:00,[],2024-09-11 04:03:23+00:00,,https://github.com/tensorflow/tensorflow/pull/75544,[],[],
2518391520,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-11 03:48:10+00:00,[],2024-09-11 03:48:10+00:00,,https://github.com/tensorflow/tensorflow/pull/75543,[],[],
2518159952,pull_request,closed,,[XLA] Make async call names unique,"[XLA] Make async call names unique

We need the instruction name to go through the name uniquer in order for it to be correctly parsable when being cloned, as we'll attempt to increment the trailing suffix digit. Otherwise the cloned instruction will have the same name as its original version.
",copybara-service[bot],2024-09-10 23:36:21+00:00,[],2024-09-20 00:24:02+00:00,2024-09-20 00:24:01+00:00,https://github.com/tensorflow/tensorflow/pull/75542,[],[],
2518156284,pull_request,open,,Add a pass to remove redundant check (select) in the SPMD generated decomposed collective permute with cycle.,"Add a pass to remove redundant check (select) in the SPMD generated decomposed collective permute with cycle.
",copybara-service[bot],2024-09-10 23:32:50+00:00,[],2024-09-13 20:27:52+00:00,,https://github.com/tensorflow/tensorflow/pull/75541,[],[],
2518066258,pull_request,open,,Integrate LLVM at llvm/llvm-project@d1cad2290c10,"Integrate LLVM at llvm/llvm-project@d1cad2290c10

Updates LLVM usage to match
[d1cad2290c10](https://github.com/llvm/llvm-project/commit/d1cad2290c10)
",copybara-service[bot],2024-09-10 22:28:36+00:00,[],2024-09-11 00:32:30+00:00,,https://github.com/tensorflow/tensorflow/pull/75540,[],[],
2517976969,pull_request,closed,,"Some code cleanup and refactoring - remove a few uses of auto, use a smart pointer instead of manually managing a raw one, and added simplified some code.","Some code cleanup and refactoring - remove a few uses of auto, use a smart pointer instead of manually managing a raw one, and added simplified some code.
",copybara-service[bot],2024-09-10 22:02:29+00:00,[],2024-09-11 17:34:07+00:00,2024-09-11 17:34:07+00:00,https://github.com/tensorflow/tensorflow/pull/75539,[],[],
2517969804,pull_request,closed,,Fix test to pass on GPU,"Fix test to pass on GPU
",copybara-service[bot],2024-09-10 21:59:54+00:00,[],2024-09-10 23:21:39+00:00,2024-09-10 23:21:39+00:00,https://github.com/tensorflow/tensorflow/pull/75538,[],[],
2517951694,pull_request,closed,,Turn resharding reshapes on for dot/conv ops,"Turn resharding reshapes on for dot/conv ops

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16632 from shraiysh:degenerate_rs_dynamic_slice_fusion 0385dd42a9c325750b34a240a883a3b22718962f
",copybara-service[bot],2024-09-10 21:52:33+00:00,[],2024-09-11 19:00:28+00:00,2024-09-11 19:00:27+00:00,https://github.com/tensorflow/tensorflow/pull/75537,[],[],
2517946123,pull_request,closed,,Allow nullptr as input parameter in MatchShapeCoveringDynamicIndexInstruction to bypass check for input equality.,"Allow nullptr as input parameter in MatchShapeCoveringDynamicIndexInstruction to bypass check for input equality.

Reverts 19ea45a64e69c98ff9760be529065d1353babb91
",copybara-service[bot],2024-09-10 21:50:16+00:00,[],2024-09-11 03:22:56+00:00,2024-09-11 03:22:56+00:00,https://github.com/tensorflow/tensorflow/pull/75536,[],[],
2517839974,pull_request,closed,,Add default implementation for Finalize().,"Add default implementation for Finalize().
",copybara-service[bot],2024-09-10 21:12:44+00:00,[],2024-09-10 22:36:18+00:00,2024-09-10 22:36:17+00:00,https://github.com/tensorflow/tensorflow/pull/75535,[],[],
2517835760,pull_request,closed,,[tf.data] Fix broken sparse/ragged iterators on TPU devices.,"[tf.data] Fix broken sparse/ragged iterators on TPU devices.

In a recent change, we added a colocation constraint between iterator ops and the sparse/ragged decoding ops that transform their outputs into structured tensors. This broke some workloads that attempted to prefetch SparseTensor and RaggedTensor objects to TPU memory, because the relevant decoding kernels were not registered for `DEVICE_TPU`. This change adds the missing kernel registrations, using the same host-memory annotations that are present for `DEVICE_GPU` and preserving the previous behavior (when the decoding ops would fall back to running on some `DEVICE_CPU`... although not necessarily in the same process).
",copybara-service[bot],2024-09-10 21:11:21+00:00,[],2024-09-10 22:53:52+00:00,2024-09-10 22:53:52+00:00,https://github.com/tensorflow/tensorflow/pull/75534,[],[],
2517831140,pull_request,closed,,disable testLarge for float16 when numpy 2.0 is used,"disable testLarge for float16 when numpy 2.0 is used
",copybara-service[bot],2024-09-10 21:09:53+00:00,[],2024-09-10 22:21:31+00:00,2024-09-10 22:21:31+00:00,https://github.com/tensorflow/tensorflow/pull/75533,[],[],
2517827226,pull_request,open,,Test log capture,"Test log capture
",copybara-service[bot],2024-09-10 21:08:28+00:00,['ddunl'],2024-09-10 23:56:14+00:00,,https://github.com/tensorflow/tensorflow/pull/75532,[],[],
2517802391,pull_request,closed,,Cut by half the number of TensorShape.dims calls from TensorShape.is_compatible_with().,"Cut by half the number of TensorShape.dims calls from TensorShape.is_compatible_with().
Allows an earlier exit in cases where ranks do not match, skipping expensive as_dimension() calls from dims().
",copybara-service[bot],2024-09-10 20:59:55+00:00,[],2024-09-10 22:46:09+00:00,2024-09-10 22:46:08+00:00,https://github.com/tensorflow/tensorflow/pull/75531,[],[],
2517799516,pull_request,open,,PR #16696: Various macOS QOL enchancements,"PR #16696: Various macOS QOL enchancements

Imported from GitHub PR https://github.com/openxla/xla/pull/16696

This PR adds various small quality of life improvements to macOS builds:
- drop the `.so` suffix for PjRt plugin targets (`.dylib` on macOS)
- add compatibility with Apple Command Line Tools (no need for Xcode anymore)
- only export the `GetPjrtApi` symbol on macOS
- leverage bazel's `cc_binary.additional_linker_inputs` instead of using `deps`

It is probable the `.so` change my break some other builds, but I couldn't find any use in the XLA repo to patch ?
Copybara import of the project:

--
2d392b016730e8811ea72f90d9b5ee67126e61e6 by Steeve Morin <steeve.morin@gmail.com>:

[PjRt] Only export GetPjrtApi symbol on macOS

Also add missing macOS linkops, remove the .so suffix to the plugin targets
and add additional_linker_inputs for the linker script instead of deps.

--
9dbba3433bd88d3db95f1647782ba3bf9d3462cf by Steeve Morin <steeve.morin@gmail.com>:

Do not force DEVELOPER_DIR on macOS

It's is autodetected by Bazel and supports building using
only the Apple Command Line Tools.

--
336122e2fb0e3d7dd93b5a4a30f7551d8e1a21b6 by Steeve Morin <steeve.morin@gmail.com>:

Set the macosx deployment target via the bazel command line

Instead of via an action env.

Merging this change closes #16696

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16696 from zml:steeve/cc_shared_library 336122e2fb0e3d7dd93b5a4a30f7551d8e1a21b6
",copybara-service[bot],2024-09-10 20:58:51+00:00,[],2024-09-10 20:58:51+00:00,,https://github.com/tensorflow/tensorflow/pull/75530,[],[],
2517791518,pull_request,open,,[tflite-gpu] Add reverse to gpu_compatibility,"[tflite-gpu] Add reverse to gpu_compatibility
",copybara-service[bot],2024-09-10 20:56:01+00:00,['grantjensen'],2024-09-10 20:56:02+00:00,,https://github.com/tensorflow/tensorflow/pull/75529,[],[],
2517784553,pull_request,closed,,Do not fold BF16 conversions with asynchronous operations.,"Do not fold BF16 conversions with asynchronous operations.
",copybara-service[bot],2024-09-10 20:53:34+00:00,[],2024-09-18 17:32:21+00:00,2024-09-18 17:32:21+00:00,https://github.com/tensorflow/tensorflow/pull/75528,[],[],
2517679232,pull_request,closed,,[tflite-gpu] Fix model_builder_test,"[tflite-gpu] Fix model_builder_test
",copybara-service[bot],2024-09-10 20:18:26+00:00,['grantjensen'],2024-09-10 21:26:33+00:00,2024-09-10 21:26:33+00:00,https://github.com/tensorflow/tensorflow/pull/75527,[],[],
2517615278,pull_request,open,,Reverts b7684ffb6a32fc613729075ac224595e9aa4a677,"Reverts b7684ffb6a32fc613729075ac224595e9aa4a677

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16954 from shraiysh:fix_reduce_scatter_ds_fusion 3920334579c5a64f7de2adc97fb1ff3107ce9a04
",copybara-service[bot],2024-09-10 19:57:43+00:00,[],2024-09-10 19:57:43+00:00,,https://github.com/tensorflow/tensorflow/pull/75526,[],[],
2517586912,pull_request,open,,PR #17013: [GPU] Gracefully handle ptxas insufficient registers error.,"PR #17013: [GPU] Gracefully handle ptxas insufficient registers error.

Imported from GitHub PR https://github.com/openxla/xla/pull/17013


Copybara import of the project:

--
71bffb2713e0d88f069720187aa84d04246891d3 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Gracefully handle ptxas insufficient registers error.

This lets exhaustive GEMM fusion autotuning skip configs that do fail to
compile.

Merging this change closes #17013

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17013 from openxla:fix_exhaustive_autotuning 71bffb2713e0d88f069720187aa84d04246891d3
",copybara-service[bot],2024-09-10 19:48:52+00:00,[],2024-09-11 07:13:35+00:00,,https://github.com/tensorflow/tensorflow/pull/75525,[],[],
2517573505,pull_request,closed,,Integrate LLVM at llvm/llvm-project@7ba6768df818,"Integrate LLVM at llvm/llvm-project@7ba6768df818

Updates LLVM usage to match
[7ba6768df818](https://github.com/llvm/llvm-project/commit/7ba6768df818)
",copybara-service[bot],2024-09-10 19:44:43+00:00,[],2024-09-10 22:03:14+00:00,2024-09-10 22:03:13+00:00,https://github.com/tensorflow/tensorflow/pull/75524,[],[],
2517491108,pull_request,closed,,Introduce a config setting to selectively disable the hexagon delegate. ,"Introduce a config setting to selectively disable the hexagon delegate. 
Useful for building TfLite runtime tools without dependencies on hexagon(enabled by default). To disable hexagon dependencies, simply add `--define EXPLICIT_DISABLE_HEXAGON=1` or `--//tensorflow/lite/delegates/hexagon:explicit_disable_hexagon=true` to your build command.
",copybara-service[bot],2024-09-10 19:14:30+00:00,[],2024-09-10 21:49:14+00:00,2024-09-10 21:49:14+00:00,https://github.com/tensorflow/tensorflow/pull/75523,[],[],
2517464309,pull_request,closed,,"Make concrete classes for RocmExecutor and CudaExecutor, and a common virtual base class named GpuExecutor.","Make concrete classes for RocmExecutor and CudaExecutor, and a common virtual base class named GpuExecutor.
",copybara-service[bot],2024-09-10 19:08:36+00:00,[],2024-09-10 23:55:49+00:00,2024-09-10 23:55:49+00:00,https://github.com/tensorflow/tensorflow/pull/75522,[],[],
2517264340,pull_request,closed,,[internal change] Add saved_model_cli gensignature rule for BinFS.,"[internal change] Add saved_model_cli gensignature rule for BinFS.
",copybara-service[bot],2024-09-10 18:16:01+00:00,[],2024-09-10 18:43:22+00:00,2024-09-10 18:43:21+00:00,https://github.com/tensorflow/tensorflow/pull/75521,[],[],
2517224545,pull_request,closed,,NFC: Remove default constructor in HloInstructionAdaptor so that the `parent_` member is never null.,"NFC: Remove default constructor in HloInstructionAdaptor so that the `parent_` member is never null.

This requires some code changes in GroupDisjointReductions.
",copybara-service[bot],2024-09-10 18:07:40+00:00,['chsigg'],2024-09-12 05:54:48+00:00,2024-09-12 05:54:47+00:00,https://github.com/tensorflow/tensorflow/pull/75520,[],[],
2517191600,pull_request,open,,"PR #17021: [NV] Use FP8 conversion intrinsics, when available","PR #17021: [NV] Use FP8 conversion intrinsics, when available

Imported from GitHub PR https://github.com/openxla/xla/pull/17021

The previous #16734 was rolled back. This PR addresses some comments from it.

PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in compute-bound FP8 kernels).

The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions.

Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version < 12.1, the ptxas will complain..

Reference:
https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt (see ""PTX ISA Notes"" and ""Target ISA Notes"").
Copybara import of the project:

--
c7d86e0e1ee5a7e1db6c0828009c992639b48af0 by Sergey Kozub <skozub@nvidia.com>:

[NV] Use FP8 conversion intrinsics, when available

The previous https://github.com/openxla/xla/pull/16734 was rolled back. This PR addresses some comments from #16734.

PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types.
This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in compute-bound FP8 kernels).

The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions.

Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version < 12.1, the ptxas will complain..

Reference:
https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt (see ""PTX ISA Notes"" and ""Target ISA Notes"").

Merging this change closes #17021

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17021 from openxla:skozub/f8-cvt-intrinsics-v2 c7d86e0e1ee5a7e1db6c0828009c992639b48af0
",copybara-service[bot],2024-09-10 17:59:15+00:00,[],2024-09-11 07:14:26+00:00,,https://github.com/tensorflow/tensorflow/pull/75519,[],[],
2517191171,pull_request,closed,,#sdy Move shard map import pass to mhlo_round_trip.,"#sdy Move shard map import pass to mhlo_round_trip.

In a follow-up I will add a specific shard map import pass for sdy round tripping.
",copybara-service[bot],2024-09-10 17:59:06+00:00,[],2024-09-13 13:18:15+00:00,2024-09-13 13:18:15+00:00,https://github.com/tensorflow/tensorflow/pull/75518,[],[],
2517190579,pull_request,closed,,#sdy Add support for exporting nested ManualComputations in MHLO export.,"#sdy Add support for exporting nested ManualComputations in MHLO export.

Also adds some extra debug dumps during import/export.
",copybara-service[bot],2024-09-10 17:58:55+00:00,[],2024-09-13 11:55:17+00:00,2024-09-13 11:55:14+00:00,https://github.com/tensorflow/tensorflow/pull/75517,[],[],
2517164045,pull_request,closed,,Rollback empty conditional workaround due to test breakage.,"Rollback empty conditional workaround due to test breakage.

Reverts 0086a8ee9898b758ce753b216fd41113577dd776
",copybara-service[bot],2024-09-10 17:50:20+00:00,[],2024-09-10 19:53:25+00:00,2024-09-10 19:53:24+00:00,https://github.com/tensorflow/tensorflow/pull/75516,[],[],
2517131115,pull_request,closed,,Move `tsl/profiler/builds` to `xla/tsl/profiler/builds`,"Move `tsl/profiler/builds` to `xla/tsl/profiler/builds`
",copybara-service[bot],2024-09-10 17:40:49+00:00,['ddunl'],2024-09-11 20:54:34+00:00,2024-09-11 20:54:33+00:00,https://github.com/tensorflow/tensorflow/pull/75515,[],[],
2517080485,pull_request,closed,,count xla_gpu_enable_triton_gemm_int4 as xla_gpu_cublas_fallback=false,"count xla_gpu_enable_triton_gemm_int4 as xla_gpu_cublas_fallback=false

In the majority of the cases we want to use int4 params because we do not have enough memory. The cublass fallback leads to the full materialisation of the parameters and as a result may lead to the OOM. Let's count xla_gpu_enable_triton_gemm_int4=true flag the same way as xla_gpu_cublas_fallback=false.
",copybara-service[bot],2024-09-10 17:24:51+00:00,[],2024-09-10 18:27:18+00:00,2024-09-10 18:27:17+00:00,https://github.com/tensorflow/tensorflow/pull/75514,[],[],
2517040356,pull_request,closed,,Remove unsupported_*_linux configs from .bazelrc as planned,"Remove unsupported_*_linux configs from .bazelrc as planned

These configs are no longer needed as the old GCC toolchain is no longer supported.

If you still need these configs, add them to a user-specific bazelrc file for your project. Refer to the Bazel documentation https://bazel.build/run/bazelrc for more guidance.
",copybara-service[bot],2024-09-10 17:13:06+00:00,['kanglant'],2024-09-10 19:22:50+00:00,2024-09-10 19:22:49+00:00,https://github.com/tensorflow/tensorflow/pull/75513,[],[],
2517006516,pull_request,closed,,Check for `jax.Sharding`'s number of devices instead of `py_array.num_shards` which looks at IFRT sharding's num_devices to check against `global_devices` and deciding whether to fall back to python shard_arg.,"Check for `jax.Sharding`'s number of devices instead of `py_array.num_shards` which looks at IFRT sharding's num_devices to check against `global_devices` and deciding whether to fall back to python shard_arg.

This is because IFRT sharding's `num_shards` method is busted. It doesn't return the global shards (in some cases) which leads to JAX program unnecessarily falling back to python.
",copybara-service[bot],2024-09-10 17:00:49+00:00,['yashk2810'],2024-09-10 20:10:14+00:00,2024-09-10 20:10:13+00:00,https://github.com/tensorflow/tensorflow/pull/75512,[],[],
2516984415,pull_request,open,,Apply Conditional empty node graph workaround to only nested graphs.,"Apply Conditional empty node graph workaround to only nested graphs.
",copybara-service[bot],2024-09-10 16:52:59+00:00,[],2024-09-10 16:52:59+00:00,,https://github.com/tensorflow/tensorflow/pull/75511,[],[],
2516953024,pull_request,closed,,[XLA:GPU] Use Cost Model in SoftmaxRewriterTriton to estimate if fusion is beneficial.,"[XLA:GPU] Use Cost Model in SoftmaxRewriterTriton to estimate if fusion is beneficial.

Fusing normalization diamond is not always beneficial, especially for large rows. It is sometimes better to use reduction splitter and have multiple kernels with better occupancy.

To make the decision, we outline the normalization diamond, extract into a separate module, run a few passes and estimate the result with Cost Model. Then we compare the result with Tiled Cost Model to make the decision.
",copybara-service[bot],2024-09-10 16:39:57+00:00,[],2024-09-11 18:18:21+00:00,2024-09-11 18:18:21+00:00,https://github.com/tensorflow/tensorflow/pull/75510,[],[],
2516909961,pull_request,closed,,Bump the version of WEEK_12 to 1.1.0.,"Bump the version of WEEK_12 to 1.1.0.
",copybara-service[bot],2024-09-10 16:22:16+00:00,[],2024-09-11 23:01:15+00:00,2024-09-11 23:01:14+00:00,https://github.com/tensorflow/tensorflow/pull/75509,[],[],
2516823595,pull_request,closed,,Reverts 84cf3429b5cdbd98fc611c9ac0053b7d4d7d73c3,"Reverts 84cf3429b5cdbd98fc611c9ac0053b7d4d7d73c3
",copybara-service[bot],2024-09-10 15:44:19+00:00,[],2024-09-10 17:54:22+00:00,2024-09-10 17:54:21+00:00,https://github.com/tensorflow/tensorflow/pull/75508,[],[],
2516792414,pull_request,open,,Update visibility for an internal project.,"Update visibility for an internal project.
",copybara-service[bot],2024-09-10 15:30:52+00:00,[],2024-09-11 08:10:02+00:00,,https://github.com/tensorflow/tensorflow/pull/75507,[],[],
2516790380,pull_request,open,,Update visibility for an internal project.,"Update visibility for an internal project.
",copybara-service[bot],2024-09-10 15:29:59+00:00,[],2024-09-11 07:56:46+00:00,,https://github.com/tensorflow/tensorflow/pull/75506,[],[],
2516785059,pull_request,closed,,Update visibility for an internal project.,"Update visibility for an internal project.
",copybara-service[bot],2024-09-10 15:27:38+00:00,[],2024-09-13 15:36:03+00:00,2024-09-13 15:36:02+00:00,https://github.com/tensorflow/tensorflow/pull/75505,[],[],
2516756638,pull_request,open,,[XLA] Templatize minus one LiteralUtil method.,"[XLA] Templatize minus one LiteralUtil method.
",copybara-service[bot],2024-09-10 15:15:22+00:00,[],2024-09-10 16:36:32+00:00,,https://github.com/tensorflow/tensorflow/pull/75504,[],[],
2516728580,pull_request,open,,[xla][gpu] Order send/recv chains across decomposed collective permutes,"[xla][gpu] Order send/recv chains across decomposed collective permutes

Insert control dependencies between send and recv operations across decomposed collective permutes. This addresses a potential NCCL deadlock that may occur if every device tries to execute recv with no devices executing send.
",copybara-service[bot],2024-09-10 15:04:29+00:00,[],2024-09-18 11:28:25+00:00,,https://github.com/tensorflow/tensorflow/pull/75503,[],[],
2516609592,pull_request,closed,,[XLA:GPU][IndexAnalysis] Move the symbols->dims conversion logic to IndexingMap.,"[XLA:GPU][IndexAnalysis] Move the symbols->dims conversion logic to IndexingMap.
",copybara-service[bot],2024-09-10 14:16:27+00:00,['pifon2a'],2024-09-10 14:38:56+00:00,2024-09-10 14:38:55+00:00,https://github.com/tensorflow/tensorflow/pull/75502,[],[],
2516550927,pull_request,closed,,"Hook up the `FullyConnected` TFLite delegate to use `qp8` inputs with both signed and unsigned weights, when appropriate.","Hook up the `FullyConnected` TFLite delegate to use `qp8` inputs with both signed and unsigned weights, when appropriate.
",copybara-service[bot],2024-09-10 13:53:32+00:00,[],2024-11-06 11:37:50+00:00,2024-11-06 11:37:49+00:00,https://github.com/tensorflow/tensorflow/pull/75501,[],[],
2516541370,pull_request,closed,,[XLA:GPU] Remove `BlockLevelParameters` parameter to `CreateTritonIrAndFileCheck`.,"[XLA:GPU] Remove `BlockLevelParameters` parameter to `CreateTritonIrAndFileCheck`.

We now parse these parameters directly from the fusion instruction under test.
",copybara-service[bot],2024-09-10 13:49:46+00:00,[],2024-09-12 12:07:52+00:00,2024-09-12 12:07:51+00:00,https://github.com/tensorflow/tensorflow/pull/75500,[],[],
2516528613,pull_request,open,,Update ByteSwap functions for s390x,"After https://github.com/tensorflow/tensorflow/pull/72450, tensorflow build started to fail compilation for s390x(big endian) machines.
Seems like the big endian specific code (following `#if FLATBUFFERS_LITTLEENDIAN == 0`) was missed while implementing https://github.com/tensorflow/tensorflow/pull/72450.
Hence extending the changes to big endian code.",yasiribmcon,2024-09-10 13:44:49+00:00,['gbaned'],2025-01-16 06:20:00+00:00,,https://github.com/tensorflow/tensorflow/pull/75499,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('size:S', 'CL Change Size: Small')]","[{'comment_id': 2367635127, 'issue_id': 2516528613, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 23, 9, 10, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384865279, 'issue_id': 2516528613, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 1, 5, 57, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399564133, 'issue_id': 2516528613, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 8, 11, 17, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431108755, 'issue_id': 2516528613, 'author': 'keerthanakadiri', 'body': 'Hi @talumbau, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 23, 7, 11, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469827322, 'issue_id': 2516528613, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 11, 12, 7, 56, 7, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-09-23 09:10:25 UTC): Hi @talumbau, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 05:57:11 UTC): Hi @talumbau, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-08 11:17:14 UTC): Hi @talumbau, Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-23 07:11:59 UTC): Hi @talumbau, Can you please review this PR? Thank you !

keerthanakadiri on (2024-11-12 07:56:07 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

"
2516513523,pull_request,closed,,Prepare code for breaking change in Protobuf C++ API.,"Prepare code for breaking change in Protobuf C++ API.

Protobuf 6.30.0 will change the return types of `Descriptor::name()` and other methods to `absl::string_view`. This makes the code work both before and after such a change.

Reverts e4a35b40a8d3251fa871c1a3400b8596c2ee3b86
",copybara-service[bot],2024-09-10 13:39:06+00:00,[],2024-09-13 00:57:33+00:00,2024-09-13 00:57:32+00:00,https://github.com/tensorflow/tensorflow/pull/75498,[],[],
2516492150,pull_request,closed,,Introduce a dependency violation check,"Introduce a dependency violation check

This check ensure that general build targets don't unconditionally depend on more specific targets. For example:

1. General targets may not depend on GPU-only targets
2. General targets may not depend on CUDA-only targets
3. General targets may not depend on ROCm-only targets (Will follow when we introduce the `rocm-only` tag)
4. GPU-only targets may not depend on CUDA-only targets (For this test we will need a working GPU-but-not-CUDA build. A hermetic ROCm build would work for this)
5. GPU-only targets may not depend on ROCm-only targets (Will follow when we introduce the `rocm-only` tag

This is currently introducing the first two checks for GPU-only-targets (which are tagged `gpu`) and CUDA-only targets (which are tagged `no_rocm`). Renaming those tags and adding the check for ROCM-only targets will follow in a subsequent change.
",copybara-service[bot],2024-09-10 13:30:37+00:00,[],2024-09-12 08:27:53+00:00,2024-09-12 08:27:52+00:00,https://github.com/tensorflow/tensorflow/pull/75497,[],[],
2516434007,pull_request,open,,[XLA:GPU] Make `GetGoodTilings` only issue powers of 2 tile sizes to fit,"[XLA:GPU] Make `GetGoodTilings` only issue powers of 2 tile sizes to fit
Triton's restrictions.

Previously, we'd also allow capturing fully a dimension (i.e. leaving it
untiled). This is fine in most cases, but can cause us to generate incorrect
code for collapsing reshapes---since code generation will make the reshape
operate on a tile padded to a power of two. This is required to avoid this
discrepancy.

`GetGoodTilings` is renamed to `GetValidTilings` in this change (since it
actually doesn't actually check anything about the quality of the tiles),
and we take advantage of the opportunity to do some (slight) cleanups in
the formatting of `triton_fusion_emitter_device_test.cc` as we update the
provided tile sizes to match the new derivation.
",copybara-service[bot],2024-09-10 13:10:07+00:00,[],2024-09-10 13:41:06+00:00,,https://github.com/tensorflow/tensorflow/pull/75496,[],[],
2516396900,pull_request,closed,,[XLA:GPU][OCD] Match HloInstructionIndexing in indexing_analysis_test.cc,"[XLA:GPU][OCD] Match HloInstructionIndexing in indexing_analysis_test.cc
",copybara-service[bot],2024-09-10 12:57:05+00:00,['pifon2a'],2024-09-10 13:22:03+00:00,2024-09-10 13:22:03+00:00,https://github.com/tensorflow/tensorflow/pull/75495,[],[],
2516347735,pull_request,open,,Reverts f277540b4d6f8f2ee10f94c2e2fc8f47a8f14aaa,"Reverts f277540b4d6f8f2ee10f94c2e2fc8f47a8f14aaa
",copybara-service[bot],2024-09-10 12:36:37+00:00,['akuegel'],2024-09-10 12:36:38+00:00,,https://github.com/tensorflow/tensorflow/pull/75494,[],[],
2516285118,pull_request,closed,,NFC: Move GraphCycles and OrderedSet to xla namespace.,"NFC: Move GraphCycles and OrderedSet to xla namespace.
",copybara-service[bot],2024-09-10 12:17:44+00:00,['chsigg'],2024-09-10 16:04:59+00:00,2024-09-10 16:04:59+00:00,https://github.com/tensorflow/tensorflow/pull/75493,[],[],
2516268443,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-09-10 12:11:55+00:00,[],2024-09-12 08:56:09+00:00,2024-09-12 08:56:08+00:00,https://github.com/tensorflow/tensorflow/pull/75492,[],[],
2516261721,pull_request,closed,,NFC: Move UnionFind to xla namespace.,"NFC: Move UnionFind to xla namespace.
",copybara-service[bot],2024-09-10 12:09:02+00:00,['chsigg'],2024-09-10 13:42:23+00:00,2024-09-10 13:42:22+00:00,https://github.com/tensorflow/tensorflow/pull/75491,[],[],
