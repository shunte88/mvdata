id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2718675789,pull_request,closed,,[xla:collectives] NFC: Move communicator splitting to Collectives API and switch gpu locking to GpuCollectives,"[xla:collectives] NFC: Move communicator splitting to Collectives API and switch gpu locking to GpuCollectives
",copybara-service[bot],2024-12-04 20:00:29+00:00,['ezhulenev'],2024-12-05 05:51:40+00:00,2024-12-05 05:51:38+00:00,https://github.com/tensorflow/tensorflow/pull/82227,[],[],
2718673376,pull_request,closed,,Add function to CombineInferenceStatsResults,"Add function to CombineInferenceStatsResults
",copybara-service[bot],2024-12-04 19:59:01+00:00,['cliveverghese'],2024-12-05 00:59:07+00:00,2024-12-05 00:59:06+00:00,https://github.com/tensorflow/tensorflow/pull/82226,[],[],
2718628392,pull_request,closed,,"Fix a bug when graph output tensor is used by other Ops, it won't be treated as an output tensor due to loose isOutput() logic.","Fix a bug when graph output tensor is used by other Ops, it won't be treated as an output tensor due to loose isOutput() logic.
",copybara-service[bot],2024-12-04 19:33:18+00:00,[],2024-12-04 23:48:06+00:00,2024-12-04 23:48:05+00:00,https://github.com/tensorflow/tensorflow/pull/82225,[],[],
2718616080,pull_request,closed,,Add Logical_And Op legalization and test data.,"Add Logical_And Op legalization and test data.
",copybara-service[bot],2024-12-04 19:26:13+00:00,[],2024-12-04 20:41:52+00:00,2024-12-04 20:41:51+00:00,https://github.com/tensorflow/tensorflow/pull/82224,[],[],
2718610490,pull_request,closed,,Add Greater OP legalization and test data.,"Add Greater OP legalization and test data.

Reverts 1c27e02524eda1031728b0a0c00fbf3c9be93248
",copybara-service[bot],2024-12-04 19:22:59+00:00,[],2024-12-04 23:29:28+00:00,2024-12-04 23:29:27+00:00,https://github.com/tensorflow/tensorflow/pull/82223,[],[],
2718606907,pull_request,closed,,Add Less Op legalization and test data.,"Add Less Op legalization and test data.
",copybara-service[bot],2024-12-04 19:21:21+00:00,[],2024-12-04 22:25:38+00:00,2024-12-04 22:25:36+00:00,https://github.com/tensorflow/tensorflow/pull/82222,[],[],
2718590799,pull_request,closed,,[XLA] Fix a struct/class tag mismatch in cutlass code.,"[XLA] Fix a struct/class tag mismatch in cutlass code.

Change in preparation for setting -Werror=mismatched-tags in XLA.
",copybara-service[bot],2024-12-04 19:12:10+00:00,[],2024-12-04 21:51:34+00:00,2024-12-04 21:51:33+00:00,https://github.com/tensorflow/tensorflow/pull/82221,[],[],
2718573394,pull_request,open,,Integrate LLVM at llvm/llvm-project@026fbe519e16,"Integrate LLVM at llvm/llvm-project@026fbe519e16

Updates LLVM usage to match
[026fbe519e16](https://github.com/llvm/llvm-project/commit/026fbe519e16)
",copybara-service[bot],2024-12-04 19:03:23+00:00,[],2024-12-04 20:06:38+00:00,,https://github.com/tensorflow/tensorflow/pull/82220,[],[],
2718569053,pull_request,closed,,[xla:collectives] NFC: Move CommInitRanks to Collectives API,"[xla:collectives] NFC: Move CommInitRanks to Collectives API
",copybara-service[bot],2024-12-04 19:01:40+00:00,['ezhulenev'],2024-12-05 04:35:34+00:00,2024-12-05 04:35:33+00:00,https://github.com/tensorflow/tensorflow/pull/82219,[],[],
2718530743,pull_request,closed,,Move splatted registration to tf lite directory.,"Move splatted registration to tf lite directory.
",copybara-service[bot],2024-12-04 18:40:41+00:00,['rocketas'],2024-12-04 20:49:32+00:00,2024-12-04 20:49:31+00:00,https://github.com/tensorflow/tensorflow/pull/82218,[],[],
2718491796,pull_request,closed,,[XLA:GPU] Add a test for RaggedAllToAll that runs on 8 GPUs.,"[XLA:GPU] Add a test for RaggedAllToAll that runs on 8 GPUs.

Writing correct input and expected data by hand is hard. We can't rely on default XLA random generator for input data, because all parameters (ragged data, sizes and slices) are dependent.

Writing a test manually would require a lot of constant that are hard to read, change and debug. I decided to write a helper `CreateRandomTestData` that takes `input_sizes` as a parameter and computes all necessary sizes and strides, fills input with random data and calculates expected output. This way we can write concise tests that are also testing correctness.
",copybara-service[bot],2024-12-04 18:22:46+00:00,[],2024-12-06 11:41:12+00:00,2024-12-06 11:41:11+00:00,https://github.com/tensorflow/tensorflow/pull/82217,[],[],
2718335967,pull_request,closed,,[xla:collectives] NFC: Move stubs from NcclApiStub to GpuCollectivesStub,"[xla:collectives] NFC: Move stubs from NcclApiStub to GpuCollectivesStub
",copybara-service[bot],2024-12-04 17:05:45+00:00,['ezhulenev'],2024-12-05 00:46:52+00:00,2024-12-05 00:46:52+00:00,https://github.com/tensorflow/tensorflow/pull/82216,[],[],
2718209858,pull_request,closed,,Update the TODO bug number in xla_triton_sparse_passes.cc.,"Update the TODO bug number in xla_triton_sparse_passes.cc.
",copybara-service[bot],2024-12-04 16:08:54+00:00,[],2024-12-05 10:54:34+00:00,2024-12-05 10:54:33+00:00,https://github.com/tensorflow/tensorflow/pull/82215,[],[],
2718207191,pull_request,closed,,Remove more unused hlo passes.,"Remove more unused hlo passes.
",copybara-service[bot],2024-12-04 16:07:43+00:00,['chsigg'],2024-12-04 19:45:59+00:00,2024-12-04 19:45:58+00:00,https://github.com/tensorflow/tensorflow/pull/82213,[],[],
2718164606,pull_request,closed,,Reverts f3b52fda85288c9766e4f9724634c0b3fe2db4f1,"Reverts f3b52fda85288c9766e4f9724634c0b3fe2db4f1
",copybara-service[bot],2024-12-04 15:51:21+00:00,[],2024-12-04 16:20:39+00:00,2024-12-04 16:20:38+00:00,https://github.com/tensorflow/tensorflow/pull/82212,[],[],
2718163640,pull_request,closed,,Fix CheckAndCanonicalizeMemoryKind when used with non-addressable device list.,"Fix CheckAndCanonicalizeMemoryKind when used with non-addressable device list.

In JAX you can reproduce this issue in a multi-controller setup with the
following code:

```
>>> d = next(d for d in jax.devices() if d not in jax.local_devices())
>>> jax.sharding.SingleDeviceSharding(d, memory_kind='device')
IndexError:
```
",copybara-service[bot],2024-12-04 15:50:54+00:00,['tomhennigan'],2024-12-04 16:35:54+00:00,2024-12-04 16:35:53+00:00,https://github.com/tensorflow/tensorflow/pull/82211,[],[],
2718059314,pull_request,closed,,Align num_threads parameter validation with documentation,"Hi, Team
Modify the TensorFlow Lite Interpreter's `num_threads` parameter validation to match its documented behavior specifically allowing `-1` for implementation-defined threading and treating `0` as disabling multithreading.
#### Current Behavior
- Current implementation raises a `ValueError` if `num_threads` is less than 1
- Contradicts the documented behavior in TensorFlow Lite's documentation here is tf.lite.Interpreter [documentation](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter)
#### Proposed Changes
```
if num_threads is not None:
    if not isinstance(num_threads, int):
        raise ValueError('type of num_threads should be int')
    if num_threads < -1:
        raise ValueError('num_threads should be >= -1')
    if num_threads == 0:
        num_threads = 1
    elif num_threads == -1:
        pass
    self._interpreter.SetNumThreads(num_threads)
```
It will fix this issue : https://github.com/tensorflow/tensorflow/issues/80579

I appreciate your thorough review and collaborative input with any feedback or suggestion. Thank you",gaikwadrahul8,2024-12-04 15:12:37+00:00,['gbaned'],2025-01-29 23:39:32+00:00,2025-01-29 23:39:31+00:00,https://github.com/tensorflow/tensorflow/pull/82210,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('ready to pull', 'PR ready for merge process'), ('size:S', 'CL Change Size: Small')]","[{'comment_id': 2562208379, 'issue_id': 2718059314, 'author': 'keerthanakadiri', 'body': 'Hi @chunnienc  ,Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 26, 6, 28, 33, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-12-26 06:28:33 UTC): Hi @chunnienc  ,Can you please review this PR? Thank you !

"
2717991201,pull_request,closed,,Enable XLA:TPU client to take in parameters,"Enable XLA:TPU client to take in parameters
",copybara-service[bot],2024-12-04 14:49:36+00:00,['changm'],2024-12-16 23:12:13+00:00,2024-12-16 23:12:12+00:00,https://github.com/tensorflow/tensorflow/pull/82207,[],[],
2717943853,pull_request,open,,Make Tensorflow runtime handle errors returned by CudaExecutor::CreateDeviceDescription,"Make Tensorflow runtime handle errors returned by CudaExecutor::CreateDeviceDescription

Tensorflow is structured in a way that `GpuDevice::ListPhysicalDevices` is called even when the user installed the CPU-only Python package. `GpuDevice::ListPhysicalDevices` calls `CudaPlatform::VisibleDeviceCount` to determine the number of GPUs available. Then for each GPU `CudaExecutor::CreateDeviceDescription` is called.

The problem is that when the CUDA driver is available `VisibleDeviceCount` might return a number greater than zero but everything else - including `CreateDeviceDescription` will fail if the CUDA libraries are not available.

With a recent XLA change, `CreateDeviceDescription` fails if the CUDA runtime is not available and TF will refuse to load - even in CPU-only mode.

So with this change TF will only log a warning if `CreateDeviceDescription` fails and not abort entirely.
",copybara-service[bot],2024-12-04 14:30:59+00:00,[],2024-12-10 16:41:25+00:00,,https://github.com/tensorflow/tensorflow/pull/82206,[],[],
2717923714,pull_request,open,,Integrate LLVM at llvm/llvm-project@026fbe519e16,"Integrate LLVM at llvm/llvm-project@026fbe519e16

Updates LLVM usage to match
[026fbe519e16](https://github.com/llvm/llvm-project/commit/026fbe519e16)
",copybara-service[bot],2024-12-04 14:22:50+00:00,[],2024-12-04 16:28:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82205,[],[],
2717897351,pull_request,closed,,#sdy Fix MHLO<->HLO translation bug with multi result host offload functions.,"#sdy Fix MHLO<->HLO translation bug with multi result host offload functions.

We don't copy over the backend config from HLO->MHLO and don't correctly create `get-tuple-element` instructions with the right shardings.
",copybara-service[bot],2024-12-04 14:14:59+00:00,[],2024-12-06 17:53:16+00:00,2024-12-06 17:53:15+00:00,https://github.com/tensorflow/tensorflow/pull/82204,[],[],
2717838760,pull_request,closed,,[XLA:GPU][Emitters] Use DeviceDescription in lower_to_llvm.cc.,"[XLA:GPU][Emitters] Use DeviceDescription in lower_to_llvm.cc.

Also copy DeviceDescription in lower_to_llvm and lower_tensors passes instead of serializing/deserializing GpuDeviceInfoProto.
",copybara-service[bot],2024-12-04 13:56:31+00:00,['pifon2a'],2024-12-04 14:38:26+00:00,2024-12-04 14:38:25+00:00,https://github.com/tensorflow/tensorflow/pull/82203,[],[],
2717772836,pull_request,closed,,This cherry-picks https://github.com/triton-lang/triton/commit/7f063387991744e2d6d3fbc1e9485ccfaa70f053 to land fixes for mixed-precision dots. This unblocks removing restriction for s8xf32 dots being disabled on Tensor Cores.,"This cherry-picks https://github.com/triton-lang/triton/commit/7f063387991744e2d6d3fbc1e9485ccfaa70f053 to land fixes for mixed-precision dots. This unblocks removing restriction for s8xf32 dots being disabled on Tensor Cores.
",copybara-service[bot],2024-12-04 13:38:00+00:00,[],2024-12-04 17:12:35+00:00,2024-12-04 17:12:34+00:00,https://github.com/tensorflow/tensorflow/pull/82202,[],[],
2717683342,pull_request,closed,,Deduplicate getValuesFromDotOperandLayoutStruct function,"Deduplicate getValuesFromDotOperandLayoutStruct function
",copybara-service[bot],2024-12-04 13:04:54+00:00,[],2024-12-04 15:13:24+00:00,2024-12-04 15:13:22+00:00,https://github.com/tensorflow/tensorflow/pull/82201,[],[],
2717646034,pull_request,open,,More thorough propagation of host linear layout. Currently linear layout on host,"More thorough propagation of host linear layout. Currently linear layout on host
can only originate from entry computation. Propagation only goes strickly down/up.
More needs to be done later if such layout can original from host compute itself.
Removed the temporary pattern match solution.
",copybara-service[bot],2024-12-04 12:49:33+00:00,[],2024-12-05 05:31:22+00:00,,https://github.com/tensorflow/tensorflow/pull/82200,[],[],
2717634176,pull_request,closed,,Add a tuple sharding when creating get-tuple-element(tuple(single_result)).,"Add a tuple sharding when creating get-tuple-element(tuple(single_result)).
",copybara-service[bot],2024-12-04 12:45:00+00:00,[],2024-12-13 00:46:12+00:00,2024-12-13 00:46:11+00:00,https://github.com/tensorflow/tensorflow/pull/82199,[],[],
2717456231,pull_request,closed,,Integrate LLVM at llvm/llvm-project@9c9d4b9e73c1,"Integrate LLVM at llvm/llvm-project@9c9d4b9e73c1

Updates LLVM usage to match
[9c9d4b9e73c1](https://github.com/llvm/llvm-project/commit/9c9d4b9e73c1)
",copybara-service[bot],2024-12-04 11:36:52+00:00,[],2024-12-04 13:05:26+00:00,2024-12-04 13:05:23+00:00,https://github.com/tensorflow/tensorflow/pull/82198,[],[],
2717408598,pull_request,open,,Regenerate stubs with Mypy 1.13.0,"Regenerate stubs with Mypy 1.13.0
",copybara-service[bot],2024-12-04 11:22:35+00:00,[],2024-12-04 13:21:12+00:00,,https://github.com/tensorflow/tensorflow/pull/82197,[],[],
2717383749,pull_request,closed,,Create snapshot for HloUnoptimized module.,"Create snapshot for HloUnoptimized module.
HloUnoptimizedSnapshot stores:
1) hlo module: unoptimized hlo graph
2) partitions: input data for unoptimized hlo graph for one shard
",copybara-service[bot],2024-12-04 11:12:22+00:00,[],2024-12-04 12:42:42+00:00,2024-12-04 12:42:41+00:00,https://github.com/tensorflow/tensorflow/pull/82196,[],[],
2717373188,pull_request,closed,,Introduce xla_gpu flag for Dumping HloUnoptimizedSnapshots,"Introduce xla_gpu flag for Dumping HloUnoptimizedSnapshots

Adds a new boolean `xla_dump_hlo_unoptimized_snapshots` to the `DebugOptions` protobuf. When enabled, we'll dump an `HloUnoptimizedSnapshot` for each execution of an HLO module. This option only affects GPU targets for now.
",copybara-service[bot],2024-12-04 11:08:23+00:00,[],2024-12-05 08:47:39+00:00,2024-12-05 08:47:38+00:00,https://github.com/tensorflow/tensorflow/pull/82194,[],[],
2717348898,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20084 from yliu120:add_trace 6542539445d032f27319cf7bfd3edfe610afe748
",copybara-service[bot],2024-12-04 10:59:55+00:00,[],2024-12-04 10:59:55+00:00,,https://github.com/tensorflow/tensorflow/pull/82193,[],[],
2717268664,pull_request,closed,,Fix broken link in video classification overview.md,"Hi, Team
I found 01 broken documentation link for [TensorFlow Lite Support Library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support) hyperlink in this [overview.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/video_classification/overview.md#get-started) file so I have updated that link to functional link. Please review and merge this change as appropriate.

Thank you for your consideration.",gaikwadrahul8,2024-12-04 10:32:03+00:00,['gbaned'],2024-12-26 18:18:06+00:00,2024-12-26 18:18:05+00:00,https://github.com/tensorflow/tensorflow/pull/82192,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2562214482, 'issue_id': 2717268664, 'author': 'keerthanakadiri', 'body': 'Hi @chunnienc, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 26, 6, 38, 9, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-12-26 06:38:09 UTC): Hi @chunnienc, Can you please review this PR? Thank you !

"
2717201464,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 10:08:41+00:00,[],2024-12-04 10:08:41+00:00,,https://github.com/tensorflow/tensorflow/pull/82191,[],[],
2717195834,pull_request,open,,PR #20006: [XLA:GPU] Only allow horizontal loop fusion for default memory space,"PR #20006: [XLA:GPU] Only allow horizontal loop fusion for default memory space

Imported from GitHub PR https://github.com/openxla/xla/pull/20006

Horizontal loop fusion currently breaks weight offloading in JAX if it fuses a host-memory copy and device-memory copy because such a fusion results in a same-space buffer, triggering memory space assertions in JAX.

This PR avoids any horizontal loop fusions for host-memory (even though in practice, some fusions would work even in host space).
Copybara import of the project:

--
6a3b325aae43227b847e1124a85236ab89e6d7e2 by Jaroslav Sevcik <jsevcik@nvidia.com>:

[XLA:GPU] Only allow horizontal loop fusion for default memory space

Merging this change closes #20006

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20006 from jaro-sevcik:jsevcik/horizontal-loop-fusion-default-memory-space-only 6a3b325aae43227b847e1124a85236ab89e6d7e2
",copybara-service[bot],2024-12-04 10:06:55+00:00,[],2024-12-11 07:36:45+00:00,,https://github.com/tensorflow/tensorflow/pull/82190,[],[],
2717184540,pull_request,closed,,PR #20084: Adds a TraceMe in the blocking while loop.,"PR #20084: Adds a TraceMe in the blocking while loop.

Imported from GitHub PR https://github.com/openxla/xla/pull/20084

Adds a TraceMe in the blocking while loop in WhileThunk so that we know where the blocking memcpyD2H comes from.
Copybara import of the project:

--
e50e3e440c812e981be0aad3b0efadf098d087d4 by Yunlong Liu <yunlongl@x.ai>:

add a traceme in blocking while

--
6542539445d032f27319cf7bfd3edfe610afe748 by Yunlong Liu <yunlongl@x.ai>:

adds the lib dependency in TraceMe

Merging this change closes #20084

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20084 from yliu120:add_trace 6542539445d032f27319cf7bfd3edfe610afe748
",copybara-service[bot],2024-12-04 10:02:26+00:00,[],2024-12-04 10:53:51+00:00,2024-12-04 10:53:51+00:00,https://github.com/tensorflow/tensorflow/pull/82189,[],[],
2717173820,pull_request,closed,,Fix 04 broken links in text classification overview.md,"Hi, Team
I found 04 broken documentation links in this [overview.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/text_classification/overview.md) file so I have updated those links to functional links. Please review and merge this change as appropriate.

Thank you for your consideration.",gaikwadrahul8,2024-12-04 09:58:21+00:00,['gbaned'],2024-12-26 18:27:03+00:00,2024-12-26 18:27:02+00:00,https://github.com/tensorflow/tensorflow/pull/82188,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2562215905, 'issue_id': 2717173820, 'author': 'keerthanakadiri', 'body': 'Hi @chunnienc , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 26, 6, 40, 11, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-12-26 06:40:11 UTC): Hi @chunnienc , Can you please review this PR? Thank you !

"
2717172713,pull_request,open,,[XLA:GPU] Enable native BF16 arithmetic ops on Ampere GPUs.,"[XLA:GPU] Enable native BF16 arithmetic ops on Ampere GPUs.

Ampere also supports bf16 operations
",copybara-service[bot],2024-12-04 09:57:53+00:00,[],2024-12-04 10:49:56+00:00,,https://github.com/tensorflow/tensorflow/pull/82187,[],[],
2717166363,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:55:10+00:00,[],2024-12-04 09:55:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82186,[],[],
2717156028,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:52:10+00:00,[],2024-12-04 09:52:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82185,[],[],
2717151394,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:50:45+00:00,[],2024-12-04 09:50:45+00:00,,https://github.com/tensorflow/tensorflow/pull/82184,[],[],
2717146912,pull_request,closed,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20084 from yliu120:add_trace 6542539445d032f27319cf7bfd3edfe610afe748
",copybara-service[bot],2024-12-04 09:49:15+00:00,[],2024-12-04 11:31:45+00:00,2024-12-04 11:31:44+00:00,https://github.com/tensorflow/tensorflow/pull/82183,[],[],
2717144753,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:48:34+00:00,[],2024-12-04 09:48:34+00:00,,https://github.com/tensorflow/tensorflow/pull/82182,[],[],
2717138636,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:46:20+00:00,[],2024-12-04 09:46:20+00:00,,https://github.com/tensorflow/tensorflow/pull/82181,[],[],
2717137456,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:45:55+00:00,[],2024-12-04 09:45:55+00:00,,https://github.com/tensorflow/tensorflow/pull/82180,[],[],
2717124993,pull_request,closed,,[XLA:GPU] Drop unnecessary bitcast from the chain convert(s4)->bitcast->dot operation,"[XLA:GPU] Drop unnecessary bitcast from the chain convert(s4)->bitcast->dot operation

The fix is covering the problem with lowering convert(s4)->dot. S4 argument should be unpacked. The unpack logic cannot handle the case when we also have the bitcast in the middle.
",copybara-service[bot],2024-12-04 09:41:14+00:00,[],2024-12-06 16:05:33+00:00,2024-12-06 16:05:33+00:00,https://github.com/tensorflow/tensorflow/pull/82179,[],[],
2717123071,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:40:25+00:00,[],2024-12-04 09:40:25+00:00,,https://github.com/tensorflow/tensorflow/pull/82178,[],[],
2717120154,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:39:12+00:00,[],2024-12-04 11:17:38+00:00,2024-12-04 11:17:37+00:00,https://github.com/tensorflow/tensorflow/pull/82177,[],[],
2717111039,pull_request,closed,,PR #20028: [GPU][NFC] Cleanup horizontal loop fusion.,"PR #20028: [GPU][NFC] Cleanup horizontal loop fusion.

Imported from GitHub PR https://github.com/openxla/xla/pull/20028


Copybara import of the project:

--
b4af0bc14794b381a8c21dfe9e3d5b5f6e7ae6f7 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Cleanup horizontal loop fusion.

Merging this change closes #20028

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20028 from openxla:cleanup b4af0bc14794b381a8c21dfe9e3d5b5f6e7ae6f7
",copybara-service[bot],2024-12-04 09:35:23+00:00,[],2024-12-04 10:12:20+00:00,2024-12-04 10:12:20+00:00,https://github.com/tensorflow/tensorflow/pull/82173,[],[],
2717108946,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20028 from openxla:cleanup b4af0bc14794b381a8c21dfe9e3d5b5f6e7ae6f7
",copybara-service[bot],2024-12-04 09:34:25+00:00,[],2024-12-04 10:17:01+00:00,,https://github.com/tensorflow/tensorflow/pull/82171,[],[],
2717107369,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 09:33:44+00:00,[],2024-12-04 09:33:44+00:00,,https://github.com/tensorflow/tensorflow/pull/82169,[],[],
2717034119,pull_request,closed,,Integrate LLVM at llvm/llvm-project@109e4a147faa,"Integrate LLVM at llvm/llvm-project@109e4a147faa

Updates LLVM usage to match
[109e4a147faa](https://github.com/llvm/llvm-project/commit/109e4a147faa)
",copybara-service[bot],2024-12-04 09:05:22+00:00,[],2024-12-04 10:43:44+00:00,2024-12-04 10:43:44+00:00,https://github.com/tensorflow/tensorflow/pull/82151,[],[],
2716894001,pull_request,closed,,Integrate CompilationProvider framework into NVPTXCompiler,"Integrate CompilationProvider framework into NVPTXCompiler

Removes all the logic that deals with PTX from NVPTXCompiler and instead
uses the `CompilationProvider` classes for PTX compilation.

This has the advantage that it
- Removes a bunch of untested code from nvptx_compiler.cc
- Fixes bugs in the caching logic
- Makes it possible to reuse the same PTX compilation pipeline for Triton Kernels in JAX
- Makes it possible to reuse the same PTX compilation pipeline in the Runtime where we currently only support driver compilation.
",copybara-service[bot],2024-12-04 08:18:06+00:00,[],2024-12-04 14:53:35+00:00,2024-12-04 14:53:35+00:00,https://github.com/tensorflow/tensorflow/pull/82150,[],[],
2716876474,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 08:09:29+00:00,[],2024-12-04 08:09:29+00:00,,https://github.com/tensorflow/tensorflow/pull/82149,[],[],
2716874423,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 08:08:21+00:00,[],2024-12-04 08:08:21+00:00,,https://github.com/tensorflow/tensorflow/pull/82148,[],[],
2716823131,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 07:49:10+00:00,[],2024-12-04 07:49:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82147,[],[],
2716711690,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 06:47:20+00:00,[],2024-12-04 06:47:20+00:00,,https://github.com/tensorflow/tensorflow/pull/82145,[],[],
2716682544,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 06:29:23+00:00,[],2024-12-04 06:29:23+00:00,,https://github.com/tensorflow/tensorflow/pull/82144,[],[],
2716642191,pull_request,closed,,[xla:collectives] NFC: Move NCCL clique locking to collectives component,"[xla:collectives] NFC: Move NCCL clique locking to collectives component

Replace all mentions of NCCL with a generic GpuCollectives.
",copybara-service[bot],2024-12-04 06:00:49+00:00,['ezhulenev'],2024-12-05 00:25:08+00:00,2024-12-05 00:25:05+00:00,https://github.com/tensorflow/tensorflow/pull/82143,[],[],
2716605553,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-04 05:34:06+00:00,[],2024-12-04 05:34:06+00:00,,https://github.com/tensorflow/tensorflow/pull/82142,[],[],
2716579259,pull_request,closed,,Add ppc64le support and improve linker version handling,"**Changes in this PR:**
1. **`configure.py`:**
   - Enhanced linker version detection to support varied output formats.
   - Added fallback logic for version parsing.
2. **`tensorflow/tools/pip_package/BUILD`:**
   - Added support for `ppc64le` architecture in the build configuration.

**Testing:**
- Successfully validated changes locally using Bazel build and tests.
- No regressions detected in the existing functionality.

**Notes:**
- These changes enable better support for `ppc64le` architecture and improve compatibility with varied environments.",sandeepgupta12,2024-12-04 05:11:28+00:00,['gbaned'],2024-12-05 09:59:16+00:00,2024-12-05 09:59:16+00:00,https://github.com/tensorflow/tensorflow/pull/82141,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:S', 'CL Change Size: Small')]",[],
2716395864,pull_request,closed,,Add model_runtime_info_pb2 to lite.py,"Add model_runtime_info_pb2 to lite.py
",copybara-service[bot],2024-12-04 02:16:10+00:00,[],2024-12-05 02:03:28+00:00,2024-12-05 02:03:27+00:00,https://github.com/tensorflow/tensorflow/pull/82138,[],[],
2716379206,pull_request,open,,Support int4 in most ops on CPUs/GPUs.,"Support int4 in most ops on CPUs/GPUs.

Now most ops support int4. FloatSupport is used to upcast most int4 ops to int8, since codegen does not natively support int4 in certain cases and GPUs do not natively support int4 math regardless.

The main challenge here is that the element_size_in_bits layout field must be set to 4 for all int4 arrays. This Layout field exists since on the host, int4 arrays are unpacked while on the device, int4 arrays are packed packed. element_size_in_bits specifies whether the array is packed, and should always be set for device tensors.

Many passes create new shapes for new instructions without setting element_size_in_bits, e.g. with ShapeUtil::MakeShape. This is problematic as it can cause int4 arrays to be created without element_size_in_bits set. This becomes especially problematic when int4 is supported in more ops, as it presents more opportunities for passes to create shapes without the element_size_in_bits set. For each known case where this is done, we address this is one of two ways:

1. We call SubByteNormalization after the pass runs, which properly sets element_size_in_bits on all instructions
2. We call a special method UpdateLayout() on the shape, which sets element_size_in_bits on it. Many passes already call UpdateLayout() before this change, and this change adds setting element_size_in_bits to UpdateLayout.
",copybara-service[bot],2024-12-04 02:05:18+00:00,['reedwm'],2024-12-04 02:05:19+00:00,,https://github.com/tensorflow/tensorflow/pull/82137,[],[],
2716363820,pull_request,closed,,Integrate LLVM at llvm/llvm-project@a201ba1b57aa,"Integrate LLVM at llvm/llvm-project@a201ba1b57aa

Updates LLVM usage to match
[a201ba1b57aa](https://github.com/llvm/llvm-project/commit/a201ba1b57aa)
",copybara-service[bot],2024-12-04 01:49:51+00:00,[],2024-12-04 03:53:14+00:00,2024-12-04 03:53:14+00:00,https://github.com/tensorflow/tensorflow/pull/82136,[],[],
2716362662,pull_request,open,,Add actions runner logs.,"Add actions runner logs.
",copybara-service[bot],2024-12-04 01:48:37+00:00,['quoctruong'],2024-12-04 01:48:38+00:00,,https://github.com/tensorflow/tensorflow/pull/82135,[],[],
2716342166,pull_request,closed,,Reshard indices when partitioning gather/scatter along index pass-through dims if the gather output / scatter update has better sharding.,"Reshard indices when partitioning gather/scatter along index pass-through dims if the gather output / scatter update has better sharding.

We can reshard gather indices at first and then reshard the gather output such that their shardings are the same along the index pass-through dims. Before this change, we only reshard the gather output.

See the added examples for more details.
",copybara-service[bot],2024-12-04 01:27:58+00:00,[],2024-12-04 17:23:35+00:00,2024-12-04 17:23:34+00:00,https://github.com/tensorflow/tensorflow/pull/82133,[],[],
2716323123,pull_request,closed,,Set the XlaCallModule's StableHLO payload version during deserialization so that it can be used in re-serialization.,"Set the XlaCallModule's StableHLO payload version during deserialization so that it can be used in re-serialization.
",copybara-service[bot],2024-12-04 01:10:11+00:00,['GleasonK'],2024-12-05 00:11:13+00:00,2024-12-05 00:11:08+00:00,https://github.com/tensorflow/tensorflow/pull/82132,[],[],
2716315625,pull_request,closed,,[StableHLO][API] Add API to get StableHLO version from PortableArtifact,"[StableHLO][API] Add API to get StableHLO version from PortableArtifact
",copybara-service[bot],2024-12-04 01:03:58+00:00,['GleasonK'],2024-12-04 21:22:19+00:00,2024-12-04 21:22:18+00:00,https://github.com/tensorflow/tensorflow/pull/82131,[],[],
2716291869,pull_request,closed,,"[hlo-opt] Register gpu passes and add ""--list-passes"" option","[hlo-opt] Register gpu passes and add ""--list-passes"" option
",copybara-service[bot],2024-12-04 00:41:21+00:00,[],2024-12-05 21:44:47+00:00,2024-12-05 21:44:46+00:00,https://github.com/tensorflow/tensorflow/pull/82130,[],[],
2716288616,pull_request,closed,,[IFRT] Add `custom_options` in `ifrt::ExecuteOptions`,"[IFRT] Add `custom_options` in `ifrt::ExecuteOptions`

The users can provide any runtime-specific execution options at
`ifrt::LoadedExecutable::Execute()` using
`ifrt::ExecuteOptions::custom_options`. These options are intended to convey
small per-execution metadata that are to be used by implementation internals
with little to (ideally) no interaction with IFRT API semantics. Example use
cases are propagating a profiling key, which currently goes out of scope of
IFRT APIs.
",copybara-service[bot],2024-12-04 00:38:23+00:00,[],2024-12-04 03:17:18+00:00,2024-12-04 03:17:18+00:00,https://github.com/tensorflow/tensorflow/pull/82129,[],[],
2716280246,pull_request,closed,,Add a new field duty_cycle to op_metrics,"Add a new field duty_cycle to op_metrics
",copybara-service[bot],2024-12-04 00:29:47+00:00,[],2024-12-12 02:48:21+00:00,2024-12-12 02:48:21+00:00,https://github.com/tensorflow/tensorflow/pull/82128,[],[],
2716276284,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2024-12-04 00:25:55+00:00,[],2024-12-05 19:42:34+00:00,2024-12-05 19:42:32+00:00,https://github.com/tensorflow/tensorflow/pull/82127,[],[],
2716252870,pull_request,open,,[xla:collectives] NFC: Extract NcclCommunicators into GpuClique and NcclCliqueImpl,"[xla:collectives] NFC: Extract NcclCommunicators into GpuClique and NcclCliqueImpl
",copybara-service[bot],2024-12-04 00:04:11+00:00,['majiddadashi'],2024-12-04 00:21:22+00:00,,https://github.com/tensorflow/tensorflow/pull/82126,[],[],
2716245282,pull_request,open,,Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.,"Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.

The name `NewHloTestBase` is potentially confusing beacuse it does not reflect
the use & purpose of this class. `HloRunnerAgnosticTestBase` is a lot clearer.
",copybara-service[bot],2024-12-03 23:56:53+00:00,['cliveverghese'],2024-12-04 00:10:57+00:00,,https://github.com/tensorflow/tensorflow/pull/82125,[],[],
2716234836,pull_request,closed,,Add minimal test case to collective pipeliner,"Add minimal test case to collective pipeliner
",copybara-service[bot],2024-12-03 23:47:13+00:00,['frgossen'],2024-12-06 00:26:27+00:00,2024-12-06 00:26:27+00:00,https://github.com/tensorflow/tensorflow/pull/82124,[],[],
2716232558,pull_request,open,,Reverts fe06c6bed3e7d2e06972ceaa5013caa5750c5ca9,"Reverts fe06c6bed3e7d2e06972ceaa5013caa5750c5ca9
",copybara-service[bot],2024-12-03 23:45:32+00:00,[],2024-12-03 23:45:32+00:00,,https://github.com/tensorflow/tensorflow/pull/82123,[],[],
2716153725,pull_request,closed,,Allow including a notice file in `aar_with_jni`.,"Allow including a notice file in `aar_with_jni`.
",copybara-service[bot],2024-12-03 23:07:39+00:00,[],2024-12-04 01:17:49+00:00,2024-12-04 01:17:49+00:00,https://github.com/tensorflow/tensorflow/pull/82122,[],[],
2716147897,pull_request,closed,,Dont stop execution when bazel --version fails,"Dont stop execution when bazel --version fails
",copybara-service[bot],2024-12-03 23:04:34+00:00,['rtg0795'],2024-12-04 19:55:39+00:00,2024-12-04 19:55:38+00:00,https://github.com/tensorflow/tensorflow/pull/82121,[],[],
2716133430,pull_request,closed,,Build RBE container in parallel with the normal ml_build container. The RBE container will have the base image with nvidia driver because this is required for it to run the tests.,"Build RBE container in parallel with the normal ml_build container. The RBE container will have the base image with nvidia driver because this is required for it to run the tests.
",copybara-service[bot],2024-12-03 22:55:49+00:00,['quoctruong'],2024-12-12 00:15:26+00:00,2024-12-12 00:15:26+00:00,https://github.com/tensorflow/tensorflow/pull/82120,[],[],
2716094514,pull_request,closed,,[hlo-opt] Register HWI passes and move test files to dedicated test directory,"[hlo-opt] Register HWI passes and move test files to dedicated test directory
",copybara-service[bot],2024-12-03 22:26:03+00:00,[],2024-12-04 01:43:26+00:00,2024-12-04 01:43:26+00:00,https://github.com/tensorflow/tensorflow/pull/82119,[],[],
2716076371,pull_request,open,,incorporate Range into while_loop_analysis,"incorporate Range into while_loop_analysis
",copybara-service[bot],2024-12-03 22:12:14+00:00,[],2024-12-03 22:12:14+00:00,,https://github.com/tensorflow/tensorflow/pull/82118,[],[],
2716050776,pull_request,closed,,Fix documentation for CommandBufferCmdEmitter ConvertToCommands,"Fix documentation for CommandBufferCmdEmitter ConvertToCommands
",copybara-service[bot],2024-12-03 21:54:47+00:00,[],2024-12-04 01:07:12+00:00,2024-12-04 01:07:11+00:00,https://github.com/tensorflow/tensorflow/pull/82117,[],[],
2716036103,pull_request,closed,,"Fixes an input signature display issue in loaded TF1 SavedModels, and fixes a flattening-order discrepancy bug in ConcreteFunction.unpack_inputs().","Fixes an input signature display issue in loaded TF1 SavedModels, and fixes a flattening-order discrepancy bug in ConcreteFunction.unpack_inputs().
",copybara-service[bot],2024-12-03 21:44:51+00:00,['wangpengmit'],2024-12-06 23:05:16+00:00,2024-12-06 23:05:15+00:00,https://github.com/tensorflow/tensorflow/pull/82116,[],[],
2716014797,pull_request,closed,,Update partitioning method for gather/scatter along implicit batch dimensions.,"Update partitioning method for gather/scatter along implicit batch dimensions.

We first use `GatherScatterOperandsShardedAcrossParallelDims` to obtain the shardings for operands and indices. We can use the new indices sharding to infer the output sharding (incl. expliict/implicit batch dims and indices pass-through dims). Previously, we only consider expliict batch dims and indices pass-through dims.

This cl impacts the implicit batch dims only, without impact on explicit batch dims.
",copybara-service[bot],2024-12-03 21:30:30+00:00,[],2024-12-04 14:26:36+00:00,2024-12-04 14:26:35+00:00,https://github.com/tensorflow/tensorflow/pull/82115,[],[],
2715976373,pull_request,closed,,[xla:collectives] NFC: Move GetUniqueId to Collectives API,"[xla:collectives] NFC: Move GetUniqueId to Collectives API
",copybara-service[bot],2024-12-03 21:06:05+00:00,['ezhulenev'],2024-12-04 05:55:12+00:00,2024-12-04 05:55:11+00:00,https://github.com/tensorflow/tensorflow/pull/82114,[],[],
2715976133,pull_request,closed,,[xla-auto-sharding] Move helper functions into anonymous namespace.,"[xla-auto-sharding] Move helper functions into anonymous namespace.

Notes:
- Reorder helper functions to reflect the order in which they're called.
- Remove forward declaration to `SolveAndExtractSolution()` since this causes the function to appear twice in the Cider outline panel.
- Move helper functions not exposed in the .h file into anonymous namespaces.
",copybara-service[bot],2024-12-03 21:05:56+00:00,[],2024-12-03 22:34:16+00:00,2024-12-03 22:34:15+00:00,https://github.com/tensorflow/tensorflow/pull/82113,[],[],
2715965682,pull_request,open,,Fix input signature display issue in loaded TF1 SavedModels.,"Fix input signature display issue in loaded TF1 SavedModels.

Likely an uncaught bug from cl/537194542. There was only a discrepancy when printing or inspecting the parameters, the signature functions were still callable (as can be seen from the tests).
",copybara-service[bot],2024-12-03 20:59:11+00:00,['wangpengmit'],2024-12-03 20:59:12+00:00,,https://github.com/tensorflow/tensorflow/pull/82112,[],[],
2715924463,pull_request,closed,,Integrate StableHLO at openxla/stablehlo@d1db6dfe,"Integrate StableHLO at openxla/stablehlo@d1db6dfe
",copybara-service[bot],2024-12-03 20:33:24+00:00,[],2024-12-03 23:10:23+00:00,2024-12-03 23:10:23+00:00,https://github.com/tensorflow/tensorflow/pull/82111,[],[],
2715893822,pull_request,closed,,Remove stale hlo_instruction comment in thunk.h,"Remove stale hlo_instruction comment in thunk.h
",copybara-service[bot],2024-12-03 20:15:35+00:00,[],2024-12-03 21:03:38+00:00,2024-12-03 21:03:37+00:00,https://github.com/tensorflow/tensorflow/pull/82110,[],[],
2715864582,pull_request,closed,,Reuse buffers across scopes.,"Reuse buffers across scopes.

Previously buffer reuse pass would merge allocations at the innermost scope and hoist allocations to the outermost scope. This misses some reuse opportunities. For example, the two allocations in the separate scf blocks won't get merged before this CL.
```
  scf.if {
    scf.if {
      %a = memref.alloc() : memref<f32>
      ""some_a.op""(%a) : (memref<f32>) -> ()
    }
    scf.if {
      %b = memref.alloc() : memref<f32>
      ""some_b.op""(%b) : (memref<f32>) -> ()
    }
  }
```
Before this CL, two allocations are hoisted to the top level.
```      
  %a = memref.alloc() : memref<f32>
  %b = memref.alloc() : memref<f32>
  scf.if {
    scf.if {
      ""some_a.op""(%a) : (memref<f32>) -> ()
    }
    scf.if {
      ""some_b.op""(%b) : (memref<f32>) -> ()
    }
  }
```
After this CL, only one allocation is needed because we run buffer reuse after each hoist. It detected that the buffers could be reused after the two allocations are hoisted out of the their separate scf blocks but before they are hoisted again to the top.
```
  memref.alloc() : memref<f32>
  scf.if {
    scf.if {
      ""some_a.op""(%a) : (memref<f32>) -> ()
    }
    scf.if {
      ""some_b.op""(%b) : (memref<f32>) -> ()
    }
  }
```
In summary this CL tries to apply buffer reuse after hoisting allocations at each scope level, which fixes this issue.
",copybara-service[bot],2024-12-03 20:01:27+00:00,[],2024-12-04 20:26:11+00:00,2024-12-04 20:26:10+00:00,https://github.com/tensorflow/tensorflow/pull/82109,[],[],
2715796270,pull_request,closed,,add stamp argument to tflite_jni_binary,"add stamp argument to tflite_jni_binary
",copybara-service[bot],2024-12-03 19:06:39+00:00,[],2024-12-03 22:27:47+00:00,2024-12-03 22:27:46+00:00,https://github.com/tensorflow/tensorflow/pull/82108,[],[],
2715790505,pull_request,closed,,Add _raw_platform to work around extra platform normalization logic and enable,"Add _raw_platform to work around extra platform normalization logic and enable
GPU aot compilation without a GPU present.

Fixes https://github.com/jax-ml/jax/issues/23971
",copybara-service[bot],2024-12-03 19:03:11+00:00,['pschuh'],2024-12-04 00:58:34+00:00,2024-12-04 00:58:33+00:00,https://github.com/tensorflow/tensorflow/pull/82107,[],[],
2715786819,pull_request,open,,internal,"internal
",copybara-service[bot],2024-12-03 19:00:58+00:00,[],2024-12-03 19:00:58+00:00,,https://github.com/tensorflow/tensorflow/pull/82106,[],[],
2715781408,pull_request,closed,,Integrate LLVM at llvm/llvm-project@fed3a9b8f81f,"Integrate LLVM at llvm/llvm-project@fed3a9b8f81f

Updates LLVM usage to match
[fed3a9b8f81f](https://github.com/llvm/llvm-project/commit/fed3a9b8f81f)
",copybara-service[bot],2024-12-03 18:57:49+00:00,[],2024-12-04 00:37:17+00:00,2024-12-04 00:37:16+00:00,https://github.com/tensorflow/tensorflow/pull/82105,[],[],
2715773978,pull_request,closed,,Update `py_import` macros for the ability to unpack additional wheels in the same folder as the main wheel.,"Update `py_import` macros for the ability to unpack additional wheels in the same folder as the main wheel.
     
Usage example: provide NVIDIA wheel dependencies for ML wheels that have rpaths pointing to NVIDIA folders. When a user executes `pip install tensorflow[and_cuda]`, NVIDIA wheels are installed together with Tensorflow wheel. To reproduce this behavior in hermetic Python approach, we need to define `py_import` as follows (provided NVIDIA dependencies are defined in `requirements.in` and requirements lock files):

        py_import(
            name = ""tf_py_import"",
            wheel = "":wheel"",
            deps = [
                ""@pypi_absl_py//:pkg"",
                ""@pypi_astunparse//:pkg"",
                ""@pypi_flatbuffers//:pkg"",
                ""@pypi_gast//:pkg"",
                ""@pypi_ml_dtypes//:pkg"",
                ""@pypi_numpy//:pkg"",
                ""@pypi_opt_einsum//:pkg"",
                ""@pypi_packaging//:pkg"",
                ""@pypi_protobuf//:pkg"",
                ""@pypi_requests//:pkg"",
                ""@pypi_termcolor//:pkg"",
                ""@pypi_typing_extensions//:pkg"",
                ""@pypi_wrapt//:pkg"",
            ],
            wheel_deps = [
                ""@pypi_nvidia_cublas_cu12//:whl"",
                ""@pypi_nvidia_cuda_cupti_cu12//:whl"",
                ""@pypi_nvidia_cuda_nvcc_cu12//:whl"",
                ""@pypi_nvidia_cuda_nvrtc_cu12//:whl"",
                ""@pypi_nvidia_cuda_runtime_cu12//:whl"",
                ""@pypi_nvidia_cudnn_cu12//:whl"",
                ""@pypi_nvidia_cufft_cu12//:whl"",
                ""@pypi_nvidia_curand_cu12//:whl"",
                ""@pypi_nvidia_cusolver_cu12//:whl"",
                ""@pypi_nvidia_cusparse_cu12//:whl"",
                ""@pypi_nvidia_nccl_cu12//:whl"",
                ""@pypi_nvidia_nvjitlink_cu12//:whl"",
            ],
        )
",copybara-service[bot],2024-12-03 18:53:22+00:00,[],2024-12-13 00:33:15+00:00,2024-12-13 00:33:14+00:00,https://github.com/tensorflow/tensorflow/pull/82104,[],[],
2715771130,pull_request,open,,[XLA] Add -Werror=mismatched-tags to --config=linux.,"[XLA] Add -Werror=mismatched-tags to --config=linux.

Catches mismatched class/struct tags with a helpful compiler error rather than a cryptic linker error on Windows.
",copybara-service[bot],2024-12-03 18:51:41+00:00,[],2024-12-03 18:51:41+00:00,,https://github.com/tensorflow/tensorflow/pull/82103,[],[],
2715765252,pull_request,open,,Remove unused ptxas and nvlink build targets,"Remove unused ptxas and nvlink build targets
",copybara-service[bot],2024-12-03 18:48:13+00:00,[],2024-12-03 18:48:13+00:00,,https://github.com/tensorflow/tensorflow/pull/82102,[],[],
2715738113,pull_request,closed,,[ifrt] Fix tag mismatch for xla::ifrt::CompileOptions.,"[ifrt] Fix tag mismatch for xla::ifrt::CompileOptions.

Fixes build error for JAX on Windows.
",copybara-service[bot],2024-12-03 18:32:52+00:00,[],2024-12-03 19:34:54+00:00,2024-12-03 19:34:54+00:00,https://github.com/tensorflow/tensorflow/pull/82101,[],[],
2715534541,pull_request,closed,,Make CustomCallThunk not depend on gpu_types.h anymore,"Make CustomCallThunk not depend on gpu_types.h anymore

- `CustomCallThunk` takes a `GpuStreamHandle` as a parameter for the callback which is needed to support the legacy foreign function interface.
- I've changed the callback parameter to a `stream_executor::Stream*` which means the users of `CustomCallThunk` now need to provide callbacks that accept a `Stream*` instead of a `GpuStreamHandle`.
- The two users (IrEmitterUnnested and CustomFusion) now wrap the actual callback in a lambda which calls `platform_specific_handle()` on the `Stream*` parameter and pass the handle as a `void*` to the callback.
- That's not great since it calls the function pointer with the wrong type (`void*` instead of `cuStream` or `hipStream_t`). But we assume in many more places that the `GpuStreamHandle` is ABI-compatible to a `void` pointer. It's also the old API which is deprecated, not used a lot anymore, and will hopefully go away soon.
- We also save a bunch of other casts in different places since we now just use `Stream*` parameters.
- This also fixes an ODR violation coming from the two definitions of CustomCallTarget in CustomCallThunk.
",copybara-service[bot],2024-12-03 16:53:49+00:00,[],2024-12-03 21:12:51+00:00,2024-12-03 21:12:51+00:00,https://github.com/tensorflow/tensorflow/pull/82100,[],[],
2715345671,pull_request,closed,,[XLA:GPU] Consolidate logic to enable expensive optimisation passes.,"[XLA:GPU] Consolidate logic to enable expensive optimisation passes.

The intent is to make it easier to figure out which passes are enabled via `exec_time_optimization_effort`.
",copybara-service[bot],2024-12-03 15:31:20+00:00,[],2024-12-04 16:48:11+00:00,2024-12-04 16:48:10+00:00,https://github.com/tensorflow/tensorflow/pull/82085,[],[],
2715325514,pull_request,closed,,#sdy support StableHLO from refining Shardy ops with polymorphic shapes,"#sdy support StableHLO from refining Shardy ops with polymorphic shapes
",copybara-service[bot],2024-12-03 15:22:49+00:00,[],2025-01-16 17:25:07+00:00,2025-01-16 17:25:04+00:00,https://github.com/tensorflow/tensorflow/pull/82084,[],[],
2715264122,pull_request,open,,PR #19096: Add F4E2M1FN and F8E8M0FNU types,"PR #19096: Add F4E2M1FN and F8E8M0FNU types

Imported from GitHub PR https://github.com/openxla/xla/pull/19096

This PR adds F4E2M1FN primitive type (4-bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8-bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented.

This will enable using microscaling (MX) formats ([RFC](https://github.com/openxla/xla/discussions/18085)), such as MXFP4.

```c
F4E2M1FN
- Exponent bias: 1
- Maximum stored exponent value: 3 (binary 11)
- Maximum unbiased exponent value: 3 - 1 = 2
- Minimum stored exponent value: 1 (binary 01)
- Minimum unbiased exponent value: 1 − 1 = 0
- Has Positive and Negative zero
- Doesn't have infinity
- Doesn't have NaNs

Additional details:
- Zeros (+/-): S.00.0
- Max normal number: S.11.1 = ±2^(2) x (1 + 0.5) = ±6.0
- Min normal number: S.01.0 = ±2^(0) = ±1.0
- Min subnormal number: S.00.1 = ±2^(0) x 0.5 = ±0.5

F8E8M0FNU
- Exponent bias: 127
- Maximum stored exponent value: 254 (binary 1111'1110)
- Maximum unbiased exponent value: 254 - 127 = 127
- Minimum stored exponent value: 0 (binary 0000'0000)
- Minimum unbiased exponent value: 0 − 127 = -127
- Doesn't have zero
- Doesn't have infinity
- NaN is encoded as binary 1111'1111

Additional details:
- Zeros cannot be represented
- Negative values cannot be represented
- Mantissa is always 1
```

Related PRs:
- https://github.com/openxla/stablehlo/pull/2582
- https://github.com/jax-ml/ml_dtypes/pull/181
- https://github.com/llvm/llvm-project/pull/95392
- https://github.com/llvm/llvm-project/pull/108877
- https://github.com/jax-ml/ml_dtypes/pull/166
- https://github.com/llvm/llvm-project/pull/107127
- https://github.com/llvm/llvm-project/pull/111028

The PR is split into multiple commits just to make the review easier, it is possible that some tests could fail if only some (i.e. not all) of these commits are applied.
Copybara import of the project:

--
fa539fbde987ff6421fd2937fade495baf633630 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: import mxfloat.h

--
2c014035923e0394b2cfcb81eaf090a96621b0aa by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: primitive type

--
e919ed54e825f2e905aaf0cc279dd21cd80f1ce9 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: literal support

--
ca16839096feb93e0454ec380c5c707c30199346 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: conversion codegen

--
eedc079ca9a4db9e611d84877a25b3da21386f16 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: python interface

--
8e0305cd47002f0c1f8668a3cbcbce5428f2a4c6 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: FFI

--
aabe9c68d964609f78f29e17ee0680798ad0c6ac by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: HLO evaluator

--
87da2ebfab388f113482e852009401a9e416974a by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN type: add tests

--
e0ee48c3a37018ba985c850931592d62eadf7c2e by Sergey Kozub <skozub@nvidia.com>:

Add F8E8M0FNU type

--
be2e457922e2cddeaf5aca13dd022f3ac2a1393b by Sergey Kozub <skozub@nvidia.com>:

Addressing PR#19096 review comments

Merging this change closes #19096

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19096 from openxla:skozub/e2m1 be2e457922e2cddeaf5aca13dd022f3ac2a1393b
",copybara-service[bot],2024-12-03 14:58:17+00:00,[],2024-12-03 14:58:17+00:00,,https://github.com/tensorflow/tensorflow/pull/82081,[],[],
2715245819,pull_request,open,,Integrate LLVM at llvm/llvm-project@f71ea4bc1b01,"Integrate LLVM at llvm/llvm-project@f71ea4bc1b01

Updates LLVM usage to match
[f71ea4bc1b01](https://github.com/llvm/llvm-project/commit/f71ea4bc1b01)
",copybara-service[bot],2024-12-03 14:51:01+00:00,[],2024-12-03 14:51:01+00:00,,https://github.com/tensorflow/tensorflow/pull/82077,[],[],
2715232301,pull_request,closed,,[XLA:CPU] Add method to benchmark compile times,"[XLA:CPU] Add method to benchmark compile times
",copybara-service[bot],2024-12-03 14:45:48+00:00,[],2024-12-05 18:04:55+00:00,2024-12-05 18:04:54+00:00,https://github.com/tensorflow/tensorflow/pull/82076,[],[],
2715072345,pull_request,open,,Regenerate stubs with Mypy 1.7.1,"Regenerate stubs with Mypy 1.7.1
",copybara-service[bot],2024-12-03 13:43:26+00:00,[],2024-12-03 13:43:26+00:00,,https://github.com/tensorflow/tensorflow/pull/82067,[],[],
2714981783,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 13:04:16+00:00,[],2024-12-03 13:04:16+00:00,,https://github.com/tensorflow/tensorflow/pull/82066,[],[],
2714925688,pull_request,closed,,[XLA:GPU] Log the error if parsing of Triton IR from custom call fails.,"[XLA:GPU] Log the error if parsing of Triton IR from custom call fails.
",copybara-service[bot],2024-12-03 12:39:09+00:00,[],2024-12-03 13:19:24+00:00,2024-12-03 13:19:23+00:00,https://github.com/tensorflow/tensorflow/pull/82065,[],[],
2714827323,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 12:00:12+00:00,[],2024-12-03 12:00:12+00:00,,https://github.com/tensorflow/tensorflow/pull/82063,[],[],
2714799876,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 11:48:59+00:00,[],2024-12-03 11:48:59+00:00,,https://github.com/tensorflow/tensorflow/pull/82062,[],[],
2714775219,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 11:37:52+00:00,[],2024-12-03 11:37:52+00:00,,https://github.com/tensorflow/tensorflow/pull/82061,[],[],
2714747779,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 11:26:32+00:00,[],2024-12-03 11:26:32+00:00,,https://github.com/tensorflow/tensorflow/pull/82060,[],[],
2714733727,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 11:21:17+00:00,[],2024-12-03 11:21:17+00:00,,https://github.com/tensorflow/tensorflow/pull/82059,[],[],
2714714337,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 11:12:25+00:00,[],2024-12-03 11:12:25+00:00,,https://github.com/tensorflow/tensorflow/pull/82058,[],[],
2714713388,pull_request,closed,,Add more debug log when Shardy dumping is disabled.,"Add more debug log when Shardy dumping is disabled.
",copybara-service[bot],2024-12-03 11:11:57+00:00,[],2024-12-04 17:03:51+00:00,2024-12-04 17:03:50+00:00,https://github.com/tensorflow/tensorflow/pull/82057,[],[],
2714637754,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 10:38:48+00:00,[],2024-12-03 10:38:48+00:00,,https://github.com/tensorflow/tensorflow/pull/82056,[],[],
2714628759,pull_request,closed,,[XLA:GPU] Don't allow to fuse DUS with shared operands.,"[XLA:GPU] Don't allow to fuse DUS with shared operands.

DynamicUpdateSlice is an in-place operation. Therefore we cannot fuse two such
ops together if they share the same operand.
Also adjust the check whether the in-place emitter can be used to handle such a
case.
",copybara-service[bot],2024-12-03 10:35:02+00:00,['akuegel'],2024-12-03 12:37:13+00:00,2024-12-03 12:37:12+00:00,https://github.com/tensorflow/tensorflow/pull/82055,[],[],
2714588192,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 10:19:01+00:00,[],2024-12-03 10:19:01+00:00,,https://github.com/tensorflow/tensorflow/pull/82054,[],[],
2714583466,pull_request,closed,,Move ForAllThunk into Thunk base class,"Move ForAllThunk into Thunk base class

`ForAllThunk` is currently implemented as a free function. It's a switch statement
which handles all thunk types. Each type gets downcasted to the real type for
handling the nested buffers.

This change makes `ForAllThunk` a virtual function on `Thunk`. `Thunk` provides
a default implementation which just visits `this` and thunks with nested thunks
can override `ForAllThunk` to adjust the behaviour.
",copybara-service[bot],2024-12-03 10:16:59+00:00,[],2024-12-04 07:41:17+00:00,2024-12-04 07:41:16+00:00,https://github.com/tensorflow/tensorflow/pull/82053,[],[],
2714581218,pull_request,closed,,Add TraceMe around HLO passes and pipelines.,"Add TraceMe around HLO passes and pipelines.
",copybara-service[bot],2024-12-03 10:16:00+00:00,['tomhennigan'],2024-12-04 11:03:07+00:00,2024-12-04 11:03:05+00:00,https://github.com/tensorflow/tensorflow/pull/82052,[],[],
2714567176,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 10:09:56+00:00,[],2024-12-03 10:09:56+00:00,,https://github.com/tensorflow/tensorflow/pull/82051,[],[],
2714565024,pull_request,open,,Integrate LLVM at llvm/llvm-project@1d6ab189be03,"Integrate LLVM at llvm/llvm-project@1d6ab189be03

Updates LLVM usage to match
[1d6ab189be03](https://github.com/llvm/llvm-project/commit/1d6ab189be03)
",copybara-service[bot],2024-12-03 10:09:00+00:00,[],2024-12-03 12:11:09+00:00,,https://github.com/tensorflow/tensorflow/pull/82050,[],[],
2714563230,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 10:08:14+00:00,[],2024-12-09 04:41:05+00:00,2024-12-09 04:41:04+00:00,https://github.com/tensorflow/tensorflow/pull/82049,[],[],
2714466813,pull_request,closed,,[XLA:GPU] Add RaggedAllToAllDecomposer to the GPU compilation pipeline.,"[XLA:GPU] Add RaggedAllToAllDecomposer to the GPU compilation pipeline.

Also add a small e2e test. More e2e tests are coming later.
",copybara-service[bot],2024-12-03 09:32:10+00:00,[],2024-12-04 13:16:37+00:00,2024-12-04 13:16:36+00:00,https://github.com/tensorflow/tensorflow/pull/82047,[],[],
2714447441,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts e0ccb4b1069e70f5104cf9d1d0028cb6240db668
",copybara-service[bot],2024-12-03 09:23:45+00:00,[],2024-12-03 13:01:16+00:00,,https://github.com/tensorflow/tensorflow/pull/82046,[],[],
2714396594,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19571 from openxla:pjrt_fix_process_index_count 8620919d35f1160d993cb69e5b32c32b77cbba7d
",copybara-service[bot],2024-12-03 09:01:46+00:00,[],2024-12-03 09:01:46+00:00,,https://github.com/tensorflow/tensorflow/pull/82044,[],[],
2714386379,pull_request,open,,Integrate LLVM at llvm/llvm-project@2af2634c64b1,"Integrate LLVM at llvm/llvm-project@2af2634c64b1

Updates LLVM usage to match
[2af2634c64b1](https://github.com/llvm/llvm-project/commit/2af2634c64b1)
",copybara-service[bot],2024-12-03 08:57:14+00:00,[],2024-12-03 08:57:14+00:00,,https://github.com/tensorflow/tensorflow/pull/82043,[],[],
2714378682,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 08:53:47+00:00,[],2024-12-03 08:53:47+00:00,,https://github.com/tensorflow/tensorflow/pull/82042,[],[],
2714309566,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 08:20:44+00:00,[],2024-12-03 08:20:44+00:00,,https://github.com/tensorflow/tensorflow/pull/82041,[],[],
2714308807,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 08:20:18+00:00,[],2024-12-03 08:20:18+00:00,,https://github.com/tensorflow/tensorflow/pull/82040,[],[],
2714308011,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 08:19:54+00:00,[],2024-12-03 08:19:54+00:00,,https://github.com/tensorflow/tensorflow/pull/82039,[],[],
2714301438,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 08:16:49+00:00,[],2024-12-03 08:16:49+00:00,,https://github.com/tensorflow/tensorflow/pull/82038,[],[],
2714278683,pull_request,closed,,Turn on by default multi-process autotune shard,"Turn on by default multi-process autotune shard
",copybara-service[bot],2024-12-03 08:05:01+00:00,[],2024-12-03 13:30:23+00:00,2024-12-03 13:30:22+00:00,https://github.com/tensorflow/tensorflow/pull/82037,[],[],
2714265207,pull_request,closed,,Allow enabling cl_khr_command_buffer in RestoreDeserialized,"Allow enabling cl_khr_command_buffer in RestoreDeserialized
",copybara-service[bot],2024-12-03 07:57:32+00:00,[],2024-12-07 00:45:32+00:00,2024-12-07 00:45:31+00:00,https://github.com/tensorflow/tensorflow/pull/82036,[],[],
2714259193,pull_request,closed,,Apply layout permutation also to dynamic dimensions.,"Apply layout permutation also to dynamic dimensions.

In MakeShapeWithDescendingLayoutAndSamePhysicalLayout(), we should apply the
layout permutation not only to the dimensions, but also to the
dynamic_dimensions.
",copybara-service[bot],2024-12-03 07:54:02+00:00,['akuegel'],2024-12-04 06:48:29+00:00,2024-12-04 06:48:28+00:00,https://github.com/tensorflow/tensorflow/pull/82035,[],[],
2714242023,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 07:44:28+00:00,[],2024-12-03 07:44:28+00:00,,https://github.com/tensorflow/tensorflow/pull/82034,[],[],
2714199706,pull_request,closed,,Integrate LLVM at llvm/llvm-project@bd92e4620433,"Integrate LLVM at llvm/llvm-project@bd92e4620433

Updates LLVM usage to match
[bd92e4620433](https://github.com/llvm/llvm-project/commit/bd92e4620433)
",copybara-service[bot],2024-12-03 07:19:45+00:00,[],2024-12-03 09:16:30+00:00,2024-12-03 09:16:29+00:00,https://github.com/tensorflow/tensorflow/pull/82033,[],[],
2714180385,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 07:08:39+00:00,[],2024-12-04 10:38:17+00:00,,https://github.com/tensorflow/tensorflow/pull/82032,[],[],
2714170572,pull_request,closed,,Check whether profile info is empty to determine if the module is using profiles.,"Check whether profile info is empty to determine if the module is using profiles.
",copybara-service[bot],2024-12-03 07:02:50+00:00,[],2024-12-03 20:41:46+00:00,2024-12-03 20:41:46+00:00,https://github.com/tensorflow/tensorflow/pull/82031,[],[],
2714124326,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:32:37+00:00,[],2024-12-03 13:04:49+00:00,2024-12-03 13:04:49+00:00,https://github.com/tensorflow/tensorflow/pull/82029,[],[],
2714124322,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:32:36+00:00,[],2024-12-03 06:32:36+00:00,,https://github.com/tensorflow/tensorflow/pull/82028,[],[],
2714122860,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:31:35+00:00,[],2024-12-03 06:31:35+00:00,,https://github.com/tensorflow/tensorflow/pull/82027,[],[],
2714121588,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:30:45+00:00,[],2024-12-04 06:45:22+00:00,,https://github.com/tensorflow/tensorflow/pull/82026,[],[],
2714120839,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:30:14+00:00,[],2024-12-04 07:56:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82025,[],[],
2714118965,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:28:55+00:00,[],2024-12-03 06:28:55+00:00,,https://github.com/tensorflow/tensorflow/pull/82024,[],[],
2714118057,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:28:13+00:00,[],2024-12-03 06:28:13+00:00,,https://github.com/tensorflow/tensorflow/pull/82023,[],[],
2714113392,pull_request,closed,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19571 from openxla:pjrt_fix_process_index_count 8620919d35f1160d993cb69e5b32c32b77cbba7d
",copybara-service[bot],2024-12-03 06:24:50+00:00,[],2024-12-03 10:14:46+00:00,2024-12-03 10:14:45+00:00,https://github.com/tensorflow/tensorflow/pull/82022,[],[],
2714111455,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:23:32+00:00,[],2024-12-03 06:23:32+00:00,,https://github.com/tensorflow/tensorflow/pull/82021,[],[],
2714108436,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:21:24+00:00,[],2024-12-03 06:21:24+00:00,,https://github.com/tensorflow/tensorflow/pull/82020,[],[],
2714106625,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:20:06+00:00,[],2024-12-04 05:55:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82019,[],[],
2714106392,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:19:56+00:00,[],2024-12-04 07:04:02+00:00,2024-12-04 07:04:01+00:00,https://github.com/tensorflow/tensorflow/pull/82018,[],[],
2714101562,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:16:33+00:00,[],2024-12-04 05:42:01+00:00,,https://github.com/tensorflow/tensorflow/pull/82017,[],[],
2714099535,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:15:05+00:00,[],2024-12-04 08:07:10+00:00,,https://github.com/tensorflow/tensorflow/pull/82016,[],[],
2714099182,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:14:50+00:00,[],2024-12-05 10:09:06+00:00,2024-12-05 10:09:02+00:00,https://github.com/tensorflow/tensorflow/pull/82015,[],[],
2714098633,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:14:28+00:00,[],2024-12-03 06:14:28+00:00,,https://github.com/tensorflow/tensorflow/pull/82014,[],[],
2714087602,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 06:07:24+00:00,[],2024-12-04 05:29:58+00:00,,https://github.com/tensorflow/tensorflow/pull/82013,[],[],
2714052533,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 05:41:28+00:00,[],2024-12-03 05:41:28+00:00,,https://github.com/tensorflow/tensorflow/pull/82012,[],[],
2714033382,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-03 05:25:42+00:00,[],2024-12-03 05:25:42+00:00,,https://github.com/tensorflow/tensorflow/pull/82011,[],[],
2714015143,pull_request,closed,,[xla:collectives] NFC: Move lockable GpuClique to collectives component,"[xla:collectives] NFC: Move lockable GpuClique to collectives component
",copybara-service[bot],2024-12-03 05:11:25+00:00,['ezhulenev'],2024-12-04 05:32:07+00:00,2024-12-04 05:32:07+00:00,https://github.com/tensorflow/tensorflow/pull/82010,[],[],
2714014208,pull_request,closed,,[xla:collectives] NFC: Extract NcclCommunicators into GpuClique and NcclCliqueImpl,"[xla:collectives] NFC: Extract NcclCommunicators into GpuClique and NcclCliqueImpl
",copybara-service[bot],2024-12-03 05:10:42+00:00,['ezhulenev'],2024-12-04 00:29:26+00:00,2024-12-04 00:29:25+00:00,https://github.com/tensorflow/tensorflow/pull/82009,[],[],
2714014154,pull_request,closed,,[xla:collectives] NFC: Move NcclApi::CommCount to Communicator API,"[xla:collectives] NFC: Move NcclApi::CommCount to Communicator API
",copybara-service[bot],2024-12-03 05:10:39+00:00,['ezhulenev'],2024-12-04 04:25:44+00:00,2024-12-04 04:25:43+00:00,https://github.com/tensorflow/tensorflow/pull/82008,[],[],
2714012738,pull_request,closed,,[xla:collectives] NFC: Extract shared Clique from NcclCommunicators,"[xla:collectives] NFC: Extract shared Clique from NcclCommunicators
",copybara-service[bot],2024-12-03 05:09:24+00:00,['ezhulenev'],2024-12-03 23:01:28+00:00,2024-12-03 23:01:25+00:00,https://github.com/tensorflow/tensorflow/pull/82007,[],[],
2714012670,pull_request,closed,,[xla:collectives] NFC: Remove unused NcclApi CommFinalize function,"[xla:collectives] NFC: Remove unused NcclApi CommFinalize function
",copybara-service[bot],2024-12-03 05:09:20+00:00,['ezhulenev'],2024-12-04 03:36:18+00:00,2024-12-04 03:36:18+00:00,https://github.com/tensorflow/tensorflow/pull/82006,[],[],
2714012222,pull_request,closed,,[xla:collectives] NFC: Move communicator error checking to Communicator API,"[xla:collectives] NFC: Move communicator error checking to Communicator API
",copybara-service[bot],2024-12-03 05:08:59+00:00,['ezhulenev'],2024-12-04 02:23:32+00:00,2024-12-04 02:23:32+00:00,https://github.com/tensorflow/tensorflow/pull/82005,[],[],
2714011937,pull_request,closed,,[xla:collectives] NFC: Move CommAbort to Communicator API,"[xla:collectives] NFC: Move CommAbort to Communicator API
",copybara-service[bot],2024-12-03 05:08:43+00:00,['ezhulenev'],2024-12-04 02:43:24+00:00,2024-12-04 02:43:23+00:00,https://github.com/tensorflow/tensorflow/pull/82004,[],[],
2713917636,pull_request,open,,<REMOVE THIS TAG ONCE DIFFBASE IS SUBMITTED>,"<REMOVE THIS TAG ONCE DIFFBASE IS SUBMITTED>
",copybara-service[bot],2024-12-03 03:50:40+00:00,[],2024-12-03 03:50:40+00:00,,https://github.com/tensorflow/tensorflow/pull/82003,[],[],
2713854520,pull_request,closed,,[XLA:GPU] Pipeline send/recv ops but not *-done ops in experimental PP opts,"[XLA:GPU] Pipeline send/recv ops but not *-done ops in experimental PP opts

Also, fix collective pipeliner for loop crossing control dependencies.
This pipeliner heuristic is needed for SPMD-based pipeline parallelism.
",copybara-service[bot],2024-12-03 02:52:16+00:00,['frgossen'],2024-12-05 03:43:48+00:00,2024-12-05 03:43:47+00:00,https://github.com/tensorflow/tensorflow/pull/82001,[],[],
2713840755,pull_request,closed,,Add flatten_conditional and conditional_value to HloControlFlowFlattening to allow flattening conditionals,"Add flatten_conditional and conditional_value to HloControlFlowFlattening to allow flattening conditionals
",copybara-service[bot],2024-12-03 02:39:51+00:00,[],2024-12-05 21:33:28+00:00,2024-12-05 21:33:27+00:00,https://github.com/tensorflow/tensorflow/pull/81999,[],[],
2713814329,pull_request,closed,,Implement flatten one level with keys in C++ and use it for the prefix/equality error printing.,"Implement flatten one level with keys in C++ and use it for the prefix/equality error printing.

With this, we should be able to safely delete the python with-path registry after a new jaxlib release.

Also changed all `std::string_view` to `absl::string_view` per requirements of TF repository.
",copybara-service[bot],2024-12-03 02:19:54+00:00,[],2024-12-13 01:07:53+00:00,2024-12-13 01:07:52+00:00,https://github.com/tensorflow/tensorflow/pull/81998,[],[],
2713791359,pull_request,closed,,Support int4 in most ops on CPUs/GPUs.,"Support int4 in most ops on CPUs/GPUs.

Now most ops support int4. FloatSupport is used to upcast most int4 ops to int8, since codegen does not natively support int4 in certain cases and GPUs do not natively support int4 math regardless.

The main challenge here is that the element_size_in_bits layout field must be set to 4 for all int4 arrays. This Layout field exists since on the host, int4 arrays are unpacked while on the device, int4 arrays are packed packed. element_size_in_bits specifies whether the array is packed, and should always be set for device tensors.

Many passes create new shapes for new instructions without setting element_size_in_bits, e.g. with ShapeUtil::MakeShape. This is problematic as it can cause int4 arrays to be created without element_size_in_bits set. This becomes especially problematic when int4 is supported in more ops, as it presents more opportunities for passes to create shapes without the element_size_in_bits set. For each known case where this is done, we address this is one of two ways:

1. We call SubByteNormalization after the pass runs, which properly sets element_size_in_bits on all instructions
2. We call a special method UpdateLayout() on the shape, which sets element_size_in_bits on it. Many passes already call UpdateLayout() before this change, and this change adds setting element_size_in_bits to UpdateLayout.
",copybara-service[bot],2024-12-03 02:01:47+00:00,['reedwm'],2024-12-04 02:29:32+00:00,2024-12-04 02:29:31+00:00,https://github.com/tensorflow/tensorflow/pull/81997,[],[],
2713769147,pull_request,open,,PR #19161: Asymmetrically Replicated Instructions in Replication Analysis,"PR #19161: Asymmetrically Replicated Instructions in Replication Analysis

Imported from GitHub PR https://github.com/openxla/xla/pull/19161

Extends the HLO replication analysis to handle asymmetrically replicated instructions with replica groups covering multiple partitions and replicas.
Copybara import of the project:

--
2142a2275583a1de3e9c73cf4c751b3d06d43b4d by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

--
9b0a98454533b6dc98320963ce88c6f17e8d9fb8 by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

--
22ff34d0d02acf2ad2fad5caacf10a40233e3193 by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

--
337a28a1992dea5ea4d2432f229f56f7409025de by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

--
300c9e52325f65952f93d685c4730b479c3a6fd2 by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

--
3721d68e2b373533d8d114474a53d66543f868c7 by Philipp Hack <phack@nvidia.com>:

Extends the HLO replication analysis to asymmetrically replicated instructions.

Merging this change closes #19161

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19161 from philipphack:u_replication_asymmetric_xla 3721d68e2b373533d8d114474a53d66543f868c7
",copybara-service[bot],2024-12-03 01:42:21+00:00,[],2024-12-10 06:37:49+00:00,,https://github.com/tensorflow/tensorflow/pull/81996,[],[],
2713751341,pull_request,closed,,Use default HloParserOptions in HLO runner.,"Use default HloParserOptions in HLO runner.

This will unify parsing behaviour in tests cases and the HLO runner. If we
think filling in missing layouts does or does not make sense, we can change the
default in HloParserOptions.
",copybara-service[bot],2024-12-03 01:25:03+00:00,['frgossen'],2024-12-05 22:35:21+00:00,2024-12-05 22:35:20+00:00,https://github.com/tensorflow/tensorflow/pull/81995,[],[],
2713747528,pull_request,closed,,[xla][gpu] Order send/recv chains across decomposed collective permutes,"[xla][gpu] Order send/recv chains across decomposed collective permutes

Insert control dependencies between send and recv operations across decomposed collective permutes. This addresses a potential NCCL deadlock that may occur if every device tries to execute recv with no devices executing send.
",copybara-service[bot],2024-12-03 01:21:25+00:00,['frgossen'],2024-12-05 03:17:11+00:00,2024-12-05 03:17:10+00:00,https://github.com/tensorflow/tensorflow/pull/81994,[],[],
2713731121,pull_request,open,,Integrate LLVM at llvm/llvm-project@1250a1db1a37,"Integrate LLVM at llvm/llvm-project@1250a1db1a37

Updates LLVM usage to match
[1250a1db1a37](https://github.com/llvm/llvm-project/commit/1250a1db1a37)
",copybara-service[bot],2024-12-03 01:08:57+00:00,[],2024-12-03 06:27:01+00:00,,https://github.com/tensorflow/tensorflow/pull/81993,[],[],
2713696158,pull_request,closed,,[XLA:Collective] Add utility functions.,"[XLA:Collective] Add utility functions.
",copybara-service[bot],2024-12-03 00:41:06+00:00,['Tongfei-Guo'],2024-12-13 19:59:40+00:00,2024-12-13 19:59:39+00:00,https://github.com/tensorflow/tensorflow/pull/81992,[],[],
2713685324,pull_request,closed,,Remove unused ptxas and nvlink build targets,"Remove unused ptxas and nvlink build targets
",copybara-service[bot],2024-12-03 00:33:00+00:00,[],2024-12-03 18:52:01+00:00,2024-12-03 18:52:00+00:00,https://github.com/tensorflow/tensorflow/pull/81991,[],[],
2713669503,pull_request,closed,,"Rollback of ""Require packed dot operands to be packed along contracting dimension.""","Rollback of ""Require packed dot operands to be packed along contracting dimension.""

Reverts 35bf327d76549a0398a7e0b7cce65698138cf9c8
",copybara-service[bot],2024-12-03 00:20:56+00:00,[],2024-12-03 01:30:38+00:00,2024-12-03 01:30:36+00:00,https://github.com/tensorflow/tensorflow/pull/81990,[],[],
2713651156,pull_request,open,,Add GPU configuration options to TensorFlow Go bindings,"- Introduced a new GPUOptions struct in TensorFlow's Go bindings to allow users to configure GPU-specific settings, such as enabling memory growth (AllowGrowth) and specifying a GPU allocator type (AllocatorType).
- Added unit tests to ensure the GPUOptions behave as expected and integrate seamlessly with existing TensorFlow functionality.
- Updated the session creation API to accept and apply GPUOptions during initialization, aligning the Go API with TensorFlow’s Python API.",kayladoann,2024-12-03 00:03:40+00:00,['gbaned'],2025-01-16 04:41:49+00:00,,https://github.com/tensorflow/tensorflow/pull/81989,"[('comp:gpu', 'GPU related issues'), ('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2513229957, 'issue_id': 2713651156, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81989/checks?check_run_id=33820108492) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 12, 3, 0, 3, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514052524, 'issue_id': 2713651156, 'author': 'mihaimaruseac', 'body': 'Please sign the CLA and then tag me again to review.', 'created_at': datetime.datetime(2024, 12, 3, 9, 50, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515353052, 'issue_id': 2713651156, 'author': 'kayladoann', 'body': ""I've signed the CLA"", 'created_at': datetime.datetime(2024, 12, 3, 18, 58, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518381186, 'issue_id': 2713651156, 'author': 'mihaimaruseac', 'body': 'It still shows as unsigned,\r\n\r\n```\r\n❌ https://github.com/tensorflow/tensorflow/commit/fdbcb89b9911acecdc7d2704176c2a4632eafb0b Author: <ka*****an\u200b@10-249-45-207.wireless.oregonstate.edu>\r\n```\r\n\r\nI cannot debug more for the next ~2 weeks due to not being home, sorry for the delay.', 'created_at': datetime.datetime(2024, 12, 4, 19, 26, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562237983, 'issue_id': 2713651156, 'author': 'keerthanakadiri', 'body': '@kayladoann, Could you please sign CLA, as it is still unsigned?  Thank you !', 'created_at': datetime.datetime(2024, 12, 26, 7, 9, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562995776, 'issue_id': 2713651156, 'author': 'mihaimaruseac', 'body': 'So the issue with the CLA is that you signed with your Github account but the email associated to the commit (`...\u200b@10-249-45-207.wireless.oregonstate.edu`) is not associated to your GitHub account, @kayladoann.\r\n\r\nIf you can attach them, I think that would work. Alternatively, you can amend your current commit to change the email address (`git commit --amend ...` followed by `git push -f`)\r\n\r\nI might need to click one button on our side if the process is not automated, so please ping me once you fix this.', 'created_at': datetime.datetime(2024, 12, 26, 18, 4, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594485599, 'issue_id': 2713651156, 'author': 'keerthanakadiri', 'body': 'Hi @kayladoann, Please refer to the [comment ](https://github.com/tensorflow/tensorflow/pull/81989#issuecomment-2562995776) to resolve any issue with the CLA signing process.', 'created_at': datetime.datetime(2025, 1, 16, 4, 41, 46, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-12-03 00:03:44 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81989/checks?check_run_id=33820108492) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

mihaimaruseac on (2024-12-03 09:50:24 UTC): Please sign the CLA and then tag me again to review.

kayladoann (Issue Creator) on (2024-12-03 18:58:51 UTC): I've signed the CLA

mihaimaruseac on (2024-12-04 19:26:42 UTC): It still shows as unsigned,

```
❌ https://github.com/tensorflow/tensorflow/commit/fdbcb89b9911acecdc7d2704176c2a4632eafb0b Author: <ka*****an​@10-249-45-207.wireless.oregonstate.edu>
```

I cannot debug more for the next ~2 weeks due to not being home, sorry for the delay.

keerthanakadiri on (2024-12-26 07:09:58 UTC): @kayladoann, Could you please sign CLA, as it is still unsigned?  Thank you !

mihaimaruseac on (2024-12-26 18:04:03 UTC): So the issue with the CLA is that you signed with your Github account but the email associated to the commit (`...​@10-249-45-207.wireless.oregonstate.edu`) is not associated to your GitHub account, @kayladoann.

If you can attach them, I think that would work. Alternatively, you can amend your current commit to change the email address (`git commit --amend ...` followed by `git push -f`)

I might need to click one button on our side if the process is not automated, so please ping me once you fix this.

keerthanakadiri on (2025-01-16 04:41:46 UTC): Hi @kayladoann, Please refer to the [comment ](https://github.com/tensorflow/tensorflow/pull/81989#issuecomment-2562995776) to resolve any issue with the CLA signing process.

"
2713643376,pull_request,closed,,Split up cusolver_context into CUDA-specific and ROCM-specific parts.,"Split up cusolver_context into CUDA-specific and ROCM-specific parts.

Eliminate many #ifdefs and prepare for a future where the last couple remaining can be removed.
",copybara-service[bot],2024-12-02 23:56:28+00:00,[],2024-12-09 19:51:01+00:00,2024-12-09 19:51:00+00:00,https://github.com/tensorflow/tensorflow/pull/81988,[],[],
2713618372,pull_request,closed,,Fix resource number calculation in the latency hiding scheduler.,"Fix resource number calculation in the latency hiding scheduler.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20861 from Cjkkkk:segment_id ae2c14a7c2391f1b343c3721d739a1588360841f
",copybara-service[bot],2024-12-02 23:32:59+00:00,[],2025-01-08 00:10:50+00:00,2025-01-08 00:10:49+00:00,https://github.com/tensorflow/tensorflow/pull/81986,[],[],
2713614791,pull_request,closed,,Model: Add signature support,"Model: Add signature support
",copybara-service[bot],2024-12-02 23:31:08+00:00,['terryheo'],2024-12-04 22:06:14+00:00,2024-12-04 22:06:13+00:00,https://github.com/tensorflow/tensorflow/pull/81985,[],[],
2713601246,pull_request,closed,,[XLA] Move the scheduling annotation from fused instruction to the caller if the caller is not annotated. Return error if fused instructions contain ops with different annotation ids.,"[XLA] Move the scheduling annotation from fused instruction to the caller if the caller is not annotated. Return error if fused instructions contain ops with different annotation ids.
",copybara-service[bot],2024-12-02 23:20:54+00:00,['seherellis'],2024-12-06 18:45:09+00:00,2024-12-06 18:45:08+00:00,https://github.com/tensorflow/tensorflow/pull/81984,[],[],
2713578323,pull_request,closed,,Mark `tsl/default:criticality` as `nobuilder`,"Mark `tsl/default:criticality` as `nobuilder`
",copybara-service[bot],2024-12-02 23:01:38+00:00,['ddunl'],2024-12-03 01:07:56+00:00,2024-12-03 01:07:55+00:00,https://github.com/tensorflow/tensorflow/pull/81983,[],[],
2713561825,pull_request,closed,,ModelRepacker: Populate CustomOpCode only if not empty,"ModelRepacker: Populate CustomOpCode only if not empty
",copybara-service[bot],2024-12-02 22:49:14+00:00,['terryheo'],2024-12-03 23:16:24+00:00,2024-12-03 23:16:23+00:00,https://github.com/tensorflow/tensorflow/pull/81982,[],[],
2713551073,pull_request,closed,,Add public visibility for exported files under third_party/tensorflow/compiler/mlir/lite/core/c.,"Add public visibility for exported files under third_party/tensorflow/compiler/mlir/lite/core/c.
",copybara-service[bot],2024-12-02 22:41:47+00:00,['junjiang-lab'],2024-12-03 22:47:20+00:00,2024-12-03 22:47:19+00:00,https://github.com/tensorflow/tensorflow/pull/81981,[],[],
2713545161,pull_request,closed,,"Move `tsl/platform:{subprocess,subprocess_test}` to XLA","Move `tsl/platform:{subprocess,subprocess_test}` to XLA
",copybara-service[bot],2024-12-02 22:37:28+00:00,['ddunl'],2024-12-04 01:51:39+00:00,2024-12-04 01:51:38+00:00,https://github.com/tensorflow/tensorflow/pull/81979,[],[],
2713492474,pull_request,closed,,Add propagation to caller when it's a conditional in HloDataflowAnalysis,"Add propagation to caller when it's a conditional in HloDataflowAnalysis
",copybara-service[bot],2024-12-02 22:03:30+00:00,[],2024-12-03 20:06:40+00:00,2024-12-03 20:06:39+00:00,https://github.com/tensorflow/tensorflow/pull/81978,[],[],
2713474364,pull_request,closed,,[XLA] Support splitting ragged all-to-all into async start and done.,"[XLA] Support splitting ragged all-to-all into async start and done.
",copybara-service[bot],2024-12-02 21:53:52+00:00,[],2024-12-06 07:06:10+00:00,2024-12-06 07:06:09+00:00,https://github.com/tensorflow/tensorflow/pull/81977,[],[],
2713465358,pull_request,open,,Update clone with new operands to handle ragged all-to-all.,"Update clone with new operands to handle ragged all-to-all.
",copybara-service[bot],2024-12-02 21:48:48+00:00,[],2024-12-02 21:48:48+00:00,,https://github.com/tensorflow/tensorflow/pull/81976,[],[],
2713409908,pull_request,closed,,Remove overly restricted checking in RemapPlan::Validate(),"Remove overly restricted checking in RemapPlan::Validate()
",copybara-service[bot],2024-12-02 21:18:17+00:00,[],2024-12-03 02:51:25+00:00,2024-12-03 02:51:25+00:00,https://github.com/tensorflow/tensorflow/pull/81975,[],[],
2713397246,pull_request,closed,,Guard `PipelinedP2PRewriter` behind xla_gpu_enable_experimental_pipeline_parallelism_opt,"Guard `PipelinedP2PRewriter` behind xla_gpu_enable_experimental_pipeline_parallelism_opt

The pass makes makes generally incorrect assumptions that causes scheduling to crash.
",copybara-service[bot],2024-12-02 21:12:04+00:00,['frgossen'],2024-12-04 22:37:43+00:00,2024-12-04 22:37:42+00:00,https://github.com/tensorflow/tensorflow/pull/81974,[],[],
2713391510,pull_request,closed,,Add option to force snappy compression.,"Add option to force snappy compression.
",copybara-service[bot],2024-12-02 21:08:56+00:00,[],2024-12-03 00:57:28+00:00,2024-12-03 00:57:27+00:00,https://github.com/tensorflow/tensorflow/pull/81973,[],[],
2713376466,pull_request,closed,,[XLA:Collective] Add common utility functions into,"[XLA:Collective] Add common utility functions into
  (i) hlo_sharding_util
  (ii). collective_util
",copybara-service[bot],2024-12-02 21:00:28+00:00,['Tongfei-Guo'],2024-12-04 23:20:42+00:00,2024-12-04 23:20:41+00:00,https://github.com/tensorflow/tensorflow/pull/81972,[],[],
2713333819,pull_request,closed,,Legalize TFL Embedding lookup Op to QNN Gather Op.,"Legalize TFL Embedding lookup Op to QNN Gather Op.
",copybara-service[bot],2024-12-02 20:39:10+00:00,[],2024-12-04 19:12:19+00:00,2024-12-04 19:12:18+00:00,https://github.com/tensorflow/tensorflow/pull/81971,[],[],
2713241005,pull_request,closed,,Integrate LLVM at llvm/llvm-project@1b03747ed85c,"Integrate LLVM at llvm/llvm-project@1b03747ed85c

Updates LLVM usage to match
[1b03747ed85c](https://github.com/llvm/llvm-project/commit/1b03747ed85c)
",copybara-service[bot],2024-12-02 20:13:09+00:00,[],2024-12-02 22:10:41+00:00,2024-12-02 22:10:40+00:00,https://github.com/tensorflow/tensorflow/pull/81970,[],[],
2713225097,pull_request,closed,,Internal CI/CD change,"Internal CI/CD change
",copybara-service[bot],2024-12-02 20:05:11+00:00,['changm'],2024-12-03 15:40:00+00:00,2024-12-03 15:39:58+00:00,https://github.com/tensorflow/tensorflow/pull/81969,[],[],
2713214100,pull_request,open,,Move test to public XLA:CPU test,"Move test to public XLA:CPU test
",copybara-service[bot],2024-12-02 19:59:51+00:00,['changm'],2024-12-02 19:59:51+00:00,,https://github.com/tensorflow/tensorflow/pull/81968,[],[],
2713207952,pull_request,closed,,Add a pattern to convert 1D FFT to 2D FFT. And legalize 2D mhlo.fft with fft_type=RFFT to TFL RFFT.,"Add a pattern to convert 1D FFT to 2D FFT. And legalize 2D mhlo.fft with fft_type=RFFT to TFL RFFT.

This is needed because TFLite only supports 2D RFFT.
",copybara-service[bot],2024-12-02 19:56:45+00:00,['vamsimanchala'],2024-12-03 22:21:18+00:00,2024-12-03 22:21:18+00:00,https://github.com/tensorflow/tensorflow/pull/81967,[],[],
2713195162,pull_request,closed,,[XLA] Update documentation with CompositeCall operation.,"[XLA] Update documentation with CompositeCall operation.
",copybara-service[bot],2024-12-02 19:50:44+00:00,['ghpvnist'],2024-12-02 20:34:10+00:00,2024-12-02 20:34:09+00:00,https://github.com/tensorflow/tensorflow/pull/81966,[],[],
2713071939,pull_request,closed,,[XLA:CPU] Use `ArrayTypeSwitch` to avoid having to write out the switch,"[XLA:CPU] Use `ArrayTypeSwitch` to avoid having to write out the switch

No functional change is intended, just avoids some boilerplate.
",copybara-service[bot],2024-12-02 19:09:28+00:00,['majnemer'],2024-12-03 03:30:39+00:00,2024-12-03 03:30:38+00:00,https://github.com/tensorflow/tensorflow/pull/81965,[],[],
2712954491,pull_request,closed,,Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.,"Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.

The name `NewHloTestBase` is potentially confusing beacuse it does not reflect
the use & purpose of this class. `HloRunnerAgnosticTestBase` is a lot clearer.
",copybara-service[bot],2024-12-02 18:42:57+00:00,[],2024-12-04 00:18:33+00:00,2024-12-04 00:18:32+00:00,https://github.com/tensorflow/tensorflow/pull/81963,[],[],
2712935212,pull_request,closed,,Add support for globally ranking batches based on priority.,"Add support for globally ranking batches based on priority.
",copybara-service[bot],2024-12-02 18:33:31+00:00,[],2024-12-02 21:12:58+00:00,2024-12-02 21:12:58+00:00,https://github.com/tensorflow/tensorflow/pull/81962,[],[],
2712923263,pull_request,closed,,Extend FindLiteRtSharedLibsHelper to work also on Android,"Extend FindLiteRtSharedLibsHelper to work also on Android
",copybara-service[bot],2024-12-02 18:28:03+00:00,[],2024-12-04 17:54:03+00:00,2024-12-04 17:54:02+00:00,https://github.com/tensorflow/tensorflow/pull/81961,[],[],
2712917902,pull_request,closed,,[XLA][IndexAnalysis] Move indexing_analysis to hlo/analysis.,"[XLA][IndexAnalysis] Move indexing_analysis to hlo/analysis.
",copybara-service[bot],2024-12-02 18:25:14+00:00,['pifon2a'],2024-12-03 08:13:31+00:00,2024-12-03 08:13:30+00:00,https://github.com/tensorflow/tensorflow/pull/81960,[],[],
2712880075,pull_request,closed,,[XLA:GPU] Minor reformat in collective_permute_cycle_decomposer.cc,"[XLA:GPU] Minor reformat in collective_permute_cycle_decomposer.cc
",copybara-service[bot],2024-12-02 18:06:17+00:00,['frgossen'],2024-12-02 20:23:17+00:00,2024-12-02 20:23:16+00:00,https://github.com/tensorflow/tensorflow/pull/81955,[],[],
2712875770,pull_request,closed,,Remove ifdefs from ir_emitter_unnested,"Remove ifdefs from ir_emitter_unnested

This removes a bunch of ifdefs from `ir_emitter_unnested.cc`. They are not needed anymore since now all the thunks don't depend on `gpu_types.h` and `gpu_stream.h` anymore.
",copybara-service[bot],2024-12-02 18:04:03+00:00,[],2024-12-04 08:30:06+00:00,2024-12-04 08:30:04+00:00,https://github.com/tensorflow/tensorflow/pull/81953,[],[],
2712869906,pull_request,closed,,Remove some #ifdefs in the StreamExecutor PjRt Compiler code,"Remove some #ifdefs in the StreamExecutor PjRt Compiler code

- Make StreamExecutorGpuCompiler's constructor take a stream_executor platform id
- Uses the Compiler registry to obtain the correct compiler instance
- Moves CUDA and ROCm AOT compiler registration into separate files

By using the compiler from the registry we also make the instance longer lived which makes all the caches work.
",copybara-service[bot],2024-12-02 18:01:17+00:00,[],2024-12-03 19:57:01+00:00,2024-12-03 19:57:01+00:00,https://github.com/tensorflow/tensorflow/pull/81952,[],[],
2712847302,pull_request,closed,,[Cleanup] Use absl::StrCat,"[Cleanup] Use absl::StrCat
",copybara-service[bot],2024-12-02 17:51:34+00:00,['frgossen'],2024-12-06 01:39:11+00:00,2024-12-06 01:39:10+00:00,https://github.com/tensorflow/tensorflow/pull/81951,[],[],
2712845283,pull_request,closed,,[Cleanup] Use HloPredicateIs(Not)Op,"[Cleanup] Use HloPredicateIs(Not)Op
",copybara-service[bot],2024-12-02 17:50:39+00:00,['frgossen'],2024-12-09 20:19:09+00:00,2024-12-09 20:19:08+00:00,https://github.com/tensorflow/tensorflow/pull/81950,[],[],
2712828288,pull_request,closed,,[Cleanup] Use push_back instead of emplace_back where appropriate,"[Cleanup] Use push_back instead of emplace_back where appropriate
",copybara-service[bot],2024-12-02 17:44:09+00:00,['frgossen'],2024-12-06 02:07:18+00:00,2024-12-06 02:07:17+00:00,https://github.com/tensorflow/tensorflow/pull/81949,[],[],
2712751519,pull_request,closed,,Un-deprecate `HloTestBase`.,"Un-deprecate `HloTestBase`.

In #78137 we previously deprecated HloTestBase in favor of `NewHloTestBase`.
`NewHloTestBase` is not a drop-in replacement for `HloTestBase` and requires
test-writers to adapt their tests to not relying on a specific runner
implementation.

There are still a few problems with switching everything to `HloRunnerPjRt`
right away, so we cannot yet offer an equivalent shim that can serve as a
convenient drop-in replacement.

For the time being, we have decided to therefore un-deprecate the class to avoid
further confusion. We will migrate all users of `HloTestBase` in due course.
",copybara-service[bot],2024-12-02 17:30:44+00:00,[],2024-12-02 19:23:51+00:00,2024-12-02 19:23:50+00:00,https://github.com/tensorflow/tensorflow/pull/81948,[],[],
2712710216,pull_request,closed,,Add fine grained visibility rules to XLA,"Add fine grained visibility rules to XLA
",copybara-service[bot],2024-12-02 17:14:20+00:00,['ezhulenev'],2024-12-02 18:35:00+00:00,2024-12-02 18:34:59+00:00,https://github.com/tensorflow/tensorflow/pull/81947,[],[],
2712675808,pull_request,closed,,[XLA:GPU] Do not compute suggested combiner threshold if there are no pipelined collectives in IR.,"[XLA:GPU] Do not compute suggested combiner threshold if there are no pipelined collectives in IR.
",copybara-service[bot],2024-12-02 17:02:21+00:00,[],2024-12-03 11:13:25+00:00,2024-12-03 11:13:24+00:00,https://github.com/tensorflow/tensorflow/pull/81946,[],[],
2712671715,pull_request,closed,,Integrate LLVM at llvm/llvm-project@21d27b3aabf3,"Integrate LLVM at llvm/llvm-project@21d27b3aabf3

Updates LLVM usage to match
[21d27b3aabf3](https://github.com/llvm/llvm-project/commit/21d27b3aabf3)
",copybara-service[bot],2024-12-02 17:00:29+00:00,[],2024-12-02 19:12:56+00:00,2024-12-02 19:12:56+00:00,https://github.com/tensorflow/tensorflow/pull/81945,[],[],
2712483869,pull_request,closed,,[XLA:GPU] Add RaggedAllToAllDecomposer pass.,"[XLA:GPU] Add RaggedAllToAllDecomposer pass.

The pass rewrites `ragged-all-to-all` as a regular `all-to-all`.

This rewrite is not intended to be the production implementation of `ragged-all-to-all`, because it uses much more memory than necessary.

Adding this pass had the following goals:
  * Unblock end-to-end integration of `ragged-all-to-all` in XLA:GPU.
  * Serve as a reference implementation.
  * Help write end-to-end tests

Once we have a proper implementation with NCCL, this pass should be removed.

Integration into the GPU compilation pipeline will follow.
",copybara-service[bot],2024-12-02 16:13:28+00:00,[],2024-12-03 19:28:51+00:00,2024-12-03 19:28:50+00:00,https://github.com/tensorflow/tensorflow/pull/81943,[],[],
2712327280,pull_request,closed,,[XLA:CPU] Implement the `xla::cpu::KernelRunner` internals to compile and run a provided `LlvmIrKernelSpec`.,"[XLA:CPU] Implement the `xla::cpu::KernelRunner` internals to compile and run a provided `LlvmIrKernelSpec`.
",copybara-service[bot],2024-12-02 15:29:09+00:00,[],2024-12-04 17:44:29+00:00,2024-12-04 17:44:28+00:00,https://github.com/tensorflow/tensorflow/pull/81942,[],[],
2712302492,pull_request,closed,,ROCm include and dependency cleanup,"ROCm include and dependency cleanup
",copybara-service[bot],2024-12-02 15:20:44+00:00,[],2024-12-02 22:36:58+00:00,2024-12-02 22:36:57+00:00,https://github.com/tensorflow/tensorflow/pull/81941,[],[],
2712286111,pull_request,open,,Internal CI/CD change,"Internal CI/CD change
",copybara-service[bot],2024-12-02 15:14:17+00:00,['changm'],2024-12-03 15:12:33+00:00,,https://github.com/tensorflow/tensorflow/pull/81940,[],[],
2712284963,pull_request,closed,,Integrate LLVM at llvm/llvm-project@2474cf7ad123,"Integrate LLVM at llvm/llvm-project@2474cf7ad123

Updates LLVM usage to match
[2474cf7ad123](https://github.com/llvm/llvm-project/commit/2474cf7ad123)
",copybara-service[bot],2024-12-02 15:13:50+00:00,[],2024-12-02 16:12:14+00:00,2024-12-02 16:12:13+00:00,https://github.com/tensorflow/tensorflow/pull/81939,[],[],
2712260945,pull_request,closed,,Move Internal XLA:CPU to public XLA:CPU API,"Move Internal XLA:CPU to public XLA:CPU API
",copybara-service[bot],2024-12-02 15:04:02+00:00,['changm'],2024-12-02 18:03:23+00:00,2024-12-02 18:03:22+00:00,https://github.com/tensorflow/tensorflow/pull/81938,[],[],
2712248921,pull_request,closed,,Rename Triton extension points passes with xla_triton prefix,"Rename Triton extension points passes with xla_triton prefix
",copybara-service[bot],2024-12-02 14:59:20+00:00,[],2024-12-03 18:40:00+00:00,2024-12-03 18:39:58+00:00,https://github.com/tensorflow/tensorflow/pull/81937,[],[],
2712244554,pull_request,closed,,Move test to public XLA:CPU test,"Move test to public XLA:CPU test
",copybara-service[bot],2024-12-02 14:57:52+00:00,['changm'],2024-12-02 20:14:37+00:00,2024-12-02 20:14:36+00:00,https://github.com/tensorflow/tensorflow/pull/81936,[],[],
2712231689,pull_request,closed,,Move Python XLA extension to public XLA:CPU API,"Move Python XLA extension to public XLA:CPU API
",copybara-service[bot],2024-12-02 14:53:59+00:00,['changm'],2024-12-02 17:51:32+00:00,2024-12-02 17:51:30+00:00,https://github.com/tensorflow/tensorflow/pull/81935,[],[],
2712230270,pull_request,closed,,CHLO defns for a ragged dot that permits ragged batch and contraction.,"CHLO defns for a ragged dot that permits ragged batch and contraction.
",copybara-service[bot],2024-12-02 14:53:25+00:00,[],2024-12-16 19:11:00+00:00,2024-12-16 19:10:59+00:00,https://github.com/tensorflow/tensorflow/pull/81934,[],[],
2712225090,pull_request,closed,,MHLO defns for a ragged dot that permits ragged batch and contraction.,"MHLO defns for a ragged dot that permits ragged batch and contraction.
",copybara-service[bot],2024-12-02 14:51:28+00:00,[],2024-12-05 19:32:45+00:00,2024-12-05 19:32:44+00:00,https://github.com/tensorflow/tensorflow/pull/81933,[],[],
2712191345,pull_request,closed,,Move XLA Tests to XLA:CPU public PJRT API,"Move XLA Tests to XLA:CPU public PJRT API
",copybara-service[bot],2024-12-02 14:43:41+00:00,['changm'],2024-12-02 17:40:54+00:00,2024-12-02 17:40:54+00:00,https://github.com/tensorflow/tensorflow/pull/81932,[],[],
2712142109,pull_request,closed,,Move test files to XLA:CPU public API,"Move test files to XLA:CPU public API
",copybara-service[bot],2024-12-02 14:38:27+00:00,['changm'],2024-12-02 17:46:15+00:00,2024-12-02 17:46:13+00:00,https://github.com/tensorflow/tensorflow/pull/81931,[],[],
2712140445,pull_request,closed,,[XLA] Move hlo_traversal.h to hlo/utils.,"[XLA] Move hlo_traversal.h to hlo/utils.
",copybara-service[bot],2024-12-02 14:37:58+00:00,['pifon2a'],2024-12-02 15:59:39+00:00,2024-12-02 15:59:38+00:00,https://github.com/tensorflow/tensorflow/pull/81930,[],[],
2712136447,pull_request,open,,Integrate LLVM at llvm/llvm-project@c1dcf75a7cf8,"Integrate LLVM at llvm/llvm-project@c1dcf75a7cf8

Updates LLVM usage to match
[c1dcf75a7cf8](https://github.com/llvm/llvm-project/commit/c1dcf75a7cf8)
",copybara-service[bot],2024-12-02 14:36:52+00:00,[],2024-12-02 14:36:52+00:00,,https://github.com/tensorflow/tensorflow/pull/81929,[],[],
2712082164,pull_request,closed,,[XLA:CPU] Disable a subset of failing test cases in complex_unary_op_test on ARM.,"[XLA:CPU] Disable a subset of failing test cases in complex_unary_op_test on ARM.
",copybara-service[bot],2024-12-02 14:16:29+00:00,[],2024-12-02 15:28:12+00:00,2024-12-02 15:28:11+00:00,https://github.com/tensorflow/tensorflow/pull/81928,[],[],
2712067419,pull_request,closed,,[XLA:GPU] Fix typo in custom_kernel_fusion.h,"[XLA:GPU] Fix typo in custom_kernel_fusion.h
",copybara-service[bot],2024-12-02 14:10:46+00:00,['pifon2a'],2024-12-02 14:42:49+00:00,2024-12-02 14:42:48+00:00,https://github.com/tensorflow/tensorflow/pull/81927,[],[],
2711997585,pull_request,closed,,Prepare custom call tests for API_VERSION_ORIGINAL removal,"Prepare custom call tests for API_VERSION_ORIGINAL removal
",copybara-service[bot],2024-12-02 13:44:21+00:00,[],2024-12-02 17:31:56+00:00,2024-12-02 17:31:55+00:00,https://github.com/tensorflow/tensorflow/pull/81926,[],"[{'comment_id': 2511583560, 'issue_id': 2711997585, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81926/checks?check_run_id=33789773590) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 12, 2, 13, 44, 27, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-12-02 13:44:27 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81926/checks?check_run_id=33789773590) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2711990786,pull_request,closed,,PR #19655: [ROCm] Make MLIR Math dialect lowering more deterministic,"PR #19655: [ROCm] Make MLIR Math dialect lowering more deterministic

Imported from GitHub PR https://github.com/openxla/xla/pull/19655

First apply patterns from GpuToROCDLConversionPatterns then do the cleanup with MathToLLVMConversionPatterns
Copybara import of the project:

--
377d5a1f1a624196eef3a241c65c388ba886e5ef by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:

[ROCm] Make MLIR Math dialect lowering more deterministic

First apply patterns from GpuToROCDLConversionPatterns then
do the cleanup with MathToLLVMConversionPatterns

Merging this change closes #19655

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19655 from ROCm:ci_mlir_math_fix 377d5a1f1a624196eef3a241c65c388ba886e5ef
",copybara-service[bot],2024-12-02 13:41:55+00:00,[],2024-12-02 14:15:48+00:00,2024-12-02 14:15:46+00:00,https://github.com/tensorflow/tensorflow/pull/81925,[],[],
2711980166,pull_request,closed,,Provide more CUDA diagnostic information,"Provide more CUDA diagnostic information

* CUDA_VISIBLE_DEVICES can cause cuInit to fail, print it
* Mention verbose logging
* Clarify the error in the DSO search - not finding it is not necessarily bad
* Add a basic test to make sure the diagnostic log at least doesn't crash
",copybara-service[bot],2024-12-02 13:39:19+00:00,[],2024-12-02 15:41:17+00:00,2024-12-02 15:41:16+00:00,https://github.com/tensorflow/tensorflow/pull/81924,[],[],
2711883305,pull_request,closed,,[XLA:GPU] Use Cub RaddixSort for bf16 sorts in Numpy order (NaNs go last).,"[XLA:GPU] Use Cub RaddixSort for bf16 sorts in Numpy order (NaNs go last).

The support is limited to bf16. Generalizing this to other dtypes is straightforward and will follow in a separate change.
",copybara-service[bot],2024-12-02 13:17:12+00:00,['thomasjoerg'],2024-12-03 09:24:48+00:00,2024-12-03 09:24:47+00:00,https://github.com/tensorflow/tensorflow/pull/81923,[],[],
2711876936,pull_request,closed,,PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,"PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default

Imported from GitHub PR https://github.com/openxla/xla/pull/19451

We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability.
Copybara import of the project:

--
f8e88e6b22ec4702081a4949b60fd669bead8c33 by Tj Xu <tjx@nvidia.com>:

Turn xla_gpu_multi_streamed_windowed_einsum on by default

--
fbd2f328954a2a66be7a8cbd1e4570908b3424b7 by TJ Xu <tjx@nvidia.com>:

Add an option to StreamAttributeAnnotator to skip annotating copy-start
and async DUS
Don't annotate copy-start and async DUS when the pass is run before
remat

--
43f9dbbc55dd1eb84c3b4452e5274b2571cf78cd by TJ Xu <tjx@nvidia.com>:

Remove the option to skip annotating copy start and inpect if the module
has schedule

Merging this change closes #19451

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag 43f9dbbc55dd1eb84c3b4452e5274b2571cf78cd
",copybara-service[bot],2024-12-02 13:14:37+00:00,[],2024-12-09 15:08:38+00:00,2024-12-09 15:08:37+00:00,https://github.com/tensorflow/tensorflow/pull/81922,[],[],
2711831356,pull_request,closed,,Fix issue #70730,Improve regular expression for filtering `neon` and `sse` related sources. The improved expression avoids missing files in case the absolute path contains the terms `neon` or `sse` which leads to linker errors. This resolves issue #70730.,pasweistorz,2024-12-02 12:56:37+00:00,['gbaned'],2025-01-09 09:55:07+00:00,2025-01-09 09:42:58+00:00,https://github.com/tensorflow/tensorflow/pull/81920,"[('awaiting review', 'Pull request awaiting review'), ('comp:lite', 'TF Lite related issues'), ('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2511471114, 'issue_id': 2711831356, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81920/checks?check_run_id=33787182040) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 12, 2, 12, 56, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562244431, 'issue_id': 2711831356, 'author': 'keerthanakadiri', 'body': 'Hi @mattsoulanille, Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 12, 26, 7, 17, 46, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-12-02 12:56:42 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/81920/checks?check_run_id=33787182040) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

keerthanakadiri on (2024-12-26 07:17:46 UTC): Hi @mattsoulanille, Can you please review this PR? Thank you !

"
2711717385,pull_request,open,,Add link to openxla.org homepage to README,"Add link to openxla.org homepage to README
",copybara-service[bot],2024-12-02 12:08:25+00:00,[],2024-12-02 12:08:25+00:00,,https://github.com/tensorflow/tensorflow/pull/81919,[],[],
2711571631,pull_request,closed,,Integrate LLVM at llvm/llvm-project@fe1c4f0106fe,"Integrate LLVM at llvm/llvm-project@fe1c4f0106fe

Updates LLVM usage to match
[fe1c4f0106fe](https://github.com/llvm/llvm-project/commit/fe1c4f0106fe)
",copybara-service[bot],2024-12-02 11:25:52+00:00,[],2024-12-02 13:24:38+00:00,2024-12-02 13:24:37+00:00,https://github.com/tensorflow/tensorflow/pull/81918,[],[],
2711409097,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 10:39:28+00:00,[],2024-12-02 10:39:28+00:00,,https://github.com/tensorflow/tensorflow/pull/81916,[],[],
2711273920,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:44:57+00:00,[],2024-12-04 10:25:12+00:00,2024-12-04 10:25:11+00:00,https://github.com/tensorflow/tensorflow/pull/81915,[],[],
2711254366,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:41:10+00:00,[],2024-12-05 08:55:53+00:00,2024-12-05 08:55:52+00:00,https://github.com/tensorflow/tensorflow/pull/81914,[],[],
2711247826,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:40:28+00:00,[],2024-12-02 09:40:28+00:00,,https://github.com/tensorflow/tensorflow/pull/81913,[],[],
2711241714,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:39:54+00:00,[],2024-12-04 06:45:39+00:00,,https://github.com/tensorflow/tensorflow/pull/81912,[],[],
2711202805,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:34:39+00:00,[],2024-12-02 09:34:39+00:00,,https://github.com/tensorflow/tensorflow/pull/81911,[],[],
2711198376,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:32:52+00:00,[],2024-12-02 11:47:48+00:00,2024-12-02 11:47:47+00:00,https://github.com/tensorflow/tensorflow/pull/81910,[],[],
2711197978,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:32:42+00:00,[],2024-12-05 07:17:43+00:00,2024-12-05 07:17:42+00:00,https://github.com/tensorflow/tensorflow/pull/81909,[],[],
2711195297,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:31:32+00:00,[],2024-12-07 09:38:30+00:00,2024-12-07 09:38:30+00:00,https://github.com/tensorflow/tensorflow/pull/81908,[],[],
2711187294,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:28:13+00:00,[],2024-12-05 05:17:37+00:00,,https://github.com/tensorflow/tensorflow/pull/81907,[],[],
2711184794,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:27:12+00:00,[],2024-12-02 09:27:12+00:00,,https://github.com/tensorflow/tensorflow/pull/81906,[],[],
2711183946,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:26:52+00:00,[],2024-12-02 10:14:01+00:00,,https://github.com/tensorflow/tensorflow/pull/81905,[],[],
2711180367,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:25:27+00:00,[],2024-12-02 10:13:35+00:00,,https://github.com/tensorflow/tensorflow/pull/81904,[],[],
2711180016,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:25:18+00:00,[],2024-12-02 09:25:18+00:00,,https://github.com/tensorflow/tensorflow/pull/81903,[],[],
2711171183,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:21:31+00:00,[],2024-12-04 13:07:10+00:00,,https://github.com/tensorflow/tensorflow/pull/81902,[],[],
2711163881,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 09:18:11+00:00,[],2024-12-03 10:29:15+00:00,2024-12-03 10:29:15+00:00,https://github.com/tensorflow/tensorflow/pull/81901,[],[],
2710965693,pull_request,closed,,Integrate LLVM at llvm/llvm-project@b22cc5a650de,"Integrate LLVM at llvm/llvm-project@b22cc5a650de

Updates LLVM usage to match
[b22cc5a650de](https://github.com/llvm/llvm-project/commit/b22cc5a650de)
",copybara-service[bot],2024-12-02 08:09:57+00:00,[],2024-12-02 09:48:03+00:00,2024-12-02 09:48:03+00:00,https://github.com/tensorflow/tensorflow/pull/81899,[],[],
2710962006,pull_request,open,,Integrate LLVM at llvm/llvm-project@010317e1731d,"Integrate LLVM at llvm/llvm-project@010317e1731d

Updates LLVM usage to match
[010317e1731d](https://github.com/llvm/llvm-project/commit/010317e1731d)
",copybara-service[bot],2024-12-02 08:07:55+00:00,[],2024-12-02 08:07:55+00:00,,https://github.com/tensorflow/tensorflow/pull/81898,[],[],
2710810006,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 07:14:42+00:00,[],2024-12-02 07:14:42+00:00,,https://github.com/tensorflow/tensorflow/pull/81897,[],[],
2710769096,pull_request,closed,,Add AssembleCompilationProvider routine,"Add AssembleCompilationProvider routine

`AssembleCompilationProvider` checks the availability of all the different PTX compilation methods and takes the user's preferences (DebugOptions) into account to create the best suitable PTX CompilationProvider.

Unfortunately the logic is rather convoluted since it mimics the current behaviour in NVPTXCompiler. More cleanup can be done at a later point in small steps.
",copybara-service[bot],2024-12-02 06:54:15+00:00,[],2024-12-03 08:01:11+00:00,2024-12-03 08:01:09+00:00,https://github.com/tensorflow/tensorflow/pull/81894,[],[],
2710395712,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 04:09:09+00:00,[],2024-12-04 08:02:22+00:00,,https://github.com/tensorflow/tensorflow/pull/81893,[],[],
2710267064,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:53:05+00:00,[],2024-12-02 02:53:05+00:00,,https://github.com/tensorflow/tensorflow/pull/81892,[],[],
2710256397,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:43:04+00:00,[],2024-12-02 02:43:04+00:00,,https://github.com/tensorflow/tensorflow/pull/81891,[],[],
2710253998,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:40:58+00:00,[],2024-12-02 02:40:58+00:00,,https://github.com/tensorflow/tensorflow/pull/81890,[],[],
2710247867,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:35:45+00:00,[],2024-12-02 02:35:45+00:00,,https://github.com/tensorflow/tensorflow/pull/81889,[],[],
2710243111,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:31:23+00:00,[],2024-12-02 02:31:23+00:00,,https://github.com/tensorflow/tensorflow/pull/81888,[],[],
2710235092,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-02 02:23:43+00:00,[],2024-12-02 02:23:43+00:00,,https://github.com/tensorflow/tensorflow/pull/81887,[],[],
2709923868,pull_request,closed,,[xla:collectives] NFC: Move CliqueIsCallback alias to gpu_executable_run_options,"[xla:collectives] NFC: Move CliqueIsCallback alias to gpu_executable_run_options
",copybara-service[bot],2024-12-01 21:45:20+00:00,['ezhulenev'],2024-12-03 09:53:16+00:00,2024-12-03 09:53:14+00:00,https://github.com/tensorflow/tensorflow/pull/81886,[],[],
2709922754,pull_request,closed,,[xla:collectives] NFC: Move NcclCliqueKey to GpuCliqueKey,"[xla:collectives] NFC: Move NcclCliqueKey to GpuCliqueKey
",copybara-service[bot],2024-12-01 21:43:22+00:00,['ezhulenev'],2024-12-03 10:54:10+00:00,2024-12-03 10:54:08+00:00,https://github.com/tensorflow/tensorflow/pull/81885,[],[],
2709922598,pull_request,closed,,[xla:collectives] NFC: Introduce GpuCollectives interface,"[xla:collectives] NFC: Introduce GpuCollectives interface
",copybara-service[bot],2024-12-01 21:43:06+00:00,['ezhulenev'],2024-12-03 09:03:46+00:00,2024-12-03 09:03:45+00:00,https://github.com/tensorflow/tensorflow/pull/81884,[],[],
2709922021,pull_request,closed,,[xla:collectives] NFC: Remove NcclCliqueKey alias,"[xla:collectives] NFC: Remove NcclCliqueKey alias
",copybara-service[bot],2024-12-01 21:42:08+00:00,['ezhulenev'],2024-12-03 19:00:37+00:00,2024-12-03 19:00:36+00:00,https://github.com/tensorflow/tensorflow/pull/81883,[],[],
2709920561,pull_request,closed,,[xla:collectives] NFC: Rename NcclCommHandleWrapper to CommunicatorHandle,"[xla:collectives] NFC: Rename NcclCommHandleWrapper to CommunicatorHandle
",copybara-service[bot],2024-12-01 21:40:11+00:00,['ezhulenev'],2024-12-03 07:38:14+00:00,2024-12-03 07:38:14+00:00,https://github.com/tensorflow/tensorflow/pull/81882,[],[],
2709431063,pull_request,closed,,[XLA:GPU][Emitters] Use DeviceDescription in lower_tensors.cc.,"[XLA:GPU][Emitters] Use DeviceDescription in lower_tensors.cc.

I tried to initialize se::DeviceDescription in the LowerTensorsPass constructor, but TableGen does not like it. I will try to fix it in a follow-up.
",copybara-service[bot],2024-12-01 16:12:57+00:00,['pifon2a'],2024-12-03 19:12:21+00:00,2024-12-03 19:12:20+00:00,https://github.com/tensorflow/tensorflow/pull/81852,[],[],
2708998907,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 10:53:06+00:00,[],2024-12-01 10:53:06+00:00,,https://github.com/tensorflow/tensorflow/pull/81734,[],[],
2708938373,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 10:34:24+00:00,[],2024-12-01 10:34:24+00:00,,https://github.com/tensorflow/tensorflow/pull/81731,[],[],
2708930708,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 10:20:31+00:00,[],2024-12-07 08:38:10+00:00,2024-12-07 08:38:10+00:00,https://github.com/tensorflow/tensorflow/pull/81729,[],[],
2708930531,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 10:20:11+00:00,[],2024-12-07 10:48:32+00:00,2024-12-07 10:48:31+00:00,https://github.com/tensorflow/tensorflow/pull/81728,[],[],
2708919287,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 10:02:30+00:00,[],2024-12-01 20:10:31+00:00,2024-12-01 20:10:30+00:00,https://github.com/tensorflow/tensorflow/pull/81727,[],[],
2708909452,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 09:49:15+00:00,[],2024-12-03 08:43:02+00:00,2024-12-03 08:43:01+00:00,https://github.com/tensorflow/tensorflow/pull/81726,[],[],
2708863664,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 09:31:16+00:00,[],2024-12-01 09:31:16+00:00,,https://github.com/tensorflow/tensorflow/pull/81725,[],[],
2708827795,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 08:53:57+00:00,[],2024-12-05 07:02:23+00:00,2024-12-05 07:02:22+00:00,https://github.com/tensorflow/tensorflow/pull/81723,[],[],
2708827581,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 08:53:30+00:00,[],2024-12-01 08:53:30+00:00,,https://github.com/tensorflow/tensorflow/pull/81722,[],[],
2708755304,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 08:12:07+00:00,[],2024-12-02 00:23:09+00:00,2024-12-02 00:23:08+00:00,https://github.com/tensorflow/tensorflow/pull/81721,[],[],
2708755276,pull_request,closed,,Bump ubuntu from `99c3519` to `278628f` in /tensorflow/tools/gcs_test,"Bumps ubuntu from `99c3519` to `278628f`.


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ubuntu&package-manager=docker&previous-version=24.04&new-version=24.04)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>",dependabot[bot],2024-12-01 08:12:03+00:00,['gbaned'],2024-12-02 07:47:00+00:00,2024-12-02 07:46:59+00:00,https://github.com/tensorflow/tensorflow/pull/81720,"[('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small'), ('dependencies', 'Pull requests that update a dependency file'), ('docker', 'Pull requests that update Docker code')]",[],
2708754696,pull_request,closed,,Bump the github-actions group with 2 updates,"Bumps the github-actions group with 2 updates: [github/codeql-action](https://github.com/github/codeql-action) and [docker/build-push-action](https://github.com/docker/build-push-action).

Updates `github/codeql-action` from 3.27.0 to 3.27.5
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/releases"">github/codeql-action's releases</a>.</em></p>
<blockquote>
<h2>v3.27.5</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.5 - 19 Nov 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.5/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.4</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.4 - 14 Nov 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.4/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.3</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.3 - 12 Nov 2024</h2>
<p>No user facing changes.</p>
<p>See the full <a href=""https://github.com/github/codeql-action/blob/v3.27.3/CHANGELOG.md"">CHANGELOG.md</a> for more information.</p>
<h2>v3.27.2</h2>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>3.27.2 - 12 Nov 2024</h2>
<ul>
<li>Fixed an issue where setting up the CodeQL tools would sometimes fail with the message &quot;Invalid value 'undefined' for header 'authorization'&quot;. <a href=""https://redirect.github.com/github/codeql-action/pull/2590"">#2590</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/github/codeql-action/blob/main/CHANGELOG.md"">github/codeql-action's changelog</a>.</em></p>
<blockquote>
<h1>CodeQL Action Changelog</h1>
<p>See the <a href=""https://github.com/github/codeql-action/releases"">releases page</a> for the relevant changes to the CodeQL CLI and language packs.</p>
<p>Note that the only difference between <code>v2</code> and <code>v3</code> of the CodeQL Action is the node version they support, with <code>v3</code> running on node 20 while we continue to release <code>v2</code> to support running on node 16. For example <code>3.22.11</code> was the first <code>v3</code> release and is functionally identical to <code>2.22.11</code>. This approach ensures an easy way to track exactly which features are included in different versions, indicated by the minor and patch version numbers.</p>
<h2>[UNRELEASED]</h2>
<p>No user facing changes.</p>
<h2>3.27.5 - 19 Nov 2024</h2>
<p>No user facing changes.</p>
<h2>3.27.4 - 14 Nov 2024</h2>
<p>No user facing changes.</p>
<h2>3.27.3 - 12 Nov 2024</h2>
<p>No user facing changes.</p>
<h2>3.27.2 - 12 Nov 2024</h2>
<ul>
<li>Fixed an issue where setting up the CodeQL tools would sometimes fail with the message &quot;Invalid value 'undefined' for header 'authorization'&quot;. <a href=""https://redirect.github.com/github/codeql-action/pull/2590"">#2590</a></li>
</ul>
<h2>3.27.1 - 08 Nov 2024</h2>
<ul>
<li>The CodeQL Action now downloads bundles compressed using Zstandard on GitHub Enterprise Server when using Linux or macOS runners. This speeds up the installation of the CodeQL tools. This feature is already available to GitHub.com users. <a href=""https://redirect.github.com/github/codeql-action/pull/2573"">#2573</a></li>
<li>Update default CodeQL bundle version to 2.19.3. <a href=""https://redirect.github.com/github/codeql-action/pull/2576"">#2576</a></li>
</ul>
<h2>3.27.0 - 22 Oct 2024</h2>
<ul>
<li>Bump the minimum CodeQL bundle version to 2.14.6. <a href=""https://redirect.github.com/github/codeql-action/pull/2549"">#2549</a></li>
<li>Fix an issue where the <code>upload-sarif</code> Action would fail with &quot;upload-sarif post-action step failed: Input required and not supplied: token&quot; when called in a composite Action that had a different set of inputs to the ones expected by the <code>upload-sarif</code> Action. <a href=""https://redirect.github.com/github/codeql-action/pull/2557"">#2557</a></li>
<li>Update default CodeQL bundle version to 2.19.2. <a href=""https://redirect.github.com/github/codeql-action/pull/2552"">#2552</a></li>
</ul>
<h2>3.26.13 - 14 Oct 2024</h2>
<p>No user facing changes.</p>
<h2>3.26.12 - 07 Oct 2024</h2>
<ul>
<li>
<p><em>Upcoming breaking change</em>: Add a deprecation warning for customers using CodeQL version 2.14.5 and earlier. These versions of CodeQL were discontinued on 24 September 2024 alongside GitHub Enterprise Server 3.10, and will be unsupported by CodeQL Action versions 3.27.0 and later and versions 2.27.0 and later. <a href=""https://redirect.github.com/github/codeql-action/pull/2520"">#2520</a></p>
<ul>
<li>
<p>If you are using one of these versions, please update to CodeQL CLI version 2.14.6 or later. For instance, if you have specified a custom version of the CLI using the 'tools' input to the 'init' Action, you can remove this input to use the default version.</p>
</li>
<li>
<p>Alternatively, if you want to continue using a version of the CodeQL CLI between 2.13.5 and 2.14.5, you can replace <code>github/codeql-action/*@v3</code> by <code>github/codeql-action/*@v3.26.11</code> and <code>github/codeql-action/*@v2</code> by <code>github/codeql-action/*@v2.26.11</code> in your code scanning workflow to ensure you continue using this version of the CodeQL Action.</p>
</li>
</ul>
</li>
</ul>
<h2>3.26.11 - 03 Oct 2024</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/github/codeql-action/commit/f09c1c0a94de965c15400f5634aa42fac8fb8f88""><code>f09c1c0</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2616"">#2616</a> from github/update-v3.27.5-a6c8729a5</li>
<li><a href=""https://github.com/github/codeql-action/commit/67b73eaba559c7e6913377065b0362ccbfc94e87""><code>67b73ea</code></a> Update changelog for v3.27.5</li>
<li><a href=""https://github.com/github/codeql-action/commit/a6c8729a5d7573eb8d440e52a9645ce4db61d97c""><code>a6c8729</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2614"">#2614</a> from github/marcogario/per-platform-proxy</li>
<li><a href=""https://github.com/github/codeql-action/commit/8f3b48727ff1b076c28967a258b95fcee30a3a48""><code>8f3b487</code></a> Start-proxy: Fetch OS specific binary</li>
<li><a href=""https://github.com/github/codeql-action/commit/cba5fb58d4f85affaf03eb9da32f5b6c9d76838b""><code>cba5fb5</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2613"">#2613</a> from github/dependabot/npm_and_yarn/npm_and_yarn-018...</li>
<li><a href=""https://github.com/github/codeql-action/commit/e782c3a145d9946aba8fa390e406acbe4e4c05c5""><code>e782c3a</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2612"">#2612</a> from github/angelapwen/report-linux-runner-release</li>
<li><a href=""https://github.com/github/codeql-action/commit/db6788195b646f87b3d1c616b0c14a6d5b7fa9a6""><code>db67881</code></a> Update checked-in dependencies</li>
<li><a href=""https://github.com/github/codeql-action/commit/ecde4d232d18cf2dba6c1a6b76810332abff736f""><code>ecde4d2</code></a> Bump cross-spawn from 7.0.3 to 7.0.6 in the npm_and_yarn group</li>
<li><a href=""https://github.com/github/codeql-action/commit/e3c67a01d31d9c173ba5ffccc9d0f275540d99de""><code>e3c67a0</code></a> Merge pull request <a href=""https://redirect.github.com/github/codeql-action/issues/2610"">#2610</a> from github/dependabot/npm_and_yarn/npm-d2ca52e617</li>
<li><a href=""https://github.com/github/codeql-action/commit/f9ada54538b47b6db28c4d11f53848689968909e""><code>f9ada54</code></a> Telemetry: report OS release for GitHub-hosted Linux runners</li>
<li>Additional commits viewable in <a href=""https://github.com/github/codeql-action/compare/662472033e021d55d94146f66f6058822b0b39fd...f09c1c0a94de965c15400f5634aa42fac8fb8f88"">compare view</a></li>
</ul>
</details>
<br />

Updates `docker/build-push-action` from 6.9.0 to 6.10.0
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/docker/build-push-action/releases"">docker/build-push-action's releases</a>.</em></p>
<blockquote>
<h2>v6.10.0</h2>
<ul>
<li>Add <code>call</code> input to set method for evaluating build by <a href=""https://github.com/crazy-max""><code>@​crazy-max</code></a> in <a href=""https://redirect.github.com/docker/build-push-action/pull/1265"">docker/build-push-action#1265</a></li>
<li>Bump <code>@​actions/core</code> from 1.10.1 to 1.11.1 in <a href=""https://redirect.github.com/docker/build-push-action/pull/1238"">docker/build-push-action#1238</a></li>
<li>Bump <code>@​docker/actions-toolkit</code> from 0.39.0 to 0.46.0 in <a href=""https://redirect.github.com/docker/build-push-action/pull/1268"">docker/build-push-action#1268</a></li>
<li>Bump cross-spawn from 7.0.3 to 7.0.6 in <a href=""https://redirect.github.com/docker/build-push-action/pull/1261"">docker/build-push-action#1261</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/docker/build-push-action/compare/v6.9.0...v6.10.0"">https://github.com/docker/build-push-action/compare/v6.9.0...v6.10.0</a></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/docker/build-push-action/commit/48aba3b46d1b1fec4febb7c5d0c644b249a11355""><code>48aba3b</code></a> Merge pull request <a href=""https://redirect.github.com/docker/build-push-action/issues/1268"">#1268</a> from docker/dependabot/npm_and_yarn/docker/actions-t...</li>
<li><a href=""https://github.com/docker/build-push-action/commit/678328cf8e3098e9f2f1d936ae548c9479d6df42""><code>678328c</code></a> chore: update generated content</li>
<li><a href=""https://github.com/docker/build-push-action/commit/cdf0a37e6f1233dd28f23c10211c33e67a7bec71""><code>cdf0a37</code></a> chore(deps): Bump <code>@​docker/actions-toolkit</code> from 0.39.0 to 0.46.0</li>
<li><a href=""https://github.com/docker/build-push-action/commit/d719b79de1e8e269d4fcc5a80898196da2d0c5b6""><code>d719b79</code></a> Merge pull request <a href=""https://redirect.github.com/docker/build-push-action/issues/1238"">#1238</a> from docker/dependabot/npm_and_yarn/actions/core-1.11.1</li>
<li><a href=""https://github.com/docker/build-push-action/commit/c333dfd43deaf1620b3379589ac39a11be13c72c""><code>c333dfd</code></a> chore: update generated content</li>
<li><a href=""https://github.com/docker/build-push-action/commit/6b56a4c3f83c50fa6630a247100ee2d2905aaa5f""><code>6b56a4c</code></a> chore(deps): Bump <code>@​actions/core</code> from 1.10.1 to 1.11.1</li>
<li><a href=""https://github.com/docker/build-push-action/commit/92fb0d73b623b7ebf48bd248bd465b6a5cbe7c60""><code>92fb0d7</code></a> Merge pull request <a href=""https://redirect.github.com/docker/build-push-action/issues/1259"">#1259</a> from docker/dependabot/github_actions/codecov/codeco...</li>
<li><a href=""https://github.com/docker/build-push-action/commit/40532c5d6fa1c2aef883289629dcadf2e77165a4""><code>40532c5</code></a> ci: fix deprecated input for codecov-action</li>
<li><a href=""https://github.com/docker/build-push-action/commit/70dd95342711510431dc0bd25494df47756d27c3""><code>70dd953</code></a> Merge pull request <a href=""https://redirect.github.com/docker/build-push-action/issues/1267"">#1267</a> from crazy-max/fix-allow</li>
<li><a href=""https://github.com/docker/build-push-action/commit/41b4e8020e9e4e2a35082a19644371a54db50097""><code>41b4e80</code></a> Merge pull request <a href=""https://redirect.github.com/docker/build-push-action/issues/1261"">#1261</a> from docker/dependabot/npm_and_yarn/cross-spawn-7.0.6</li>
<li>Additional commits viewable in <a href=""https://github.com/docker/build-push-action/compare/4f58ea79222b3b9dc2c8bbdd6debcef730109a75...48aba3b46d1b1fec4febb7c5d0c644b249a11355"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore <dependency name> major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)
- `@dependabot ignore <dependency name> minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)
- `@dependabot ignore <dependency name>` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)
- `@dependabot unignore <dependency name>` will remove all of the ignore conditions of the specified dependency
- `@dependabot unignore <dependency name> <ignore condition>` will remove the ignore condition of the specified dependency and ignore conditions


</details>",dependabot[bot],2024-12-01 08:10:53+00:00,['gbaned'],2024-12-02 06:14:30+00:00,2024-12-02 06:14:29+00:00,https://github.com/tensorflow/tensorflow/pull/81719,"[('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small'), ('dependencies', 'Pull requests that update a dependency file'), ('github_actions', 'Pull requests that update GitHub Actions code')]",[],
2708751302,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 08:03:30+00:00,[],2024-12-01 08:03:30+00:00,,https://github.com/tensorflow/tensorflow/pull/81718,[],[],
2708734029,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 07:37:02+00:00,[],2024-12-01 16:41:28+00:00,2024-12-01 16:41:27+00:00,https://github.com/tensorflow/tensorflow/pull/81717,[],[],
2708682967,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 07:26:15+00:00,[],2024-12-01 07:26:15+00:00,,https://github.com/tensorflow/tensorflow/pull/81716,[],[],
2708682586,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 07:25:30+00:00,[],2024-12-03 06:29:35+00:00,2024-12-03 06:29:34+00:00,https://github.com/tensorflow/tensorflow/pull/81715,[],[],
2708675818,pull_request,closed,,[ODML] Add symbol DCE pass after composite lowering pass.,"[ODML] Add symbol DCE pass after composite lowering pass.

This is to remove unused symbols after composite lowering pass.
",copybara-service[bot],2024-12-01 07:13:59+00:00,['vamsimanchala'],2024-12-02 19:35:15+00:00,2024-12-02 19:35:14+00:00,https://github.com/tensorflow/tensorflow/pull/81714,[],[],
2708665930,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 06:55:01+00:00,[],2024-12-05 09:49:53+00:00,2024-12-05 09:49:52+00:00,https://github.com/tensorflow/tensorflow/pull/81713,[],[],
2708508249,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:54:30+00:00,[],2024-12-01 07:34:04+00:00,,https://github.com/tensorflow/tensorflow/pull/81705,[],[],
2708438580,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:11:15+00:00,[],2024-12-04 08:05:05+00:00,,https://github.com/tensorflow/tensorflow/pull/81704,[],[],
2708437818,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:09:39+00:00,[],2024-12-02 04:37:35+00:00,2024-12-02 04:37:34+00:00,https://github.com/tensorflow/tensorflow/pull/81703,[],[],
2708437322,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:08:28+00:00,[],2024-12-06 08:13:09+00:00,2024-12-06 08:13:08+00:00,https://github.com/tensorflow/tensorflow/pull/81702,[],[],
2708437110,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:07:58+00:00,[],2024-12-02 08:27:52+00:00,2024-12-02 08:27:51+00:00,https://github.com/tensorflow/tensorflow/pull/81701,[],[],
2708436171,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:05:37+00:00,[],2024-12-04 06:10:30+00:00,,https://github.com/tensorflow/tensorflow/pull/81700,[],[],
2708436106,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:05:28+00:00,[],2024-12-04 07:04:27+00:00,,https://github.com/tensorflow/tensorflow/pull/81699,[],[],
2708436092,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:05:26+00:00,[],2024-12-03 10:05:46+00:00,2024-12-03 10:05:45+00:00,https://github.com/tensorflow/tensorflow/pull/81698,[],[],
2708436079,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:05:25+00:00,[],2024-12-05 08:10:07+00:00,2024-12-05 08:10:06+00:00,https://github.com/tensorflow/tensorflow/pull/81697,[],[],
2708435039,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:03:50+00:00,[],2024-12-01 04:03:50+00:00,,https://github.com/tensorflow/tensorflow/pull/81696,[],[],
2708433403,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:01:27+00:00,[],2024-12-01 06:53:33+00:00,,https://github.com/tensorflow/tensorflow/pull/81695,[],[],
2708432635,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-12-01 04:00:29+00:00,[],2024-12-07 10:04:23+00:00,,https://github.com/tensorflow/tensorflow/pull/81694,[],[],
