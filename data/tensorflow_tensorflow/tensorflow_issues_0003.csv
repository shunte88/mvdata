id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2467052283,issue,closed,completed,"tf.numpy_function doesn't work with keras tensors in tf2.17,  but previous versions do","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I need to use tf.numpy_function to create some custom metric in model training. But it throw out an error in TF2.17.
The same code runs fine in TF2.11

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model


def calc_accuracy(y_true, y_pred):
    # Calculate the accuracy
    return np.mean(np.equal(y_true, np.round(y_pred)).astype(np.float32))


# Define the model using the Functional API
x = tf.keras.Input(shape=(20,),name=""x"", dtype=tf.float32)
y = tf.keras.Input(shape=(), name=""y"", dtype=tf.float32)
tmp = Dense(64, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(tmp)
model = Model(inputs=[x, y], outputs=outputs)

accuracy = tf.numpy_function(
            func=calc_accuracy,
            inp=[y, outputs],
            Tout=tf.float32)

model.add_metric(accuracy, name=""accuracy"", aggregation='mean')

# Compile the model with the custom metric
model.compile(optimizer='adam',
              loss='binary_crossentropy')

# Dummy data for demonstration
import numpy as np
X_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000, 1)).astype(np.float32)

# Train the model
model.fit([X_train, y_train], y_train, epochs=10, batch_size=32)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/wangx286/rnn-base-caller/base_caller/scripts/example_metric.py"", line 18, in <module>
    accuracy = tf.numpy_function(
  File ""/home/wangx286/miniconda3/envs/tf217/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/wangx286/miniconda3/envs/tf217/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py"", line 61, in __array__
    raise ValueError(
ValueError: A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.
```
",MeowTheCat,2024-08-15 00:33:07+00:00,['Venkat6871'],2024-08-22 18:06:53+00:00,2024-08-22 18:06:50+00:00,https://github.com/tensorflow/tensorflow/issues/73835,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2295788842, 'issue_id': 2467052283, 'author': 'Venkat6871', 'body': 'Hi **@MeowTheCat** ,\r\n Sorry for the delay, I tried to run your code on Colab using TF v2.15, 2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/fae296b6d8ffa3188ee338347cbb332d/73835_2-15-0-2-17-0-nightly-v.ipynb) here for reference. Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 19, 6, 47, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305352485, 'issue_id': 2467052283, 'author': 'MeowTheCat', 'body': 'I walked around this issue. Closing it.', 'created_at': datetime.datetime(2024, 8, 22, 18, 6, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305352549, 'issue_id': 2467052283, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73835"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73835"">No</a>', 'created_at': datetime.datetime(2024, 8, 22, 18, 6, 52, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-19 06:47:39 UTC): Hi **@MeowTheCat** ,
 Sorry for the delay, I tried to run your code on Colab using TF v2.15, 2.17.0 & nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/fae296b6d8ffa3188ee338347cbb332d/73835_2-15-0-2-17-0-nightly-v.ipynb) here for reference. Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

MeowTheCat (Issue Creator) on (2024-08-22 18:06:50 UTC): I walked around this issue. Closing it.

google-ml-butler[bot] on (2024-08-22 18:06:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73835"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73835"">No</a>

"
2466377276,issue,closed,not_planned,App Storage Size Increases with CoreML or Metal Usage on iOS,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

0.0.1-nightly

### Custom code

No

### OS platform and distribution

iPhone 17.5.1

### Mobile device

iPhone 13 mini

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm encountering an issue where the app's storage size increases each time an AI model is loaded, and the storage doesn't decrease afterward. Specifically, I'm using the [PoseNet TensorFlow Lite model on iPhone](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/ios) to demonstrate the problem.

for more detail checkout  [this StackOverflow Post](https://stackoverflow.com/questions/78872138/app-storage-size-increases-with-coreml-or-metal-usage-on-ios)

### Standalone code to reproduce the issue

```md
Run the [TensorFlow Lite Pose Estimation iOS Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/ios#tensorflow-lite-pose-estimation-ios-demo) using `Metal` or `CoreML`, and you'll notice the app's storage size increasing each time the pose detection model is executed.
```


### Relevant log output

_No response_",leoull,2024-08-14 17:10:29+00:00,"['yishuangP', 'pkgoogle', 'sawantkumar']",2024-11-26 18:01:57+00:00,2024-11-26 18:01:53+00:00,https://github.com/tensorflow/tensorflow/issues/73800,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('iOS', '')]","[{'comment_id': 2307884009, 'issue_id': 2466377276, 'author': 'pkgoogle', 'body': 'Need a real device to test this. @yishuangP can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 8, 23, 22, 26, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475073686, 'issue_id': 2466377276, 'author': 'leoull', 'body': ""I got reply on Apple's Developer Forum: [CoreML - doUnloadModel:options:qos:error](https://developer.apple.com/forums/thread/768367?login=true)\r\n\r\nlooks like the issue is with tflite's coreML delegate implementation. I don't know much about this stuff, but the issue is probably here: [coreml_executor.mm](https://github.com/tensorflow/tensorflow/blob/8a3c6ad49d51857cc7ebda10ee93332f16f719ff/tensorflow/lite/delegates/coreml/coreml_executor.mm)"", 'created_at': datetime.datetime(2024, 11, 14, 0, 6, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499940538, 'issue_id': 2466377276, 'author': 'gaikwadrahul8', 'body': ""Hi, @leoull \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/40\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 8, 11, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501606579, 'issue_id': 2466377276, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73800"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73800"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 1, 55, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-23 22:26:56 UTC): Need a real device to test this. @yishuangP can you please take a look? Thanks.

leoull (Issue Creator) on (2024-11-14 00:06:09 UTC): I got reply on Apple's Developer Forum: [CoreML - doUnloadModel:options:qos:error](https://developer.apple.com/forums/thread/768367?login=true)

looks like the issue is with tflite's coreML delegate implementation. I don't know much about this stuff, but the issue is probably here: [coreml_executor.mm](https://github.com/tensorflow/tensorflow/blob/8a3c6ad49d51857cc7ebda10ee93332f16f719ff/tensorflow/lite/delegates/coreml/coreml_executor.mm)

gaikwadrahul8 on (2024-11-26 08:11:38 UTC): Hi, @leoull 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/40

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:01:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73800"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73800"">No</a>

"
2466289578,issue,closed,completed,[Building from source with bazel] Failure when linking libm-libraries during compilation of flatbuffers,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17

### Custom code

No

### OS platform and distribution

Linux Debian 5.10.216-1 x86_64

### Mobile device

_No response_

### Python version

3.12

### Bazel version

7.3.0

### GCC/compiler version

clang 17.0.2

### CUDA/cuDNN version

CUDA 12 /cuDNN 8

### GPU model and memory

NVIDIA A2

### Current behavior?

I'm trying to build tensorflow from source with bazel for use in a C++ project. I'm working in a conda environment wit the specifics mentioned above and try to compile the libraries with the following command:

`bazel build //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so`

Bazel fails with the follwing error message:
```
ERROR: /my/home/path/cache/bazel/_bazel_myuser/45c6a78e7c7688232d885d39238fef70/external/flatbuffers/BUILD.bazel:84:10: Linking external/flatbuffers/flatc [for tool] failed: (Exit 1): clang-17 failed: error executing command (from target @flatbuffers//:flatc) /my/home/path/.conda/envs/poet/bin/clang-17 @bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc-2.params
/usr/bin/ld: warning: /my/home/path/.conda/envs/poet/bin/../lib/gcc/x86_64-conda-linux-gnu/14.1.0/crtbeginS.o: unsupported GNU_PROPERTY_TYPE (5) type: 0xc0010002
/usr/bin/ld: warning: /my/home/path/.conda/envs/poet/bin/../lib/gcc/x86_64-conda-linux-gnu/14.1.0/crtbeginS.o: unsupported GNU_PROPERTY_TYPE (5) type: 0xc0010001
/usr/bin/ld: cannot find /lib64/libm.so.6 inside /
/usr/bin/ld: cannot find /usr/lib64/libmvec_nonshared.a inside /
/usr/bin/ld: cannot find /lib64/libmvec.so.1 inside /
clang-17: error: linker command failed with exit code 1 (use -v to see invocation)
```
 I tried to add the following lines to `.bazelrc` to explicitly link the locations of the libraries that were not found. That did not change the error.
```
build:linux --linkopt=-L/mnt/beegfs/home/straile/.conda/envs/poet/x86_64-conda-linux-gnu/sysroot/lib64
build:linux --linkopt=-L/mnt/beegfs/home/straile/.conda/envs/poet/x86_64-conda-linux-gnu/sysroot/usr/lib64
build:linux --linkopt=-Wl,-rpath,/mnt/beegfs/home/straile/.conda/envs/poet/x86_64-conda-linux-gnu/sysroot/lib64
build:linux --linkopt=-Wl,-rpath,/mnt/beegfs/home/straile/.conda/envs/poet/x86_64-conda-linux-gnu/sysroot/usr/lib64
```

Any help on this issue is much appreciated.

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.17
./configure 
# [with CUDA version 12, cuDNN version 8, NCCL version 2.18, compute capabilities 8.6]
# I also had to add ""linux:build --copt=-Qunused-arguments"" to .bazelrc to ignore compile errors in protoc
bazel build //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so
```


### Relevant log output

_No response_",hanssssssssssssss,2024-08-14 16:26:46+00:00,['Venkat6871'],2024-09-03 01:56:57+00:00,2024-09-03 01:56:53+00:00,https://github.com/tensorflow/tensorflow/issues/73798,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2295676363, 'issue_id': 2466289578, 'author': 'Venkat6871', 'body': 'Hi **@hanssssssssssssss** ,\r\n- Apologies for the delay. This might be due to a version incompatibility. Please verify version compatibility using the attached [documentation](https://www.tensorflow.org/install/source#gpu). Let me know if the issue persists. \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 19, 5, 4, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419445, 'issue_id': 2466289578, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464789, 'issue_id': 2466289578, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325464859, 'issue_id': 2466289578, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73798"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73798"">No</a>', 'created_at': datetime.datetime(2024, 9, 3, 1, 56, 56, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-19 05:04:23 UTC): Hi **@hanssssssssssssss** ,
- Apologies for the delay. This might be due to a version incompatibility. Please verify version compatibility using the attached [documentation](https://www.tensorflow.org/install/source#gpu). Let me know if the issue persists. 

Thank you!

github-actions[bot] on (2024-08-27 01:55:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-03 01:56:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-03 01:56:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73798"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73798"">No</a>

"
2465040564,issue,closed,completed,Failed to load the native TensorFlow runtime,"Traceback (most recent call last):
  File ""C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Asus\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",lilisrachman26,2024-08-14 06:51:29+00:00,['tilakrayal'],2024-08-31 01:56:46+00:00,2024-08-31 01:56:43+00:00,https://github.com/tensorflow/tensorflow/issues/73765,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2287989787, 'issue_id': 2465040564, 'author': 'Agzamxoja', 'body': ""The infamous DLL load failed error!\r\n\r\nThis error typically occurs when there's an issue with the TensorFlow installation or the underlying system configuration. Here are some common causes and potential solutions:\r\n\r\n1.Missing or corrupted DLL files: TensorFlow relies on several DLL files, which might be missing or corrupted. Try reinstalling TensorFlow using pip.\r\n2.Incompatible Python version: Ensure you're using a compatible Python version with TensorFlow. TensorFlow supports Python 3.7-3.10. You're using Python 3.11, which might not be supported. Try downgrading to a compatible version.\r\n3.CUDA and cuDNN versions: If you're using a GPU, ensure you have the correct versions of CUDA and cuDNN installed. TensorFlow requires specific versions of these libraries. Check the TensorFlow installation guide for more information.\r\n4.Visual Studio redistributable package: TensorFlow requires the Microsoft Visual Studio redistributable package to be installed. You can download it from the official Microsoft website.\r\n5.System configuration issues: Sometimes, system configuration issues, such as corrupted registry entries or conflicting software installations, can cause DLL load failures. Try resetting your system or seeking help from a system administrator.\r\nTo troubleshoot further, you can try the following:\r\n\r\n1.Check the TensorFlow installation logs for any errors.\r\n2.Verify that the tensorflow package is installed correctly by running pip show tensorflow.\r\n3.Try importing TensorFlow in a new Python environment or a different Python version to isolate the issue.\r\n4.If you're using a GPU, try running TensorFlow on CPU-only mode by setting the CUDA_VISIBLE_DEVICES environment variable to an empty string.\r\nI think this helpful for you"", 'created_at': datetime.datetime(2024, 8, 14, 6, 55, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292939242, 'issue_id': 2465040564, 'author': 'tilakrayal', 'body': '@lilisrachman26,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 16, 7, 0, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987372, 'issue_id': 2465040564, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322725774, 'issue_id': 2465040564, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 31, 1, 56, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322725802, 'issue_id': 2465040564, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73765"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73765"">No</a>', 'created_at': datetime.datetime(2024, 8, 31, 1, 56, 45, tzinfo=datetime.timezone.utc)}]","Agzamxoja on (2024-08-14 06:55:54 UTC): The infamous DLL load failed error!

This error typically occurs when there's an issue with the TensorFlow installation or the underlying system configuration. Here are some common causes and potential solutions:

1.Missing or corrupted DLL files: TensorFlow relies on several DLL files, which might be missing or corrupted. Try reinstalling TensorFlow using pip.
2.Incompatible Python version: Ensure you're using a compatible Python version with TensorFlow. TensorFlow supports Python 3.7-3.10. You're using Python 3.11, which might not be supported. Try downgrading to a compatible version.
3.CUDA and cuDNN versions: If you're using a GPU, ensure you have the correct versions of CUDA and cuDNN installed. TensorFlow requires specific versions of these libraries. Check the TensorFlow installation guide for more information.
4.Visual Studio redistributable package: TensorFlow requires the Microsoft Visual Studio redistributable package to be installed. You can download it from the official Microsoft website.
5.System configuration issues: Sometimes, system configuration issues, such as corrupted registry entries or conflicting software installations, can cause DLL load failures. Try resetting your system or seeking help from a system administrator.
To troubleshoot further, you can try the following:

1.Check the TensorFlow installation logs for any errors.
2.Verify that the tensorflow package is installed correctly by running pip show tensorflow.
3.Try importing TensorFlow in a new Python environment or a different Python version to isolate the issue.
4.If you're using a GPU, try running TensorFlow on CPU-only mode by setting the CUDA_VISIBLE_DEVICES environment variable to an empty string.
I think this helpful for you

tilakrayal (Assginee) on (2024-08-16 07:00:51 UTC): @lilisrachman26,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-08-24 01:53:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-31 01:56:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-31 01:56:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73765"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73765"">No</a>

"
2465002164,issue,closed,completed,EventMgr::PollLoop causes graph execution hang,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux debian 10, kernel 5.15

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After tens of thousands of steps, I sometimes encounter a situation where the graph execution hangs. This probability is extremely low.

My graph nodes may like this:
```
WhereOp(GPU) --> ShapeOp
             --> TensorScatterUpdateOp --> HistogramSummary
```

Here is the pstack:
<img width=""1415"" alt=""image"" src=""https://github.com/user-attachments/assets/843e7fc1-efec-4bb7-b73e-7b4c0f90fe96"">

the first frame:
```
void EventMgr::PollLoop() {
  while (true) {
    bool events_still_pending;
    {
      mutex_lock l(mu_);    //////////////////    lock mu_ here
      if (stop_polling_) {
        break;
      }
      if (callbacks_.empty()) {
        events_pending_.wait(l);
      }
      PollEvents(/*stream=*/nullptr);  // poll all streams
      events_still_pending = !callbacks_.empty();
    }

    if (events_still_pending) {
      Env::Default()->SleepForMicroseconds(polling_active_delay_usecs_);
    }
  }
  polling_stopped_->Notify();
}
```
the second frame:
```
// call from GPUUtil::CopyGPUTensorToCPU
void ThenExecute(se::Stream* stream, std::function<void()> func) {
    mutex_lock l(mu_);    //////////////////    lock mu_ here
    EnqueueCallback(stream, std::move(func));
    PollEvents(stream);
}
```
I guess that when the EventMgr::PollLoop thread executes PollLoop to run the done_callback of a async operator(WhereOp), it trigger a SendOp to CopyGPUTensorToCPU, which requires EventMgr::ThenExecute, which eventually causes the lock to hang.

I can only reproduce it in my production environment, tf.ConfigProto:
config.inter_op_parallelism_threads = 4
config.intra_op_parallelism_threads = 4

Other information:
TF version: tag v2.15.0, commit 6887368d6d46223f460358323c4b76d61d1558a8
Python: 3.9
OS: Linux debian 10, kernel 5.15

GPU: NVIDIA A100
Driver Version: 535.161.08
CUDA Version: 12.2

However, I cannot reproduce this issue with tiny code, could anyone reproduce it?


### Standalone code to reproduce the issue

```shell
However, I cannot reproduce this issue with tiny code, could anyone reproduce it?
```


### Relevant log output

_No response_",ganyu1992,2024-08-14 06:25:10+00:00,['Venkat6871'],2024-09-04 19:28:47+00:00,2024-09-04 19:28:44+00:00,https://github.com/tensorflow/tensorflow/issues/73764,"[('type:bug', 'Bug'), ('comp:core', 'issues related to core part of tensorflow'), ('awaiting PR merge', 'awaiting PR merge'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2288822149, 'issue_id': 2465002164, 'author': 'ganyu1992', 'body': ""I've tried TF 2.8.0, it never hung.\r\n\r\n```\r\nvoid EventMgr::PollLoop() {\r\n  ToFreeVector to_free;\r\n  while (true) {\r\n    bool events_still_pending;\r\n    {\r\n      mutex_lock l(mu_);\r\n      if (stop_polling_) {\r\n        break;\r\n      }\r\n      if (used_events_.empty()) {\r\n        events_pending_.wait(l);\r\n      }\r\n      PollEvents(true, &to_free);\r\n      events_still_pending = !used_events_.empty();\r\n    }\r\n    FreeMemory(to_free);     ///////////////  FreeMemory was out of mu_ scope\r\n    to_free.clear();\r\n\r\n    if (events_still_pending) {\r\n      Env::Default()->SleepForMicroseconds(polling_active_delay_usecs_);\r\n    }\r\n  }\r\n  polling_stopped_->Notify();\r\n}\r\n\r\nvoid FreeMemory(const ToFreeVector& to_free) {\r\n    for (const auto& iu : to_free) {\r\n      // The function must be called in another thread.\r\n      if (iu.func != nullptr) threadpool_.Schedule(iu.func);\r\n    }\r\n  }\r\n```\r\n\r\nIt's most likely caused by this problem."", 'created_at': datetime.datetime(2024, 8, 14, 13, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329816621, 'issue_id': 2465002164, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73764"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73764"">No</a>', 'created_at': datetime.datetime(2024, 9, 4, 19, 28, 46, tzinfo=datetime.timezone.utc)}]","ganyu1992 (Issue Creator) on (2024-08-14 13:53:00 UTC): I've tried TF 2.8.0, it never hung.

```
void EventMgr::PollLoop() {
  ToFreeVector to_free;
  while (true) {
    bool events_still_pending;
    {
      mutex_lock l(mu_);
      if (stop_polling_) {
        break;
      }
      if (used_events_.empty()) {
        events_pending_.wait(l);
      }
      PollEvents(true, &to_free);
      events_still_pending = !used_events_.empty();
    }
    FreeMemory(to_free);     ///////////////  FreeMemory was out of mu_ scope
    to_free.clear();

    if (events_still_pending) {
      Env::Default()->SleepForMicroseconds(polling_active_delay_usecs_);
    }
  }
  polling_stopped_->Notify();
}

void FreeMemory(const ToFreeVector& to_free) {
    for (const auto& iu : to_free) {
      // The function must be called in another thread.
      if (iu.func != nullptr) threadpool_.Schedule(iu.func);
    }
  }
```

It's most likely caused by this problem.

google-ml-butler[bot] on (2024-09-04 19:28:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73764"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73764"">No</a>

"
2464609829,issue,open,,The average should not be computed in L2Pool2d,"The [ONNX L2Pool2d](https://github.com/onnx/onnx/blob/main/docs/Operators.md#lppool), [DirectML L2 Pooling Desc](https://docs.microsoft.com/en-us/windows/win32/api/directml/ns-directml-dml_lp_pooling_operator_desc) and [CoreML's l2_pool2d](https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.iOS15.pool.l2_pool) calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2)`,  but [TFLite L2_PooL2d kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=3341?q=src%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Foptimized%2Foptimized_ops.h) has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)/n) ^ (1/2)`,  is it an issue of the kernel implementation?

BTW, [the kernel of l2_norm](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=1424?q=L2Normalization&ss=chromium%2Fchromium%2Fsrc) also has no the average.

",fujunwei,2024-08-14 00:51:39+00:00,"['pkgoogle', 'sawantkumar']",2024-09-19 00:27:04+00:00,,https://github.com/tensorflow/tensorflow/issues/73742,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2287998605, 'issue_id': 2464609829, 'author': 'Agzamxoja', 'body': ""Yes, this is an issue with the TFLite L2_Pool2d kernel implementation. The correct implementation of L2 pooling should be Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2), without averaging by the count of sum elements.\r\n\r\nThe averaging step is not part of the standard L2 pooling operation, and it changes the result. The correct implementation should only sum the squared values and then take the square root, without dividing by the count of elements.\r\n\r\nIt's good that you also noticed the same issue with the L2 norm kernel implementation. Both L2 pooling and L2 norm should follow the same formula without averaging.\r\n\r\nThis discrepancy might cause issues when converting models between different frameworks, or when trying to reproduce results from other frameworks. It's recommended to report this issue to the TFLite developers and ask them to fix the kernel implementation to match the standard L2 pooling and L2 norm formulas."", 'created_at': datetime.datetime(2024, 8, 14, 7, 2, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307885807, 'issue_id': 2464609829, 'author': 'pkgoogle', 'body': 'Current PR: https://github.com/tensorflow/tensorflow/pull/74079', 'created_at': datetime.datetime(2024, 8, 23, 22, 29, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359670671, 'issue_id': 2464609829, 'author': 'fujunwei', 'body': ""Thanks, what's the status of this PR, does it have any blocking issue?"", 'created_at': datetime.datetime(2024, 9, 19, 0, 27, 2, tzinfo=datetime.timezone.utc)}]","Agzamxoja on (2024-08-14 07:02:03 UTC): Yes, this is an issue with the TFLite L2_Pool2d kernel implementation. The correct implementation of L2 pooling should be Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2), without averaging by the count of sum elements.

The averaging step is not part of the standard L2 pooling operation, and it changes the result. The correct implementation should only sum the squared values and then take the square root, without dividing by the count of elements.

It's good that you also noticed the same issue with the L2 norm kernel implementation. Both L2 pooling and L2 norm should follow the same formula without averaging.

This discrepancy might cause issues when converting models between different frameworks, or when trying to reproduce results from other frameworks. It's recommended to report this issue to the TFLite developers and ask them to fix the kernel implementation to match the standard L2 pooling and L2 norm formulas.

pkgoogle (Assginee) on (2024-08-23 22:29:25 UTC): Current PR: https://github.com/tensorflow/tensorflow/pull/74079

fujunwei (Issue Creator) on (2024-09-19 00:27:02 UTC): Thanks, what's the status of this PR, does it have any blocking issue?

"
2462176190,issue,closed,completed,Tensor Flow GPU support on Windows,"Hi. Balachander here. I am a visually impaired person working on AI as a data scientist and ML engineer. I am predominantly using Windows for all my development activities as Windows provides a full fledged support for Screen Readers. Also they provide development tools those are fully accessible to people like me. Removing GPU support from Tensorflow on Windows is a worst violation and discriminatory, as it hits people like me. Dockers with Linux OS do not come with screen readers. I request you to re-enable TF to provide support GPUs on Windows and MAC systems so that we don't get hit. I appreciate your quick attention and help. Thank you.",balachander1964,2024-08-13 01:22:25+00:00,['Venkat6871'],2024-12-03 02:08:36+00:00,2024-12-03 02:08:33+00:00,https://github.com/tensorflow/tensorflow/issues/73652,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:gpu', 'GPU related issues'), ('subtype:windows', 'Windows Build/Installation Issues')]","[{'comment_id': 2287857195, 'issue_id': 2462176190, 'author': 'Venkat6871', 'body': 'Hi **@balachander1964** ,\r\nThere is an ongoing issue related to this conversion. Please review issue number [59918](https://github.com/tensorflow/tensorflow/issues/59918) for more details. And  GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 14, 4, 58, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298751155, 'issue_id': 2462176190, 'author': 'balachander1964', 'body': 'Thank you Venkat. I will try your suggestion. I appreciate your reply.', 'created_at': datetime.datetime(2024, 8, 20, 12, 34, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499454326, 'issue_id': 2462176190, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 26, 2, 6, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513384395, 'issue_id': 2462176190, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513384465, 'issue_id': 2462176190, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73652"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73652"">No</a>', 'created_at': datetime.datetime(2024, 12, 3, 2, 8, 35, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-14 04:58:11 UTC): Hi **@balachander1964** ,
There is an ongoing issue related to this conversion. Please review issue number [59918](https://github.com/tensorflow/tensorflow/issues/59918) for more details. And  GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin.
Thank you!

balachander1964 (Issue Creator) on (2024-08-20 12:34:46 UTC): Thank you Venkat. I will try your suggestion. I appreciate your reply.

github-actions[bot] on (2024-11-26 02:06:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-03 02:08:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-03 02:08:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73652"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73652"">No</a>

"
2461919579,issue,open,,Unable to train/take gradient of integer variable under any condition,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.16.1-19-g810f233968c

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

Models with integer variables cannot have gradients computed (or be trained because of this). They fail with `ValueError: No gradients provided for any variable.` Please note that I'm referring to building/training models, _not_ post-training model quantization.

For context, I'm working on a somewhat novel approach to solving a type of engineering problem that includes both discrete and continuous values. In some cases, variables must be one of a set of numeric values (e.g. `1`, `2`, `3000`, etc). This isn't something that can really be split out into separate models and trained independently, as both these variables are used throughout a complex, multiple-input multiple-output layer based model. An approximation cannot be used either, as even a highly accurate approximation can feasibly result in an incorrect result under certain conditions.

Intuitively, a derivative cannot be calculated on integer values because they're discrete, and differentiation requires a continuous function over the domain of differentiation. However, floats (and any limited-precision data type) also suffer from this - but differentiation is generally considered ""viable"" for floats. Under any argument made for floats, integers should also be considered differentiable. There are certainly some issues with this for small number, but they have much less of an impact for larger ones (just like floats very close to 0 vs ""single digit"" floats i.e. `1`, `2`, etc).

If maintainers/triagers/project folks don't want to go in this direction, users like myself should be able to implement this via a `tf.custom_gradient` function. However, even when implementing this function, gradients/training fail in the exact same manner in the exact same place. When using a custom gradient function with an integer variable, TF fails with the aforementioned error message without even calling the custom gradient function. IMO this is pretty clearly a bug. See below for specific code to reproduce this.

### Standalone code to reproduce the issue

This fails (no custom gradient function, int32 type):

```python
import keras
import tensorflow as tf
import numpy as np

variable_dtype = tf.int32
# variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This works (no custom gradient function, float32 type):
```python
import keras
import tensorflow as tf
import numpy as np

# variable_dtype = tf.int32
variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This fails (custom gradient function, int32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.int32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```

This works (custom gradient function, float32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.float32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```


### Relevant log output

Logs for the last failure:

```shell
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File /workspaces/power sim 2/test_docs.py:14
      3 values = np.array([
      4     [i]
      5     for i in range(1000)
      6 ])
      8 model.compile(
      9     loss=[keras.losses.MeanSquaredError()],
     10     optimizer=keras.optimizers.RMSprop(),
     11     metrics=[keras.metrics.MeanSquaredError()],
     12 )
---> 14 history = model.fit(values, values, batch_size=1, epochs=10)

File ~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.local/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:662, in BaseOptimizer._filter_empty_gradients(self, grads, vars)
    659         missing_grad_vars.append(v.name)
    661 if not filtered_grads:
--> 662     raise ValueError(""No gradients provided for any variable."")
    663 if missing_grad_vars:
    664     warnings.warn(
    665         ""Gradients do not exist for variables ""
    666         f""{list(reversed(missing_grad_vars))} when minimizing the loss.""
    667         "" If using `model.compile()`, did you forget to provide a ""
    668         ""`loss` argument?""
    669     )

ValueError: No gradients provided for any variable.
```
",solidDoWant,2024-08-12 21:23:47+00:00,['tilakrayal'],2024-08-14 05:09:46+00:00,,https://github.com/tensorflow/tensorflow/issues/73631,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2287867546, 'issue_id': 2461919579, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow [v2.17](https://colab.research.google.com/gist/tilakrayal/1d6aa5e87a319ed1c612f91b3172b8ff/untitled2073.ipynb) and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a22ce04f84adf3e2afe43b61139887d5/untitled2074.ipynb).', 'created_at': datetime.datetime(2024, 8, 14, 5, 9, 38, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-14 05:09:38 UTC): I was able to reproduce the issue on tensorflow [v2.17](https://colab.research.google.com/gist/tilakrayal/1d6aa5e87a319ed1c612f91b3172b8ff/untitled2073.ipynb) and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a22ce04f84adf3e2afe43b61139887d5/untitled2074.ipynb).

"
2460830393,issue,closed,completed,op_util_common.cc causes function parameter mismatch and cannot compile normally,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master

### Custom code

Yes

### OS platform and distribution

Linub Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

gcc 11.4

### CUDA/cuDNN version

12.1/9.2

### GPU model and memory

_No response_

### Current behavior?

There is a type mismatch in the ResolvePadding function defined at tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/op_util_common.cc:71. The return type is `llvm::Small Vector<DimPadding, 4>`, but the `res` defined are `llvm::Small Vector <DimPadding, 2>`. 


### Standalone code to reproduce the issue

```shell
bazel build --config=opt //tensorflow/tools/pip_package:wheel --repo_env=tf0812=tensorflow --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
```


### Relevant log output

```shell
ensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/op_util_common.cc: In function llvm::SmallVector<mlir::odml::DimPadding, 2> mlir::odml::ResolvePadding(int64_t, std::optional<mlir::DenseIntElementsAttr>):
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/op_util_common.cc:78:12: error: could not convert res from SmallVector<[...],4> to SmallVector<[...],2>
   78 |     return res;
      |            ^~~
      |            |
      |            SmallVector<[...],4>
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/op_util_common.cc:86:12: error: could not convert res from SmallVector<[...],4> to SmallVector<[...],2>
   86 |     return res;
      |            ^~~
      |            |
      |            SmallVector<[...],4>
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions/op_util_common.cc:96:10: error: could not convert res from SmallVector<[...],4> to SmallVector<[...],2>
   96 |   return res;
      |          ^~~
      |          |
      |          SmallVector<[...],4>
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/tensorflow/tensorflow/tools/pip_package/BUILD:266:9 Action tensorflow/tools/pip_package/wheel_house failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo_conversions:op_util_common) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ... (remaining 111 arguments skipped)
```
",ChrisHuang96,2024-08-12 12:12:49+00:00,"['pkgoogle', 'sawantkumar']",2024-12-20 03:37:52+00:00,2024-12-20 03:37:08+00:00,https://github.com/tensorflow/tensorflow/issues/73603,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2307591205, 'issue_id': 2460830393, 'author': 'pkgoogle', 'body': 'Testing this out..., changing return size to 4.\r\n\r\ncurrent PR: https://github.com/tensorflow/tensorflow/pull/74423', 'created_at': datetime.datetime(2024, 8, 23, 18, 21, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532990577, 'issue_id': 2460830393, 'author': 'pkgoogle', 'body': 'Hi @ChrisHuang96, this PR is now merged: https://github.com/tensorflow/tensorflow/pull/74423. Let us know if it resolves your issue.', 'created_at': datetime.datetime(2024, 12, 10, 21, 56, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550130059, 'issue_id': 2460830393, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 18, 2, 5, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556222026, 'issue_id': 2460830393, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73603"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73603"">No</a>', 'created_at': datetime.datetime(2024, 12, 20, 3, 37, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556222500, 'issue_id': 2460830393, 'author': 'ChrisHuang96', 'body': 'This PR works well.', 'created_at': datetime.datetime(2024, 12, 20, 3, 37, 50, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-23 18:21:45 UTC): Testing this out..., changing return size to 4.

current PR: https://github.com/tensorflow/tensorflow/pull/74423

pkgoogle (Assginee) on (2024-12-10 21:56:34 UTC): Hi @ChrisHuang96, this PR is now merged: https://github.com/tensorflow/tensorflow/pull/74423. Let us know if it resolves your issue.

github-actions[bot] on (2024-12-18 02:05:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

google-ml-butler[bot] on (2024-12-20 03:37:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73603"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73603"">No</a>

ChrisHuang96 (Issue Creator) on (2024-12-20 03:37:50 UTC): This PR works well.

"
2460294633,issue,closed,completed,Building from Source without internet connection,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

Branch r2.17

### Custom code

Yes

### OS platform and distribution

Debian 11 Linux 5.10.0-28-amd64 x86_64

### Mobile device

_No response_

### Python version

Python 3.9.2

### Bazel version

bazel 6.5.0

### GCC/compiler version

gcc (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Context:

I have my computer blocked from internet. 
Furthemore, as I run it in a VM, my VM does not have AVX available and I do not have access to the host machine to change this.
Because all TF 2.x.x compiled versions use AVX, I decided to compile tf myself. I cloned the repo and switch to branch `r.2.17`.
At first, many files failed to be found, I downloaded each file in the error message and copied to a `../shared_folder/dist` folder. I then use:

`bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --distdir ../shared_folder/dist` 

Although I downloaded like 30 packages already, it seems to be advancing, however, now I get an error message that I do not know how to get passed. The not found file does not really have a file name but it is: `[https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all]`. Normally it was a `[github.com/<stuff>/<filename>.tar.gz]` and just downloading `<filename>.tar.gz` into `../shared_folder/dist`  worked.

I put `which bazel` and saw it is installed in `/usr/bin/bazel` but there is no folder, I installed it using the `sudo apt install ./bazel_6.5.0-linux-x86_64.deb` and I don't see any file to change any link.

This [link](https://stackoverflow.com/questions/39032329/is-there-any-way-to-build-tensorflow-from-source-without-having-internet) is what I want, but it's quite outdated and the solution is no longer viable (or at least it needs some modifications I could not find).

### Standalone code to reproduce the issue

```shell
Not possible I believe. If there is a way, please let me know.
```


### Relevant log output

```shell
INFO: Reading 'startup' options from /home/abarrachina/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/abarrachina/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/abarrachina/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/abarrachina/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/abarrachina/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/abarrachina/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository go_sdk instantiated at:
  /home/abarrachina/tensorflow/WORKSPACE:103:14: in <toplevel>
  /home/abarrachina/tensorflow/tensorflow/workspace0.bzl:135:20: in workspace
  /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_depscd BUI
  /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains
  /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>
WARNING: Download from https://golang.org/dl/?mode=json&include=all failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Unknown host: golang.org
WARNING: Download from https://golang.google.cn/dl/?mode=json&include=all failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Unknown host: golang.google.cn
ERROR: An error occurred during the fetch of repository 'go_sdk':
   Traceback (most recent call last):
	File ""/home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/io_bazel_rules_go/go/private/sdk.bzl"", line 71, column 21, in _go_download_sdk_impl
		ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/go_sdk/versions.json: Unknown host: golang.google.cn
ERROR: /home/abarrachina/tensorflow/WORKSPACE:103:14: fetching _go_download_sdk rule //external:go_sdk: Traceback (most recent call last):
	File ""/home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/io_bazel_rules_go/go/private/sdk.bzl"", line 71, column 21, in _go_download_sdk_impl
		ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/go_sdk/versions.json: Unknown host: golang.google.cn
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /home/abarrachina/.cache/bazel/_bazel_abarrachina/7bf8e2307b94e4bac50e88e2b60f4f57/external/go_sdk/versions.json: Unknown host: golang.google.cn
INFO: Elapsed time: 0.637s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (5 packages loaded, 41 targets configured)
    currently loading: @rules_python//python/runfiles ... (7 packages)
    Fetching repository @pypi_typing_extensions; starting
```
",NEGU93,2024-08-12 07:58:04+00:00,['tilakrayal'],2024-08-22 14:02:11+00:00,2024-08-22 14:02:07+00:00,https://github.com/tensorflow/tensorflow/issues/73572,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2283635162, 'issue_id': 2460294633, 'author': 'joydeep049', 'body': 'If this issue is free to work on, can I try?\r\n@NEGU93', 'created_at': datetime.datetime(2024, 8, 12, 10, 44, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283713123, 'issue_id': 2460294633, 'author': 'NEGU93', 'body': ""I'm sorry, not sure what you mean. If I can give you the VM file directly? If so, I'm afraid not. It's my company work so I cannot share it. This is also the reason I have this complex environment and restrictions."", 'created_at': datetime.datetime(2024, 8, 12, 11, 25, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283905034, 'issue_id': 2460294633, 'author': 'joydeep049', 'body': 'I meant to ask if I can work on solving this issue?\r\nHow do I work on issues here? Do they get assigned to somenone on asking? Or some other way?', 'created_at': datetime.datetime(2024, 8, 12, 12, 52, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285585756, 'issue_id': 2460294633, 'author': 'tilakrayal', 'body': '@NEGU93,\r\nGenerally, it is not suggestible to do this process because Bazel automatically caches the external dependencies it downloads. As the alternative you can try to build the docker or the another VM environment and try to use that.\r\n\r\nhttps://discuss.ai.google.dev/t/tensorflow-installation-without-internet/31021\r\nhttps://discuss.ai.google.dev/t/installing-tf-on-offline-ubuntu/29473/3\r\nhttps://github.com/tensorflow/tensorflow/issues/3194\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 13, 7, 49, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2300137264, 'issue_id': 2460294633, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 21, 1, 54, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304753986, 'issue_id': 2460294633, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73572"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73572"">No</a>', 'created_at': datetime.datetime(2024, 8, 22, 14, 2, 9, tzinfo=datetime.timezone.utc)}]","joydeep049 on (2024-08-12 10:44:02 UTC): If this issue is free to work on, can I try?
@NEGU93

NEGU93 (Issue Creator) on (2024-08-12 11:25:51 UTC): I'm sorry, not sure what you mean. If I can give you the VM file directly? If so, I'm afraid not. It's my company work so I cannot share it. This is also the reason I have this complex environment and restrictions.

joydeep049 on (2024-08-12 12:52:35 UTC): I meant to ask if I can work on solving this issue?
How do I work on issues here? Do they get assigned to somenone on asking? Or some other way?

tilakrayal (Assginee) on (2024-08-13 07:49:57 UTC): @NEGU93,
Generally, it is not suggestible to do this process because Bazel automatically caches the external dependencies it downloads. As the alternative you can try to build the docker or the another VM environment and try to use that.

https://discuss.ai.google.dev/t/tensorflow-installation-without-internet/31021
https://discuss.ai.google.dev/t/installing-tf-on-offline-ubuntu/29473/3
https://github.com/tensorflow/tensorflow/issues/3194

Thank you!

github-actions[bot] on (2024-08-21 01:54:03 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

google-ml-butler[bot] on (2024-08-22 14:02:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73572"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73572"">No</a>

"
2459776877,issue,closed,completed,Pip install of Tensorflow may be causing issues with MacOS,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

MacOS Sonoma 14.1

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I recently installed tensorflow (2 days ago) using Pip. Ever since downloading Tensorflow I have had an issue with my MacBook. Specifically a process called fileproviderd has been using over 110% of my CPU usage. This is causing my laptop to heat up and is draining the battery. I have since deleted tensorflow from my virtual environment and have noticed a slight improvement (averaging out at about 40%-90% - which is still not normal). I have just reinstalled Tensorflow and the process activity skyrocketed using about 140% of the CPU usage. 

I am also contacting Apple support with the issue and will report any solutions that we discover however I do believe that this is an issue with Tensorflow as Ive never had this issue before installing it. 

Also not that I never ran any code the second time I installed tensorflow, just installed it using Pip. 

### Standalone code to reproduce the issue

```shell
1. Installing Tensorflow using Pip to a virtual environment.
2. Activity monitor reports a high CPU usage from a process called fileproviderd.
3. Deleting Tensorflow from the virtual environment and a restart.
4. Slightly lower usage from the process.
```


### Relevant log output

_No response_",Salvo9879,2024-08-11 21:52:01+00:00,['Venkat6871'],2024-08-13 14:09:23+00:00,2024-08-13 14:09:18+00:00,https://github.com/tensorflow/tensorflow/issues/73561,"[('type:support', 'Support issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2284416662, 'issue_id': 2459776877, 'author': 'mihaimaruseac', 'body': 'Can you post the output of `pip list` once TF is installed?', 'created_at': datetime.datetime(2024, 8, 12, 16, 21, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286209993, 'issue_id': 2459776877, 'author': 'Salvo9879', 'body': 'Package                 Version\r\n----------------------- --------\r\nabsl-py                 2.1.0\r\nastunparse              1.6.3\r\ncertifi                 2024.7.4\r\ncharset-normalizer      3.3.2\r\nflatbuffers             24.3.25\r\ngast                    0.6.0\r\ngoogle-pasta            0.2.0\r\ngrpcio                  1.65.4\r\nh5py                    3.11.0\r\nidna                    3.7\r\nkeras                   3.5.0\r\nlibclang                18.1.1\r\nMarkdown                3.6\r\nmarkdown-it-py          3.0.0\r\nMarkupSafe              2.1.5\r\nmdurl                   0.1.2\r\nml-dtypes               0.4.0\r\nnamex                   0.0.8\r\nnumpy                   1.26.4\r\nopt-einsum              3.3.0\r\noptree                  0.12.1\r\npackaging               24.1\r\npip                     24.2\r\nprotobuf                4.25.4\r\nPygments                2.18.0\r\nrequests                2.32.3\r\nrich                    13.7.1\r\nsetuptools              72.1.0\r\nsix                     1.16.0\r\ntensorboard             2.17.0\r\ntensorboard-data-server 0.7.2\r\ntensorflow              2.17.0\r\ntermcolor               2.4.0\r\ntyping_extensions       4.12.2\r\nurllib3                 2.2.2\r\nWerkzeug                3.0.3\r\nwheel                   0.44.0\r\nwrapt                   1.16.0', 'created_at': datetime.datetime(2024, 8, 13, 13, 1, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286276896, 'issue_id': 2459776877, 'author': 'mihaimaruseac', 'body': ""Cannot reproduce with the exact same versions.\r\n\r\nIs it possible you are syncing the virtual environment to the cloud? You should not. There are many files in these wheels and there's nothing to be gained by syncing them compared to just installing them again when needed."", 'created_at': datetime.datetime(2024, 8, 13, 13, 37, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286285678, 'issue_id': 2459776877, 'author': 'Salvo9879', 'body': 'Yes, My project is stored using iCloud. However I have deleted the environment completely yet the issue persists. I will investigate further. Thank you for the response.', 'created_at': datetime.datetime(2024, 8, 13, 13, 41, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286356187, 'issue_id': 2459776877, 'author': 'mihaimaruseac', 'body': 'It takes a while for the sync to finish. And when you delete your files, the deletion needs to be synced to the cloud too.', 'created_at': datetime.datetime(2024, 8, 13, 14, 9, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286356637, 'issue_id': 2459776877, 'author': 'mihaimaruseac', 'body': 'Closing this since it is not a TF issue.', 'created_at': datetime.datetime(2024, 8, 13, 14, 9, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286356741, 'issue_id': 2459776877, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73561"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73561"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 14, 9, 22, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-08-12 16:21:47 UTC): Can you post the output of `pip list` once TF is installed?

Salvo9879 (Issue Creator) on (2024-08-13 13:01:45 UTC): Package                 Version
----------------------- --------
absl-py                 2.1.0
astunparse              1.6.3
certifi                 2024.7.4
charset-normalizer      3.3.2
flatbuffers             24.3.25
gast                    0.6.0
google-pasta            0.2.0
grpcio                  1.65.4
h5py                    3.11.0
idna                    3.7
keras                   3.5.0
libclang                18.1.1
Markdown                3.6
markdown-it-py          3.0.0
MarkupSafe              2.1.5
mdurl                   0.1.2
ml-dtypes               0.4.0
namex                   0.0.8
numpy                   1.26.4
opt-einsum              3.3.0
optree                  0.12.1
packaging               24.1
pip                     24.2
protobuf                4.25.4
Pygments                2.18.0
requests                2.32.3
rich                    13.7.1
setuptools              72.1.0
six                     1.16.0
tensorboard             2.17.0
tensorboard-data-server 0.7.2
tensorflow              2.17.0
termcolor               2.4.0
typing_extensions       4.12.2
urllib3                 2.2.2
Werkzeug                3.0.3
wheel                   0.44.0
wrapt                   1.16.0

mihaimaruseac on (2024-08-13 13:37:24 UTC): Cannot reproduce with the exact same versions.

Is it possible you are syncing the virtual environment to the cloud? You should not. There are many files in these wheels and there's nothing to be gained by syncing them compared to just installing them again when needed.

Salvo9879 (Issue Creator) on (2024-08-13 13:41:21 UTC): Yes, My project is stored using iCloud. However I have deleted the environment completely yet the issue persists. I will investigate further. Thank you for the response.

mihaimaruseac on (2024-08-13 14:09:06 UTC): It takes a while for the sync to finish. And when you delete your files, the deletion needs to be synced to the cloud too.

mihaimaruseac on (2024-08-13 14:09:18 UTC): Closing this since it is not a TF issue.

google-ml-butler[bot] on (2024-08-13 14:09:22 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73561"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73561"">No</a>

"
2459549341,issue,closed,completed,How to build python .whl on cmake with flex delegate support? ,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Linux ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to build tflite with flex delegate support on risc-v. I tried it through the ```build_pip_package_with_cmake.sh``` file, adding the necessary code for compilation under risc-v. It works, but some models require flex delegate. 
I saw that this can be done through bazel in readme, but I need to do this with cmake. How can I do this? (if I can)

### Standalone code to reproduce the issue

```shell
If you want to use TF ops with Python API, you need to enable flex support.
You can build TFLite interpreter with flex ops support by providing
`--define=tflite_pip_with_flex=true` to Bazel.

Is there something similar for cmake?
```


### Relevant log output

_No response_",ismukhin,2024-08-11 11:26:46+00:00,"['pkgoogle', 'sawantkumar']",2024-09-18 01:59:00+00:00,2024-09-18 01:58:51+00:00,https://github.com/tensorflow/tensorflow/issues/73533,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:lite-flex', ''), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2327351495, 'issue_id': 2459549341, 'author': 'pkgoogle', 'body': ""Hi @ismukhin, there is currently no way to build them with cmake, unless you make your own CMakeLists.txt file (I don't recommend that unless you have a lot of time to figure it all out). Is there a reason you can't use Bazel?"", 'created_at': datetime.datetime(2024, 9, 3, 20, 11, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342463137, 'issue_id': 2459549341, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 58, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336570, 'issue_id': 2459549341, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336777, 'issue_id': 2459549341, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73533"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73533"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 59, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-09-03 20:11:20 UTC): Hi @ismukhin, there is currently no way to build them with cmake, unless you make your own CMakeLists.txt file (I don't recommend that unless you have a lot of time to figure it all out). Is there a reason you can't use Bazel?

github-actions[bot] on (2024-09-11 01:58:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:58:59 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73533"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73533"">No</a>

"
2459538470,issue,closed,completed,AttributeError: 'Sequential' object has no attribute '_get_save_spec',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",sakshigithubit,2024-08-11 10:55:46+00:00,['Venkat6871'],2024-09-19 04:13:13+00:00,2024-09-19 02:00:07+00:00,https://github.com/tensorflow/tensorflow/issues/73532,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2327915214, 'issue_id': 2459538470, 'author': 'sawantkumar', 'body': 'Hi @sakshigithubit ,\r\n\r\nCan you give more details , like which tensorflow version and your os details?', 'created_at': datetime.datetime(2024, 9, 4, 4, 50, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102342, 'issue_id': 2459538470, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826553, 'issue_id': 2459538470, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 6, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-09-04 04:50:31 UTC): Hi @sakshigithubit ,

Can you give more details , like which tensorflow version and your os details?

github-actions[bot] on (2024-09-12 01:58:29 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:06 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2459534970,issue,closed,completed,Conv2D is no longer supporting Masking in TF v2.17.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS and Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=12.6.0, cudnn=8.9.7.29

### GPU model and memory

_No response_

### Current behavior?

Dear TF team,

Conv2D layer no longer supports Masking layer in TensorFlow v2.17.0.

Due the dimensions of our input (i.e. (timesteps, width, channels)), size of the input shape (i.e. (2048, 2000, 3)) and size of the dataset (i.e. over 1 million samples), it is not practical to use LSTM, GRU, RNN or ConvLSTM1D layers, and therefore, Conv2D layers worked sufficiently well in our applications. The gaps in the dataset was handled with the Masking layer, and the masking layer was compatible with the Conv layers (among other layers, such as Cropping) from all TF versions up to (and including) TF v2.16. However, in TF v2.17.0, we get the following user warning ""Layer 'conv2d' (of type Conv2D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask"".

Is this a bug in TF v2.17.0?
Or is this feature now depreciated in TF v2.17.0?
Would you be able to reintroduce this feature in future versions?

Best

Kav

### Standalone code to reproduce the issue

```shell
LINK TO COLAB NOTEBOOK:
https://colab.research.google.com/drive/102k6UNSKb-d03DcmcUtCxmV9Qz9bjZoD?usp=drive_link


STANDALONE CODE:
from tensorflow.keras.layers import Conv2D, Masking, Flatten
from tensorflow.keras import Model, Input

batch = 1
timesteps = 10
width = 10
channels = 2
filters = 4
kernel_size = 3
mask_value = -1

x_input = Input(shape=(timesteps, width, channels))
x_masking = Masking(mask_value)(x_input)
x_conv2d = Conv2D(filters, kernel_size)(x_masking)
x_flatten = Flatten()(x_conv2d)

model = Model(x_input, x_flatten)
model.compile(loss='mse')
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'conv2d' (of type Conv2D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.
  warnings.warn(
```
",kavjayawardana,2024-08-11 10:45:11+00:00,['tilakrayal'],2024-08-14 10:59:39+00:00,2024-08-14 10:59:36+00:00,https://github.com/tensorflow/tensorflow/issues/73531,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2282810053, 'issue_id': 2459534970, 'author': 'Atg1234e', 'body': 'How  to  install  pip', 'created_at': datetime.datetime(2024, 8, 11, 16, 17, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283427092, 'issue_id': 2459534970, 'author': 'tilakrayal', 'body': '@kavjayawardana,\r\nLooks like this is an issue related to keras. Could you please raise the issue in the keras-team/keras repo for quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 12, 8, 53, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283912878, 'issue_id': 2459534970, 'author': 'kavjayawardana', 'body': ""@tilakrayal, Thank you for the prompt reply. I've just raised the issue at keras-team/keras repo. I'll close this thread once I hear form them. Thanks again\r\nbest\r\n\r\nKav"", 'created_at': datetime.datetime(2024, 8, 12, 12, 56, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285853865, 'issue_id': 2459534970, 'author': 'tilakrayal', 'body': '@kavjayawardana,\r\nThank you for raising the issue in the Keras repo. .Can you please close this issue, since it is already being tracked there?', 'created_at': datetime.datetime(2024, 8, 13, 9, 56, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288449763, 'issue_id': 2459534970, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73531"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73531"">No</a>', 'created_at': datetime.datetime(2024, 8, 14, 10, 59, 38, tzinfo=datetime.timezone.utc)}]","Atg1234e on (2024-08-11 16:17:47 UTC): How  to  install  pip

tilakrayal (Assginee) on (2024-08-12 08:53:02 UTC): @kavjayawardana,
Looks like this is an issue related to keras. Could you please raise the issue in the keras-team/keras repo for quick resolution. Thank you!

kavjayawardana (Issue Creator) on (2024-08-12 12:56:06 UTC): @tilakrayal, Thank you for the prompt reply. I've just raised the issue at keras-team/keras repo. I'll close this thread once I hear form them. Thanks again
best

Kav

tilakrayal (Assginee) on (2024-08-13 09:56:59 UTC): @kavjayawardana,
Thank you for raising the issue in the Keras repo. .Can you please close this issue, since it is already being tracked there?

google-ml-butler[bot] on (2024-08-14 10:59:38 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73531"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73531"">No</a>

"
2459339939,issue,closed,completed,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows10

### Mobile device

a

### Python version

3.12

### Bazel version

a

### GCC/compiler version

a

### CUDA/cuDNN version

a

### GPU model and memory

a

### Current behavior?

a

### Standalone code to reproduce the issue

```shell
a
```


### Relevant log output

```shell
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.     

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\ariel\AppData\Local\Programs\Python\Python312\Lib\threading.py"", line 1052, in _bootstrap_inner
    self.run()
  File ""C:\Users\ariel\AppData\Local\Programs\Python\Python312\Lib\threading.py"", line 989, in run
    self._target(*self._args, **self._kwargs)
  File ""G:\Server-report\venv\Lib\site-packages\django\utils\autoreload.py"", line 64, in wrapper
    fn(*args, **kwargs)
  File ""G:\Server-report\venv\Lib\site-packages\django\core\management\commands\runserver.py"", line 133, in inner_run
    self.check(display_num_errors=True)
  File ""G:\Server-report\venv\Lib\site-packages\django\core\management\base.py"", line 486, in check
    all_issues = checks.run_checks(
                 ^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\core\checks\registry.py"", line 88, in run_checks
    new_errors = check(app_configs=app_configs, databases=databases)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\core\checks\urls.py"", line 14, in check_url_config
    return check_resolver(resolver)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\core\checks\urls.py"", line 24, in check_resolver
    return check_method()
           ^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\urls\resolvers.py"", line 519, in check
    for pattern in self.url_patterns:
                   ^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\utils\functional.py"", line 47, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
                                         ^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\urls\resolvers.py"", line 738, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
                       ^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\utils\functional.py"", line 47, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
                                         ^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\django\urls\resolvers.py"", line 731, in urlconf_module
    return import_module(self.urlconf_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\ariel\AppData\Local\Programs\Python\Python312\Lib\importlib\__init__.py"", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap>"", line 1381, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1354, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1325, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 929, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 994, in exec_module
  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed
  File ""G:\Server-report\report\urls.py"", line 3, in <module>
    from chat import views as views_chat
  File ""G:\Server-report\chat\views.py"", line 22, in <module>
    model = TFAutoModel.from_pretrained(model_name)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 563, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 384, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 735, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 749, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 693, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\utils\import_utils.py"", line 1576, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""G:\Server-report\venv\Lib\site-packages\transformers\utils\import_utils.py"", line 1588, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):   
Traceback (most recent call last):
  File ""G:\Server-report\venv\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).
```
",arielmedinaa,2024-08-11 00:01:14+00:00,['Venkat6871'],2024-08-14 11:48:33+00:00,2024-08-14 11:48:30+00:00,https://github.com/tensorflow/tensorflow/issues/73525,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2285446842, 'issue_id': 2459339939, 'author': 'Venkat6871', 'body': 'Hi **@arielmedinaa** ,\r\nThere are at least 3 possible scenarios:\r\nYou need to install the MSVC 2019 redistributable\r\nYour CPU does not support AVX2 instructions\r\nYour CPU/Python is on 32 bits\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nAlso in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 13, 6, 36, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286035625, 'issue_id': 2459339939, 'author': 'arielmedinaa', 'body': ""> Hola**@arielmedinaa**Hay al menos 3 escenarios posibles: Necesita instalar el redistribuible MSVC 2019 Su CPU no admite instrucciones AVX2 Su CPU/Python es de 32 bits Hay una biblioteca que est en una ubicacin diferente/no est instalada en su sistema y que no se puede cargar.\r\n> \r\n> Adems, para acelerar el proceso de resolucin de problemas, podra proporcionar la siguiente informacin? Plataforma y distribucin del sistema operativo (por ejemplo, Linux Ubuntu 16.04): Dispositivo mvil (por ejemplo, iPhone 8, Pixel 2, Samsung Galaxy) si el problema ocurre en el dispositivo mvil: TensorFlow instalado desde (fuente o binario): Instalado usando virtualenv? pip? conda?: Versin de Bazel (si se compila desde la fuente): Versin de GCC/compilador (si se compila desde la fuente): Versin de CUDA/cuDNN: Modelo de GPU y memoria: y la secuencia exacta de comandos/pasos que ejecut antes de encontrarse con el problema. #61887\r\n> \r\n> Gracias!\r\n\r\nI think it's more because of my cpu, the error I was having was not repeated on other machines that had a Windows operating system (and one of them Linux). But it is already solved, it uses an environment and to install packages I used pip (now that I am on Linux I must use pip3) thats right?"", 'created_at': datetime.datetime(2024, 8, 13, 11, 40, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287891877, 'issue_id': 2459339939, 'author': 'Venkat6871', 'body': 'Hi **@arielmedinaa** ,\r\n- pip should work fine in this environment. Please refer to the following [link](https://pypi.org/project/tensorflow/) for instructions. I am glad to hear the issue is resolved. Feel free to close this issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 14, 5, 35, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288533020, 'issue_id': 2459339939, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73525"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73525"">No</a>', 'created_at': datetime.datetime(2024, 8, 14, 11, 48, 32, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-13 06:36:40 UTC): Hi **@arielmedinaa** ,
There are at least 3 possible scenarios:
You need to install the MSVC 2019 redistributable
Your CPU does not support AVX2 instructions
Your CPU/Python is on 32 bits
There is a library that is in a different location/not installed on your system that cannot be loaded.

Also in order to expedite the trouble-shooting process, could you please provide the following information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
Installed using virtualenv? pip? conda?:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
and the exact sequence of commands / steps that you executed before running into the problem.
https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

arielmedinaa (Issue Creator) on (2024-08-13 11:40:41 UTC): I think it's more because of my cpu, the error I was having was not repeated on other machines that had a Windows operating system (and one of them Linux). But it is already solved, it uses an environment and to install packages I used pip (now that I am on Linux I must use pip3) thats right?

Venkat6871 (Assginee) on (2024-08-14 05:35:21 UTC): Hi **@arielmedinaa** ,
- pip should work fine in this environment. Please refer to the following [link](https://pypi.org/project/tensorflow/) for instructions. I am glad to hear the issue is resolved. Feel free to close this issue.
Thank you!

google-ml-butler[bot] on (2024-08-14 11:48:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73525"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73525"">No</a>

"
2459229472,issue,closed,completed,Can the TensorFlow  Android object detection example support MobileNetV2 and MobileNetV3 tflite models?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can the TensorFlow  Android object detection example https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection support MobileNetV2 and MobileNetV3 tflite models?It seems that it only supports MobileNetV1?

### Standalone code to reproduce the issue

```shell
......
```


### Relevant log output

_No response_",libofei2004,2024-08-10 17:50:44+00:00,"['gaikwadrahul8', 'sawantkumar']",2024-10-12 02:00:05+00:00,2024-10-12 02:00:03+00:00,https://github.com/tensorflow/tensorflow/issues/73522,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:lite-examples', 'TensorFlow Lite Examples'), ('TF 2.8', '')]","[{'comment_id': 2303976225, 'issue_id': 2459229472, 'author': 'sawantkumar', 'body': 'Hi @libofei2004 ,\r\n\r\nI tried plugging in the mobilenetv2 in the object detection example. It is giving error but i think with some code changes you can get it to work.', 'created_at': datetime.datetime(2024, 8, 22, 7, 30, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305062467, 'issue_id': 2459229472, 'author': 'libofei2004', 'body': '@sawantkumar Maybebut do you know how?', 'created_at': datetime.datetime(2024, 8, 22, 15, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338793743, 'issue_id': 2459229472, 'author': 'gaikwadrahul8', 'body': ""Hi, @libofei2004 \r\n\r\nI apologize for the delayed response, I see at the moment we support object detection models mentioned in this file https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/download_models.gradle for object detection TFLite examples, if you're looking to use `ssd_mobilenet_v2` then you can download TFLite model from here https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v2 and you may have to change your code as per the `ssd_mobilenet_v2` model (input and output), please check this [github repo](https://github.com/sayannath/American-Sign-Language-Detection) which may help you.\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 9, 18, 19, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344190228, 'issue_id': 2459229472, 'author': 'libofei2004', 'body': '@gaikwadrahul8 [github repo](https://github.com/sayannath/American-Sign-Language-Detection) this repo is just provides classification, but what I want is object detection.', 'created_at': datetime.datetime(2024, 9, 11, 16, 55, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376979101, 'issue_id': 2459229472, 'author': 'gaikwadrahul8', 'body': 'Hi, @libofei2004 \r\n\r\nI apologize for the delayed response, you can refer this [MediaPipe Tasks Object Detection Android Demo](https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/object_detection/android) with the option to use a quantized [MobileNetV2](https://storage.cloud.google.com/tf_model_garden/vision/qat/mobilenetv2_ssd_coco/mobilenetv2_ssd_256_uint8.tflite) please refer this [Object detection guide for Android](https://ai.google.dev/edge/mediapipe/solutions/vision/object_detector/android)\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 26, 13, 28, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392646969, 'issue_id': 2459229472, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408301699, 'issue_id': 2459229472, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 12, 2, 0, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408301728, 'issue_id': 2459229472, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73522"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73522"">No</a>', 'created_at': datetime.datetime(2024, 10, 12, 2, 0, 4, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-08-22 07:30:47 UTC): Hi @libofei2004 ,

I tried plugging in the mobilenetv2 in the object detection example. It is giving error but i think with some code changes you can get it to work.

libofei2004 (Issue Creator) on (2024-08-22 15:38:00 UTC): @sawantkumar Maybebut do you know how?

gaikwadrahul8 (Assginee) on (2024-09-09 18:19:40 UTC): Hi, @libofei2004 

I apologize for the delayed response, I see at the moment we support object detection models mentioned in this file https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/download_models.gradle for object detection TFLite examples, if you're looking to use `ssd_mobilenet_v2` then you can download TFLite model from here https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v2 and you may have to change your code as per the `ssd_mobilenet_v2` model (input and output), please check this [github repo](https://github.com/sayannath/American-Sign-Language-Detection) which may help you.

Thank you for your cooperation and patience.

libofei2004 (Issue Creator) on (2024-09-11 16:55:36 UTC): @gaikwadrahul8 [github repo](https://github.com/sayannath/American-Sign-Language-Detection) this repo is just provides classification, but what I want is object detection.

gaikwadrahul8 (Assginee) on (2024-09-26 13:28:05 UTC): Hi, @libofei2004 

I apologize for the delayed response, you can refer this [MediaPipe Tasks Object Detection Android Demo](https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/object_detection/android) with the option to use a quantized [MobileNetV2](https://storage.cloud.google.com/tf_model_garden/vision/qat/mobilenetv2_ssd_coco/mobilenetv2_ssd_256_uint8.tflite) please refer this [Object detection guide for Android](https://ai.google.dev/edge/mediapipe/solutions/vision/object_detector/android)

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-04 02:01:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-12 02:00:02 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-12 02:00:04 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73522"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73522"">No</a>

"
2459223287,issue,closed,completed,Can tflite model maker be used to train MobileNet models?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can tflite model maker  be used to train MobileNetv1 or MobileNetv2 models?I know It can be used to train efficientdet_lite model.

### Standalone code to reproduce the issue

```shell
Can tflite model maker  be used to train MobileNetv1 or MobileNetv2 models?I know It can be used to train efficientdet_lite model.
```


### Relevant log output

_No response_",libofei2004,2024-08-10 17:43:17+00:00,['pkgoogle'],2024-11-09 01:58:46+00:00,2024-11-09 01:58:43+00:00,https://github.com/tensorflow/tensorflow/issues/73521,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.8', '')]","[{'comment_id': 2320621315, 'issue_id': 2459223287, 'author': 'sawantkumar', 'body': ""Hi @libofei2004 ,\r\n\r\nI'm sorry, but TFLite Model Maker is currently experiencing installation issues, and it is unlikely to be resolved soon. Therefore, I recommend using MediaPipe Model Maker for now. Please check out this [link](https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/image_classifier.ipynb#scrollTo=E6Ar9Os1E3de) which demonstrates training a MobileNetV2 model using MediaPipe Model Maker. Let me know if you encounter any issues."", 'created_at': datetime.datetime(2024, 8, 30, 9, 21, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321213967, 'issue_id': 2459223287, 'author': 'libofei2004', 'body': '@sawantkumar I tried MediaPipe Model Maker.But I meet some issues, like:\r\nhttps://github.com/google-ai-edge/mediapipe/issues/5557\r\nhttps://github.com/google-ai-edge/mediapipe-samples/issues/432\r\nhttps://github.com/tensorflow/tensorflow/issues/73522', 'created_at': datetime.datetime(2024, 8, 30, 13, 12, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393522765, 'issue_id': 2459223287, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 10, 4, 11, 48, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436456097, 'issue_id': 2459223287, 'author': 'pkgoogle', 'body': ""Hi @libofei2004, you will have to resolve the mediapipe issues as tflite-model-maker isn't working right now."", 'created_at': datetime.datetime(2024, 10, 24, 22, 33, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157775, 'issue_id': 2459223287, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985917, 'issue_id': 2459223287, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985944, 'issue_id': 2459223287, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73521"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73521"">No</a>', 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 45, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-08-30 09:21:40 UTC): Hi @libofei2004 ,

I'm sorry, but TFLite Model Maker is currently experiencing installation issues, and it is unlikely to be resolved soon. Therefore, I recommend using MediaPipe Model Maker for now. Please check out this [link](https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/image_classifier.ipynb#scrollTo=E6Ar9Os1E3de) which demonstrates training a MobileNetV2 model using MediaPipe Model Maker. Let me know if you encounter any issues.

libofei2004 (Issue Creator) on (2024-08-30 13:12:02 UTC): @sawantkumar I tried MediaPipe Model Maker.But I meet some issues, like:
https://github.com/google-ai-edge/mediapipe/issues/5557
https://github.com/google-ai-edge/mediapipe-samples/issues/432
https://github.com/tensorflow/tensorflow/issues/73522

gaikwadrahul8 on (2024-10-04 11:48:15 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-10-24 22:33:55 UTC): Hi @libofei2004, you will have to resolve the mediapipe issues as tflite-model-maker isn't working right now.

github-actions[bot] on (2024-11-01 02:07:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-09 01:58:43 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-09 01:58:45 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73521"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73521"">No</a>

"
2459088735,issue,open,,Segmentation fault (core dumped) in `tf.config.threading.set_inter_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input boundary values into tf.config.threading.set_intra_op_parallelism_threads

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/12s6D2GuBFEWAdvFdCvjbs4nK888vLGvm?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:36:47.636016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:36:47.703371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:36:47.791020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:36:47.817978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:36:47.883418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:36:56.611712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-08-10 21:36:56.617085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1717 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Segmentation fault (core dumped)
```
",x0w3n,2024-08-10 13:39:20+00:00,['tilakrayal'],2024-09-03 07:57:51+00:00,,https://github.com/tensorflow/tensorflow/issues/73519,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2293903723, 'issue_id': 2459088735, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nI was able to reproduce the issue in both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/1a275471a913d5863dad95e3feb8d7f2/untitled2076.ipynb). Please allow some time to deepdive into the issue and debug the same. Thank you!', 'created_at': datetime.datetime(2024, 8, 16, 17, 40, 12, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-16 17:40:12 UTC): @x0w3n,
I was able to reproduce the issue in both tensorflow v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/1a275471a913d5863dad95e3feb8d7f2/untitled2076.ipynb). Please allow some time to deepdive into the issue and debug the same. Thank you!

"
2459086929,issue,open,,Aborted (core dumped) in `tf.config.threading.set_intra_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input negative numbers into tf.config.threading.set_intra_op_parallelism_threads.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/17JV6ppGU1XtQg25PKa8itDhihAspgHIz?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:29:55.538868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:29:55.869155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:29:55.967845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:29:56.002644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:29:56.222202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:30:05.961788: F external/local_tsl/tsl/platform/threadpool.cc:112] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```
",x0w3n,2024-08-10 13:33:57+00:00,['Venkat6871'],2024-08-19 04:17:27+00:00,,https://github.com/tensorflow/tensorflow/issues/73518,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2285309301, 'issue_id': 2459086929, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/c07595cf2a566466e5650bcacb2d0d99/73518_2-17-0-nightly-v.ipynb) here for reference. \r\nThank you!', 'created_at': datetime.datetime(2024, 8, 13, 4, 17, 52, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-13 04:17:52 UTC): I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/c07595cf2a566466e5650bcacb2d0d99/73518_2-17-0-nightly-v.ipynb) here for reference. 
Thank you!

"
2459045322,issue,closed,completed,"Converting model for on-device training fails with ""LLVM ERROR: Failed to infer result type(s).""","### 1. System information

- OS Platform and Distribution: Debian GNU/Linux 10
- TensorFlow installation: pip package (Python 3.12.4)
- TensorFlow library version: 2.17.0
### 2. Code
I'm trying to follow the instructions that are given here: [](https://www.tensorflow.org/lite/examples/on_device_training/overview)

The model can be saved with tf.saved_model.save(), including the custom function signatures (Although the code from the example has to be adapted slightly to work with TensorFlow 2.17.0).  But the conversion step fails with the following output:

```
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1723288568.132630   44010 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.
W0000 00:00:1723288568.132682   44010 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.
2024-08-10 13:16:08.136142: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: saved_model
2024-08-10 13:16:08.138981: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-10 13:16:08.139000: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: saved_model
2024-08-10 13:16:08.150149: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-08-10 13:16:08.151022: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-10 13:16:08.207132: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: saved_model
2024-08-10 13:16:08.214336: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 78198 microseconds.
2024-08-10 13:16:08.244932: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", ""sequential_1/dense_1/Add/ReadVariableOp@__inference_infer_97""]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).
Aborted
```

Below is a minimal example that recreates the error.
```
import tensorflow as tf

class Model(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(9,)),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(9)
            ])

        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(),
            loss=tf.keras.losses.MeanSquaredError())
    
    @tf.function(input_signature=[
        tf.TensorSpec([None, 9], tf.float32),
    ])
    def infer(self, x): 
        prediction = self.model(x)
        return prediction

model_custom = Model()
SAVED_MODEL_DIR = ""saved_model""

tf.saved_model.save(
    model_custom,
    SAVED_MODEL_DIR,
    signatures={
        ""infer"":
            model_custom.infer.get_concrete_function(),
        })

converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
converter.allow_custom_ops = True
tflite_model = converter.convert()
```

",hanssssssssssssss,2024-08-10 11:30:27+00:00,['Venkat6871'],2024-09-19 04:09:19+00:00,2024-09-19 02:00:10+00:00,https://github.com/tensorflow/tensorflow/issues/73517,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2281197097, 'issue_id': 2459045322, 'author': 'hanssssssssssssss', 'body': ""It looks like github removed the link to the instructions that I'm referring to. Anyway, it can be found on tensorflow dot org /lite/examples/on_device_training/overview"", 'created_at': datetime.datetime(2024, 8, 10, 11, 34, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328342417, 'issue_id': 2459045322, 'author': 'sawantkumar', 'body': 'Hi @hanssssssssssssss ,\r\n\r\nI was wondering if you may be able to resolve your issue by using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch), you can find more information here: [googleblog](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/).\r\n\r\nI have actually created a simple script for converting your model to tflite \r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nimport ai_edge_torch\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.model = nn.Sequential(\r\n            nn.Linear(9, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 9)\r\n        )\r\n\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n# Create the model instance\r\nmodel = Model()\r\n\r\n# Example input that matches the expected input shape of (batch_size, 9)\r\nsample_inputs = (torch.randn(1, 9),)\r\n\r\n# Convert the PyTorch model to a TensorFlow Lite model using ai_edge_torch\r\nedge_model = ai_edge_torch.convert(model.eval(), sample_inputs)\r\n\r\n# Export the converted model to a .tflite file\r\nedge_model.export(""model.tflite"")\r\n```\r\n\r\nUsing this script i was able to get the model.tflite file without any errors. If you want to, you can actually try visualizing the result in [model-explorer](https://github.com/google-ai-edge/model-explorer) as well.\r\n\r\nPlease try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.', 'created_at': datetime.datetime(2024, 9, 4, 9, 17, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102362, 'issue_id': 2459045322, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826621, 'issue_id': 2459045322, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826676, 'issue_id': 2459045322, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73517"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73517"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 12, tzinfo=datetime.timezone.utc)}]","hanssssssssssssss (Issue Creator) on (2024-08-10 11:34:43 UTC): It looks like github removed the link to the instructions that I'm referring to. Anyway, it can be found on tensorflow dot org /lite/examples/on_device_training/overview

sawantkumar on (2024-09-04 09:17:24 UTC): Hi @hanssssssssssssss ,

I was wondering if you may be able to resolve your issue by using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch), you can find more information here: [googleblog](https://developers.googleblog.com/en/ai-edge-torch-high-performance-inference-of-pytorch-models-on-mobile-devices/).

I have actually created a simple script for converting your model to tflite 

```py
import torch
import torch.nn as nn
import ai_edge_torch

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(9, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 9)
        )

    def forward(self, x):
        return self.model(x)

# Create the model instance
model = Model()

# Example input that matches the expected input shape of (batch_size, 9)
sample_inputs = (torch.randn(1, 9),)

# Convert the PyTorch model to a TensorFlow Lite model using ai_edge_torch
edge_model = ai_edge_torch.convert(model.eval(), sample_inputs)

# Export the converted model to a .tflite file
edge_model.export(""model.tflite"")
```

Using this script i was able to get the model.tflite file without any errors. If you want to, you can actually try visualizing the result in [model-explorer](https://github.com/google-ai-edge/model-explorer) as well.

Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.

github-actions[bot] on (2024-09-12 01:58:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:10 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 02:00:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73517"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73517"">No</a>

"
2458950170,issue,open,,Issue with softmax warning appearing in Tensorflow 2.17.0,"### TensorFlow version

2.17.0

### OS platform and distribution

Google Colab

### Current behavior?

Hi, everyone.

I am practicing implementing a Transformer model that machine translates English into Korean by reading TensorFlow guides and books. However, I am having trouble because an unknown UserWarning appears during the final translation process. In that issue, I've never used softmax before, but warning me about using it. This problem appears after the model has finished training and when making inferences.

I searched to see if there were any cases similar to mine, but it seems that no solution was found in any of them. 



same issue: https://github.com/tensorflow/tensorflow/issues/67758



This problem did not exist in tensorflow 2.15 but appeared in 2.17.0. I can't even guess what could be causing it. For those of you who are curious about the full code, I am leaving a Colab link. You can easily reproduce it by running it with Ctrl + F9 in Google Colab. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom.

Colab Link: https://colab.research.google.com/drive/1IMFWoJ1s5ReKU9LYENROpAsZ47D6cG8T?usp=sharing

The data I used is 'kor-eng.zip' located at ""https://www.manythings.org/anki/"".

I'm really sorry for not writing the comments in English.

### Standalone code to reproduce the issue

```shell
class Transformer(keras.Model):
    def __init__(self, *, num_layers, encoder_sequence_length, decoder_sequence_length, source_vocab_size, target_vocab_size, embed_dim,
                 dense_dim, num_heads, dropout_rate):
        super().__init__()
        self.encoder = Encoder(num_layers=num_layers, sequence_length=encoder_sequence_length, input_dim=source_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.decoder = Decoder(num_layers=num_layers, sequence_length=decoder_sequence_length, input_dim=target_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.final_layer = tf.keras.layers.Dense(units=target_vocab_size)

    def call(self, inputs):
        encoder_inputs, decoder_inputs = inputs
        encoder_pad_mask = tf.math.not_equal(encoder_inputs, 0)[:, tf.newaxis]
        decoder_pad_mask = tf.math.not_equal(decoder_inputs, 0)[:, tf.newaxis]

        decoder_sequence_length = tf.shape(decoder_inputs)[1]
        causal_mask = tf.linalg.band_part(tf.ones((decoder_sequence_length, decoder_sequence_length), tf.bool), -1, 0)

        encoder_inputs = self.encoder(inputs=encoder_inputs, encoder_pad_mask=encoder_pad_mask)  # Shape: (batch_size, encoder_sequence_length, embed_dim)
        decoder_inputs = self.decoder(inputs=decoder_inputs, encoder_outputs=encoder_inputs,
                                      encoder_pad_mask=encoder_pad_mask, decoder_pad_mask=decoder_pad_mask,
                                      causal_mask=causal_mask)  # Shape: (batch_size, decoder_sequence_length, embed_dim)

        logits = self.final_layer(decoder_inputs)  # Shape: (batch_size, decoder_sequence_length, target_vocab_size)

        try:
            # losses/metrics   keras_mask 
            del logits._keras_mask
        except AttributeError:
            pass

        return logits


class Translator(tf.Module):
    #   
    @staticmethod
    def preprocess_text(text_: str, max_repeat: int=2) -> str:
        """"""        

        Args:
            text_:   -> str
            max_repeat:         -> int

        Returns:
            text_:     -> str
        """"""

        text_ = text_.lower()
        text_ = re.sub(pattern=rf""[^\w\s{string.punctuation}]"", repl=r"""", string=text_)
        text_ = re.sub(pattern=r""\?+"", repl=r""?"", string=text_) # ? 2   ? 
        text_ = re.sub(pattern=r""\!+"", repl=r""!"", string=text_) # ! 2   ! 
        text_ = re.sub(pattern=r""(?P<char>\D)(?P=char){"" + str(max_repeat - 1) + r"",}"", repl=r""\g<char>"" * max_repeat, string=text_) #     repeat   repeat 
        text_ = re.sub(pattern=r""\.{2,}"", repl=r""..."", string=text_) # .. ... 
        text_ = re.sub(pattern=r""\.\.\.(?P<s>\w)"", repl=r""... \g<s>"", string=text_) #  
        text_ = re.sub(pattern=r""\s+"", repl="" "", string=text_)

        #   
        # 's  is / has     
        # 'd  had / would / could   
        # 'll  shall / will  
        text_ = re.sub(pattern=r""\bi'm\b"", repl=r""i am"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>you|we|they|there|who|when|where|what|how|why)'re\b"", repl=r""\g<subj> are"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<verb>is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", repl=r""\g<verb> not"", string=text_)
        text_ = re.sub(pattern=r""\bwon't\b"", repl=r""will not"", string=text_)
        text_ = re.sub(pattern=r""\bcan't\b"", repl=r""can not"", string=text_)
        text_ = re.sub(pattern=r""\bshan't\b"", repl=r""shall not"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>i|you|they|we|should|could|would|must|not)'ve\b"", repl=r""\g<subj> have"", string=text_)

        text_ = text_.strip()
        return text_

    # tensorflow     
    @tf.function
    def tf_preprocess_text(text_: tf.string, max_repeat: int=2) -> tf.string:
        """"""        

        Args:
            text_:   -> tf.string
            max_repeat:         -> int

        Returns:
            text_:     -> tf.string
        """"""

        text_ = tf.strings.lower(input=text_)
        text_ = tf.strings.regex_replace(input=text_, pattern=rf""[^\w\s{string.punctuation}]"", rewrite=r"""")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\?+"", rewrite=r""?"") # ? 2   ? 
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\!+"", rewrite=r""!"") # ! 2   ! 

        #     repeat   repeat 
        stacks = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)
        for s in tf.strings.bytes_split(text_):
            if stacks.size() >= max_repeat:
                if tf.strings.regex_full_match(input=s, pattern=r""\D""):
                    back_s = stacks.gather(indices=tf.range(start=stacks.size() - max_repeat, limit=stacks.size(), delta=1))
                    if tf.math.reduce_all(back_s == s):
                        continue

            stacks = stacks.write(stacks.size(), s)

        text_ = tf.strings.reduce_join(inputs=stacks.stack())

        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.{2,}"", rewrite=r""..."") # .. ... 
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.\.\.(\w)"", rewrite=r""... \1"") #  
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\s+"", rewrite=r"" "")

        #   
        # 's  is / has     
        # 'd  had / would / could   
        # 'll  shall / will  
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bi'm\b"", rewrite=r""i am"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(you|we|they|there|who|when|where|what|how|why)'re\b"", rewrite=r""\1 are"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", rewrite=r""\1 not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bwon't\b"", rewrite=r""will not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bcan't\b"", rewrite=r""can not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bsha't\b"", rewrite=r""shall not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(i|you|they|we|should|could|would|must|not)'ve\b"", rewrite=r""\1 have"")

        text_ = tf.strings.strip(input=text_)
        return text_

    def __init__(self, source_tokenizer, target_tokenizer, target_length, model):
        super().__init__()
        self.source_tokenizer = source_tokenizer
        self.target_tokenizer = target_tokenizer
        self.target_length = target_length
        self.model = model

    def __call__(self, sentence):
        sentence = Translator.tf_preprocess_text(text_=sentence)
        sentence_token = self.source_tokenizer.tokenize(sentence)[tf.newaxis]

        encoder_input = sentence_token

        starts = tf.constant(2, dtype=tf.int32)[tf.newaxis]
        ends = tf.constant(3, dtype=tf.int32)[tf.newaxis]
        decoder_token = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
        decoder_token = decoder_token.write(0, starts)

        for i in tf.range(self.target_length):
            decoder_input = tf.transpose(decoder_token.stack())
            predictions = self.model([encoder_input, decoder_input], training=False)
            predictions = predictions[0, -1, :] #   
            predicted_id = tf.argmax(input=predictions, output_type=tf.int32)[tf.newaxis]

            if predicted_id == ends:
                break

            decoder_token = decoder_token.write(i + 1, predicted_id)

        decoder_token = tf.transpose(decoder_token.stack())[0]
        decoder_token = decoder_token[1:]

        return self.target_tokenizer.detokenize(decoder_token)
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
  warnings.warn(
English: tom came here to learn french.
Translated Korean:     . 
Real Korean: <s>     . </s>
```
",privatepeople,2024-08-10 07:12:43+00:00,['Venkat6871'],2024-08-19 11:48:13+00:00,,https://github.com/tensorflow/tensorflow/issues/73516,"[('type:bug', 'Bug'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2282368734, 'issue_id': 2458950170, 'author': 'privatepeople', 'body': 'I modified some parts of the code to make it easy to reproduce by just running it. And I changed it to a Colab link rather than a Github link. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom. And although not all of them were edited, some comments were edited to English rather than Korean. I will edit the remaining parts so that you do not have any inconvenience while reading them.', 'created_at': datetime.datetime(2024, 8, 11, 3, 10, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288697520, 'issue_id': 2458950170, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.17.0 and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/b538bf22e88953cfff745c954b071989/73516_2-17-0.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 14, 13, 9, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296382845, 'issue_id': 2458950170, 'author': 'Bresenham', 'body': ""I've the same problem. Is there a solution yet? Can we ignore this warning?"", 'created_at': datetime.datetime(2024, 8, 19, 11, 48, 12, tzinfo=datetime.timezone.utc)}]","privatepeople (Issue Creator) on (2024-08-11 03:10:46 UTC): I modified some parts of the code to make it easy to reproduce by just running it. And I changed it to a Colab link rather than a Github link. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom. And although not all of them were edited, some comments were edited to English rather than Korean. I will edit the remaining parts so that you do not have any inconvenience while reading them.

Venkat6871 (Assginee) on (2024-08-14 13:09:01 UTC): I tried to run your code on Colab using TF v2.17.0 and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/b538bf22e88953cfff745c954b071989/73516_2-17-0.ipynb) here for reference.
Thank you!

Bresenham on (2024-08-19 11:48:12 UTC): I've the same problem. Is there a solution yet? Can we ignore this warning?

"
2458639804,issue,closed,completed,"""Skipping the delay kernel"" warning emitted thousands of times.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No - I get other errors when I install tf-nightly.

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Rocky Linux 9 and unknown

### Mobile device

_No response_

### Python version

3.12.5 and unknown

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda version 12.0 and unknown

### GPU model and memory

_No response_

### Current behavior?

I get an alarming number of warnings about skipping of a delay kernel.
This occurs on a pretty standard install of tensorflow that I use for a project. (I have been unable to reproduce it with tf-nightly because I get crashes due to incompatible shapes in some of my code.) But I have good news for reproducibility! The same log message is also created in the examples in the TF documentation, so however TF is built to generate the guides on the website (see below for links) also triggers this behavior. (This is why some of the versions are unknown - I haven't tracked down exactly how the docs are built.)

This occurs in training and in predicting, and it fills the log files with warnings that I cannot disable. I've pasted one in the section below. A typical run of my program produced just shy of 150,000 repeated warnings about skipping the delay kernel.

This warning does not seem to listen to TF_CPP_MIN_LOG_LEVEL, and it also still gets emitted after I add
tf.get_logger().setLevel(""ERROR"") and warnings.simplefilter(""ignore""). The log file attached below includes all three of those attempts to suppress the messages.

I don't know what the solution is to these warnings, and I honestly don't even know if I'm doing anything wrong.
Is it something that I need to fix? If so, where do I specify that I would like to include delay kernels when building and training a model?
Is it something that should be ignored in every case? If so, why is it printed at all? Given that the documentation, which I would assume is using tensorflow correctly, produces these warnings, I'm leaning toward this being the case. In this instance, how do I disable the message?
Where does the warning even come from? Commit 10df33f84983f1f789f1215fc766e12ad046ff7b moved the warning message to a new file (so it's now in third_party/xla/xla/stream_executor/cuda/cuda_executor.cc instead of gpu_timer.cc), but without a traceback I can't tell which call to the function is triggering the bad behavior.

I would build my own copy of TensorFlow to debug further, but I don't have root access to a machine at work and building without that has been unsuccessful.

### Standalone code to reproduce the issue

```shell
The message is present in many of the examples on the TF documentation, and often fills output boxes that are intended to contain other information, making reading the documentation rather frustrating.

https://www.tensorflow.org/guide/function

https://www.tensorflow.org/tutorials/quickstart/advanced

https://www.tensorflow.org/tutorials/generative/adversarial_fgsm

https://www.tensorflow.org/tutorials/images/transfer_learning

https://www.tensorflow.org/tutorials/generative/autoencoder

https://www.tensorflow.org/tutorials/audio/simple_audio

https://www.tensorflow.org/tutorials/generative/cyclegan

https://www.tensorflow.org/tutorials/generative/style_transfer
```


### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232760.037824 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232760.038491 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232760.039293 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.060998 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.060998 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.060998 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.061204 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.061207 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.061210 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.308930 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.309064 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.309108 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.762708 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.762862 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232760.762911 1704933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232761.182515 1705056 service.cc:146] XLA service 0x7f67c8003f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1723232761.182536 1705056 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6
I0000 00:00:1723232761.265672 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232761.265826 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232761.265879 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232761.742874 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232761.743078 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232761.743152 1704934 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232762.175289 1705161 service.cc:146] XLA service 0x7f67d0003290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1723232762.175319 1705161 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6
I0000 00:00:1723232762.269671 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232762.269891 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232762.269936 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232762.811310 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232762.811427 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723232762.811474 1704937 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723232763.316489 1705273 service.cc:146] XLA service 0x7f67f80043a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1723232763.316524 1705273 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6
I0000 00:00:1723232763.328407 1705056 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
I0000 00:00:1723232764.801232 1705161 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
W0000 00:00:1723232764.833870 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.852954 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.853403 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.853845 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.854270 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.855588 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.856585 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.857226 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.857815 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.858271 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.859216 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.859872 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.864199 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.865096 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.866303 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.866899 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.867695 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.868588 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.869332 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.870090 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.870974 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.872071 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.872808 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.874219 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.874952 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.876521 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.877960 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.879262 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.879764 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.880946 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.882610 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.883286 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.884262 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.885159 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.938520 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.939871 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.941237 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.942563 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.944529 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.946130 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.947458 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.949099 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.950547 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.951902 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.953549 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.955035 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.956444 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.959359 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.961647 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.963095 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.964935 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.968955 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.970589 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.972500 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.974051 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.975503 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.977670 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.979426 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.981071 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.982999 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.984944 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.986547 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.988105 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.989497 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.991030 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.992826 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232764.994553 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.006351 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.008144 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.010465 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.012574 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.014353 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.015983 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.017785 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.019209 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.021683 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.023968 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.026816 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.030096 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.033443 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.036090 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.038314 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.040368 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.043280 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.045456 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.048798 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.051494 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.053658 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.055930 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.058870 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.063870 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.066426 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.069411 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.071951 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.074379 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.076916 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.079077 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.081423 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.083426 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.086207 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.095480 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.097701 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.099533 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.102417 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.104760 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.106645 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.109518 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.112559 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.114620 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.117068 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.119439 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.121426 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.125844 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.128501 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.129907 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.132639 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.135037 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.136905 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.140405 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.142656 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.144536 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.147934 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.150320 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.152360 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.158254 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.160801 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.162895 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.166604 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.168923 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.170685 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.174693 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.177255 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.179223 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.189563 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.192249 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.194193 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.196283 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.199712 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.201671 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.204049 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.207716 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.209591 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.212083 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.215755 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.220159 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.222598 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.225472 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.227479 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.229706 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.231615 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.232971 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.235788 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.237162 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.239782 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.242132 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.244283 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.248891 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.252746 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.254671 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.258255 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.260796 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.262522 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.265756 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.268230 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.270198 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.273803 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.283381 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.286579 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.288364 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.290891 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.293585 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.295439 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.297559 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.300255 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.302112 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.304314 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.307786 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.310909 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.315045 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.318333 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.320218 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.322896 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.326314 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.328383 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.331175 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.333914 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.335733 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.338057 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.339842 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.344540 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.346982 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.348444 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.351905 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.353990 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.356203 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.359343 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.361575 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.363981 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.367310 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.376307 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.378576 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.380323 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.383666 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.385889 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.387699 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.391181 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.393651 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.395483 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.398535 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.400612 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.402488 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.407495 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.409849 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.411846 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.415162 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.417432 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.419402 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.423073 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.425380 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.427480 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.431200 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.433669 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.436550 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.441645 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.444023 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.446073 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.448074 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.449769 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.451185 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.453771 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.455250 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.465666 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.469818 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.472128 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.475028 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.476893 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.479211 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.482098 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.483886 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.486365 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.489674 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.491510 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.494149 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.498504 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.501810 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.504348 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.506943 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.508713 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.510955 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.513580 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.515384 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.517688 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.521208 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.523155 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.525427 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.531938 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.533916 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.536442 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.539925 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.541828 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.544377 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.547393 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.549334 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.558568 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.563588 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.565909 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.568067 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.570669 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.572723 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.574810 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.577710 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.579802 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.581933 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.584333 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.586309 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.588492 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.592051 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.595625 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.597937 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.600781 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.602961 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.605194 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.607698 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.609791 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.612048 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.614991 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.617124 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.619411 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.623481 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.627261 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.629473 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.632518 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.634773 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.637166 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.639076 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.647533 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.649634 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.651332 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.654803 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.658555 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.660239 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.663157 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.665295 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.667099 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.670007 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.672266 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.674068 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.676654 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.679067 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.680874 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.683038 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.687995 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.689779 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.692386 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.694990 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.696832 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.698930 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.701470 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.703326 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.706019 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.708725 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.710649 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.713118 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.718185 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.720031 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.726136 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.727756 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.729836 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.730946 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.733076 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.735545 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.737739 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.739550 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.741139 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.742954 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.745182 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.747946 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.751953 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.753967 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.755742 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.758301 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.760613 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.762080 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.764203 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.767168 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.769898 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.773173 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232765.776661 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.099818 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.103232 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.110863 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.117101 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.118906 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.120300 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.125715 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.134527 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.140363 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.146259 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.147661 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.149023 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.156721 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.166398 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.173983 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.181315 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.182520 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.183984 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.184996 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.187285 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.196973 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.200989 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.219062 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.237505 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.248860 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.250557 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.252136 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.253462 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.257982 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.259694 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.261177 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.263185 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.264906 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.266256 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.268344 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.270020 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.271417 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.273704 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.275516 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.276927 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.279074 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.280939 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.282233 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.284802 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.291359 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.293467 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.302223 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.311843 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.327655 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.330126 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.331492 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.332706 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.336856 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.338229 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.339504 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.343685 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.345186 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1723232766.346497 1705064 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
I have not included the remaining 1,337 warnings.
```
",mmtrebuchet,2024-08-09 20:40:06+00:00,['klucke'],2024-10-29 06:36:11+00:00,2024-08-29 14:03:04+00:00,https://github.com/tensorflow/tensorflow/issues/73487,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2285594028, 'issue_id': 2458639804, 'author': 'tilakrayal', 'body': 'Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.\r\n\r\nI request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/71791#issuecomment-2237115569\r\nhttps://github.com/tensorflow/tensorflow/issues/70947\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 13, 7, 53, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295384014, 'issue_id': 2458639804, 'author': 'aa956', 'body': ""> This occurs in training and in predicting, and it fills the log files with warnings that I cannot disable. I've pasted one in the section below. A typical run of my program produced just shy of 150,000 repeated warnings about skipping the delay kernel.\r\n\r\nGot the same problem, [partially resolved by this:](https://github.com/tensorflow/tensorflow/issues/26348#issuecomment-1206286540)\r\n\r\n```python\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*- \r\n\r\n# Suppress tensorflow spam, see https://github.com/tensorflow/tensorflow/issues/26348\r\n# Not the complete fix, cuda_executor.cc still ignores all these settings\r\n# and spams something successful NUMA node read\r\n# but probably it's best that can be done without redirecting stderr\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport tensorflow as tf\r\nimport logging\r\nlogger = tf.get_logger()\r\nlogger.setLevel(logging.ERROR) # or logging.INFO, logging.WARNING, etc.\r\n\r\n# Actual app starts here:\r\n\r\nimport argparse\r\nimport pathlib\r\nimport pprint\r\nfrom enum import Enum\r\nfrom tqdm import tqdm\r\n...\r\n\r\n```\r\n\r\nSo now instead of 1343 lines of `Skipping the delay` there's just 10 spam lines from the one line in cuda_executor.cc.\r\nStill very spammy but better than 1500 repeats of the warning.\r\n\r\n```console\r\nsimilarity ~/tmp/images/2024-08-18/\r\n  0%|                                                                                                                                 | 0/280 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1724013025.313366 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.339050 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.339224 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.340675 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.340776 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.340832 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.391595 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.391736 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1724013025.391807 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n100%|| 280/280 [01:24<00:00,  3.32it/s]\r\n```"", 'created_at': datetime.datetime(2024, 8, 18, 20, 41, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296882337, 'issue_id': 2458639804, 'author': 'forthmedia', 'body': ""Actually, the above suggestion is not a fix. Setting TF_CPP_MIN_LOG_LEVEL = 3 logs only error and fatal messages. Similar for setLevel(logging.ERROR) only error is logged. So, no change.\r\n\r\nIn my case the 'Skipping the delay kernel' warning occurs numerous times during both training and subsequent build.\r\n\r\nWe await a response."", 'created_at': datetime.datetime(2024, 8, 19, 15, 41, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312891009, 'issue_id': 2458639804, 'author': 'fanny-j', 'body': 'I am also affected by this issue and hope to get a way to clean my log from hundreds if not thousands of these messages \r\nI would like to keep other warnings alive in case I am doing something wrong or inefficient somewhere else.', 'created_at': datetime.datetime(2024, 8, 27, 15, 32, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317727430, 'issue_id': 2458639804, 'author': 'klucke', 'body': 'This is fixed by https://github.com/openxla/xla/pull/16566', 'created_at': datetime.datetime(2024, 8, 29, 13, 52, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317760879, 'issue_id': 2458639804, 'author': 'belitskiy', 'body': 'Thanks, @klucke\r\nThis should be in Tensorflow 2.18.0, and likely 2.17.1, and of course, nightlies\r\n\r\nClosing', 'created_at': datetime.datetime(2024, 8, 29, 14, 3, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317761033, 'issue_id': 2458639804, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73487"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73487"">No</a>', 'created_at': datetime.datetime(2024, 8, 29, 14, 3, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443329277, 'issue_id': 2458639804, 'author': 'LucaCappelletti94', 'body': 'For future TensorFlownauts trying to solve weird logs, I can confirm that the issue is solved as of version `2.18.0` for GPU published on the 25th of October 2024.', 'created_at': datetime.datetime(2024, 10, 29, 6, 36, 10, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-13 07:53:48 UTC): Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same.

I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.

https://github.com/tensorflow/tensorflow/issues/71791#issuecomment-2237115569
https://github.com/tensorflow/tensorflow/issues/70947

Thank you!

aa956 on (2024-08-18 20:41:33 UTC): Got the same problem, [partially resolved by this:](https://github.com/tensorflow/tensorflow/issues/26348#issuecomment-1206286540)

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*- 

# Suppress tensorflow spam, see https://github.com/tensorflow/tensorflow/issues/26348
# Not the complete fix, cuda_executor.cc still ignores all these settings
# and spams something successful NUMA node read
# but probably it's best that can be done without redirecting stderr

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import logging
logger = tf.get_logger()
logger.setLevel(logging.ERROR) # or logging.INFO, logging.WARNING, etc.

# Actual app starts here:

import argparse
import pathlib
import pprint
from enum import Enum
from tqdm import tqdm
...

```

So now instead of 1343 lines of `Skipping the delay` there's just 10 spam lines from the one line in cuda_executor.cc.
Still very spammy but better than 1500 repeats of the warning.

```console
similarity ~/tmp/images/2024-08-18/
  0%|                                                                                                                                 | 0/280 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1724013025.313366 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.339050 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.339224 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.340675 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.340776 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.340832 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.391595 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.391736 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724013025.391807 3889312 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
100%|| 280/280 [01:24<00:00,  3.32it/s]
```

forthmedia on (2024-08-19 15:41:34 UTC): Actually, the above suggestion is not a fix. Setting TF_CPP_MIN_LOG_LEVEL = 3 logs only error and fatal messages. Similar for setLevel(logging.ERROR) only error is logged. So, no change.

In my case the 'Skipping the delay kernel' warning occurs numerous times during both training and subsequent build.

We await a response.

fanny-j on (2024-08-27 15:32:50 UTC): I am also affected by this issue and hope to get a way to clean my log from hundreds if not thousands of these messages 
I would like to keep other warnings alive in case I am doing something wrong or inefficient somewhere else.

klucke (Assginee) on (2024-08-29 13:52:48 UTC): This is fixed by https://github.com/openxla/xla/pull/16566

belitskiy on (2024-08-29 14:03:04 UTC): Thanks, @klucke
This should be in Tensorflow 2.18.0, and likely 2.17.1, and of course, nightlies

Closing

google-ml-butler[bot] on (2024-08-29 14:03:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73487"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73487"">No</a>

LucaCappelletti94 on (2024-10-29 06:36:10 UTC): For future TensorFlownauts trying to solve weird logs, I can confirm that the issue is solved as of version `2.18.0` for GPU published on the 25th of October 2024.

"
2458608504,issue,closed,completed,Issue with Loading libdelegate.so on Ubuntu 22.04 LTS Using TensorFlow Lite GPU Delegate,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4

### Custom code

Yes

### OS platform and distribution

WSL Linux Ubuntu-22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

Bazelisk (automatically manages Bazel version)

### GCC/compiler version

10.5.0

### CUDA/cuDNN version

11.2.r11.2

### GPU model and memory

NVidia 4060Ti

### Current behavior?

(base) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/x64$ LD_DEBUG=libs ./test


... skipped lines with no errors or warnings ...

     27014:
     27014:     calling init: /usr/lib/x86_64-linux-gnu/gconv/UTF-16.so
     27014:
     27014:     /lib/x86_64-linux-gnu/libglib-2.0.so.0: error: symbol lookup error: undefined symbol: g_object_ref_sink (fatal)
     27014:     find library=libgdk-x11-2.0.so [0]; searching
     27014:      search path=/usr/local/cuda-11.2/lib64:glibc-hwcaps/x86-64-v3:glibc-hwcaps/x86-64-v2:tls/x86_64/x86_64:tls/x86_64:tls/x86_64:tls:x86_64/x86_64:x86_64:x86_64:          (LD_LIBRARY_PATH)
     27014:       trying file=/usr/local/cuda-11.2/lib64/libgdk-x11-2.0.so
     27014:       trying file=glibc-hwcaps/x86-64-v3/libgdk-x11-2.0.so
     27014:       trying file=glibc-hwcaps/x86-64-v2/libgdk-x11-2.0.so
     27014:       trying file=tls/x86_64/x86_64/libgdk-x11-2.0.so
     27014:       trying file=tls/x86_64/libgdk-x11-2.0.so
     27014:       trying file=tls/x86_64/libgdk-x11-2.0.so
     27014:       trying file=tls/libgdk-x11-2.0.so
     27014:       trying file=x86_64/x86_64/libgdk-x11-2.0.so
     27014:       trying file=x86_64/libgdk-x11-2.0.so
     27014:       trying file=x86_64/libgdk-x11-2.0.so
     27014:       trying file=libgdk-x11-2.0.so
     27014:      search cache=/etc/ld.so.cache
     27014:       trying file=/lib/x86_64-linux-gnu/libgdk-x11-2.0.so
     27014:
     27014:     ./test: error: symbol lookup error: undefined symbol: gtk_widget_device_is_shadowed (fatal)
     27014:     find library=libstdc++.so.6 [0]; searching
     27014:      search path=/usr/local/cuda-11.2/lib64:glibc-hwcaps/x86-64-v3:glibc-hwcaps/x86-64-v2:tls/x86_64/x86_64:tls/x86_64:tls/x86_64:tls:x86_64/x86_64:x86_64:x86_64:          (LD_LIBRARY_PATH)
     27014:       trying file=/usr/local/cuda-11.2/lib64/libstdc++.so.6
     27014:       trying file=glibc-hwcaps/x86-64-v3/libstdc++.so.6
     27014:       trying file=glibc-hwcaps/x86-64-v2/libstdc++.so.6
     27014:       trying file=tls/x86_64/x86_64/libstdc++.so.6
     27014:       trying file=tls/x86_64/libstdc++.so.6
     27014:       trying file=tls/x86_64/libstdc++.so.6
     27014:       trying file=tls/libstdc++.so.6
     27014:       trying file=x86_64/x86_64/libstdc++.so.6
     27014:       trying file=x86_64/libstdc++.so.6
     27014:       trying file=x86_64/libstdc++.so.6
     27014:       trying file=libstdc++.so.6
     27014:      search cache=/etc/ld.so.cache
     27014:       trying file=/lib/x86_64-linux-gnu/libstdc++.so.6
     27014:
     27014:     find library=libgcc_s.so.1 [0]; searching
     27014:      search path=/usr/local/cuda-11.2/lib64:glibc-hwcaps/x86-64-v3:glibc-hwcaps/x86-64-v2:tls/x86_64/x86_64:tls/x86_64:tls/x86_64:tls:x86_64/x86_64:x86_64:x86_64:          (LD_LIBRARY_PATH)
     27014:       trying file=/usr/local/cuda-11.2/lib64/libgcc_s.so.1
     27014:       trying file=glibc-hwcaps/x86-64-v3/libgcc_s.so.1
     27014:       trying file=glibc-hwcaps/x86-64-v2/libgcc_s.so.1
     27014:       trying file=tls/x86_64/x86_64/libgcc_s.so.1
     27014:       trying file=tls/x86_64/libgcc_s.so.1
     27014:       trying file=tls/x86_64/libgcc_s.so.1
     27014:       trying file=tls/libgcc_s.so.1
     27014:       trying file=x86_64/x86_64/libgcc_s.so.1
     27014:       trying file=x86_64/libgcc_s.so.1
     27014:       trying file=x86_64/libgcc_s.so.1
     27014:       trying file=libgcc_s.so.1
     27014:      search cache=/etc/ld.so.cache
     27014:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1
     27014:
     27014:     ./bin/tflite/libdelegate.so: error: symbol lookup error: undefined symbol: _ZN4absl14lts_2020_09_236Status16kMovedFromStringE (fatal)
     27014:

... skipped lines with no errors or warnings ...

### Standalone code to reproduce the issue

```shell
I expected the program to run without issues, successfully loading the libdelegate.so library and executing the intended functions without any runtime errors. It's important to note that the `libtensorflowlite_c.so` library, which was built in the same environment, loads without any issues or errors.
```


### Relevant log output

```shell
Additionally, the libdelegate.so library build process completes successfully with the following key outputs:

WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/master/miniconda3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=/home/master/miniconda3/envs/tf/lib/python3.9/site-packages --python_path=/home/master/miniconda3/envs/tf/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.9 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib64: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-10 --config=cuda
INFO: Found applicable config definition build:short_logs in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:monolithic in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:cuda in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/master/.cache/bazel/_bazel_master/8a68c52b18b2f377d8f5efcf04fab9da/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
Loading:  (0 packages loaded)
Loading: 0 packages loaded
Analyzing: target //tensorflow/lite/delegates/gpu:delegate (0 packages loaded, 0 targets configured)
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/WORKSPACE:23:14: in <toplevel>
  /mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-22.04/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace
  /home/master/.cache/bazel/_bazel_master/8a68c52b18b2f377d8f5efcf04fab9da/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/master/.cache/bazel/_bazel_master/8a68c52b18b2f377d8f5efcf04fab9da/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/lite/delegates/gpu:delegate (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
[0 / 4] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[9 / 344] Compiling tensorflow/lite/delegates/gpu/gl/api2.cc; 1s local ... (8 actions, 7 running)
[12 / 344] Compiling tensorflow/lite/delegates/gpu/gl/api2.cc; 2s local ... (8 actions running)
[15 / 351] Compiling flatbuffers/src/idl_gen_cpp.cpp [for host]; 3s local ... (8 actions running)
[20 / 353] Compiling flatbuffers/src/idl_gen_python.cpp [for host]; 2s local ... (8 actions running)
[59 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 1s local ... (8 actions, 7 running)
[67 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 2s local ... (8 actions, 7 running)
[75 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 4s local ... (8 actions, 7 running)
[83 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 5s local ... (8 actions, 7 running)
[104 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 7s local ... (8 actions, 7 running)
[136 / 388] Compiling flatbuffers/src/idl_parser.cpp [for host]; 8s local ... (8 actions, 7 running)
[156 / 388] Compiling flatbuffers/src/idl_parser.cpp; 8s local ... (8 actions, 7 running)
[167 / 388] Compiling tensorflow/lite/delegates/gpu/cl/kernels/converter.cc; 2s local ... (8 actions, 7 running)
[178 / 388] Compiling tensorflow/lite/delegates/gpu/common/model_builder.cc; 5s local ... (8 actions running)
[193 / 388] Compiling tensorflow/lite/delegates/gpu/common/tasks/depthwise_conv_3x3_stride_h2.cc; 1s local ... (8 actions running)
[214 / 388] Compiling tensorflow/lite/delegates/gpu/common/tasks/convolution_transposed.cc; 3s local ... (8 actions, 7 running)
[253 / 389] Compiling tensorflow/lite/delegates/gpu/gl/gl_program.cc; 0s local ... (8 actions, 7 running)
[285 / 389] Compiling tensorflow/lite/delegates/gpu/gl/kernels/depthwise_conv.cc; 1s local ... (8 actions, 7 running)
[316 / 391] Compiling tensorflow/lite/delegates/gpu/common/tasks/conv_powervr.cc; 4s local ... (8 actions, 7 running)
[365 / 391] Compiling tensorflow/lite/delegates/gpu/cl/cl_arguments.cc; 3s local ... (8 actions, 7 running)
Target //tensorflow/lite/delegates/gpu:delegate up-to-date:
  bazel-bin/tensorflow/lite/delegates/gpu/libdelegate.a
  bazel-bin/tensorflow/lite/delegates/gpu/libdelegate.pic.a
  bazel-bin/tensorflow/lite/delegates/gpu/libdelegate.so
INFO: Elapsed time: 54.796s, Critical Path: 17.86s
INFO: 354 processes: 1 internal, 353 local.
INFO: Build completed successfully, 354 total actions
INFO: Build completed successfully, 354 total actions
```
",pisarev,2024-08-09 20:17:19+00:00,"['pkgoogle', 'sawantkumar']",2024-09-18 01:59:04+00:00,2024-09-18 01:58:54+00:00,https://github.com/tensorflow/tensorflow/issues/73484,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.4', 'for issues related to TF 2.4')]","[{'comment_id': 2283408509, 'issue_id': 2458608504, 'author': 'Venkat6871', 'body': 'Hi **@pisarev** ,\r\nSorry for the delay. There seem to be version compatibility issues here. Could you please check this [documentation](https://www.tensorflow.org/install/source_windows#gpu)? Please update the documentation accordingly and let us know if the issue still persists.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 12, 8, 43, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2286667939, 'issue_id': 2458608504, 'author': 'pisarev', 'body': 'Thank you for your response!\r\n\r\nI have checked the versions of all relevant components and it seems everything is selected correctly:\r\n\r\n- **Ubuntu Version**: 22.04.4 LTS (Jammy)\r\n- **Bazel Version**: 3.7.2 (installed via Bazelisk)\r\n- **CUDA Version**: 11.2 (V11.2.152)\r\n- **cuDNN Version**: 8.1.1\r\n- **Python Version**: 3.9.19 (Conda environment)\r\n- **GCC Version**: Default system GCC is used.\r\n\r\nConfiguration details:\r\n- CUDA compute capability: 8.9\r\n- No ROCm, TensorRT, or Clang as CUDA compiler.\r\n\r\nThe ""libdelegate.so"" library, when loaded by my application, leads to a symbol lookup error, while the ""libtensorflowlite_c.so"" library loads without any issues.\r\n\r\nHere are the specific commands I ran in the terminal along with their outputs:\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 22.04.4 LTS\r\nRelease:        22.04\r\nCodename:       jammy\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ bazel --version\r\nbazel 3.7.2\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 8\r\n#define CUDNN_MINOR 1\r\n#define CUDNN_PATCHLEVEL 1\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n#endif /* CUDNN_VERSION_H */\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ conda activate tf\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ which python\r\n/home/master/miniconda3/envs/tf/bin/python\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ python --version\r\nPython 3.9.19\r\n\r\nConfiguration steps:\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /bin/python3]: /home/master/miniconda3/envs/tf/bin/python\r\n\r\nFound possible Python library paths:\r\n  /home/master/miniconda3/envs/tf/lib/python3.9/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/master/miniconda3/envs/tf/lib/python3.9/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]: 8.9\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]:\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nDespite these configurations, the ""libdelegate.so"" library still encounters the following errors when loaded by the application:\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/x64$ LD_DEBUG=libs ./eye\r\n...\r\n      6411:     /lib/x86_64-linux-gnu/libglib-2.0.so.0: error: symbol lookup error: undefined symbol: g_object_ref_sink (fatal)\r\n...\r\n      6411:     ./eye: error: symbol lookup error: undefined symbol: gtk_widget_device_is_shadowed (fatal)\r\n...\r\n      6411:     ./bin/tflite/libdelegate.so: error: symbol lookup error: undefined symbol: _ZN4absl14lts_2020_09_236Status16kMovedFromStringE (fatal)\r\n\r\nWhen checking the shared library dependencies of libdelegate.so, the following output was observed:\r\n\r\n(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ ldd ../x64/bin/tflite/libdelegate.so\r\n        linux-vdso.so.1 (0x00007ffd835b6000)\r\n        libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff59dcf6000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff59dcd6000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff59daad000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff59d9c6000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007ff59df38000)', 'created_at': datetime.datetime(2024, 8, 13, 16, 35, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327376385, 'issue_id': 2458608504, 'author': 'pkgoogle', 'body': 'Hi @pisarev, what command did you use to build `libdelegate.so`? Also what command(s)/program(s) did you use to check it (`./eye` ??) can you please explain what that is? Please also format code/terminal with 3 backtick code formatting:\r\n\r\nex:\r\n\\`\\`\\`\r\n\\# code/terminal output\r\n\\`\\`\\`\r\n\r\noutputs:\r\n```\r\n# code/terminal output\r\n```\r\n\r\nThanks.', 'created_at': datetime.datetime(2024, 9, 3, 20, 27, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342463166, 'issue_id': 2458608504, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 11, 1, 58, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336643, 'issue_id': 2458608504, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336836, 'issue_id': 2458608504, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73484"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73484"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 59, 2, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-08-12 08:43:09 UTC): Hi **@pisarev** ,
Sorry for the delay. There seem to be version compatibility issues here. Could you please check this [documentation](https://www.tensorflow.org/install/source_windows#gpu)? Please update the documentation accordingly and let us know if the issue still persists.
Thank you!

pisarev (Issue Creator) on (2024-08-13 16:35:47 UTC): Thank you for your response!

I have checked the versions of all relevant components and it seems everything is selected correctly:

- **Ubuntu Version**: 22.04.4 LTS (Jammy)
- **Bazel Version**: 3.7.2 (installed via Bazelisk)
- **CUDA Version**: 11.2 (V11.2.152)
- **cuDNN Version**: 8.1.1
- **Python Version**: 3.9.19 (Conda environment)
- **GCC Version**: Default system GCC is used.

Configuration details:
- CUDA compute capability: 8.9
- No ROCm, TensorRT, or Clang as CUDA compiler.

The ""libdelegate.so"" library, when loaded by my application, leads to a symbol lookup error, while the ""libtensorflowlite_c.so"" library loads without any issues.

Here are the specific commands I ran in the terminal along with their outputs:

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.4 LTS
Release:        22.04
Codename:       jammy

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ bazel --version
bazel 3.7.2

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 8
#define CUDNN_MINOR 1
#define CUDNN_PATCHLEVEL 1
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
#endif /* CUDNN_VERSION_H */

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ conda activate tf
(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ which python
/home/master/miniconda3/envs/tf/bin/python

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin$ python --version
Python 3.9.19

Configuration steps:

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /bin/python3]: /home/master/miniconda3/envs/tf/bin/python

Found possible Python library paths:
  /home/master/miniconda3/envs/tf/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/home/master/miniconda3/envs/tf/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]: 8.9

Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]:

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Despite these configurations, the ""libdelegate.so"" library still encounters the following errors when loaded by the application:

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/x64$ LD_DEBUG=libs ./eye
...
      6411:     /lib/x86_64-linux-gnu/libglib-2.0.so.0: error: symbol lookup error: undefined symbol: g_object_ref_sink (fatal)
...
      6411:     ./eye: error: symbol lookup error: undefined symbol: gtk_widget_device_is_shadowed (fatal)
...
      6411:     ./bin/tflite/libdelegate.so: error: symbol lookup error: undefined symbol: _ZN4absl14lts_2020_09_236Status16kMovedFromStringE (fatal)

When checking the shared library dependencies of libdelegate.so, the following output was observed:

(tf) master@WIN-1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu-24.04/tensorflow$ ldd ../x64/bin/tflite/libdelegate.so
        linux-vdso.so.1 (0x00007ffd835b6000)
        libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff59dcf6000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff59dcd6000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff59daad000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff59d9c6000)
        /lib64/ld-linux-x86-64.so.2 (0x00007ff59df38000)

pkgoogle (Assginee) on (2024-09-03 20:27:10 UTC): Hi @pisarev, what command did you use to build `libdelegate.so`? Also what command(s)/program(s) did you use to check it (`./eye` ??) can you please explain what that is? Please also format code/terminal with 3 backtick code formatting:

ex:
\`\`\`
\# code/terminal output
\`\`\`

outputs:
```
# code/terminal output
```

Thanks.

github-actions[bot] on (2024-09-11 01:58:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:59:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73484"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73484"">No</a>

"
2458042827,issue,closed,completed,Aborted (core dumped) in `tf.io.TFRecordOptions`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Triggers crash when the compression_strategy parameter of tf.io.TFRecordOptions is negative.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1nIfAqRUryYMSdJfZTnIs2jAUW8nUON5s?usp=sharing
```


### Relevant log output

```shell
2024-08-09 22:20:07.358412: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-09 22:20:07.627813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-09 22:20:10.998865: F external/local_tsl/tsl/lib/io/record_writer.cc:74] Failed to initialize Zlib inputbuffer. Error: INVALID_ARGUMENT: deflateInit failed with status-2
Aborted (core dumped)
```
",x0w3n,2024-08-09 14:22:36+00:00,['tilakrayal'],2024-08-31 01:56:49+00:00,2024-08-31 01:56:45+00:00,https://github.com/tensorflow/tensorflow/issues/73465,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2293896908, 'issue_id': 2458042827, 'author': 'tilakrayal', 'body': '@x0w3n,\r\nLooks like there is an open issue that was raised for the similar issue and it was already in the open stage. Could you please check and follow the similar issue for the updates.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/63337\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 16, 17, 34, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987402, 'issue_id': 2458042827, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322725788, 'issue_id': 2458042827, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 31, 1, 56, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322725825, 'issue_id': 2458042827, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73465"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73465"">No</a>', 'created_at': datetime.datetime(2024, 8, 31, 1, 56, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-16 17:34:53 UTC): @x0w3n,
Looks like there is an open issue that was raised for the similar issue and it was already in the open stage. Could you please check and follow the similar issue for the updates.

https://github.com/tensorflow/tensorflow/issues/63337

Thank you!

github-actions[bot] on (2024-08-24 01:53:16 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-31 01:56:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-31 01:56:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73465"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73465"">No</a>

"
2457654212,issue,open,,Memory leak in training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda/12.0.0_gcc-10.4.0 and cudnn/8.9.7.29-12_gcc-10.4.0

### GPU model and memory

different GPU, among which Tesla V100-SXM2-32GB

### Current behavior?

I have a memory leak (GPU memory and RAM constantly increase) during my training. 

**This does not happen with Tensorflow 2.11.1**. 

[Here is the project](https://github.com/deep-finder/tirfm-deepfinder).
Unfortunately, this is not my code and I do not have time to create a minimal standalone code.

### Standalone code to reproduce the issue

```shell
def printMemoryUsage(self):
        gpus = tf.config.list_physical_devices('GPU')             
        for gpu in gpus:
            gpuNameRoot = gpu.name.split(':')[0] + ':'
            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
            try:
                import psutil
                virtual_memory = psutil.virtual_memory()
                print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
            except:
                pass

[...]
        # Training loop:
        for e in range(self.epochs):
            # TRAINING:
            start = time.time()
            list_loss_train = []
            list_acc_train = []
            for it in range(self.steps_per_epoch):
                if self.flag_direct_read:
                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)
                else:
                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)

                if self.sample_weights is not None:
                    sample_weight = self.sample_weights[idx_list]
                else:
                    sample_weight = None

                loss_train = self.net.train_on_batch(batch_data, batch_target,
                                                     class_weight=self.class_weight,
                                                     sample_weight=sample_weight)

                self.display('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))

                self.printMemoryUsage()

                list_loss_train.append(loss_train[0])
                list_acc_train.append(loss_train[1])
                del batch_data
                del batch_target
                if idx_list is not None:
                    del idx_list
                gc.collect()
```


### Relevant log output

```shell
With Tensorflow 2.11.1:


=============================================================
epoch 3/50 - it 1/100 - loss: 2.012 - acc: 0.976
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 2/100 - loss: 2.008 - acc: 0.985
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 3/100 - loss: 2.004 - acc: 0.992
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 4/100 - loss: 2.006 - acc: 0.987
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 5/100 - loss: 2.006 - acc: 0.989
```


With Tensorflow 2.16.1 and 2.17:

```
epoch 1/50 - it 6/100 - loss: 2.473 - acc: 0.305
Memory info of GPU /physical_device:GPU:0: current: 0.18, peak: 2.55
Memory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%
epoch 1/50 - it 7/100 - loss: 2.470 - acc: 0.394
Memory info of GPU /physical_device:GPU:0: current: 0.21, peak: 2.58
Memory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%
epoch 1/50 - it 8/100 - loss: 2.466 - acc: 0.463
Memory info of GPU /physical_device:GPU:0: current: 0.24, peak: 2.61
Memory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%
epoch 1/50 - it 9/100 - loss: 2.461 - acc: 0.519
Memory info of GPU /physical_device:GPU:0: current: 0.27, peak: 2.64
Memory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%
epoch 1/50 - it 10/100 - loss: 2.455 - acc: 0.564
Memory info of GPU /physical_device:GPU:0: current: 0.29, peak: 2.67
Memory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%
epoch 1/50 - it 11/100 - loss: 2.448 - acc: 0.603
Memory info of GPU /physical_device:GPU:0: current: 0.32, peak: 2.70
Memory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%
epoch 1/50 - it 12/100 - loss: 2.437 - acc: 0.634
Memory info of GPU /physical_device:GPU:0: current: 0.35, peak: 2.72
Memory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%
epoch 1/50 - it 13/100 - loss: 2.425 - acc: 0.662
Memory info of GPU /physical_device:GPU:0: current: 0.38, peak: 2.75
Memory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%
epoch 1/50 - it 14/100 - loss: 2.408 - acc: 0.685
Memory info of GPU /physical_device:GPU:0: current: 0.41, peak: 2.78
Memory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%
epoch 1/50 - it 15/100 - loss: 2.389 - acc: 0.705
Memory info of GPU /physical_device:GPU:0: current: 0.44, peak: 2.81
Memory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%
epoch 1/50 - it 16/100 - loss: 2.369 - acc: 0.723
Memory info of GPU /physical_device:GPU:0: current: 0.46, peak: 2.84
Memory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%
epoch 1/50 - it 17/100 - loss: 2.350 - acc: 0.739
Memory info of GPU /physical_device:GPU:0: current: 0.49, peak: 2.87
Memory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%
epoch 1/50 - it 18/100 - loss: 2.332 - acc: 0.753
Memory info of GPU /physical_device:GPU:0: current: 0.52, peak: 2.89
Memory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%
epoch 1/50 - it 19/100 - loss: 2.315 - acc: 0.765
Memory info of GPU /physical_device:GPU:0: current: 0.55, peak: 2.92
Memory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%
epoch 1/50 - it 20/100 - loss: 2.300 - acc: 0.776
Memory info of GPU /physical_device:GPU:0: current: 0.58, peak: 2.95
Memory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%
epoch 1/50 - it 21/100 - loss: 2.286 - acc: 0.786
Memory info of GPU /physical_device:GPU:0: current: 0.61, peak: 2.98
Memory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%
epoch 1/50 - it 22/100 - loss: 2.274 - acc: 0.795
Memory info of GPU /physical_device:GPU:0: current: 0.63, peak: 3.01
Memory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%
epoch 1/50 - it 23/100 - loss: 2.262 - acc: 0.804
Memory info of GPU /physical_device:GPU:0: current: 0.66, peak: 3.04
Memory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%
epoch 1/50 - it 24/100 - loss: 2.251 - acc: 0.811
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 3.06
Memory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%
epoch 1/50 - it 25/100 - loss: 2.241 - acc: 0.818
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 3.09
Memory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%
epoch 1/50 - it 26/100 - loss: 2.232 - acc: 0.825
Memory info of GPU /physical_device:GPU:0: current: 0.75, peak: 3.12
Memory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%
epoch 1/50 - it 27/100 - loss: 2.224 - acc: 0.831
Memory info of GPU /physical_device:GPU:0: current: 0.78, peak: 3.15
Memory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%
epoch 1/50 - it 28/100 - loss: 2.216 - acc: 0.836
Memory info of GPU /physical_device:GPU:0: current: 0.80, peak: 3.18
Memory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%
epoch 1/50 - it 29/100 - loss: 2.209 - acc: 0.841
Memory info of GPU /physical_device:GPU:0: current: 0.83, peak: 3.21
Memory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%
epoch 1/50 - it 30/100 - loss: 2.202 - acc: 0.846
Memory info of GPU /physical_device:GPU:0: current: 0.86, peak: 3.23
Memory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%
epoch 1/50 - it 31/100 - loss: 2.196 - acc: 0.851
Memory info of GPU /physical_device:GPU:0: current: 0.89, peak: 3.26
Memory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%
epoch 1/50 - it 32/100 - loss: 2.190 - acc: 0.855
Memory info of GPU /physical_device:GPU:0: current: 0.92, peak: 3.29
Memory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%
epoch 1/50 - it 33/100 - loss: 2.185 - acc: 0.859
Memory info of GPU /physical_device:GPU:0: current: 0.95, peak: 3.32
Memory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%
epoch 1/50 - it 34/100 - loss: 2.180 - acc: 0.863
Memory info of GPU /physical_device:GPU:0: current: 0.97, peak: 3.35
Memory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%
epoch 1/50 - it 35/100 - loss: 2.174 - acc: 0.866
Memory info of GPU /physical_device:GPU:0: current: 1.00, peak: 3.38
Memory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%
epoch 1/50 - it 36/100 - loss: 2.170 - acc: 0.870
Memory info of GPU /physical_device:GPU:0: current: 1.03, peak: 3.40
Memory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%
epoch 1/50 - it 37/100 - loss: 2.165 - acc: 0.873
Memory info of GPU /physical_device:GPU:0: current: 1.06, peak: 3.43
Memory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%
epoch 1/50 - it 38/100 - loss: 2.161 - acc: 0.876
Memory info of GPU /physical_device:GPU:0: current: 1.09, peak: 3.46
Memory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%
epoch 1/50 - it 39/100 - loss: 2.157 - acc: 0.879
Memory info of GPU /physical_device:GPU:0: current: 1.11, peak: 3.49
Memory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%
epoch 1/50 - it 40/100 - loss: 2.153 - acc: 0.881
Memory info of GPU /physical_device:GPU:0: current: 1.14, peak: 3.52
Memory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%
epoch 1/50 - it 41/100 - loss: 2.150 - acc: 0.884
Memory info of GPU /physical_device:GPU:0: current: 1.17, peak: 3.54
Memory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%
```
```
",arthursw,2024-08-09 10:51:49+00:00,"['reedwm', 'Venkat6871']",2025-01-02 07:02:02+00:00,,https://github.com/tensorflow/tensorflow/issues/73457,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2283447329, 'issue_id': 2457654212, 'author': 'Venkat6871', 'body': 'Hi **@arthursw** ,\r\nApologies for the delay. Could you please provide a Colab gist to help troubleshoot the above issue? The provided code is quite extensive, making it challenging to pinpoint the source of the memory leaks.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 12, 9, 3, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284477047, 'issue_id': 2457654212, 'author': 'arthursw', 'body': 'Hi, thanks for your answer.\r\n\r\nHere is a minimal reproducible example:\r\n\r\n```\r\nimport numpy as np\r\nfrom keras.layers import Input, concatenate\r\nfrom keras.models import Model\r\nfrom keras.layers import Conv3D, MaxPooling3D, UpSampling3D\r\n\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.utils import to_categorical\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nimport psutil\r\n\r\ndef tversky_loss(y_true, y_pred):\r\n    alpha = 0.5\r\n    beta = 0.5\r\n\r\n    ones = tf.ones(tf.shape(y_true))\r\n    p0 = y_pred  # proba that voxels are class i\r\n    p1 = ones - y_pred  # proba that voxels are not class i\r\n    g0 = y_true\r\n    g1 = ones - y_true\r\n\r\n    num = K.sum(p0 * g0, (0, 1, 2, 3))\r\n    den = num + alpha * K.sum(p0 * g1, (0, 1, 2, 3)) + beta * K.sum(p1 * g0, (0, 1, 2, 3))\r\n\r\n    T = K.sum(num / den)  # when summing over classes, T has dynamic range [0 Ncl]\r\n\r\n    Ncl = tf.cast(tf.shape(y_true)[-1], \'float32\')\r\n    return Ncl - T\r\n\r\ndef my_model(dim_in, Ncl):\r\n    input = Input(shape=(dim_in, dim_in, dim_in, 1))\r\n\r\n    x = Conv3D(32, (3, 3, 3), padding=\'same\', activation=\'relu\')(input)\r\n    high = Conv3D(32, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = MaxPooling3D((1, 2, 2), strides=None)(high)\r\n\r\n    x = Conv3D(48, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    mid = Conv3D(48, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = MaxPooling3D((1, 2, 2), strides=None)(mid)\r\n\r\n    x = Conv3D(64, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    x = Conv3D(64, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    x = Conv3D(64, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    x = Conv3D(64, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = UpSampling3D(size=(1, 2, 2), data_format=\'channels_last\')(x)\r\n    x = Conv3D(64, (2, 2, 2), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = concatenate([x, mid])\r\n    x = Conv3D(48, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    x = Conv3D(48, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = UpSampling3D(size=(1, 2, 2), data_format=\'channels_last\')(x)\r\n    x = Conv3D(48, (2, 2, 2), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    x = concatenate([x, high])\r\n    x = Conv3D(32, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n    x = Conv3D(32, (3, 3, 3), padding=\'same\', activation=\'relu\')(x)\r\n\r\n    output = Conv3D(Ncl, (1, 1, 1), padding=\'same\', activation=\'softmax\')(x)\r\n\r\n    model = Model(input, output)\r\n    return model\r\n\r\n\r\n# Training parameters:\r\nbatch_size = 4\r\npatch_size = 48\r\nepochs = 100\r\nsteps_per_epoch = 100\r\noptimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)\r\nnet = my_model(patch_size, 3)\r\nnet.compile(optimizer=optimizer, loss=tversky_loss, metrics=[\'accuracy\'])\r\n\r\n# data_list, target_list = cm.load_dataset(path_data, path_target, h5_dset_name)\r\n\r\ndef printMemoryUsage():\r\n    gpus = tf.config.list_physical_devices(\'GPU\')             \r\n    for gpu in gpus:\r\n        gpuNameRoot = gpu.name.split(\':\')[0] + \':\'\r\n        memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, \'\'))\r\n        print(f\'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}\')\r\n        virtual_memory = psutil.virtual_memory()\r\n        print(f\'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%\')\r\n        \r\nprint(\'train...\')\r\n# Training loop:\r\nfor e in range(epochs):\r\n\r\n    for it in range(steps_per_epoch):\r\n\r\n        batch_data = np.random.rand(batch_size, patch_size, patch_size, patch_size, 1)\r\n        batch_target = np.random.rand(batch_size, patch_size, patch_size, patch_size, 1)\r\n        loss_train = net.train_on_batch(batch_data, batch_target)\r\n        printMemoryUsage()\r\n        print(\'epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f\' % (e + 1, epochs, it + 1, steps_per_epoch, loss_train[0], loss_train[1]))\r\n\r\n```\r\n\r\nWith tensorflow 2.11.1, no memory leak:\r\n```\r\n(/home/amasson/storage/conda-envs/tensorflow) amasson@abacus25-3:~/tensorflow_test$ python main.py \r\n2024-08-12 18:25:35.639588: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-12 18:25:37.007876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libnvinfer.so.7\'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cudnn-8.9.7.29-12-nxxaznjmoeoswhybcfmdhcohjvgwryky/lib:/grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cuda-12.0.0-g2atnxvq3akekpc6otev56c2rqzmnr7y/lib64\r\n2024-08-12 18:25:37.007999: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libnvinfer_plugin.so.7\'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cudnn-8.9.7.29-12-nxxaznjmoeoswhybcfmdhcohjvgwryky/lib:/grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cuda-12.0.0-g2atnxvq3akekpc6otev56c2rqzmnr7y/lib64\r\n2024-08-12 18:25:37.008013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2024-08-12 18:25:38.881767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-12 18:25:40.034512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43435 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:a3:00.0, compute capability: 8.6\r\ntrain...\r\n2024-08-12 18:25:43.817654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\r\n2024-08-12 18:25:47.825264: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f227f5880f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2024-08-12 18:25:47.825365: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\r\n2024-08-12 18:25:47.854377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-08-12 18:25:48.122897: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.28Gb, percent: 6.2%\r\nepoch 1/100 - it 1/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.85Gb, percent: 6.1%\r\nepoch 1/100 - it 2/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.83Gb, percent: 6.1%\r\nepoch 1/100 - it 3/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.68Gb, percent: 6.1%\r\nepoch 1/100 - it 4/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.49Gb, percent: 6.2%\r\nepoch 1/100 - it 5/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%\r\nepoch 1/100 - it 6/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.35Gb, percent: 6.2%\r\nepoch 1/100 - it 7/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%\r\nepoch 1/100 - it 8/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.36Gb, percent: 6.2%\r\nepoch 1/100 - it 9/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%\r\nepoch 1/100 - it 10/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%\r\nepoch 1/100 - it 11/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%\r\nepoch 1/100 - it 12/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%\r\nepoch 1/100 - it 13/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%\r\nepoch 1/100 - it 14/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.07Gb, percent: 6.2%\r\nepoch 1/100 - it 15/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.27Gb, percent: 6.2%\r\nepoch 1/100 - it 16/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%\r\nepoch 1/100 - it 17/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.69Gb, percent: 6.1%\r\nepoch 1/100 - it 18/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.61Gb, percent: 6.1%\r\nepoch 1/100 - it 19/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.66Gb, percent: 6.1%\r\nepoch 1/100 - it 20/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.72Gb, percent: 6.1%\r\nepoch 1/100 - it 21/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.49Gb, percent: 6.2%\r\nepoch 1/100 - it 22/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.41Gb, percent: 6.2%\r\nepoch 1/100 - it 23/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%\r\nepoch 1/100 - it 24/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%\r\nepoch 1/100 - it 25/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.57Gb, percent: 6.1%\r\nepoch 1/100 - it 26/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.36Gb, percent: 6.2%\r\nepoch 1/100 - it 27/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%\r\nepoch 1/100 - it 28/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.11Gb, percent: 6.2%\r\nepoch 1/100 - it 29/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.06Gb, percent: 6.2%\r\nepoch 1/100 - it 30/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 506.97Gb, percent: 6.3%\r\nepoch 1/100 - it 31/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 506.98Gb, percent: 6.3%\r\nepoch 1/100 - it 32/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.28Gb, percent: 6.2%\r\nepoch 1/100 - it 33/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%\r\nepoch 1/100 - it 34/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.42Gb, percent: 6.2%\r\nepoch 1/100 - it 35/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.56Gb, percent: 6.2%\r\nepoch 1/100 - it 36/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.66Gb, percent: 6.1%\r\nepoch 1/100 - it 37/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.59Gb, percent: 6.1%\r\nepoch 1/100 - it 38/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.67Gb, percent: 6.1%\r\nepoch 1/100 - it 39/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.40Gb, percent: 6.2%\r\nepoch 1/100 - it 40/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.63Gb, percent: 6.1%\r\nepoch 1/100 - it 41/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%\r\nepoch 1/100 - it 42/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%\r\nepoch 1/100 - it 43/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.41Gb, percent: 6.2%\r\nepoch 1/100 - it 44/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.27Gb, percent: 6.2%\r\nepoch 1/100 - it 45/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.22Gb, percent: 6.2%\r\nepoch 1/100 - it 46/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%\r\nepoch 1/100 - it 47/100 - loss: -0.201 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.00Gb, percent: 6.3%\r\nepoch 1/100 - it 48/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%\r\nepoch 1/100 - it 49/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.06Gb, percent: 6.2%\r\nepoch 1/100 - it 50/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%\r\nepoch 1/100 - it 51/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%\r\nepoch 1/100 - it 52/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.32Gb, percent: 6.2%\r\nepoch 1/100 - it 53/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.64Gb, percent: 6.1%\r\nepoch 1/100 - it 54/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.74Gb, percent: 6.1%\r\nepoch 1/100 - it 55/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.75Gb, percent: 6.1%\r\nepoch 1/100 - it 56/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.50Gb, percent: 6.2%\r\nepoch 1/100 - it 57/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%\r\nepoch 1/100 - it 58/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.19Gb, percent: 6.2%\r\nepoch 1/100 - it 59/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%\r\nepoch 1/100 - it 60/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%\r\nepoch 1/100 - it 61/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%\r\nepoch 1/100 - it 62/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.23Gb, percent: 6.2%\r\nepoch 1/100 - it 63/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%\r\nepoch 1/100 - it 64/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.21Gb, percent: 6.2%\r\nepoch 1/100 - it 65/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.11Gb, percent: 6.2%\r\nepoch 1/100 - it 66/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.29Gb, percent: 6.2%\r\nepoch 1/100 - it 67/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%\r\nepoch 1/100 - it 68/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.22Gb, percent: 6.2%\r\nepoch 1/100 - it 69/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%\r\nepoch 1/100 - it 70/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.53Gb, percent: 6.2%\r\nepoch 1/100 - it 71/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.91Gb, percent: 6.1%\r\nepoch 1/100 - it 72/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.84Gb, percent: 6.1%\r\nepoch 1/100 - it 73/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.73Gb, percent: 6.1%\r\nepoch 1/100 - it 74/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%\r\nepoch 1/100 - it 75/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%\r\nepoch 1/100 - it 76/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.73Gb, percent: 6.1%\r\nepoch 1/100 - it 77/100 - loss: -0.199 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.47Gb, percent: 6.2%\r\nepoch 1/100 - it 78/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.24Gb, percent: 6.2%\r\nepoch 1/100 - it 79/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52\r\nMemory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%\r\nepoch 1/100 - it 80/100 - loss: -0.200 - acc: 0.000\r\n...\r\n```\r\n\r\nWith tensorflow==2.16.1, memory leak?:\r\n\r\n```\r\n2024-08-12 18:38:50.624772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-12 18:38:52.170885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-08-12 18:38:53.769351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43607 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:a3:00.0, compute capability: 8.6\r\ntrain...\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1723480737.529350  213588 service.cc:145] XLA service 0x7f49cc001e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\nI0000 00:00:1723480737.529433  213588 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\r\n2024-08-12 18:38:57.663660: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-08-12 18:38:58.246372: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\r\nI0000 00:00:1723480748.391728  213588 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nMemory info of GPU /physical_device:GPU:0: current: 0.02, peak: 1.10\r\nMemory info of CPU: total:540.82Gb, available: 508.45Gb, percent: 6.0%\r\nepoch 1/100 - it 1/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.03, peak: 1.11\r\nMemory info of CPU: total:540.82Gb, available: 508.43Gb, percent: 6.0%\r\nepoch 1/100 - it 2/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.03, peak: 1.11\r\nMemory info of CPU: total:540.82Gb, available: 508.50Gb, percent: 6.0%\r\nepoch 1/100 - it 3/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.04, peak: 1.12\r\nMemory info of CPU: total:540.82Gb, available: 507.92Gb, percent: 6.1%\r\nepoch 1/100 - it 4/100 - loss: -0.200 - acc: 0.000\r\nWARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f4b3070b490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nMemory info of GPU /physical_device:GPU:0: current: 0.05, peak: 1.13\r\nMemory info of CPU: total:540.82Gb, available: 508.26Gb, percent: 6.0%\r\nepoch 1/100 - it 5/100 - loss: -0.200 - acc: 0.000\r\nWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f4b3070b490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nMemory info of GPU /physical_device:GPU:0: current: 0.05, peak: 1.13\r\nMemory info of CPU: total:540.82Gb, available: 508.18Gb, percent: 6.0%\r\nepoch 1/100 - it 6/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.06, peak: 1.14\r\nMemory info of CPU: total:540.82Gb, available: 507.63Gb, percent: 6.1%\r\nepoch 1/100 - it 7/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.07, peak: 1.15\r\nMemory info of CPU: total:540.82Gb, available: 508.16Gb, percent: 6.0%\r\nepoch 1/100 - it 8/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.07, peak: 1.15\r\nMemory info of CPU: total:540.82Gb, available: 507.96Gb, percent: 6.1%\r\nepoch 1/100 - it 9/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.08, peak: 1.16\r\nMemory info of CPU: total:540.82Gb, available: 508.10Gb, percent: 6.1%\r\nepoch 1/100 - it 10/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.09, peak: 1.17\r\nMemory info of CPU: total:540.82Gb, available: 507.55Gb, percent: 6.2%\r\nepoch 1/100 - it 11/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.10, peak: 1.18\r\nMemory info of CPU: total:540.82Gb, available: 507.60Gb, percent: 6.1%\r\nepoch 1/100 - it 12/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.10, peak: 1.18\r\nMemory info of CPU: total:540.82Gb, available: 507.37Gb, percent: 6.2%\r\nepoch 1/100 - it 13/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.11, peak: 1.19\r\nMemory info of CPU: total:540.82Gb, available: 508.07Gb, percent: 6.1%\r\nepoch 1/100 - it 14/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.12, peak: 1.20\r\nMemory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%\r\n[...]\r\nepoch 1/100 - it 95/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.69, peak: 1.77\r\nMemory info of CPU: total:540.82Gb, available: 502.91Gb, percent: 7.0%\r\nepoch 1/100 - it 96/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.70, peak: 1.78\r\nMemory info of CPU: total:540.82Gb, available: 501.89Gb, percent: 7.2%\r\nepoch 1/100 - it 97/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.70, peak: 1.78\r\nMemory info of CPU: total:540.82Gb, available: 502.15Gb, percent: 7.2%\r\nepoch 1/100 - it 98/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.71, peak: 1.79\r\nMemory info of CPU: total:540.82Gb, available: 502.48Gb, percent: 7.1%\r\nepoch 1/100 - it 99/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.72, peak: 1.80\r\nMemory info of CPU: total:540.82Gb, available: 502.41Gb, percent: 7.1%\r\nepoch 1/100 - it 100/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.73, peak: 1.81\r\nMemory info of CPU: total:540.82Gb, available: 501.75Gb, percent: 7.2%\r\nepoch 2/100 - it 1/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.73, peak: 1.81\r\nMemory info of CPU: total:540.82Gb, available: 501.69Gb, percent: 7.2%\r\nepoch 2/100 - it 2/100 - loss: -0.200 - acc: 0.000\r\nMemory info of GPU /physical_device:GPU:0: current: 0.74, peak: 1.82\r\nMemory info of CPU: total:540.82Gb, available: 502.45Gb, percent: 7.1%\r\nepoch 2/100 - it 3/100 - loss: -0.200 - acc: 0.000\r\n```\r\n\r\nThis with `cuda/12.0.0_gcc-10.4.0` and `cudnn/8.9.7.29-12_gcc-10.4.0` ; on python 3.10.14 (also tested with python 3.9 and tensorflow 2.11.1), on Debian GNU/Linux 11 (bullseye).', 'created_at': datetime.datetime(2024, 8, 12, 16, 42, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295577623, 'issue_id': 2457654212, 'author': 'mtburt', 'body': 'Hi All, \r\n\r\nJust chiming in that I see the same issue on a different project. When training Tensorflow 2.16.2 or 2.17 and python 3.12.3 on Ubuntu 24.04, Tensorflow suffers a very similar gradual increase in CPU Memory usage until the training is killed. \r\n\r\nOur project also trains the model using `model.train_on_batch`.  FWIW.', 'created_at': datetime.datetime(2024, 8, 19, 3, 3, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567330733, 'issue_id': 2457654212, 'author': 'Venkat6871', 'body': 'I tried running your code on Colab using TensorFlow 2.18.0 version and faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/969691b773a3b528ed600115881f4ef2/73457_tf-2-18-9-gpu-v.ipynb#scrollTo=fyoYAyXqEpL2) attached here for reference.\r\nThank you!', 'created_at': datetime.datetime(2025, 1, 2, 6, 28, 3, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-12 09:03:39 UTC): Hi **@arthursw** ,
Apologies for the delay. Could you please provide a Colab gist to help troubleshoot the above issue? The provided code is quite extensive, making it challenging to pinpoint the source of the memory leaks.
Thank you!

arthursw (Issue Creator) on (2024-08-12 16:42:45 UTC): Hi, thanks for your answer.

Here is a minimal reproducible example:

```
import numpy as np
from keras.layers import Input, concatenate
from keras.models import Model
from keras.layers import Conv3D, MaxPooling3D, UpSampling3D

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

import tensorflow as tf
from tensorflow.keras import backend as K
import psutil

def tversky_loss(y_true, y_pred):
    alpha = 0.5
    beta = 0.5

    ones = tf.ones(tf.shape(y_true))
    p0 = y_pred  # proba that voxels are class i
    p1 = ones - y_pred  # proba that voxels are not class i
    g0 = y_true
    g1 = ones - y_true

    num = K.sum(p0 * g0, (0, 1, 2, 3))
    den = num + alpha * K.sum(p0 * g1, (0, 1, 2, 3)) + beta * K.sum(p1 * g0, (0, 1, 2, 3))

    T = K.sum(num / den)  # when summing over classes, T has dynamic range [0 Ncl]

    Ncl = tf.cast(tf.shape(y_true)[-1], 'float32')
    return Ncl - T

def my_model(dim_in, Ncl):
    input = Input(shape=(dim_in, dim_in, dim_in, 1))

    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(input)
    high = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)

    x = MaxPooling3D((1, 2, 2), strides=None)(high)

    x = Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)
    mid = Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)

    x = MaxPooling3D((1, 2, 2), strides=None)(mid)

    x = Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
    x = Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
    x = Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
    x = Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)

    x = UpSampling3D(size=(1, 2, 2), data_format='channels_last')(x)
    x = Conv3D(64, (2, 2, 2), padding='same', activation='relu')(x)

    x = concatenate([x, mid])
    x = Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)
    x = Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)

    x = UpSampling3D(size=(1, 2, 2), data_format='channels_last')(x)
    x = Conv3D(48, (2, 2, 2), padding='same', activation='relu')(x)

    x = concatenate([x, high])
    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)
    x = Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)

    output = Conv3D(Ncl, (1, 1, 1), padding='same', activation='softmax')(x)

    model = Model(input, output)
    return model


# Training parameters:
batch_size = 4
patch_size = 48
epochs = 100
steps_per_epoch = 100
optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)
net = my_model(patch_size, 3)
net.compile(optimizer=optimizer, loss=tversky_loss, metrics=['accuracy'])

# data_list, target_list = cm.load_dataset(path_data, path_target, h5_dset_name)

def printMemoryUsage():
    gpus = tf.config.list_physical_devices('GPU')             
    for gpu in gpus:
        gpuNameRoot = gpu.name.split(':')[0] + ':'
        memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
        print(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
        virtual_memory = psutil.virtual_memory()
        print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
        
print('train...')
# Training loop:
for e in range(epochs):

    for it in range(steps_per_epoch):

        batch_data = np.random.rand(batch_size, patch_size, patch_size, patch_size, 1)
        batch_target = np.random.rand(batch_size, patch_size, patch_size, patch_size, 1)
        loss_train = net.train_on_batch(batch_data, batch_target)
        printMemoryUsage()
        print('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, epochs, it + 1, steps_per_epoch, loss_train[0], loss_train[1]))

```

With tensorflow 2.11.1, no memory leak:
```
(/home/amasson/storage/conda-envs/tensorflow) amasson@abacus25-3:~/tensorflow_test$ python main.py 
2024-08-12 18:25:35.639588: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-12 18:25:37.007876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cudnn-8.9.7.29-12-nxxaznjmoeoswhybcfmdhcohjvgwryky/lib:/grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cuda-12.0.0-g2atnxvq3akekpc6otev56c2rqzmnr7y/lib64
2024-08-12 18:25:37.007999: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cudnn-8.9.7.29-12-nxxaznjmoeoswhybcfmdhcohjvgwryky/lib:/grid5000/spack/v1/opt/spack/linux-debian11-x86_64_v2/gcc-10.4.0/cuda-12.0.0-g2atnxvq3akekpc6otev56c2rqzmnr7y/lib64
2024-08-12 18:25:37.008013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2024-08-12 18:25:38.881767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-12 18:25:40.034512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43435 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:a3:00.0, compute capability: 8.6
train...
2024-08-12 18:25:43.817654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907
2024-08-12 18:25:47.825264: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f227f5880f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-08-12 18:25:47.825365: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
2024-08-12 18:25:47.854377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-08-12 18:25:48.122897: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.28Gb, percent: 6.2%
epoch 1/100 - it 1/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.85Gb, percent: 6.1%
epoch 1/100 - it 2/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.83Gb, percent: 6.1%
epoch 1/100 - it 3/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.68Gb, percent: 6.1%
epoch 1/100 - it 4/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.49Gb, percent: 6.2%
epoch 1/100 - it 5/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%
epoch 1/100 - it 6/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.35Gb, percent: 6.2%
epoch 1/100 - it 7/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%
epoch 1/100 - it 8/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.36Gb, percent: 6.2%
epoch 1/100 - it 9/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%
epoch 1/100 - it 10/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%
epoch 1/100 - it 11/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%
epoch 1/100 - it 12/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%
epoch 1/100 - it 13/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%
epoch 1/100 - it 14/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.07Gb, percent: 6.2%
epoch 1/100 - it 15/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.27Gb, percent: 6.2%
epoch 1/100 - it 16/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%
epoch 1/100 - it 17/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.69Gb, percent: 6.1%
epoch 1/100 - it 18/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.61Gb, percent: 6.1%
epoch 1/100 - it 19/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.66Gb, percent: 6.1%
epoch 1/100 - it 20/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.72Gb, percent: 6.1%
epoch 1/100 - it 21/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.49Gb, percent: 6.2%
epoch 1/100 - it 22/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.41Gb, percent: 6.2%
epoch 1/100 - it 23/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.48Gb, percent: 6.2%
epoch 1/100 - it 24/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%
epoch 1/100 - it 25/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.57Gb, percent: 6.1%
epoch 1/100 - it 26/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.36Gb, percent: 6.2%
epoch 1/100 - it 27/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%
epoch 1/100 - it 28/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.11Gb, percent: 6.2%
epoch 1/100 - it 29/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.06Gb, percent: 6.2%
epoch 1/100 - it 30/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 506.97Gb, percent: 6.3%
epoch 1/100 - it 31/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 506.98Gb, percent: 6.3%
epoch 1/100 - it 32/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.28Gb, percent: 6.2%
epoch 1/100 - it 33/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%
epoch 1/100 - it 34/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.42Gb, percent: 6.2%
epoch 1/100 - it 35/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.56Gb, percent: 6.2%
epoch 1/100 - it 36/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.66Gb, percent: 6.1%
epoch 1/100 - it 37/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.59Gb, percent: 6.1%
epoch 1/100 - it 38/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.67Gb, percent: 6.1%
epoch 1/100 - it 39/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.40Gb, percent: 6.2%
epoch 1/100 - it 40/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.63Gb, percent: 6.1%
epoch 1/100 - it 41/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%
epoch 1/100 - it 42/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%
epoch 1/100 - it 43/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.41Gb, percent: 6.2%
epoch 1/100 - it 44/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.27Gb, percent: 6.2%
epoch 1/100 - it 45/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.22Gb, percent: 6.2%
epoch 1/100 - it 46/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%
epoch 1/100 - it 47/100 - loss: -0.201 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.00Gb, percent: 6.3%
epoch 1/100 - it 48/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%
epoch 1/100 - it 49/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.06Gb, percent: 6.2%
epoch 1/100 - it 50/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%
epoch 1/100 - it 51/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.20Gb, percent: 6.2%
epoch 1/100 - it 52/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.32Gb, percent: 6.2%
epoch 1/100 - it 53/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.64Gb, percent: 6.1%
epoch 1/100 - it 54/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.74Gb, percent: 6.1%
epoch 1/100 - it 55/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.75Gb, percent: 6.1%
epoch 1/100 - it 56/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.50Gb, percent: 6.2%
epoch 1/100 - it 57/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%
epoch 1/100 - it 58/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.19Gb, percent: 6.2%
epoch 1/100 - it 59/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.12Gb, percent: 6.2%
epoch 1/100 - it 60/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%
epoch 1/100 - it 61/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%
epoch 1/100 - it 62/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.23Gb, percent: 6.2%
epoch 1/100 - it 63/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.15Gb, percent: 6.2%
epoch 1/100 - it 64/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.21Gb, percent: 6.2%
epoch 1/100 - it 65/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.11Gb, percent: 6.2%
epoch 1/100 - it 66/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.29Gb, percent: 6.2%
epoch 1/100 - it 67/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%
epoch 1/100 - it 68/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.22Gb, percent: 6.2%
epoch 1/100 - it 69/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.33Gb, percent: 6.2%
epoch 1/100 - it 70/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.53Gb, percent: 6.2%
epoch 1/100 - it 71/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.91Gb, percent: 6.1%
epoch 1/100 - it 72/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.84Gb, percent: 6.1%
epoch 1/100 - it 73/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.73Gb, percent: 6.1%
epoch 1/100 - it 74/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.45Gb, percent: 6.2%
epoch 1/100 - it 75/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.25Gb, percent: 6.2%
epoch 1/100 - it 76/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.73Gb, percent: 6.1%
epoch 1/100 - it 77/100 - loss: -0.199 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.47Gb, percent: 6.2%
epoch 1/100 - it 78/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.24Gb, percent: 6.2%
epoch 1/100 - it 79/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.01, peak: 1.52
Memory info of CPU: total:540.82Gb, available: 507.16Gb, percent: 6.2%
epoch 1/100 - it 80/100 - loss: -0.200 - acc: 0.000
...
```

With tensorflow==2.16.1, memory leak?:

```
2024-08-12 18:38:50.624772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-12 18:38:52.170885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-12 18:38:53.769351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43607 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:a3:00.0, compute capability: 8.6
train...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723480737.529350  213588 service.cc:145] XLA service 0x7f49cc001e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1723480737.529433  213588 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
2024-08-12 18:38:57.663660: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-08-12 18:38:58.246372: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907
I0000 00:00:1723480748.391728  213588 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 1.10
Memory info of CPU: total:540.82Gb, available: 508.45Gb, percent: 6.0%
epoch 1/100 - it 1/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.03, peak: 1.11
Memory info of CPU: total:540.82Gb, available: 508.43Gb, percent: 6.0%
epoch 1/100 - it 2/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.03, peak: 1.11
Memory info of CPU: total:540.82Gb, available: 508.50Gb, percent: 6.0%
epoch 1/100 - it 3/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.04, peak: 1.12
Memory info of CPU: total:540.82Gb, available: 507.92Gb, percent: 6.1%
epoch 1/100 - it 4/100 - loss: -0.200 - acc: 0.000
WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f4b3070b490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Memory info of GPU /physical_device:GPU:0: current: 0.05, peak: 1.13
Memory info of CPU: total:540.82Gb, available: 508.26Gb, percent: 6.0%
epoch 1/100 - it 5/100 - loss: -0.200 - acc: 0.000
WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f4b3070b490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Memory info of GPU /physical_device:GPU:0: current: 0.05, peak: 1.13
Memory info of CPU: total:540.82Gb, available: 508.18Gb, percent: 6.0%
epoch 1/100 - it 6/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.06, peak: 1.14
Memory info of CPU: total:540.82Gb, available: 507.63Gb, percent: 6.1%
epoch 1/100 - it 7/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.07, peak: 1.15
Memory info of CPU: total:540.82Gb, available: 508.16Gb, percent: 6.0%
epoch 1/100 - it 8/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.07, peak: 1.15
Memory info of CPU: total:540.82Gb, available: 507.96Gb, percent: 6.1%
epoch 1/100 - it 9/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.08, peak: 1.16
Memory info of CPU: total:540.82Gb, available: 508.10Gb, percent: 6.1%
epoch 1/100 - it 10/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.09, peak: 1.17
Memory info of CPU: total:540.82Gb, available: 507.55Gb, percent: 6.2%
epoch 1/100 - it 11/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.10, peak: 1.18
Memory info of CPU: total:540.82Gb, available: 507.60Gb, percent: 6.1%
epoch 1/100 - it 12/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.10, peak: 1.18
Memory info of CPU: total:540.82Gb, available: 507.37Gb, percent: 6.2%
epoch 1/100 - it 13/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.11, peak: 1.19
Memory info of CPU: total:540.82Gb, available: 508.07Gb, percent: 6.1%
epoch 1/100 - it 14/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.12, peak: 1.20
Memory info of CPU: total:540.82Gb, available: 507.65Gb, percent: 6.1%
[...]
epoch 1/100 - it 95/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 1.77
Memory info of CPU: total:540.82Gb, available: 502.91Gb, percent: 7.0%
epoch 1/100 - it 96/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.70, peak: 1.78
Memory info of CPU: total:540.82Gb, available: 501.89Gb, percent: 7.2%
epoch 1/100 - it 97/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.70, peak: 1.78
Memory info of CPU: total:540.82Gb, available: 502.15Gb, percent: 7.2%
epoch 1/100 - it 98/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.71, peak: 1.79
Memory info of CPU: total:540.82Gb, available: 502.48Gb, percent: 7.1%
epoch 1/100 - it 99/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 1.80
Memory info of CPU: total:540.82Gb, available: 502.41Gb, percent: 7.1%
epoch 1/100 - it 100/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.73, peak: 1.81
Memory info of CPU: total:540.82Gb, available: 501.75Gb, percent: 7.2%
epoch 2/100 - it 1/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.73, peak: 1.81
Memory info of CPU: total:540.82Gb, available: 501.69Gb, percent: 7.2%
epoch 2/100 - it 2/100 - loss: -0.200 - acc: 0.000
Memory info of GPU /physical_device:GPU:0: current: 0.74, peak: 1.82
Memory info of CPU: total:540.82Gb, available: 502.45Gb, percent: 7.1%
epoch 2/100 - it 3/100 - loss: -0.200 - acc: 0.000
```

This with `cuda/12.0.0_gcc-10.4.0` and `cudnn/8.9.7.29-12_gcc-10.4.0` ; on python 3.10.14 (also tested with python 3.9 and tensorflow 2.11.1), on Debian GNU/Linux 11 (bullseye).

mtburt on (2024-08-19 03:03:04 UTC): Hi All, 

Just chiming in that I see the same issue on a different project. When training Tensorflow 2.16.2 or 2.17 and python 3.12.3 on Ubuntu 24.04, Tensorflow suffers a very similar gradual increase in CPU Memory usage until the training is killed. 

Our project also trains the model using `model.train_on_batch`.  FWIW.

Venkat6871 (Assginee) on (2025-01-02 06:28:03 UTC): I tried running your code on Colab using TensorFlow 2.18.0 version and faced the same issue. Please find the [gist1](https://colab.sandbox.google.com/gist/Venkat6871/969691b773a3b528ed600115881f4ef2/73457_tf-2-18-9-gpu-v.ipynb#scrollTo=fyoYAyXqEpL2) attached here for reference.
Thank you!

"
2457255866,issue,closed,completed,adding type annotations in TensorFlow,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have started adding type annotations to the TensorFlow codebase, focusing initially on core utility functions that are widely used. The goal is to improve code clarity, enhance IDE support, and enable static analysis tools to catch bugs earlier.

Given the size and complexity of the TensorFlow codebase, this will be an incremental process. The annotations are being added in `.pyi` files to maintain compatibility with older Python versions (2.7, 3.3, 3.4). Ive also ensured that all new annotations are tested with `mypy` to verify their correctness.

Please review the initial set of changes and provide feedback on this approach. I encourage other contributors to start adding annotations as they work on different parts of the codebase.

### Standalone code to reproduce the issue

```shell
from typing import List, Optional

def process_data(inputs: List[float], threshold: Optional[float] = None) -> List[float]:
    """"""
    Processes the input data by applying a threshold.

    Args:
        inputs (List[float]): A list of floating-point numbers to be processed.
        threshold (Optional[float]): A threshold value to apply. If None, no thresholding is done.

    Returns:
        List[float]: The processed list of floats.
    """"""
    if threshold is not None:
        # Apply the threshold to the inputs
        return [x for x in inputs if x > threshold]
    return inputs
```


### Relevant log output

```shell
=================================== test session starts ====================================
platform darwin -- Python 3.8.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /path/to/your/project
collected 10 items

test_module.py ..........                                                     [100%]

==================================== 10 passed in 2.34s ====================================
```
",chungyu1108,2024-08-09 07:06:28+00:00,['tilakrayal'],2024-09-18 08:04:22+00:00,2024-08-27 01:55:51+00:00,https://github.com/tensorflow/tensorflow/issues/73444,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:performance', 'Performance Issue'), ('TF 2.8', '')]","[{'comment_id': 2283209905, 'issue_id': 2457255866, 'author': 'tilakrayal', 'body': '@chungyu1108,\r\nIn the given code snippet you have defined the class and its methods but are not calling them anywhere. Also the tensorflow v2.8 is pretty old, could you please try to upgrade to latest tensorflow v2.17 and provide the update if you are facing the same issue. Thank you!', 'created_at': datetime.datetime(2024, 8, 12, 6, 41, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297815754, 'issue_id': 2457255866, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 20, 1, 53, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419489, 'issue_id': 2457255866, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419555, 'issue_id': 2457255866, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73444"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73444"">No</a>', 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 54, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-12 06:41:01 UTC): @chungyu1108,
In the given code snippet you have defined the class and its methods but are not calling them anywhere. Also the tensorflow v2.8 is pretty old, could you please try to upgrade to latest tensorflow v2.17 and provide the update if you are facing the same issue. Thank you!

github-actions[bot] on (2024-08-20 01:53:44 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-27 01:55:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-27 01:55:54 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73444"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73444"">No</a>

"
2457252246,issue,closed,completed,Failure building wheel for v2.16.1 from scratch: cannot stat cudnn_backend_base.h,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

Alma Linux 8

### Mobile device

n/a

### Python version

3.12

### Bazel version

6.1.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

CUDA 12.4.1, cuDNN 8.9.7.29

### GPU model and memory

None

### Current behavior?

```
+ cp ./cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_backend_base.h /data/cmsbld/jenkins/workspace/ib-run-pr-tests/testBuildDir/BUILD/el8_amd64_gcc12/external/tensorflow-sources/2.16.1-c5b09320be47428430884939ba837d47/cmsdist-tmp/tmp.C6XGHynCRW/tensorflow/include/external/./cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/
cp: cannot stat './cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_backend_base.h': No such file or directory
```

I expect the build to pass.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_",iarspider,2024-08-09 07:04:24+00:00,['Venkat6871'],2024-08-12 07:20:05+00:00,2024-08-12 07:20:03+00:00,https://github.com/tensorflow/tensorflow/issues/73442,"[('type:build/install', 'Build and install issues')]","[{'comment_id': 2283264016, 'issue_id': 2457252246, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73442"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73442"">No</a>', 'created_at': datetime.datetime(2024, 8, 12, 7, 20, 4, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-08-12 07:20:04 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73442"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73442"">No</a>

"
2456711340,issue,closed,completed,Web,https://github.com/material-components/material-web/blob/main/docs%2Fquick-start.md,Morehman27,2024-08-08 22:07:19+00:00,['tilakrayal'],2024-08-09 15:24:15+00:00,2024-08-09 15:24:15+00:00,https://github.com/tensorflow/tensorflow/issues/73424,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('invalid', 'Hacktoberfest spam PR')]","[{'comment_id': 2277841227, 'issue_id': 2456711340, 'author': 'tilakrayal', 'body': '@Morehman27,\r\nCould you please provide more context/information/code about the issue you are facing on the tensorflow which helps to debug the issue? Thank you!', 'created_at': datetime.datetime(2024, 8, 9, 12, 31, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278200054, 'issue_id': 2456711340, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2024, 8, 9, 15, 24, 15, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-09 12:31:46 UTC): @Morehman27,
Could you please provide more context/information/code about the issue you are facing on the tensorflow which helps to debug the issue? Thank you!

mihaimaruseac on (2024-08-09 15:24:15 UTC): Please don't spam

"
2456373136,issue,closed,completed,Convergence of Actor critic algorthim ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

V1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello, 

I am using the following actor critic agent, but it does not converge at all. Please could you help me with this matter.
In the following  the code and the results

`
import numpy as np
import tensorflow as tf
import tensorflow.compat.v1 as tf
import json

tf.disable_v2_behavior()
tf.reset_default_graph()
class A2CLSTM(object):
    def __init__(
            self,
            sess,
            n_actions,
            n_features,
            lr_a,
            lr_c,
            entropy_beta,
            batch_size=32  # Default batch size
    ):
        self.sess = sess
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr_a = lr_a
        self.lr_c = lr_c
        self.entroy_beta = entropy_beta
        self.batch_size = batch_size  # Set the batch size

        self.lstm_cell_size = 64

        OPT_A = tf.train.AdamOptimizer(self.lr_a)
        OPT_C = tf.train.AdamOptimizer(self.lr_c)

        with tf.name_scope('inputs'):
            self.s = tf.placeholder(tf.float32, [None, self.n_features], ""state"")
            self.a = tf.placeholder(tf.int32, [None, 1], ""action"")
            self.td_target = tf.placeholder(tf.float32, [None, 1], ""td_target"")

        self.acts_prob, self.v, self.a_params, self.c_params = self._build_net()

        with tf.name_scope('TD_error'):
            self.td_error = tf.subtract(self.td_target, self.v, name='TD_error')

        with tf.name_scope('c_loss'):
            self.c_loss = tf.reduce_mean(tf.square(self.td_error))

        with tf.name_scope('a_loss'):
            log_prob = tf.reduce_sum(tf.log(self.acts_prob + 1e-5) * tf.one_hot(self.a, self.n_actions, dtype=tf.float32),
                                      axis=1, keepdims=True)
            exp_v = log_prob * tf.stop_gradient(self.td_error)
            entropy = -tf.reduce_sum(self.acts_prob * tf.log(self.acts_prob + 1e-5), axis=1,
                                      keepdims=True)  # encourage exploration
            self.exp_v = self.entroy_beta * entropy + exp_v
            self.a_loss = tf.reduce_mean(-self.exp_v)

        with tf.name_scope('compute_grads'):
            self.a_grads = tf.gradients(self.a_loss, self.a_params)
            self.c_grads = tf.gradients(self.c_loss, self.c_params)

        with tf.name_scope('c_train'):
            self.c_train_op = OPT_C.apply_gradients(zip(self.c_grads, self.c_params))

        with tf.name_scope('a_train'):
            self.a_train_op = OPT_A.apply_gradients(zip(self.a_grads, self.a_params))

        self.sess.run(tf.global_variables_initializer())

        # Initialize lists to store losses
        self.actor_loss_history = []
        self.critic_loss_history = []

    def _build_net(self):
            w_init = tf.random_normal_initializer(0., .1)
            b_init = tf.constant_initializer(0.1)

            with tf.variable_scope('Critic'):
                # [time_step, feature] => [time_step, batch, feature]
                s = tf.expand_dims(self.s, axis=1, name='timely_input')

                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.lstm_cell_size)
                self.lstm_state_init = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)

                outputs, _ = tf.nn.dynamic_rnn(
                    cell=lstm_cell,
                    inputs=s,
                    initial_state=self.lstm_state_init,
                    time_major=True
                )
                cell_out = tf.reshape(outputs[-1, :, :], [-1, self.lstm_cell_size],
                                      name='flatten_lstm_outputs')  # joined state representation

                l_c1 = tf.layers.dense(
                    inputs=cell_out,
                    units=32,
                    activation=tf.nn.tanh,
                    kernel_initializer=w_init,
                    bias_initializer=b_init,
                    name='l_c1'
                )

                v = tf.layers.dense(
                    inputs=l_c1,
                    units=1,
                    kernel_initializer=w_init,
                    bias_initializer=b_init,
                    name='V'
                )  # state value

            with tf.variable_scope('Actor'):
                l_a1 = tf.layers.dense(
                    inputs=cell_out,
                    units=32,  # number of hidden units
                    activation=tf.nn.tanh,  # the activation function
                    kernel_initializer=w_init,  # weights
                    bias_initializer=b_init,  # biases
                    name='l_a1'
                )

                acts_prob = tf.layers.dense(
                    inputs=l_a1,
                    units=self.n_actions,  # output units
                    activation=tf.nn.softmax,  # get action probabilities
                    kernel_initializer=w_init,  # weights
                    name='acts_prob'
                )
            a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Actor')
            c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Critic')

            return acts_prob, v, a_params, c_params

    def choose_action(self, s):
            probs = self.sess.run(self.acts_prob, feed_dict={self.s: s})  # get probabilities for all actions
            a = np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())
            return a
    def learn(self, feed_dict):
        # Ensure that feed_dict contains batches of data
        _, _, actor_loss, critic_loss = self.sess.run(
            [self.a_train_op, self.c_train_op, self.a_loss, self.c_loss],
            feed_dict=feed_dict
        )

        # Append current losses to history
        self.actor_loss_history.append(float(actor_loss))
        self.critic_loss_history.append(float(critic_loss))

        # Save losses to JSON files every 100 iterations
        if len(self.actor_loss_history) % 100 == 0:
            actor_loss_filename = 'logs/actor_loss.json'
            critic_loss_filename = 'logs/critic_loss.json'

            with open(actor_loss_filename, 'w') as f:
                json.dump({'actor_loss': self.actor_loss_history}, f)

            with open(critic_loss_filename, 'w') as f:
                json.dump({'critic_loss': self.critic_loss_history}, f)

    def choose_action(self, s):
        probs = self.sess.run(self.acts_prob, feed_dict={self.s: s})  # get probabilities for all actions
        a = np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())
        return a

    def target_v(self, s):
        v = self.sess.run(self.v, {self.s: s})
        return v
`
![Fig_1_Actor_loss](https://github.com/user-attachments/assets/c498df43-b98c-46e4-ba19-57a0d684b7a9)
![Fig_1_Critic_Loss](https://github.com/user-attachments/assets/a26f866d-f003-450d-b7a4-3e2497d26611)
![Reward Function](https://github.com/user-attachments/assets/4cdac1b1-ad82-42c9-b710-00843d943201)

For the learning rate, I have tried different values, and here is the most recent one: LR_A =0.005
Learning Rate _ Actor =0.005
Learning Rate _Critic= 0.008
GAMMA = 0.9
ENTROY_BETA = 0.01

### Standalone code to reproduce the issue

```shell
Hello, 

I am using the following actor critic agent, but it does not converge at all. Please could you help me with this matter.
In the following  the code and the results

`
import numpy as np
import tensorflow as tf
import tensorflow.compat.v1 as tf
import json

tf.disable_v2_behavior()
tf.reset_default_graph()
class A2CLSTM(object):
    def __init__(
            self,
            sess,
            n_actions,
            n_features,
            lr_a,
            lr_c,
            entropy_beta,
            batch_size=32  # Default batch size
    ):
        self.sess = sess
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr_a = lr_a
        self.lr_c = lr_c
        self.entroy_beta = entropy_beta
        self.batch_size = batch_size  # Set the batch size

        self.lstm_cell_size = 64

        OPT_A = tf.train.AdamOptimizer(self.lr_a)
        OPT_C = tf.train.AdamOptimizer(self.lr_c)

        with tf.name_scope('inputs'):
            self.s = tf.placeholder(tf.float32, [None, self.n_features], ""state"")
            self.a = tf.placeholder(tf.int32, [None, 1], ""action"")
            self.td_target = tf.placeholder(tf.float32, [None, 1], ""td_target"")

        self.acts_prob, self.v, self.a_params, self.c_params = self._build_net()

        with tf.name_scope('TD_error'):
            self.td_error = tf.subtract(self.td_target, self.v, name='TD_error')

        with tf.name_scope('c_loss'):
            self.c_loss = tf.reduce_mean(tf.square(self.td_error))

        with tf.name_scope('a_loss'):
            log_prob = tf.reduce_sum(tf.log(self.acts_prob + 1e-5) * tf.one_hot(self.a, self.n_actions, dtype=tf.float32),
                                      axis=1, keepdims=True)
            exp_v = log_prob * tf.stop_gradient(self.td_error)
            entropy = -tf.reduce_sum(self.acts_prob * tf.log(self.acts_prob + 1e-5), axis=1,
                                      keepdims=True)  # encourage exploration
            self.exp_v = self.entroy_beta * entropy + exp_v
            self.a_loss = tf.reduce_mean(-self.exp_v)

        with tf.name_scope('compute_grads'):
            self.a_grads = tf.gradients(self.a_loss, self.a_params)
            self.c_grads = tf.gradients(self.c_loss, self.c_params)

        with tf.name_scope('c_train'):
            self.c_train_op = OPT_C.apply_gradients(zip(self.c_grads, self.c_params))

        with tf.name_scope('a_train'):
            self.a_train_op = OPT_A.apply_gradients(zip(self.a_grads, self.a_params))

        self.sess.run(tf.global_variables_initializer())

        # Initialize lists to store losses
        self.actor_loss_history = []
        self.critic_loss_history = []

    def _build_net(self):
            w_init = tf.random_normal_initializer(0., .1)
            b_init = tf.constant_initializer(0.1)

            with tf.variable_scope('Critic'):
                # [time_step, feature] => [time_step, batch, feature]
                s = tf.expand_dims(self.s, axis=1, name='timely_input')

                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.lstm_cell_size)
                self.lstm_state_init = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)

                outputs, _ = tf.nn.dynamic_rnn(
                    cell=lstm_cell,
                    inputs=s,
                    initial_state=self.lstm_state_init,
                    time_major=True
                )
                cell_out = tf.reshape(outputs[-1, :, :], [-1, self.lstm_cell_size],
                                      name='flatten_lstm_outputs')  # joined state representation

                l_c1 = tf.layers.dense(
                    inputs=cell_out,
                    units=32,
                    activation=tf.nn.tanh,
                    kernel_initializer=w_init,
                    bias_initializer=b_init,
                    name='l_c1'
                )

                v = tf.layers.dense(
                    inputs=l_c1,
                    units=1,
                    kernel_initializer=w_init,
                    bias_initializer=b_init,
                    name='V'
                )  # state value

            with tf.variable_scope('Actor'):
                l_a1 = tf.layers.dense(
                    inputs=cell_out,
                    units=32,  # number of hidden units
                    activation=tf.nn.tanh,  # the activation function
                    kernel_initializer=w_init,  # weights
                    bias_initializer=b_init,  # biases
                    name='l_a1'
                )

                acts_prob = tf.layers.dense(
                    inputs=l_a1,
                    units=self.n_actions,  # output units
                    activation=tf.nn.softmax,  # get action probabilities
                    kernel_initializer=w_init,  # weights
                    name='acts_prob'
                )
            a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Actor')
            c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Critic')

            return acts_prob, v, a_params, c_params

    def choose_action(self, s):
            probs = self.sess.run(self.acts_prob, feed_dict={self.s: s})  # get probabilities for all actions
            a = np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())
            return a
    def learn(self, feed_dict):
        # Ensure that feed_dict contains batches of data
        _, _, actor_loss, critic_loss = self.sess.run(
            [self.a_train_op, self.c_train_op, self.a_loss, self.c_loss],
            feed_dict=feed_dict
        )

        # Append current losses to history
        self.actor_loss_history.append(float(actor_loss))
        self.critic_loss_history.append(float(critic_loss))

        # Save losses to JSON files every 100 iterations
        if len(self.actor_loss_history) % 100 == 0:
            actor_loss_filename = 'logs/actor_loss.json'
            critic_loss_filename = 'logs/critic_loss.json'

            with open(actor_loss_filename, 'w') as f:
                json.dump({'actor_loss': self.actor_loss_history}, f)

            with open(critic_loss_filename, 'w') as f:
                json.dump({'critic_loss': self.critic_loss_history}, f)

    def choose_action(self, s):
        probs = self.sess.run(self.acts_prob, feed_dict={self.s: s})  # get probabilities for all actions
        a = np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())
        return a

    def target_v(self, s):
        v = self.sess.run(self.v, {self.s: s})
        return v
`
![Fig_1_Actor_loss](https://github.com/user-attachments/assets/c498df43-b98c-46e4-ba19-57a0d684b7a9)
![Fig_1_Critic_Loss](https://github.com/user-attachments/assets/a26f866d-f003-450d-b7a4-3e2497d26611)
![Reward Function](https://github.com/user-attachments/assets/4cdac1b1-ad82-42c9-b710-00843d943201)

For the learning rate, I have tried different values, and here is the most recent one: LR_A =0.005
Learning Rate _ Actor =0.005
Learning Rate _Critic= 0.008
GAMMA = 0.9
ENTROY_BETA = 0.01
```


### Relevant log output

_No response_",AhdHazim,2024-08-08 18:17:16+00:00,['Venkat6871'],2025-01-07 02:02:54+00:00,2025-01-07 02:02:47+00:00,https://github.com/tensorflow/tensorflow/issues/73404,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('1.4.0', '')]","[{'comment_id': 2283370389, 'issue_id': 2456373136, 'author': 'Venkat6871', 'body': 'Hi **@AhdHazim** ,\r\nSorry for the dealy, I tried to run your code on Colab using the TF-nightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached [documentation](https://www.tensorflow.org/guide/migrate) and a replicated [gist](https://colab.research.google.com/gist/Venkat6871/71ea1e56b00d7c30bb793de2251a81b2/73404_tf-nightly-2-18-0-v.ipynb) for reference. \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 12, 8, 23, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284136039, 'issue_id': 2456373136, 'author': 'AhdHazim', 'body': ""Thank you for your kind reply!\r\n\r\nYou are right,  I am using TensorFlow 1.x . Indeed I have tried before to moved  2.x  but I have faced many issues.  You  can run it with TensorFlow 1.x if you add  the following lines to your run file:\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Use ConfigProto with tf.compat.v1\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\n\r\n________________________________\r\nFrom: Venkat6871 ***@***.***>\r\nSent: 12 August 2024 09:23\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Sabr, Ohood ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] Convergence of Actor critic algorthim (Issue #73404)\r\n\r\n[ATTENTION : Ce courriel provient de l'extrieur de l'TS]\r\nvitez de cliquer sur un lien ou d'ouvrir une pice jointe si vous ne connaissez pas l'expditeur du courriel. En cas de doute, veuillez SVP crer un billet Problme de courriel au GUS<https://gus.etsmtl.ca/c2atom/LoginAzure?landingPage=/portal-request-form/714d9e14-bc17-47b8-a0e6-e154184ecbc2> .\r\n\r\n\r\n\r\nHi @AhdHazim<https://github.com/AhdHazim> ,\r\nI tried to run your code on Colab using the TF-nightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached documentation<https://www.tensorflow.org/guide/migrate> and a replicated gist<https://colab.research.google.com/gist/Venkat6871/71ea1e56b00d7c30bb793de2251a81b2/73404_tf-nightly-2-18-0-v.ipynb> for reference.\r\n\r\nThank you!\r\n\r\n\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/73404#issuecomment-2283370389>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BDSWQX4PRMM7TDNXNT3VHNDZRBWIRAVCNFSM6AAAAABMHAIR7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEOBTGM3TAMZYHE>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>"", 'created_at': datetime.datetime(2024, 8, 12, 14, 26, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558921546, 'issue_id': 2456373136, 'author': 'Venkat6871', 'body': 'Hi **@AhdHazim** ,\r\nApologies for the delay, and thank you for your patience. TensorFlow 1.x is already deprecated, so you need to migrate to TensorFlow 2.x. For your reference, I have attached the  [documentation](https://www.tensorflow.org/guide/migrate) on migrating.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 5, 11, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566070069, 'issue_id': 2456373136, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 31, 2, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250947, 'issue_id': 2456373136, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574251026, 'issue_id': 2456373136, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73404"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73404"">No</a>', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 52, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-12 08:23:12 UTC): Hi **@AhdHazim** ,
Sorry for the dealy, I tried to run your code on Colab using the TF-nightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached [documentation](https://www.tensorflow.org/guide/migrate) and a replicated [gist](https://colab.research.google.com/gist/Venkat6871/71ea1e56b00d7c30bb793de2251a81b2/73404_tf-nightly-2-18-0-v.ipynb) for reference. 

Thank you!

AhdHazim (Issue Creator) on (2024-08-12 14:26:54 UTC): Thank you for your kind reply!

You are right,  I am using TensorFlow 1.x . Indeed I have tried before to moved  2.x  but I have faced many issues.  You  can run it with TensorFlow 1.x if you add  the following lines to your run file:

tf.compat.v1.disable_eager_execution()

# Use ConfigProto with tf.compat.v1
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=config)

________________________________
From: Venkat6871 ***@***.***>
Sent: 12 August 2024 09:23
To: tensorflow/tensorflow ***@***.***>
Cc: Sabr, Ohood ***@***.***>; Mention ***@***.***>
Subject: Re: [tensorflow/tensorflow] Convergence of Actor critic algorthim (Issue #73404)

[ATTENTION : Ce courriel provient de l'extrieur de l'TS]
vitez de cliquer sur un lien ou d'ouvrir une pice jointe si vous ne connaissez pas l'expditeur du courriel. En cas de doute, veuillez SVP crer un billet Problme de courriel au GUS<https://gus.etsmtl.ca/c2atom/LoginAzure?landingPage=/portal-request-form/714d9e14-bc17-47b8-a0e6-e154184ecbc2> .



Hi @AhdHazim<https://github.com/AhdHazim> ,
I tried to run your code on Colab using the TF-nightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached documentation<https://www.tensorflow.org/guide/migrate> and a replicated gist<https://colab.research.google.com/gist/Venkat6871/71ea1e56b00d7c30bb793de2251a81b2/73404_tf-nightly-2-18-0-v.ipynb> for reference.

Thank you!


Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/73404#issuecomment-2283370389>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BDSWQX4PRMM7TDNXNT3VHNDZRBWIRAVCNFSM6AAAAABMHAIR7GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEOBTGM3TAMZYHE>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

Venkat6871 (Assginee) on (2024-12-23 05:11:18 UTC): Hi **@AhdHazim** ,
Apologies for the delay, and thank you for your patience. TensorFlow 1.x is already deprecated, so you need to migrate to TensorFlow 2.x. For your reference, I have attached the  [documentation](https://www.tensorflow.org/guide/migrate) on migrating.
Thank you!

github-actions[bot] on (2024-12-31 02:01:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-07 02:02:47 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-07 02:02:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73404"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73404"">No</a>

"
2455397851,issue,closed,completed,"Regarding lstm's full integer quantisation, does it quantise its intermediate output? The lstm full integer quantisation I get has only one input and one output quantisation parameter (minus the arguments)","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
![Uploading _20240808180440.png]()

- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",1778813838,2024-08-08 10:05:27+00:00,"['pkgoogle', 'sawantkumar']",2024-09-19 02:00:16+00:00,2024-09-19 02:00:12+00:00,https://github.com/tensorflow/tensorflow/issues/73369,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2329775022, 'issue_id': 2455397851, 'author': 'pkgoogle', 'body': 'Hi @1778813838, can you please provide a reproducible code example? So that we may verify that what you are seeing is expected or not expected. Though in general, we recommend that people use Transformers instead of LSTMs.', 'created_at': datetime.datetime(2024, 9, 4, 19, 2, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102418, 'issue_id': 2455397851, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826659, 'issue_id': 2455397851, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826759, 'issue_id': 2455397851, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73369"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73369"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 15, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-09-04 19:02:17 UTC): Hi @1778813838, can you please provide a reproducible code example? So that we may verify that what you are seeing is expected or not expected. Though in general, we recommend that people use Transformers instead of LSTMs.

github-actions[bot] on (2024-09-12 01:58:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:11 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 02:00:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73369"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73369"">No</a>

"
2455364368,issue,closed,completed,Tensorflow can't detect my GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tensorflow can't detect my gpu.
But Cuda and cuDNN is installed

CUDA version: 12.5

### Standalone code to reproduce the issue

```shell
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Wed_Apr_17_19:36:51_Pacific_Daylight_Time_2024
Cuda compilation tools, release 12.5, V12.5.40
Build cuda_12.5.r12.5/compiler.34177558_0
```

+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   44C    P8              1W /   45W |       0MiB /   6141MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

``` python
tf.test.gpu_device_name()
# ''
tf.config.list_physical_devices()
# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
```

### Relevant log output

_No response_",siy415,2024-08-08 09:49:39+00:00,['Venkat6871'],2024-08-27 01:55:57+00:00,2024-08-27 01:55:52+00:00,https://github.com/tensorflow/tensorflow/issues/73368,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2283319959, 'issue_id': 2455364368, 'author': 'Venkat6871', 'body': 'Hi **@siy415** ,\r\nI apologize for the delayed response. GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin. Here i am giving [documentation](https://www.tensorflow.org/install/source_windows#gpu) for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 12, 7, 55, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297815783, 'issue_id': 2455364368, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 20, 1, 53, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419518, 'issue_id': 2455364368, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419608, 'issue_id': 2455364368, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73368"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73368"">No</a>', 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 56, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-12 07:55:43 UTC): Hi **@siy415** ,
I apologize for the delayed response. GPU support on native-Windows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflow-cpu with TensorFlow-DirectML-Plugin. Here i am giving [documentation](https://www.tensorflow.org/install/source_windows#gpu) for reference.
Thank you!

github-actions[bot] on (2024-08-20 01:53:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-27 01:55:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-27 01:55:56 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73368"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73368"">No</a>

"
2455335663,issue,open,,Gradient computation for `vectorized_map` nested inside `while_loop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Mint 21.3

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to compute gradients for a `tf.vectorized_map`ped function nested within a call to `tf.while_loop` (and hence also `tf.map_fn`) as in [this (trivial!) Colab example](https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing).  The top level function can compute its return value in all three execution modes (eager, graph, XLA).  

I expected to also be able to compute the gradients of the function with respect to its inputs.  This works under eager and graph mode, but not XLA mode where an InvalidArgument exception occurs:

> InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

In the example, all shapes are well-specified (by hard coding in this trivial example), so it feels like some shape information is getting lost somewhere.  Is this a bug, or a ""feature"" for which there is a workaround, I wonder? 



### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-151283a3b91b> in <cell line: 36>()
     34 print(""Eager mode:"", value_and_grads(0.1, 0.4))  # Eager mode runs
     35 print(""Graph mode:"", tf.function(lambda: value_and_grads(0.1, 0.4))()) # Graph mode runs
---> 36 print(""XLA mode:"", tf.function(lambda: value_and_grads(0.1, 0.4), jit_compile=True)()) # XLA fails

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[{{function_node __inference___backward_f_460_482}}{{node gradients/pfor/Tile_grad/Reshape_1}}]]
	tf2xla conversion failed while converting while_body_347_grad_431_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[gradient_tape/while/while_grad]]
	tf2xla conversion failed while converting __inference_<lambda>_538[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_<lambda>_538]
```
",chrism0dwk,2024-08-08 09:36:08+00:00,"['cheshire', 'wangpengmit']",2024-08-09 08:06:13+00:00,,https://github.com/tensorflow/tensorflow/issues/73367,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:xla', 'XLA'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2277384471, 'issue_id': 2455335663, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflower v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/36a73649258caa353e172d512c57c814/untitled2059.ipynb).', 'created_at': datetime.datetime(2024, 8, 9, 8, 1, 7, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-09 08:01:07 UTC): I was able to reproduce the issue on tensorflower v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/36a73649258caa353e172d512c57c814/untitled2059.ipynb).

"
2455305180,issue,closed,completed,"ImportError when trying to run ""from tensorflow import keras""","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

import keras from tensorflow successfully

### Standalone code to reproduce the issue

```shell
from tensorflow import keras
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File C:\ProgramData\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[2], line 1
----> 1 from tensorflow import keras

File C:\ProgramData\anaconda3\Lib\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File C:\ProgramData\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",akoushandeh11,2024-08-08 09:22:12+00:00,['Venkat6871'],2024-08-28 01:56:25+00:00,2024-08-28 01:56:22+00:00,https://github.com/tensorflow/tensorflow/issues/73366,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2283092314, 'issue_id': 2455305180, 'author': 'Venkat6871', 'body': 'Hi **@akoushandeh11** ,\r\nI apologize for the delayed response. Could you please install Keras separately using ```pip install keras```. Then, import it as ```import keras``` instead of  ```from tensorflow import keras```. If you still facing any issue please let us know.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 12, 4, 27, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2300137514, 'issue_id': 2455305180, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 21, 1, 54, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935875, 'issue_id': 2455305180, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935933, 'issue_id': 2455305180, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73366"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73366"">No</a>', 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 24, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-12 04:27:15 UTC): Hi **@akoushandeh11** ,
I apologize for the delayed response. Could you please install Keras separately using ```pip install keras```. Then, import it as ```import keras``` instead of  ```from tensorflow import keras```. If you still facing any issue please let us know.

Thank you!

github-actions[bot] on (2024-08-21 01:54:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-28 01:56:21 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-28 01:56:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73366"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73366"">No</a>

"
2455290137,issue,closed,completed,sample_weight taking long to initialize,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When training a keras model on in-memory data (e.g., numpy arrays or pandas DataFrames) and simultaneously providing the `sample_weight` parameter in `fit`, the initialization takes very long (ca. 5 minutes with 1 Mio. examples). This problem amplifies when the number of examples increases.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras
import numpy as np

x_train = np.random.rand(10000, 10)
y_train = np.random.rand(10000, 1)
w_train = np.random.rand(10000, 1)

def run(with_sample_weights=True):
  model = keras.Sequential()
  model.add(keras.layers.Dense(64, input_dim=x_train.shape[1], activation='relu')) 
  model.add(keras.layers.Dropout(0.1))
  model.add(keras.layers.Dense(64, activation='relu'))
  model.add(keras.layers.Dropout(0.1))
  model.add(keras.layers.Dense(64, activation='relu'))
  model.add(keras.layers.Dropout(0.1))
  model.add(keras.layers.Dense(64, activation='relu'))
  model.add(keras.layers.Dropout(0.1))
  model.add(keras.layers.Dense(1, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

  if with_sample_weights:
    model.fit(x_train, y_train, sample_weight=w_train)
  else:
    model.fit(x_train, y_train)

import time

start = time.time()
for i in range(10):
  run(True)
end = time.time()
print(end - start)

start = time.time()
for i in range(10):
  run(False)
end = time.time()
print(end - start)
```


### Relevant log output

```shell
For 10,000 examples: 31s vs 14s
For 1,000,000: 1253.8s vs 17.1s
```
",jubebo,2024-08-08 09:14:54+00:00,['tilakrayal'],2024-08-24 01:53:21+00:00,2024-08-24 01:53:19+00:00,https://github.com/tensorflow/tensorflow/issues/73362,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('type:performance', 'Performance Issue'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2275357975, 'issue_id': 2455290137, 'author': 'jubebo', 'body': 'From own investigation (so far only tested on tf/keras in version 2.12.0), the implementation of `handle_partial_sample_weights` in `training_utils.py` is inefficient. In my case (`sample_weight` is a dense numpy array with shape `(1000000,)` and all entries filled/non-null) the following [lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L79-L80) were causing the bottleneck.\r\n\r\nI suggest exchanging [lines 77-78](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L77-L78) with `any_sample_weight = sample_weights.numpy().any()` and exchanging [lines 79-80](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L79-L80) with `partial_sample_weight = sample_weights.numpy().all()`.\r\n\r\nIn my case, `handle_partial_sample_weights` was called from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L249-L250).\r\n\r\nPlease let me know if this could have side effects in case of other usage patterns.', 'created_at': datetime.datetime(2024, 8, 8, 9, 23, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275366931, 'issue_id': 2455290137, 'author': 'jubebo', 'body': 'FYI: Issue was also discussed for standalone keras [here](https://stackoverflow.com/questions/63158424/why-does-keras-model-fit-with-sample-weight-have-long-initialization-time) and [here](https://github.com/keras-team/keras/issues/14877).', 'created_at': datetime.datetime(2024, 8, 8, 9, 27, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277345566, 'issue_id': 2455290137, 'author': 'tilakrayal', 'body': '@jubebo,\r\nI tried to execute the mentioned code on tensorflower v2.17 which contains keras3.0, and observed that the time taking is lesser when compared to tensorflow v2.12. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/75b1c4e1ae5cc00dcd757dfd9397f805/untitled2058.ipynb).\r\n\r\nAlso please try to use the latest tensorflow v2.17 and as this issue is more related to keras, raise the issue in keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 9, 7, 37, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527137, 'issue_id': 2455290137, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987431, 'issue_id': 2455290137, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987457, 'issue_id': 2455290137, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73362"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73362"">No</a>', 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 21, tzinfo=datetime.timezone.utc)}]","jubebo (Issue Creator) on (2024-08-08 09:23:13 UTC): From own investigation (so far only tested on tf/keras in version 2.12.0), the implementation of `handle_partial_sample_weights` in `training_utils.py` is inefficient. In my case (`sample_weight` is a dense numpy array with shape `(1000000,)` and all entries filled/non-null) the following [lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L79-L80) were causing the bottleneck.

I suggest exchanging [lines 77-78](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L77-L78) with `any_sample_weight = sample_weights.numpy().any()` and exchanging [lines 79-80](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L79-L80) with `partial_sample_weight = sample_weights.numpy().all()`.

In my case, `handle_partial_sample_weights` was called from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L249-L250).

Please let me know if this could have side effects in case of other usage patterns.

jubebo (Issue Creator) on (2024-08-08 09:27:38 UTC): FYI: Issue was also discussed for standalone keras [here](https://stackoverflow.com/questions/63158424/why-does-keras-model-fit-with-sample-weight-have-long-initialization-time) and [here](https://github.com/keras-team/keras/issues/14877).

tilakrayal (Assginee) on (2024-08-09 07:37:39 UTC): @jubebo,
I tried to execute the mentioned code on tensorflower v2.17 which contains keras3.0, and observed that the time taking is lesser when compared to tensorflow v2.12. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/75b1c4e1ae5cc00dcd757dfd9397f805/untitled2058.ipynb).

Also please try to use the latest tensorflow v2.17 and as this issue is more related to keras, raise the issue in keras-team/keras repo for the quick resolution. Thank you!

github-actions[bot] on (2024-08-17 01:51:47 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-24 01:53:18 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-24 01:53:21 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73362"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73362"">No</a>

"
2454246617,issue,closed,completed,HTML preview,Time,RubanChristian,2024-08-07 20:03:37+00:00,['Venkat6871'],2024-08-09 05:59:35+00:00,2024-08-09 05:59:35+00:00,https://github.com/tensorflow/tensorflow/issues/73326,[],"[{'comment_id': 2277206217, 'issue_id': 2454246617, 'author': 'Venkat6871', 'body': ""Please don't spam."", 'created_at': datetime.datetime(2024, 8, 9, 5, 59, 29, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-09 05:59:29 UTC): Please don't spam.

"
2453858746,issue,closed,completed,Tensorflow incompatible shapes.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using a dictionary to pass data into model.fit with the names of the input layers corresponding in the dictionary with data. However, for some reason, tensorflow is mixing up the data in category and amzData but if you look at the output from `input({layer: tf.convert_to_tensor(X_train[layer].to_list()).numpy().shape for layer in layer_names})` it shows that the data is correctly labelled. So why is tensorflow mixing up these two variables despite it being correctly correlated in the dictionary?

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
import keras
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from keras.api.utils import pad_sequences
import logging
import sys
import json

keras.config.disable_traceback_filtering()

root = logging.getLogger()
root.setLevel(logging.INFO)
logFormatter = logging.Formatter(fmt=""%(asctime)s-%(name)s-%(levelname)s:%(message)s"", 
    datefmt=""%X"")
fileHandler = logging.FileHandler(filename=""./data/logs/scraper.log"")
fileHandler.setFormatter(logFormatter)
root.addHandler(fileHandler)
root.addHandler(logging.StreamHandler(sys.stdout))

def create_ts_model(*args, **kwargs) -> tuple[keras.Layer, keras.Layer]:
    input = keras.layers.Input(shape=(None, 2), dtype=tf.float32, *args, **kwargs)
    x = keras.layers.LSTM(16, return_sequences=True)(input)
    x = keras.layers.Dropout(0.2)(x)
    x = keras.layers.LSTM(8, return_sequences=True)(x)
    x = keras.layers.Dropout(0.2)(x)
    x = keras.layers.GlobalAveragePooling1D()(x)

    return input, x


def get_compiled_model(catvocab: list) -> keras.Model:
    logging.info(""Checkpoint not found. Compiling new model..."")

    # Category
    category_feature = keras.Input(shape=(1,), dtype=tf.string, name=""category"")
    
    strlookup = keras.layers.StringLookup(output_mode='int')
    strlookup.adapt(catvocab)
    layer = strlookup(category_feature)

    embedding_dim = 8
    # Add 1 to include [OOV]
    category_output = keras.layers.Embedding(input_dim=len(catvocab) + 1, output_dim=embedding_dim)(layer)

    # buyBoxIsFBA & buyBoxIsAmazon
    bbinfo_feature = keras.Input(shape=(2,), dtype=tf.uint8, name=""bbinfo"")

    # All numeric features
    numinput_feature = keras.Input(shape=(10,), dtype=tf.float32, name=""numeric_inputs"")

    # Time series data
    fba_feature, fba_ts_output = create_ts_model(name=""fbaData"")
    amz_feature, amz_ts_output = create_ts_model(name=""amzData"")
    fbm_feature, fbm_ts_output = create_ts_model(name=""fbmData"")
    bb_feature, bb_ts_output = create_ts_model(name=""buyBoxData"")
    monthly_sold_feature, monthly_sold_ts_output = create_ts_model(name=""monthlySoldData"")
    sales_rank_feature, sales_rank_ts_output = create_ts_model(name=""salesRankData"")
    offer_count_feature, offer_count_ts_output = create_ts_model(name=""offerCountData"")
    review_count_feature, review_count_ts_output = create_ts_model(name=""reviewCountData"")
    rating_feature, rating_ts_output = create_ts_model(name=""ratingData"")

    x = keras.layers.Concatenate()([
        category_output, 
        bbinfo_feature, 
        numinput_feature,
        fba_ts_output,
        amz_ts_output,
        fbm_ts_output,
        bb_ts_output,
        monthly_sold_ts_output,
        sales_rank_ts_output,
        offer_count_ts_output,
        review_count_ts_output,
        rating_ts_output,
    ])

    x = keras.layers.Dense(128, activation='relu')(x)
    x = keras.layers.Dense(64, activation='relu')(x)

    score_pred = keras.layers.Dense(1, activation=""sigmoid"", name=""score"")(x)


    return keras.Model(
        inputs=[
            category_feature, 
            bbinfo_feature, 
            numinput_feature,
            fba_feature,
            amz_feature,
            fbm_feature,
            bb_feature,
            monthly_sold_feature,
            sales_rank_feature,
            offer_count_feature,
            review_count_feature,
            rating_feature
        ],
        outputs=[score_pred]
    )

logging.info(""Reading data"")
data = pd.read_json('./data/out/data_processed.jsonl', lines=True, nrows=10)

logging.info(""Processing features..."")


# For later: Masking layer should use value of zero since the min is -1 and this will scale -1 to 0
scalar = MinMaxScaler(feature_range=(0, 1))

# Scaling continuous features
features_to_scale = [
    'bsr',
    'buy_box',
    'estimated_sales',
    'category_product_count',
    'salesRankDrops30',
    'rating',
    'reviewCount',
    'offerCount'
    ]


for feature in features_to_scale:
    data[feature] = scalar.fit_transform(data[[feature]]).astype(np.float32)

# Scale each graph individually
graphs_to_scale = [
    ""fbaData"",
    ""amzData"",
    ""fbmData"",
    ""buyBoxData"",
    ""monthlySoldData"",
    ""salesRankData"",
    ""offerCountData"",
    ""reviewCountData"",
    ""ratingData""
]

for graph in graphs_to_scale:
    for row in data.T:
        if (data[graph][row]):
            data.at[row, graph] = scalar.fit_transform(data.at[row, graph]).astype(np.float32)
    data[graph] = list(pad_sequences(data[graph], padding='post'))


# Multi-hot encoding two boolean vectors
data['bbinfo'] = data.apply(lambda x: [int(x['buyBoxIsFBA']), int(x['buyBoxIsAmazon'])], axis=1)


# Creating numeric_inputs column (input to model)
data['numeric_inputs'] = data.apply(lambda row: [
    row['bsr'],
    row['buy_box'],
    row['estimated_sales'],
    row['category_product_count'],
    row['salesRankDrops30'],
    row['rating'],
    row['reviewCount'],
    row['offerCount'],
    row['bsr_rank'],
    row['estimated_sales_by_category_cnt']
    ], axis=1)

logging.info(""Creating model"")

model: keras.Model = get_compiled_model(data['category'])
keras.utils.plot_model(model, ""data/out/model.png"", show_shapes=True, show_layer_names=True)

model.compile(
    optimizer='adam',
    loss=keras.losses.MeanSquaredError,
    metrics=[keras.metrics.MeanSquaredError],

)




logging.info(""Training model"")

y = data.pop('score')
X_train, X_test, Y_train, Y_test = train_test_split(data, y, test_size=0.2)

callbacks = [
    keras.callbacks.TensorBoard(
        log_dir=""./data/logs/tb/"",
        histogram_freq=0,
        embeddings_freq=0,
        update_freq=""epoch"",
    ),
    keras.callbacks.ModelCheckpoint(
        filepath=""/data/models/model_{epoch}.keras"",
        save_best_only=True,
        monitor=""val_loss"",
        verbose=1,
    )
]

layer_names = [layer.name for layer in model.input]
# print({layer: X_train[layer] for layer in layer_names})
# for layer in layer_names:
#     print(layer)
#     print(len(X_train[layer][0]))
    # print(tf.convert_to_tensor(X_train[layer].to_list()))

input({layer: tf.convert_to_tensor(X_train[layer].to_list()).numpy().shape for layer in layer_names})

model.fit(
    {layer: tf.convert_to_tensor(X_train[layer].to_list()) for layer in layer_names},
    {'score': Y_train},
    epochs=35,
    batch_size=30,
    callbacks=callbacks
)

logging.info(""Model successfully trained... Saving model"")

model.save('./data/out/model.keras')

logging.info(""Evaluating model..."")

results = model.evaluate(
    {layer: tf.convert_to_tensor(X_train[layer].to_list()) for layer in layer_names},
    Y_test
)

logging.info(""Evaluation results: %s"" % results)
```


### Relevant log output

```shell
Reading data
Processing features...
Creating model
Checkpoint not found. Compiling new model...
2024-08-07 12:12:14.148288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Training model
{'category': (8,), 'bbinfo': (8, 2), 'numeric_inputs': (8, 10), 'fbaData': (8, 215, 2), 'amzData': (8, 481, 2), 'fbmData': (8, 177, 2), 'buyBoxData': (8, 848, 2), 'monthlySoldData': (8, 0), 'salesRankData': (8, 9685, 2), 'offerCountData': (8, 995, 2), 'reviewCountData': (8, 5435, 2), 'ratingData': (8, 250, 2)}
C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\callbacks\tensorboard.py:667: UserWarning: Model failed to serialize as JSON. Ignoring... MeanSquaredError.get_config() missing 1 required positional argument: 'self'
  warnings.warn(f""Model failed to serialize as JSON. Ignoring... {exc}"")
Epoch 1/35
Traceback (most recent call last):
  File ""c:\Users\austi\Downloads\code\py\solarisAIO\model.py"", line 211, in <module>
    model.fit(
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 113, in error_handler
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 320, in fit
    logs = self.train_function(iterator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 121, in one_step_on_iterator
    outputs = self.distribute_strategy.run(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 108, in one_step_on_data
    return self.train_step(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 51, in train_step
    y_pred = self(x, training=True)
             ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 113, in error_handler
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\layers\layer.py"", line 901, in __call__
    outputs = super().__call__(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 113, in error_handler
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\ops\operation.py"", line 54, in __call__
    return self.call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\models\functional.py"", line 167, in call
    inputs = self._standardize_inputs(inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\models\functional.py"", line 259, in _standardize_inputs
    return self._adjust_input_rank(flat_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\austi\Downloads\code\py\solarisAIO\venv\Lib\site-packages\keras\src\models\functional.py"", line 244, in _adjust_input_rank
    raise ValueError(
ValueError: Invalid input shape for input Tensor(""functional_1/Cast:0"", shape=(None, 481, 2), dtype=string). Expected shape (None, 1), but input has incompatible shape (None, 481, 2)
```


data/out/model.png:
![model](https://github.com/user-attachments/assets/d77ad098-af7c-4873-b83a-eb3058aea10b)
",TheWalkingSea,2024-08-07 16:14:31+00:00,['tilakrayal'],2025-01-03 02:55:20+00:00,2024-08-23 01:54:34+00:00,https://github.com/tensorflow/tensorflow/issues/73301,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2275649464, 'issue_id': 2453858746, 'author': 'tilakrayal', 'body': '@TheWalkingSea,\r\nI tried to execute the mentioned  code and faced a different error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/237696dd3a4ece173eefec5525eb1f71/untitled2050.ipynb) and also looks like this issue is more related to keras, so could you please raise a new issue in keras-team/keras repo for quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 8, 12, 1, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590344, 'issue_id': 2453858746, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 16, 1, 53, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306032863, 'issue_id': 2453858746, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 23, 1, 54, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306032937, 'issue_id': 2453858746, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73301"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73301"">No</a>', 'created_at': datetime.datetime(2024, 8, 23, 1, 54, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568633544, 'issue_id': 2453858746, 'author': 'CharlesAverill', 'body': '@TheWalkingSea did this ever get fixed?', 'created_at': datetime.datetime(2025, 1, 3, 2, 55, 19, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-08 12:01:52 UTC): @TheWalkingSea,
I tried to execute the mentioned  code and faced a different error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/237696dd3a4ece173eefec5525eb1f71/untitled2050.ipynb) and also looks like this issue is more related to keras, so could you please raise a new issue in keras-team/keras repo for quick resolution. Thank you!

github-actions[bot] on (2024-08-16 01:53:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-23 01:54:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-23 01:54:36 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73301"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73301"">No</a>

CharlesAverill on (2025-01-03 02:55:19 UTC): @TheWalkingSea did this ever get fixed?

"
2453589498,issue,closed,completed,TensorBuffer does not support data type: INT32,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Windows 11 x64

### Mobile device

Andorid Studio virtual device: Medium Phone

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My app is crashing with the error: TensorBuffer does not support data type: INT32.
I expected it to work normally and not to crash.

### Standalone code to reproduce the issue

```java

package com.example.bert_app;

import android.os.Bundle;
import android.util.Log;
import androidx.appcompat.app.AppCompatActivity;
import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.support.common.FileUtil;
import org.tensorflow.lite.support.tensorbuffer.TensorBuffer;
import org.tensorflow.lite.DataType;
import com.example.bert_app.tokenizerimpl.BertTokenizer;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;

public class MainActivity extends AppCompatActivity {
    private static final String TAG = ""MainActivity"";
    private static final String MODEL_PATH = ""bert_ner_model.tflite"";
    private static final int MAX_SEQ_LENGTH = 512;

    private Interpreter tfliteInterpreter;
    private BertTokenizer tokenizer;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        // tokenizer
        tokenizer = new BertTokenizer(this, ""vocab.txt"", true, true, new ArrayList<>(),
                ""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"", true);

        try {
            tfliteInterpreter = new Interpreter(FileUtil.loadMappedFile(this, MODEL_PATH));
        } catch (Exception e) {
            Log.e(TAG, ""Error loading model"", e);
        }

        
        String inputText = ""I am Wolfgang and I live in Berlin."";
        List<String> tokenizedInput = tokenizer.tokenize(inputText);
        List<Integer> inputIds = tokenizer.convert_tokens_to_ids(tokenizedInput);

        while (inputIds.size() < MAX_SEQ_LENGTH) {
            inputIds.add(tokenizer.convert_tokens_to_ids(Collections.singletonList(""[PAD]"")).get(0));
        }
        if (inputIds.size() > MAX_SEQ_LENGTH) {
            inputIds = inputIds.subList(0, MAX_SEQ_LENGTH);
        }

        List<Integer> segmentIds = new ArrayList<>(Collections.nCopies(MAX_SEQ_LENGTH, 0));

        List<Integer> attentionMask = new ArrayList<>();
        for (int i = 0; i < MAX_SEQ_LENGTH; i++) {
            attentionMask.add(Objects.equals(inputIds.get(i), tokenizer.convert_tokens_to_ids(Collections.singletonList(""[PAD]"")).get(0)) ? 0 : 1);
        }

        runInference(inputIds, attentionMask, segmentIds);
    }

    private void runInference(List<Integer> inputIds, List<Integer> attentionMask, List<Integer> segmentIds) {
        int[] inputIdsArray = listToIntArray(inputIds);
        int[] attentionMaskArray = listToIntArray(attentionMask);
        int[] segmentIdsArray = listToIntArray(segmentIds);

        for (int i = 0; i < tfliteInterpreter.getInputTensorCount(); i++) {
            Log.d(TAG, ""Input Tensor "" + i + "": "" + tfliteInterpreter.getInputTensor(i).dataType());
        }
        for (int i = 0; i < tfliteInterpreter.getOutputTensorCount(); i++) {
            Log.d(TAG, ""Output Tensor "" + i + "": "" + tfliteInterpreter.getOutputTensor(i).dataType());
        }

        TensorBuffer inputIdsBuffer = TensorBuffer.createFixedSize(new int[]{1, MAX_SEQ_LENGTH}, DataType.INT32);
        inputIdsBuffer.loadArray(inputIdsArray);

        TensorBuffer attentionMaskBuffer = TensorBuffer.createFixedSize(new int[]{1, MAX_SEQ_LENGTH}, DataType.INT32);
        attentionMaskBuffer.loadArray(attentionMaskArray);

        TensorBuffer segmentIdsBuffer = TensorBuffer.createFixedSize(new int[]{1, MAX_SEQ_LENGTH}, DataType.INT32);
        segmentIdsBuffer.loadArray(segmentIdsArray);

        TensorBuffer outputBuffer = TensorBuffer.createFixedSize(new int[]{1, MAX_SEQ_LENGTH, 2}, DataType.FLOAT32);  // Assuming output is of shape [1, MAX_SEQ_LENGTH, 2]

        Object[] inputs = {inputIdsBuffer.getBuffer(), attentionMaskBuffer.getBuffer(), segmentIdsBuffer.getBuffer()};
        Map<Integer, Object> outputs = new HashMap<>();
        outputs.put(0, outputBuffer.getBuffer());
        

        tfliteInterpreter.runForMultipleInputsOutputs(inputs, outputs);

        //  output
        float[] outputArray = outputBuffer.getFloatArray();
        processOutput(outputArray);
    }

    private int[] listToIntArray(List<Integer> list) {
        int[] array = new int[list.size()];
        for (int i = 0; i < list.size(); i++) {
            array[i] = list.get(i);
        }
        return array;
    }

    private void processOutput(float[] outputArray) {
        for (float value : outputArray) {
            Log.d(TAG, ""Output: "" + value);
        }
    }
}
```


### Relevant log output

```shell
024-08-07 16:03:11.485  8162-8162  MainActivity            com.example.bert_app                 D  Input Tensor 0: INT32
2024-08-07 16:03:11.485  8162-8162  MainActivity            com.example.bert_app                 D  Input Tensor 1: INT32
2024-08-07 16:03:11.485  8162-8162  MainActivity            com.example.bert_app                 D  Input Tensor 2: INT32
2024-08-07 16:03:11.485  8162-8162  MainActivity            com.example.bert_app                 D  Output Tensor 0: FLOAT32
2024-08-07 16:03:11.489  8162-8162  AndroidRuntime          com.example.bert_app                 D  Shutting down VM
2024-08-07 16:03:11.491  8162-8162  AndroidRuntime          com.example.bert_app                 E  FATAL EXCEPTION: main (Ask Gemini)
                                                                                                    Process: com.example.bert_app, PID: 8162
                                                                                                    java.lang.AssertionError: TensorBuffer does not support data type: INT32
                                                                                                    	at org.tensorflow.lite.support.tensorbuffer.TensorBuffer.createFixedSize(TensorBuffer.java:82)
                                                                                                    	at com.example.bert_app.MainActivity.runInference(MainActivity.java:85)
                                                                                                    	at com.example.bert_app.MainActivity.onCreate(MainActivity.java:68)
                                                                                                    	at android.app.Activity.performCreate(Activity.java:8980)
                                                                                                    	at android.app.Activity.performCreate(Activity.java:8958)
                                                                                                    	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1526)
                                                                                                    	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:4029)
                                                                                                    	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4234)
                                                                                                    	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:112)
                                                                                                    	at android.app.servertransaction.TransactionExecutor.executeNonLifecycleItem(TransactionExecutor.java:174)
                                                                                                    	at android.app.servertransaction.TransactionExecutor.executeTransactionItems(TransactionExecutor.java:109)
                                                                                                    	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:81)
                                                                                                    	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2635)
                                                                                                    	at android.os.Handler.dispatchMessage(Handler.java:107)
                                                                                                    	at android.os.Looper.loopOnce(Looper.java:232)
                                                                                                    	at android.os.Looper.loop(Looper.java:317)
                                                                                                    	at android.app.ActivityThread.main(ActivityThread.java:8699)
                                                                                                    	at java.lang.reflect.Method.invoke(Native Method)
                                                                                                    	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:580)
                                                                                                    	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:886)
2024-08-07 16:03:11.502  8162-8162  Process                 com.example.bert_app                 I  Sending signal. PID: 8162 SIG: 9
```
",RaoufiTech,2024-08-07 14:10:01+00:00,['Venkat6871'],2024-09-16 05:54:21+00:00,2024-08-24 01:53:20+00:00,https://github.com/tensorflow/tensorflow/issues/73295,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2275338157, 'issue_id': 2453589498, 'author': 'sawantkumar', 'body': 'Hi @AiTester950 ,\r\n\r\nBy looking at the error  \r\n```\r\nprocess: com.example.bert_app, PID: 8162\r\n java.lang.AssertionError: TensorBuffer does not support data type: INT32  \r\n```\r\nit looks like DataType.INT32 is not supported by TensorBuffer , can you use DataType.FLOAT32  instead and let me know if it worked for you.', 'created_at': datetime.datetime(2024, 8, 8, 9, 13, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275355941, 'issue_id': 2453589498, 'author': 'RaoufiTech', 'body': 'Hey\r\nFLOAT32 works but the ai model that I am working with needs INT32...', 'created_at': datetime.datetime(2024, 8, 8, 9, 22, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277247953, 'issue_id': 2453589498, 'author': 'sawantkumar', 'body': 'Hi @AiTester950 ,\r\n\r\nYou will need to use Bytebuffer to accomplish your task . However the output can still handled as a FLOAT32 TensorBuffer .\r\nPlease take a look at [this](https://developer.android.com/reference/java/nio/ByteBuffer)', 'created_at': datetime.datetime(2024, 8, 9, 6, 38, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527162, 'issue_id': 2453589498, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987449, 'issue_id': 2453589498, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987496, 'issue_id': 2453589498, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73295"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73295"">No</a>', 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 24, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-08-08 09:13:23 UTC): Hi @AiTester950 ,

By looking at the error  
```
process: com.example.bert_app, PID: 8162
 java.lang.AssertionError: TensorBuffer does not support data type: INT32  
```
it looks like DataType.INT32 is not supported by TensorBuffer , can you use DataType.FLOAT32  instead and let me know if it worked for you.

RaoufiTech (Issue Creator) on (2024-08-08 09:22:10 UTC): Hey
FLOAT32 works but the ai model that I am working with needs INT32...

sawantkumar on (2024-08-09 06:38:02 UTC): Hi @AiTester950 ,

You will need to use Bytebuffer to accomplish your task . However the output can still handled as a FLOAT32 TensorBuffer .
Please take a look at [this](https://developer.android.com/reference/java/nio/ByteBuffer)

github-actions[bot] on (2024-08-17 01:51:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-24 01:53:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-24 01:53:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73295"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73295"">No</a>

"
2453326184,issue,open,,[BUG] Optimizer crash on TPU: `AttributeError: 'NoneType' object has no attribute 'extended'`,"### Issue type: Bug

System info:

- Kaggle TPU VM v3-8
- Python 3.10
- TensorFlow 2.16.1

### Standalone code to reproduce the issue

```python
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    tpu_strategy = tf.distribute.TPUStrategy(tpu)
    print(""TPU setup successful"")
except (ValueError, ImportError) as e:
    tpu_strategy = tf.distribute.get_strategy()

class BertSLPModel(tf.keras.Model):
    def __init__(self):
        super(BertSLPModel, self).__init__()
        self.bert = bert
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.classifier = tf.keras.layers.Dense(num_classes)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        bert_output = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = bert_output.pooler_output
        dropout_output = self.dropout(pooled_output)
        logit = self.classifier(dropout_output)
        return logit

with tpu_strategy.scope():
    model = BertSLPModel()
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
    )
    model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=epochs,
        batch_size=batch_size
    )
```


### Relevant log output

```shell
AttributeError: in user code:

    File ""/tf_keras/src/engine/training.py"", line 1398, in train_function  *
        return step_function(self, iterator)
    File ""/tf_keras/src/engine/training.py"", line 1370, in run_step  *
        outputs = model.train_step(data)
    File ""/tf_keras/src/engine/training.py"", line 1151, in train_step  *
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 621, in minimize  *
        self.apply_gradients(grads_and_vars)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1300, in apply_gradients  *
        return super().apply_gradients(grads_and_vars, name=name)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 715, in apply_gradients  *
        self.build(trainable_variables)
    File ""/tf_keras/src/optimizers/rmsprop.py"", line 121, in build  *
        self._velocities.append(
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1201, in add_variable_from_reference  *
        with strategy.extended.colocate_vars_with(model_variable):

    AttributeError: 'NoneType' object has no attribute 'extended'
```",steveepreston,2024-08-07 12:09:34+00:00,['tilakrayal'],2024-08-15 04:32:14+00:00,,https://github.com/tensorflow/tensorflow/issues/73288,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:tpus', 'tpu, tpuestimator'), ('TF 2.16', '')]","[{'comment_id': 2273383848, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'a related notebook with this error:\r\n\r\nhttps://www.kaggle.com/code/xiefei/deep-learning-for-nlp-zero-to-transformers-bert', 'created_at': datetime.datetime(2024, 8, 7, 12, 43, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273392780, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'a related issue on other repo:\r\nhttps://github.com/tensorflow/recommenders-addons/issues/182', 'created_at': datetime.datetime(2024, 8, 7, 12, 47, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273403469, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'please take a look to this bug if you can. Thanks\r\n@tilakrayal @sawantkumar @pkgoogle @Venkat6871 @SuryanarayanaY', 'created_at': datetime.datetime(2024, 8, 7, 12, 53, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273430672, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'note: changed optimizer to Adam and AdamW, they also produce similar error.', 'created_at': datetime.datetime(2024, 8, 7, 13, 6, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275758502, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'a related issue on huggingface discuss:\r\nhttps://discuss.huggingface.co/t/tfvit-model-keeps-throwing-error-while-training-it-using-tftrainer/44411', 'created_at': datetime.datetime(2024, 8, 8, 12, 56, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276549517, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'Update:\r\ni used this ready model:\r\n`model = TFBertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)`\r\ninstead of model defined in the code:\r\n`model = BertSLPModel()`\r\nand error has gone.\r\n\r\nso i think there is bug in processing defined model.', 'created_at': datetime.datetime(2024, 8, 8, 19, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285399743, 'issue_id': 2453326184, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4665d789dc148632bbab81b55bc4084d/deep-learning-for-nlp-zero-to-transformers-bert.ipynb).', 'created_at': datetime.datetime(2024, 8, 13, 6, 1, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2290556720, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': ""@tilakrayal thank you for your attention. I don't know why this is happening and how it should be fixed."", 'created_at': datetime.datetime(2024, 8, 15, 4, 26, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2290566292, 'issue_id': 2453326184, 'author': 'steveepreston', 'body': 'I confused. As occurring in 2.17, it means tf-tpu is dead and nobody uses it?', 'created_at': datetime.datetime(2024, 8, 15, 4, 30, 52, tzinfo=datetime.timezone.utc)}]","steveepreston (Issue Creator) on (2024-08-07 12:43:15 UTC): a related notebook with this error:

https://www.kaggle.com/code/xiefei/deep-learning-for-nlp-zero-to-transformers-bert

steveepreston (Issue Creator) on (2024-08-07 12:47:52 UTC): a related issue on other repo:
https://github.com/tensorflow/recommenders-addons/issues/182

steveepreston (Issue Creator) on (2024-08-07 12:53:12 UTC): please take a look to this bug if you can. Thanks
@tilakrayal @sawantkumar @pkgoogle @Venkat6871 @SuryanarayanaY

steveepreston (Issue Creator) on (2024-08-07 13:06:29 UTC): note: changed optimizer to Adam and AdamW, they also produce similar error.

steveepreston (Issue Creator) on (2024-08-08 12:56:43 UTC): a related issue on huggingface discuss:
https://discuss.huggingface.co/t/tfvit-model-keeps-throwing-error-while-training-it-using-tftrainer/44411

steveepreston (Issue Creator) on (2024-08-08 19:55:00 UTC): Update:
i used this ready model:
`model = TFBertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)`
instead of model defined in the code:
`model = BertSLPModel()`
and error has gone.

so i think there is bug in processing defined model.

tilakrayal (Assginee) on (2024-08-13 06:01:54 UTC): I was able to reproduce the issue on tensorflow v2.17, tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4665d789dc148632bbab81b55bc4084d/deep-learning-for-nlp-zero-to-transformers-bert.ipynb).

steveepreston (Issue Creator) on (2024-08-15 04:26:41 UTC): @tilakrayal thank you for your attention. I don't know why this is happening and how it should be fixed.

steveepreston (Issue Creator) on (2024-08-15 04:30:52 UTC): I confused. As occurring in 2.17, it means tf-tpu is dead and nobody uses it?

"
2451833759,issue,closed,not_planned,[RNN] TFLite does not appear to be using the UnidirectionalSequenceLSTM,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installation (pip package or built from source): from pip
- TensorFlow library (version, if pip package or github SHA, if built from source): keras-nightly-3.4.1.dev2024080603 tb-nightly-2.18.0a20240806 tf-nightly-2.18.0.dev20240806

### 2. Code

1) https://colab.research.google.com/gist/eric/f00f071e527f9fa7b2ed39f8d482fbb4/tensorflow-datasets.ipynb
2) https://colab.research.google.com/gist/eric/a292799568831371b7686a2b8cefcd0b/tensorflow-lite-debugger-colab.ipynb

[model.zip](https://github.com/user-attachments/files/16516496/model.zip)

### 3. Failure after conversion

I've found that any Keras LSTM causes this issue.

I expected TFLite to use the `UnidirectionalSequenceLSTM` op, but instead it seems to be doing something else that then requires the use of flex ops, which I would like to avoid trying to get to work with my tflite deployment situation.
",eric,2024-08-06 23:15:11+00:00,"['zichuan-wei', 'pkgoogle', 'sawantkumar']",2024-11-26 18:06:43+00:00,2024-11-26 18:06:38+00:00,https://github.com/tensorflow/tensorflow/issues/73254,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2287378080, 'issue_id': 2451833759, 'author': 'eric', 'body': 'Is there anything I can do to tweak my model to allow it to work without the flex ops?', 'created_at': datetime.datetime(2024, 8, 13, 23, 23, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332446414, 'issue_id': 2451833759, 'author': 'pkgoogle', 'body': 'Hi @eric, using only TFLite BuiltIn Ops, will do this but that can be quite constraining. For most LSTM use cases we highly recommend upgrading to Transformers and using PyTorch and [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) and the [Generative API](https://github.com/google-ai-edge/ai-edge-torch?tab=readme-ov-file#generative-api) to convert to .tflite models. Does that work for you?', 'created_at': datetime.datetime(2024, 9, 5, 19, 5, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344637799, 'issue_id': 2451833759, 'author': 'eric', 'body': ""So far I've found that using an LSTM has been a good trade-off for performance and accuracy, so it would be nice if they could work again. Is there a current working example that shows how to use it with tensorflow?"", 'created_at': datetime.datetime(2024, 9, 11, 20, 26, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344811224, 'issue_id': 2451833759, 'author': 'pkgoogle', 'body': ""Hi @eric, with raw tensorflow w/o Keras, it does not seem like there is an easy way to create an LSTM. It was sunsetted awhile ago ... I did find a raw_ops for an LSTM Cell: https://www.tensorflow.org/api_docs/python/tf/raw_ops/BlockLSTMV2. I don't believe you will want to reconstruct an LSTM with more primitive ops w/o Keras but that is one possibility."", 'created_at': datetime.datetime(2024, 9, 11, 22, 25, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345017709, 'issue_id': 2451833759, 'author': 'eric', 'body': 'Is there a way to do it with Keras? I am using Keras so that would  be fine.', 'created_at': datetime.datetime(2024, 9, 12, 0, 23, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346991481, 'issue_id': 2451833759, 'author': 'pkgoogle', 'body': 'Hi @eric, my current attempts seems to be running into a prior bug ... would you be willing to use [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) instead?\r\n\r\nI was able to convert an lstm that way:\r\n```py\r\nimport ai_edge_torch\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nlstm = nn.LSTM(10, 20, 2)\r\nh0 = torch.randn(2, 3, 20)\r\nc0 = torch.randn(2, 3, 20)\r\n\r\nsample_input = (torch.randn(5, 3, 10), (h0, c0))\r\n\r\nedge_model = ai_edge_torch.convert(lstm.eval(), sample_input)\r\nedge_model.export(""lstm.tflite"")\r\n```', 'created_at': datetime.datetime(2024, 9, 12, 18, 40, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347032361, 'issue_id': 2451833759, 'author': 'eric', 'body': ""I've been using keras-tcn for other parts of my model and found that it performed better than LSTM, so I would have to evaluate the pytorch landscape to see if it would work for me."", 'created_at': datetime.datetime(2024, 9, 12, 19, 2, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2353994889, 'issue_id': 2451833759, 'author': 'pkgoogle', 'body': ""Understood, let us know if for some reason that workflow doesn't work. You are able to reopen the issue in the future."", 'created_at': datetime.datetime(2024, 9, 16, 20, 43, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354444678, 'issue_id': 2451833759, 'author': 'eric', 'body': 'Is there any hope of fixing the bug in Keras that is preventing it from working? It would be nice to not have to rewrite everything.', 'created_at': datetime.datetime(2024, 9, 17, 3, 43, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356650988, 'issue_id': 2451833759, 'author': 'pkgoogle', 'body': ""Hi @eric, we can turn this issue into that issue for now, if that is preferred. Here's a gist showing the issue with tf-nightly (I have attempted on previous versions of tf up to and including 2.15) [gist](https://colab.sandbox.google.com/gist/pkgoogle/71316c19106db05fc455913dea6cf366/73254.ipynb)\r\n\r\nHi @zichuan-wei, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 9, 17, 18, 45, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356653386, 'issue_id': 2451833759, 'author': 'eric', 'body': 'That would be great. Thanks!', 'created_at': datetime.datetime(2024, 9, 17, 18, 47, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500173910, 'issue_id': 2451833759, 'author': 'gaikwadrahul8', 'body': ""Hi, @eric \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/41\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 0, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501615200, 'issue_id': 2451833759, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73254"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73254"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 6, 40, tzinfo=datetime.timezone.utc)}]","eric (Issue Creator) on (2024-08-13 23:23:37 UTC): Is there anything I can do to tweak my model to allow it to work without the flex ops?

pkgoogle (Assginee) on (2024-09-05 19:05:22 UTC): Hi @eric, using only TFLite BuiltIn Ops, will do this but that can be quite constraining. For most LSTM use cases we highly recommend upgrading to Transformers and using PyTorch and [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) and the [Generative API](https://github.com/google-ai-edge/ai-edge-torch?tab=readme-ov-file#generative-api) to convert to .tflite models. Does that work for you?

eric (Issue Creator) on (2024-09-11 20:26:13 UTC): So far I've found that using an LSTM has been a good trade-off for performance and accuracy, so it would be nice if they could work again. Is there a current working example that shows how to use it with tensorflow?

pkgoogle (Assginee) on (2024-09-11 22:25:02 UTC): Hi @eric, with raw tensorflow w/o Keras, it does not seem like there is an easy way to create an LSTM. It was sunsetted awhile ago ... I did find a raw_ops for an LSTM Cell: https://www.tensorflow.org/api_docs/python/tf/raw_ops/BlockLSTMV2. I don't believe you will want to reconstruct an LSTM with more primitive ops w/o Keras but that is one possibility.

eric (Issue Creator) on (2024-09-12 00:23:05 UTC): Is there a way to do it with Keras? I am using Keras so that would  be fine.

pkgoogle (Assginee) on (2024-09-12 18:40:24 UTC): Hi @eric, my current attempts seems to be running into a prior bug ... would you be willing to use [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) instead?

I was able to convert an lstm that way:
```py
import ai_edge_torch
import torch
import torch.nn as nn


lstm = nn.LSTM(10, 20, 2)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)

sample_input = (torch.randn(5, 3, 10), (h0, c0))

edge_model = ai_edge_torch.convert(lstm.eval(), sample_input)
edge_model.export(""lstm.tflite"")
```

eric (Issue Creator) on (2024-09-12 19:02:22 UTC): I've been using keras-tcn for other parts of my model and found that it performed better than LSTM, so I would have to evaluate the pytorch landscape to see if it would work for me.

pkgoogle (Assginee) on (2024-09-16 20:43:51 UTC): Understood, let us know if for some reason that workflow doesn't work. You are able to reopen the issue in the future.

eric (Issue Creator) on (2024-09-17 03:43:32 UTC): Is there any hope of fixing the bug in Keras that is preventing it from working? It would be nice to not have to rewrite everything.

pkgoogle (Assginee) on (2024-09-17 18:45:58 UTC): Hi @eric, we can turn this issue into that issue for now, if that is preferred. Here's a gist showing the issue with tf-nightly (I have attempted on previous versions of tf up to and including 2.15) [gist](https://colab.sandbox.google.com/gist/pkgoogle/71316c19106db05fc455913dea6cf366/73254.ipynb)

Hi @zichuan-wei, can you please take a look? Thanks.

eric (Issue Creator) on (2024-09-17 18:47:18 UTC): That would be great. Thanks!

gaikwadrahul8 on (2024-11-26 10:00:07 UTC): Hi, @eric 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/41

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:06:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73254"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73254"">No</a>

"
2451196022,issue,closed,completed,Failed to load the native TensorFlow runtime.,"ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\app.py"", line 12, in <module>
    from detection.face_matching import detect_faces, align_face
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\detection\__init__.py"", line 1, in <module>
    from .face_matching import *
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\detection\face_matching.py"", line 4, in <module>
    from deepface import DeepFace
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\deepface\DeepFace.py"", line 15, in <module>
    import tensorflow as tf
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
PS C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main> python app.py 
Traceback (most recent call last):
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\app.py"", line 12, in <module>
    from detection.face_matching import detect_faces, align_face
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\detection\__init__.py"", line 1, in <module>
    from .face_matching import *
  File ""C:\Users\joshua\Desktop\Intelligent-Face-Recognition-Attendance-System-main\detection\face_matching.py"", line 4, in <module>
    from deepface import DeepFace
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\deepface\DeepFace.py"", line 15, in <module>
    import tensorflow as tf
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\joshua\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",joshuaemmanuel30,2024-08-06 15:43:18+00:00,['tilakrayal'],2024-08-23 01:54:40+00:00,2024-08-23 01:54:36+00:00,https://github.com/tensorflow/tensorflow/issues/73220,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2272010516, 'issue_id': 2451196022, 'author': 'AdvaitDongre', 'body': 'so here are a couple of my suggestions, you can try and see if it works: \r\n1. As of now, TensorFlow does not officially support Python 3.10. You may need to downgrade your Python version to 3.8 or 3.9.\r\n2. try installing tensorflow-cpu it generally solves DLL issues:\r\n```\r\npip install tensorflow-cpu\r\n```\r\n3. check the installation of tensorflow\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n\r\n\r\nlet me know if this fixes your issues or what problem you face next', 'created_at': datetime.datetime(2024, 8, 6, 19, 37, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272967440, 'issue_id': 2451196022, 'author': 'tilakrayal', 'body': '@joshuaemmanuel30,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 7, 8, 55, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274514089, 'issue_id': 2451196022, 'author': 'socampo17', 'body': '@tilakrayal  if my processor does not support avx2 instructions, it means that i wont be able to use tf? thank you', 'created_at': datetime.datetime(2024, 8, 7, 23, 29, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275663590, 'issue_id': 2451196022, 'author': 'tilakrayal', 'body': '@socampo17,\r\nPlease take a look at the hardware requirements from the official [document](https://www.tensorflow.org/install/pip#hardware_requirements) and try to follow the same for the smoother installation. Thank you!', 'created_at': datetime.datetime(2024, 8, 8, 12, 9, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590360, 'issue_id': 2451196022, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 16, 1, 53, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306032932, 'issue_id': 2451196022, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 23, 1, 54, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306033009, 'issue_id': 2451196022, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73220"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73220"">No</a>', 'created_at': datetime.datetime(2024, 8, 23, 1, 54, 39, tzinfo=datetime.timezone.utc)}]","AdvaitDongre on (2024-08-06 19:37:32 UTC): so here are a couple of my suggestions, you can try and see if it works: 
1. As of now, TensorFlow does not officially support Python 3.10. You may need to downgrade your Python version to 3.8 or 3.9.
2. try installing tensorflow-cpu it generally solves DLL issues:
```
pip install tensorflow-cpu
```
3. check the installation of tensorflow
```
import tensorflow as tf
print(tf.__version__)
```


let me know if this fixes your issues or what problem you face next

tilakrayal (Assginee) on (2024-08-07 08:55:57 UTC): @joshuaemmanuel30,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

socampo17 on (2024-08-07 23:29:04 UTC): @tilakrayal  if my processor does not support avx2 instructions, it means that i wont be able to use tf? thank you

tilakrayal (Assginee) on (2024-08-08 12:09:09 UTC): @socampo17,
Please take a look at the hardware requirements from the official [document](https://www.tensorflow.org/install/pip#hardware_requirements) and try to follow the same for the smoother installation. Thank you!

github-actions[bot] on (2024-08-16 01:53:58 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-23 01:54:36 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-23 01:54:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73220"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73220"">No</a>

"
2450192670,issue,closed,completed,OpenGL error in Raspberry Pi 4B with tensorflow lite c++,"**System information**
-  Raspberry Pi 4B 8GB 
- Tensorflow Lite v2.9.3
- OpenGL 3.1

**Standalone code to reproduce the issue**

I insert delegate code in [example code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) in Raspberry Pi 4B.
I also execute `cmake .. -DTFLITE_ENABLE_GPU=ON` on examples.
It can not find openGL in compile and occurs errors like below. 
But OpenGL run well in other application.
How Can I use OpenGL with tensorflow lite in Raspberry Pi 4?


**Any other info / logs**

INFO: Created TensorFlow Lite delegate for GPU.
ERROR: clGetPlatformIDs returned -1001
ERROR: Falling back to OpenGL
ERROR: TfLiteGpuDelegate Init: OpenGL-based API disabled
INFO: Created 0 GPU delegate kernels.
ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized
ERROR: Node number 1 (TfLiteGpuDelegateV2) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Error at /home/user/workspace/minimal.cc:94
",ww5862,2024-08-06 07:48:34+00:00,"['pkgoogle', 'sawantkumar']",2024-08-27 01:55:55+00:00,2024-08-27 01:55:54+00:00,https://github.com/tensorflow/tensorflow/issues/73194,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2276400119, 'issue_id': 2450192670, 'author': 'pkgoogle', 'body': 'Hi @ww5862, have you tried the minimal example without any changes first? What happens in that case? Also please share your changes to the example code and the exact commands before your cmake command as well as the full cmake command. Thanks.', 'created_at': datetime.datetime(2024, 8, 8, 18, 15, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283067001, 'issue_id': 2450192670, 'author': 'ww5862', 'body': 'Thank you for reply.\r\nI solve this problem with compile with bazel instead of cmake.\r\nThank you.', 'created_at': datetime.datetime(2024, 8, 12, 3, 49, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284632723, 'issue_id': 2450192670, 'author': 'pkgoogle', 'body': 'Hi @ww5862, awesome thanks -- if you have no more open items, please feel free to close the issue.', 'created_at': datetime.datetime(2024, 8, 12, 18, 12, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297815859, 'issue_id': 2450192670, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 20, 1, 53, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419558, 'issue_id': 2450192670, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 54, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-08 18:15:42 UTC): Hi @ww5862, have you tried the minimal example without any changes first? What happens in that case? Also please share your changes to the example code and the exact commands before your cmake command as well as the full cmake command. Thanks.

ww5862 (Issue Creator) on (2024-08-12 03:49:33 UTC): Thank you for reply.
I solve this problem with compile with bazel instead of cmake.
Thank you.

pkgoogle (Assginee) on (2024-08-12 18:12:35 UTC): Hi @ww5862, awesome thanks -- if you have no more open items, please feel free to close the issue.

github-actions[bot] on (2024-08-20 01:53:49 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-27 01:55:54 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2449586175,issue,open,,Presubmit checks for Windows Failing,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.17

### Custom code

Yes

### OS platform and distribution

Windows 10/11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Windows Presubmit seems to be failing for every commit. 

![image](https://github.com/user-attachments/assets/c67d6fb7-19ac-4680-bab4-20221f4a31f2)


### Standalone code to reproduce the issue

```shell
I am unable to see the log or the command running which is Google internal. Whenever I try to go to the details, it gives me a permission denied error.
```


### Relevant log output

_No response_",mraunak,2024-08-05 22:33:02+00:00,['belitskiy'],2024-08-09 16:43:26+00:00,,https://github.com/tensorflow/tensorflow/issues/73175,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype:windows', 'Windows Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2270142201, 'issue_id': 2449586175, 'author': 'ddunl', 'body': ""Just as an FYI that may need further investigating, there are also some non-windows builds which seem to consistently fail. I see them on commits to main here: https://github.com/tensorflow/tensorflow/commits/master/. I haven't looked too closely at all of them but just wanted to point out that this problem is not limited to Windows builds (though it may be out of scope for this issue)."", 'created_at': datetime.datetime(2024, 8, 6, 0, 24, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278241225, 'issue_id': 2449586175, 'author': 'belitskiy', 'body': ""Thanks, Mayank\r\n\r\nThe Windows ones were deleted.\r\n\r\n@ddunl I know there's also the AMD ROCm presubmit that's been failing a while. I believe that runs on AMD CI, so we don't have as much control over it. I think @MichaelHudgins probably reached out to them about it some time back."", 'created_at': datetime.datetime(2024, 8, 9, 15, 44, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278343287, 'issue_id': 2449586175, 'author': 'belitskiy', 'body': ""Oh, never mind, I see the other Windows builds. I think those can also be deleted at this point. I'll look into them."", 'created_at': datetime.datetime(2024, 8, 9, 16, 43, 25, tzinfo=datetime.timezone.utc)}]","ddunl on (2024-08-06 00:24:36 UTC): Just as an FYI that may need further investigating, there are also some non-windows builds which seem to consistently fail. I see them on commits to main here: https://github.com/tensorflow/tensorflow/commits/master/. I haven't looked too closely at all of them but just wanted to point out that this problem is not limited to Windows builds (though it may be out of scope for this issue).

belitskiy (Assginee) on (2024-08-09 15:44:54 UTC): Thanks, Mayank

The Windows ones were deleted.

@ddunl I know there's also the AMD ROCm presubmit that's been failing a while. I believe that runs on AMD CI, so we don't have as much control over it. I think @MichaelHudgins probably reached out to them about it some time back.

belitskiy (Assginee) on (2024-08-09 16:43:25 UTC): Oh, never mind, I see the other Windows builds. I think those can also be deleted at this point. I'll look into them.

"
2449577827,issue,closed,completed,Impossible to build with Python3.12,"Using latest Ubuntu (24.04 with Python3.12), the following command fails:
```
bazel build ...
```

With this error:
```
AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?
```

This is because `rules_python` is outdated.",LucasChollet,2024-08-05 22:24:59+00:00,['Venkat6871'],2024-08-12 22:01:45+00:00,2024-08-12 22:01:43+00:00,https://github.com/tensorflow/tensorflow/issues/73174,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:micro', 'Related to TensorFlow Lite Microcontrollers')]","[{'comment_id': 2270027048, 'issue_id': 2449577827, 'author': 'LucasChollet', 'body': 'Wait, I clicked on the link in the tflite-micro repository, not sure is it supposed to end up in this repo :thinking:.', 'created_at': datetime.datetime(2024, 8, 5, 22, 29, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272645733, 'issue_id': 2449577827, 'author': 'Venkat6871', 'body': 'Hi **@LucasChollet** ,\r\n- Please verify your project on the latest build once your [PR](https://github.com/tensorflow/tflite-micro/pull/2654) is getting merged. Please let us know the response.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 7, 5, 27, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284978050, 'issue_id': 2449577827, 'author': 'LucasChollet', 'body': ""As mentioned in the PR, it still fails to build later on. But it's now clear why you shouldn't build with Python 3.12."", 'created_at': datetime.datetime(2024, 8, 12, 22, 1, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284978109, 'issue_id': 2449577827, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73174"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73174"">No</a>', 'created_at': datetime.datetime(2024, 8, 12, 22, 1, 44, tzinfo=datetime.timezone.utc)}]","LucasChollet (Issue Creator) on (2024-08-05 22:29:06 UTC): Wait, I clicked on the link in the tflite-micro repository, not sure is it supposed to end up in this repo :thinking:.

Venkat6871 (Assginee) on (2024-08-07 05:27:17 UTC): Hi **@LucasChollet** ,
- Please verify your project on the latest build once your [PR](https://github.com/tensorflow/tflite-micro/pull/2654) is getting merged. Please let us know the response.

Thank you!

LucasChollet (Issue Creator) on (2024-08-12 22:01:43 UTC): As mentioned in the PR, it still fails to build later on. But it's now clear why you shouldn't build with Python 3.12.

google-ml-butler[bot] on (2024-08-12 22:01:44 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73174"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73174"">No</a>

"
2449392095,issue,closed,completed,TFLite inference on iOS works with Metal GPU delegate but fails with CoreML delegate,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

MacOS Sonoma 14.6

### Mobile device

iPhone 12 Pro

### Python version

3.12.4

### Bazel version

7.2.1

### GCC/compiler version

15.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 Max 36GB

### Current behavior?

I tested a TFLite model inference with the C API on iOS (code attached below). The code tries to initialize a CoreML delegate when it's available, otherwise, it falls back to Metal GPU delegate. **It works fine with Metal GPU delegate when the CoreML delegate is commented out, but produces errors when CoreML delegate is enabled at the line:**
   ` modelInterpreter = TfLiteInterpreterCreate(tfModel, options); // <-----Thread 1: signal. SIGABRT`

The error from the log output is attached below. It looks like it has to do with the TFLite model. I also tested on other TFLite models, they all have the same error. Can someone help point out what the root cause is? 


### Standalone code to reproduce the issue

```shell
// Initialize model
    tfModel = TfLiteModelCreateFromFile(modelPath.c_str());
    
    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
    TfLiteInterpreterOptionsSetNumThreads(options, 4);
    
// CoreML delegate
    const TfLiteCoreMlDelegateOptions coreMLOptions = {
        .enabled_devices = TfLiteCoreMlDelegateAllDevices
    };
    tfDelegate = TfLiteCoreMlDelegateCreate(&coreMLOptions);
    if (tfDelegate == NULL) {
        // Fall back to GPU delegate
        const TFLGpuDelegateOptions gpuOptions = {
          .allow_precision_loss = true,
          .wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,
          .enable_quantization = true
        };
        
        tfDelegate = TFLGpuDelegateCreate(&gpuOptions);
    }
    
    TfLiteInterpreterOptionsAddDelegate(options, tfDelegate);
    
    modelInterpreter = TfLiteInterpreterCreate(tfModel, options); // <-----Thread 1: signal. SIGABRT
    TfLiteInterpreterOptionsDelete(options);
```


### Relevant log output

```shell
coreml_version must be 2 or 3. Setting to 3.
Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
CoreML delegate: 50 nodes delegated out of 55 nodes, with 6 partitions.
INFO: CoreML delegate: 50 nodes delegated out of 55 nodes, with 6 partitions.

[libprotobuf FATAL google/protobuf/io/zero_copy_stream_impl_lite.cc:334] CHECK failed: (count) <= (buffer_used_):  Can't back up over more bytes than were returned by the last call to Next().
libc++abi: terminating due to uncaught exception of type google::protobuf::FatalException: CHECK failed: (count) <= (buffer_used_):  Can't back up over more bytes than were returned by the last call to Next().
```
",x-xiao,2024-08-05 20:08:04+00:00,['sawantkumar'],2024-08-07 15:59:30+00:00,2024-08-07 15:59:27+00:00,https://github.com/tensorflow/tensorflow/issues/73167,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2271359037, 'issue_id': 2449392095, 'author': 'AdvaitDongre', 'body': 'i think these couple of steps might fix your issue: \r\n\r\n1. i hope you\'ve checked if the TensorflowLite version is compatible with your CoreML delegate\r\n2. try updating or downgrading the Protobuf library, it can resolve compatibility issues sometimes.\r\n3. if none of the above steps work try this code instead: \r\n\r\n```tfModel = TfLiteModelCreateFromFile(modelPath.c_str());\r\n    \r\nTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\nTfLiteInterpreterOptionsSetNumThreads(options, 4);\r\n\r\nconst TfLiteCoreMlDelegateOptions coreMLOptions = {\r\n    .enabled_devices = TfLiteCoreMlDelegateAllDevices,\r\n    .coreml_version = 3,\r\n    .max_delegated_partitions = 1,\r\n    .min_nodes_per_partition = 1\r\n};\r\ntfDelegate = TfLiteCoreMlDelegateCreate(&coreMLOptions);\r\n\r\nif (tfDelegate == NULL) {\r\n    const TFLGpuDelegateOptions gpuOptions = {\r\n      .allow_precision_loss = true,\r\n      .wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,\r\n      .enable_quantization = true\r\n    };\r\n    \r\n    tfDelegate = TFLGpuDelegateCreate(&gpuOptions);\r\n}\r\n\r\nTfLiteInterpreterOptionsAddDelegate(options, tfDelegate);\r\n\r\nif (!tfModel) {\r\n    fprintf(stderr, ""Failed to load model\\n"");\r\n    return;\r\n}\r\n\r\nif (!options) {\r\n    fprintf(stderr, ""Failed to create interpreter options\\n"");\r\n    return;\r\n}\r\n\r\nmodelInterpreter = TfLiteInterpreterCreate(tfModel, options);\r\n\r\nif (!modelInterpreter) {\r\n    fprintf(stderr, ""Failed to create interpreter\\n"");\r\n    return;\r\n}\r\n\r\nTfLiteInterpreterOptionsDelete(options);\r\n```\r\n\r\nlet me know what log output is showing if it still fails', 'created_at': datetime.datetime(2024, 8, 6, 13, 54, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271771407, 'issue_id': 2449392095, 'author': 'x-xiao', 'body': '@AdvaitDongre Thanks for your suggestions. \r\n1. The tensorflow I have is version 2.17.0. I built it from source using bazel. For my tflite model, the format is ""TensorFlow Lite v3"". I think they should be compatible with CoreML delegate.\r\n2. I think Protobuf library is included in the tensorflow package as third-party APIs, should I upgrade/downgrade myself?\r\n3. I have tried your code, the app still crashes at the line:\r\n`modelInterpreter = TfLiteInterpreterCreate(tfModel, options); // <-----Thread 1: signal. SIGABRT`\r\nwith the same error:\r\n```\r\n[libprotobuf FATAL google/protobuf/io/zero_copy_stream_impl_lite.cc:334] CHECK failed: (count) <= (buffer_used_):  Can\'t back up over more bytes than were returned by the last call to Next().\r\nlibc++abi: terminating due to uncaught exception of type google::protobuf::FatalException: CHECK failed: (count) <= (buffer_used_):  Can\'t back up over more bytes than were returned by the last call to Next().\r\n```', 'created_at': datetime.datetime(2024, 8, 6, 17, 17, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271950653, 'issue_id': 2449392095, 'author': 'AdvaitDongre', 'body': 'okay, it seems the main problem is related to protobuf, firstly try rebuilding your tensorflow lite and go for a version 3.19.... like 3.19.1, since it seems to work well with tensorflow 2.17.0, i feel like that would definitely do the work', 'created_at': datetime.datetime(2024, 8, 6, 19, 1, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271958457, 'issue_id': 2449392095, 'author': 'x-xiao', 'body': '@AdvaitDongre Can you elaborate on how i can change the version of Protobuf? Some configure file in the tensorflow package?', 'created_at': datetime.datetime(2024, 8, 6, 19, 6, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271972030, 'issue_id': 2449392095, 'author': 'AdvaitDongre', 'body': 'have you used pip or manually cloned the repo in your system?', 'created_at': datetime.datetime(2024, 8, 6, 19, 14, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272011846, 'issue_id': 2449392095, 'author': 'x-xiao', 'body': ""@AdvaitDongre I just downloaded the source zip file for 2.17.0 and built it with Bazel locally. I haven't used pip or cloned the repo."", 'created_at': datetime.datetime(2024, 8, 6, 19, 38, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272024445, 'issue_id': 2449392095, 'author': 'AdvaitDongre', 'body': 'you should try changing the protobuf dependency in the WORKSPACE file before building tensorflow again\r\n\r\nyou should try this code (given by GPT):\r\n```\r\nwget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.1/protobuf-all-3.19.1.tar.gz\r\ntar -xzf protobuf-all-3.19.1.tar.gz\r\ncd protobuf-3.19.1\r\n./configure\r\nmake\r\nsudo make install\r\ncd ..\r\n```\r\n\r\nupdate this desired section, you might need to modify the \r\n```\r\nhttp_archive(\r\n    name = ""com_google_protobuf"",\r\n    urls = [""https://mirror.bazel.build/github.com/protocolbuffers/protobuf/archive/v3.19.1.tar.gz""],\r\n    strip_prefix = ""protobuf-3.19.1"",\r\n    sha256 = ""a0d1a16ef23a15761a216c6dc7c7857b22f6b88ff52e19adad0c703f4b69e4b4"",\r\n)\r\n```\r\nnow for sha256 file you\'ll need to run this command in your terminal\r\n```\r\nsha256sum protobuf-all-3.19.1.tar.gz\r\n```\r\n\r\nand then paste that thing in the sha256.\r\n\r\nand then you can clean and rebuild the tensorflow lite', 'created_at': datetime.datetime(2024, 8, 6, 19, 46, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272163844, 'issue_id': 2449392095, 'author': 'x-xiao', 'body': '@AdvaitDongre Thanks for your suggestion. I downgraded the Protobuf from 3.21.9 to 3.19.2. The crash is gone.', 'created_at': datetime.datetime(2024, 8, 6, 21, 14, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272651400, 'issue_id': 2449392095, 'author': 'AdvaitDongre', 'body': ""You're welcome, and I think you should close this issue, if you have any other issues feel free to ask"", 'created_at': datetime.datetime(2024, 8, 7, 5, 32, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273809511, 'issue_id': 2449392095, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73167"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73167"">No</a>', 'created_at': datetime.datetime(2024, 8, 7, 15, 59, 29, tzinfo=datetime.timezone.utc)}]","AdvaitDongre on (2024-08-06 13:54:52 UTC): i think these couple of steps might fix your issue: 

1. i hope you've checked if the TensorflowLite version is compatible with your CoreML delegate
2. try updating or downgrading the Protobuf library, it can resolve compatibility issues sometimes.
3. if none of the above steps work try this code instead: 

```tfModel = TfLiteModelCreateFromFile(modelPath.c_str());
    
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreterOptionsSetNumThreads(options, 4);

const TfLiteCoreMlDelegateOptions coreMLOptions = {
    .enabled_devices = TfLiteCoreMlDelegateAllDevices,
    .coreml_version = 3,
    .max_delegated_partitions = 1,
    .min_nodes_per_partition = 1
};
tfDelegate = TfLiteCoreMlDelegateCreate(&coreMLOptions);

if (tfDelegate == NULL) {
    const TFLGpuDelegateOptions gpuOptions = {
      .allow_precision_loss = true,
      .wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,
      .enable_quantization = true
    };
    
    tfDelegate = TFLGpuDelegateCreate(&gpuOptions);
}

TfLiteInterpreterOptionsAddDelegate(options, tfDelegate);

if (!tfModel) {
    fprintf(stderr, ""Failed to load model\n"");
    return;
}

if (!options) {
    fprintf(stderr, ""Failed to create interpreter options\n"");
    return;
}

modelInterpreter = TfLiteInterpreterCreate(tfModel, options);

if (!modelInterpreter) {
    fprintf(stderr, ""Failed to create interpreter\n"");
    return;
}

TfLiteInterpreterOptionsDelete(options);
```

let me know what log output is showing if it still fails

x-xiao (Issue Creator) on (2024-08-06 17:17:29 UTC): @AdvaitDongre Thanks for your suggestions. 
1. The tensorflow I have is version 2.17.0. I built it from source using bazel. For my tflite model, the format is ""TensorFlow Lite v3"". I think they should be compatible with CoreML delegate.
2. I think Protobuf library is included in the tensorflow package as third-party APIs, should I upgrade/downgrade myself?
3. I have tried your code, the app still crashes at the line:
`modelInterpreter = TfLiteInterpreterCreate(tfModel, options); // <-----Thread 1: signal. SIGABRT`
with the same error:
```
[libprotobuf FATAL google/protobuf/io/zero_copy_stream_impl_lite.cc:334] CHECK failed: (count) <= (buffer_used_):  Can't back up over more bytes than were returned by the last call to Next().
libc++abi: terminating due to uncaught exception of type google::protobuf::FatalException: CHECK failed: (count) <= (buffer_used_):  Can't back up over more bytes than were returned by the last call to Next().
```

AdvaitDongre on (2024-08-06 19:01:42 UTC): okay, it seems the main problem is related to protobuf, firstly try rebuilding your tensorflow lite and go for a version 3.19.... like 3.19.1, since it seems to work well with tensorflow 2.17.0, i feel like that would definitely do the work

x-xiao (Issue Creator) on (2024-08-06 19:06:04 UTC): @AdvaitDongre Can you elaborate on how i can change the version of Protobuf? Some configure file in the tensorflow package?

AdvaitDongre on (2024-08-06 19:14:22 UTC): have you used pip or manually cloned the repo in your system?

x-xiao (Issue Creator) on (2024-08-06 19:38:27 UTC): @AdvaitDongre I just downloaded the source zip file for 2.17.0 and built it with Bazel locally. I haven't used pip or cloned the repo.

AdvaitDongre on (2024-08-06 19:46:34 UTC): you should try changing the protobuf dependency in the WORKSPACE file before building tensorflow again

you should try this code (given by GPT):
```
wget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.1/protobuf-all-3.19.1.tar.gz
tar -xzf protobuf-all-3.19.1.tar.gz
cd protobuf-3.19.1
./configure
make
sudo make install
cd ..
```

update this desired section, you might need to modify the 
```
http_archive(
    name = ""com_google_protobuf"",
    urls = [""https://mirror.bazel.build/github.com/protocolbuffers/protobuf/archive/v3.19.1.tar.gz""],
    strip_prefix = ""protobuf-3.19.1"",
    sha256 = ""a0d1a16ef23a15761a216c6dc7c7857b22f6b88ff52e19adad0c703f4b69e4b4"",
)
```
now for sha256 file you'll need to run this command in your terminal
```
sha256sum protobuf-all-3.19.1.tar.gz
```

and then paste that thing in the sha256.

and then you can clean and rebuild the tensorflow lite

x-xiao (Issue Creator) on (2024-08-06 21:14:13 UTC): @AdvaitDongre Thanks for your suggestion. I downgraded the Protobuf from 3.21.9 to 3.19.2. The crash is gone.

AdvaitDongre on (2024-08-07 05:32:03 UTC): You're welcome, and I think you should close this issue, if you have any other issues feel free to ask

google-ml-butler[bot] on (2024-08-07 15:59:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73167"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73167"">No</a>

"
2449005346,issue,closed,completed,Kubeflow Pipeline with Kserve running into errors with Tensorflow,"We ran a kubeflow pipeline and created an endpoint(inferenceservice), it is in ready state, but when I curl it to get predictions i get attached issue, i already saved model in proper format and loaded model from storage uri to make predictions which worked, even tensorflow runtime version for training and kserve is same. We have used same tf version(2.16.1) for training and serving.

Here is the link to the Curl command, logs and the yaml file.
[Tensorflow error with Kubeflow Pipelines](https://docs.google.com/document/d/1EB8d7z8xecO5cUKyah2XujyqNxshFdOI0hooc3QvpTg/edit?usp=sharing)",ReemaHVT,2024-08-05 16:45:01+00:00,['Venkat6871'],2024-08-27 01:55:56+00:00,2024-08-27 01:55:55+00:00,https://github.com/tensorflow/tensorflow/issues/73158,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.16', '')]","[{'comment_id': 2272733765, 'issue_id': 2449005346, 'author': 'yashprakashsharma', 'body': 'Hi @Venkat6871 could you please provide some insights into the issue, we are facing it for a long time now', 'created_at': datetime.datetime(2024, 8, 7, 6, 40, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275574409, 'issue_id': 2449005346, 'author': 'yashprakashsharma', 'body': 'also getting same issue from cifar10_model and iris_model, logs: \r\n2024-08-08 08:16:11.407139: I external/org_tensorflow/tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point `round-off` errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0.\r\n2024-08-08 08:16:11.500499: I tensorflow_serving/model_servers/server.cc:77] Building single TensorFlow model file config: model_name: cifar10_model model_base_path: /models/cifar10_model\r\n2024-08-08 08:16:11.503247: I tensorflow_serving/model_servers/server_core.cc:474] Adding/updating models.\r\n2024-08-08 08:16:11.503278: I tensorflow_serving/model_servers/server_core.cc:603] (Re-)adding model: cifar10_model\r\n2024-08-08 08:16:11.709794: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: cifar10_model version: 1}\r\n2024-08-08 08:16:11.709855: I tensorflow_serving/core/loader_harness.cc:68] Approving load for servable version {name: cifar10_model version: 1}\r\n2024-08-08 08:16:11.709884: I tensorflow_serving/core/loader_harness.cc:76] Loading servable version {name: cifar10_model version: 1}\r\n2024-08-08 08:16:11.716917: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /models/cifar10_model/1\r\n2024-08-08 08:16:11.729105: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\r\n2024-08-08 08:16:11.729159: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /models/cifar10_model/1\r\n2024-08-08 08:16:11.730837: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-08 08:16:11.775236: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-08-08 08:16:11.782870: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\r\n2024-08-08 08:16:11.948435: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /models/cifar10_model/1\r\n2024-08-08 08:16:11.961415: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 244832 microseconds.\r\n2024-08-08 08:16:11.963256: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:82] No warmup data file found at /models/cifar10_model/1/assets.extra/tf_serving_warmup_requests\r\n2024-08-08 08:16:12.070870: I tensorflow_serving/core/loader_harness.cc:97] Successfully loaded servable version {name: cifar10_model version: 1}\r\n2024-08-08 08:16:12.073985: I tensorflow_serving/model_servers/server_core.cc:495] Finished adding/updating models\r\n2024-08-08 08:16:12.074074: I tensorflow_serving/model_servers/server.cc:121] Using InsecureServerCredentials\r\n2024-08-08 08:16:12.074608: I tensorflow_serving/model_servers/server.cc:388] Profiler service is enabled\r\n2024-08-08 08:16:12.079460: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:8500 ...\r\n[warn] getaddrinfo: address family for nodename not supported\r\n2024-08-08 08:16:12.083437: I tensorflow_serving/model_servers/server.cc:444] Exporting HTTP/REST API at:localhost:8501 ...\r\n[evhttp_server.cc : 250] NET_LOG: Entering the event loop ...\r\n2024-08-08 08:17:38.815509: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable sequential/conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/sequential/conv2d/kernel/N10tensorflow3VarE does not exist.\r\n[[{{function_node __inference_serving_default_45404}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]]\r\n2024-08-08 08:17:38.815626: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.\r\n2024-08-08 08:17:45.111537: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.', 'created_at': datetime.datetime(2024, 8, 8, 11, 19, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277543282, 'issue_id': 2449005346, 'author': 'Venkat6871', 'body': 'Hi **@ReemaHVT** ,\r\n\r\nI apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please try by downgrading the TensorFlow version to `2.15.0 `and latest TensorFlow version `2.17.0` and see is it resolving your issue ? \r\n\r\nMeanwhile, please refer this issue https://github.com/tensorflow/tensorflow/issues/57803 which is similar in some extent which may help you to solve your issue.\r\n\r\nIf issue still persists please help us with minimal code-snippet/Github repo along with complete steps to reproduce the same behavior from our end to help you further on this issue.\r\n\r\nThank you for cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 9, 9, 30, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277567611, 'issue_id': 2449005346, 'author': 'yashprakashsharma', 'body': 'Hi @Venkat6871 , already tried both version 2.15.0  and 2.17.0 but issue still persists, sharing minimal code-snippe along with complete steps to reproduce the same behavior in some time.', 'created_at': datetime.datetime(2024, 8, 9, 9, 44, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277660840, 'issue_id': 2449005346, 'author': 'yashprakashsharma', 'body': ""Hey @Venkat6871, while preparing minimal code snippet for you, the versioin 2.15.0 worked, previously when I tested it with 2.15.0 in my kubeflow pipeline, it failed cause I used replace all to replace version, it also replaced  tf-serving version, tf-serving don't have a 2.15.0 version, now it works thanks."", 'created_at': datetime.datetime(2024, 8, 9, 10, 39, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297815890, 'issue_id': 2449005346, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 20, 1, 53, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419588, 'issue_id': 2449005346, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 27, 1, 55, 55, tzinfo=datetime.timezone.utc)}]","yashprakashsharma on (2024-08-07 06:40:59 UTC): Hi @Venkat6871 could you please provide some insights into the issue, we are facing it for a long time now

yashprakashsharma on (2024-08-08 11:19:29 UTC): also getting same issue from cifar10_model and iris_model, logs: 
2024-08-08 08:16:11.407139: I external/org_tensorflow/tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point `round-off` errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0.
2024-08-08 08:16:11.500499: I tensorflow_serving/model_servers/server.cc:77] Building single TensorFlow model file config: model_name: cifar10_model model_base_path: /models/cifar10_model
2024-08-08 08:16:11.503247: I tensorflow_serving/model_servers/server_core.cc:474] Adding/updating models.
2024-08-08 08:16:11.503278: I tensorflow_serving/model_servers/server_core.cc:603] (Re-)adding model: cifar10_model
2024-08-08 08:16:11.709794: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: cifar10_model version: 1}
2024-08-08 08:16:11.709855: I tensorflow_serving/core/loader_harness.cc:68] Approving load for servable version {name: cifar10_model version: 1}
2024-08-08 08:16:11.709884: I tensorflow_serving/core/loader_harness.cc:76] Loading servable version {name: cifar10_model version: 1}
2024-08-08 08:16:11.716917: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /models/cifar10_model/1
2024-08-08 08:16:11.729105: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-08 08:16:11.729159: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /models/cifar10_model/1
2024-08-08 08:16:11.730837: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-08 08:16:11.775236: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-08-08 08:16:11.782870: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-08 08:16:11.948435: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /models/cifar10_model/1
2024-08-08 08:16:11.961415: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 244832 microseconds.
2024-08-08 08:16:11.963256: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:82] No warmup data file found at /models/cifar10_model/1/assets.extra/tf_serving_warmup_requests
2024-08-08 08:16:12.070870: I tensorflow_serving/core/loader_harness.cc:97] Successfully loaded servable version {name: cifar10_model version: 1}
2024-08-08 08:16:12.073985: I tensorflow_serving/model_servers/server_core.cc:495] Finished adding/updating models
2024-08-08 08:16:12.074074: I tensorflow_serving/model_servers/server.cc:121] Using InsecureServerCredentials
2024-08-08 08:16:12.074608: I tensorflow_serving/model_servers/server.cc:388] Profiler service is enabled
2024-08-08 08:16:12.079460: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:8500 ...
[warn] getaddrinfo: address family for nodename not supported
2024-08-08 08:16:12.083437: I tensorflow_serving/model_servers/server.cc:444] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 250] NET_LOG: Entering the event loop ...
2024-08-08 08:17:38.815509: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable sequential/conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/sequential/conv2d/kernel/N10tensorflow3VarE does not exist.
[[{{function_node __inference_serving_default_45404}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]]
2024-08-08 08:17:38.815626: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.
2024-08-08 08:17:45.111537: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.

Venkat6871 (Assginee) on (2024-08-09 09:30:54 UTC): Hi **@ReemaHVT** ,

I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please try by downgrading the TensorFlow version to `2.15.0 `and latest TensorFlow version `2.17.0` and see is it resolving your issue ? 

Meanwhile, please refer this issue https://github.com/tensorflow/tensorflow/issues/57803 which is similar in some extent which may help you to solve your issue.

If issue still persists please help us with minimal code-snippet/Github repo along with complete steps to reproduce the same behavior from our end to help you further on this issue.

Thank you for cooperation and patience.

yashprakashsharma on (2024-08-09 09:44:51 UTC): Hi @Venkat6871 , already tried both version 2.15.0  and 2.17.0 but issue still persists, sharing minimal code-snippe along with complete steps to reproduce the same behavior in some time.

yashprakashsharma on (2024-08-09 10:39:11 UTC): Hey @Venkat6871, while preparing minimal code snippet for you, the versioin 2.15.0 worked, previously when I tested it with 2.15.0 in my kubeflow pipeline, it failed cause I used replace all to replace version, it also replaced  tf-serving version, tf-serving don't have a 2.15.0 version, now it works thanks.

github-actions[bot] on (2024-08-20 01:53:51 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-27 01:55:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2447725292,issue,closed,completed,Unable to build 16KB aligned Tensorflow lite library for Android V,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

Android V

### Python version

_No response_

### Bazel version

bazel 6.0.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently we are unable to find the right flag needed to be added in bazel build for building 16KB aligned libraries.


### Standalone code to reproduce the issue

```shell
Currently we are unable to find the right flag needed to be added in bazel build for building 16KB aligned libraries.
16 KB libraries are supposed to give performance benefit in Android V.
```


### Relevant log output

_No response_",pallaviNNT,2024-08-05 06:33:47+00:00,['tilakrayal'],2024-08-22 01:55:29+00:00,2024-08-22 01:55:26+00:00,https://github.com/tensorflow/tensorflow/issues/73121,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2272768269, 'issue_id': 2447725292, 'author': 'tilakrayal', 'body': '@pallaviNNT,\r\nCould you please share a reproducible code or the colab gist that supports your statement so that the issue can be easily analysed/debugged? Thank you!', 'created_at': datetime.datetime(2024, 8, 7, 7, 4, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2290323381, 'issue_id': 2447725292, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 15, 1, 49, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303527148, 'issue_id': 2447725292, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 22, 1, 55, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303527208, 'issue_id': 2447725292, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73121"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73121"">No</a>', 'created_at': datetime.datetime(2024, 8, 22, 1, 55, 28, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-07 07:04:56 UTC): @pallaviNNT,
Could you please share a reproducible code or the colab gist that supports your statement so that the issue can be easily analysed/debugged? Thank you!

github-actions[bot] on (2024-08-15 01:49:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-22 01:55:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-22 01:55:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73121"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73121"">No</a>

"
2447678982,issue,closed,completed,`tf.random.stateless_categorical` get different output when using same seed and same input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2.140

### GPU model and memory

GPU T4

### Current behavior?

I run `tf.random.stateless_categorical` with the same inputs within a batch and the same seed but get different outputs. Is it suppose to get same output?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
logits = tf.constant([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]], dtype=tf.float32)
sample_seed = tf.constant([3723, 3723], dtype=tf.int32)
output = tf.random.stateless_categorical(
        logits=logits,
        num_samples=1,
        seed=sample_seed,
        dtype=tf.int32,
    )
print(output)

```


### Relevant log output
```
tf.Tensor(
[[4]
 [2]], shape=(2, 1), dtype=int32)
```",trfnhle,2024-08-05 06:04:28+00:00,['Venkat6871'],2024-08-05 08:23:14+00:00,2024-08-05 08:23:10+00:00,https://github.com/tensorflow/tensorflow/issues/73119,"[('type:bug', 'Bug')]","[{'comment_id': 2268465460, 'issue_id': 2447678982, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73119"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73119"">No</a>', 'created_at': datetime.datetime(2024, 8, 5, 8, 23, 12, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-08-05 08:23:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73119"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73119"">No</a>

"
2447152418,issue,closed,completed,PyCharm cannot parse any content under tensorflow.keras 2.16.2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

Yes

### OS platform and distribution

windows10

### Mobile device

windows10

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

autocomplete is not working in Pycharm

### Standalone code to reproduce the issue

```shell
autocomplete is not working in Pycharm
```


### Relevant log output

_No response_",zzzzzzzs,2024-08-04 14:11:31+00:00,['tilakrayal'],2024-09-12 01:58:38+00:00,2024-09-12 01:58:35+00:00,https://github.com/tensorflow/tensorflow/issues/73110,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.16', '')]","[{'comment_id': 2268236068, 'issue_id': 2447152418, 'author': 'miteshgupta07', 'body': ""> ### Issue type\r\n> Bug\r\n> \r\n> ### Have you reproduced the bug with TensorFlow Nightly?\r\n> Yes\r\n> \r\n> ### Source\r\n> binary\r\n> \r\n> ### TensorFlow version\r\n> tf 2.16.2\r\n> \r\n> ### Custom code\r\n> Yes\r\n> \r\n> ### OS platform and distribution\r\n> windows10\r\n> \r\n> ### Mobile device\r\n> windows10\r\n> \r\n> ### Python version\r\n> _No response_\r\n> \r\n> ### Bazel version\r\n> _No response_\r\n> \r\n> ### GCC/compiler version\r\n> _No response_\r\n> \r\n> ### CUDA/cuDNN version\r\n> _No response_\r\n> \r\n> ### GPU model and memory\r\n> _No response_\r\n> \r\n> ### Current behavior?\r\n> autocomplete is not working in Pycharm\r\n> \r\n> ### Standalone code to reproduce the issue\r\n> ```shell\r\n> autocomplete is not working in Pycharm\r\n> ```\r\n> \r\n> ### Relevant log output\r\n> _No response_\r\n\r\nHere are a few steps you can try to resolve the issue:\r\n\r\n1. **Update PyCharm:** Make sure you are using the latest version of PyCharm. Sometimes, issues with autocomplete can be resolved by updating to the latest IDE version.\r\n\r\n2. **Check Project Interpreter:** Verify that the project interpreter in PyCharm is correctly configured to use the environment where TensorFlow 2.16.2 is installed. You can do this by:\r\n\r\n     - Going to File > Settings > Project: <Your Project Name> > Python Interpreter.\r\n\r\n     - Ensure the interpreter is pointing to the correct environment.\r\n\r\n3. **Reindex PyCharm:** Sometimes, PyCharm's indexing can become corrupted. You can force it to reindex by:\r\n\r\n     - Going to File > Invalidate Caches / Restart.\r\n     - Choose Invalidate and Restart.\r\n\r\n4. **Update TensorFlow:** Ensure you have the latest version of TensorFlow 2.16.2 installed. You might want to consider upgrading or downgrading TensorFlow to see if the issue persists with different versions.\r\n\r\n5. **Virtual Environment:** If you are using a virtual environment, try recreating it. Sometimes, issues with autocomplete can be related to environment-specific problems.\r\n\r\n6. **Check PyCharm's Python Console:** Open the Python Console in PyCharm and try importing tensorflow.keras directly. This can help identify if the issue is specific to the autocomplete feature or a broader problem with the library import.\r\n\r\n7. **Install tensorflow with PyCharm Terminal:** Try installing TensorFlow directly from PyCharms terminal to ensure its properly linked:\r\n\r\n     - Open the terminal in PyCharm.\r\n     - Run pip install tensorflow==2.16.2.\r\n\r\n8. **Consider IDE Alternatives:** If the issue persists and you need a workaround, consider using another IDE or text editor temporarily while you investigate further."", 'created_at': datetime.datetime(2024, 8, 5, 5, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269125182, 'issue_id': 2447152418, 'author': 'zzzzzzzs', 'body': ""> > ### Issue type\r\n> > Bug\r\n> > ### Have you reproduced the bug with TensorFlow Nightly?\r\n> > Yes\r\n> > ### Source\r\n> > binary\r\n> > ### TensorFlow version\r\n> > tf 2.16.2\r\n> > ### Custom code\r\n> > Yes\r\n> > ### OS platform and distribution\r\n> > windows10\r\n> > ### Mobile device\r\n> > windows10\r\n> > ### Python version\r\n> > _No response_\r\n> > ### Bazel version\r\n> > _No response_\r\n> > ### GCC/compiler version\r\n> > _No response_\r\n> > ### CUDA/cuDNN version\r\n> > _No response_\r\n> > ### GPU model and memory\r\n> > _No response_\r\n> > ### Current behavior?\r\n> > autocomplete is not working in Pycharm\r\n> > ### Standalone code to reproduce the issue\r\n> > ```shell\r\n> > autocomplete is not working in Pycharm\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > ### Relevant log output\r\n> > _No response_\r\n> \r\n> Here are a few steps you can try to resolve the issue:\r\n> \r\n> 1. **Update PyCharm:** Make sure you are using the latest version of PyCharm. Sometimes, issues with autocomplete can be resolved by updating to the latest IDE version.\r\n> 2. **Check Project Interpreter:** Verify that the project interpreter in PyCharm is correctly configured to use the environment where TensorFlow 2.16.2 is installed. You can do this by:\r\n>    \r\n>    * Going to File > Settings > Project:  > Python Interpreter.\r\n>    * Ensure the interpreter is pointing to the correct environment.\r\n> 3. **Reindex PyCharm:** Sometimes, PyCharm's indexing can become corrupted. You can force it to reindex by:\r\n>    \r\n>    * Going to File > Invalidate Caches / Restart.\r\n>    * Choose Invalidate and Restart.\r\n> 4. **Update TensorFlow:** Ensure you have the latest version of TensorFlow 2.16.2 installed. You might want to consider upgrading or downgrading TensorFlow to see if the issue persists with different versions.\r\n> 5. **Virtual Environment:** If you are using a virtual environment, try recreating it. Sometimes, issues with autocomplete can be related to environment-specific problems.\r\n> 6. **Check PyCharm's Python Console:** Open the Python Console in PyCharm and try importing tensorflow.keras directly. This can help identify if the issue is specific to the autocomplete feature or a broader problem with the library import.\r\n> 7. **Install tensorflow with PyCharm Terminal:** Try installing TensorFlow directly from PyCharms terminal to ensure its properly linked:\r\n>    \r\n>    * Open the terminal in PyCharm.\r\n>    * Run pip install tensorflow==2.16.2.\r\n> 8. **Consider IDE Alternatives:** If the issue persists and you need a workaround, consider using another IDE or text editor temporarily while you investigate further.\r\n\r\nDowngrading TensorFlow is useful."", 'created_at': datetime.datetime(2024, 8, 5, 13, 48, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272763573, 'issue_id': 2447152418, 'author': 'tilakrayal', 'body': '@zzzzzzzs,\r\nKeras is migrated to Keras 3 with multi backend support.\r\n\r\nCould you please install Keras separately using pip install keras and import keras directly and let us know the outcome with keras.layers etc.\r\n\r\nWhen we are importing the keras from tensorflow till version 2.15, keras 2.0 is imported. Whereas when we try to import the tensorflow 2.16 it imports keras3.0 directly.\r\n\r\n\r\n**TF 2.15: Keras2.0**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n```\r\n\r\n**TF 2.16: Keras 3.0**\r\n```python\r\nimport tensorflow as tf\r\nimport keras\r\n```\r\n\r\n\r\nKeras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch\r\nhttps://keras.io/keras_3/\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 7, 7, 1, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273203234, 'issue_id': 2447152418, 'author': 'zzzzzzzs', 'body': '> @zzzzzzzs, Keras is migrated to Keras 3 with multi backend support.\r\n> \r\n> Could you please install Keras separately using pip install keras and import keras directly and let us know the outcome with keras.layers etc.\r\n> \r\n> When we are importing the keras from tensorflow till version 2.15, keras 2.0 is imported. Whereas when we try to import the tensorflow 2.16 it imports keras3.0 directly.\r\n> \r\n> **TF 2.15: Keras2.0**\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow import keras\r\n> ```\r\n> \r\n> **TF 2.16: Keras 3.0**\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> import keras\r\n> ```\r\n> \r\n> Keras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch https://keras.io/keras_3/\r\n> \r\n> Thank you!\r\n\r\nNow I am directly using Keras in TensorFlow, and after trying it out, I found that pycharm 2023.2.7 corresponds to TensorFlow 2.10.0 and works', 'created_at': datetime.datetime(2024, 8, 7, 11, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285860127, 'issue_id': 2447152418, 'author': 'tilakrayal', 'body': '@zzzzzzzs,\r\nHave you tried with the tensorflow latest version 2.17.0? and provide the update and try as mentioned import Keras.\r\n\r\nAlso this is more related to keras. Please raise the request in keras-team/keras repo. \r\n Thank you!', 'created_at': datetime.datetime(2024, 8, 13, 10, 0, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2290292203, 'issue_id': 2447152418, 'author': 'zzzzzzzs', 'body': ""> @zzzzzzzs, Have you tried with the tensorflow latest version 2.17.0? and provide the update and try as mentioned import Keras.\r\n> \r\n> Also this is more related to keras. Please raise the request in keras-team/keras repo. Thank you!\r\n\r\nOkay, I'm willing to try."", 'created_at': datetime.datetime(2024, 8, 15, 1, 33, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330450562, 'issue_id': 2447152418, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 5, 1, 57, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102468, 'issue_id': 2447152418, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102497, 'issue_id': 2447152418, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73110"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73110"">No</a>', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 37, tzinfo=datetime.timezone.utc)}]","miteshgupta07 on (2024-08-05 05:59:27 UTC): Here are a few steps you can try to resolve the issue:

1. **Update PyCharm:** Make sure you are using the latest version of PyCharm. Sometimes, issues with autocomplete can be resolved by updating to the latest IDE version.

2. **Check Project Interpreter:** Verify that the project interpreter in PyCharm is correctly configured to use the environment where TensorFlow 2.16.2 is installed. You can do this by:

     - Going to File > Settings > Project: <Your Project Name> > Python Interpreter.

     - Ensure the interpreter is pointing to the correct environment.

3. **Reindex PyCharm:** Sometimes, PyCharm's indexing can become corrupted. You can force it to reindex by:

     - Going to File > Invalidate Caches / Restart.
     - Choose Invalidate and Restart.

4. **Update TensorFlow:** Ensure you have the latest version of TensorFlow 2.16.2 installed. You might want to consider upgrading or downgrading TensorFlow to see if the issue persists with different versions.

5. **Virtual Environment:** If you are using a virtual environment, try recreating it. Sometimes, issues with autocomplete can be related to environment-specific problems.

6. **Check PyCharm's Python Console:** Open the Python Console in PyCharm and try importing tensorflow.keras directly. This can help identify if the issue is specific to the autocomplete feature or a broader problem with the library import.

7. **Install tensorflow with PyCharm Terminal:** Try installing TensorFlow directly from PyCharms terminal to ensure its properly linked:

     - Open the terminal in PyCharm.
     - Run pip install tensorflow==2.16.2.

8. **Consider IDE Alternatives:** If the issue persists and you need a workaround, consider using another IDE or text editor temporarily while you investigate further.

zzzzzzzs (Issue Creator) on (2024-08-05 13:48:03 UTC): Downgrading TensorFlow is useful.

tilakrayal (Assginee) on (2024-08-07 07:01:51 UTC): @zzzzzzzs,
Keras is migrated to Keras 3 with multi backend support.

Could you please install Keras separately using pip install keras and import keras directly and let us know the outcome with keras.layers etc.

When we are importing the keras from tensorflow till version 2.15, keras 2.0 is imported. Whereas when we try to import the tensorflow 2.16 it imports keras3.0 directly.


**TF 2.15: Keras2.0**
```python
import tensorflow as tf
from tensorflow import keras
```

**TF 2.16: Keras 3.0**
```python
import tensorflow as tf
import keras
```


Keras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch
https://keras.io/keras_3/

Thank you!

zzzzzzzs (Issue Creator) on (2024-08-07 11:01:53 UTC): Now I am directly using Keras in TensorFlow, and after trying it out, I found that pycharm 2023.2.7 corresponds to TensorFlow 2.10.0 and works

tilakrayal (Assginee) on (2024-08-13 10:00:18 UTC): @zzzzzzzs,
Have you tried with the tensorflow latest version 2.17.0? and provide the update and try as mentioned import Keras.

Also this is more related to keras. Please raise the request in keras-team/keras repo. 
 Thank you!

zzzzzzzs (Issue Creator) on (2024-08-15 01:33:53 UTC): Okay, I'm willing to try.

github-actions[bot] on (2024-09-05 01:57:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-12 01:58:35 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-12 01:58:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73110"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73110"">No</a>

"
2447041041,issue,closed,not_planned,tvos supporttflite,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.9.3

### Custom code

No

### OS platform and distribution

macOS 14.4

### Mobile device

tvOS 17.5.1

### Python version

3.13

### Bazel version

6.5.0

### GCC/compiler version

Apple clang version 15.0.0 (clang-1500.1.0.2.5) Target: arm64-apple-darwin23.4.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

### CUDA/cuDNN version

no

### GPU model and memory

m1 pro

### Current behavior?

no tvOS support

### Standalone code to reproduce the issue

```shell
Due to unfamiliarity with the Bazel build system and the lack of available resources, I have attempted multiple times but have not successfully cross-compiled TensorFlow Lite to tvOS. I hope the official team can provide support for cross-compiling to tvOS.
```


### Relevant log output

_No response_",Tinuv-Dev,2024-08-04 09:49:40+00:00,"['yishuangP', 'pkgoogle', 'sawantkumar']",2024-11-26 18:07:00+00:00,2024-11-26 18:06:55+00:00,https://github.com/tensorflow/tensorflow/issues/73108,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)'), ('iOS', '')]","[{'comment_id': 2269913277, 'issue_id': 2447041041, 'author': 'pkgoogle', 'body': 'I am not sure tvOS is currently supported by us, @yishuangP, would you have a better idea? Thanks.\r\n\r\n@Tinuv-Dev, have you tried following iOS instructions for now? What issues do you run into?', 'created_at': datetime.datetime(2024, 8, 5, 21, 2, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271233241, 'issue_id': 2447041041, 'author': 'Tinuv-Dev', 'body': 'iOS can compile normally, including both the real device and the simulator.', 'created_at': datetime.datetime(2024, 8, 6, 12, 59, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271256451, 'issue_id': 2447041041, 'author': 'Tinuv-Dev', 'body': 'I tried to compile the tvOS version using a script like ""bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_static_framework"", but it failed because there is no tvOS build configuration in the TensorFlow repository. Therefore, it is not possible to compile for tvOS. My goal is to compile the tvOS version, and once successful, package it into an xcframework to use in my application.', 'created_at': datetime.datetime(2024, 8, 6, 13, 9, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500181794, 'issue_id': 2447041041, 'author': 'gaikwadrahul8', 'body': ""Hi, @Tinuv-Dev \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/42\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 3, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501615761, 'issue_id': 2447041041, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73108"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73108"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 6, 57, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-05 21:02:40 UTC): I am not sure tvOS is currently supported by us, @yishuangP, would you have a better idea? Thanks.

@Tinuv-Dev, have you tried following iOS instructions for now? What issues do you run into?

Tinuv-Dev (Issue Creator) on (2024-08-06 12:59:08 UTC): iOS can compile normally, including both the real device and the simulator.

Tinuv-Dev (Issue Creator) on (2024-08-06 13:09:39 UTC): I tried to compile the tvOS version using a script like ""bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_static_framework"", but it failed because there is no tvOS build configuration in the TensorFlow repository. Therefore, it is not possible to compile for tvOS. My goal is to compile the tvOS version, and once successful, package it into an xcframework to use in my application.

gaikwadrahul8 on (2024-11-26 10:03:27 UTC): Hi, @Tinuv-Dev 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/42

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:06:57 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73108"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73108"">No</a>

"
2446231557,issue,closed,completed,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.  Aborted (core dumped)","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.9.3

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.1 LTS

### Mobile device

Ubuntu 22.04.1 LTS

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

No GPU

### GPU model and memory

_No response_

### Current behavior?

I am not able to use [spleeter](https://github.com/deezer/spleeter) as it rely on tensorflow. [I am getting msg when  using spleeter as it depends on tensorflow](https://github.com/deezer/spleeter/issues/902)
`The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
Aborted (core dumped)
`

It can be tested by just installing `pip install tensorflow==2.9.3` also
(spleeterenv) root@mypc:~/spleeter_app# python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.9.3

There should be ready binary compiled with sse2 capable cpu to install via pip
```


### Relevant log output

```shell
The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
Aborted (core dumped)
```
",bhavesh-hirpara,2024-08-03 08:05:09+00:00,['Venkat6871'],2024-08-28 01:56:28+00:00,2024-08-28 01:56:23+00:00,https://github.com/tensorflow/tensorflow/issues/73084,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2270801040, 'issue_id': 2446231557, 'author': 'Venkat6871', 'body': 'Hi @bhavesh-hirpara ,\r\n- Thank you for bringing this issue to our attention and I was trying to replicate the same behaviour from my end with sample .mp3 file in Google colab with Tensorflow version 2.9.3 and I did not observe behavior which is mentioned in the issue template so it seems like working as expected, please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/eb5905856150c4034b8c4f2b05b0480e/test-73084.ipynb) if I have missed something here please let me know\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 8, 6, 9, 19, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287684804, 'issue_id': 2446231557, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 14, 1, 54, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935911, 'issue_id': 2446231557, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313935988, 'issue_id': 2446231557, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73084"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73084"">No</a>', 'created_at': datetime.datetime(2024, 8, 28, 1, 56, 27, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-06 09:19:33 UTC): Hi @bhavesh-hirpara ,
- Thank you for bringing this issue to our attention and I was trying to replicate the same behaviour from my end with sample .mp3 file in Google colab with Tensorflow version 2.9.3 and I did not observe behavior which is mentioned in the issue template so it seems like working as expected, please refer this [gist-file](https://colab.sandbox.google.com/gist/gaikwadrahul8/eb5905856150c4034b8c4f2b05b0480e/test-73084.ipynb) if I have missed something here please let me know

Thank you for your cooperation and patience.

github-actions[bot] on (2024-08-14 01:54:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-28 01:56:23 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-28 01:56:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73084"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73084"">No</a>

"
2445194850,issue,open,,Proposal: Conditionally substitute boringSSL with system OpenSSL,"Hello All,

As we know boringSSL doesn't support IBM Power architecture anymore, we are finding it difficult to build Tensorflow on Power with proper and updated boringSSL. Even if we just patch boringSSL to have Power support (by using older boringSSL or applying patch from previous commits that had Power support), it is difficult to maintain if boringSSL version gets updated with no equivalent changes for Power. 

Our Proposal: Use system installed OpenSSL instead of boringSSL based on a flag **only if** set. For e.g. USE_SYSTEM_OPENSSL

TF brings in boringSSL dependency directly and indirectly (through curl and grpc) during bazel build process. So, we have tried some changes which does following -
1. Use boringSSL and curl from the system (using TF_SYSTEM_LIBS)
2. Patched grpc to fallback to system openSSL include and library paths during its build. 
Note: grpc already provides similar mechanism when grpcio's wheel is being built through [setup.py](https://github.com/grpc/grpc/blob/master/setup.py#L301). But this mechanism doesn't work when we build grpc through bazel.

These changes have enabled us to build TF on Power, and our testing also didn't give any issues so far (some of the bazel tests and tensorflow/models). 

We want your opinion on this approach. Also, kindly let us know what all tests we should do to have better coverage of this replacement.

Thanks in advance. ",npanpaliya,2024-08-02 15:16:19+00:00,['Venkat6871'],2024-08-20 05:56:33+00:00,,https://github.com/tensorflow/tensorflow/issues/73029,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues')]","[{'comment_id': 2272793449, 'issue_id': 2445194850, 'author': 'Venkat6871', 'body': '@belitskiy , @learning-to-play .', 'created_at': datetime.datetime(2024, 8, 7, 7, 20, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298025559, 'issue_id': 2445194850, 'author': 'npanpaliya', 'body': 'Hello @Venkat6871 - Could you please let us know your opinion on this? Also, kindly help me tag the main Community members and get their views too on this?', 'created_at': datetime.datetime(2024, 8, 20, 5, 56, 32, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-07 07:20:38 UTC): @belitskiy , @learning-to-play .

npanpaliya (Issue Creator) on (2024-08-20 05:56:32 UTC): Hello @Venkat6871 - Could you please let us know your opinion on this? Also, kindly help me tag the main Community members and get their views too on this?

"
2444620552,issue,open,,Seeking information on low-level TPU interaction and libtpu.so API,"I'm looking to build an automatic differentiation library for TPUs without using high-level front-ends like TensorFlow/JAX/PyTorch-XLA, but I'm finding information about lower-level TPU usage is practically non-existent.

Specifically, I'm interested in:
1. How to interact with TPUs at a lower level than what's typically exposed in TensorFlow
2. Information about the libtpu.so library and its API
3. Any resources or documentation on implementing custom TPU operations

Are there any insights or suggestions on how to approach this, particularly regarding TPU support? Any ideas or help would be greatly appreciated.

I understand that some of this information might be proprietary, but any guidance on what is possible or available would be very helpful.",notlober,2024-08-02 10:10:36+00:00,['Venkat6871'],2024-09-17 06:18:03+00:00,,https://github.com/tensorflow/tensorflow/issues/73017,"[('type:docs-bug', 'Document issues'), ('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:tpus', 'tpu, tpuestimator')]",[],
2443912227,issue,closed,not_planned,"The Tensorflow Profiler and it does not work, and no one replies. ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10-2.18

### Custom code

Yes

### OS platform and distribution

Colab linux/ubuntu 20

### Python version

3.9-3.12

### CUDA/cuDNN version

12.3 but also on CPU

### GPU model and memory

Colab

### Current behavior?

![Screenshot from 2024-08-02 04-16-24](https://github.com/user-attachments/assets/aca88be2-0273-46a1-aa5f-19e6af81088c)


### Standalone code to reproduce the issue

```shell
Notebook

https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb


Page

https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
```",ghsanti,2024-08-02 03:18:05+00:00,['Venkat6871'],2024-08-02 11:59:17+00:00,2024-08-02 11:58:45+00:00,https://github.com/tensorflow/tensorflow/issues/73004,"[('type:bug', 'Bug')]","[{'comment_id': 2265201873, 'issue_id': 2443912227, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73004"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73004"">No</a>', 'created_at': datetime.datetime(2024, 8, 2, 11, 58, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265202592, 'issue_id': 2443912227, 'author': 'ghsanti', 'body': 'Closing since there are other open issues about this, and no one replies.', 'created_at': datetime.datetime(2024, 8, 2, 11, 59, 16, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-08-02 11:58:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73004"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73004"">No</a>

ghsanti (Issue Creator) on (2024-08-02 11:59:16 UTC): Closing since there are other open issues about this, and no one replies.

"
2443849351,issue,closed,completed,[RNN] unable to quantize RNN with customized cell,"### 1. System information

- OS Platform and Distribution: Ubuntu 22.04.4 LTS
- TensorFlow installation: pip package
- TensorFlow library: 2.18.0-dev20240801

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

A minimal example is available in this [Colab notebook](https://colab.research.google.com/drive/1GGdTio3XNj9Jg-CKpyV376RRlRy8_Gs1?usp=sharing).

### 3. Failure after conversion

None

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I wanted to implement a RNN using customized cells. When I try to quantize the model, the conversion can not be finished.

The error message looks like:

`
INFO:tensorflow:Assets written to: [/tmp/tmp6jab2_an/assets](https://file+.vscode-resource.vscode-cdn.net/tmp/tmp6jab2_an/assets)
INFO:tensorflow:Assets written to: [/tmp/tmp6jab2_an/assets](https://file+.vscode-resource.vscode-cdn.net/tmp/tmp6jab2_an/assets)
[/home/d/miniconda3/envs/tflite_model/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953](https://file+.vscode-resource.vscode-cdn.net/home/dd/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953): UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(
2024-08-01 21:32:47.659189: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
2024-08-01 21:32:47.659202: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.

The Kernel crashed while executing code in the current cell or a previous cell. 
Please review the code in the cell(s) to identify a possible cause of the failure. 
Click [here](https://aka.ms/vscodeJupyterKernelCrash) for more info. 
View Jupyter [log](command:jupyter.viewOutput) for further details.
`

No traceback is available as the notebook kernel crashes. When running in python script, segmentation fault happens.

By setting breakpoints, the error seems to happen with Line 153 tensorflow/tensorflow/lite/python/optimize/calibrator.py ([here](https://github.com/tensorflow/tensorflow/blob/02e39547a0d551e5bb0c66e5e0e0c319a33f75ff/tensorflow/lite/python/optimize/calibrator.py#L152)). The underlying cc code is raising the error,

Many thanks for the help with this issue!",Voivio,2024-08-02 02:14:21+00:00,"['pkgoogle', 'sawantkumar']",2024-08-05 22:02:52+00:00,2024-08-05 22:02:49+00:00,https://github.com/tensorflow/tensorflow/issues/73002,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2265872279, 'issue_id': 2443849351, 'author': 'pkgoogle', 'body': 'Hi @Voivio, the easiest way to accomplish this right now is to create a [custom RNN in PyTorch](https://discuss.pytorch.org/t/implementation-of-multiplicative-lstm/2328/4) and convert using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch), does this work for you?', 'created_at': datetime.datetime(2024, 8, 2, 17, 50, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265975187, 'issue_id': 2443849351, 'author': 'Voivio', 'body': 'Hi @pkgoogle, thanks for your quick reply. I have quickly tested this out in the [Colab Notebook](https://colab.research.google.com/drive/1bkinOGC1Hwcr3rq1VDKNPNDgcXiUv2W1?usp=sharing) using this library. This ai-edge-torch library can convert the RNN with customized cells into a TFLite model without errors, yet it is not actually converting operations happened inside (though those outside are fine). Operations are still float32-based rather than int8.\r\n\r\nSo now I give up on using the RNN structure provided from keras and decide to manually handling all hidden states. I have no more questions regarding this issue and please feel free to close it if there will be no more updates on it! \r\n\r\nMany thanks again.', 'created_at': datetime.datetime(2024, 8, 2, 19, 3, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269996685, 'issue_id': 2443849351, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73002"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73002"">No</a>', 'created_at': datetime.datetime(2024, 8, 5, 22, 2, 51, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-02 17:50:15 UTC): Hi @Voivio, the easiest way to accomplish this right now is to create a [custom RNN in PyTorch](https://discuss.pytorch.org/t/implementation-of-multiplicative-lstm/2328/4) and convert using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch), does this work for you?

Voivio (Issue Creator) on (2024-08-02 19:03:51 UTC): Hi @pkgoogle, thanks for your quick reply. I have quickly tested this out in the [Colab Notebook](https://colab.research.google.com/drive/1bkinOGC1Hwcr3rq1VDKNPNDgcXiUv2W1?usp=sharing) using this library. This ai-edge-torch library can convert the RNN with customized cells into a TFLite model without errors, yet it is not actually converting operations happened inside (though those outside are fine). Operations are still float32-based rather than int8.

So now I give up on using the RNN structure provided from keras and decide to manually handling all hidden states. I have no more questions regarding this issue and please feel free to close it if there will be no more updates on it! 

Many thanks again.

google-ml-butler[bot] on (2024-08-05 22:02:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73002"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/73002"">No</a>

"
2442761797,issue,closed,completed,Failure running a SavedModel exported from a tf.Module with a Keras model as an instance variable,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No, because the sample code produces a core dump.

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a `tf.Module` using `tf.saved_model.save` when that class contains a Keras model in an instance variable results in a `FAILED_PRECONDITION` when run using saved_model_cli or libtensorflow.

In Tensorflow v2.15.0, the behavior is as expected: the graph execution proceeds without any errors and the expected results are produced.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

SHAPE = (1, 5)

class TestModel(tf.Module):
    def __init__(self):
        super().__init__()
        self.dense_layer = tf.keras.layers.Dense(10)

    @tf.function(input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.float32)])
    def run(self, x):
        return self.dense_layer(x)


module = TestModel()
sample_input = tf.random.normal(SHAPE, dtype=tf.float32)
module.run(sample_input)

np.save('sample_input.npy', sample_input.numpy())
tf.saved_model.save(module, ""test_model"")

# # To reproduce, run the following:
# python test.py && saved_model_cli run --dir test_model --tag_set serve --signature_def serving_default --inputs 'x=sample_input.npy'
```


### Relevant log output

```shell
2024-08-01 15:25:35.204057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-01 15:25:35.261217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-01 15:25:35.278801: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-01 15:25:35.313892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-01 15:25:37.270110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-01 15:25:38.898105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4281 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0
2024-08-01 15:25:38.898868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 944 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:d8:00.0, compute capability: 7.0
WARNING:tensorflow:From /home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.saved_model.load` instead.
W0801 15:25:38.903731 139977869341120 deprecation.py:50] From /home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.saved_model.load` instead.
INFO:tensorflow:Restoring parameters from test_model/variables/variables
I0801 15:25:38.936800 139977869341120 saver.py:1417] Restoring parameters from test_model/variables/variables
2024-08-01 15:25:38.941206: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-08-01 15:25:39.144248: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]
2024-08-01 15:25:39.144340: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]
	 [[StatefulPartitionedCall/_21]]
2024-08-01 15:25:39.144423: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12615348601576968325
Traceback (most recent call last):
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1401, in _do_call
    return fn(*args)
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1384, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1477, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.
  (0) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]
	 [[StatefulPartitionedCall/_21]]
  (1) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/iantolic-soban/tf_bug/.venv/bin/saved_model_cli"", line 8, in <module>
    sys.exit(main())
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 1340, in main
    app.run(smcli_main)
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 1338, in smcli_main
    args.func()
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 1036, in run
    run_saved_model_with_feed_dict(
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 721, in run_saved_model_with_feed_dict
    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 971, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1214, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1420, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:

2 root error(s) found.
  (0) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{node dense_1/Add/ReadVariableOp}}]]
	 [[StatefulPartitionedCall/_21]]
  (1) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.
	 [[{{node dense_1/Add/ReadVariableOp}}]]
0 successful operations.
0 derived errors ignored.
```
",ivansoban,2024-08-01 15:27:28+00:00,['Venkat6871'],2025-01-07 02:02:56+00:00,2025-01-07 02:02:50+00:00,https://github.com/tensorflow/tensorflow/issues/72963,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2265089619, 'issue_id': 2442761797, 'author': 'Venkat6871', 'body': 'I tried to run your code on Colab using TF v2.15.0, 2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/385f7c5bfc4d9660cb68e5f3fc8feb04/72963_2-15-2-17-nightly-v.ipynb) here for reference.\r\nThank you.', 'created_at': datetime.datetime(2024, 8, 2, 10, 41, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272651761, 'issue_id': 2442761797, 'author': 'Venkat6871', 'body': 'Hi **@ivansoban** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 7, 5, 32, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273210355, 'issue_id': 2442761797, 'author': 'yashprakashsharma', 'body': 'Hi @Venkat6871 , I am also facing similar issue while deploying a trained tensorflow model using tfserving the dicussion ling you gave above gives Page not found issue, logs for my issue :     external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/conv2d/kernel/N10tensorflow3VarE does not exist.\r\n         [[{{function_node __inference_serving_default_11757}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]]\r\n2024-08-07 10:11:13.600499: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.\r\n2024-08-07 10:17:47.725694: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors', 'created_at': datetime.datetime(2024, 8, 7, 11, 6, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273296262, 'issue_id': 2442761797, 'author': 'yashprakashsharma', 'body': 'link to issue for reference: \r\nhttps://github.com/tensorflow/tensorflow/issues/73158', 'created_at': datetime.datetime(2024, 8, 7, 11, 56, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275919972, 'issue_id': 2442761797, 'author': 'ivansoban', 'body': ""@Venkat6871 thank you. The second link doesn't work for me but I have created this issue: https://github.com/keras-team/keras/issues/20095"", 'created_at': datetime.datetime(2024, 8, 8, 14, 4, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558912757, 'issue_id': 2442761797, 'author': 'Venkat6871', 'body': 'Hi **@ivansoban** ,\r\nCould you please confirm if this issue has been resolved for you? If it is resolved, feel free to close the issue.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 23, 5, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566070096, 'issue_id': 2442761797, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 31, 2, 1, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574250989, 'issue_id': 2442761797, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574251083, 'issue_id': 2442761797, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72963"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72963"">No</a>', 'created_at': datetime.datetime(2025, 1, 7, 2, 2, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 10:41:19 UTC): I tried to run your code on Colab using TF v2.15.0, 2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/385f7c5bfc4d9660cb68e5f3fc8feb04/72963_2-15-2-17-nightly-v.ipynb) here for reference.
Thank you.

Venkat6871 (Assginee) on (2024-08-07 05:32:27 UTC): Hi **@ivansoban** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

yashprakashsharma on (2024-08-07 11:06:09 UTC): Hi @Venkat6871 , I am also facing similar issue while deploying a trained tensorflow model using tfserving the dicussion ling you gave above gives Page not found issue, logs for my issue :     external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/conv2d/kernel/N10tensorflow3VarE does not exist.
         [[{{function_node __inference_serving_default_11757}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]]
2024-08-07 10:11:13.600499: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.
2024-08-07 10:17:47.725694: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors

yashprakashsharma on (2024-08-07 11:56:41 UTC): link to issue for reference: 
https://github.com/tensorflow/tensorflow/issues/73158

ivansoban (Issue Creator) on (2024-08-08 14:04:57 UTC): @Venkat6871 thank you. The second link doesn't work for me but I have created this issue: https://github.com/keras-team/keras/issues/20095

Venkat6871 (Assginee) on (2024-12-23 05:00:46 UTC): Hi **@ivansoban** ,
Could you please confirm if this issue has been resolved for you? If it is resolved, feel free to close the issue.
Thank you!

github-actions[bot] on (2024-12-31 02:01:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-07 02:02:49 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-07 02:02:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72963"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72963"">No</a>

"
2442408568,issue,closed,completed,TimeDistributed step error,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Model fit is not able to take TimeDistributed step

### Standalone code to reproduce the issue

```shell
ValueError                                Traceback (most recent call last)
Cell In[121], line 2
      1 epochs = 200
----> 2 modelCNNLSTM310724.fit(
      3     train_dataset.prefetch(tf.data.experimental.AUTOTUNE),
      4     validation_data=validation_dataset.prefetch(tf.data.experimental.AUTOTUNE),
      5     epochs=epochs,
      6     shuffle=True,batch_size=4,
      7     verbose=2,
      8     callbacks=[checkpoint_cb,tensorboard_cb,early_stopping_cb]
      9 )

File ~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /tmp/__autograph_generated_filecb7v_b6i.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""/home/mayur/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1401, in train_function  *
        return step_function(self, iterator)
    File ""/home/mayur/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1384, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/mayur/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1373, in run_step  **
        outputs = model.train_step(data)
    File ""/home/mayur/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1150, in train_step
        y_pred = self(x, training=True)
    File ""/home/mayur/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer 'time_distributed_21' (type TimeDistributed).
    
    as_list() is not defined on an unknown TensorShape.
    
    Call arguments received by layer 'time_distributed_21' (type TimeDistributed):
       inputs=tf.Tensor(shape=<unknown>, dtype=float32)
       training=True
       mask=None
```


### Relevant log output

_No response_",mayurmunshi,2024-08-01 13:04:31+00:00,['tilakrayal'],2024-08-18 01:56:53+00:00,2024-08-18 01:56:50+00:00,https://github.com/tensorflow/tensorflow/issues/72954,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2265456216, 'issue_id': 2442408568, 'author': 'tilakrayal', 'body': '@mayurmunshi,\r\nI tried with the sample code on tf-nightly which it was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9c012edb6a7433a0a48f5f00c8fc9cb0/untitled2042.ipynb). Also this has been resolved with the respective PR.\r\nhttps://github.com/keras-team/keras/pull/19799\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 13, 55, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278934999, 'issue_id': 2442408568, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295061833, 'issue_id': 2442408568, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 18, 1, 56, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295061860, 'issue_id': 2442408568, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72954"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72954"">No</a>', 'created_at': datetime.datetime(2024, 8, 18, 1, 56, 52, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-02 13:55:09 UTC): @mayurmunshi,
I tried with the sample code on tf-nightly which it was executed without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9c012edb6a7433a0a48f5f00c8fc9cb0/untitled2042.ipynb). Also this has been resolved with the respective PR.
https://github.com/keras-team/keras/pull/19799

Thank you!

github-actions[bot] on (2024-08-10 01:54:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-18 01:56:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-18 01:56:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72954"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72954"">No</a>

"
2442380268,issue,closed,completed,Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


ValueError: Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: KerasTensor(type_spec=TensorSpec(shape=(None, 3, 32, 32, 16, 256), dtype=tf.float32, name=None), name='activation_19/Relu:0', description=""created by layer 'activation_19'"")

### Standalone code to reproduce the issue

```shell
Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance
```


### Relevant log output

```shell
alueError                                Traceback (most recent call last)
Cell In[103], line 5
      1 #Model parameters
      3 input_shape = (3,128,128,64,1)
----> 5 modelCNNLSTM310724 = ResNet50_3DCNNLSTM310724(input_shape )#= (128,128,64,1))
      7 modelCNNLSTM310724.summary()

Cell In[102], line 42, in ResNet50_3DCNNLSTM310724(input_shape)
     39 x = layers.BatchNormalization()(x)
     40 x = layers.Dropout(0.5)(x)
---> 42 x = tf.keras.layers.TimeDistributed(conv_block_3d(x, kernel_size=(3, 3, 3), filters=[64, 64, 256], strides=(1, 1, 1)))
     43 x = identity_block_3d(x, kernel_size=(3, 3, 3), filters=[64, 64, 256])
     44 x = identity_block_3d(x, kernel_size=(3, 3, 3), filters=[64, 64, 256])

File ~/.local/lib/python3.10/site-packages/keras/src/layers/rnn/time_distributed.py:74, in TimeDistributed.__init__(self, layer, **kwargs)
     72 def __init__(self, layer, **kwargs):
     73     if not isinstance(layer, Layer):
---> 74         raise ValueError(
     75             ""Please initialize `TimeDistributed` layer with a ""
     76             f""`tf.keras.layers.Layer` instance. Received: {layer}""
     77         )
     78     super().__init__(layer, **kwargs)
     79     self.supports_masking = True

ValueError: Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: KerasTensor(type_spec=TensorSpec(shape=(None, 3, 32, 32, 16, 256), dtype=tf.float32, name=None), name='activation_19/Relu:0', description=""created by layer 'activation_19'"")
```
",mayurmunshi,2024-08-01 12:52:10+00:00,['Venkat6871'],2024-12-27 02:01:31+00:00,2024-12-27 02:01:28+00:00,https://github.com/tensorflow/tensorflow/issues/72953,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2264988553, 'issue_id': 2442380268, 'author': 'Venkat6871', 'body': 'Hi **@mayurmunshi** ,\r\n- In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 9, 44, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935019, 'issue_id': 2442380268, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2291017647, 'issue_id': 2442380268, 'author': 'mayurmunshi', 'body': 'Hello,\r\n\r\nThe code snippet is as follows \r\n\r\n inputs = Input(input_shape)\r\n    custom_layer = ZeroPadding3D((3,3,1))\r\n    time_distributed_custom_layer = TimeDistributed(custom_layer)\r\n    x = time_distributed_custom_layer(inputs)\r\n\r\nOn  trying to execute this step at model.fit stage I am getting following error message :\r\n\r\n epochs = 20\r\n----> 2 model.fit(\r\n      3     train_dataset,\r\n      4     validation_data=validation_dataset,\r\n      5     epochs=epochs,\r\n      6     shuffle=True,batch_size=16,\r\n      7     verbose=2,\r\n      8     callbacks=[checkpoint_cb,tensorboard_cb,early_stopping_cb]\r\n      9 )\r\n\r\nFile ~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n     68     # To get the full stack trace, call:\r\n     69     # `tf.debugging.disable_traceback_filtering()`\r\n---> 70     raise e.with_traceback(filtered_tb) from None\r\n     71 finally:\r\n     72     del filtered_tb\r\n\r\nFile /tmp/__autograph_generated_filecb7v_b6i.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)\r\n     13 try:\r\n     14     do_return = True\r\n---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\r\n     16 except:\r\n     17     do_return = False\r\n\r\nValueError: in user code:\r\n\r\n    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1401, in train_function  *\r\n        return step_function(self, iterator)\r\n    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1384, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1373, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1150, in train_step\r\n        y_pred = self(x, training=True)\r\n    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n\r\n    ValueError: Exception encountered when calling layer \'time_distributed_21\' (type TimeDistributed).\r\n    \r\n    as_list() is not defined on an unknown TensorShape.\r\n    \r\n    Call arguments received by layer \'time_distributed_21\' (type TimeDistributed):\r\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\r\n       training=True\r\n       mask=None', 'created_at': datetime.datetime(2024, 8, 15, 10, 11, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533871152, 'issue_id': 2442380268, 'author': 'Venkat6871', 'body': 'Hi **@mayurmunshi** ,\r\nApologies for the delay, and thank you for your patience. The root cause of this issue is likely a mismatch in the expected input shape for the TimeDistributed layer, which applies a wrapped layer across the time dimension of a 4D or higher-dimensional input. The TimeDistributed layer expects the input tensor to have at least 3 dimensions. If the input tensor does not include these dimensions or has an unknown shape, it results in the error: as_list() is not defined on an unknown TensorShape. I tried modifying the code, and it is working fine now. I hope this will be helpful for you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f44d62dafd1e755e7514f48b7a969a53/72953_tf-2-18-0-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 11, 7, 10, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552621756, 'issue_id': 2442380268, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 19, 2, 5, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233475, 'issue_id': 2442380268, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233496, 'issue_id': 2442380268, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72953"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72953"">No</a>', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 30, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 09:44:09 UTC): Hi **@mayurmunshi** ,
- In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.
Thank you!

github-actions[bot] on (2024-08-10 01:54:06 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mayurmunshi (Issue Creator) on (2024-08-15 10:11:07 UTC): Hello,

The code snippet is as follows 

 inputs = Input(input_shape)
    custom_layer = ZeroPadding3D((3,3,1))
    time_distributed_custom_layer = TimeDistributed(custom_layer)
    x = time_distributed_custom_layer(inputs)

On  trying to execute this step at model.fit stage I am getting following error message :

 epochs = 20
----> 2 model.fit(
      3     train_dataset,
      4     validation_data=validation_dataset,
      5     epochs=epochs,
      6     shuffle=True,batch_size=16,
      7     verbose=2,
      8     callbacks=[checkpoint_cb,tensorboard_cb,early_stopping_cb]
      9 )

File ~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /tmp/__autograph_generated_filecb7v_b6i.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1401, in train_function  *
        return step_function(self, iterator)
    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1384, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1373, in run_step  **
        outputs = model.train_step(data)
    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1150, in train_step
        y_pred = self(x, training=True)
    File ""/home/user/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer 'time_distributed_21' (type TimeDistributed).
    
    as_list() is not defined on an unknown TensorShape.
    
    Call arguments received by layer 'time_distributed_21' (type TimeDistributed):
       inputs=tf.Tensor(shape=<unknown>, dtype=float32)
       training=True
       mask=None

Venkat6871 (Assginee) on (2024-12-11 07:10:48 UTC): Hi **@mayurmunshi** ,
Apologies for the delay, and thank you for your patience. The root cause of this issue is likely a mismatch in the expected input shape for the TimeDistributed layer, which applies a wrapped layer across the time dimension of a 4D or higher-dimensional input. The TimeDistributed layer expects the input tensor to have at least 3 dimensions. If the input tensor does not include these dimensions or has an unknown shape, it results in the error: as_list() is not defined on an unknown TensorShape. I tried modifying the code, and it is working fine now. I hope this will be helpful for you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/f44d62dafd1e755e7514f48b7a969a53/72953_tf-2-18-0-v.ipynb) here for reference.
Thank you!

github-actions[bot] on (2024-12-19 02:05:51 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-27 02:01:28 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-27 02:01:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72953"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72953"">No</a>

"
2441891657,issue,closed,completed,Import errors for python keras module,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14+

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While working with `saving` module in `python.keras` some functions raise `importError`.
```cmd
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name '__version__' from 'tensorflow.python.keras'
```

Since `tensorflow==2.14.0` the `__version__` is missing from the `__init__.py`. ( removed in https://github.com/tensorflow/tensorflow/commit/5368a3ad4719842eecae9e4c965b1a5d072cb9e5 )

- [2.13 __init__.py](https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/keras/__init__.py)
- [2.14+ __init__.py](https://github.com/tensorflow/tensorflow/blob/v2.14.0/tensorflow/python/keras/__init__.py)

Some functions eg. `hdf5_format.save_weights_to_hdf_5_group()` still import it in versions `2.14+` [See here for tf 2.15.1](https://github.com/tensorflow/tensorflow/blob/v2.15.1/tensorflow/python/keras/saving/hdf5_format.py#L625)


```
def save_weights_to_hdf5_group(f, layers):
  """"""Saves the weights of a list of layers to a HDF5 group.

  Args:
      f: HDF5 group.
      layers: List of layer instances.
  """"""
  from tensorflow.python.keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top
  ...
```


### Standalone code to reproduce the issue

```shell
from tensorflow.python.keras.saving import hdf5_format

# raises import error
hdf5_format.save_weights_to_hdf_5_group(...)

OR

from tensorflow.python.keras import __version__
```


### Relevant log output

_No response_",vixhead,2024-08-01 09:00:15+00:00,['tilakrayal'],2024-08-02 11:19:22+00:00,2024-08-02 11:19:18+00:00,https://github.com/tensorflow/tensorflow/issues/72946,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('TF2.14', 'For issues related to Tensorflow 2.14.x')]","[{'comment_id': 2264958840, 'issue_id': 2441891657, 'author': 'tilakrayal', 'body': '@vixhead,\r\ntensorflow/python/keras code is a legacy copy of Keras since the TensorFlow v2.7 release. Please try to remove any import of tensorflow.python.keras and use the public API **from tensorflow import keras** till the tensorflow v2.15 and for later releases try to import  as **import keras** for the keras3.0 version. \r\n\r\nAlso please try to raise the issue of Keras related in the Keras-team/Keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 2, 9, 27, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265146690, 'issue_id': 2441891657, 'author': 'vixhead', 'body': 'Thanks for the answer. I guess we will have to switch to keras public API then if we want to use 2.14+ versions. (and import keras for 2.15+)', 'created_at': datetime.datetime(2024, 8, 2, 11, 19, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265146747, 'issue_id': 2441891657, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72946"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72946"">No</a>', 'created_at': datetime.datetime(2024, 8, 2, 11, 19, 20, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-02 09:27:07 UTC): @vixhead,
tensorflow/python/keras code is a legacy copy of Keras since the TensorFlow v2.7 release. Please try to remove any import of tensorflow.python.keras and use the public API **from tensorflow import keras** till the tensorflow v2.15 and for later releases try to import  as **import keras** for the keras3.0 version. 

Also please try to raise the issue of Keras related in the Keras-team/Keras repo for the quick resolution. Thank you!

vixhead (Issue Creator) on (2024-08-02 11:19:19 UTC): Thanks for the answer. I guess we will have to switch to keras public API then if we want to use 2.14+ versions. (and import keras for 2.15+)

google-ml-butler[bot] on (2024-08-02 11:19:20 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72946"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72946"">No</a>

"
2441332166,issue,closed,not_planned,Supportment for 16KB page sizes on other processor architectures,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow-lite 2.16.1

### Custom code

Yes

### OS platform and distribution

Android 15

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The tensorflow-lite does not support 16KB page sizes on x86, x86_64, armeavi-v7a architecture.

Android 15, which will be released next year, supports 16 KB page sizes.
I'm going to introduce it in my app to improve performance.
However, tensorflow-lite does not seem to support 16 KB page sizes.
Is it possible to support 16 KB page sizes?

My app supports the architecture below.
- x86
- x86_64
- arm64-v8a
- armeabi-v7a

I found that support for arm64-v8a was supported through the following issues.
https://github.com/tensorflow/tensorflow/issues/69459

Please review whether it is possible to support x86, x86_64, armeavi-v7a architecture also.

The android guide related to this is as follows.
https://developer.android.com/guide/practices/page-sizes

### Standalone code to reproduce the issue

```shell
1. APK with tensorflow-lite v2.16.1
2. Create an alignment.sh file for 16KB alignment test with reference to the link below
https://developer.android.com/guide/practices/page-sizes?hl=en#test
3. Run below command
$ ./alignment.sh ${UNZIPPED_APK_FOLDER}

currently the output is below,

apk/lib/x86/libtensorflowlite_jni.so: UNALIGNED (2**12)
apk/lib/x86_64/libtensorflowlite_jni.so: UNALIGNED (2**12)
apk/lib/arm64-v8a/libtensorflowlite_jni.so: UNALIGNED (2**12)
apk/lib/armeabi-v7a/libtensorflowlite_jni.so: UNALIGNED (2**12)
```


### Relevant log output

_No response_",juvelop17,2024-08-01 02:55:08+00:00,"['arfaian', 'pkgoogle', 'sawantkumar']",2024-11-26 18:07:12+00:00,2024-11-26 18:07:12+00:00,https://github.com/tensorflow/tensorflow/issues/72933,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2269921502, 'issue_id': 2441332166, 'author': 'pkgoogle', 'body': 'Hi @arfaian, I believe you would have the best insight into this. Thanks.', 'created_at': datetime.datetime(2024, 8, 5, 21, 8, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363326647, 'issue_id': 2441332166, 'author': 'sebouh00', 'body': 'Is there a plan to address this for the other archs as well, and release it on Maven Central for all archs?', 'created_at': datetime.datetime(2024, 9, 20, 9, 52, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500190495, 'issue_id': 2441332166, 'author': 'gaikwadrahul8', 'body': ""Hi, @juvelop17 \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/43\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 7, 13, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-08-05 21:08:17 UTC): Hi @arfaian, I believe you would have the best insight into this. Thanks.

sebouh00 on (2024-09-20 09:52:35 UTC): Is there a plan to address this for the other archs as well, and release it on Maven Central for all archs?

gaikwadrahul8 on (2024-11-26 10:07:13 UTC): Hi, @juvelop17 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/43

Let us know if you have any questions. Thanks.

"
2440947669,issue,closed,completed,"""ValueError: The layer sequential has never been called and thus has no defined output."" when the model's been build and called","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tensorflow[and-cuda]==2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4

### GPU model and memory

RTX A4500 laptop

### Current behavior?

I loaded a .keras model that I've just trained on a server. The model has been trained on the very same versions of TF and Keras that I am using locally. 

I loaded the model and used a custom implementation of the LayerCAM saliency function where I am creating a submodel using the following: grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output]). However, the error arises. 

### Standalone code to reproduce the issue

```shell
Here's a Google Drive link to a code, the model weights and an input to reproduce the error: https://drive.google.com/drive/folders/15J_ghWXWbs8EmSVXedH6sJRvJcPUSTIW?usp=sharing
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
Cell In[1], line 45
     42         class_activation_map = tf.expand_dims(class_activation_map, axis=-1)
     43     return class_activation_map
---> 45 layer_cam_test = layer_cam(img = test_sample, model=model, label_index = 0)

Cell In[1], line 24, in layer_cam(img, label_index, model)
     22 print(layer_names)
     23 for layer_name in layer_names[-1:]:
---> 24     grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output]) #bug's here
     25     with tf.GradientTape() as tape:
     26         tape.watch(img)

File ~/miniconda3/envs/envtfnightly/lib/python3.11/site-packages/keras/src/ops/operation.py:266, in Operation.output(self)
    256 @property
    257 def output(self):
    258     """"""Retrieves the output tensor(s) of a layer.
    259 
    260     Only returns the tensor(s) corresponding to the *first time*
   (...)
    264         Output tensor or list of output tensors.
    265     """"""
--> 266     return self._get_node_attribute_at_index(0, ""output_tensors"", ""output"")

File ~/miniconda3/envs/envtfnightly/lib/python3.11/site-packages/keras/src/ops/operation.py:285, in Operation._get_node_attribute_at_index(self, node_index, attr, attr_name)
    269 """"""Private utility to retrieves an attribute (e.g. inputs) from a node.
    270 
    271 This is used to implement the properties:
   (...)
    282     The operation's attribute `attr` at the node of index `node_index`.
    283 """"""
    284 if not self._inbound_nodes:
--> 285     raise ValueError(
    286         f""The layer {self.name} has never been called ""
    287         f""and thus has no defined {attr_name}.""
    288     )
    289 if not len(self._inbound_nodes) > node_index:
    290     raise ValueError(
    291         f""Asked to get {attr_name} at node ""
    292         f""{node_index}, but the operation has only ""
    293         f""{len(self._inbound_nodes)} inbound nodes.""
    294     )

ValueError: The layer sequential has never been called and thus has no defined output.
Click to add a cell.
```
",Senantq,2024-07-31 21:00:51+00:00,['tilakrayal'],2024-09-03 12:43:07+00:00,2024-09-03 12:43:05+00:00,https://github.com/tensorflow/tensorflow/issues/72915,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2262359877, 'issue_id': 2440947669, 'author': 'LorenzoTassellari', 'body': ""I get exactly the same problem when trying to create a gradcam model, to explain a CNN prediction, I'm using TensorFLow 2.17.\r\nStill haven't found a solution."", 'created_at': datetime.datetime(2024, 8, 1, 8, 31, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262400272, 'issue_id': 2440947669, 'author': 'LorenzoTassellari', 'body': 'I\'ve figured it out.\r\nI believe the layer ""Sequential"" is considered the last layer of our networks (for some reason), but this layer doesn\'t actually have an output, instead, you need to replace this with the last actual layer.\r\nSo instead of:\r\ngrad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output]) #bug\'s here\r\n\r\nUse this:\r\ngrad_model = keras.models.Model(\r\n[model.inputs], \r\n[model.get_layer(last_conv_layer_name).output, \r\nmodel.get_layer(Name_of_last_deep_layer).output]) #right here you should put your last layer\'s name (you can figure that out from running model.summary) mine was \'dense_2\'\r\n\r\nHope this helps :)', 'created_at': datetime.datetime(2024, 8, 1, 8, 44, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265092651, 'issue_id': 2440947669, 'author': 'Senantq', 'body': ""Thank you for this workaround sc21lt. I don't think we are supposed to do so as in the Keras tutorial for graCAM they are still using 'model.outputs', but thanks, now this step works. \r\nHowever, have you got anything else than None gradients? I don't know if this is due to my time distributed layers but it can't get any gradients when using whether grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.layers[-1].output]) or grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.get_layer('dense').output]). However it works perfectly fine when I use vanilla saliency that doesn't create a new grad_model.."", 'created_at': datetime.datetime(2024, 8, 2, 10, 43, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265451147, 'issue_id': 2440947669, 'author': 'Senantq', 'body': ""Just a little update, the problem with the null gradients only appears when loading the layer from the .keras file. If the model code is written in the code I don't have null gradients... So there seems to be two different bugs here. Here is the model if one wants to investigate: \r\n```\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.Input(shape=(27, 75, 93, 81, 1)),  # time_steps, depth, height, width, channels\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(6, kernel_size=7, activation='relu', kernel_initializer='he_normal')),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),\r\n\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(32, kernel_size=3, activation='relu', kernel_initializer='he_normal')),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),\r\n\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(128, kernel_size=2, activation='relu', kernel_initializer='he_normal')),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(256, kernel_size=2, activation='relu', kernel_initializer='he_normal')),\r\n\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),\r\n\r\n    tf.keras.layers.Conv1D(256, kernel_size=5, activation='relu', kernel_initializer='he_normal'),\r\n    tf.keras.layers.MaxPooling1D(pool_size=2),\r\n    tf.keras.layers.BatchNormalization(),\r\n\r\n    tf.keras.layers.Conv1D(512, kernel_size=3, activation='relu', kernel_initializer='he_normal'),\r\n    tf.keras.layers.MaxPooling1D(pool_size=2),\r\n    tf.keras.layers.BatchNormalization(),\r\n\r\n    tf.keras.layers.Conv1D(1024, kernel_size=2, activation='relu', kernel_initializer='he_normal'),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(2, activation='softmax')]) \r\n```"", 'created_at': datetime.datetime(2024, 8, 2, 13, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282262503, 'issue_id': 2440947669, 'author': '25aswada', 'body': 'sc21It Thank you so much!!', 'created_at': datetime.datetime(2024, 8, 10, 20, 0, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284381977, 'issue_id': 2440947669, 'author': 'tilakrayal', 'body': '@Senantq,\r\nLooks like this issue is more related to keras issue. Could you please try to raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 8, 12, 16, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285693459, 'issue_id': 2440947669, 'author': 'Senantq', 'body': 'It is done, but the workaround found by sc21It produce null gradient with tf gradient tape, am i supposed to close this one ?', 'created_at': datetime.datetime(2024, 8, 13, 8, 42, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324271033, 'issue_id': 2440947669, 'author': 'tilakrayal', 'body': '@Senantq,\r\nYeah, please feel free to move this issue to close this issue and raise another issue for the gradient tape which helps to track easily. Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 9, 31, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326429372, 'issue_id': 2440947669, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72915"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72915"">No</a>', 'created_at': datetime.datetime(2024, 9, 3, 12, 43, 7, tzinfo=datetime.timezone.utc)}]","LorenzoTassellari on (2024-08-01 08:31:03 UTC): I get exactly the same problem when trying to create a gradcam model, to explain a CNN prediction, I'm using TensorFLow 2.17.
Still haven't found a solution.

LorenzoTassellari on (2024-08-01 08:44:04 UTC): I've figured it out.
I believe the layer ""Sequential"" is considered the last layer of our networks (for some reason), but this layer doesn't actually have an output, instead, you need to replace this with the last actual layer.
So instead of:
grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output]) #bug's here

Use this:
grad_model = keras.models.Model(
[model.inputs], 
[model.get_layer(last_conv_layer_name).output, 
model.get_layer(Name_of_last_deep_layer).output]) #right here you should put your last layer's name (you can figure that out from running model.summary) mine was 'dense_2'

Hope this helps :)

Senantq (Issue Creator) on (2024-08-02 10:43:24 UTC): Thank you for this workaround sc21lt. I don't think we are supposed to do so as in the Keras tutorial for graCAM they are still using 'model.outputs', but thanks, now this step works. 
However, have you got anything else than None gradients? I don't know if this is due to my time distributed layers but it can't get any gradients when using whether grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.layers[-1].output]) or grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.get_layer('dense').output]). However it works perfectly fine when I use vanilla saliency that doesn't create a new grad_model..

Senantq (Issue Creator) on (2024-08-02 13:52:26 UTC): Just a little update, the problem with the null gradients only appears when loading the layer from the .keras file. If the model code is written in the code I don't have null gradients... So there seems to be two different bugs here. Here is the model if one wants to investigate: 
```
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(27, 75, 93, 81, 1)),  # time_steps, depth, height, width, channels
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(6, kernel_size=7, activation='relu', kernel_initializer='he_normal')),
    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),
    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(32, kernel_size=3, activation='relu', kernel_initializer='he_normal')),
    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),
    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(128, kernel_size=2, activation='relu', kernel_initializer='he_normal')),
    tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))),
    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv3D(256, kernel_size=2, activation='relu', kernel_initializer='he_normal')),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),
    tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()),

    tf.keras.layers.Conv1D(256, kernel_size=5, activation='relu', kernel_initializer='he_normal'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.BatchNormalization(),

    tf.keras.layers.Conv1D(512, kernel_size=3, activation='relu', kernel_initializer='he_normal'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.BatchNormalization(),

    tf.keras.layers.Conv1D(1024, kernel_size=2, activation='relu', kernel_initializer='he_normal'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(2, activation='softmax')]) 
```

25aswada on (2024-08-10 20:00:21 UTC): sc21It Thank you so much!!

tilakrayal (Assginee) on (2024-08-12 16:11:33 UTC): @Senantq,
Looks like this issue is more related to keras issue. Could you please try to raise the issue in the Keras-team/keras repo for the quick resolution. Thank you!

Senantq (Issue Creator) on (2024-08-13 08:42:05 UTC): It is done, but the workaround found by sc21It produce null gradient with tf gradient tape, am i supposed to close this one ?

tilakrayal (Assginee) on (2024-09-02 09:31:59 UTC): @Senantq,
Yeah, please feel free to move this issue to close this issue and raise another issue for the gradient tape which helps to track easily. Thank you!

google-ml-butler[bot] on (2024-09-03 12:43:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72915"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72915"">No</a>

"
2440245901,issue,closed,completed,Version Issue,"Numpy's version has been already upgraded to 2.x, but now tensorflow can't support it.Can you upgrade the tensorflow library?",bash-s,2024-07-31 14:46:20+00:00,['Venkat6871'],2024-07-31 16:46:35+00:00,2024-07-31 16:45:38+00:00,https://github.com/tensorflow/tensorflow/issues/72888,[],"[{'comment_id': 2260698853, 'issue_id': 2440245901, 'author': 'bash-s', 'body': ""I'll appreciate it if you can upgrade it."", 'created_at': datetime.datetime(2024, 7, 31, 14, 47, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260943091, 'issue_id': 2440245901, 'author': 'mihaimaruseac', 'body': 'duplicate of #67291', 'created_at': datetime.datetime(2024, 7, 31, 16, 45, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260944641, 'issue_id': 2440245901, 'author': 'mihaimaruseac', 'body': 'Also, see #72883', 'created_at': datetime.datetime(2024, 7, 31, 16, 46, 33, tzinfo=datetime.timezone.utc)}]","bash-s (Issue Creator) on (2024-07-31 14:47:02 UTC): I'll appreciate it if you can upgrade it.

mihaimaruseac on (2024-07-31 16:45:38 UTC): duplicate of #67291

mihaimaruseac on (2024-07-31 16:46:33 UTC): Also, see #72883

"
2439945075,issue,closed,completed,Tensorflow compilation: Could not find compiler for platform CUDA: NOT_FOUND,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

buntu 20.04.6 LTS

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

A30

### Current behavior?

Manually compiling tensorflow + gpu support and getting this error:
 xla_ops.cc:799] Compilation failed:UNIMPLEMENTED: Could not find compiler for platform CUDA: NOT_FOUND: could not find registered compiler for platform CUDA -- was support for that platform linked in?.  Falling back to TF function call.

Any idea what does that mean? Does it also mean that some operators would NOT run on the GPU and fallback to the cpu?



### Standalone code to reproduce the issue

```shell
Not relevant
```


### Relevant log output

```shell
2024-07-31 11:38:58.350411: I external/org_tensorflow/tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.

2024-07-31 11:38:58.536230: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: cvr version: 1}
2024-07-31 11:38:58.652653: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1722425940.584331  243979 xla_ops.cc:799] Compilation failed:UNIMPLEMENTED: Could not find compiler for platform CUDA: NOT_FOUND: could not find registered compiler for platform CUDA -- was support for that platform linked in?.  Falling back to TF function call.
2024-07-31 11:39:00.749104: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /opt/models/cvr/1
2024-07-31 11:39:05.177024: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 6640666 microseconds.
2024-07-31 11:39:05.482499: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:5051 ...
```
",eyalhir74,2024-07-31 12:31:43+00:00,['tilakrayal'],2024-11-09 01:58:49+00:00,2024-11-09 01:58:46+00:00,https://github.com/tensorflow/tensorflow/issues/72879,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:gpu', 'GPU related issues'), ('awaiting PR merge', 'awaiting PR merge'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2265622795, 'issue_id': 2439945075, 'author': 'tilakrayal', 'body': 'The respective PR raised has been assigned for reviewing and once it is merged this issue will move to closed status.', 'created_at': datetime.datetime(2024, 8, 2, 15, 14, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434593367, 'issue_id': 2439945075, 'author': 'tilakrayal', 'body': '@eyalhir74,\r\nThe PR which was raised for the similar issue has been closed by the contributor. Could you please try with the latest tensorflow v2.17, v2.18.rc1 and update the information for the same. \r\nhttps://github.com/tensorflow/tensorflow/releases\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 24, 8, 12, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451157818, 'issue_id': 2439945075, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 1, 2, 7, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985951, 'issue_id': 2439945075, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465985985, 'issue_id': 2439945075, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72879"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72879"">No</a>', 'created_at': datetime.datetime(2024, 11, 9, 1, 58, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-02 15:14:28 UTC): The respective PR raised has been assigned for reviewing and once it is merged this issue will move to closed status.

tilakrayal (Assginee) on (2024-10-24 08:12:22 UTC): @eyalhir74,
The PR which was raised for the similar issue has been closed by the contributor. Could you please try with the latest tensorflow v2.17, v2.18.rc1 and update the information for the same. 
https://github.com/tensorflow/tensorflow/releases

Thank you!

github-actions[bot] on (2024-11-01 02:07:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-09 01:58:45 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-09 01:58:48 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72879"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72879"">No</a>

"
2439892959,issue,closed,completed,How to reduce the size of the shared library,"**System information**
- OS Platform and Distribution : Ubuntu 22.04.4 LTS
- TensorFlow version 2.7.4
I need to use the tflite c library in a beaglebone black rev3 with this specs:

> processor       : 0
> model name      : ARMv7 Processor rev 2 (v7l)
> BogoMIPS        : 995.32
> Features        : half thumb fastmult vfp edsp thumbee neon vfpv3 tls vfpd32
> CPU implementer : 0x41
> CPU architecture: 7
> CPU variant     : 0x3
> CPU part        : 0xc08
> CPU revision    : 2
> 
> Hardware        : Generic AM33XX (Flattened Device Tree)
> Revision        : 0000
> Serial          : 2218SBB15982
> ldd (Debian GLIBC 2.28-10) 2.28
> gcc version 8.3.0

i'm trying to build the library using this toolchain : gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf
I successfully built a shared library, but the size of the .so file is 4 MB, which is too big for me. How can I reduce it?
is there any way to limit the operators and types required by my model

",saad-koukous,2024-07-31 12:09:06+00:00,['Venkat6871'],2024-09-04 11:25:28+00:00,2024-08-18 01:56:51+00:00,https://github.com/tensorflow/tensorflow/issues/72878,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.7', 'Issues related to TF 2.7.0')]","[{'comment_id': 2264972764, 'issue_id': 2439892959, 'author': 'Venkat6871', 'body': 'Hi **@saad-koukous** ,\r\n- Could you please use strip command to remove unnecessary symbols from the library. This command can significantly reduce the file size. I giving that command below:\r\n```\r\narm-linux-gnueabihf-strip libtensorflowlite_c.so\r\n```\r\n- And ensure you are compiling specifically for the ARMv7 architecture to avoid including unnecessary code for other architectures:\r\n\r\n```\r\nbazel build --config=elinux_armhf --cpu=armhf //tensorflow/lite:libtensorflowlite_c.so\r\n```\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 9, 35, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935031, 'issue_id': 2439892959, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295061848, 'issue_id': 2439892959, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 18, 1, 56, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328669979, 'issue_id': 2439892959, 'author': 'saad-koukous', 'body': 'Hi @Venkat6871 \r\nI tried using the strip command as you suggested to remove unnecessary symbols from the libtensorflowlite_c.so\r\nHowever, the size of the library did not change after stripping.\r\n\r\nAdditionally, I tried compiling the library specifically for the ARMv7 architecture\r\nUnfortunately, the file size remained the same even after these attempts. It seems that there might not be any significant symbols or unnecessary code that can be stripped from the library.\r\n\r\nIf you have any other suggestions or further optimizations to try, please let me know!\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 4, 11, 25, 27, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 09:35:18 UTC): Hi **@saad-koukous** ,
- Could you please use strip command to remove unnecessary symbols from the library. This command can significantly reduce the file size. I giving that command below:
```
arm-linux-gnueabihf-strip libtensorflowlite_c.so
```
- And ensure you are compiling specifically for the ARMv7 architecture to avoid including unnecessary code for other architectures:

```
bazel build --config=elinux_armhf --cpu=armhf //tensorflow/lite:libtensorflowlite_c.so
```
Thank you!

github-actions[bot] on (2024-08-10 01:54:07 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-18 01:56:51 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

saad-koukous (Issue Creator) on (2024-09-04 11:25:27 UTC): Hi @Venkat6871 
I tried using the strip command as you suggested to remove unnecessary symbols from the libtensorflowlite_c.so
However, the size of the library did not change after stripping.

Additionally, I tried compiling the library specifically for the ARMv7 architecture
Unfortunately, the file size remained the same even after these attempts. It seems that there might not be any significant symbols or unnecessary code that can be stripped from the library.

If you have any other suggestions or further optimizations to try, please let me know!

Thank you!

"
2439834021,issue,closed,not_planned,How can I get the android tflite r2.16.1 not stripped so ?,"I use the official aar, I have a crash, I want the symbol table to locate the crash stack",orange-Pai,2024-07-31 11:38:33+00:00,"['arfaian', 'pkgoogle', 'sawantkumar']",2024-11-26 18:07:29+00:00,2024-11-26 18:07:29+00:00,https://github.com/tensorflow/tensorflow/issues/72877,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2262221024, 'issue_id': 2439834021, 'author': 'tilakrayal', 'body': '@nbNobody,\r\nCould you please provide more info/context about the issue like environment which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 8, 1, 7, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262993477, 'issue_id': 2439834021, 'author': 'orange-Pai', 'body': ""@tilakrayal I recently upgraded tensorflow lite from 2.14.0 to 2.16.1, and now there is a crash that didn't occur before. Since there is no symbol table, I can't figure out what went wrong. So I want to know where I can get the so symbol table for release 2.16.1?"", 'created_at': datetime.datetime(2024, 8, 1, 13, 6, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263005770, 'issue_id': 2439834021, 'author': 'orange-Pai', 'body': 'it\'s a SIGSEGV(SEGV_MAPERR)\r\n<img width=""975"" alt=""image"" src=""https://github.com/user-attachments/assets/27d30182-064f-4bac-b762-3418a245fa7a"">', 'created_at': datetime.datetime(2024, 8, 1, 13, 10, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268514816, 'issue_id': 2439834021, 'author': 'sawantkumar', 'body': 'Hi @nbNobody ,\r\n\r\nYou will need to build tflite with debugging symbols enabled. Below command can do the same . Let me know if this solves your issue.\r\n\r\n`bazel build -c dbg //tensorflow/lite:libtensorflowlite.so`', 'created_at': datetime.datetime(2024, 8, 5, 8, 48, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270298821, 'issue_id': 2439834021, 'author': 'orange-Pai', 'body': 'yes, I can build the so by myself.\r\nWhat I want to know is whether TFLite can provide a symbol table? Because I use the official aar provided by TFLite, the symbol table of the so compiled by myself does not match the official one.\r\n@sawantkumar', 'created_at': datetime.datetime(2024, 8, 6, 3, 9, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272678778, 'issue_id': 2439834021, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle , can you please take a look ?', 'created_at': datetime.datetime(2024, 8, 7, 5, 59, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274056593, 'issue_id': 2439834021, 'author': 'pkgoogle', 'body': ""Hi @nbNobody, we don't compile with debug symbols for the official aar for obvious reasons. To accomplish what you want, please build your own custom aar with debug symbols and use that aar when reproducing your issue: https://www.tensorflow.org/lite/android/lite_build. The docker workflow is recommended (to ensure you will not run into environmental issues)\r\n\r\nFor this command:\r\n\r\n```sh\r\nbazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```\r\n\r\nswitch to debug:\r\n```sh\r\nbazel build -c dbg --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```"", 'created_at': datetime.datetime(2024, 8, 7, 18, 16, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274786667, 'issue_id': 2439834021, 'author': 'orange-Pai', 'body': ""Thanks. @pkgoogle \r\nBut I still don't understand why it is not provided. Can you explain it more clearly?"", 'created_at': datetime.datetime(2024, 8, 8, 1, 54, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276389912, 'issue_id': 2439834021, 'author': 'pkgoogle', 'body': 'Hi @nbNobody, in general it\'s not expected to debug w/ the official AAR, it is built for efficiency/optimization... I suppose we can release one with debug symbols but that will require maintenance in documentation. Also if you need it that means you can probably rebuild it, also to test out any changes you would need to rebuild it, so you would have to do something like this anyways. So it doesn\'t seem that productive/useful to include an ""official"" debug symbol AAR.', 'created_at': datetime.datetime(2024, 8, 8, 18, 9, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277489817, 'issue_id': 2439834021, 'author': 'orange-Pai', 'body': 'Thanks. It would be great if Tflite could maintain a symbol table.\r\n@pkgoogle', 'created_at': datetime.datetime(2024, 8, 9, 9, 0, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277489890, 'issue_id': 2439834021, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72877"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72877"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 9, 0, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314312292, 'issue_id': 2439834021, 'author': 'ninh-huynh', 'body': '> Hi @nbNobody, in general it\'s not expected to debug w/ the official AAR, it is built for efficiency/optimization... I suppose we can release one with debug symbols but that will require maintenance in documentation. Also if you need it that means you can probably rebuild it, also to test out any changes you would need to rebuild it, so you would have to do something like this anyways. So it doesn\'t seem that productive/useful to include an ""official"" debug symbol AAR.\r\n\r\nI still do not understand. Why we don\'t include symbols in the release build? If we build with `-c opt` and combine with `--define=tflite_keep_symbols=true`, we can still have efficiency & symbols. I agree that `-c dbg` is not a best choice (Already tried it, and run-time performance decreased a lot) but our users also have a lot crashes with unknown stack trace in firebase crashlytic and the problem just happened when we upgraded to TensorFlow Lite 2.16.1 (we use version 2.9 before)\r\n\r\n\r\nBefore\r\n```\r\nbazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```\r\n\r\nAfter\r\n```\r\nbazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --define=android_dexmerger_tool=d8_dexmerger \\\r\n  --define=android_incremental_dexing_tool=d8_dexbuilder \\\r\n  --define=tflite_keep_symbols=true \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```\r\n\r\n\r\n+CC @tilakrayal, @pkgoogle', 'created_at': datetime.datetime(2024, 8, 28, 4, 53, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315952907, 'issue_id': 2439834021, 'author': 'pkgoogle', 'body': 'Hi @arfaian, can you please take a look ... do you feel we can do this? Thanks.', 'created_at': datetime.datetime(2024, 8, 28, 17, 58, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500211402, 'issue_id': 2439834021, 'author': 'gaikwadrahul8', 'body': ""Hi, @orange-Pai \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/44\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 16, 5, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-08-01 07:14:36 UTC): @nbNobody,
Could you please provide more info/context about the issue like environment which helps to debug the issue. Thank you!

orange-Pai (Issue Creator) on (2024-08-01 13:06:08 UTC): @tilakrayal I recently upgraded tensorflow lite from 2.14.0 to 2.16.1, and now there is a crash that didn't occur before. Since there is no symbol table, I can't figure out what went wrong. So I want to know where I can get the so symbol table for release 2.16.1?

orange-Pai (Issue Creator) on (2024-08-01 13:10:49 UTC): it's a SIGSEGV(SEGV_MAPERR)
<img width=""975"" alt=""image"" src=""https://github.com/user-attachments/assets/27d30182-064f-4bac-b762-3418a245fa7a"">

sawantkumar (Assginee) on (2024-08-05 08:48:32 UTC): Hi @nbNobody ,

You will need to build tflite with debugging symbols enabled. Below command can do the same . Let me know if this solves your issue.

`bazel build -c dbg //tensorflow/lite:libtensorflowlite.so`

orange-Pai (Issue Creator) on (2024-08-06 03:09:28 UTC): yes, I can build the so by myself.
What I want to know is whether TFLite can provide a symbol table? Because I use the official aar provided by TFLite, the symbol table of the so compiled by myself does not match the official one.
@sawantkumar

sawantkumar (Assginee) on (2024-08-07 05:59:47 UTC): Hi @pkgoogle , can you please take a look ?

pkgoogle (Assginee) on (2024-08-07 18:16:50 UTC): Hi @nbNobody, we don't compile with debug symbols for the official aar for obvious reasons. To accomplish what you want, please build your own custom aar with debug symbols and use that aar when reproducing your issue: https://www.tensorflow.org/lite/android/lite_build. The docker workflow is recommended (to ensure you will not run into environmental issues)

For this command:

```sh
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```

switch to debug:
```sh
bazel build -c dbg --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```

orange-Pai (Issue Creator) on (2024-08-08 01:54:04 UTC): Thanks. @pkgoogle 
But I still don't understand why it is not provided. Can you explain it more clearly?

pkgoogle (Assginee) on (2024-08-08 18:09:25 UTC): Hi @nbNobody, in general it's not expected to debug w/ the official AAR, it is built for efficiency/optimization... I suppose we can release one with debug symbols but that will require maintenance in documentation. Also if you need it that means you can probably rebuild it, also to test out any changes you would need to rebuild it, so you would have to do something like this anyways. So it doesn't seem that productive/useful to include an ""official"" debug symbol AAR.

orange-Pai (Issue Creator) on (2024-08-09 09:00:30 UTC): Thanks. It would be great if Tflite could maintain a symbol table.
@pkgoogle

google-ml-butler[bot] on (2024-08-09 09:00:32 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72877"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72877"">No</a>

ninh-huynh on (2024-08-28 04:53:45 UTC): I still do not understand. Why we don't include symbols in the release build? If we build with `-c opt` and combine with `--define=tflite_keep_symbols=true`, we can still have efficiency & symbols. I agree that `-c dbg` is not a best choice (Already tried it, and run-time performance decreased a lot) but our users also have a lot crashes with unknown stack trace in firebase crashlytic and the problem just happened when we upgraded to TensorFlow Lite 2.16.1 (we use version 2.9 before)


Before
```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```

After
```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  --define=tflite_keep_symbols=true \
  //tensorflow/lite/java:tensorflow-lite
```


+CC @tilakrayal, @pkgoogle

pkgoogle (Assginee) on (2024-08-28 17:58:11 UTC): Hi @arfaian, can you please take a look ... do you feel we can do this? Thanks.

gaikwadrahul8 on (2024-11-26 10:16:05 UTC): Hi, @orange-Pai 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/44

Let us know if you have any questions. Thanks.

"
2439591230,issue,open,,TF strings do not work on the GPU as indices for  `tf.gather` / `tf.nn.embedding_lookup`,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

As the [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) documentation suggests, the following code will indeed break on the CPU, but work on the GPU

![image](https://github.com/user-attachments/assets/4c3180fb-92ba-4d34-9518-8af5968f490e)

```py
indices= tf.constant([ 31., 117., 180., 255., 127.,  14.], dtype=tf.float32)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

But this does not to seem to be the case when indices are of type `tf.string` instead:

```py
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

As the op indeed runs on the CPU (see log output)

A better approach is to indeed use a [`StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable), and then lookup the attribute labels:

```py
text_values= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)
text_indices = tf.range(tf.size(text_values), dtype=tf.int32)

text_table = tf.lookup.StaticHashTable(
    initializer = tf.lookup.KeyValueTensorInitializer(
        keys=text_indices,
        values=text_values,
        key_dtype=tf.int32,
        value_dtype=tf.string
    ),
    default_value="""",
)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

text_table.lookup(labels)
```

But I think this should be documented somewhere in `tf.gather` / `tf.nn.embedding_lookup`


Here is a notebook that demonstrates this behaviour:

https://colab.research.google.com/drive/15Ig6Sw39lXRkZ40TvZhs4Aq_v96t-caL?usp=sharing


### Standalone code to reproduce the issue

```shell
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```


### Relevant log output

```shell
(6,)
(1, 100)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-9-e96bf33d6da0> in <cell line: 15>()
     13 print(labels.shape)
     14 
---> 15 tf.nn.embedding_lookup(indices, labels)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5881 def raise_from_not_ok_status(e, name) -> NoReturn:
   5882   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5884 
   5885 

InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,1] = -1 is not in [0, 6) [Op:GatherV2] name:
```
",AndreiMoraru123,2024-07-31 09:35:26+00:00,['Venkat6871'],2024-08-07 06:04:19+00:00,,https://github.com/tensorflow/tensorflow/issues/72873,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2264940943, 'issue_id': 2439591230, 'author': 'Venkat6871', 'body': '- I tried to run your code on Colab using TF v2.17.0, nightly with CPU and GPU. And faced the same issue. Please find the [gist1](https://colab.research.google.com/gist/Venkat6871/1443693954ec6cdb8f6277dbbe1b1ff0/72873_cpu_2-17-0-nightly.ipynb), [gist2](https://colab.research.google.com/gist/Venkat6871/ba3dd740437feef9d3289ce8aebf5bb9/72873_gpu_2-17-v.ipynb) here for reference.\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 9, 17, 5, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 09:17:05 UTC): - I tried to run your code on Colab using TF v2.17.0, nightly with CPU and GPU. And faced the same issue. Please find the [gist1](https://colab.research.google.com/gist/Venkat6871/1443693954ec6cdb8f6277dbbe1b1ff0/72873_cpu_2-17-0-nightly.ipynb), [gist2](https://colab.research.google.com/gist/Venkat6871/ba3dd740437feef9d3289ce8aebf5bb9/72873_gpu_2-17-v.ipynb) here for reference.
Thank you!

"
2438609909,issue,closed,completed,Mmap of '41' at offset '0' failed with error '22'. - pose classification,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was expecting the code to run without errors

### Standalone code to reproduce the issue

```shell
`!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
!git clone https://github.com/tensorflow/examples.git
pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')
sys.path.append(pose_sample_rpi_path)

# Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('movenet_thunder')`
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-8-fcb5b307ea77>](https://kygyo76i4o-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240729-060216_RC00_657091656#) in <cell line: 17>()
     15 from data import BodyPart
     16 from ml import Movenet
---> 17 movenet = Movenet('movenet_thunder')
     18 
     19 # Define function to run pose estimation using MoveNet Thunder.

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py](https://kygyo76i4o-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240729-060216_RC00_657091656#) in __init__(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering)
    462 
    463     if model_path and not model_content:
--> 464       custom_op_registerers_by_name = [
    465           x for x in self._custom_op_registerers if isinstance(x, str)
    466       ]

ValueError: Mmap of '46' at offset '0' failed with error '22'.
```
",ConnorLynchLV8,2024-07-30 20:18:58+00:00,['tilakrayal'],2024-08-18 01:56:56+00:00,2024-08-18 01:56:53+00:00,https://github.com/tensorflow/tensorflow/issues/72802,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2260832555, 'issue_id': 2438609909, 'author': 'tilakrayal', 'body': '@ConnorLynchLV8,\r\nI tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/72202#issuecomment-2245024521\r\nhttps://github.com/tensorflow/tensorflow/issues/70841\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 31, 15, 46, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261171177, 'issue_id': 2438609909, 'author': 'ConnorLynchLV8', 'body': 'Thank you for your response @tilakrayal I was referring to this gist [https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb](url).\r\n\r\nBased on the comment you linked, is the suggested fix to download the model and upload to Colab and use that instead?\r\n\r\nCheers', 'created_at': datetime.datetime(2024, 7, 31, 18, 38, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262365113, 'issue_id': 2438609909, 'author': 'ConnorLynchLV8', 'body': ""I have fixed the problem by downloading google/movenet/tfLite/singlepose-thunder-tflite-float16 from gaggle, however, I now get this error \r\n`---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-47-87cdca3ca58e>](https://j8dka4z0ho-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240730-060116_RC00_657538329#) in <cell line: 1>()\r\n     10   )\r\n     11 \r\n---> 12   preprocessor.process(per_pose_class_limit=None)\r\n\r\n1 frames\r\n[/usr/local/lib/python3.10/dist-packages/numpy/__init__.py](https://j8dka4z0ho-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240730-060116_RC00_657538329#) in __getattr__(attr)\r\n    322 \r\n    323         if attr in __former_attrs__:\r\n--> 324             raise AttributeError(__former_attrs__[attr])\r\n    325 \r\n    326         if attr == 'testing':\r\n\r\nAttributeError: module 'numpy' has no attribute 'str'.\r\n`np.str` was a deprecated alias for the builtin `str`. To avoid this error in existing code, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\r\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\r\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations`\r\n\r\nfor this line of code\r\n`preprocessor.process(per_pose_class_limit=None)`"", 'created_at': datetime.datetime(2024, 8, 1, 8, 33, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265271588, 'issue_id': 2438609909, 'author': 'tilakrayal', 'body': '@ConnorLynchLV8,\r\nGlad the mentioned error was resolved. Also the error ""module \'numpy\' has no attribute \'str\'"" is not related to tensorflow. From the numpy doc, it is mentioned that it was deprecated in numpy 1.20. Please try to check by upgrading the python. Thank you!', 'created_at': datetime.datetime(2024, 8, 2, 12, 35, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935047, 'issue_id': 2438609909, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295061866, 'issue_id': 2438609909, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 18, 1, 56, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295061895, 'issue_id': 2438609909, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72802"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72802"">No</a>', 'created_at': datetime.datetime(2024, 8, 18, 1, 56, 55, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-31 15:46:15 UTC): @ConnorLynchLV8,
I tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.

https://github.com/tensorflow/tensorflow/issues/72202#issuecomment-2245024521
https://github.com/tensorflow/tensorflow/issues/70841

Thank you!

ConnorLynchLV8 (Issue Creator) on (2024-07-31 18:38:34 UTC): Thank you for your response @tilakrayal I was referring to this gist [https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb](url).

Based on the comment you linked, is the suggested fix to download the model and upload to Colab and use that instead?

Cheers

ConnorLynchLV8 (Issue Creator) on (2024-08-01 08:33:43 UTC): I have fixed the problem by downloading google/movenet/tfLite/singlepose-thunder-tflite-float16 from gaggle, however, I now get this error 
`---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-47-87cdca3ca58e>](https://j8dka4z0ho-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240730-060116_RC00_657538329#) in <cell line: 1>()
     10   )
     11 
---> 12   preprocessor.process(per_pose_class_limit=None)

1 frames
[/usr/local/lib/python3.10/dist-packages/numpy/__init__.py](https://j8dka4z0ho-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240730-060116_RC00_657538329#) in __getattr__(attr)
    322 
    323         if attr in __former_attrs__:
--> 324             raise AttributeError(__former_attrs__[attr])
    325 
    326         if attr == 'testing':

AttributeError: module 'numpy' has no attribute 'str'.
`np.str` was a deprecated alias for the builtin `str`. To avoid this error in existing code, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations`

for this line of code
`preprocessor.process(per_pose_class_limit=None)`

tilakrayal (Assginee) on (2024-08-02 12:35:50 UTC): @ConnorLynchLV8,
Glad the mentioned error was resolved. Also the error ""module 'numpy' has no attribute 'str'"" is not related to tensorflow. From the numpy doc, it is mentioned that it was deprecated in numpy 1.20. Please try to check by upgrading the python. Thank you!

github-actions[bot] on (2024-08-10 01:54:09 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-18 01:56:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-18 01:56:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72802"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72802"">No</a>

"
2438381561,issue,closed,completed,Shape Mismatch,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have two computers, one with tf 2.11.0, the other with tf.2.17.0. Using tf 2.11.0, the code given above generates no error message, while using tf 2.17.0, the code generates error message. Can anyone help?

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.keras.constraints import non_neg
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model

# Example input data
X_train_dict = {
    'green_fin_const': np.random.rand(558, 3),
    'green_fin_inst': np.random.rand(558, 4),
    'gov_sup': np.random.rand(558, 5),
    'com_act': np.random.rand(558, 5),
    'eco_city': np.random.rand(558, 4),
    'type': np.random.rand(558, 1)
}
Y_train = np.random.rand(558, 2)

X_test_dict = {
    'green_fin_const': np.random.rand(140, 3),
    'green_fin_inst': np.random.rand(140, 4),
    'gov_sup': np.random.rand(140, 5),
    'com_act': np.random.rand(140, 5),
    'eco_city': np.random.rand(140, 4),
    'type': np.random.rand(140, 1)
}
Y_test = np.random.rand(140, 2)

# Define input layers
inputs_1 = Input(shape=(3,), name='green_fin_const')
inputs_2 = Input(shape=(4,), name='green_fin_inst')
inputs_3 = Input(shape=(5,), name='gov_sup')
inputs_4 = Input(shape=(5,), name='com_act')
inputs_5 = Input(shape=(4,), name='eco_city')
inputs_6 = Input(shape=(1,), name='type')

# Define dense layers
score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1)
score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2)
score_3 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_3)
score_4 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_4)
score_5 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_5)

# Concatenate scores and type input
concatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])

# Define output layer
outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)

# Create the model
model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)

# Plot model architecture
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True)

# Compile model
model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])

# Print input shapes to verify
for key, value in X_train_dict.items():
    print(f'{key}: {value.shape}')

# Fit the model
model.fit(
    x=X_train_dict,
    y=Y_train,
    validation_data=(X_test_dict, Y_test),
    epochs=100,
    batch_size=32,
    verbose=0
)
```


### Relevant log output

```shell
I have two computers, one with tf 2.11.0, the other with tf.2.17.0. Using tf 2.11.0, the code given above generates no error message, while using tf 2.17.0, the code generates error message. Can anyone help?

Here are the details. 
On the one with tf 2.11.0, the output is 
green_fin_const: (558, 3)
green_fin_inst: (558, 4)
gov_sup: (558, 5)
com_act: (558, 5)
eco_city: (558, 4)
type: (558, 1)

On the one with tf 2.17.0, the outcome is 
green_fin_const: (558, 3)
green_fin_inst: (558, 4)
gov_sup: (558, 5)
com_act: (558, 5)
eco_city: (558, 4)
type: (558, 1)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 60
     57     print(f'{key}: {value.shape}')
     59 # Fit the model
---> 60 model.fit(
     61     x=X_train_dict,
     62     y=Y_train,
     63     validation_data=(X_test_dict, Y_test),
     64     epochs=100,
     65     batch_size=32,
     66     verbose=0
     67 )

File ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~\anaconda3\Lib\site-packages\keras\src\layers\input_spec.py:227, in assert_input_compatibility(input_spec, inputs, layer_name)
    222     for axis, value in spec.axes.items():
    223         if value is not None and shape[axis] not in {
    224             value,
    225             None,
    226         }:
--> 227             raise ValueError(
    228                 f'Input {input_index} of layer ""{layer_name}"" is '
    229                 f""incompatible with the layer: expected axis {axis} ""
    230                 f""of input shape to have value {value}, ""
    231                 ""but received input with ""
    232                 f""shape {shape}""
    233             )
    234 # Check shape.
    235 if spec.shape is not None:

ValueError: Exception encountered when calling Functional.call().

Input 0 of layer ""dense"" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 5)

Arguments received by Functional.call():
   inputs={'green_fin_const': 'tf.Tensor(shape=(None, 3), dtype=float32)', 'green_fin_inst': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'gov_sup': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'com_act': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'eco_city': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'type': 'tf.Tensor(shape=(None, 1), dtype=float32)'}
   training=True
   mask={'green_fin_const': 'None', 'green_fin_inst': 'None', 'gov_sup': 'None', 'com_act': 'None', 'eco_city': 'None', 'type': 'None'}

```
",tanwang2020,2024-07-30 17:57:02+00:00,['Venkat6871'],2024-12-27 02:01:34+00:00,2024-12-27 02:01:30+00:00,https://github.com/tensorflow/tensorflow/issues/72797,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2264530640, 'issue_id': 2438381561, 'author': 'Venkat6871', 'body': 'Hi **@tanwang2020** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 4, 35, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265909357, 'issue_id': 2438381561, 'author': 'tanwang2020', 'body': 'I will try to post this issue, but in a slightly different form, as I discovered that a related issue may arise within tf 2.17.0. Below is the code. If this is indeed the case, then I think there might be a bug in tf 2.17.0.\r\nHere is the first version of the code:\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.constraints import non_neg\r\nfrom tensorflow.keras.layers import Input, Dense, concatenate\r\nfrom tensorflow.keras.models import Model\r\nprint(\'TensorFlow Version: \',tf.__version__)\r\n# Example input data\r\nX_train_dict = {\r\n    \'green_fin_const\': np.random.rand(558, 3),\r\n    \'green_fin_inst\': np.random.rand(558, 4),\r\n    \'gov_sup\': np.random.rand(558, 5),\r\n    \'com_act\': np.random.rand(558, 5),\r\n    \'eco_city\': np.random.rand(558, 4),\r\n    \'type\': np.random.rand(558, 1)\r\n}\r\nY_train = np.random.rand(558, 2)\r\n\r\nX_test_dict = {\r\n    \'green_fin_const\': np.random.rand(140, 3),\r\n    \'green_fin_inst\': np.random.rand(140, 4),\r\n    \'gov_sup\': np.random.rand(140, 5),\r\n    \'com_act\': np.random.rand(140, 5),\r\n    \'eco_city\': np.random.rand(140, 4),\r\n    \'type\': np.random.rand(140, 1)\r\n}\r\nY_test = np.random.rand(140, 2)\r\n\r\n# Define input layers\r\ninputs_1 = Input(shape=(3,), name=\'green_fin_const\')\r\ninputs_2 = Input(shape=(4,), name=\'green_fin_inst\')\r\ninputs_3 = Input(shape=(5,), name=\'gov_sup\')\r\ninputs_4 = Input(shape=(5,), name=\'com_act\')\r\ninputs_5 = Input(shape=(4,), name=\'eco_city\')\r\ninputs_6 = Input(shape=(1,), name=\'type\')\r\n\r\n# Define dense layers\r\nscore_1 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_1)\r\nscore_2 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_2)\r\nscore_3 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_3)\r\nscore_4 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_4)\r\nscore_5 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_5)\r\n\r\n# Concatenate scores and type input\r\nconcatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])\r\n\r\n# Define output layer\r\noutputs = Dense(2, activation=\'softmax\', kernel_constraint=non_neg())(concatenated_scores)\r\n\r\n# Create the model\r\nmodel = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)\r\n\r\n\r\n# Compile model\r\nmodel.compile(optimizer=\'nadam\', loss=\'mse\', metrics=[\'KLDivergence\'])\r\n\r\n# Fit the model\r\nmodel.fit(\r\n    x=X_train_dict,\r\n    y=Y_train,\r\n    validation_data=(X_test_dict, Y_test),\r\n    epochs=100,\r\n    batch_size=32,\r\n    verbose=0\r\n)\r\n# Print input shapes to verify\r\nfor key, value in X_train_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\nfor key, value in X_test_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\n\r\nmodel.summary()\r\n\r\nThe message from the system is:\r\nTensorFlow Version:  2.17.0\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[6], line 57\r\n     54 model.compile(optimizer=\'nadam\', loss=\'mse\', metrics=[\'KLDivergence\'])\r\n     56 # Fit the model\r\n---> 57 model.fit(\r\n     58     x=X_train_dict,\r\n     59     y=Y_train,\r\n     60     validation_data=(X_test_dict, Y_test),\r\n     61     epochs=100,\r\n     62     batch_size=32,\r\n     63     verbose=0\r\n     64 )\r\n     65 # Print input shapes to verify\r\n     66 for key, value in X_train_dict.items():\r\n\r\nFile ~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nFile ~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227, in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    222     for axis, value in spec.axes.items():\r\n    223         if value is not None and shape[axis] not in {\r\n    224             value,\r\n    225             None,\r\n    226         }:\r\n--> 227             raise ValueError(\r\n    228                 f\'Input {input_index} of layer ""{layer_name}"" is \'\r\n    229                 f""incompatible with the layer: expected axis {axis} ""\r\n    230                 f""of input shape to have value {value}, ""\r\n    231                 ""but received input with ""\r\n    232                 f""shape {shape}""\r\n    233             )\r\n    234 # Check shape.\r\n    235 if spec.shape is not None:\r\n\r\nValueError: Exception encountered when calling Functional.call().\r\n\r\nInput 0 of layer ""dense_27"" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 5)\r\n\r\nArguments received by Functional.call():\r\n   inputs={\'green_fin_const\': \'tf.Tensor(shape=(None, 3), dtype=float32)\', \'green_fin_inst\': \'tf.Tensor(shape=(None, 4), dtype=float32)\', \'gov_sup\': \'tf.Tensor(shape=(None, 5), dtype=float32)\', \'com_act\': \'tf.Tensor(shape=(None, 5), dtype=float32)\', \'eco_city\': \'tf.Tensor(shape=(None, 4), dtype=float32)\', \'type\': \'tf.Tensor(shape=(None, 1), dtype=float32)\'}\r\n   training=True\r\n   mask={\'green_fin_const\': \'None\', \'green_fin_inst\': \'None\', \'gov_sup\': \'None\', \'com_act\': \'None\', \'eco_city\': \'None\', \'type\': \'None\'}\r\n  \r\n  Here is the second version of the code. In this version, I made only one change. I changed the name of the keys to shorter ones and the corresponding names in the specification of the inputs of the model. Now the program works. There is no error message.\r\n  import numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.constraints import non_neg\r\nfrom tensorflow.keras.layers import Input, Dense, concatenate\r\nfrom tensorflow.keras.models import Model\r\nprint(\'TensorFlow Version: \',tf.__version__)\r\n# Example input data\r\n\r\n\r\nX_train_dict = {\r\n    \'A\': np.random.rand(558, 3),\r\n    \'B\': np.random.rand(558, 4),\r\n    \'C\': np.random.rand(558, 5),\r\n    \'D\': np.random.rand(558, 5),\r\n    \'E\': np.random.rand(558, 4),\r\n    \'T\': np.random.rand(558, 1)\r\n}\r\nY_train = np.random.rand(558, 2)\r\n\r\nX_test_dict = {\r\n    \'A\': np.random.rand(140, 3),\r\n    \'B\': np.random.rand(140, 4),\r\n    \'C\': np.random.rand(140, 5),\r\n    \'D\': np.random.rand(140, 5),\r\n    \'E\': np.random.rand(140, 4),\r\n    \'T\': np.random.rand(140, 1)\r\n}\r\nY_test = np.random.rand(140, 2)\r\n\r\n# Define input layers\r\ninputs_1 = Input(shape=(3,), name=\'A\')\r\ninputs_2 = Input(shape=(4,), name=\'B\')\r\ninputs_3 = Input(shape=(5,), name=\'C\')\r\ninputs_4 = Input(shape=(5,), name=\'D\')\r\ninputs_5 = Input(shape=(4,), name=\'E\')\r\ninputs_6 = Input(shape=(1,), name=\'T\')\r\n\r\n# Define dense layers\r\nscore_1 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_1)\r\nscore_2 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_2)\r\nscore_3 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_3)\r\nscore_4 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_4)\r\nscore_5 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_5)\r\n\r\n# Concatenate scores and type input\r\nconcatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])\r\n\r\n# Define output layer\r\noutputs = Dense(2, activation=\'softmax\', kernel_constraint=non_neg())(concatenated_scores)\r\n\r\n# Create the model\r\nmodel = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)\r\n\r\n# Compile model\r\nmodel.compile(optimizer=\'nadam\', loss=\'mse\', metrics=[\'KLDivergence\'])\r\n\r\n# Fit the model\r\nmodel.fit(\r\n    x=X_train_dict,\r\n    y=Y_train,\r\n    validation_data=(X_test_dict, Y_test),\r\n    epochs=100,\r\n    batch_size=32,\r\n    verbose=0\r\n)\r\n# Print input shapes to verify\r\nfor key, value in X_train_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\nfor key, value in X_test_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\n\r\nmodel.summary()\r\n\r\nI also tried a third version, another change from the first version. In this version, I did not change the lengths of the names of keys, but reduced the number of inputs and hence layers. The code also worked without generating error. Here is the code:\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.constraints import non_neg\r\nfrom tensorflow.keras.layers import Input, Dense, concatenate\r\nfrom tensorflow.keras.models import Model\r\nprint(\'TensorFlow Version: \',tf.__version__)\r\n\r\n# Example input data\r\nX_train_dict = {\r\n    \'green_fin_const\': np.random.rand(558, 3),\r\n    \'green_fin_inst\': np.random.rand(558, 4)\r\n}\r\nY_train = np.random.rand(558, 2)\r\n\r\nX_test_dict = {\r\n    \'green_fin_const\': np.random.rand(140, 3),\r\n    \'green_fin_inst\': np.random.rand(140, 4)\r\n}\r\nY_test = np.random.rand(140, 2)\r\n\r\n# Define input layers\r\ninputs_1 = Input(shape=(3,), name=\'green_fin_const\')\r\ninputs_2 = Input(shape=(4,), name=\'green_fin_inst\')\r\n\r\n\r\n# Define dense layers\r\nscore_1 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_1)\r\nscore_2 = Dense(1, activation=\'sigmoid\', kernel_constraint=non_neg())(inputs_2)\r\n\r\n\r\n# Concatenate scores and type input\r\nconcatenated_scores = concatenate([score_1, score_2])\r\n\r\n# Define output layer\r\noutputs = Dense(2, activation=\'softmax\', kernel_constraint=non_neg())(concatenated_scores)\r\n\r\n# Create the model\r\nmodel = Model(inputs=[inputs_1, inputs_2], outputs=outputs)\r\n\r\n\r\n# Compile model\r\nmodel.compile(optimizer=\'nadam\', loss=\'mse\', metrics=[\'KLDivergence\'])\r\n\r\n# Print input shapes to verify\r\nfor key, value in X_train_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\nfor key, value in X_test_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\n\r\nmodel.summary()\r\n# Fit the model\r\nmodel.fit(\r\n    x=X_train_dict,\r\n    y=Y_train,\r\n    validation_data=(X_test_dict, Y_test),\r\n    epochs=100,\r\n    batch_size=32,\r\n    verbose=0\r\n)\r\n\r\n# Print input shapes to verify\r\nfor key, value in X_train_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\nfor key, value in X_test_dict.items():\r\n    print(f\'{key}: {value.shape}\')\r\n\r\nmodel.summary()', 'created_at': datetime.datetime(2024, 8, 2, 18, 16, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535821790, 'issue_id': 2438381561, 'author': 'Venkat6871', 'body': 'Hi **@tanwang2020** ,\r\nApologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.18 and observed the same issue as you. As an alternative, I tried using different shapes, and it worked fine. I hope this will be helpful for you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/523a8b54fdbc14b4e53ce49cdc2affd0/72797_tf_2-18-0-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 11, 12, 11, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552621786, 'issue_id': 2438381561, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 12, 19, 2, 5, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233495, 'issue_id': 2438381561, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563233523, 'issue_id': 2438381561, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72797"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72797"">No</a>', 'created_at': datetime.datetime(2024, 12, 27, 2, 1, 33, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 04:35:12 UTC): Hi **@tanwang2020** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

tanwang2020 (Issue Creator) on (2024-08-02 18:16:32 UTC): I will try to post this issue, but in a slightly different form, as I discovered that a related issue may arise within tf 2.17.0. Below is the code. If this is indeed the case, then I think there might be a bug in tf 2.17.0.
Here is the first version of the code:
import numpy as np
import tensorflow as tf
from tensorflow.keras.constraints import non_neg
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model
print('TensorFlow Version: ',tf.__version__)
# Example input data
X_train_dict = {
    'green_fin_const': np.random.rand(558, 3),
    'green_fin_inst': np.random.rand(558, 4),
    'gov_sup': np.random.rand(558, 5),
    'com_act': np.random.rand(558, 5),
    'eco_city': np.random.rand(558, 4),
    'type': np.random.rand(558, 1)
}
Y_train = np.random.rand(558, 2)

X_test_dict = {
    'green_fin_const': np.random.rand(140, 3),
    'green_fin_inst': np.random.rand(140, 4),
    'gov_sup': np.random.rand(140, 5),
    'com_act': np.random.rand(140, 5),
    'eco_city': np.random.rand(140, 4),
    'type': np.random.rand(140, 1)
}
Y_test = np.random.rand(140, 2)

# Define input layers
inputs_1 = Input(shape=(3,), name='green_fin_const')
inputs_2 = Input(shape=(4,), name='green_fin_inst')
inputs_3 = Input(shape=(5,), name='gov_sup')
inputs_4 = Input(shape=(5,), name='com_act')
inputs_5 = Input(shape=(4,), name='eco_city')
inputs_6 = Input(shape=(1,), name='type')

# Define dense layers
score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1)
score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2)
score_3 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_3)
score_4 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_4)
score_5 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_5)

# Concatenate scores and type input
concatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])

# Define output layer
outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)

# Create the model
model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)


# Compile model
model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])

# Fit the model
model.fit(
    x=X_train_dict,
    y=Y_train,
    validation_data=(X_test_dict, Y_test),
    epochs=100,
    batch_size=32,
    verbose=0
)
# Print input shapes to verify
for key, value in X_train_dict.items():
    print(f'{key}: {value.shape}')
for key, value in X_test_dict.items():
    print(f'{key}: {value.shape}')

model.summary()

The message from the system is:
TensorFlow Version:  2.17.0
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[6], line 57
     54 model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])
     56 # Fit the model
---> 57 model.fit(
     58     x=X_train_dict,
     59     y=Y_train,
     60     validation_data=(X_test_dict, Y_test),
     61     epochs=100,
     62     batch_size=32,
     63     verbose=0
     64 )
     65 # Print input shapes to verify
     66 for key, value in X_train_dict.items():

File ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~\anaconda3\Lib\site-packages\keras\src\layers\input_spec.py:227, in assert_input_compatibility(input_spec, inputs, layer_name)
    222     for axis, value in spec.axes.items():
    223         if value is not None and shape[axis] not in {
    224             value,
    225             None,
    226         }:
--> 227             raise ValueError(
    228                 f'Input {input_index} of layer ""{layer_name}"" is '
    229                 f""incompatible with the layer: expected axis {axis} ""
    230                 f""of input shape to have value {value}, ""
    231                 ""but received input with ""
    232                 f""shape {shape}""
    233             )
    234 # Check shape.
    235 if spec.shape is not None:

ValueError: Exception encountered when calling Functional.call().

Input 0 of layer ""dense_27"" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 5)

Arguments received by Functional.call():
   inputs={'green_fin_const': 'tf.Tensor(shape=(None, 3), dtype=float32)', 'green_fin_inst': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'gov_sup': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'com_act': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'eco_city': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'type': 'tf.Tensor(shape=(None, 1), dtype=float32)'}
   training=True
   mask={'green_fin_const': 'None', 'green_fin_inst': 'None', 'gov_sup': 'None', 'com_act': 'None', 'eco_city': 'None', 'type': 'None'}
  
  Here is the second version of the code. In this version, I made only one change. I changed the name of the keys to shorter ones and the corresponding names in the specification of the inputs of the model. Now the program works. There is no error message.
  import numpy as np
import tensorflow as tf
from tensorflow.keras.constraints import non_neg
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model
print('TensorFlow Version: ',tf.__version__)
# Example input data


X_train_dict = {
    'A': np.random.rand(558, 3),
    'B': np.random.rand(558, 4),
    'C': np.random.rand(558, 5),
    'D': np.random.rand(558, 5),
    'E': np.random.rand(558, 4),
    'T': np.random.rand(558, 1)
}
Y_train = np.random.rand(558, 2)

X_test_dict = {
    'A': np.random.rand(140, 3),
    'B': np.random.rand(140, 4),
    'C': np.random.rand(140, 5),
    'D': np.random.rand(140, 5),
    'E': np.random.rand(140, 4),
    'T': np.random.rand(140, 1)
}
Y_test = np.random.rand(140, 2)

# Define input layers
inputs_1 = Input(shape=(3,), name='A')
inputs_2 = Input(shape=(4,), name='B')
inputs_3 = Input(shape=(5,), name='C')
inputs_4 = Input(shape=(5,), name='D')
inputs_5 = Input(shape=(4,), name='E')
inputs_6 = Input(shape=(1,), name='T')

# Define dense layers
score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1)
score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2)
score_3 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_3)
score_4 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_4)
score_5 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_5)

# Concatenate scores and type input
concatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])

# Define output layer
outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)

# Create the model
model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)

# Compile model
model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])

# Fit the model
model.fit(
    x=X_train_dict,
    y=Y_train,
    validation_data=(X_test_dict, Y_test),
    epochs=100,
    batch_size=32,
    verbose=0
)
# Print input shapes to verify
for key, value in X_train_dict.items():
    print(f'{key}: {value.shape}')
for key, value in X_test_dict.items():
    print(f'{key}: {value.shape}')

model.summary()

I also tried a third version, another change from the first version. In this version, I did not change the lengths of the names of keys, but reduced the number of inputs and hence layers. The code also worked without generating error. Here is the code:
import numpy as np
import tensorflow as tf
from tensorflow.keras.constraints import non_neg
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model
print('TensorFlow Version: ',tf.__version__)

# Example input data
X_train_dict = {
    'green_fin_const': np.random.rand(558, 3),
    'green_fin_inst': np.random.rand(558, 4)
}
Y_train = np.random.rand(558, 2)

X_test_dict = {
    'green_fin_const': np.random.rand(140, 3),
    'green_fin_inst': np.random.rand(140, 4)
}
Y_test = np.random.rand(140, 2)

# Define input layers
inputs_1 = Input(shape=(3,), name='green_fin_const')
inputs_2 = Input(shape=(4,), name='green_fin_inst')


# Define dense layers
score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1)
score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2)


# Concatenate scores and type input
concatenated_scores = concatenate([score_1, score_2])

# Define output layer
outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)

# Create the model
model = Model(inputs=[inputs_1, inputs_2], outputs=outputs)


# Compile model
model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])

# Print input shapes to verify
for key, value in X_train_dict.items():
    print(f'{key}: {value.shape}')
for key, value in X_test_dict.items():
    print(f'{key}: {value.shape}')

model.summary()
# Fit the model
model.fit(
    x=X_train_dict,
    y=Y_train,
    validation_data=(X_test_dict, Y_test),
    epochs=100,
    batch_size=32,
    verbose=0
)

# Print input shapes to verify
for key, value in X_train_dict.items():
    print(f'{key}: {value.shape}')
for key, value in X_test_dict.items():
    print(f'{key}: {value.shape}')

model.summary()

Venkat6871 (Assginee) on (2024-12-11 12:11:16 UTC): Hi **@tanwang2020** ,
Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.18 and observed the same issue as you. As an alternative, I tried using different shapes, and it worked fine. I hope this will be helpful for you. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/523a8b54fdbc14b4e53ce49cdc2affd0/72797_tf_2-18-0-v.ipynb) here for reference.

Thank you!

github-actions[bot] on (2024-12-19 02:05:52 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-12-27 02:01:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-12-27 02:01:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72797"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72797"">No</a>

"
2438064438,issue,open,,Sparse segment mean/sum gives random result on empty tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.1

### Custom code

No

### OS platform and distribution

Linux GPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When inputs to `math_ops.sparse_segment_mean` or `math_ops.sparse_segment_sum` are empty. The gradient of the OP is not 0, but some random values from previous tensors. The gradient should be zero when inputs are empty

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import timeline
from tensorflow.python.ops import math_ops

tf.compat.v1.disable_eager_execution()


def construct_model():
  embed_dim = 128
  cache_size = 2**18 + 1
  with tf.device(""/GPU:0""):
    params = tf.compat.v1.Variable(
        shape=(cache_size, embed_dim),
        dtype=tf.float32,
        initial_value=tf.ones(shape=(cache_size, embed_dim), dtype=tf.float32))

  ids = tf.constant([], dtype=tf.int32)
  segment_ids = tf.constant([], dtype=tf.int32)

  embed_pooling = math_ops.sparse_segment_sum(
      params,
      ids,
      segment_ids,
      num_segments=8192,
      name=""sss"",
  )

  #print(embed_pooling)
  loss = tf.reduce_sum(embed_pooling)
  trainable_vars = tf.compat.v1.trainable_variables()
  grads = tf.gradients(loss, trainable_vars)
  grads_and_vars = [(g, v) for g, v in zip(grads, trainable_vars)]

  sanity_check_ops = [
      #tf.print(grads[0]),
      tf.debugging.check_numerics(grads[0], message=""""),
      tf.debugging.assert_equal(tf.reduce_sum(grads[0]), tf.constant(0.)),
      #CRITICAL step to reproduce
      tf.random.uniform((81920, 1280), 100, 101)
  ]
  adam_opt = tf.compat.v1.train.AdamOptimizer(
      learning_rate=0.001, beta1=0.8, beta2=0.88)
  with tf.control_dependencies(sanity_check_ops):

    step = adam_opt.apply_gradients(grads_and_vars)

  return step


tf.config.threading.set_inter_op_parallelism_threads(32)
tf.config.threading.set_intra_op_parallelism_threads(32)
run_op = construct_model()

profile_options = tf.compat.v1.RunOptions(
    trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
run_metadata = tf.compat.v1.RunMetadata()

with tf.compat.v1.Session() as sess:
  sess.run(tf.compat.v1.global_variables_initializer())
  print(sess._config)

  for i in range(10):
    sess.run(run_op, run_metadata=run_metadata)
```


### Relevant log output

```shell
2024-07-30 14:39:23.458752: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-30 14:39:23.505787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-30 14:39:25.698665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e1:00.0, compute capability: 8.0
2024-07-30 14:39:25.701973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2024-07-30 14:39:27.165519: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]
device_count {
  key: ""CPU""
  value: 1
}
device_count {
  key: ""GPU""
  value: 1
}
intra_op_parallelism_threads: 32
inter_op_parallelism_threads: 32
gpu_options {
  visible_device_list: ""0""
  experimental {
  }
}
experimental {
}

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/t.py"", line 65, in <module>
    sess.run(run_op, run_metadata=run_metadata)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 968, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{node Assert}}]]
```
",372046933,2024-07-30 15:03:03+00:00,['tilakrayal'],2024-08-07 08:57:57+00:00,,https://github.com/tensorflow/tensorflow/issues/72785,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2264879030, 'issue_id': 2438064438, 'author': 'tilakrayal', 'body': '@372046933,\r\nCould you please let us know if there is any specific context to use **tf.compat.v1** in the above code. Also sess.run is also not available in the latest tensorflow version. Thank you!', 'created_at': datetime.datetime(2024, 8, 2, 8, 44, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264957096, 'issue_id': 2438064438, 'author': '372046933', 'body': '@tilakrayal What do you mean by latest? I tested it on 2.17 and session works.', 'created_at': datetime.datetime(2024, 8, 2, 9, 26, 1, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-08-02 08:44:15 UTC): @372046933,
Could you please let us know if there is any specific context to use **tf.compat.v1** in the above code. Also sess.run is also not available in the latest tensorflow version. Thank you!

372046933 (Issue Creator) on (2024-08-02 09:26:01 UTC): @tilakrayal What do you mean by latest? I tested it on 2.17 and session works.

"
2437838158,issue,closed,completed,Tensorflow Issue,"I would like to be assisted in the issue I arise on when running tensorflow



### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf24.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ImportError                               Traceback (most recent call last)
File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[4], line 1
----> 1 import tensorflow as tf
      2 print(tf.__version__)

File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\Luvolwethu Tokwe\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

### Standalone code to reproduce the issue

```shell
ImportError                               Traceback (most recent call last)
File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[4], line 1
----> 1 import tensorflow as tf
      2 print(tf.__version__)

File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\__init__.py:38
     35 import sys as _sys
     37 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 38 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     39 from tensorflow.python.tools import module_util as _module_util
     40 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\Luvolwethu Tokwe\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```


### Relevant log output

_No response_",LuvolwethuTokwe,2024-07-30 13:24:57+00:00,['Venkat6871'],2024-08-08 13:47:28+00:00,2024-08-08 13:47:24+00:00,https://github.com/tensorflow/tensorflow/issues/72782,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.4', 'for issues related to TF 2.4')]","[{'comment_id': 2259763591, 'issue_id': 2437838158, 'author': 'Venkat6871', 'body': 'Hi **@LuvolwethuTokwe** ,\r\n- Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from the issue with similar error.It helps.Also in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 31, 6, 25, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274787035, 'issue_id': 2437838158, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 8, 1, 54, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275878888, 'issue_id': 2437838158, 'author': 'mihaimaruseac', 'body': 'Duplicate of #36167', 'created_at': datetime.datetime(2024, 8, 8, 13, 47, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275878970, 'issue_id': 2437838158, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72782"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72782"">No</a>', 'created_at': datetime.datetime(2024, 8, 8, 13, 47, 26, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-31 06:25:03 UTC): Hi **@LuvolwethuTokwe** ,
- Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from the issue with similar error.It helps.Also in order to expedite the trouble-shooting process, could you please provide the following information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
Installed using virtualenv? pip? conda?:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
and the exact sequence of commands / steps that you executed before running into the problem.

Thank you!

github-actions[bot] on (2024-08-08 01:54:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

mihaimaruseac on (2024-08-08 13:47:24 UTC): Duplicate of #36167

google-ml-butler[bot] on (2024-08-08 13:47:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72782"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72782"">No</a>

"
2437079769,issue,closed,completed,Can not provide a sparse tensor to calculate ctc loss in TnesorFlow2,"When I use TensorFlow2 to train a CTC model like this:

`ctc_loss = tf.nn.ctc_loss(labels=ys, logits=ctc_logits, label_length=ylen, logit_length=xlen, blank_index=-1)`

It's ok, but the train speed is very slow compared with TensorFlow1 environment with the same train batch size (ablout 2~3 times slowerwhat's morethe GPU memory usage increase so huge that the train batch size must be set to very small. So, I try to replace the input dense label with sparse tensor like this:

`ys_sparse = tf.sparse.from_dense(ys)`
`ctc_loss = tf.nn.ctc_loss(labels=ys_sparse, logits=ctc_logits, label_length=None, logit_length=xlen, blank_index=-1)`

The train speed is much faster and the GPU memory usage will significant decreased. But I find another problem, in this condition, the normal training bath size can only be set to 1, when the batch size increase to 2, the training network will deteriorate rapidlyand the output logits will become nanthe ctc_loss will become nan tooand the training can't continue anymore. What's wrongIs there any solution

![1722323201749](https://github.com/user-attachments/assets/e45ecce2-7583-46cf-85df-73ae4a2664ca)

![image](https://github.com/user-attachments/assets/8064c23b-43c4-40c2-a29e-1d7e398fb523)
",yjiangling,2024-07-30 07:10:50+00:00,['tilakrayal'],2024-08-16 01:54:04+00:00,2024-08-16 01:54:01+00:00,https://github.com/tensorflow/tensorflow/issues/72755,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues')]","[{'comment_id': 2259801340, 'issue_id': 2437079769, 'author': 'tilakrayal', 'body': '@yjiangling,\r\nCould you please share the code in the reproducible format or the colab gist & the tensorflow version you are using which helps to analyse the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 31, 6, 52, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274787063, 'issue_id': 2437079769, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 8, 1, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590428, 'issue_id': 2437079769, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 16, 1, 54, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590480, 'issue_id': 2437079769, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72755"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72755"">No</a>', 'created_at': datetime.datetime(2024, 8, 16, 1, 54, 3, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-31 06:52:49 UTC): @yjiangling,
Could you please share the code in the reproducible format or the colab gist & the tensorflow version you are using which helps to analyse the issue in an effective way. Thank you!

github-actions[bot] on (2024-08-08 01:54:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-16 01:54:01 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-16 01:54:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72755"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72755"">No</a>

"
2435705683,issue,closed,completed,`tf.bitwise.left_shift`'s behavior is different on cpu and gpu on tensorflow 2.18.0-dev20240728,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-113709-gdc368f6cbd8 2.18.0-dev20240728

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.11.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_12.4.r12.4/compiler.34097967_0

### GPU model and memory

_No response_

### Current behavior?

`tf.bitwise.left_shift`'s behavior is different on cpu and gpu.

on cpu:
```python
with tf.device(""/cpu:0""):
    x = tf.constant([-2, -1, 0, 1, 2])
    y = tf.constant([3, 2, 1, 0, -1])
    z = tf.bitwise.left_shift(x, y)
    print(z.numpy()) # [-16  -4   0   1   2]
```

on gpu:
```python
with tf.device(""/gpu:0""):
    x = tf.constant([-2, -1, 0, 1, 2])
    y = tf.constant([3, 2, 1, 0, -1])
    z = tf.bitwise.left_shift(x, y)
    print(z.numpy())
```

`z` on cpu and gpu are not equal.

### Standalone code to reproduce the issue

```shell
with tf.device(""/cpu:0""):
    x = tf.constant([-2, -1, 0, 1, 2])
    y = tf.constant([3, 2, 1, 0, -1])
    z = tf.bitwise.left_shift(x, y)
    print(z.numpy()) # [-16  -4   0   1   2]

with tf.device(""/gpu:0""):
    x = tf.constant([-2, -1, 0, 1, 2])
    y = tf.constant([3, 2, 1, 0, -1])
    z = tf.bitwise.left_shift(x, y)
    print(z.numpy()) # [-16  -4   0   1   0]
```


### Relevant log output

_No response_",wangzhen0518,2024-07-29 15:18:49+00:00,['tilakrayal'],2024-09-02 16:31:31+00:00,2024-09-02 16:31:28+00:00,https://github.com/tensorflow/tensorflow/issues/72709,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2259983213, 'issue_id': 2435705683, 'author': '372046933', 'body': 'Maybe not a bug, because\r\n> If `y` is negative, or greater than or equal to the width of `x` in bits the\r\nresult is implementation defined.', 'created_at': datetime.datetime(2024, 7, 31, 8, 45, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259984791, 'issue_id': 2435705683, 'author': 'tilakrayal', 'body': '@wangzhen0518,\r\nI was able to reproduce the issue on tensorflow 2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3da0435d63f8a53a33bf281d2cbf4469/untitled2035.ipynb).', 'created_at': datetime.datetime(2024, 7, 31, 8, 46, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264701388, 'issue_id': 2435705683, 'author': 'tilakrayal', 'body': '@wangzhen0518,\r\nAs per the document it is mentioned that **If y is negative, or greater than or equal to the width of x in bits the result is implementation defined**, also I checked with alternative inputs for the **y** and observed when the y is +ve, it is providing the same result and when **y** is -ve, it provided the same result as mentioned.\r\nhttps://www.tensorflow.org/api_docs/python/tf/bitwise/left_shift\r\n\r\nKindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/73907c1d487ba220a26f9fa3547d64e8/untitled2040.ipynb) for the reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 6, 59, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935074, 'issue_id': 2435705683, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2280708198, 'issue_id': 2435705683, 'author': 'wangzhen0518', 'body': 'Thanks for your explanation. I am curious about whether it is necessary to maintain consistent results across different backends.', 'created_at': datetime.datetime(2024, 8, 10, 9, 52, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324282593, 'issue_id': 2435705683, 'author': 'tilakrayal', 'body': '@wangzhen0518,\r\nThe reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 9, 37, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325075993, 'issue_id': 2435705683, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72709"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72709"">No</a>', 'created_at': datetime.datetime(2024, 9, 2, 16, 31, 30, tzinfo=datetime.timezone.utc)}]","372046933 on (2024-07-31 08:45:43 UTC): Maybe not a bug, because
result is implementation defined.

tilakrayal (Assginee) on (2024-07-31 08:46:39 UTC): @wangzhen0518,
I was able to reproduce the issue on tensorflow 2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3da0435d63f8a53a33bf281d2cbf4469/untitled2035.ipynb).

tilakrayal (Assginee) on (2024-08-02 06:59:30 UTC): @wangzhen0518,
As per the document it is mentioned that **If y is negative, or greater than or equal to the width of x in bits the result is implementation defined**, also I checked with alternative inputs for the **y** and observed when the y is +ve, it is providing the same result and when **y** is -ve, it provided the same result as mentioned.
https://www.tensorflow.org/api_docs/python/tf/bitwise/left_shift

Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/73907c1d487ba220a26f9fa3547d64e8/untitled2040.ipynb) for the reference.

Thank you!

github-actions[bot] on (2024-08-10 01:54:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

wangzhen0518 (Issue Creator) on (2024-08-10 09:52:14 UTC): Thanks for your explanation. I am curious about whether it is necessary to maintain consistent results across different backends.

tilakrayal (Assginee) on (2024-09-02 09:37:39 UTC): @wangzhen0518,
The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. Thank you!

google-ml-butler[bot] on (2024-09-02 16:31:30 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72709"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72709"">No</a>

"
2435124855,issue,closed,completed,keras functional api fit() with tf.data.Dataset for validation data is resulting in an error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

Kaggle Notebook

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

12.3.0

### CUDA/cuDNN version

12.1.105

### GPU model and memory

_No response_

### Current behavior?

Current behavior: AttributeError: 'NoneType' object has no attribute 'items'.
Expected behavior: standard fit() function returning history object. The code works without val_dataset 
train_dataset is <_TakeDataset element_spec=((TensorSpec(shape=(None, 73), dtype=tf.float64, name=None), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>
val_dataset is <_TakeDataset element_spec=((TensorSpec(shape=(None, 73), dtype=tf.float64, name=None), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>
test_dataset is <_PrefetchDataset element_spec=((TensorSpec(shape=(None, 73), dtype=tf.float64, name=None), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>


### Standalone code to reproduce the issue
[Kaggle Notebook](https://www.kaggle.com/code/sabyrbazarymbetov/multi-input-effnetv2-feature-extraction-dnn)

### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1722248226.794089      68 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
W0000 00:00:1722248226.850691      68 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
4386/4386  0s 78ms/step - accuracy: 0.9955 - loss: 0.2826
/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.
  self.gen.throw(typ, value, traceback)

File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:354, in TensorFlowTrainer.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)
    333         self._eval_epoch_iterator = TFEpochIterator(
    334             x=val_x,
    335             y=val_y,
   (...)
    341             shuffle=False,
    342         )
    343     val_logs = self.evaluate(
    344         x=val_x,
    345         y=val_y,
   (...)
    351         _use_cached_eval_dataset=True,
    352     )
    353     val_logs = {
--> 354         ""val_"" + name: val for name, val in val_logs.items()
    355     }
    356     epoch_logs.update(val_logs)
    358 callbacks.on_epoch_end(epoch, epoch_logs)

AttributeError: 'NoneType' object has no attribute 'items'
```
",just-sabyr,2024-07-29 11:08:17+00:00,['Venkat6871'],2024-11-02 02:01:16+00:00,2024-11-02 02:01:13+00:00,https://github.com/tensorflow/tensorflow/issues/72696,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2264517135, 'issue_id': 2435124855, 'author': 'Venkat6871', 'body': 'Hi **@just-sabyr** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 2, 4, 22, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935082, 'issue_id': 2435124855, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439174141, 'issue_id': 2435124855, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 26, 2, 0, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797283, 'issue_id': 2435124855, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452797306, 'issue_id': 2435124855, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72696"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72696"">No</a>', 'created_at': datetime.datetime(2024, 11, 2, 2, 1, 15, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-08-02 04:22:59 UTC): Hi **@just-sabyr** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

github-actions[bot] on (2024-08-10 01:54:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-26 02:00:12 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-02 02:01:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-02 02:01:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72696"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72696"">No</a>

"
2434679437,issue,closed,completed,MirroredStrategy() getting stuck at optimizer level when calling apply_gradients with tf2.16.2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.2

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22, MacOS M1, MacOS Intel chip

### Mobile device

_No response_

### Python version

3.11

### Bazel version

7.2.1

### GCC/compiler version

14.0.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The code will get stuck when applying gradients with the optimizer with both `tf.keras.optimizers.legacy.Adam` and `tf.keras.optimizers.Adam`

### Standalone code to reproduce the issue

```shell
# You can reproduce the problem by running the example in the documentation https://www.tensorflow.org/tutorials/distribute/custom_training
# Import TensorFlow
import os

# Helper libraries
import numpy as np
import tensorflow as tf

print(tf.__version__)


def main():
    fashion_mnist = tf.keras.datasets.fashion_mnist

    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

    # Add a dimension to the array -> new shape == (28, 28, 1)
    # This is done because the first layer in our model is a convolutional
    # layer and it requires a 4D input (batch_size, height, width, channels).
    # batch_size dimension will be added later on.
    train_images = train_images[..., None]
    test_images = test_images[..., None]

    # Scale the images to the [0, 1] range.
    train_images = train_images / np.float32(255)
    test_images = test_images / np.float32(255)

    # If the list of devices is not specified in
    # `tf.distribute.MirroredStrategy` constructor, they will be auto-detected.
    strategy = tf.distribute.MirroredStrategy([""/cpu:0"", ""/cpu:1""])
    print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

    BUFFER_SIZE = len(train_images)

    BATCH_SIZE_PER_REPLICA = 64
    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

    EPOCHS = 10

    train_dataset = (
        tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)
    )
    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)

    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
    test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)

    def create_model():
        regularizer = tf.keras.regularizers.L2(1e-5)
        model = tf.keras.Sequential(
            [
                tf.keras.layers.Conv2D(32, 3, activation=""relu"", kernel_regularizer=regularizer),
                tf.keras.layers.MaxPooling2D(),
                tf.keras.layers.Conv2D(64, 3, activation=""relu"", kernel_regularizer=regularizer),
                tf.keras.layers.MaxPooling2D(),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(64, activation=""relu"", kernel_regularizer=regularizer),
                tf.keras.layers.Dense(10, kernel_regularizer=regularizer),
            ]
        )

        return model

    # Create a checkpoint directory to store the checkpoints.
    checkpoint_dir = ""./training_checkpoints""
    checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")

    with strategy.scope():
        # Set reduction to `NONE` so you can do the reduction yourself.
        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True, reduction=tf.keras.losses.Reduction.NONE
        )

        def compute_loss(labels, predictions, model_losses):
            per_example_loss = loss_object(labels, predictions)
            loss = tf.nn.compute_average_loss(per_example_loss)
            if model_losses:
                loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))
            return loss

    with strategy.scope():
        test_loss = tf.keras.metrics.Mean(name=""test_loss"")

        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=""train_accuracy"")
        test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=""test_accuracy"")

    # A model, an optimizer, and a checkpoint must be created under `strategy.scope`.
    with strategy.scope():
        model = create_model()

        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)

        checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

    def train_step(inputs):
        images, labels = inputs

        with tf.GradientTape() as tape:
            predictions = model(images, training=True)
            loss = compute_loss(labels, predictions, model.losses)

        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        train_accuracy.update_state(labels, predictions)
        return loss

    def test_step(inputs):
        images, labels = inputs

        predictions = model(images, training=False)
        t_loss = loss_object(labels, predictions)

        test_loss.update_state(t_loss)
        test_accuracy.update_state(labels, predictions)

    # `run` replicates the provided computation and runs it
    # with the distributed input.
    @tf.function
    def distributed_train_step(dataset_inputs):
        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

    @tf.function
    def distributed_test_step(dataset_inputs):
        return strategy.run(test_step, args=(dataset_inputs,))

    for epoch in range(EPOCHS):
        # TRAIN LOOP
        total_loss = 0.0
        num_batches = 0
        for x in train_dist_dataset:
            total_loss += distributed_train_step(x)
            num_batches += 1
        train_loss = total_loss / num_batches

        # TEST LOOP
        for x in test_dist_dataset:
            distributed_test_step(x)

        if epoch % 2 == 0:
            checkpoint.save(checkpoint_prefix)

        template = ""Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, "" ""Test Accuracy: {}""
        print(
            template.format(
                epoch + 1, train_loss, train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100
            )
        )

        test_loss.reset_states()
        train_accuracy.reset_states()
        test_accuracy.reset_states()

    eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=""eval_accuracy"")

    new_model = create_model()
    new_optimizer = tf.keras.optimizers.Adam()

    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)


if __name__ == ""__main__"":
    main()
```


### Relevant log output

_No response_",grep-mb,2024-07-29 07:33:52+00:00,['tilakrayal'],2024-08-16 01:54:07+00:00,2024-08-16 01:54:03+00:00,https://github.com/tensorflow/tensorflow/issues/72686,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.16', '')]","[{'comment_id': 2259922065, 'issue_id': 2434679437, 'author': 'tilakrayal', 'body': '@grep-mb,\r\nLooks like this issue is happening with the tensorflow v2.16, 2.17 which contains the keras3.0 by default causing the issue. Could you please raise the issue in keras-team/keras repo for the quick resolution. Also there is a similar [issue](https://github.com/keras-team/keras/issues/19308) in the keras repo which is in open state. Thank you!', 'created_at': datetime.datetime(2024, 7, 31, 8, 10, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274787098, 'issue_id': 2434679437, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 8, 1, 54, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590467, 'issue_id': 2434679437, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 16, 1, 54, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292590544, 'issue_id': 2434679437, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72686"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72686"">No</a>', 'created_at': datetime.datetime(2024, 8, 16, 1, 54, 6, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-31 08:10:12 UTC): @grep-mb,
Looks like this issue is happening with the tensorflow v2.16, 2.17 which contains the keras3.0 by default causing the issue. Could you please raise the issue in keras-team/keras repo for the quick resolution. Also there is a similar [issue](https://github.com/keras-team/keras/issues/19308) in the keras repo which is in open state. Thank you!

github-actions[bot] on (2024-08-08 01:54:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-16 01:54:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-16 01:54:06 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72686"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72686"">No</a>

"
2434486214,issue,closed,completed,losses return None if dictionary is used in pipeline and model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.2, tf 2.17.0

### Custom code

No

### OS platform and distribution

Rocky Linux 9.3

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If I use a dictionary at the output of my pipeline and my model, the loss function returns None instead of the value.
If you run this code with tf>=2.16.2, it will fail with ""no loss function"" error (see the map_fct output returns ""custom_out"") as well as the model definition uses ""custom_out"". If you change the map_fct and the model definition to remove the dictionary, it is working. This is working on 2.14.x (have not tried on 2.15.x)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
from keras import layers

def map_fct(x, y):
    return x, {'custom_out': y}

inputs = keras.Input(shape=(784,), name=""digits"")
x = layers.Dense(64, activation=""relu"", name=""dense_1"")(inputs)
x = layers.Dense(64, activation=""relu"", name=""dense_2"")(x)
outputs = layers.Dense(10, activation=""softmax"", name=""predictions"")(x)

model = keras.Model(inputs=inputs, outputs={'custom_out': outputs})

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are NumPy arrays)
x_train = x_train.reshape(60000, 784).astype(""float32"") / 255
x_test = x_test.reshape(10000, 784).astype(""float32"") / 255

y_train = y_train.astype(""float32"")
y_test = y_test.astype(""float32"")

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))

train_dataset = train_dataset.map(
        map_func = map_fct,
        num_parallel_calls = tf.data.experimental.AUTOTUNE
    )

val_dataset = val_dataset.map(
        map_func = map_fct,
        num_parallel_calls = tf.data.experimental.AUTOTUNE
    )

model.compile(
    optimizer=keras.optimizers.RMSprop(),  # Optimizer
    # Loss function to minimize
    loss=keras.losses.SparseCategoricalCrossentropy(),
    # List of metrics to monitor
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)

train_dataset = train_dataset.batch(
            batch_size = 64,
            drop_remainder=True,
        )
val_dataset = val_dataset.batch(
            batch_size = 64,
            drop_remainder=True,
        )

print(""Fit model on training data"")
history = model.fit(
    train_dataset,
    batch_size=64,
    epochs=2,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=val_dataset,
)
```


### Relevant log output

```shell
2024-07-29 07:13:09.432389: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-29 07:13:09.445499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-29 07:13:09.465390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-29 07:13:09.465415: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-29 07:13:09.477398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-29 07:13:10.111192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-07-29 07:13:10.748552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20282 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:9e:00.0, compute capability: 8.6
Fit model on training data
Epoch 1/2
Traceback (most recent call last):
  File ""/home/xxxxxx/workspace/tmp/dict_dataset_tf2_16_2.py"", line 61, in <module>
    history = model.fit(
  File ""/home/xxxxxx/python_venv/tensorflow_probability/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/xxxxxx/python_venv/tensorflow_probability/lib64/python3.9/site-packages/keras/src/trainers/trainer.py"", line 331, in compute_loss
    raise ValueError(
ValueError: No loss to compute. Provide a `loss` argument in `compile()`.
```
",pmdaye,2024-07-29 05:28:49+00:00,['Venkat6871'],2024-10-03 02:01:45+00:00,2024-10-03 02:01:41+00:00,https://github.com/tensorflow/tensorflow/issues/72679,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2254991988, 'issue_id': 2434486214, 'author': 'pmdaye', 'body': 'After more testing, I realized that if I name my last layer ""custom_out"" (and remove the dictionary in the model definition), then I can keep the dictionary in my dataset. I found the fact that the output in the model can used a dictionary a very useful feature. When was this capability removed from tf 2?\r\nThe reason why I used this naming is when my model have multiple outputs to ensure the coherence between the outputs and the data provided by my pipeline. If I have a list of output layers with the corresponding names, will this work as well (then I can adapt my code)?', 'created_at': datetime.datetime(2024, 7, 29, 5, 44, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257581962, 'issue_id': 2434486214, 'author': 'just-sabyr', 'body': '@pmdaye, your code has a slight mistake. Here\'s how to fix it: rename \'predictions\' to \'custom_out\'. It looks like all of the three output names (outputs Dense(10) layer, dataset y_true label name, model_outputs name) have to be called by the same name.\r\nThis is the correct code that runs on google colab with tf.__version__ == \'2.17.0\', and keras.__version__ == \'3.4.1\'\r\n\r\n```python\r\n!pip3 install tensorflow==2.17.0 --quiet\r\n!pip3 install --upgrade tf-keras --quiet\r\n\r\nimport tensorflow as tf\r\nimport keras\r\nfrom keras import layers\r\n\r\ndef map_fct(x, y):\r\n    return x, {\'y_true\': y}\r\n\r\ninputs = keras.Input(shape=(784,), name=""digits"")\r\nx = layers.Dense(64, activation=""relu"", name=""dense_1"")(inputs)\r\nx = layers.Dense(64, activation=""relu"", name=""dense_2"")(x)\r\noutputs = layers.Dense(10, activation=""softmax"", name=""y_true"")(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs={\'y_true\': outputs})\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\n# Preprocess the data (these are NumPy arrays)\r\nx_train = x_train.reshape(60000, 784).astype(""float32"") / 255\r\nx_test = x_test.reshape(10000, 784).astype(""float32"") / 255\r\n\r\ny_train = y_train.astype(""float32"")\r\ny_test = y_test.astype(""float32"")\r\n\r\n# Reserve 10,000 samples for validation\r\nx_val = x_train[-10000:]\r\ny_val = y_train[-10000:]\r\nx_train = x_train[:-10000]\r\ny_train = y_train[:-10000]\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\n\r\ntrain_dataset = train_dataset.map(\r\n        map_func = map_fct,\r\n        num_parallel_calls = tf.data.experimental.AUTOTUNE\r\n    )\r\n\r\nval_dataset = val_dataset.map(\r\n        map_func = map_fct,\r\n        num_parallel_calls = tf.data.experimental.AUTOTUNE\r\n    )\r\n\r\nmodel.compile(\r\n    optimizer=keras.optimizers.RMSprop(),  # Optimizer\r\n    # Loss function to minimize\r\n    loss=keras.losses.SparseCategoricalCrossentropy(),\r\n    # List of metrics to monitor\r\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\r\n)\r\n\r\ntrain_dataset = train_dataset.batch(\r\n            batch_size = 64,\r\n            drop_remainder=True,\r\n        )\r\nval_dataset = val_dataset.batch(\r\n            batch_size = 64,\r\n            drop_remainder=True,\r\n        )\r\n\r\nprint(""Fit model on training data"")\r\nhistory = model.fit(\r\n    train_dataset,\r\n    batch_size=64,\r\n    epochs=2,\r\n    # We pass some validation for\r\n    # monitoring validation loss and metrics\r\n    # at the end of each epoch\r\n    validation_data=val_dataset,\r\n)\r\n```', 'created_at': datetime.datetime(2024, 7, 30, 6, 39, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257644514, 'issue_id': 2434486214, 'author': 'pmdaye', 'body': '@just-sabyr,\r\n\r\nThanks for the update. You came to the same conclusion as me, great! From my analysis, we do not need to name the output in the model definition, only the last layer is sufficient.\r\nHowever, I must stress that the original version of the code was working without issues on 2.14.1. If this has changed on purpose, I think it would be great to have this explained somewhere within the documentation. If this change was not spotted, I would suggest to file it as a bug.\r\nThanks again!', 'created_at': datetime.datetime(2024, 7, 30, 7, 17, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2258068261, 'issue_id': 2434486214, 'author': 'just-sabyr', 'body': ""@pmdaye, as per my understanding, it is safer if all of them are named the same.  After some considerations here is my try to explain what happens: \r\n\r\n1. There is an object (Trainer._compile_loss in the file) of type `CompileLoss` in trainer.py file, which is initialized `output_names of the model` as one of it's parameters.\r\n2. `Trainer._compile_loss`'s `call` method is called with `y`, `y_pred`, `sample_weight` (which returns the loss value) \r\n3. `CompileLoss.call()` has a utitlity function (in the compile_utils.py file)  `_flatten_y` which is called for both `y` and `y_pred` to convert them into lists. \r\n4. Both `y` and `y_pred` are of type dict before _`_flatten_y`_ (each with one key if it is single-output model) processes them and this function uses `_output_names of the model_` passed at step 1 to do so. \r\n\r\nThe fourth step is the reason for the necessity that all of the three `y` (input to the model), `y_pred` (output of the model) and last_layer.outputs must be dictionaries with the same keys. I did not look up how they handled it in the previous versions, but (to me at least) this way seems sensible."", 'created_at': datetime.datetime(2024, 7, 30, 10, 56, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2258084832, 'issue_id': 2434486214, 'author': 'just-sabyr', 'body': 'However there should be a proper error handling to warn users about non-matching dict keys.', 'created_at': datetime.datetime(2024, 7, 30, 11, 5, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263235552, 'issue_id': 2434486214, 'author': 'Venkat6871', 'body': 'Hi **@pmdaye** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 1, 14, 34, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277004934, 'issue_id': 2434486214, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 9, 1, 54, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375600837, 'issue_id': 2434486214, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 26, 2, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390350079, 'issue_id': 2434486214, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390350152, 'issue_id': 2434486214, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72679"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72679"">No</a>', 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 43, tzinfo=datetime.timezone.utc)}]","pmdaye (Issue Creator) on (2024-07-29 05:44:57 UTC): After more testing, I realized that if I name my last layer ""custom_out"" (and remove the dictionary in the model definition), then I can keep the dictionary in my dataset. I found the fact that the output in the model can used a dictionary a very useful feature. When was this capability removed from tf 2?
The reason why I used this naming is when my model have multiple outputs to ensure the coherence between the outputs and the data provided by my pipeline. If I have a list of output layers with the corresponding names, will this work as well (then I can adapt my code)?

just-sabyr on (2024-07-30 06:39:42 UTC): @pmdaye, your code has a slight mistake. Here's how to fix it: rename 'predictions' to 'custom_out'. It looks like all of the three output names (outputs Dense(10) layer, dataset y_true label name, model_outputs name) have to be called by the same name.
This is the correct code that runs on google colab with tf.__version__ == '2.17.0', and keras.__version__ == '3.4.1'

```python
!pip3 install tensorflow==2.17.0 --quiet
!pip3 install --upgrade tf-keras --quiet

import tensorflow as tf
import keras
from keras import layers

def map_fct(x, y):
    return x, {'y_true': y}

inputs = keras.Input(shape=(784,), name=""digits"")
x = layers.Dense(64, activation=""relu"", name=""dense_1"")(inputs)
x = layers.Dense(64, activation=""relu"", name=""dense_2"")(x)
outputs = layers.Dense(10, activation=""softmax"", name=""y_true"")(x)

model = keras.Model(inputs=inputs, outputs={'y_true': outputs})

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are NumPy arrays)
x_train = x_train.reshape(60000, 784).astype(""float32"") / 255
x_test = x_test.reshape(10000, 784).astype(""float32"") / 255

y_train = y_train.astype(""float32"")
y_test = y_test.astype(""float32"")

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))

train_dataset = train_dataset.map(
        map_func = map_fct,
        num_parallel_calls = tf.data.experimental.AUTOTUNE
    )

val_dataset = val_dataset.map(
        map_func = map_fct,
        num_parallel_calls = tf.data.experimental.AUTOTUNE
    )

model.compile(
    optimizer=keras.optimizers.RMSprop(),  # Optimizer
    # Loss function to minimize
    loss=keras.losses.SparseCategoricalCrossentropy(),
    # List of metrics to monitor
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)

train_dataset = train_dataset.batch(
            batch_size = 64,
            drop_remainder=True,
        )
val_dataset = val_dataset.batch(
            batch_size = 64,
            drop_remainder=True,
        )

print(""Fit model on training data"")
history = model.fit(
    train_dataset,
    batch_size=64,
    epochs=2,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=val_dataset,
)
```

pmdaye (Issue Creator) on (2024-07-30 07:17:50 UTC): @just-sabyr,

Thanks for the update. You came to the same conclusion as me, great! From my analysis, we do not need to name the output in the model definition, only the last layer is sufficient.
However, I must stress that the original version of the code was working without issues on 2.14.1. If this has changed on purpose, I think it would be great to have this explained somewhere within the documentation. If this change was not spotted, I would suggest to file it as a bug.
Thanks again!

just-sabyr on (2024-07-30 10:56:19 UTC): @pmdaye, as per my understanding, it is safer if all of them are named the same.  After some considerations here is my try to explain what happens: 

1. There is an object (Trainer._compile_loss in the file) of type `CompileLoss` in trainer.py file, which is initialized `output_names of the model` as one of it's parameters.
2. `Trainer._compile_loss`'s `call` method is called with `y`, `y_pred`, `sample_weight` (which returns the loss value) 
3. `CompileLoss.call()` has a utitlity function (in the compile_utils.py file)  `_flatten_y` which is called for both `y` and `y_pred` to convert them into lists. 
4. Both `y` and `y_pred` are of type dict before _`_flatten_y`_ (each with one key if it is single-output model) processes them and this function uses `_output_names of the model_` passed at step 1 to do so. 

The fourth step is the reason for the necessity that all of the three `y` (input to the model), `y_pred` (output of the model) and last_layer.outputs must be dictionaries with the same keys. I did not look up how they handled it in the previous versions, but (to me at least) this way seems sensible.

just-sabyr on (2024-07-30 11:05:10 UTC): However there should be a proper error handling to warn users about non-matching dict keys.

Venkat6871 (Assginee) on (2024-08-01 14:34:43 UTC): Hi **@pmdaye** ,
Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras

Thank you!

github-actions[bot] on (2024-08-09 01:54:57 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-26 02:01:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-03 02:01:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-03 02:01:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72679"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72679"">No</a>

"
2434013705,issue,closed,completed,Clang 17 failed to compile TensorFlow master ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Ubuntu 22.04 x86

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.50

### GCC/compiler version

Clang 17.06

### CUDA/cuDNN version

8.9

### GPU model and memory

RTX 3060 

### Current behavior?

As following from https://www.tensorflow.org/install/source#gpu and https://www.tensorflow.org/install/source , the tensorflow build is failing due to clang 17 not compiling org_brotli. 

### Standalone code to reproduce the issue

```shell
clone from https://www.tensorflow.org/install/sourc
./configure with below configurations . 
Try build command 
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda

avaish@avaish-dekstop:/media/avaish/labdisk/clang-tf/tensorflow$ ./configure 
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3.10/dist-packages
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.10/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.10/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 12.4 in:
    /usr/local/cuda-12.4/targets/x86_64-linux/lib
    /usr/local/cuda-12.4/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Do you want to use clang as CUDA compiler? [Y/n]: Y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-17/bin/clang]: 


You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.


Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 12.4 in:
    /usr/local/cuda-12.4/targets/x86_64-linux/lib
    /usr/local/cuda-12.4/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
```


### Relevant log output

```shell
ERROR: /home/avaish/.cache/bazel/_bazel_avaish/2b06f750a2a65e1321540478ec17bba7/external/org_brotli/BUILD:121:11: Compiling c/enc/utf8_util.c failed: (Exit 1): clang failed: error executing command (from target @org_brotli//:brotlienc) /usr/lib/llvm-17/bin/clang -MD -MF bazel-out/k8-opt/bin/external/org_brotli/_objs/brotlienc/utf8_util.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_brotli/_objs/brotlienc/utf8_util.pic.o' ... (remaining 52 arguments skipped)
clang: error: argument unused during compilation: '--cuda-path=/usr/local/cuda-12.4' [-Werror,-Wunused-command-line-argument]
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /media/avaish/labdisk/clang-tf/tensorflow/tensorflow/tools/pip_package/BUILD:266:9 Action tensorflow/tools/pip_package/wheel_house failed: (Exit 1): clang failed: error executing command (from target @org_brotli//:brotlienc) /usr/lib/llvm-17/bin/clang -MD -MF bazel-out/k8-opt/bin/external/org_brotli/_objs/brotlienc/utf8_util.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_brotli/_objs/brotlienc/utf8_util.pic.o' ... (remaining 52 arguments skipped)
INFO: Elapsed time: 21.943s, Critical Path: 7.65s
INFO: 349 processes: 15 internal, 334 local.
FAILED: Build did NOT complete successfully
```
",intelav,2024-07-28 16:24:00+00:00,['tilakrayal'],2024-07-31 10:31:52+00:00,2024-07-31 10:31:49+00:00,https://github.com/tensorflow/tensorflow/issues/72655,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2254582044, 'issue_id': 2434013705, 'author': 'intelav', 'body': '[tf_fail_log.txt](https://github.com/user-attachments/files/16404597/tf_fail_log.txt)\r\nAttaching the log file in verbose mode.', 'created_at': datetime.datetime(2024, 7, 28, 17, 4, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2258357514, 'issue_id': 2434013705, 'author': 'do4fun', 'body': ""Same log error as this issue with bazel build command for WHEEL_NAME=tensorflow-cpu on the branch master.\r\n'bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow-cpu'"", 'created_at': datetime.datetime(2024, 7, 30, 13, 29, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259713257, 'issue_id': 2434013705, 'author': 'intelav', 'body': 'Resolved by adding **build:linux --copt=""-Wno-unused-command-line-argument""** in .bazelrc file.', 'created_at': datetime.datetime(2024, 7, 31, 5, 41, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259763818, 'issue_id': 2434013705, 'author': 'tilakrayal', 'body': '@intelav,\r\nGlad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!', 'created_at': datetime.datetime(2024, 7, 31, 6, 25, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260195460, 'issue_id': 2434013705, 'author': 'intelav', 'body': 'Thanks', 'created_at': datetime.datetime(2024, 7, 31, 10, 31, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260196443, 'issue_id': 2434013705, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72655"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72655"">No</a>', 'created_at': datetime.datetime(2024, 7, 31, 10, 31, 51, tzinfo=datetime.timezone.utc)}]","intelav (Issue Creator) on (2024-07-28 17:04:30 UTC): [tf_fail_log.txt](https://github.com/user-attachments/files/16404597/tf_fail_log.txt)
Attaching the log file in verbose mode.

do4fun on (2024-07-30 13:29:52 UTC): Same log error as this issue with bazel build command for WHEEL_NAME=tensorflow-cpu on the branch master.
'bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow-cpu'

intelav (Issue Creator) on (2024-07-31 05:41:08 UTC): Resolved by adding **build:linux --copt=""-Wno-unused-command-line-argument""** in .bazelrc file.

tilakrayal (Assginee) on (2024-07-31 06:25:15 UTC): @intelav,
Glad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!

intelav (Issue Creator) on (2024-07-31 10:31:26 UTC): Thanks

google-ml-butler[bot] on (2024-07-31 10:31:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72655"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72655"">No</a>

"
2433724289,issue,closed,completed,Compare to different GPUs (RTX 4060Ti and GTX 1660Ti) performance,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.2.3/tf2.3/ tf2.5/tf2.10/tf2.12

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I'm comparing the performance of two GPUs and the differences between different TensorFlow versions.
I plan to upgrade from GTX 1660 Ti to RTX 4060Ti. I expected the inference speed to be faster after the upgrade, but surprisingly it wasn't.
Below are the results of inferring different models and batch sizes. I used Docker to test different versions of TF.
Due to cuda version, it's hard to compare almost tf versions.
The number is average inference ms

###  GTX 1660 Ti
<img width=""920"" alt="" 2024-07-28 11 17 47"" src=""https://github.com/user-attachments/assets/3b5c6831-fa82-48cf-aeae-3294d3c53c72"">

###  RTX 4060 Ti
<img width=""746"" alt="" 2024-07-28 11 17 56"" src=""https://github.com/user-attachments/assets/9fc8d50a-6c0a-4776-81c0-451d936b0955"">

### GTX 1660 Ti vs RTX 4060 Ti (w/ same tf version)
<img width=""571"" alt="" 2024-07-28 11 18 12"" src=""https://github.com/user-attachments/assets/2b607367-a84b-4c21-a0b6-0c4a012ffce7"">


From the chart above, I've drawn a few conclusions:

1. The higher the TF version, the longer the inference time... why???
2. I expected the 4060Ti to be faster than the 1660Ti for the same TF version, but this is only observed when bs=8/16, while my use case is for a single image.


I suspected it might be a card issue, but when I tested the PyTorch version of YOLOv5, the 4060Ti was indeed faster.

I wonder if any seniors can give me some clues or help me understand this, thank you.

Is there anything else you'd like me to help with regarding this translation or the content of your message?

Thanks, 
KT

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1e_lXPvQ82lCuOyFs9xK8oztagNwxFfkd?usp=sharing
```


### Relevant log output

_No response_",tom2002965,2024-07-28 03:32:05+00:00,['Venkat6871'],2024-12-11 09:07:51+00:00,2024-12-11 09:07:47+00:00,https://github.com/tensorflow/tensorflow/issues/72635,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:performance', 'Performance Issue'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2257901060, 'issue_id': 2433724289, 'author': 'Venkat6871', 'body': 'Hi **@tom2002965** ,\r\n- Here i observed version mismatch. Could you please check with compatibility versions? And Please use latest version for better results. let us know whether the issue persists with recent TF versions.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 30, 9, 29, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2258724030, 'issue_id': 2433724289, 'author': 'tom2002965', 'body': 'Hi @Venkat6871 , \r\n\r\nVery thank for your response.\r\n- All the tf/cuda/cudnn are from docker which is [tensorflow](https://hub.docker.com/r/tensorflow/tensorflow/) provided.\r\n- Now my gpu drive of RTX 4060Ti is `535.183.01` . I have to update driver to test latest version . So I test tf-2.14 first. The result is below.\r\n- 2.14 is faster than 2.10/2.12, but it still slower than tf 2.5.\r\n<img width=""930"" alt="" 2024-07-31 00 14 11"" src=""https://github.com/user-attachments/assets/537c3156-2210-4114-9248-9244dac632cd"">\r\n\r\nThanks.', 'created_at': datetime.datetime(2024, 7, 30, 16, 15, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535038862, 'issue_id': 2433724289, 'author': 'Venkat6871', 'body': 'Hi **@tom2002965** ,\r\nApologies for the delay, and thank you for your patience. It appears there are still compatibility issues with TensorFlow 2.14.0. The compatible version of cuDNN is 8.7. Could you please double-check all the compatible versions and verify with the latest ones? If you continue to face issues, please let us know. For your convenience, here is the [documentation](https://www.tensorflow.org/install/source#gpu) to help verify the compatible versions.\r\nThank you!', 'created_at': datetime.datetime(2024, 12, 11, 8, 35, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535252102, 'issue_id': 2433724289, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72635"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72635"">No</a>', 'created_at': datetime.datetime(2024, 12, 11, 9, 7, 49, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-30 09:29:20 UTC): Hi **@tom2002965** ,
- Here i observed version mismatch. Could you please check with compatibility versions? And Please use latest version for better results. let us know whether the issue persists with recent TF versions.

Thank you!

tom2002965 (Issue Creator) on (2024-07-30 16:15:45 UTC): Hi @Venkat6871 , 

Very thank for your response.
- All the tf/cuda/cudnn are from docker which is [tensorflow](https://hub.docker.com/r/tensorflow/tensorflow/) provided.
- Now my gpu drive of RTX 4060Ti is `535.183.01` . I have to update driver to test latest version . So I test tf-2.14 first. The result is below.
- 2.14 is faster than 2.10/2.12, but it still slower than tf 2.5.
<img width=""930"" alt="" 2024-07-31 00 14 11"" src=""https://github.com/user-attachments/assets/537c3156-2210-4114-9248-9244dac632cd"">

Thanks.

Venkat6871 (Assginee) on (2024-12-11 08:35:57 UTC): Hi **@tom2002965** ,
Apologies for the delay, and thank you for your patience. It appears there are still compatibility issues with TensorFlow 2.14.0. The compatible version of cuDNN is 8.7. Could you please double-check all the compatible versions and verify with the latest ones? If you continue to face issues, please let us know. For your convenience, here is the [documentation](https://www.tensorflow.org/install/source#gpu) to help verify the compatible versions.
Thank you!

google-ml-butler[bot] on (2024-12-11 09:07:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72635"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72635"">No</a>

"
2433554849,issue,closed,completed,Failed to load the native TensorFlow runtime.,"Trying to run a captcha solving python script and got this error;


Traceback (most recent call last):
  File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\micha\Documents\PythonProgramming\IG\pytesseract_solver.py"", line 9, in <module>
    import tensorflow
  File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",MikeyD-rbg,2024-07-27 16:56:12+00:00,['tilakrayal'],2024-08-13 01:55:25+00:00,2024-08-13 01:55:22+00:00,https://github.com/tensorflow/tensorflow/issues/72629,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2255102204, 'issue_id': 2433554849, 'author': 'tilakrayal', 'body': '@MikeyD-rbg,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 7, 7, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213296, 'issue_id': 2433554849, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196270, 'issue_id': 2433554849, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196291, 'issue_id': 2433554849, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72629"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72629"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 24, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-29 07:07:01 UTC): @MikeyD-rbg,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

github-actions[bot] on (2024-08-06 01:53:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-13 01:55:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-13 01:55:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72629"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72629"">No</a>

"
2433379314,issue,closed,completed,Long error message when trying to run a tensor flow program,"Hey All,

Im new to the deep learning field and I'm trying to follow a tutorial. Im using a MacOS and PyCharm IDE. 
I have successfully installed tensorFlow package. I wrote a simple ( Hello World) and this error appeared, then I tried to trace back at which line this error appear, simply from the very basic first line 

`import tensorflow as tf`

<img width=""1133"" alt=""Screenshot 2024-07-25 at 11 22 10AM"" src=""https://github.com/user-attachments/assets/ab34bcdf-d2cf-4e8d-be42-10220af3dc0c"">
<img width=""1053"" alt=""Screenshot 2024-07-25 at 11 22 56AM"" src=""https://github.com/user-attachments/assets/be73f5b5-77c4-4462-873e-9aa790da0166"">

I tried one of the solutions online that I should install `pip install tensorflow-cpu` rather than `pip install tensorflow` but same error appear. 

I appreciate any tips ",Rosie-93,2024-07-27 08:55:28+00:00,['Venkat6871'],2024-09-18 08:59:29+00:00,2024-09-18 08:59:25+00:00,https://github.com/tensorflow/tensorflow/issues/72622,"[('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2254147783, 'issue_id': 2433379314, 'author': 'amd-rezaei', 'body': 'It seems like there may be conflicting dependencies of the package and Python version installed. To address the issue, you can simply create a conda environment (Dependency management) with a compatible version of Python for your target TensorFlow version. Next, activate it and install TensorFlow, which should run without problems.\r\n\r\n## Steps to Resolve the Issue:\r\n\r\n1. **Create a Conda Environment with a Specific Python Version**:\r\n    ```bash\r\n    conda create -n env_name python=3.x\r\n    ```\r\n    Replace `3.x` with the specific Python version you need.\r\n\r\n2. **Activate the Conda Environment**:\r\n    ```bash\r\n    conda activate env_name\r\n    ```\r\n\r\n3. **Follow TensorFlow Installation Guide**:\r\n    Follow the instructions from the official TensorFlow installation guide for macOS: [TensorFlow Installation Guide](https://www.tensorflow.org/install/pip#macos)', 'created_at': datetime.datetime(2024, 7, 27, 13, 18, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254342016, 'issue_id': 2433379314, 'author': 'Rosie-93', 'body': '> It seems like there may be conflicting dependencies of the package and Python version installed. To address the issue, you can simply create a conda environment (Dependency management) with a compatible version of Python for your target TensorFlow version. Next, activate it and install TensorFlow, which should run without problems.\r\n> \r\n> ## Steps to Resolve the Issue:\r\n> 1. **Create a Conda Environment with a Specific Python Version**:\r\n>    ```shell\r\n>    conda create -n env_name python=3.x\r\n>    ```\r\n>    \r\n>    \r\n>        \r\n>          \r\n>        \r\n>    \r\n>          \r\n>        \r\n>    \r\n>        \r\n>      \r\n>    Replace `3.x` with the specific Python version you need.\r\n> 2. **Activate the Conda Environment**:\r\n>    ```shell\r\n>    conda activate env_name\r\n>    ```\r\n> 3. **Follow TensorFlow Installation Guide**:\r\n>    Follow the instructions from the official TensorFlow installation guide for macOS: [TensorFlow Installation Guide](https://www.tensorflow.org/install/pip#macos)\r\n\r\nthank you for your comment, I have been working with Installing packages through pycharm only, do I need this ( Conda environment)? I mean is there any difference between it and what I have been doing so far with Pycharm only?', 'created_at': datetime.datetime(2024, 7, 28, 4, 45, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254460656, 'issue_id': 2433379314, 'author': 'amd-rezaei', 'body': 'There is no difference except for managing your Python and TensorFlow versions. So you can also set another Python version for the Pycharm environment and install your TensorFlow package within that one.', 'created_at': datetime.datetime(2024, 7, 28, 10, 9, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255403504, 'issue_id': 2433379314, 'author': 'Venkat6871', 'body': 'Hi **@Rosie-93** ,\r\n- Could you please ensure that the tensorflow version you installed is compatible with your macos and python version. And verify that you are working in the correct virtual environment where tensorflow installed. Try to reinstall tensorflow. And follow this [documentation](https://www.tensorflow.org/install/source#macos) for better results.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 9, 9, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257573406, 'issue_id': 2433379314, 'author': 'Rosie-93', 'body': ""@amd-rezaei @Venkat6871  I have used Conda as the environment, the first 2 sentences warning appeared but the program was executed successfully, I went and tried Python ,pip required versions and Tensorflow as well using Pycharm default environment however, it did NOT work even with the right versions thats why I found it confusing why wouldn't it work with pycharm environment"", 'created_at': datetime.datetime(2024, 7, 30, 6, 33, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262982945, 'issue_id': 2433379314, 'author': 'amd-rezaei', 'body': ""My view would be that there are some package dependencies in default environment. It would be resolved if u install a new environment with your system's compatible versions."", 'created_at': datetime.datetime(2024, 8, 1, 13, 1, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268211856, 'issue_id': 2433379314, 'author': 'Venkat6871', 'body': 'Hi **@Rosie-93** ,\r\n- Here I followed below steps to install tensorflow in PyCharm IDE. It runs successfully to me.\r\n1. I installed the PyCharm IDE by using the tool box app from the PyCharm site.\r\n2. After that python venv to create the environment  for the new project.\r\n3. Then I installed tensorflow by using pip.\r\n```\r\npip install tensorflow\r\n```\r\n4. After that import tensorflow is working fine.\r\n```\r\nimport tensorflow as tf\r\n```\r\n- Here I am providing screenshots for your reference. <img width=""1728"" alt=""Screenshot 2024-08-05 at 10 50 22\u202fAM"" src=""https://github.com/user-attachments/assets/c7b6a79b-b6f0-4a67-bb43-3547873724af"">, <img width=""1728"" alt=""Screenshot 2024-08-05 at 10 51 10\u202fAM"" src=""https://github.com/user-attachments/assets/75383ed0-adcb-4b38-95c0-9260c333e6e3"">.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 8, 5, 5, 37, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196279, 'issue_id': 2433379314, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357894997, 'issue_id': 2433379314, 'author': 'Venkat6871', 'body': 'Hi **@Rosie-93** ,\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 18, 8, 59, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357895059, 'issue_id': 2433379314, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72622"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72622"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 8, 59, 27, tzinfo=datetime.timezone.utc)}]","amd-rezaei on (2024-07-27 13:18:16 UTC): It seems like there may be conflicting dependencies of the package and Python version installed. To address the issue, you can simply create a conda environment (Dependency management) with a compatible version of Python for your target TensorFlow version. Next, activate it and install TensorFlow, which should run without problems.

## Steps to Resolve the Issue:

1. **Create a Conda Environment with a Specific Python Version**:
    ```bash
    conda create -n env_name python=3.x
    ```
    Replace `3.x` with the specific Python version you need.

2. **Activate the Conda Environment**:
    ```bash
    conda activate env_name
    ```

3. **Follow TensorFlow Installation Guide**:
    Follow the instructions from the official TensorFlow installation guide for macOS: [TensorFlow Installation Guide](https://www.tensorflow.org/install/pip#macos)

Rosie-93 (Issue Creator) on (2024-07-28 04:45:14 UTC): thank you for your comment, I have been working with Installing packages through pycharm only, do I need this ( Conda environment)? I mean is there any difference between it and what I have been doing so far with Pycharm only?

amd-rezaei on (2024-07-28 10:09:29 UTC): There is no difference except for managing your Python and TensorFlow versions. So you can also set another Python version for the Pycharm environment and install your TensorFlow package within that one.

Venkat6871 (Assginee) on (2024-07-29 09:09:52 UTC): Hi **@Rosie-93** ,
- Could you please ensure that the tensorflow version you installed is compatible with your macos and python version. And verify that you are working in the correct virtual environment where tensorflow installed. Try to reinstall tensorflow. And follow this [documentation](https://www.tensorflow.org/install/source#macos) for better results.

Thank you!

Rosie-93 (Issue Creator) on (2024-07-30 06:33:20 UTC): @amd-rezaei @Venkat6871  I have used Conda as the environment, the first 2 sentences warning appeared but the program was executed successfully, I went and tried Python ,pip required versions and Tensorflow as well using Pycharm default environment however, it did NOT work even with the right versions thats why I found it confusing why wouldn't it work with pycharm environment

amd-rezaei on (2024-08-01 13:01:16 UTC): My view would be that there are some package dependencies in default environment. It would be resolved if u install a new environment with your system's compatible versions.

Venkat6871 (Assginee) on (2024-08-05 05:37:53 UTC): Hi **@Rosie-93** ,
- Here I followed below steps to install tensorflow in PyCharm IDE. It runs successfully to me.
1. I installed the PyCharm IDE by using the tool box app from the PyCharm site.
2. After that python venv to create the environment  for the new project.
3. Then I installed tensorflow by using pip.
```
pip install tensorflow
```
4. After that import tensorflow is working fine.
```
import tensorflow as tf
```
- Here I am providing screenshots for your reference. <img width=""1728"" alt=""Screenshot 2024-08-05 at 10 50 22AM"" src=""https://github.com/user-attachments/assets/c7b6a79b-b6f0-4a67-bb43-3547873724af"">, <img width=""1728"" alt=""Screenshot 2024-08-05 at 10 51 10AM"" src=""https://github.com/user-attachments/assets/75383ed0-adcb-4b38-95c0-9260c333e6e3"">.

Thank you!

github-actions[bot] on (2024-08-13 01:55:23 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

Venkat6871 (Assginee) on (2024-09-18 08:59:25 UTC): Hi **@Rosie-93** ,
I am closing this issue as this was resolved. Please feel free to reopen if I am mistaken.
Thank you!

google-ml-butler[bot] on (2024-09-18 08:59:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72622"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72622"">No</a>

"
2432760938,issue,closed,completed,tf.distribute.MirroredStrategy error: Multiple OpKernel registrations match NodeDef at the same priority,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

Jupyter Notebook

### CUDA/cuDNN version

cuda_12.5.r12.5

### GPU model and memory

_No response_

### Current behavior?

#Minimal code to reproduce error
import tensorflow as tf
import keras 
import numpy as np

tf.debugging.set_log_device_placement(True)
gpus = tf.config.list_logical_devices('GPU')
print(gpus)

strategy = tf.distribute.MirroredStrategy(gpus)

#expect to be able have code (not shown) below this to run over 2 GPUs

### Standalone code to reproduce the issue

```shell
Output: 
[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]
Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0

---> 14 strategy = tf.distribute.MirroredStrategy(gpus)
...
InvalidArgumentError: Multiple OpKernel registrations match NodeDef at the same priority '{{node AssignVariableOp}}': 'op: ""AssignVariableOp"" device_type: ""GPU"" constraint { name: ""dtype"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""resource""' and 'op: ""AssignVariableOp"" device_type: ""GPU"" constraint { name: ""dtype"" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: ""resource""'
	 [[AssignVariableOp]] [Op:AssignVariableOp]
```


### Relevant log output

_No response_",jmmelen,2024-07-26 18:18:08+00:00,['tilakrayal'],2024-09-18 01:59:07+00:00,2024-09-18 01:58:57+00:00,https://github.com/tensorflow/tensorflow/issues/72585,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.10', '')]","[{'comment_id': 2255287543, 'issue_id': 2432760938, 'author': 'tilakrayal', 'body': ""@jmmelen,\r\nThe tensorflow v2.10 is an old version. Could you please try to install the latest tensorflow v2.15, v2.16 or v2.17 and it was able to execute the code and detect the GPU's. Kindly find the screenshot for the reference.\r\n\r\n![Screenshot 2024-05-21 1 43 20 PM](https://github.com/user-attachments/assets/9483ec08-65b7-462d-93e2-c9dcadde8ca3)\r\n\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 29, 8, 12, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256692166, 'issue_id': 2432760938, 'author': 'jmmelen', 'body': '1) remove existing tensorflow 2.10.0\r\n2) installed tensorflow=2.17.0\r\n2) Now in JupyterNotebook  the GPU is no longer being seen:\r\nimport tensorflow as tf  \r\nprint(tf.__version__)\r\nprint(""Num GPUs Available: "", len(tf.config.list_physical_devices(\'GPU\')))\r\nprint(""Num CPUs Available: "", len(tf.config.list_physical_devices(\'CPU\')))\r\nprint(tf.config.list_physical_devices(\'GPU\'))\r\nprint(tf.config.list_physical_devices(\'CPU\'))\r\n\r\nOutput:\r\n2.17.0\r\nNum GPUs Available:  0\r\nNum CPUs Available:  1\r\n[]\r\n[PhysicalDevice(name=\'/physical_device:CPU:0\', device_type=\'CPU\')]', 'created_at': datetime.datetime(2024, 7, 29, 19, 4, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256709461, 'issue_id': 2432760938, 'author': 'jmmelen', 'body': 'Also tried: pip install --force-reinstall tensorflow==2.17.0\r\nand got the same error as above', 'created_at': datetime.datetime(2024, 7, 29, 19, 14, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257074848, 'issue_id': 2432760938, 'author': 'jmmelen', 'body': 'Also tried:\r\n1) uninstalled tensorflow, tensorboard-data-server, tensorboard, keras, tensorflow-intel. \r\n2) installed tensorflow 2.17.0 and got the same error as above:\r\nimport tensorflow as tf\r\nprint(tf.version)\r\nprint(""Num GPUs Available: "", len(tf.config.list_physical_devices(\'GPU\')))\r\nprint(""Num CPUs Available: "", len(tf.config.list_physical_devices(\'CPU\')))\r\nprint(tf.config.list_physical_devices(\'GPU\'))\r\nprint(tf.config.list_physical_devices(\'CPU\'))\r\nOutput:\r\n2.17.0\r\nNum GPUs Available: 0\r\nNum CPUs Available: 1\r\n[]', 'created_at': datetime.datetime(2024, 7, 29, 21, 53, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257088808, 'issue_id': 2432760938, 'author': 'jmmelen', 'body': 'Remember this is on Windows 11.', 'created_at': datetime.datetime(2024, 7, 29, 22, 4, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324322606, 'issue_id': 2432760938, 'author': 'tilakrayal', 'body': 'TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-)', 'created_at': datetime.datetime(2024, 9, 2, 9, 57, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460348, 'issue_id': 2432760938, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336714, 'issue_id': 2432760938, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336898, 'issue_id': 2432760938, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72585"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72585"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 59, 5, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-29 08:12:48 UTC): @jmmelen,
The tensorflow v2.10 is an old version. Could you please try to install the latest tensorflow v2.15, v2.16 or v2.17 and it was able to execute the code and detect the GPU's. Kindly find the screenshot for the reference.

![Screenshot 2024-05-21 1 43 20 PM](https://github.com/user-attachments/assets/9483ec08-65b7-462d-93e2-c9dcadde8ca3)


Thank you!

jmmelen (Issue Creator) on (2024-07-29 19:04:05 UTC): 1) remove existing tensorflow 2.10.0
2) installed tensorflow=2.17.0
2) Now in JupyterNotebook  the GPU is no longer being seen:
import tensorflow as tf  
print(tf.__version__)
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
print(""Num CPUs Available: "", len(tf.config.list_physical_devices('CPU')))
print(tf.config.list_physical_devices('GPU'))
print(tf.config.list_physical_devices('CPU'))

Output:
2.17.0
Num GPUs Available:  0
Num CPUs Available:  1
[]
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]

jmmelen (Issue Creator) on (2024-07-29 19:14:44 UTC): Also tried: pip install --force-reinstall tensorflow==2.17.0
and got the same error as above

jmmelen (Issue Creator) on (2024-07-29 21:53:27 UTC): Also tried:
1) uninstalled tensorflow, tensorboard-data-server, tensorboard, keras, tensorflow-intel. 
2) installed tensorflow 2.17.0 and got the same error as above:
import tensorflow as tf
print(tf.version)
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
print(""Num CPUs Available: "", len(tf.config.list_physical_devices('CPU')))
print(tf.config.list_physical_devices('GPU'))
print(tf.config.list_physical_devices('CPU'))
Output:
2.17.0
Num GPUs Available: 0
Num CPUs Available: 1
[]

jmmelen (Issue Creator) on (2024-07-29 22:04:50 UTC): Remember this is on Windows 11.

tilakrayal (Assginee) on (2024-09-02 09:57:16 UTC): TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-)

github-actions[bot] on (2024-09-10 01:59:00 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:58:57 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:59:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72585"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72585"">No</a>

"
2431916716,issue,closed,completed,Issue with tf version 2.15 (ModuleNotFoundError: No module named 'tensorflow.python.distribute.distribution_strategy_context'),"from tensorflow.python.distribute.distribution_strategy_context import variable_sync_on_read_context
ModuleNotFoundError: No module named 'tensorflow.python.distribute.distribution_strategy_context'


!pip uninstall tensorflow
!pip uninstall tensorflow

pip install tensorflow==2.15.0
import tensorflow as tf 
tf.__version__
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In [1], line 1
----> 1 import tensorflow as tf 
      2 tf.__version__

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\__init__.py:45
     42 from tensorflow.python import tf2 as _tf2
     43 _tf2.enable()
---> 45 from ._api.v2 import __internal__
     46 from ._api.v2 import __operators__
     47 from ._api.v2 import audio

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\_api\v2\__internal__\__init__.py:11
      9 from . import decorator
     10 from . import dispatch
---> 11 from . import distribute
     12 from . import eager_context
     13 from . import feature_column

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\_api\v2\__internal__\distribute\__init__.py:11
      9 from . import interim
     10 from . import multi_process_runner
---> 11 from tensorflow.python.distribute.distribution_strategy_context import variable_sync_on_read_context
     12 from tensorflow.python.distribute.merge_call_interim import strategy_supports_no_merge_call
     13 from tensorflow.python.distribute.sharded_variable import ShardedVariable

ModuleNotFoundError: No module named 'tensorflow.python.distribute.distribution_strategy_context'",pradeep10kumar,2024-07-26 10:19:07+00:00,['Venkat6871'],2024-08-09 09:42:21+00:00,2024-08-09 09:42:21+00:00,https://github.com/tensorflow/tensorflow/issues/72565,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2255078736, 'issue_id': 2431916716, 'author': 'Venkat6871', 'body': 'Hi **@pradeep10kumar** ,\r\n- Could you provide more information about this issue? I tried on colab it works fine for me. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/0df6ecbcd61f38073cbcb7fbe415bb9d/72565_2-15-0-v.ipynb) here for reference. \r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 6, 51, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213314, 'issue_id': 2431916716, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 27, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-29 06:51:56 UTC): Hi **@pradeep10kumar** ,
- Could you provide more information about this issue? I tried on colab it works fine for me. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/0df6ecbcd61f38073cbcb7fbe415bb9d/72565_2-15-0-v.ipynb) here for reference. 
Thank you!

github-actions[bot] on (2024-08-06 01:53:27 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

"
2431809706,issue,closed,completed,TfLiteVision can't be initialised,"**System information**
- Android Device information: `samsung/o1sxeea/o1s:14/UP1A.231005.007/G991BXXSBGXED:user/release-keys`
- TensorFlow Lite in Play Services SDK version:

```
tflite_vision = { module = ""org.tensorflow:tensorflow-lite-task-vision-play-services"", version.ref = ""0.4.4"" }
tflite_gpu = { module = ""com.google.android.gms:play-services-tflite-gpu"", version = ""16.2.0"" }
tflite_java = { module = ""com.google.android.gms:play-services-tflite-java"", version = ""16.2.0-beta02"" }
tflite_support = { module = ""com.google.android.gms:play-services-tflite-support"", version = ""16.1.0"" }
tflite_metadata = { module = ""org.tensorflow:tensorflow-lite-metadata"", version.ref = ""0.4.4"" }
```

- Google Play Services version: `24.26.32`

**Code to reproduce the issue**
The app prints a stacktrace in the logs but doesn't crash when calling inside the activities `onCreate` function:

```
    TfLiteVision.initialize(
        context,
        TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(isGpuDelegateAvailable).build()
    )
```

Log that is printed:
```
Failed to get service from broker.  
java.lang.SecurityException: Unknown calling package name 'com.google.android.gms'.
at android.os.Parcel.createExceptionOrNull(Parcel.java:3069)
at android.os.Parcel.createException(Parcel.java:3053)
at android.os.Parcel.readException(Parcel.java:3036)
at android.os.Parcel.readException(Parcel.java:2978)
at m.hv.q(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:206)
at m.gi.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:54)
at android.os.Handler.handleCallback(Handler.java:958)
at android.os.Handler.dispatchMessage(Handler.java:99)
at android.os.Looper.loopOnce(Looper.java:230)
at android.os.Looper.loop(Looper.java:319)
at android.os.HandlerThread.run(HandlerThread.java:67)
```

It further prints the following warning

```
Unable to update local snapshot for com.google.android.libraries.consentverifier#com.google.android.gms, may result in stale flags. (Ask Gemini)
                                                                                         java.util.concurrent.ExecutionException: m.up: 17: 17: API: Phenotype.API is not available on this device. Connection failed with: dd{statusCode=DEVELOPER_ERROR, resolution=null, message=null}
                                                                                         	at m.aot.s(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:21)
                                                                                         	at m.aot.get(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:3)
                                                                                         	at m.aqc.g(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:11)
                                                                                         	at m.wh.d(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.vz.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:5)
                                                                                         	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:487)
                                                                                         	at java.util.concurrent.FutureTask.run(FutureTask.java:264)
                                                                                         	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:307)
                                                                                         	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                                                                                         	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:644)
                                                                                         	at java.lang.Thread.run(Thread.java:1012)
                                                                                         Caused by: m.up: 17: 17: API: Phenotype.API is not available on this device. Connection failed with: dd{statusCode=DEVELOPER_ERROR, resolution=null, message=null}
                                                                                         	at m.uq.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:13)
                                                                                         	at m.aob.f(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:3)
                                                                                         	at m.aod.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:130)
                                                                                         	at m.apn.execute(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.aot.q(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.aot.m(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:101)
                                                                                         	at m.aot.d(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:19)
                                                                                         	at m.tb.d(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.ta.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:35)
                                                                                         	at m.nn.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:12)
                                                                                         	at m.apn.execute(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.no.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:12)
                                                                                         	at m.ob.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:36)
                                                                                         	at m.og.j(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:20)
                                                                                         	at m.ni.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:736)
                                                                                         	at m.apn.execute(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:1)
                                                                                         	at m.nj.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:8)
                                                                                         	at m.ob.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:36)
                                                                                         	at m.oc.c(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:25)
                                                                                         	at m.fh.c(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:7)
                                                                                         	at m.gg.t(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:48)
11:12:04.781 21705-23046 MobStoreFlagStore       com.app.app.stagingserver     W  	at m.gg.f(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:10) (Ask Gemini)
                                                                                         	at m.gg.j(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:178)
                                                                                         	at m.gg.i(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:2)
                                                                                         	at m.gi.run(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:82)
                                                                                         	at android.os.Handler.handleCallback(Handler.java:958)
                                                                                         	at android.os.Handler.dispatchMessage(Handler.java:99)
                                                                                         	at android.os.Looper.loopOnce(Looper.java:230)
                                                                                         	at android.os.Looper.loop(Looper.java:319)
                                                                                         	at android.os.HandlerThread.run(HandlerThread.java:67)
                                                                                         Caused by: m.em: 17: API: Phenotype.API is not available on this device. Connection failed with: dd{statusCode=DEVELOPER_ERROR, resolution=null, message=null}
                                                                                         	at m.hm.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:13)
                                                                                         	at m.fh.c(:com.google.android.gms.policy_tflite_dynamite_dynamite@242327803@242327801042.644155124.644155124:3)
                                                                                         	... 10 more
```",ThomasRichtsfeld,2024-07-26 09:21:40+00:00,['pkgoogle'],2024-10-09 02:01:35+00:00,2024-10-09 02:01:32+00:00,https://github.com/tensorflow/tensorflow/issues/72561,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue')]","[{'comment_id': 2252364670, 'issue_id': 2431809706, 'author': 'bxvd', 'body': ""I am having the same issue but with the Interpreter API instead of Task Vision.\r\n\r\n```\r\nimplementation 'com.google.android.gms:play-services-tflite-java:16.0.1'\r\nimplementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'\r\nimplementation 'com.google.android.gms:play-services-tflite-support:16.0.1'\r\n```\r\n\r\n```kotlin\r\nTfLite.initialize(context, TfLiteInitializationOptions.builder()\r\n  .setEnableGpuDelegateSupport(true)\r\n  .build())\r\n```"", 'created_at': datetime.datetime(2024, 7, 26, 9, 42, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2365157450, 'issue_id': 2431809706, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle\r\n\r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 21, 11, 46, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2369040807, 'issue_id': 2431809706, 'author': 'pkgoogle', 'body': 'Hi @ThomasRichtsfeld, are you doing this in a custom app or are you following an example/documentation? For either case which of these would be closest to your use case? https://github.com/tensorflow/examples/tree/master/lite/examples, https://github.com/google-ai-edge/litert-samples/tree/main/examples. Thanks for any additional information you can provide.', 'created_at': datetime.datetime(2024, 9, 23, 18, 21, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370198550, 'issue_id': 2431809706, 'author': 'ThomasRichtsfeld', 'body': '@pkgoogle I am not sure if any of those fit as I was not able to find the code snippet that caused the trouble for us:\r\n```\r\n    TfLiteVision.initialize(\r\n        context,\r\n        TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(isGpuDelegateAvailable).build()\r\n    )\r\n```\r\n\r\nWe use it in the app of our company to do some image inference.\r\nI am not sure if it matters how we use TfLite as the code I shared above is the one that has to be executed before using anything else from the TfLite API', 'created_at': datetime.datetime(2024, 9, 24, 5, 23, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372414659, 'issue_id': 2431809706, 'author': 'pkgoogle', 'body': 'So the tflite_support library is not recommended for current development: https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/, since you are having an issue with it -- can you try using [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks) instead?', 'created_at': datetime.datetime(2024, 9, 24, 21, 25, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387488865, 'issue_id': 2431809706, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 2, 2, 1, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123747, 'issue_id': 2431809706, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401123794, 'issue_id': 2431809706, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72561"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72561"">No</a>', 'created_at': datetime.datetime(2024, 10, 9, 2, 1, 34, tzinfo=datetime.timezone.utc)}]","bxvd on (2024-07-26 09:42:11 UTC): I am having the same issue but with the Interpreter API instead of Task Vision.

```
implementation 'com.google.android.gms:play-services-tflite-java:16.0.1'
implementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'
implementation 'com.google.android.gms:play-services-tflite-support:16.0.1'
```

```kotlin
TfLite.initialize(context, TfLiteInitializationOptions.builder()
  .setEnableGpuDelegateSupport(true)
  .build())
```

gaikwadrahul8 on (2024-09-21 11:46:24 UTC): Hi, @pkgoogle

Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-23 18:21:34 UTC): Hi @ThomasRichtsfeld, are you doing this in a custom app or are you following an example/documentation? For either case which of these would be closest to your use case? https://github.com/tensorflow/examples/tree/master/lite/examples, https://github.com/google-ai-edge/litert-samples/tree/main/examples. Thanks for any additional information you can provide.

ThomasRichtsfeld (Issue Creator) on (2024-09-24 05:23:34 UTC): @pkgoogle I am not sure if any of those fit as I was not able to find the code snippet that caused the trouble for us:
```
    TfLiteVision.initialize(
        context,
        TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(isGpuDelegateAvailable).build()
    )
```

We use it in the app of our company to do some image inference.
I am not sure if it matters how we use TfLite as the code I shared above is the one that has to be executed before using anything else from the TfLite API

pkgoogle (Assginee) on (2024-09-24 21:25:09 UTC): So the tflite_support library is not recommended for current development: https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/, since you are having an issue with it -- can you try using [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks) instead?

github-actions[bot] on (2024-10-02 02:01:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-09 02:01:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-09 02:01:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72561"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72561"">No</a>

"
2430885198,issue,closed,completed, DLL load failed while importing _pywrap_tensorflow_internal,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

want to run the model

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""C:\Users\Azhar Shaikh\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Azhar Shaikh\Desktop\AiMl\model.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Azhar Shaikh\AppData\Roaming\Python\Python312\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Azhar Shaikh\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Azhar Shaikh\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Relevant log output

_No response_",azhar47-sk,2024-07-25 19:58:17+00:00,['Venkat6871'],2024-08-14 01:54:42+00:00,2024-08-14 01:54:38+00:00,https://github.com/tensorflow/tensorflow/issues/72527,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.0', 'Issues relating to TensorFlow 2.0')]","[{'comment_id': 2254737388, 'issue_id': 2430885198, 'author': 'tcreek', 'body': 'I am seeing the same errors with Python 3.10.6 and Tensorflow 2.17\r\n\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py"", line 38, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n  File ""C:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 85, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File ""C:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py"", line 70, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n```', 'created_at': datetime.datetime(2024, 7, 29, 0, 36, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255037265, 'issue_id': 2430885198, 'author': 'Venkat6871', 'body': 'Hi **@azhar47-sk** ,\r\n- Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from the issue with similar error.It helps.Also in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 6, 22, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256772686, 'issue_id': 2430885198, 'author': 'tcreek', 'body': 'From that comment you linked, it appears I am at  number 2 on the list.\r\n\r\nhttps://ark.intel.com/content/www/us/en/ark/products/33921/intel-core-2-extreme-processor-qx9650-12m-cache-3-00-ghz-1333-mhz-fsb.html', 'created_at': datetime.datetime(2024, 7, 29, 19, 49, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272469938, 'issue_id': 2430885198, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 7, 1, 54, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287684920, 'issue_id': 2430885198, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 14, 1, 54, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287684970, 'issue_id': 2430885198, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72527"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72527"">No</a>', 'created_at': datetime.datetime(2024, 8, 14, 1, 54, 40, tzinfo=datetime.timezone.utc)}]","tcreek on (2024-07-29 00:36:45 UTC): I am seeing the same errors with Python 3.10.6 and Tensorflow 2.17

```
Traceback (most recent call last):
  File ""C:\Users\Test\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Test\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Test\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Test\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.
```

Venkat6871 (Assginee) on (2024-07-29 06:22:27 UTC): Hi **@azhar47-sk** ,
- Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from the issue with similar error.It helps.Also in order to expedite the trouble-shooting process, could you please provide the following information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
Installed using virtualenv? pip? conda?:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

and the exact sequence of commands / steps that you executed before running into the problem.

Thank you!

tcreek on (2024-07-29 19:49:47 UTC): From that comment you linked, it appears I am at  number 2 on the list.

https://ark.intel.com/content/www/us/en/ark/products/33921/intel-core-2-extreme-processor-qx9650-12m-cache-3-00-ghz-1333-mhz-fsb.html

github-actions[bot] on (2024-08-07 01:54:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-14 01:54:38 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-14 01:54:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72527"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72527"">No</a>

"
2430718956,issue,closed,completed,TensorFlow Tutorials not compatible with latest release.,"Until the docs are updated and the Colab backend is upgraded to use the latest TensorFlow version, please flag the tutorials that are not compatible with the latest stable release so it's clear to users with the latest version that they must either rollback or modify their scripts to user Keras 2 to follow the tutorials as they are.",Ayman250,2024-07-25 18:20:58+00:00,['tilakrayal'],2024-08-14 12:40:56+00:00,2024-08-14 12:40:53+00:00,https://github.com/tensorflow/tensorflow/issues/72520,"[('type:docs-bug', 'Document issues'), ('comp:model', 'Model related issues'), ('awaiting PR merge', 'awaiting PR merge')]","[{'comment_id': 2255116449, 'issue_id': 2430718956, 'author': 'tilakrayal', 'body': '@Ayman250,\r\nThank you for reporting the issue. Could you please provide tensorflow tutorial examples where you are facing the issue in the execution. Thank you!', 'created_at': datetime.datetime(2024, 7, 29, 7, 12, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256608626, 'issue_id': 2430718956, 'author': 'Ayman250', 'body': ""Here is one example:\r\n[https://www.tensorflow.org/tutorials/load_data/pandas_dataframe](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)\r\n\r\nRunning tensorflow v 2.17.0 there is an error on the line:\r\n```normalizer.adapt(numeric_features)```\r\n```local variable 'input_shape' referenced before assignment```\r\n\r\nYou need to convert this to a tensor first it cannot be done implicitly.\r\n```numeric_features_tensor = tf.convert_to_tensor(numeric_features)```\r\n\r\nThere are likely more issues downstream in this tutorial."", 'created_at': datetime.datetime(2024, 7, 29, 18, 17, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260950689, 'issue_id': 2430718956, 'author': 'mihaimaruseac', 'body': 'In the past, documentation was tested before the final release. It seems now that step has been skipped.', 'created_at': datetime.datetime(2024, 7, 31, 16, 50, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288636596, 'issue_id': 2430718956, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72520"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72520"">No</a>', 'created_at': datetime.datetime(2024, 8, 14, 12, 40, 55, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-29 07:12:26 UTC): @Ayman250,
Thank you for reporting the issue. Could you please provide tensorflow tutorial examples where you are facing the issue in the execution. Thank you!

Ayman250 (Issue Creator) on (2024-07-29 18:17:14 UTC): Here is one example:
[https://www.tensorflow.org/tutorials/load_data/pandas_dataframe](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)

Running tensorflow v 2.17.0 there is an error on the line:
```normalizer.adapt(numeric_features)```
```local variable 'input_shape' referenced before assignment```

You need to convert this to a tensor first it cannot be done implicitly.
```numeric_features_tensor = tf.convert_to_tensor(numeric_features)```

There are likely more issues downstream in this tutorial.

mihaimaruseac on (2024-07-31 16:50:15 UTC): In the past, documentation was tested before the final release. It seems now that step has been skipped.

google-ml-butler[bot] on (2024-08-14 12:40:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72520"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72520"">No</a>

"
2429402412,issue,closed,completed,A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.,"I am using following version: 

TF Version:  2.18.0-dev20240716
Eager mode:  True
Hub version:  0.16.1


While create the model I am facing the follwoing problem. I tried different version but it did not help.

---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)
Cell In [23], line 1----> 1 model = create_model()      2 model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5),      3              loss= tf.keras.losses.CategoricalCrossentropy(),      4              metrices = [tf.keras.metrics.Accuracy])      5 model.summary()

Cell In [22], line 10, in create_model()      5 input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,      6                                    name=""input_mask"")      7 input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,      8                                     name=""input_type_ids"")---> 10 pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])     11 drop =  tf.keras.layers.Dropout(0.2)(pooled_output)     12 output = tf.keras.Dense(label_list, activation = 'sigmoid', name = ""output"")(drop)

File ~\AppData\Roaming\Python\Python310\site-packages\tf_keras\src\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)     67     filtered_tb = _process_traceback_frames(e.__traceback__)     68     # To get the full stack trace, call:     69     # `tf.debugging.disable_traceback_filtering()`---> 70     raise e.with_traceback(filtered_tb) from None     71 finally:     72     del filtered_tb

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_hub\keras_layer.py:250, in KerasLayer.call(self, inputs, training)    247   else:    248     # Behave like BatchNormalization. (Dropout is different, b/181839368.)    249     training = False--> 250   result = smart_cond.smart_cond(training,    251                                  lambda: f(training=True),    252                                  lambda: f(training=False))    254 # Unwrap dicts returned by signatures.    255 if self._output_key:

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_hub\keras_layer.py:252, in KerasLayer.call.<locals>.<lambda>()    247   else:    248     # Behave like BatchNormalization. (Dropout is different, b/181839368.)    249     training = False    250   result = smart_cond.smart_cond(training,    251                                  lambda: f(training=True),--> 252                                  lambda: f(training=False))    254 # Unwrap dicts returned by signatures.    255 if self._output_key:

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\core\function\polymorphism\function_type.py:583, in canonicalize_to_monomorphic(args, kwargs, default_values, capture_types, polymorphic_type)    577       parameters.append(    578           _make_validated_mono_param(kwarg_name, arg[kwarg_name],    579                                      Parameter.KEYWORD_ONLY, type_context,    580                                      poly_parameter.type_constraint))    581   else:    582     parameters.append(--> 583         _make_validated_mono_param(name, arg, poly_parameter.kind,    584                                    type_context,    585                                    poly_parameter.type_constraint))    587 return FunctionType(parameters, capture_types), type_context

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\core\function\polymorphism\function_type.py:522, in _make_validated_mono_param(name, value, kind, type_context, poly_type)    518 def _make_validated_mono_param(    519     name, value, kind, type_context, poly_type    520 ) -> Parameter:    521   """"""Generates and validates a parameter for Monomorphic FunctionType.""""""--> 522   mono_type = trace_type.from_value(value, type_context)    524   if poly_type and not mono_type.is_subtype_of(poly_type):    525     raise TypeError(f""Parameter `{name}` was expected to be of type ""    526                     f""{poly_type} but is {mono_type}"")

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\core\function\trace_type\trace_type_builder.py:162, in from_value(value, context)    159   return from_value(value.__wrapped__, context)    161 if isinstance(value, list):--> 162   return default_types.List(*(from_value(c, context) for c in value))    164 if isinstance(value, tuple):    165   if util.is_namedtuple(value):

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\core\function\trace_type\trace_type_builder.py:162, in <genexpr>(.0)    159   return from_value(value.__wrapped__, context)    161 if isinstance(value, list):--> 162   return default_types.List(*(from_value(c, context) for c in value))    164 if isinstance(value, tuple):    165   if util.is_namedtuple(value):

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\core\function\trace_type\trace_type_builder.py:185, in from_value(value, context)    178   return default_types.Attrs.from_type_and_attributes(    179       type(value),    180       tuple(    181           from_value(getattr(value, a.name), context)    182           for a in value.__attrs_attrs__))    184 if util.is_np_ndarray(value):--> 185   ndarray = value.__array__()    186   return default_types.TENSOR(ndarray.shape, ndarray.dtype)    188 if isinstance(value, custom_nest_protocol.CustomNestProtocol):

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\backend\common\keras_tensor.py:61, in KerasTensor.__array__(self)     60 def __array__(self):---> 61     raise ValueError(     62         ""A KerasTensor is symbolic: it's a placeholder for a shape ""     63         ""an a dtype. It doesn't have any actual numerical value. ""     64         ""You cannot convert it to a NumPy array.""     65     )

ValueError: Exception encountered when calling layer 'keras_layer' (type KerasLayer).

A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.

Call arguments received by layer 'keras_layer' (type KerasLayer):
   inputs=['<KerasTensor shape=(None, 256), dtype=int32, sparse=None, name=input_word_ids>', '<KerasTensor shape=(None, 256), dtype=int32, sparse=None, name=input_mask>', '<KerasTensor shape=(None, 256), dtype=int32, sparse=None, name=input_type_ids>']
   training=None",pradeep10kumar,2024-07-25 08:49:26+00:00,['Venkat6871'],2024-10-13 02:05:10+00:00,2024-10-13 02:05:09+00:00,https://github.com/tensorflow/tensorflow/issues/72497,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2255351060, 'issue_id': 2429402412, 'author': 'Venkat6871', 'body': 'Hi **@pradeep10kumar** ,\r\n- Make sure the bert_layer is properly loaded and initialized. If it is a TensorFlow Hub layer, Confirm it is correctly set up. And In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 8, 44, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267258011, 'issue_id': 2429402412, 'author': 'muhammadanas0716', 'body': 'import tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\n# Define IMAGE_SHAPE\r\nIMAGE_SHAPE = (224, 224)\r\n\r\ndef create_model(model_url, num_classes=10):\r\n    """"""\r\n    Takes a TensorFlow Hub URL and creates a Keras model with it using the functional API.\r\n\r\n    Args:\r\n        model_url (str): A TensorFlow Hub feature extraction URL.\r\n        num_classes (int): Number of output neurons in the output layer,\r\n          should be equal to number of target classes, default 10.\r\n\r\n    Returns:\r\n        An uncompiled Keras model with model_url as feature extractor\r\n        layer and Dense output layer with num_classes output neurons.\r\n    """"""\r\n    # Download the pretrained model and save it as a Keras layer\r\n    feature_extractor_layer = hub.KerasLayer(model_url,\r\n                                             trainable=False,  # freeze the already learned patterns\r\n                                             input_shape=IMAGE_SHAPE + (3,))\r\n    # Define the input layer\r\n    inputs = tf.keras.Input(shape=IMAGE_SHAPE + (3,))\r\n    # Pass the inputs through the feature extractor layer\r\n    x = feature_extractor_layer(inputs)  # Explicitly set training=False\r\n    # Define the output layer\r\n    outputs = tf.keras.layers.Dense(num_classes, activation=""softmax"")(x)\r\n\r\n    # Create the model\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n    return model\r\n\r\n# Example usage\r\n# Ensure you have a valid URL and the required data\r\nresnet_url = ""https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5""\r\nnum_classes = 10  # For example purposes\r\nresnet_model = create_model(resnet_url, num_classes=num_classes)\r\n\r\n# Print the model summary to verify\r\nresnet_model.summary()', 'created_at': datetime.datetime(2024, 8, 4, 2, 30, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307987480, 'issue_id': 2429402412, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 24, 1, 53, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312837528, 'issue_id': 2429402412, 'author': 'ssimrandr', 'body': ""I am also facing a similar issue with the later TF versions. The same code runs without errors on TF 2.10.0, but with 2.16.1 throws this error.\r\n---------------------------\r\nValueError: Exception encountered when calling layer 'autoregressive_network' (type AutoregressiveNetwork).\r\n\r\nA KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\r\n\r\n```\r\nx = Input(...)\r\n...\r\ntf_fn(x)  # Invalid.\r\n```\r\n\r\nWhat you should do instead is wrap `tf_fn` in a layer:\r\n\r\n```\r\nclass MyLayer(Layer):\r\n    def call(self, x):\r\n        return tf_fn(x)\r\n\r\nx = MyLayer()(x)\r\n```\r\n\r\n\r\nCall arguments received by layer 'autoregressive_network' (type AutoregressiveNetwork):\r\n   x=<KerasTensor shape=(None, 2), dtype=float\r\n\r\n--------------\r\n\r\nCan someone else try this code given here : https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/AutoregressiveNetwork\r\n\r\n---------------\r\n# Generate data -- as in Figure 1 in [Papamakarios et al. (2017)][2]).\r\nn = 2000\r\nx2 = np.random.randn(n).astype(dtype=np.float32) * 2.\r\nx1 = np.random.randn(n).astype(dtype=np.float32) + (x2 * x2 / 4.)\r\ndata = np.stack([x1, x2], axis=-1)\r\n\r\n# Density estimation with MADE.\r\nmade = tfb.AutoregressiveNetwork(params=2, hidden_units=[10, 10])\r\n\r\ndistribution = tfd.TransformedDistribution(\r\n    distribution=tfd.Sample(tfd.Normal(loc=0., scale=1.), sample_shape=[2]),\r\n    bijector=tfb.MaskedAutoregressiveFlow(made))\r\n\r\n# Construct and fit model.\r\nx_ = tfkl.Input(shape=(2,), dtype=tf.float32)\r\nlog_prob_ = distribution.log_prob(x_)\r\nmodel = tfk.Model(x_, log_prob_)\r\n\r\nmodel.compile(optimizer=tf.optimizers.Adam(),\r\n              loss=lambda _, log_prob: -log_prob)\r\n\r\nbatch_size = 25\r\nmodel.fit(x=data,\r\n          y=np.zeros((n, 0), dtype=np.float32),\r\n          batch_size=batch_size,\r\n          epochs=1,\r\n          steps_per_epoch=1,  # Usually `n // batch_size`.\r\n          shuffle=True,\r\n          verbose=True)\r\n\r\n# Use the fitted distribution.\r\ndistribution.sample((3, 1))\r\ndistribution.log_prob(np.ones((3, 2), dtype=np.float32))\r\n-----------------------\r\n\r\nI am trying to understand if this is a genuine error or due to some installation problems on my end."", 'created_at': datetime.datetime(2024, 8, 27, 15, 11, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336739, 'issue_id': 2429402412, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 18, 1, 58, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372733747, 'issue_id': 2429402412, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 25, 2, 2, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378529830, 'issue_id': 2429402412, 'author': 'Venkat6871', 'body': 'Hi **@muhammadanas0716** , **@ssimrandr** ,\r\nCould you please raise a new issue for your concern, so that it will be easier to track.\r\nHi @pradeep10kumar ,\r\nPlease try to provide the code snippet as I requested above.\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 27, 6, 53, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394860206, 'issue_id': 2429402412, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 5, 2, 0, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408785025, 'issue_id': 2429402412, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 13, 2, 5, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-29 08:44:24 UTC): Hi **@pradeep10kumar** ,
- Make sure the bert_layer is properly loaded and initialized. If it is a TensorFlow Hub layer, Confirm it is correctly set up. And In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.

Thank you!

muhammadanas0716 on (2024-08-04 02:30:22 UTC): import tensorflow as tf
import tensorflow_hub as hub

# Define IMAGE_SHAPE
IMAGE_SHAPE = (224, 224)

def create_model(model_url, num_classes=10):
    """"""
    Takes a TensorFlow Hub URL and creates a Keras model with it using the functional API.

    Args:
        model_url (str): A TensorFlow Hub feature extraction URL.
        num_classes (int): Number of output neurons in the output layer,
          should be equal to number of target classes, default 10.

    Returns:
        An uncompiled Keras model with model_url as feature extractor
        layer and Dense output layer with num_classes output neurons.
    """"""
    # Download the pretrained model and save it as a Keras layer
    feature_extractor_layer = hub.KerasLayer(model_url,
                                             trainable=False,  # freeze the already learned patterns
                                             input_shape=IMAGE_SHAPE + (3,))
    # Define the input layer
    inputs = tf.keras.Input(shape=IMAGE_SHAPE + (3,))
    # Pass the inputs through the feature extractor layer
    x = feature_extractor_layer(inputs)  # Explicitly set training=False
    # Define the output layer
    outputs = tf.keras.layers.Dense(num_classes, activation=""softmax"")(x)

    # Create the model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    return model

# Example usage
# Ensure you have a valid URL and the required data
resnet_url = ""https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5""
num_classes = 10  # For example purposes
resnet_model = create_model(resnet_url, num_classes=num_classes)

# Print the model summary to verify
resnet_model.summary()

github-actions[bot] on (2024-08-24 01:53:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

ssimrandr on (2024-08-27 15:11:29 UTC): I am also facing a similar issue with the later TF versions. The same code runs without errors on TF 2.10.0, but with 2.16.1 throws this error.
---------------------------
ValueError: Exception encountered when calling layer 'autoregressive_network' (type AutoregressiveNetwork).

A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:

```
x = Input(...)
...
tf_fn(x)  # Invalid.
```

What you should do instead is wrap `tf_fn` in a layer:

```
class MyLayer(Layer):
    def call(self, x):
        return tf_fn(x)

x = MyLayer()(x)
```


Call arguments received by layer 'autoregressive_network' (type AutoregressiveNetwork):
   x=<KerasTensor shape=(None, 2), dtype=float

--------------

Can someone else try this code given here : https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/AutoregressiveNetwork

---------------
# Generate data -- as in Figure 1 in [Papamakarios et al. (2017)][2]).
n = 2000
x2 = np.random.randn(n).astype(dtype=np.float32) * 2.
x1 = np.random.randn(n).astype(dtype=np.float32) + (x2 * x2 / 4.)
data = np.stack([x1, x2], axis=-1)

# Density estimation with MADE.
made = tfb.AutoregressiveNetwork(params=2, hidden_units=[10, 10])

distribution = tfd.TransformedDistribution(
    distribution=tfd.Sample(tfd.Normal(loc=0., scale=1.), sample_shape=[2]),
    bijector=tfb.MaskedAutoregressiveFlow(made))

# Construct and fit model.
x_ = tfkl.Input(shape=(2,), dtype=tf.float32)
log_prob_ = distribution.log_prob(x_)
model = tfk.Model(x_, log_prob_)

model.compile(optimizer=tf.optimizers.Adam(),
              loss=lambda _, log_prob: -log_prob)

batch_size = 25
model.fit(x=data,
          y=np.zeros((n, 0), dtype=np.float32),
          batch_size=batch_size,
          epochs=1,
          steps_per_epoch=1,  # Usually `n // batch_size`.
          shuffle=True,
          verbose=True)

# Use the fitted distribution.
distribution.sample((3, 1))
distribution.log_prob(np.ones((3, 2), dtype=np.float32))
-----------------------

I am trying to understand if this is a genuine error or due to some installation problems on my end.

github-actions[bot] on (2024-09-18 01:58:58 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-25 02:02:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

Venkat6871 (Assginee) on (2024-09-27 06:53:03 UTC): Hi **@muhammadanas0716** , **@ssimrandr** ,
Could you please raise a new issue for your concern, so that it will be easier to track.
Hi @pradeep10kumar ,
Please try to provide the code snippet as I requested above.
Thank you!

github-actions[bot] on (2024-10-05 02:00:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-13 02:05:09 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2428705965,issue,closed,completed,error: 'NPY_NTYPES' was not declared in this scope; did you mean 'NPY_TYPES'?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux openeuler-riscv-4-2 6.6.0

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

5.3.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current behavior?

I am trying to build TensorFlow2.13.0 on aarch64 with bazel5.3.0, but there are problems during the build process
```
ERROR: /home/tf2130/tensorflow/tensorflow/python/lib/core/BUILD:272:11: Compiling tensorflow/python/lib/core/ndarray_tensor.cc failed: (Exit 1): gcc failed: error executing command /usr/lib64/ccache/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 455 arguments skipped)
tensorflow/python/lib/core/ndarray_tensor.cc: In function 'const char* tensorflow::{anonymous}::numpy_type_name(int)':
tensorflow/python/lib/core/ndarray_tensor.cc:70:15: error: 'NPY_NTYPES' was not declared in this scope; did you mean 'NPY_TYPES'?
   70 |     TYPE_CASE(NPY_NTYPES);
      |               ^~~~~~~~~~
tensorflow/python/lib/core/ndarray_tensor.cc:43:8: note: in definition of macro 'TYPE_CASE'
   43 |   case s:            \
      |        ^
tensorflow/python/lib/core/ndarray_tensor.cc: In function 'tsl::Status tensorflow::{anonymous}::PyArrayDescr_to_TF_DataType(PyArray_Descr*, TF_DataType*)':
tensorflow/python/lib/core/ndarray_tensor.cc:87:14: error: 'PyArray_Descr' has no member named 'fields'
   87 |   if (descr->fields == nullptr) {
      |              ^~~~~~
tensorflow/python/lib/core/ndarray_tensor.cc:91:26: error: 'PyArray_Descr' has no member named 'fields'
   91 |   if (PyDict_Next(descr->fields, &pos, &key, &value)) {
      |                          ^~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 29719.590s, Critical Path: 550.42s
INFO: 13738 processes: 1412 internal, 12326 local.
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout tags/v2.13.0
bazel build //tensorflow/tools/pip_package:build_pip_package --local_ram_resoues=1024 --jobs=6
```


### Relevant log output

_No response_",6eanut,2024-07-25 00:38:26+00:00,['tilakrayal'],2024-11-26 02:06:12+00:00,2024-11-26 02:06:10+00:00,https://github.com/tensorflow/tensorflow/issues/72480,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2255742085, 'issue_id': 2428705965, 'author': 'tilakrayal', 'body': '@6eanut,\r\nCould you please try to install the tensorflow with the compatible tested build configurations.\r\ntensorflow-2.13.0, python-\t3.8-3.11,\tClang 16.0.0,\tBazel 5.3.0, cuDNN-8.6, CUDA-11.8\r\nhttps://www.tensorflow.org/install/source#gpu\r\n\r\nAnd tensorflow v2.13 is an old version, please try to install  with the latest tensorflow version. Thank you!', 'created_at': datetime.datetime(2024, 7, 29, 11, 59, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256384807, 'issue_id': 2428705965, 'author': '6eanut', 'body': '@tilakrayal thanks for helping!\r\nI did configure the tool according to the corresponding version, but I still encountered the above problem', 'created_at': datetime.datetime(2024, 7, 29, 16, 28, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333741970, 'issue_id': 2428705965, 'author': 'twinsant', 'body': 'same error', 'created_at': datetime.datetime(2024, 9, 6, 10, 18, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468407953, 'issue_id': 2428705965, 'author': 'tilakrayal', 'body': 'Could you please check whether the same issue you are facing with the latest Tensorflow v2.17 or v2.18 and provide the update where the supported python version has been upgraded to 3.9-3.12.\r\n\r\nhttps://www.tensorflow.org/install/source#cpu\r\nThank you!', 'created_at': datetime.datetime(2024, 11, 11, 15, 12, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484553824, 'issue_id': 2428705965, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 11, 19, 2, 5, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499454480, 'issue_id': 2428705965, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 11, 26, 2, 6, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499454540, 'issue_id': 2428705965, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72480"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72480"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 2, 6, 11, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-29 11:59:58 UTC): @6eanut,
Could you please try to install the tensorflow with the compatible tested build configurations.
tensorflow-2.13.0, python-	3.8-3.11,	Clang 16.0.0,	Bazel 5.3.0, cuDNN-8.6, CUDA-11.8
https://www.tensorflow.org/install/source#gpu

And tensorflow v2.13 is an old version, please try to install  with the latest tensorflow version. Thank you!

6eanut (Issue Creator) on (2024-07-29 16:28:37 UTC): @tilakrayal thanks for helping!
I did configure the tool according to the corresponding version, but I still encountered the above problem

twinsant on (2024-09-06 10:18:28 UTC): same error

tilakrayal (Assginee) on (2024-11-11 15:12:03 UTC): Could you please check whether the same issue you are facing with the latest Tensorflow v2.17 or v2.18 and provide the update where the supported python version has been upgraded to 3.9-3.12.

https://www.tensorflow.org/install/source#cpu
Thank you!

github-actions[bot] on (2024-11-19 02:05:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-11-26 02:06:10 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-11-26 02:06:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72480"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72480"">No</a>

"
2428699925,issue,open,,Does TensorFlow2.13.0 support RISC-V,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux openeuler-riscv-4-2 6.6.0

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

5.3.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current behavior?

I recently tried to build TensorFlow2.13.0 with bazel5.3.0 on RISC-V, but I encountered the following error during the build process
```
ERROR: /home/tf2130/.cache/bazel/_bazel_tf2130/4d8a15755e0d938e330a7b941554a2cb/external/mkl_dnn_v1/BUILD.bazel:146:11: Compiling src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp failed: (Exit 1): gcc failed: error executing command /usr/lib64/ccache/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 67 arguments skipped)
external/mkl_dnn_v1/src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp: In member function 'void dnnl::impl::cpu::x64::brgemm_diff_src_layer_iter_t<weights_t, scratch_t, gemm_acc_t>::execute() const':
external/mkl_dnn_v1/src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp:102:37: error: 'const struct dnnl::impl::cpu::rnn_utils::diff_src_brgemm_conf_t' has no member named 'isa'
  102 |             && rnn_.diff_src_brgemm.isa == x64::avx512_core_bf16_amx_bf16) {
      |                                     ^~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 69717.150s, Critical Path: 1675.59s
INFO: 9452 processes: 1564 internal, 7888 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout tags/v2.13.0
bazel build //tensorflow/tools/pip_package:build_pip_package --local_ram_resoues=1024 --jobs=6
```


### Relevant log output

_No response_",6eanut,2024-07-25 00:30:37+00:00,['Venkat6871'],2024-11-08 07:28:51+00:00,,https://github.com/tensorflow/tensorflow/issues/72479,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2249115466, 'issue_id': 2428699925, 'author': '6eanut', 'body': 'i use the [bazel5.3.0](https://gitee.com/link?target=https%3A%2F%2Fbuild-repo.tarsier-infra.isrc.ac.cn%2Fhome%3A%2Flaokz%3A%2Fbranches%3A%2FopenEuler%3A%2F24.03%2Fmainline_riscv64%2Friscv64%2Fbazel-5.3.0-2.oe2403.riscv64.rpm).\r\n**environment:**\r\n```\r\n$ uname -a\r\nLinux openeuler-riscv-4-2 6.6.0 #1 SMP Tue Jul  2 11:21:06 CST 2024 riscv64 riscv64 riscv64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=""openEuler""\r\nVERSION=""24.03 (LTS)""\r\nID=""openEuler""\r\nVERSION_ID=""24.03""\r\nPRETTY_NAME=""openEuler 24.03 (LTS)""\r\nANSI_COLOR=""0;31""\r\n```', 'created_at': datetime.datetime(2024, 7, 25, 0, 32, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254967389, 'issue_id': 2428699925, 'author': 'Venkat6871', 'body': 'Hi **@6eanut** ,\r\n- Is there any specific reason for using TensorFlow2.13.0? We are suggesting  to use latest version(2.17.0) is for better results. Please upgrade your version and let us know whether the issue persists with recent TF versions.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 5, 19, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255213392, 'issue_id': 2428699925, 'author': '6eanut', 'body': '@Venkat6871 thanks for helping!\r\nBecause compiling tensorflow2.17.0 requires bazel6.5.0, I chose to compile 2.13.0 (which only requires bazel5.3.0).\r\nbazel does not currently support risc-v, so the latest version of bazel I have is 5.3.0.\r\nI will try to compile bazel6.5.0 for risc-v', 'created_at': datetime.datetime(2024, 7, 29, 7, 38, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259524933, 'issue_id': 2428699925, 'author': '6eanut', 'body': '@Venkat6871 \r\nI recently tried to build bazel6.5.0 on risc-v.\r\n```\r\n$ bazel version\r\nBuild label: 6.5.0\r\nBuild target: bazel-out/riscv64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 30 08:20:13 2024 (1722327613)\r\nBuild timestamp: 1722327613\r\nBuild timestamp as int: 1722327613\r\n```\r\nI cloned tensorflow from github and switched to tag at v2.17.0, and the following issue occurred\r\n```\r\n$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow\r\nINFO: Reading \'startup\' options from /home/tf2170/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited \'common\' options: --isatty=1 --terminal_columns=121\r\nINFO: Reading rc options for \'build\' from /home/tf2170/tensorflow/.bazelrc:\r\n  Inherited \'common\' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for \'build\' from /home/tf2170/tensorflow/.bazelrc:\r\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for \'build\' from /home/tf2170/tensorflow/.tf_configure.bazelrc:\r\n  \'build\' options: --action_env PYTHON_BIN_PATH=/home/tf2170/venv00/bin/python3 --action_env PYTHON_LIB_PATH=/home/tf2170/venv00/lib/python3.11/site-packages --python_path=/home/tf2170/venv00/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /home/tf2170/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/tf2170/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/tf2170/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/tf2170/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: /DEFAULT.WORKSPACE.SUFFIX:80:31: syntax error at \'}\': expected :\r\nERROR: Error computing the main repository mapping: error loading package \'external\': Failed to parse default WORKSPACE file suffix\r\nLoading:\r\n```\r\nmore info:\r\n```\r\n$ ./configure\r\nWARNING: current bazel installation is not a release version.\r\nPlease specify the location of python. [Default is /home/tf2170/venv00/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/tf2170/venv00/lib/python3.11/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/tf2170/venv00/lib/python3.11/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you want to use Clang to build TensorFlow? [Y/n]: N\r\nGCC will be used to compile TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```', 'created_at': datetime.datetime(2024, 7, 31, 2, 35, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259762296, 'issue_id': 2428699925, 'author': '6eanut', 'body': 'In addition, I would like to know if there are other ways to build tensorflow wheel besides bazel', 'created_at': datetime.datetime(2024, 7, 31, 6, 24, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317034723, 'issue_id': 2428699925, 'author': '6eanut', 'body': '@Venkat6871 \r\nI am currently building the latest version of tensorflow2.17.0 using bazel6.5.0\r\nbazel6.5.0 is the rpm package [here](https://github.com/6eanut/bazel/releases/tag/6.5.0) which has passed cpp and java tests in [examples](https://github.com/bazelbuild/examples)\r\ntensorflow is tags/v2.17.0\r\nAnd then i have this problem\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\n... still trying to connect to local Bazel server (37701) after 10 seconds ...\r\n... still trying to connect to local Bazel server (37701) after 20 seconds ...\r\n... still trying to connect to local Bazel server (37701) after 30 seconds ...\r\n... still trying to connect to local Bazel server (37701) after 40 seconds ...\r\nINFO: Reading \'startup\' options from /root/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited \'common\' options: --isatty=1 --terminal_columns=121\r\nINFO: Reading rc options for \'build\' from /root/tensorflow/.bazelrc:\r\n  Inherited \'common\' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for \'build\' from /root/tensorflow/.bazelrc:\r\n  \'build\' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for \'build\' from /root/tensorflow/.tf_configure.bazelrc:\r\n  \'build\' options: --action_env PYTHON_BIN_PATH=/root/venv311/bin/python3 --action_env PYTHON_LIB_PATH=/root/venv311/lib/python3.11/site-packages --python_path=/root/venv311/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository python instantiated at:\r\n  /root/tensorflow/WORKSPACE:47:27: in <toplevel>\r\n  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/repositories.bzl:603:22: in python_register_toolchains\r\nRepository rule toolchain_aliases defined at:\r\n  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl:236:36: in <toplevel>\r\nERROR: An error occurred during the fetch of repository \'python\':\r\n   Traceback (most recent call last):\r\n        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl\r\n                host_platform = get_host_platform(os_name, arch)\r\n        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform\r\n                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))\r\nError in fail: No platform declared for host OS linux on arch riscv64\r\nERROR: /root/tensorflow/WORKSPACE:47:27: fetching toolchain_aliases rule //external:python: Traceback (most recent call last):\r\n        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl\r\n                host_platform = get_host_platform(os_name, arch)\r\n        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform\r\n                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))\r\nError in fail: No platform declared for host OS linux on arch riscv64\r\nERROR: Error computing the main repository mapping: no such package \'@python//\': No platform declared for host OS linux on arch riscv64\r\nLoading:\r\n```', 'created_at': datetime.datetime(2024, 8, 29, 8, 39, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463945859, 'issue_id': 2428699925, 'author': 'zinovya', 'body': ""I'm getting similar error with tensorflow 2.18.0 and Bazel 7.2.1 (with both gcc and clang)\r\n```\r\n$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\r\nWARNING: Output base '/home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nStarting local Bazel server and connecting to it...\r\n... still trying to connect to local Bazel server (45328) after 10 seconds ...\r\n... still trying to connect to local Bazel server (45328) after 20 seconds ...\r\n... still trying to connect to local Bazel server (45328) after 30 seconds ...\r\n... still trying to connect to local Bazel server (45328) after 40 seconds ...\r\nINFO: Reading 'startup' options from /home/alexzinovyev/dev/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=186\r\nINFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.11/site-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-16 --repo_env=CC=/usr/bin/clang-16 --repo_env=BAZEL_COMPILER=/usr/bin/clang-16 --copt=-Wno-gnu-offsetof-extensions\r\nINFO: Found applicable config definition build:short_logs in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: --enable_bzlmod is set, but no MODULE.bazel file was found at the workspace root. Bazel will create an empty MODULE.bazel file. Please consider migrating your external dependencies from WORKSPACE to MODULE.bazel. For more details, please refer to https://github.com/bazelbuild/bazel/issues/18958.\r\nDEBUG: /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_repo.bzl:96:14: \r\nHERMETIC_PYTHON_VERSION variable was not set correctly, using default version.\r\nPython 3.11 will be used.\r\nTo select Python version, either set HERMETIC_PYTHON_VERSION env variable in\r\nyour shell:\r\n  export HERMETIC_PYTHON_VERSION=3.12\r\nOR pass it as an argument to bazel command directly or inside your .bazelrc\r\nfile:\r\n  --repo_env=HERMETIC_PYTHON_VERSION=3.12\r\nDEBUG: /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.11\r\nERROR: Failed to load Starlark extension '@@pypi//:requirements.bzl'.\r\nCycle in the workspace file detected. This indicates that a repository is used prior to being defined.\r\nThe following chain of repository dependencies lead to the missing definition.\r\n - @@pypi\r\n - @@python_riscv64-unknown-linux-gnu\r\nThis could either mean you have to add the '@@python_riscv64-unknown-linux-gnu' repository with a statement like `http_archive` in your WORKSPACE file (note that transitive dependencies are not added automatically), or move an existing definition earlier in your WORKSPACE file.\r\nINFO: Repository pypi instantiated at:\r\n  /home/alexzinovyev/dev/tensorflow/WORKSPACE:55:16: in <toplevel>\r\n  /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_init_pip.bzl:29:14: in python_init_pip\r\nRepository rule pip_repository defined at:\r\n  /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/rules_python/python/private/pypi/pip_repository.bzl:210:33: in <toplevel>\r\nERROR: Error computing the main repository mapping: cycles detected during computation of main repo mapping\r\nComputing main repo mapping: \r\n    Fetching repository @@pypi; starting\r\n```\r\nAny suggestion how to get the build going on risc-v?"", 'created_at': datetime.datetime(2024, 11, 8, 7, 25, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463950075, 'issue_id': 2428699925, 'author': '6eanut', 'body': '@zinovya \r\nI guess this is a problem with bazel not being adapted for rv.\r\nAlso, how did you get the riscv bazel7.2.1?', 'created_at': datetime.datetime(2024, 11, 8, 7, 28, 50, tzinfo=datetime.timezone.utc)}]","6eanut (Issue Creator) on (2024-07-25 00:32:18 UTC): i use the [bazel5.3.0](https://gitee.com/link?target=https%3A%2F%2Fbuild-repo.tarsier-infra.isrc.ac.cn%2Fhome%3A%2Flaokz%3A%2Fbranches%3A%2FopenEuler%3A%2F24.03%2Fmainline_riscv64%2Friscv64%2Fbazel-5.3.0-2.oe2403.riscv64.rpm).
**environment:**
```
$ uname -a
Linux openeuler-riscv-4-2 6.6.0 #1 SMP Tue Jul  2 11:21:06 CST 2024 riscv64 riscv64 riscv64 GNU/Linux
$ cat /etc/os-release
NAME=""openEuler""
VERSION=""24.03 (LTS)""
ID=""openEuler""
VERSION_ID=""24.03""
PRETTY_NAME=""openEuler 24.03 (LTS)""
ANSI_COLOR=""0;31""
```

Venkat6871 (Assginee) on (2024-07-29 05:19:51 UTC): Hi **@6eanut** ,
- Is there any specific reason for using TensorFlow2.13.0? We are suggesting  to use latest version(2.17.0) is for better results. Please upgrade your version and let us know whether the issue persists with recent TF versions.

Thank you!

6eanut (Issue Creator) on (2024-07-29 07:38:25 UTC): @Venkat6871 thanks for helping!
Because compiling tensorflow2.17.0 requires bazel6.5.0, I chose to compile 2.13.0 (which only requires bazel5.3.0).
bazel does not currently support risc-v, so the latest version of bazel I have is 5.3.0.
I will try to compile bazel6.5.0 for risc-v

6eanut (Issue Creator) on (2024-07-31 02:35:02 UTC): @Venkat6871 
I recently tried to build bazel6.5.0 on risc-v.
```
$ bazel version
Build label: 6.5.0
Build target: bazel-out/riscv64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 30 08:20:13 2024 (1722327613)
Build timestamp: 1722327613
Build timestamp as int: 1722327613
```
I cloned tensorflow from github and switched to tag at v2.17.0, and the following issue occurred
```
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
INFO: Reading 'startup' options from /home/tf2170/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=121
INFO: Reading rc options for 'build' from /home/tf2170/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/tf2170/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/tf2170/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/tf2170/venv00/bin/python3 --action_env PYTHON_LIB_PATH=/home/tf2170/venv00/lib/python3.11/site-packages --python_path=/home/tf2170/venv00/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/tf2170/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/tf2170/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/tf2170/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/tf2170/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
ERROR: /DEFAULT.WORKSPACE.SUFFIX:80:31: syntax error at '}': expected :
ERROR: Error computing the main repository mapping: error loading package 'external': Failed to parse default WORKSPACE file suffix
Loading:
```
more info:
```
$ ./configure
WARNING: current bazel installation is not a release version.
Please specify the location of python. [Default is /home/tf2170/venv00/bin/python3]:


Found possible Python library paths:
  /home/tf2170/venv00/lib/python3.11/site-packages
Please input the desired Python library path to use.  Default is [/home/tf2170/venv00/lib/python3.11/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]: N
GCC will be used to compile TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```

6eanut (Issue Creator) on (2024-07-31 06:24:01 UTC): In addition, I would like to know if there are other ways to build tensorflow wheel besides bazel

6eanut (Issue Creator) on (2024-08-29 08:39:13 UTC): @Venkat6871 
I am currently building the latest version of tensorflow2.17.0 using bazel6.5.0
bazel6.5.0 is the rpm package [here](https://github.com/6eanut/bazel/releases/tag/6.5.0) which has passed cpp and java tests in [examples](https://github.com/bazelbuild/examples)
tensorflow is tags/v2.17.0
And then i have this problem
```
bazel build //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
... still trying to connect to local Bazel server (37701) after 10 seconds ...
... still trying to connect to local Bazel server (37701) after 20 seconds ...
... still trying to connect to local Bazel server (37701) after 30 seconds ...
... still trying to connect to local Bazel server (37701) after 40 seconds ...
INFO: Reading 'startup' options from /root/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=121
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/venv311/bin/python3 --action_env PYTHON_LIB_PATH=/root/venv311/lib/python3.11/site-packages --python_path=/root/venv311/bin/python3
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository python instantiated at:
  /root/tensorflow/WORKSPACE:47:27: in <toplevel>
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/repositories.bzl:603:22: in python_register_toolchains
Repository rule toolchain_aliases defined at:
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl:236:36: in <toplevel>
ERROR: An error occurred during the fetch of repository 'python':
   Traceback (most recent call last):
        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl
                host_platform = get_host_platform(os_name, arch)
        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform
                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))
Error in fail: No platform declared for host OS linux on arch riscv64
ERROR: /root/tensorflow/WORKSPACE:47:27: fetching toolchain_aliases rule //external:python: Traceback (most recent call last):
        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 149, column 38, in _toolchain_aliases_impl
                host_platform = get_host_platform(os_name, arch)
        File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/rules_python/python/private/toolchains_repo.bzl"", line 325, column 13, in get_host_platform
                fail(""No platform declared for host OS {} on arch {}"".format(os_name, arch))
Error in fail: No platform declared for host OS linux on arch riscv64
ERROR: Error computing the main repository mapping: no such package '@python//': No platform declared for host OS linux on arch riscv64
Loading:
```

zinovya on (2024-11-08 07:25:25 UTC): I'm getting similar error with tensorflow 2.18.0 and Bazel 7.2.1 (with both gcc and clang)
```
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu
WARNING: Output base '/home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de' is on NFS. This may lead to surprising failures and undetermined behavior.
Starting local Bazel server and connecting to it...
... still trying to connect to local Bazel server (45328) after 10 seconds ...
... still trying to connect to local Bazel server (45328) after 20 seconds ...
... still trying to connect to local Bazel server (45328) after 30 seconds ...
... still trying to connect to local Bazel server (45328) after 40 seconds ...
INFO: Reading 'startup' options from /home/alexzinovyev/dev/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=186
INFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/alexzinovyev/dev/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.11/site-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-16 --repo_env=CC=/usr/bin/clang-16 --repo_env=BAZEL_COMPILER=/usr/bin/clang-16 --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/alexzinovyev/dev/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: --enable_bzlmod is set, but no MODULE.bazel file was found at the workspace root. Bazel will create an empty MODULE.bazel file. Please consider migrating your external dependencies from WORKSPACE to MODULE.bazel. For more details, please refer to https://github.com/bazelbuild/bazel/issues/18958.
DEBUG: /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_repo.bzl:96:14: 
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.11 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.11
ERROR: Failed to load Starlark extension '@@pypi//:requirements.bzl'.
Cycle in the workspace file detected. This indicates that a repository is used prior to being defined.
The following chain of repository dependencies lead to the missing definition.
 - @@pypi
 - @@python_riscv64-unknown-linux-gnu
This could either mean you have to add the '@@python_riscv64-unknown-linux-gnu' repository with a statement like `http_archive` in your WORKSPACE file (note that transitive dependencies are not added automatically), or move an existing definition earlier in your WORKSPACE file.
INFO: Repository pypi instantiated at:
  /home/alexzinovyev/dev/tensorflow/WORKSPACE:55:16: in <toplevel>
  /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/local_xla/third_party/py/python_init_pip.bzl:29:14: in python_init_pip
Repository rule pip_repository defined at:
  /home/alexzinovyev/.cache/bazel/_bazel_alexzinovyev/0da2ad89d9ab383d81720f5a9ee2d3de/external/rules_python/python/private/pypi/pip_repository.bzl:210:33: in <toplevel>
ERROR: Error computing the main repository mapping: cycles detected during computation of main repo mapping
Computing main repo mapping: 
    Fetching repository @@pypi; starting
```
Any suggestion how to get the build going on risc-v?

6eanut (Issue Creator) on (2024-11-08 07:28:50 UTC): @zinovya 
I guess this is a problem with bazel not being adapted for rv.
Also, how did you get the riscv bazel7.2.1?

"
2428697364,issue,closed,completed,Distributed Training without strategies- can't average gradients,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu Server

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to not use a tf.dataset for a distributed learning scenario. As such, using MirroredStrategy is out of the question.
However, I can replicate MirroredStrategy by creating multiple instances of my model, one for each GPU.
And I can gather gradients and manually send batches to each model using said method.
However, once I received all the gradients from tf.gradients(), I am unable to average them. I tried tf.stack, converting to np, etc, but they all fail due to dimensions being different within the gradient itself. Each model gives the correct gradient back for each batch, but as I'm unable to average them I am unable to train the model on multiple gpus.

I'm not sure how I'm supposed to average/accumulate gradients between each GPU, and unsure how mirroredstrategy achieves this.



### Standalone code to reproduce the issue

```shell
print('Gen gradients obtained from '+str(len(gen_grads))+' gpus')
                print('Disc gradients obtained from '+str(len(disc_grads))+' gpus')
                print(gen_grads)
                print(disc_grads)
                with tf.device('/gpu:'+self.gpus[0]):
                    gen_grad=zip(*gen_grads)
                    print(gen_grad)
                    gen_grad=tf.reduce_mean(tf.stack(gen_grads),axis=0)
                    disc_grad=tf.reduce_mean(tf.stack(disc_grad),axis=0)
                    print(tf.shape(gen_grad))
                    print(tf.shape(gen_grads[0]))
                for idx,model in enumerate(self.models):
                    gpu=self.gpus[idx]
                    with tf.device('/gpu:'+gpu):
                        model.opt_gen.apply_gradients(zip(gen_grad,model.gen_model.trainable_weights))
                        model.opt_disc.apply_gradients(zip(disc_grad,model.disc_model.trainable_weights))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/data/scratch/apprisco/FREDSR_UG/final_vdsr/final_run.py"", line 192, in <module>
    run.train(100)
  File ""/data/scratch/apprisco/FREDSR_UG/final_vdsr/final_run.py"", line 134, in train
    gen_grad=tf.reduce_mean(tf.stack(gen_grads),axis=0)
                            ^^^^^^^^^^^^^^^^^^^
  File ""/data/scratch/apprisco/miniconda3/envs/TF/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/data/scratch/apprisco/miniconda3/envs/TF/lib/python3.12/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Pack_N_160_device_/job:localhost/replica:0/task:0/device:GPU:4}} Shapes of all inputs must match: values[0].shape = [5,5,3,16] != values[1].shape = [16] [Op:Pack] name: 0
```
",Apprisco,2024-07-25 00:27:21+00:00,['tilakrayal'],2024-08-10 01:54:18+00:00,2024-08-10 01:54:16+00:00,https://github.com/tensorflow/tensorflow/issues/72477,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2251974531, 'issue_id': 2428697364, 'author': 'tilakrayal', 'body': '@Apprisco,\r\nCould you please share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thank you!', 'created_at': datetime.datetime(2024, 7, 26, 4, 56, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266325986, 'issue_id': 2428697364, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 3, 1, 52, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935107, 'issue_id': 2428697364, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935137, 'issue_id': 2428697364, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72477"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72477"">No</a>', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 18, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-26 04:56:34 UTC): @Apprisco,
Could you please share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thank you!

github-actions[bot] on (2024-08-03 01:52:01 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-10 01:54:15 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-10 01:54:18 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72477"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72477"">No</a>

"
2427699422,issue,closed,completed,Unpublished CVE-2023-33976 affecting TF<2.13.0,"Good day all

Tensorflow 2.13.0 release notes mention this security fix:

> Fixes correct values rank in UpperBound and LowerBound [CVE-2023-33976](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-33976)

Checking the fix commit, it looks like a security issue with impact on Availability.
However, the CVE appears on MITRE website as RESERVED (assigned, but not published). Neither the security issue appears on [https://github.com/tensorflow/tensorflow/security](https://github.com/tensorflow/tensorflow/security), nor I found a TFSA entry for it. As a result, the issue is currently not reported by most security tools and goes unnoticed.
Is this intended?

Thank you for this awesome project!

",SCH227,2024-07-24 14:08:07+00:00,['sachinprasadhs'],2024-07-31 14:00:04+00:00,2024-07-31 14:00:04+00:00,https://github.com/tensorflow/tensorflow/issues/72441,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2259016099, 'issue_id': 2427699422, 'author': 'sachinprasadhs', 'body': '@SCH227 , Thanks for bringing this to our attention, CVE advisory has been published here https://github.com/tensorflow/tensorflow/security/advisories/GHSA-gjh7-xx4r-x345.\r\nOnce verified, feel free to close this issue.', 'created_at': datetime.datetime(2024, 7, 30, 19, 2, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260597569, 'issue_id': 2427699422, 'author': 'SCH227', 'body': 'Verified, thank you!', 'created_at': datetime.datetime(2024, 7, 31, 14, 0, 4, tzinfo=datetime.timezone.utc)}]","sachinprasadhs (Assginee) on (2024-07-30 19:02:32 UTC): @SCH227 , Thanks for bringing this to our attention, CVE advisory has been published here https://github.com/tensorflow/tensorflow/security/advisories/GHSA-gjh7-xx4r-x345.
Once verified, feel free to close this issue.

SCH227 (Issue Creator) on (2024-07-31 14:00:04 UTC): Verified, thank you!

"
2427237581,issue,closed,completed,Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android10(device)   win10 64 
 Android-studio  build.gradle:
        minSdkVersion 26
        targetSdkVersion 30
        buildToolsVersion ""30.0.2""
        ndkVersion ""21.0.6113669""
        implementation 'org.tensorflow:tensorflow-lite:2.16.1'
        implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'
 
       ndk {
                abiFilters ""armeabi-v7a"", ""arm64-v8a""
            }

### 2. Code

       Interpreter.Options options = new Interpreter.Options();
        options.setNumThreads(4);
        try {
            tfLite = new Interpreter(FileUtil.loadMappedFile(mContext,""xxxx.tflite"" ), options);
        } catch (IOException e) {
        }



### 5. (optional) Any other info / logs
07-24 17:40:30.069 26197 26197 E AndroidRuntime: FATAL EXCEPTION: main
07-24 17:40:30.069 26197 26197 E AndroidRuntime: Process:xxxxxxxxx, PID: 26197
07-24 17:40:30.069 26197 26197 E **AndroidRuntime: java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
07-24 17:40:30.069 26197 26197 E AndroidRuntime:   java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file ""/xx/priv-app/xx/xxx.apk""],nativeLibraryDirectories=[/hw_product/priv-app/eVision/lib/arm64, /xx/priv-app/xx/xxx.apk!/lib/arm64-v8a, /system/lib64, /hw_product/lib64, /system/product/lib64, /system/lib64, /hw_product/lib64, /system/product/lib64]]] _couldn't find ""libtensorflowlite_jni_gms_client.so""_**
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
xxxxxxx
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at java.lang.reflect.Method.invoke(Native Method)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1055)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: **_Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)_**
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)
",A-fliga,2024-07-24 10:45:48+00:00,['gaikwadrahul8'],2024-10-12 02:00:09+00:00,2024-10-12 02:00:07+00:00,https://github.com/tensorflow/tensorflow/issues/72434,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2249804644, 'issue_id': 2427237581, 'author': 'tilakrayal', 'body': '@A-fliga,\r\nCould you please provide the complete code to reproduce the issue which helps to analyse the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 8, 48, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249818097, 'issue_id': 2427237581, 'author': 'A-fliga', 'body': '@tilakrayal \r\n\r\n**_Load tflite file:_**\r\n        Interpreter.Options options = new Interpreter.Options();\r\n        options.setNumThreads(4);\r\n        try {\r\n            tfLite = new Interpreter(FileUtil.loadMappedFile(mContext, getModelFileName()), options);\r\n        } catch (IOException e) {\r\n        }\r\n\r\n**_and ""FileUtil.loadMappedFile()"" is provided by ""org.tensorflow:tensorflow-lite-support:0.0.0-nightly""_**\r\n\r\n![pic](https://github.com/user-attachments/assets/61bed260-e0e7-4fa4-bbb2-7cb3f4b8c9ff)', 'created_at': datetime.datetime(2024, 7, 25, 8, 55, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255872594, 'issue_id': 2427237581, 'author': 'sawantkumar', 'body': 'Hi @A-fliga ,\r\n\r\nJust wanted to confirm if you are loading the tensorflow lite jni library like below, before using it ?\r\n\r\n```\r\n static {\r\n        System.loadLibrary(""tensorflowlite_jni"");\r\n    }\r\n```', 'created_at': datetime.datetime(2024, 7, 29, 13, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264427631, 'issue_id': 2427237581, 'author': 'A-fliga', 'body': '@sawantkumar \r\nAndroidRuntime:xxxxxx **_couldn\'t find ""libtensorflowlite_jni_gms_client.so""_**\r\nAndroidRuntime: Caused by: java.lang.UnsatisfiedLinkError: **_No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing()_** (_tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing_)_\r\n\r\nThe key information of the error log is ""couldn\'t find ""libtensorflowlite_jni_gms_client.so"" and ""No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing()"", than i add the ""implementation \'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2\'"" in the build.gradle and it work fine...   \r\n\r\nI found that org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2contains libtensorflowlite_jni_gms_client..so, and the so file implements the ""nativeDoNothing"" method. Why is TF2.16 bundled with play-services SDK?', 'created_at': datetime.datetime(2024, 8, 2, 3, 9, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376781408, 'issue_id': 2427237581, 'author': 'gaikwadrahul8', 'body': 'Hi, @A-fliga \r\n\r\nI apologize for the delayed response, I believe you followed this [official documentation](https://ai.google.dev/edge/litert/android/development#minimum_android_sdk_versions_for_libraries\r\n) if possible could you please help us with android project as github repo to replicate same behavior from our end also to investigate this issue further?\r\n\r\nMeanwhile please refer this issue thread https://github.com/tensorflow/tensorflow/issues/61951 which may help you to solve this issue.\r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 26, 12, 15, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392647068, 'issue_id': 2427237581, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 4, 2, 1, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408301761, 'issue_id': 2427237581, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 12, 2, 0, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408301783, 'issue_id': 2427237581, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72434"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72434"">No</a>', 'created_at': datetime.datetime(2024, 10, 12, 2, 0, 9, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-25 08:48:43 UTC): @A-fliga,
Could you please provide the complete code to reproduce the issue which helps to analyse the issue in an effective way. Thank you!

A-fliga (Issue Creator) on (2024-07-25 08:55:12 UTC): @tilakrayal 

**_Load tflite file:_**
        Interpreter.Options options = new Interpreter.Options();
        options.setNumThreads(4);
        try {
            tfLite = new Interpreter(FileUtil.loadMappedFile(mContext, getModelFileName()), options);
        } catch (IOException e) {
        }

**_and ""FileUtil.loadMappedFile()"" is provided by ""org.tensorflow:tensorflow-lite-support:0.0.0-nightly""_**

![pic](https://github.com/user-attachments/assets/61bed260-e0e7-4fa4-bbb2-7cb3f4b8c9ff)

sawantkumar on (2024-07-29 13:00:19 UTC): Hi @A-fliga ,

Just wanted to confirm if you are loading the tensorflow lite jni library like below, before using it ?

```
 static {
        System.loadLibrary(""tensorflowlite_jni"");
    }
```

A-fliga (Issue Creator) on (2024-08-02 03:09:12 UTC): @sawantkumar 
AndroidRuntime:xxxxxx **_couldn't find ""libtensorflowlite_jni_gms_client.so""_**
AndroidRuntime: Caused by: java.lang.UnsatisfiedLinkError: **_No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing()_** (_tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing_)_

The key information of the error log is ""couldn't find ""libtensorflowlite_jni_gms_client.so"" and ""No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing()"", than i add the ""implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2'"" in the build.gradle and it work fine...   

I found that org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2contains libtensorflowlite_jni_gms_client..so, and the so file implements the ""nativeDoNothing"" method. Why is TF2.16 bundled with play-services SDK?

gaikwadrahul8 (Assginee) on (2024-09-26 12:15:52 UTC): Hi, @A-fliga 

I apologize for the delayed response, I believe you followed this [official documentation](https://ai.google.dev/edge/litert/android/development#minimum_android_sdk_versions_for_libraries
) if possible could you please help us with android project as github repo to replicate same behavior from our end also to investigate this issue further?

Meanwhile please refer this issue thread https://github.com/tensorflow/tensorflow/issues/61951 which may help you to solve this issue.

Thank you for your cooperation and patience.

github-actions[bot] on (2024-10-04 02:01:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-12 02:00:07 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-12 02:00:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72434"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72434"">No</a>

"
2427225646,issue,closed,completed,Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android10(device)   win10 64 
 Android-studio  build.gradle:
        minSdkVersion 26
        targetSdkVersion 30
        buildToolsVersion ""30.0.2""
        ndkVersion ""21.0.6113669""
        implementation 'org.tensorflow:tensorflow-lite:2.16.1'
        implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'
 
       ndk {
                abiFilters ""armeabi-v7a"", ""arm64-v8a""
            }

### 2. Code

       Interpreter.Options options = new Interpreter.Options();
        options.setNumThreads(4);
        try {
            tfLite = new Interpreter(FileUtil.loadMappedFile(mContext,""xxxx.tflite"" ), options);
        } catch (IOException e) {
        }



### 5. (optional) Any other info / logs
07-24 17:40:30.069 26197 26197 E AndroidRuntime: FATAL EXCEPTION: main
07-24 17:40:30.069 26197 26197 E AndroidRuntime: Process:xxxxxxxxx, PID: 26197
07-24 17:40:30.069 26197 26197 E **AndroidRuntime: java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
07-24 17:40:30.069 26197 26197 E AndroidRuntime:   java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file ""/xx/priv-app/xx/xxx.apk""],nativeLibraryDirectories=[/hw_product/priv-app/eVision/lib/arm64, /xx/priv-app/xx/xxx.apk!/lib/arm64-v8a, /system/lib64, /hw_product/lib64, /system/product/lib64, /system/lib64, /hw_product/lib64, /system/product/lib64]]] _couldn't find ""libtensorflowlite_jni_gms_client.so""_**
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
xxxxxxx
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at java.lang.reflect.Method.invoke(Native Method)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1055)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: **_Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)_**
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)
07-24 17:40:30.069 26197 26197 E AndroidRuntime: 	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)
",A-fliga,2024-07-24 10:41:08+00:00,['Venkat6871'],2024-07-25 05:43:44+00:00,2024-07-25 05:43:44+00:00,https://github.com/tensorflow/tensorflow/issues/72433,"[('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2247789444, 'issue_id': 2427225646, 'author': 'x0rw', 'body': 'Duplicated, you submitted this issue twice', 'created_at': datetime.datetime(2024, 7, 24, 12, 28, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249385858, 'issue_id': 2427225646, 'author': 'Venkat6871', 'body': 'Hi **@A-fliga** ,\r\n- Looks like this is duplicate of issue #[72434](https://github.com/tensorflow/tensorflow/issues/72434).Can you please close this issue, since it is already being tracked there?\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 25, 4, 47, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249506718, 'issue_id': 2427225646, 'author': 'A-fliga', 'body': 'duplicate of issue #https://github.com/tensorflow/tensorflow/issues/72434', 'created_at': datetime.datetime(2024, 7, 25, 5, 43, 17, tzinfo=datetime.timezone.utc)}]","x0rw on (2024-07-24 12:28:45 UTC): Duplicated, you submitted this issue twice

Venkat6871 (Assginee) on (2024-07-25 04:47:41 UTC): Hi **@A-fliga** ,
- Looks like this is duplicate of issue #[72434](https://github.com/tensorflow/tensorflow/issues/72434).Can you please close this issue, since it is already being tracked there?

Thank you!

A-fliga (Issue Creator) on (2024-07-25 05:43:17 UTC): duplicate of issue #https://github.com/tensorflow/tensorflow/issues/72434

"
2426897804,issue,open,,[Question] - Extend some GPU op,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A30

### Current behavior?

I'm running tensorflow_model_server with the sample saved_model_half_plus_two_gpu model. One of the layers there is Mul

""Mul"" op: ""Mul"" input: ""a/read"" input: ""_arg_x_0_0/_7"" device: ""/job:localhost/replica:0/task:0/device:GPU:0"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""_XlaHasReferenceVars"" value { b: true } }

I would like to add some custom code before/after the code runs on the GPU (I have a docker with TensorFlow serving, it downloads the TensorFlow code and comiles it via bazel). I am able to add a print in the OpKernel::OpKernel code, but I would like to do so in the relevant GPU operator compute's function. That I am unable to find/do.

Tried to print/log to file in either of the following places but it did **NOT** work:
- tensorflow/core/util/gpu_kernel_helper.h               in GpuLaunchKernel function
- tensorflow/core/kernels/cwise_ops_common.h     in BinaryOp::Compute

Any idea what am I missing? Obviously my code adds catch as they compile and the code in OpKernel::OpKernel  does work.


### Standalone code to reproduce the issue

```shell
Not relevant...
```


### Relevant log output

_No response_",eyalhir74,2024-07-24 08:18:00+00:00,['tilakrayal'],2024-07-30 11:22:00+00:00,,https://github.com/tensorflow/tensorflow/issues/72422,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:gpu', 'GPU related issues'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2250299879, 'issue_id': 2426897804, 'author': 'tilakrayal', 'body': '@eyalhir74,\r\nCould you please provide more info/context about the above statement which helps to analyse the issue. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 13, 16, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254336068, 'issue_id': 2426897804, 'author': 'eyalhir74', 'body': ""@tilakrayal \r\nYes, I would like to try and change a few things in the way TF uses the GPU. I was able to put some custom [NVTX](https://github.com/jrhemstad/nvtx_wrappers/blob/master/nvtx3.hpp) code inside the BaseGPUDevice::Compute code in tensorflow/core/common_runtime/gpu/gpu_device.cc, and it indeed works.\\\r\nI'm still looking at why the code in tensorflow/core/util/gpu_kernel_helper.h in GpuLaunchKernel function\r\ndid not catch and I'm unable to change the way a CUDA GPU kernel is being launched. Would that code be the right place\r\nto alter CUDA kernel invocations or there is somewhere else?\r\n\r\nthanks\r\nEyal"", 'created_at': datetime.datetime(2024, 7, 28, 4, 9, 32, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-25 13:16:02 UTC): @eyalhir74,
Could you please provide more info/context about the above statement which helps to analyse the issue. Thank you!

eyalhir74 (Issue Creator) on (2024-07-28 04:09:32 UTC): @tilakrayal 
Yes, I would like to try and change a few things in the way TF uses the GPU. I was able to put some custom [NVTX](https://github.com/jrhemstad/nvtx_wrappers/blob/master/nvtx3.hpp) code inside the BaseGPUDevice::Compute code in tensorflow/core/common_runtime/gpu/gpu_device.cc, and it indeed works.\
I'm still looking at why the code in tensorflow/core/util/gpu_kernel_helper.h in GpuLaunchKernel function
did not catch and I'm unable to change the way a CUDA GPU kernel is being launched. Would that code be the right place
to alter CUDA kernel invocations or there is somewhere else?

thanks
Eyal

"
2426787700,issue,closed,completed,TensorFlow 2.16 Training an image Autoencoder output pure black image while 2.10 is normal,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

Windows10

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Not using 

### GPU model and memory

No GPU

### Current behavior?

The task is for single-channel image reconstruction using a convolutional autoencoder. Previously I was using TensorFlow 2.10, and it worked fine, as the epoch proceeded, the reconstructed image output was gradually formed. However, I recently updated my TensorFlow to 2.16.2, for exactly the same code and the same dataset, and the output from the second epoch becomes pure black. I don't understand why.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf 

class Autoencoder(tf.keras.Model):
    def __init__(self, input_shape):
        super(Autoencoder, self).__init__()
        initializer = tf.random_normal_initializer(0., 0.02)
        self.encoder = tf.keras.Sequential([
            tf.keras.layers.Input(shape=input_shape),
            tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.Conv2D(256, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.Conv2D(1024, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU()
        ])
        self.decoder = tf.keras.Sequential([
            tf.keras.layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.Conv2D(input_shape[-1], kernel_size=4, activation='sigmoid', padding='same')
        ])

    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


shape = train_X.shape[1:]
autoencoder = Autoencoder(shape)
sample_data = np.random.random((1, *shape))  # Batch size of 1
autoencoder(sample_data)
autoencoder.summary()

print(f""model size: {autoencoder.count_params() * 4 / (1024**2)} MB"") 

# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07)
autoencoder.compile(optimizer=adam_optimizer, 
                    loss=tf.keras.losses.MeanSquaredError())
history = autoencoder.fit(train_X, train_Y,
                        epochs=80,
                        batch_size=4,
                        shuffle=True,
                        validation_data=(val_X, val_Y),
                        callbacks=[ImageReconstructionCallback(val_X, val_Y)]
                        )
```


### Relevant log output

_No response_",Andrew-XQY,2024-07-24 07:18:16+00:00,['Venkat6871'],2024-07-24 08:11:06+00:00,2024-07-24 08:10:13+00:00,https://github.com/tensorflow/tensorflow/issues/72420,"[('type:support', 'Support issues')]","[{'comment_id': 2247177570, 'issue_id': 2426787700, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72420"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72420"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 8, 10, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247179215, 'issue_id': 2426787700, 'author': 'Andrew-XQY', 'body': 'solved by changing sigmoid to tanh', 'created_at': datetime.datetime(2024, 7, 24, 8, 11, 5, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-07-24 08:10:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72420"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72420"">No</a>

Andrew-XQY (Issue Creator) on (2024-07-24 08:11:05 UTC): solved by changing sigmoid to tanh

"
2426487920,issue,closed,completed,tensorflow.python.framework.errors_impl.AbortedError: Exception encountered when calling MaxPooling1D.call(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17 and 2.18.0.dev20240717 

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when using the `tf.keras.layers.MaxPool1D` API after setting the input data format to 'channels_first'. The `tf.keras.layers.MaxPool1D` throws an `tensorflow.python.framework.errors_impl.AbortedError` exception. However, the average pooling operator executes successfully without any exceptions. The code is as follows:

```python
import tensorflow as tf
data_format = ""channels_first""
tf.compat.v1.keras.backend.set_image_data_format(data_format=data_format)

pool_size = 2
strides = 2
padding = ""valid""
pool_func = tf.keras.layers.MaxPool1D(pool_size=pool_size, strides=strides, padding=padding) # tensorflow.python.framework.errors_impl.AbortedError
# pool_func = tf.keras.layers.AvgPool1D(pool_size=pool_size, strides=strides, padding=padding) # success

input_tensor = tf.random.uniform([1, 5, 1], dtype=tf.float32)
input_tensor = tf.identity(input_tensor)

output_tensor = pool_func(input_tensor)

print(output_tensor.shape)

```

The error message was as follows:

```shell
# Output for tf.keras.layers.MaxPool1D + channel_first: exception
Traceback (most recent call last):
  File ""/data/test.py"", line 15, in <module>
    output_tensor = pool_func(input_tensor)
  File ""/data/anacondas/envs/code/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/data/anacondas/envs/code/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.AbortedError: Exception encountered when calling MaxPooling1D.call().

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Compute received an exception:Status: 2, message: could not create a descriptor for a pooling forward propagation primitive, in file tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:211 [Op:MaxPool] name: 

Arguments received by MaxPooling1D.call():
   inputs=tf.Tensor(shape=(1, 5, 1), dtype=float32)

# output for tf.keras.layers.MaxPool1D + channel_last: success

# output for tf.keras.layers.AvgPool1D + channel_first/channel_last: success
```

The above code would throw an exception on `tf-2.17` and `tf-nightly 2.18.0.dev20240717` (nightly-build).



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
data_format = ""channels_first""
tf.compat.v1.keras.backend.set_image_data_format(data_format=data_format)

pool_size = 2
strides = 2
padding = ""valid""
pool_func = tf.keras.layers.MaxPool1D(pool_size=pool_size, strides=strides, padding=padding) # tensorflow.python.framework.errors_impl.AbortedError
# pool_func = tf.keras.layers.AvgPool1D(pool_size=pool_size, strides=strides, padding=padding) # success

input_tensor = tf.random.uniform([1, 5, 1], dtype=tf.float32)
input_tensor = tf.identity(input_tensor)

output_tensor = pool_func(input_tensor)

print(output_tensor.shape)
```


### Relevant log output

```shell
# Output for tf.keras.layers.MaxPool1D + channel_first: exception
Traceback (most recent call last):
  File ""/data/test.py"", line 15, in <module>
    output_tensor = pool_func(input_tensor)
  File ""/data/anacondas/envs/code/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/data/anacondas/envs/code/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.AbortedError: Exception encountered when calling MaxPooling1D.call().

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Compute received an exception:Status: 2, message: could not create a descriptor for a pooling forward propagation primitive, in file tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:211 [Op:MaxPool] name: 

Arguments received by MaxPooling1D.call():
   inputs=tf.Tensor(shape=(1, 5, 1), dtype=float32)

# output for tf.keras.layers.MaxPool1D + channel_last: success

# output for tf.keras.layers.AvgPool1D + channel_first/channel_last: success
```
",Jacob-yen,2024-07-24 03:09:12+00:00,['tilakrayal'],2024-07-25 08:27:41+00:00,2024-07-25 08:27:38+00:00,https://github.com/tensorflow/tensorflow/issues/72411,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2246778606, 'issue_id': 2426487920, 'author': 'Jacob-yen', 'body': 'The same issue occurs with MaxPool2D and MaxPool3D.', 'created_at': datetime.datetime(2024, 7, 24, 3, 11, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249606742, 'issue_id': 2426487920, 'author': 'tilakrayal', 'body': '@Jacob-yen,\r\nI tried to execute the code on tensorflow tf-nightly where it was executed without any issues or errors. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/add3aa39a5813b2ba58339ced3b01c62/untitled2022.ipynb). The tf-nightly(2.18.0-dev20240722) contains keras3.0 where we should import keras and keras.layers.MaxPool1D   and try to execute the code. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 7, 5, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249758378, 'issue_id': 2426487920, 'author': 'Jacob-yen', 'body': ""Thank you for your prompt reply. I tried the notebook, and it works well.\r\n\r\nHowever, the above code still crashes on my Linux server with tf-nightly (2.18.0-dev20240722). To investigate the issue, I tested the code on three different servers, installing the same dependency packages with Python 3.10. \r\n```\r\nPackage                      Version\r\n---------------------------- -------------------\r\nabsl-py                      2.1.0\r\nastunparse                   1.6.3\r\ncertifi                      2024.7.4\r\ncharset-normalizer           3.3.2\r\nflatbuffers                  24.3.25\r\ngast                         0.6.0\r\ngoogle-pasta                 0.2.0\r\ngrpcio                       1.65.1\r\nh5py                         3.11.0\r\nidna                         3.7\r\nkeras-nightly                3.4.1.dev2024072503\r\nlibclang                     18.1.1\r\nMarkdown                     3.6\r\nmarkdown-it-py               3.0.0\r\nMarkupSafe                   2.1.5\r\nmdurl                        0.1.2\r\nml-dtypes                    0.4.0\r\nnamex                        0.0.8\r\nnumpy                        1.26.4\r\nopt-einsum                   3.3.0\r\noptree                       0.12.1\r\npackaging                    24.1\r\npip                          24.0\r\nprotobuf                     4.25.4\r\nPygments                     2.18.0\r\nrequests                     2.32.3\r\nrich                         13.7.1\r\nsetuptools                   69.5.1\r\nsix                          1.16.0\r\ntb-nightly                   2.18.0a20240724\r\ntensorboard-data-server      0.7.2\r\ntensorflow-io-gcs-filesystem 0.37.1\r\ntermcolor                    2.4.0\r\ntf_nightly                   2.18.0.dev20240722\r\ntyping_extensions            4.12.2\r\nurllib3                      2.2.2\r\nWerkzeug                     3.0.3\r\nwheel                        0.43.0\r\nwrapt                        1.16.0\r\n\r\n```\r\n\r\nI found that two servers with `Python 3.10.14 and GCC 11.2.0` crashed, while the other server with `Python 3.10.4 and GCC 7.5.0` ran the code without any problems.\r\n\r\nI'm not sure what caused the crash, but it may be a compatibility issue between the TensorFlow version and the GCC version.\r\n\r\nI will temporarily close the issue until further reasons are found."", 'created_at': datetime.datetime(2024, 7, 25, 8, 27, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249758445, 'issue_id': 2426487920, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72411"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72411"">No</a>', 'created_at': datetime.datetime(2024, 7, 25, 8, 27, 40, tzinfo=datetime.timezone.utc)}]","Jacob-yen (Issue Creator) on (2024-07-24 03:11:35 UTC): The same issue occurs with MaxPool2D and MaxPool3D.

tilakrayal (Assginee) on (2024-07-25 07:05:05 UTC): @Jacob-yen,
I tried to execute the code on tensorflow tf-nightly where it was executed without any issues or errors. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/add3aa39a5813b2ba58339ced3b01c62/untitled2022.ipynb). The tf-nightly(2.18.0-dev20240722) contains keras3.0 where we should import keras and keras.layers.MaxPool1D   and try to execute the code. Thank you!

Jacob-yen (Issue Creator) on (2024-07-25 08:27:38 UTC): Thank you for your prompt reply. I tried the notebook, and it works well.

However, the above code still crashes on my Linux server with tf-nightly (2.18.0-dev20240722). To investigate the issue, I tested the code on three different servers, installing the same dependency packages with Python 3.10. 
```
Package                      Version
---------------------------- -------------------
absl-py                      2.1.0
astunparse                   1.6.3
certifi                      2024.7.4
charset-normalizer           3.3.2
flatbuffers                  24.3.25
gast                         0.6.0
google-pasta                 0.2.0
grpcio                       1.65.1
h5py                         3.11.0
idna                         3.7
keras-nightly                3.4.1.dev2024072503
libclang                     18.1.1
Markdown                     3.6
markdown-it-py               3.0.0
MarkupSafe                   2.1.5
mdurl                        0.1.2
ml-dtypes                    0.4.0
namex                        0.0.8
numpy                        1.26.4
opt-einsum                   3.3.0
optree                       0.12.1
packaging                    24.1
pip                          24.0
protobuf                     4.25.4
Pygments                     2.18.0
requests                     2.32.3
rich                         13.7.1
setuptools                   69.5.1
six                          1.16.0
tb-nightly                   2.18.0a20240724
tensorboard-data-server      0.7.2
tensorflow-io-gcs-filesystem 0.37.1
termcolor                    2.4.0
tf_nightly                   2.18.0.dev20240722
typing_extensions            4.12.2
urllib3                      2.2.2
Werkzeug                     3.0.3
wheel                        0.43.0
wrapt                        1.16.0

```

I found that two servers with `Python 3.10.14 and GCC 11.2.0` crashed, while the other server with `Python 3.10.4 and GCC 7.5.0` ran the code without any problems.

I'm not sure what caused the crash, but it may be a compatibility issue between the TensorFlow version and the GCC version.

I will temporarily close the issue until further reasons are found.

google-ml-butler[bot] on (2024-07-25 08:27:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72411"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72411"">No</a>

"
2426045267,issue,closed,completed,error in perks,error in delicinalm,mseDPYU4,2024-07-23 20:14:12+00:00,['tilakrayal'],2024-07-23 20:14:28+00:00,2024-07-23 20:14:28+00:00,https://github.com/tensorflow/tensorflow/issues/72393,"[('TFLiteConverter', 'For issues related to TFLite converter')]",[],
2425940597,issue,closed,completed,`tf.distribute` and `keras` 3 incompatibility: ValueError: Invalid reduction dimension 0 for input with 0 dimensions.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16, 2.17, 2.18 (nightly)

### Custom code

Yes

### OS platform and distribution

MacOS + Linux

### Mobile device

_No response_

### Python version

py311, py312

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

* Running a simple quickstart DDP example with tensorflow on a single node with multiple workers (via multiprocessing or ray).
* Runs into an error during a collective sum reduction call.
* The `Dropout` layer was also causing some problems but I can't reproduce that at the moment. Comment out the dropout layer if you run into this.

## Repro setup, with workaround

**The workaround is to downgrade keras 3 to keras 2.**

```
conda create -c conda-forge python=3.12 -n debug_tf_py312
conda activate debug_tf_py312

pip install tensorflow tf-keras~=2.16 ""ray[train]""

# not working (keras 3)
TF_USE_LEGACY_KERAS=0 python repro.py mp

# working (legacy keras 2)
TF_USE_LEGACY_KERAS=1 python repro.py mp
```

See repro script below.

### Standalone code to reproduce the issue

```python
import tensorflow as tf
import json
import os


if bool(int(os.environ.get(""TF_USE_LEGACY_KERAS"", ""0""))):
    import tf_keras as keras
else:
    import tensorflow.keras as keras


print(""TensorFlow version:"", tf.__version__)


mnist = keras.datasets.mnist


def get_address_and_port():
    return ""127.0.0.1"", find_free_port()


def find_free_port():
    import socket
    from contextlib import closing

    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("""", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def _setup_tensorflow_environment(worker_addresses, index: int):
    """"""Set up distributed Tensorflow training information.
    This function should be called on each worker.
    Args:
        worker_addresses: Addresses of all the workers.
        index: Index (i.e. world rank) of the current worker.
    """"""
    config = {
        ""cluster"": {""worker"": worker_addresses},
        ""task"": {""type"": ""worker"", ""index"": index},
    }
    os.environ[""TF_CONFIG""] = json.dumps(config)


def train_fn(worker_addresses=[""127.0.0.1:12345""], index=0):
    _setup_tensorflow_environment(worker_addresses, index)

    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    with strategy.scope():
        model = keras.models.Sequential(
            [
                keras.layers.Flatten(input_shape=(28, 28)),
                keras.layers.Dense(128, activation=""relu""),
                # NOTE: This errors for some reason when running with either MP or ray
                keras.layers.Dropout(0.2),
                keras.layers.Dense(10),
            ]
        )
        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        model.compile(optimizer=""adam"", loss=loss_fn, metrics=[""accuracy""])

    predictions = model(x_train[:1]).numpy()
    predictions

    tf.nn.softmax(predictions).numpy()

    loss_fn(y_train[:1], predictions).numpy()

    model.fit(x_train, y_train, epochs=5)

    model.evaluate(x_test, y_test, verbose=2)

    probability_model = keras.Sequential([model, keras.layers.Softmax()])

    probability_model(x_test[:5])


def get_url():
    address, port = get_address_and_port()
    return f""{address}:{port}""


def run_with_ray():
    import ray

    train_fn_task = ray.remote(train_fn)

    num_workers = 2
    worker_addresses = [get_url() for _ in range(num_workers)]

    ray.get([train_fn_task.remote(worker_addresses, i) for i in range(num_workers)])


def run_with_mp():
    from multiprocessing import Process

    num_workers = 2
    worker_addresses = [get_url() for _ in range(num_workers)]

    p1 = Process(target=train_fn, args=(worker_addresses, 0))
    p2 = Process(target=train_fn, args=(worker_addresses, 1))

    p1.start()
    p2.start()

    p1.join()
    p2.join()


if __name__ == ""__main__"":
    import sys

    runner = sys.argv[1]
    if runner == ""vanilla"":
        train_fn()
    elif runner == ""ray"":
        run_with_ray()
    elif runner == ""mp"":
        run_with_mp()
```


### Relevant log output

```shell
TF_USE_LEGACY_KERAS=0 python repro.py mp

  Process Process-2:
Traceback (most recent call last):
  File ""/Users/justin/miniforge3/envs/ray_dev_py312/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/Users/justin/miniforge3/envs/ray_dev_py312/lib/python3.12/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/justin/Downloads/tf_example.py"", line 56, in train_fn
    model.fit(x_train, y_train, epochs=5)
  File ""/Users/justin/miniforge3/envs/ray_dev_py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/justin/miniforge3/envs/ray_dev_py312/lib/python3.12/site-packages/optree/ops.py"", line 747, in tree_map
    return treespec.unflatten(map(func, *flat_args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Invalid reduction dimension 0 for input with 0 dimensions. for '{{node Sum_2}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](StatefulPartitionedCall, Sum_2/reduction_indices)' with input shapes: [], [] and with computed input tensors: input[1] = <0>.
```
",justinvyu,2024-07-23 19:17:36+00:00,['tilakrayal'],2024-08-09 01:55:04+00:00,2024-08-09 01:55:01+00:00,https://github.com/tensorflow/tensorflow/issues/72388,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2250342149, 'issue_id': 2425940597, 'author': 'tilakrayal', 'body': '@justinvyu,\r\nI checked with the **TF_USE_LEGACY_KERAS=1**(Keras2.0) and observed it was working as expected. As this is an issue with keras3.0, could you please try to raise the issue in the keras-team/keras [repo](https://github.com/keras-team/keras/issues/) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 13, 36, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345575, 'issue_id': 2425940597, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005005, 'issue_id': 2425940597, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005047, 'issue_id': 2425940597, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72388"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72388"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 3, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-25 13:36:17 UTC): @justinvyu,
I checked with the **TF_USE_LEGACY_KERAS=1**(Keras2.0) and observed it was working as expected. As this is an issue with keras3.0, could you please try to raise the issue in the keras-team/keras [repo](https://github.com/keras-team/keras/issues/) for the quick resolution. Thank you!

github-actions[bot] on (2024-08-02 01:53:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-09 01:55:01 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-09 01:55:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72388"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72388"">No</a>

"
2425511585,issue,open,,Inspect `tf.data.AUTOTUNE `values during runtime,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.16.1-0-g5bc9d26649c

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When loading data with a `tf.data.Dataset`, it is possible to apply transforms like `.prefetch(buffer_size)`, where the `buffer_size` can be optimized during runtime if set to `tf.data.AUTOTUNE`. When printing `tf.data.AUTOTUNE`, it returns `-1` (as described in the documentation on this feature). However, I want to know what value the optimized `buffer_size` takes during my training loop. This does not seem to be described in the current documentation.

### Standalone code to reproduce the issue

```shell
Any training loop using a `tf.data.Dataset` dataloader & optimized buffer sizes using `tf.data.AUTOTUNE`.
```


### Relevant log output

_No response_",noahewolfe,2024-07-23 15:33:08+00:00,['aaudiber'],2024-10-18 06:28:16+00:00,,https://github.com/tensorflow/tensorflow/issues/72369,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:data', 'tf.data related issues')]","[{'comment_id': 2249814169, 'issue_id': 2425511585, 'author': 'Venkat6871', 'body': 'Hi **@noahewolfe** ,\r\n- In order to expedite the trouble-shooting process, Could you elaborate more about issue? and please provide a code snippet to reproduce the issue reported here.\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 25, 8, 53, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259561717, 'issue_id': 2425511585, 'author': 'noahewolfe', 'body': ""Hi @Venkat6871 , I don't have a specific code snippet or reproducible issue. I am requesting a feature; is there a better location to do so?"", 'created_at': datetime.datetime(2024, 7, 31, 3, 19, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387274742, 'issue_id': 2425511585, 'author': 'noahewolfe', 'body': ""Hi @Venkat6871, I wanted to follow-up as I haven't yet received a response-- is this possible as a feature, and if so, where is best to raise that request?"", 'created_at': datetime.datetime(2024, 10, 1, 23, 46, 25, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-07-25 08:53:21 UTC): Hi **@noahewolfe** ,
- In order to expedite the trouble-shooting process, Could you elaborate more about issue? and please provide a code snippet to reproduce the issue reported here.
Thank you!

noahewolfe (Issue Creator) on (2024-07-31 03:19:38 UTC): Hi @Venkat6871 , I don't have a specific code snippet or reproducible issue. I am requesting a feature; is there a better location to do so?

noahewolfe (Issue Creator) on (2024-10-01 23:46:25 UTC): Hi @Venkat6871, I wanted to follow-up as I haven't yet received a response-- is this possible as a feature, and if so, where is best to raise that request?

"
2425239732,issue,closed,completed,Custom metrics results returned by 'History' callbacks doesnt work properly,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have defined two custom metrics to track SSIM and PSNR in my denoising autoencoder during the training. While the logs printed during the training seems to work properly, it returns weird results at the end by 'History' callback. All the values for training and validation psnr and ssim equal to the last printed value for validtion psnr and ssim, respectively. Like the older versions, I have defined internal variables by using add_variable method. I have also tried with add_weight. Both showed same behaviour. 

By the way, history callback used to return metrics as numpy arrays, but for the custom metrics that I defined, it returns KerasVariable instances.  Is there any fixes for this? This is a bit annoying that some metrics like loss are returned as numpy arrays while the custom metrics are KerasVariable instances. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
import numpy as np 

class SSIMMetric(tf.keras.metrics.Metric):
    """"""
    Custom SSIM metric for image denoising.

    This class calculates the average Structural Similarity Index Measure (SSIM)
    across all batches during training or evaluation.

    Args:
      name (str, optional): A name for the metric. Defaults to 'ssim'.
      max_val (float, optional): The dynamic range of the images (usually the maximum pixel value).
          Defaults to 255.0 for images in the 0-255 range.
      **kwargs (optional): Additional keyword arguments for the base Metric class.
    """"""

    def __init__(self, name='ssim', max_val=255.0, **kwargs):
        super(SSIMMetric, self).__init__(name=name, **kwargs)
        self.max_val = max_val
        self.ssim = self.add_variable(shape=(), name='ssim', initializer='zeros')
        self.counter = self.add_variable(shape=(), name='counter', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        """"""
        Updates the internal state of the metric with a batch of data.

        Args:
            y_true (tf.Tensor): The ground truth image tensor.
            y_pred (tf.Tensor): The predicted image tensor.
            sample_weight (tf.Tensor, optional): Sample weights (not used in this implementation).
        """"""
        ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=self.max_val))
        self.ssim.assign((ssim + self.ssim * self.counter) / (self.counter + 1.))
        self.counter.assign_add(1.)
        
    def result(self):
        """"""
        Calculates and returns the current average SSIM value.

        Returns:
            tf.Tensor: The average SSIM across all batches.
        """"""
        return self.ssim 

    def reset_states(self):
        """"""
        Resets the internal state of the metric to zero.
        """"""
        self.ssim.assign(0.0)
        self.counter.assign(0.0)


class PSNRMetric(tf.keras.metrics.Metric):
    """"""
    Custom PSNR metric for image denoising.

    This class calculates the average Peak Signal-to-Noise Ratio (PSNR)
    across all batches during training or evaluation.

    Args:
      name (str, optional): A name for the metric. Defaults to 'psnr'.
      max_val (float, optional): The dynamic range of the images (usually the maximum pixel value).
          Defaults to 255.0 for images in the 0-255 range.
      **kwargs (optional): Additional keyword arguments for the base Metric class.
    """"""
    def __init__(self, name='psnr', max_val=255.0, **kwargs):
        super(PSNRMetric, self).__init__(name=name, **kwargs)
        self.max_val = max_val
        self.psnr = self.add_variable(shape=(), name='psnr', initializer='zeros')
        self.counter = self.add_variable(shape=(), name='counter', initializer='zeros')
        
    def update_state(self, y_true, y_pred, sample_weight=None):
        """"""
        Updates the internal state of the metric with a batch of data.

        Args:
            y_true (tf.Tensor): The ground truth image tensor.
            y_pred (tf.Tensor): The predicted image tensor.
            sample_weight (tf.Tensor, optional): Sample weights (not used in this implementation).
        """"""
        psnr = tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=self.max_val))
        self.psnr.assign((psnr + self.counter * self.psnr) / (self.counter + 1.))
        self.counter.assign_add(1.)

    def result(self):
        """"""
        Calculates and returns the current average PSNR value.

        Returns:
            tf.Tensor: The average PSNR across all batches.
        """"""
        return self.psnr 

    def reset_states(self):
        """"""
        Resets the internal state of the metric to zero.
        """"""
        self.psnr.assign(0.0)
        self.counter.assign(0.0)
        
        

(y_train, _), (y_test, _)   = tf.keras.datasets.cifar10.load_data()

y_train, y_test = y_train/255.0, y_test / 255.0 

x_train = y_train + 0.2 * np.random.random((y_train.shape))
x_test  = y_test  + 0.2 * np.random.random((y_test.shape))

input_layer = tf.keras.layers.Input((32, 32, 3))
x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), 
                           activation=tf.nn.leaky_relu, padding='same')(input_layer)
x = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), 
                           activation='sigmoid', padding='same')(x)

model = tf.keras.models.Model(input_layer, x)

model.compile(loss='mse', metrics=[PSNRMetric(max_val=1.), SSIMMetric(max_val=1.)], optimizer='Adam')

results = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

val_psnr = np.array([np.array(metric_val) for metric_val in results.history['val_psnr']])
psnr     = np.array([np.array(metric_val) for metric_val in results.history['psnr']])
print(f'training psnr:\n{psnr}\nval_psnr:\n{val_psnr}')
```


### Relevant log output

```shell
The logs:

Epoch 1/5
1563/1563  8s 4ms/step - loss: 0.0153 - psnr: 22.2309 - ssim: 0.7574 - val_loss: 0.0017 - val_psnr: 27.8734 - val_ssim: 0.9221
Epoch 2/5
1563/1563  4s 3ms/step - loss: 0.0017 - psnr: 28.0592 - ssim: 0.9232 - val_loss: 0.0016 - val_psnr: 28.2505 - val_ssim: 0.9243
Epoch 3/5
1563/1563  4s 3ms/step - loss: 0.0015 - psnr: 28.3847 - ssim: 0.9248 - val_loss: 0.0015 - val_psnr: 28.4067 - val_ssim: 0.9253
Epoch 4/5
1563/1563  4s 2ms/step - loss: 0.0015 - psnr: 28.4954 - ssim: 0.9255 - val_loss: 0.0015 - val_psnr: 28.5810 - val_ssim: 0.9255
Epoch 5/5
1563/1563  4s 2ms/step - loss: 0.0015 - psnr: 28.5493 - ssim: 0.9256 - val_loss: 0.0015 - val_psnr: 28.5256 - val_ssim: 0.9256
training psnr:
[28.525625 28.525625 28.525625 28.525625 28.525625]
val_psnr:
[28.525625 28.525625 28.525625 28.525625 28.525625]
```
",JVD9kh96,2024-07-23 13:34:55+00:00,['tilakrayal'],2024-09-18 01:59:09+00:00,2024-09-18 01:59:00+00:00,https://github.com/tensorflow/tensorflow/issues/72366,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2245401900, 'issue_id': 2425239732, 'author': 'JVD9kh96', 'body': 'instead of using tf.keras.metrics.Metric I also tried tf.keras.Metric for defining the metrics, the results were the same', 'created_at': datetime.datetime(2024, 7, 23, 14, 24, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2250326830, 'issue_id': 2425239732, 'author': 'tilakrayal', 'body': '@JVD9kh96,\r\nThank you for reporting the issue. As this issue is more related to keras, could you please raise the issue in keras-team/keras for quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 13, 29, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345626, 'issue_id': 2425239732, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276839600, 'issue_id': 2425239732, 'author': 'JVD9kh96', 'body': ""For those who are looking for a quick fix, you can use legacy keras:\r\n`pip install tf_keras`\r\n\r\n\r\n```shell\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os \r\nos.environ['TF_USE_LEGACY_KERAS'] = 'True'\r\n#... rest of the code \r\n\r\n#...output:\r\ntraining psnr:\r\n[25.50802231 27.92281532 28.17706299 28.29514313 28.37692451]\r\nval_psnr:\r\n[27.65083694 28.10541534 28.27854538 28.36878586 28.24624634]\r\n```"", 'created_at': datetime.datetime(2024, 8, 8, 23, 3, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324427141, 'issue_id': 2425239732, 'author': 'tilakrayal', 'body': '@JVD9kh96,\r\nGlad the issue was resolved with the tf-keras. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/30aa38cea671499a4eae496c36a391e9/untitled2091.ipynb) of it [here](https://colab.research.google.com/gist/tilakrayal/03384cf3ffd968614056e06829009499/untitled2091.ipynb). And tensorflow v2.17 contains Keras3.0, and this is more related to keras please try to raise the issue in keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 9, 2, 10, 50, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460390, 'issue_id': 2425239732, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 59, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336791, 'issue_id': 2425239732, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336974, 'issue_id': 2425239732, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72366"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72366"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 59, 8, tzinfo=datetime.timezone.utc)}]","JVD9kh96 (Issue Creator) on (2024-07-23 14:24:57 UTC): instead of using tf.keras.metrics.Metric I also tried tf.keras.Metric for defining the metrics, the results were the same

tilakrayal (Assginee) on (2024-07-25 13:29:15 UTC): @JVD9kh96,
Thank you for reporting the issue. As this issue is more related to keras, could you please raise the issue in keras-team/keras for quick resolution. Thank you!

github-actions[bot] on (2024-08-02 01:53:20 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

JVD9kh96 (Issue Creator) on (2024-08-08 23:03:27 UTC): For those who are looking for a quick fix, you can use legacy keras:
`pip install tf_keras`


```shell
import tensorflow as tf
import numpy as np
import os 
os.environ['TF_USE_LEGACY_KERAS'] = 'True'
#... rest of the code 

#...output:
training psnr:
[25.50802231 27.92281532 28.17706299 28.29514313 28.37692451]
val_psnr:
[27.65083694 28.10541534 28.27854538 28.36878586 28.24624634]
```

tilakrayal (Assginee) on (2024-09-02 10:50:27 UTC): @JVD9kh96,
Glad the issue was resolved with the tf-keras. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/30aa38cea671499a4eae496c36a391e9/untitled2091.ipynb) of it [here](https://colab.research.google.com/gist/tilakrayal/03384cf3ffd968614056e06829009499/untitled2091.ipynb). And tensorflow v2.17 contains Keras3.0, and this is more related to keras please try to raise the issue in keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!

github-actions[bot] on (2024-09-10 01:59:03 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:59:00 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:59:08 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72366"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72366"">No</a>

"
2425238901,issue,closed,completed,tf.range miss some dtypes support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-113444-ga98a073af6f 2.18.0-dev20240722

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It is a common use case to make ranges for dtypes like uint8 (when working with images) and float32 -> float16 (inside keras layers with mixed precision disabled/enabled).

In TF 2.17 release tf.range can't work with these dtypes.
One more issue is that XLA/non-XLA dtypes support is different (more dtypes supported with XLA)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.range(10., delta=1., dtype='float16')
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node Range}} = Range[Tidx=DT_HALF]
All kernels registered for op Range:
  device='XLA_CPU_JIT'; Tidx in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32]
  device='XLA_GPU_JIT'; Tidx in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32]
  device='DEFAULT'; Tidx in [DT_INT32]
  device='GPU'; Tidx in [DT_INT64]
  device='GPU'; Tidx in [DT_DOUBLE]
  device='GPU'; Tidx in [DT_FLOAT]
  device='CPU'; Tidx in [DT_INT64]
  device='CPU'; Tidx in [DT_INT32]
  device='CPU'; Tidx in [DT_DOUBLE]
  device='CPU'; Tidx in [DT_FLOAT]
 [Op:Range] name:
```
",shkarupa-alex,2024-07-23 13:34:32+00:00,['Venkat6871'],2024-08-13 09:22:10+00:00,2024-08-13 09:22:07+00:00,https://github.com/tensorflow/tensorflow/issues/72365,"[('type:bug', 'Bug'), ('comp:apis', 'Highlevel API related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2249597403, 'issue_id': 2425238901, 'author': 'Venkat6871', 'body': '- I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/aee3cbcaae3ee68fa8de6cd259a4b5e4/72365_2-17-0-nightly.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 25, 6, 58, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285778477, 'issue_id': 2425238901, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72365"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72365"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 9, 22, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-25 06:58:46 UTC): - I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/aee3cbcaae3ee68fa8de6cd259a4b5e4/72365_2-17-0-nightly.ipynb) here for reference.

Thank you!

google-ml-butler[bot] on (2024-08-13 09:22:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72365"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72365"">No</a>

"
2425219924,issue,closed,completed,TensorFlow lite micro on Cortex-M7 ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

No

### OS platform and distribution

S32K344 Cortex-M7

### Mobile device

Cortexm-M7

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

arm-none-eabi-gcc 10.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I trained the tensor-flow model with layers Conv2d followed bye relu and Maxpool2D. 
this is the architecture i used
import tensorflow as tf
model1 = tf.keras.Sequential()
# model1.add(tf.keras.Input(shape=(750,3,1)))

model1.add(Conv2D(4,kernel_size=(5,5),strides=(1,1), padding='same',input_shape=(750, 3, 1)))
model1.add(tf.keras.layers.Activation(activation='relu'))
model1.add(MaxPooling2D(pool_size=(4,4),padding='same'))   
# model1.add(Dropout(0.25))

model1.add(Conv2D(8,kernel_size=(5,5),padding='same'))
model1.add(tf.keras.layers.Activation(activation='relu'))
model1.add(MaxPooling2D(pool_size=(4,4), padding='same'))  

model1.add(Conv2D(8,kernel_size=(5,5),padding='same'))
model1.add(tf.keras.layers.Activation(activation='relu'))
model1.add(MaxPooling2D(pool_size=(4,4), padding='same'))  

model1.add(Flatten())
model1.add(Dense(7))
model1.add(tf.keras.layers.Activation(activation='softmax'))
model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004518074511548236754), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model1.summary()

training and testing  accuracy is more than 95 %.
i convert the model to tflite using
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
tflite_quant_model = converter.convert()

and the make it in flattebuffers model.h using xxd -i model.tflite > model.h

and created the inference in c++ code for tflite micro

#include ""tensorflow/lite/micro/kernels/micro_ops.h""

#include ""tensorflow/lite/micro/micro_error_reporter.h""

#include ""tensorflow/lite/micro/micro_interpreter.h""

#include ""tensorflow/lite/micro/micro_mutable_op_resolver.h""

#include ""tensorflow/lite/schema/schema_generated.h""

#include ""tensorflow/lite/c/common.h""

#include ""tensorflow/lite/micro/micro_profiler.h""

// Include the model data

#include ""new_model_data.h""


 
#define INPUT_SIZE 2250

#define OUTPUT_SIZE 7
 
 
// Calculates size of model 118380
 
constexpr int kTensorArenaSize = 10208*sizeof(float);
 
// Globals, used for compatibility with Arduino-style sketches.

namespace {

tflite::MicroErrorReporter micro_error_reporter;

tflite::ErrorReporter* error_reporter = nullptr;

const tflite::Model* model = nullptr;

tflite::MicroInterpreter* interpreter = nullptr;

TfLiteTensor* input = nullptr;

TfLiteTensor* output = nullptr;

uint8_t tensor_arena[kTensorArenaSize];

tflite::MicroProfiler profiler;

}  // namespace
 
void setup() {

  // Set up logging.

  error_reporter = &micro_error_reporter;

  error_reporter->Report(""model test on cortexm7"");
 
  model = tflite::GetModel(Conv2d_model_v3_f16_tflite);

  if(model==nullptr)

  {

	  printf(""model errror"");

  }
 
  if (model->version() != TFLITE_SCHEMA_VERSION) {

    TF_LITE_REPORT_ERROR(error_reporter,

                         ""Model provided is schema version %d not equal ""

                         ""to supported version %d."",

                         model->version(), TFLITE_SCHEMA_VERSION);

    return;

  }
 
  printf(""getting the model \n"");
 
   // This pulls in all the operation implementations we need.

  static tflite::MicroMutableOpResolver<5> micro_op_resolver;

  micro_op_resolver.AddConv2D(tflite::Register_CONV_2D());

  micro_op_resolver.AddMaxPool2D(tflite::Register_MAX_POOL_2D());

  micro_op_resolver.AddRelu();

//  micro_op_resolver.AddReshape();

  micro_op_resolver.AddFullyConnected(tflite::Register_FULLY_CONNECTED());

  micro_op_resolver.AddSoftmax(tflite::Register_SOFTMAX());
 
  // Build an interpreter to run the model with.

  static tflite::MicroInterpreter static_interpreter(

      model, micro_op_resolver, tensor_arena, kTensorArenaSize,nullptr,&profiler);

  interpreter = &static_interpreter;
 
  // Allocate memory from the tensor_arena for the model's tensors.
 
  printf(""allocating the tesnors \n "");
 
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
 
  if (allocate_status != kTfLiteOk) {

     MicroPrintf(""AllocateTensors() failed"");

     return;

   }

  	  input = interpreter->input(0);
 
}

void loop() {
 
  	  printf(""data initialisation\n"");

	  float input_data[INPUT_SIZE] = {};
printf(""allocating data to model \n"");

//	  memcpy(input->data.f16, &input_data, 2250*sizeof(float));

		 for (int i = 0; i < INPUT_SIZE; i++) {

			 input->data.f16[i] = input_data[i];

		  }
 
	  if (kTfLiteOk != interpreter->Invoke()) {

	     MicroPrintf(""Invoke failed."");

	   }

	  TfLiteTensor* output = interpreter->output(0);
 
 
  // Read the output

	  for (int i = 0; i < OUTPUT_SIZE; ++i) {

    float output_value = output->data.f[i];

    error_reporter->Report(""Output value[%d]: %f"", i, output_value);

  }

}
 
int main() {
 
  setup();

  while (true) {

    loop();

  }

  return 0;

} 
Now the problem is that when i feeding the input it is showing in the output, means it is pointing the same memory location for input tensor and output tensor.
what is the the feasible solution for this.

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

```shell
1221 input the output also 2212
```
",gajendrahatt,2024-07-23 13:26:16+00:00,"['pkgoogle', 'sawantkumar']",2024-08-13 01:55:29+00:00,2024-08-13 01:55:26+00:00,https://github.com/tensorflow/tensorflow/issues/72364,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:micro', 'Related to TensorFlow Lite Microcontrollers'), ('TFLiteConverter', 'For issues related to TFLite converter'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2257047547, 'issue_id': 2425219924, 'author': 'pkgoogle', 'body': 'Hi @gajendrahatt, I believe https://github.com/tensorflow/tflite-micro/issues will be able to assist you better. Thanks.', 'created_at': datetime.datetime(2024, 7, 29, 21, 32, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213364, 'issue_id': 2425219924, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196324, 'issue_id': 2425219924, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196363, 'issue_id': 2425219924, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72364"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72364"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 28, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-29 21:32:37 UTC): Hi @gajendrahatt, I believe https://github.com/tensorflow/tflite-micro/issues will be able to assist you better. Thanks.

github-actions[bot] on (2024-08-06 01:53:30 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-13 01:55:26 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-13 01:55:28 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72364"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72364"">No</a>

"
2425129579,issue,open,,tf.raw_ops.BlockLSTMV2: tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9) Aborted (core dumped) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.BlockLSTMV2`  . The code is as follows:

```python
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```

The error message was as follows:

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```

The above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). To reproduce the issue, I provided that a [colab notebook](https://colab.research.google.com/drive/1UxYr3Fg7uKd_cZA-ekKrM9WsfJK-ox_Z?usp=sharing) to reproduce the error.

 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```


### Relevant log output

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```
",KnightGOKU,2024-07-23 12:46:06+00:00,['Venkat6871'],2024-09-30 10:05:27+00:00,,https://github.com/tensorflow/tensorflow/issues/72362,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2247253510, 'issue_id': 2425129579, 'author': 'Venkat6871', 'body': '- I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/34d36f306573867393cb86e182b89a9c/72362_2-17-0-tf-nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 24, 8, 45, 28, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-24 08:45:28 UTC): - I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/34d36f306573867393cb86e182b89a9c/72362_2-17-0-tf-nightly-v.ipynb) here for reference.

Thank you!

"
2424627109,issue,closed,not_planned,"about the official release schedule of ""Play Services TFLite Java"" version 16.2.0","**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
samsung/d2que/d2q:12/SP1A.210812.016/N975U1UES7HVF4:user/release-keys

- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
 target sdk 34
 com.google.android.gms:play-services-tflite-java:16.2.0-beta02

- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):
24.26.32

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.


I found that if I use com.google.android.gms:play-services-tflite , ""libtensorflowlite_jni_gms_client.so"" is included in the apk.
16kb align was not applied in version 16.1.0, but
In version 16.2.0-beta02, 16kb align were applied to 64bit

However, the beta version is not safe to use.
So I would like to know the official release schedule for 16.2.0.

Thank you.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

![image](https://github.com/user-attachments/assets/ff462b90-f72e-45cb-935f-8472b0b2e30c)
",wonyoungmin,2024-07-23 08:47:37+00:00,"['arfaian', 'pkgoogle', 'sawantkumar']",2024-12-16 22:34:51+00:00,2024-11-26 18:08:14+00:00,https://github.com/tensorflow/tensorflow/issues/72352,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGooglePlayServices', 'For issues related to TensorFlow Lite in Google Play Services')]","[{'comment_id': 2261599107, 'issue_id': 2424627109, 'author': 'pkgoogle', 'body': 'Hi @arfaian, I believe you would have more insight here. Thanks.', 'created_at': datetime.datetime(2024, 7, 31, 22, 51, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500220427, 'issue_id': 2424627109, 'author': 'gaikwadrahul8', 'body': ""Hi, @wonyoungmin \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/45\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 20, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501618125, 'issue_id': 2424627109, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72352"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72352"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 8, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547001050, 'issue_id': 2424627109, 'author': 'fergushenderson', 'body': 'Versions 16.3.0 and 16.4.0 of play-services-tflite-java have been released on November 18th and December 9th respectively. Those should address this issue.', 'created_at': datetime.datetime(2024, 12, 16, 22, 34, 50, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-31 22:51:57 UTC): Hi @arfaian, I believe you would have more insight here. Thanks.

gaikwadrahul8 on (2024-11-26 10:20:05 UTC): Hi, @wonyoungmin 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/45

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:08:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72352"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72352"">No</a>

fergushenderson on (2024-12-16 22:34:50 UTC): Versions 16.3.0 and 16.4.0 of play-services-tflite-java have been released on November 18th and December 9th respectively. Those should address this issue.

"
2424016391,issue,open,,tf.strided_slice new_axis_mask inconsistency,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly, 2.17.0, 2.16.2, 2.16.1

### Custom code

No

### OS platform and distribution

macOS, Linux

### Mobile device

_No response_

### Python version

3.10.12, 3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm seeing that when `spec_size` (as described in the docs as the length of the `begin`, `end`, `strides` arrays) is less than the rank of the `input` tensor, the behavior of `strided_slice` differs when `new_axis_mask` for the bits between `len(begin)` and `tf.rank(input)` is specified.

Is this the expected behavior, and if so, is there anything else like this that differs when `spec_size != tf.rank(input)`? 

I'm noting that it's currently permitted for `spec_size > tf.rank(input)`, which yields the same result in this case as when `spec_size == tf.rank(input)`.

### Standalone code to reproduce the issue

```shell
Notebook:
https://colab.research.google.com/drive/1-LCENzCjorhzyDCqq5qfB4IFoxLbHjGI?usp=sharing
Code:

import numpy as np
import tensorflow as tf

@tf.function
def test(t, begin, end, mask):
    return tf.strided_slice(t, begin, end, new_axis_mask=mask)

t = tf.constant(np.arange(0,27).reshape((3,3,3)))
mask = 0b111

# Noting as comments the shape of the result

# shape = (1, 3, 3, 3)
print(test(t, [0], [3], mask))
# shape = (1, 1, 3, 3, 3)
print(test(t, [0, 0], [3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0], [3, 3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0, 0], [3, 3, 3, 3], mask))
```
```


### Relevant log output

_No response_",arteen1000,2024-07-23 00:44:47+00:00,['Venkat6871'],2024-10-10 09:56:42+00:00,,https://github.com/tensorflow/tensorflow/issues/72332,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2246987568, 'issue_id': 2424016391, 'author': 'Venkat6871', 'body': '- I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/79924fdc1919b1884f537098b0f72f66/72332_2-17-0-nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 24, 6, 14, 53, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-24 06:14:53 UTC): - I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/79924fdc1919b1884f537098b0f72f66/72332_2-17-0-nightly-v.ipynb) here for reference.

Thank you!

"
2422978651,issue,open,,tf.raw_ops.MapUnstage: Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.MapUnstage`  with randomly generated tensors. The code is as follows:

```python
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```

The error message was as follows:

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```

I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). Also, I provided that a [colab notebook](https://colab.research.google.com/drive/1PXsrbcckN5x_ooMjZr5Vds6KVKMve7ph?usp=sharing) to reproduce the error.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```


### Relevant log output

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```
",KnightGOKU,2024-07-22 14:08:43+00:00,['tilakrayal'],2024-07-23 08:11:19+00:00,,https://github.com/tensorflow/tensorflow/issues/72295,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2244543316, 'issue_id': 2422978651, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/db36dfc77a856176db4d5cae914378be/untitled2018.ipynb).', 'created_at': datetime.datetime(2024, 7, 23, 8, 11, 7, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-23 08:11:07 UTC): I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/db36dfc77a856176db4d5cae914378be/untitled2018.ipynb).

"
2422351226,issue,closed,completed,bc1qgldqjrfcv8umlcmz3tq4lpc0jyuvw7rnpkz0rx,1G11531KbwLLTHH2zMhTmJu9kvjL9dzMGm,Allan1974,2024-07-22 09:07:57+00:00,['Venkat6871'],2024-07-22 13:41:02+00:00,2024-07-22 13:41:02+00:00,https://github.com/tensorflow/tensorflow/issues/72284,[],"[{'comment_id': 2242993149, 'issue_id': 2422351226, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2024, 7, 22, 13, 41, 2, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-22 13:41:02 UTC): Please don't spam

"
2422311985,issue,open,,CVE-2021-35958 vulnerability in the latest v2.17,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.17

### Custom code

Yes

### OS platform and distribution

linux amd64, Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Our security scans are complaining on this Tensorflow vulnerability since we started use it (from v2.13): https://www.cvedetails.com/cve/CVE-2021-35958/
Is there any ETA of fixing it?
Thanks



### Standalone code to reproduce the issue

```shell
https://www.cvedetails.com/cve/CVE-2021-35958/
```


### Relevant log output

_No response_",OlgasAcc,2024-07-22 08:49:34+00:00,['tilakrayal'],2024-07-23 04:46:02+00:00,,https://github.com/tensorflow/tensorflow/issues/72282,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature'), ('comp:core', 'issues related to core part of tensorflow'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2242997116, 'issue_id': 2422311985, 'author': 'mihaimaruseac', 'body': ""> NOTE: the vendor's position is that tf.keras.utils.get_file is not intended for untrusted archives\r\n\r\nThis was a vulnerability submitted by a researcher after the team disagreed with the assessment. Please don't just go for zero scan results, instead analyze the reports.\r\n\r\nSince the TF team does not consider this a vulnerability, this could get closed."", 'created_at': datetime.datetime(2024, 7, 22, 13, 42, 52, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-22 13:42:52 UTC): This was a vulnerability submitted by a researcher after the team disagreed with the assessment. Please don't just go for zero scan results, instead analyze the reports.

Since the TF team does not consider this a vulnerability, this could get closed.

"
2422306328,issue,closed,completed,ARM64-v8 Android cross compile build failed,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Apple macos

### Mobile device

Android arm64-v8

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Im getting an error in build for ARM64-v8 android target device with gpu support enabled. I was following this: https://www.tensorflow.org/lite/guide/build_cmake#available_options_to_build_tensorflow_lite

### Standalone code to reproduce the issue

```shell
download [flatbuffers](https://github.com/google/flatbuffers.git)

git clone https://github.com/google/flatbuffers.git
cd flatbuffers
cmake -G ""Unix Makefiles""
make -j
```
download and build tensorflow
```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mdkir tflite_build
cd tflite_build
cmake -DCMAKE_TOOLCHAIN_FILE=/Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125/build/cmake/android.toolchain.cmake -DANDROID_PLATFORM=26 -DANDROID_ABI=arm64-v8a -DTFLITE_ENABLE_GPU=ON -DTFLITE_HOST_TOOLS_DIR=../flatbuffers/ ../tensorflow_src/tensorflow/lite/c
cmake --build . -j
```
```


### Relevant log output

```shell
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:77:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_PACKING_ALGORITHM_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:78:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_WEIGHTS_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:79:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_BIAS_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:80:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_OFFSET) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:81:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_SIZE) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.cc:15:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:170:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_BASE_OFFSET) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
[ 63%] Built target absl_status
6 errors generated.
gmake[2]: *** [tensorflow-lite/CMakeFiles/xnnpack-delegate.dir/build.make:94: tensorflow-lite/CMakeFiles/xnnpack-delegate.dir/delegates/xnnpack/weight_cache.cc.o] Error 1
gmake[2]: *** Waiting for unfinished jobs....
[ 63%] Linking CXX static library libabsl_flags_reflection.a
[ 63%] Built target absl_flags_reflection
[ 63%] Building CXX object _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags.dir/flag.cc.o
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:77:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_PACKING_ALGORITHM_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:78:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_WEIGHTS_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:79:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_BIAS_ID) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:80:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_OFFSET) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:81:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_SIZE) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:45:
In file included from /Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tensorflow_src/tensorflow/lite/delegates/xnnpack/weight_cache.h:28:
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/tensorflow-lite/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h:170:12: error: no matching member function for call to 'VerifyField'
           VerifyField<uint64_t>(verifier, VT_BASE_OFFSET) &&
           ^~~~~~~~~~~~~~~~~~~~~
/Users/varunnaw/FastConvolution/src/FastConvolution/fastconvolution24/tflite_build/flatbuffers/include/flatbuffers/table.h:125:8: note: candidate function template not viable: requires 3 arguments, but 2 were provided
  bool VerifyField(const Verifier &verifier, voffset_t field,
       ^
[ 63%] Linking CXX static library libabsl_flags.a
[ 63%] Built target absl_flags
6 errors generated.
gmake[2]: *** [tensorflow-lite/CMakeFiles/xnnpack-delegate.dir/build.make:108: tensorflow-lite/CMakeFiles/xnnpack-delegate.dir/delegates/xnnpack/xnnpack_delegate.cc.o] Error 1
gmake[1]: *** [CMakeFiles/Makefile2:1373: tensorflow-lite/CMakeFiles/xnnpack-delegate.dir/all] Error 2
gmake: *** [Makefile:136: all] Error 2
```
",poltomo,2024-07-22 08:46:51+00:00,"['pkgoogle', 'sawantkumar']",2024-09-13 01:59:00+00:00,2024-09-13 01:58:56+00:00,https://github.com/tensorflow/tensorflow/issues/72281,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('subtype:macOS', 'macOS Build/Installation issues'), ('TF 2.8', ''), ('Android', '')]","[{'comment_id': 2242430420, 'issue_id': 2422306328, 'author': 'poltomo', 'body': ""@Venkat6871 \r\nMore info:\r\nI'm trying to build tensorflow lite for an arm64v8 android device with adreno gpu and hexagon dsp.\r\n\r\nI wanted to use the c or c++ api. Preferably without the interpreter or runtime involved as I want to benchmark the convolution implementations."", 'created_at': datetime.datetime(2024, 7, 22, 8, 52, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301552421, 'issue_id': 2422306328, 'author': 'sawantkumar', 'body': 'Hi @poltomo ,\r\n\r\nI replicated your steps and was able to build successfully without any issues. I downloaded the latest sdk and ndk and installed them and ran your command.  Can you please retry with the latest android ndk and sdk and let me know if it worked for you.\r\n\r\n```\r\ncmake version : 3.24.2\r\nndk :android-ndk-r27-linux\r\nsdk : commandlinetools-linux-11076708_latest\r\n```', 'created_at': datetime.datetime(2024, 8, 21, 9, 9, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301821933, 'issue_id': 2422306328, 'author': 'poltomo', 'body': '@sawantkumar \r\nI was able to build it by following this:\r\nhttps://android.googlesource.com/platform/external/tensorflow/+/6b511124eb0/tensorflow/lite/g3doc/guide/build_cmake.md#step-1-install-cmake-tool\r\n\r\nHowever, I am still stuck on compiling the gpu delegate for my mobile device.\r\n\r\nI want to benchmark my mobile gpu\'s performance with this.\r\n\r\n\r\n```\r\nbazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate  \r\n```\r\nAfter running this, I get this error.\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_varunnaw/1da03b68673ade9f46466d0036a5c86c/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration\r\nERROR: /private/var/tmp/_bazel_varunnaw/1da03b68673ade9f46466d0036a5c86c/external/local_config_apple_cc/BUILD:61:23: Analysis of target \'@local_config_apple_cc//:cc-compiler-darwin_arm64\' failed\r\n```\r\nIs there a way to not use apple\'s toolchains?', 'created_at': datetime.datetime(2024, 8, 21, 11, 28, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303938721, 'issue_id': 2422306328, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle  ,\r\n\r\nI ran the bazel command `bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate` on a M1 mac   and i also got the below error. Can you please take a look?\r\n\r\n```\r\nDEBUG: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.12\r\nERROR: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration\r\nERROR: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_config_apple_cc/BUILD:61:23: Analysis of target \'@local_config_apple_cc//:cc-compiler-darwin_arm64\' failed\r\n```', 'created_at': datetime.datetime(2024, 8, 22, 7, 7, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307660200, 'issue_id': 2422306328, 'author': 'pkgoogle', 'body': 'Hi @poltomo, on release branch r2.17, in Debian Linux, Python=3.12 and ndk=25.2.9519653 (I think this is 25c) -- I was able to build successfully... this is after configuring the TF source (./configure in root github dir).\r\n\r\n```sh\r\n# install python=3.12 or create a venv/conda environment that has 3.12\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit switch r2.17\r\ngit pull\r\n./configure\r\n# answer all the questions, I built with Clang\r\n# For android answer default for most... I used the specific ndk version up there, Android SDK Level=34\r\nexport TF_PYTHON_VERSION=3.12\r\nbazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate  \r\n```\r\n\r\nLet me know if this works.', 'created_at': datetime.datetime(2024, 8, 23, 19, 7, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307846462, 'issue_id': 2422306328, 'author': 'poltomo', 'body': ""@pkgoogle \r\nthe Xcode problem was on a mac. On that environment, I am getting the same error as @sawantkumar.\r\n\r\nI tried building in an x86_64 docker container.\r\n\r\nNow the build gives this error\r\n```\r\nbazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n\r\nStarting local Bazel server and connecting to it...\r\nINFO: Reading 'startup' options from /root/tensorflow_src2/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=215\r\nINFO: Reading rc options for 'build' from /root/tensorflow_src2/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/tensorflow_src2/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from /root/tensorflow_src2/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/root/bin/python3 --action_env PYTHON_LIB_PATH=/root/lib/python3.12/site-packages --python_path=/root/bin/python3 --action_env CLANG_COMPILER_PATH=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=CC=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=BAZEL_COMPILER=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions\r\nINFO: Found applicable config definition build:short_logs in file /root/tensorflow_src2/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/tensorflow_src2/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm64 in file /root/tensorflow_src2/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /root/tensorflow_src2/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt\r\nINFO: Found applicable config definition build:no_tfrt in file /root/tensorflow_src2/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\r\nERROR: /root/tensorflow_src2/tensorflow/lite/delegates/gpu/BUILD:137:10: While resolving toolchains for target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so: invalid registered toolchain '@go_sdk//:go_windows_arm64': no such package '@go_sdk//': error globbing [pkg/linux_amd64/**/*.a] - [pkg/linux_amd64/**/cmd/**] op=FILES: /root/.cache/bazel/_bazel_root/3bf8bfe2267c52b1b269a3648926f129/external/go_sdk/pkg/linux_amd64/go/build (No such file or directory)\r\nERROR: Analysis of target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so' failed; build aborted: \r\nINFO: Elapsed time: 199.354s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (45 packages loaded, 9 targets configured)\r\n```\r\n\r\n\r\nIt was hanging here for a long time the last time I tried to build:\r\nFetching repository @llvm-raw; starting 83s\r\n    Fetching /root/.cache/bazel/_bazel_root/3bf8bfe2267c52b1b269a3648926f129/external/llvm-raw; Extracting ae8627809076390dbab04e01f3bf9d384c9e124e.tar.gz 79s"", 'created_at': datetime.datetime(2024, 8, 23, 21, 41, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310864908, 'issue_id': 2422306328, 'author': 'pkgoogle', 'body': ""Hi @poltomo, I reran my instructions on my Mac M1 and I was able to build successfully ... can you check if you ran the ./configure script and the values you entered? Especially the NDK path and version? Are you using Intel or a M series processor?\r\n\r\nAlso what version of clang are you using? Here's mine:\r\n```sh\r\nclang --version\r\nHomebrew clang version 18.1.4\r\nTarget: arm64-apple-darwin23.6.0\r\nThread model: posix\r\nInstalledDir: /xxxxxx/llvm/bin\r\n```"", 'created_at': datetime.datetime(2024, 8, 26, 18, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311144563, 'issue_id': 2422306328, 'author': 'poltomo', 'body': 'Here\'s my ./configure answers:\r\n```\r\nYou have bazel 6.1.0 installed.\r\nPlease specify the location of python. [Default is /opt/miniconda3/envs/fc1/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /opt/miniconda3/envs/fc1/lib/python3.10/site-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/miniconda3/envs/fc1/lib/python3.10/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Ofast\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nDo you wish to build TensorFlow with iOS support? [y/N]: n\r\nNo iOS support will be enabled for TensorFlow.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v1          \t# Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\nclang --version\r\n```\r\nApple clang version 15.0.0 (clang-1500.3.9.4)\r\nTarget: arm64-apple-darwin23.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\nDo you think maybe, I shouldn\'t use Apple\'s clang distribution? I am trying to cross-compile for android arm, so should I set the toolchain to the one in the Android NDK?\r\n\r\n\r\nBuild still fails\r\n```\r\nERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration\r\nERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/local_config_apple_cc/BUILD:61:23: Analysis of target \'@local_config_apple_cc//:cc-compiler-darwin_arm64\' failed\r\nINFO: Repository vulkan_headers instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:88:19: in _initialize_third_party\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/vulkan_headers/workspace.bzl:6:20: in repo\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository eigen_archive instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:67:11: in _initialize_third_party\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/eigen3/workspace.bzl:14:20: in repo\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository gemmlowp instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:70:13: in _initialize_third_party\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/gemmlowp/workspace.bzl:14:20: in repo\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository ruy instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:85:8: in _initialize_third_party\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/ruy/workspace.bzl:6:20: in repo\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository cpuinfo instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1026:21: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:170:20: in _tf_repositories\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository farmhash_archive instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:68:13: in _initialize_third_party\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/farmhash/workspace.bzl:14:20: in repo\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nINFO: Repository XNNPACK instantiated at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1026:21: in workspace\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:148:20: in _tf_repositories\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>\r\nERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/flatbuffers/BUILD.bazel:85:10: errors encountered resolving toolchains for @flatbuffers//:flatc\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/schema/BUILD:184:22: errors encountered resolving toolchains for //tensorflow/lite/schema:conversion_metadata_fbs\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/core/BUILD:102:11: errors encountered resolving toolchains for //tensorflow/lite/core:framework_experimental\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/core/c/BUILD:294:38: errors encountered resolving toolchains for //tensorflow/lite/core/c:c_api_experimental\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/c/BUILD:106:43: errors encountered resolving toolchains for //tensorflow/lite/c:c_api_experimental\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/delegates/utils/BUILD:103:23: errors encountered resolving toolchains for //tensorflow/lite/delegates/utils:utils\r\nERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/delegates/gpu/BUILD:225:11: errors encountered resolving toolchains for //tensorflow/lite/delegates/gpu:delegate\r\nERROR: Analysis of target \'//tensorflow/lite/delegates/gpu:delegate\' failed; build aborted: \r\nINFO: Elapsed time: 44.138s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (126 packages loaded, 1412 targets configured)\r\n    Fetching repository @local_config_cc; Running xcode-locator\r\n    Fetching https://storage.googleapis.com/.../archive/32c07c0c5334aea069e518206d75e002ccd85389.tar.gz\r\n    Fetching https://storage.googleapis.com/.../eigen-0b51f763cbbd0ed08168f88972724329f0375498.tar.gz\r\n    Fetching https://storage.googleapis.com/.../ruy/archive/3286a34cc8de6149ac6844107dfdffac91531e72.zip\r\n    Fetching https://storage.googleapis.com/.../archive/e844ffd17118c1e17d94e1ba4354c075a4577b88.zip\r\n    Fetching https://storage.googleapis.com/.../archive/0d859a811870d10f53a594927d0d0b97573ad06d.tar.gz\r\n```', 'created_at': datetime.datetime(2024, 8, 26, 21, 37, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311159486, 'issue_id': 2422306328, 'author': 'poltomo', 'body': 'Take a look at this BUILD file, should it show anything about metal? I thought the bazel build was for android_arm64\r\ncat tensorflow/lite/delegates/gpu/BUILD\r\n```\r\nload(""@bazel_skylib//lib:selects.bzl"", ""selects"")\r\nload(""//tensorflow/lite:special_rules.bzl"", ""tflite_extra_gles_deps"")\r\nload(""//tensorflow/lite/delegates/gpu:build_defs.bzl"", ""gpu_delegate_linkopts"")\r\nload(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_static_framework"")\r\nload(""@build_bazel_rules_apple//apple:macos.bzl"", ""macos_dylib"")\r\n\r\npackage(\r\n    # copybara:uncomment default_applicable_licenses = [""//tensorflow:license""],\r\n    default_visibility = [""//visibility:public""],\r\n    licenses = [""notice""],\r\n)\r\n\r\nexports_files([\r\n    ""delegate.h"",\r\n    ""delegate_options.h"",\r\n    ""metal_delegate.h"",\r\n])\r\n\r\nconfig_setting(\r\n    name = ""tflite_gpu_binary_release"",\r\n    values = {""copt"": ""-DTFLITE_GPU_BINARY_RELEASE""},\r\n)\r\n\r\nconfig_setting(\r\n    name = ""tflite_gpu_extra_gles_deps"",\r\n    values = {\r\n        ""copt"": ""-DTFLITE_GPU_EXTRA_GLES_DEPS"",\r\n        ""cpu"": ""k8"",\r\n    },\r\n)\r\n\r\ncc_library(\r\n    name = ""gl_delegate"",\r\n    srcs = [""gl_delegate.cc""],\r\n    hdrs = [""gl_delegate.h""],\r\n    linkopts = gpu_delegate_linkopts(),\r\n    deps = [\r\n        ""//tensorflow/lite:kernel_api"",\r\n        ""//tensorflow/lite:minimal_logging"",\r\n        ""//tensorflow/lite/core/c:common"",\r\n        ""//tensorflow/lite/delegates/gpu/common:convert"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_builder"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_transformer"",\r\n        ""//tensorflow/lite/delegates/gpu/common:shape"",\r\n        ""//tensorflow/lite/delegates/gpu/common:status"",\r\n        ""//tensorflow/lite/delegates/gpu/common:tensor"",\r\n        ""//tensorflow/lite/delegates/gpu/common/transformations:model_transformations"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:api"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:command_queue"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:compiler"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:egl_environment"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:gl_call"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:request_gpu_info"",\r\n        ""//tensorflow/lite/delegates/gpu/gl/converters:bhwc_to_phwc4"",\r\n        ""//tensorflow/lite/delegates/gpu/gl/converters:phwc4_to_bhwc"",\r\n        ""//tensorflow/lite/delegates/gpu/gl/kernels:registry"",\r\n        ""//tensorflow/lite/delegates/gpu/gl/workgroups:best_effort_calculator"",\r\n        ""@com_google_absl//absl/base:core_headers"",\r\n        ""@com_google_absl//absl/types:span"",\r\n    ] + select({\r\n        ""//conditions:default"": [\r\n            ""//tensorflow/lite/delegates/gpu/gl:common_cc_fbs"",\r\n            ""//tensorflow/lite/delegates/gpu/gl:metadata_cc_fbs"",\r\n            ""//tensorflow/lite/delegates/gpu/gl:workgroups_cc_fbs"",\r\n            ""//tensorflow/lite/schema:schema_fbs"",\r\n            ""@flatbuffers"",\r\n        ],\r\n        "":tflite_gpu_binary_release"": [],\r\n    }) + tflite_extra_gles_deps(),\r\n)\r\n\r\nobjc_library(\r\n    name = ""metal_delegate"",\r\n    srcs = [""metal_delegate.mm""],\r\n    hdrs = [""metal_delegate.h""],\r\n    copts = [""-std=c++17""],\r\n    features = [""-layering_check""],\r\n    module_name = ""TensorFlowLiteCMetal"",\r\n    sdk_frameworks = [""Metal""],\r\n    deps = [\r\n        ""//tensorflow/lite:kernel_api"",\r\n        ""//tensorflow/lite:minimal_logging"",\r\n        ""//tensorflow/lite/core/c:common"",\r\n        ""//tensorflow/lite/delegates/gpu/common:convert"",\r\n        ""//tensorflow/lite/delegates/gpu/common:gpu_info"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_builder"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_transformer"",\r\n        ""//tensorflow/lite/delegates/gpu/common:precision"",\r\n        ""//tensorflow/lite/delegates/gpu/common:quantization_util"",\r\n        ""//tensorflow/lite/delegates/gpu/common:shape"",\r\n        ""//tensorflow/lite/delegates/gpu/common:status"",\r\n        ""//tensorflow/lite/delegates/gpu/common:tensor"",\r\n        ""//tensorflow/lite/delegates/gpu/common:types"",\r\n        ""//tensorflow/lite/delegates/gpu/metal:buffer_convert"",\r\n        ""//tensorflow/lite/delegates/gpu/metal:inference_context"",\r\n        ""//tensorflow/lite/delegates/gpu/metal:metal_spatial_tensor"",\r\n        ""@com_google_absl//absl/types:span"",\r\n    ],\r\n)\r\n\r\nobjc_library(\r\n    name = ""metal_delegate_internal"",\r\n    hdrs = [""metal_delegate_internal.h""],\r\n    copts = [""-std=c++17""],\r\n    sdk_frameworks = [""Metal""],\r\n    deps = [""//tensorflow/lite/delegates/gpu:metal_delegate""],\r\n)\r\n\r\n# build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always :libtensorflowlite_gpu_gl.so\r\ncc_binary(\r\n    name = ""libtensorflowlite_gpu_gl.so"",\r\n    linkopts = [\r\n        ""-Wl,-soname=libtensorflowlite_gpu_gl.so"",\r\n    ] + gpu_delegate_linkopts() + select({\r\n        ""//tensorflow:windows"": [],\r\n        ""//conditions:default"": [\r\n            ""-fvisibility=hidden"",\r\n        ],\r\n    }),\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    tags = [\r\n        ""nobuilder"",\r\n        ""notap"",\r\n    ],\r\n    deps = ["":gl_delegate""],\r\n)\r\n\r\n# build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always :libtensorflowlite_gpu_delegate.so\r\ncc_binary(\r\n    name = ""libtensorflowlite_gpu_delegate.so"",\r\n    linkopts = [\r\n        ""-Wl,-soname=libtensorflowlite_gpu_delegate.so"",\r\n    ] + gpu_delegate_linkopts() + select({\r\n        ""//tensorflow:windows"": [],\r\n        ""//conditions:default"": [\r\n            ""-fvisibility=hidden"",\r\n        ],\r\n    }),\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    tags = [\r\n        ""nobuilder"",\r\n        ""notap"",\r\n    ],\r\n    deps = ["":delegate""],\r\n)\r\n\r\n# bazel build -c opt --cpu ios_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :libtensorflowlite_gpu_metal --apple_platform_type=ios\r\nios_static_framework(\r\n    name = ""tensorflow_lite_gpu_framework"",\r\n    hdrs = [\r\n        ""metal_delegate.h"",\r\n        ""metal_delegate_internal.h"",\r\n    ],\r\n    minimum_os_version = ""11.4"",\r\n    deps = ["":metal_delegate""],\r\n)\r\n\r\n# Note: Support for MacOS is best-effort at the moment.\r\n# bazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :tensorflow_lite_gpu_dylib --apple_platform_type=macos\r\nmacos_dylib(\r\n    name = ""tensorflow_lite_gpu_dylib"",\r\n    linkopts = [\r\n        ""-all_load"",\r\n        ""-dead_strip"",\r\n    ],\r\n    minimum_os_version = ""10.13"",\r\n    tags = [\r\n        ""manual"",\r\n        ""nobuilder"",\r\n        ""notap"",\r\n    ],\r\n    deps = [\r\n        "":metal_delegate"",\r\n        "":metal_delegate_internal"",\r\n    ],\r\n)\r\n\r\ncc_library(\r\n    name = ""api"",\r\n    srcs = [""api.cc""],\r\n    hdrs = [""api.h""],\r\n    deps = [\r\n        ""//tensorflow/lite/delegates/gpu/common:data_type"",\r\n        ""//tensorflow/lite/delegates/gpu/common:status"",\r\n        ""//tensorflow/lite/delegates/gpu/common:util"",\r\n        ""//tensorflow/lite/delegates/gpu/gl:portable"",\r\n        ""@com_google_absl//absl/types:span"",\r\n        ""@com_google_absl//absl/types:variant"",\r\n        ""@opencl_headers"",\r\n        ""@vulkan_headers//:vulkan_headers_no_prototypes"",\r\n    ],\r\n)\r\n\r\ncc_library(\r\n    name = ""spi"",\r\n    hdrs = [""spi.h""],\r\n    deps = [\r\n        "":api"",\r\n        ""//tensorflow/lite/delegates/gpu/common:access_type"",\r\n        ""//tensorflow/lite/delegates/gpu/common:status"",\r\n    ],\r\n)\r\n\r\n# Currently the GPU delegate needs to be built on Android (due to EGL dependency),\r\n# or built with -DCL_DELEGATE_NO_GL (disabling OpenGL backend fallback), or both.\r\nselects.config_setting_group(\r\n    name = ""supports_gpu_delegate"",\r\n    match_any = [\r\n        ""//tensorflow:android"",\r\n        ""//tensorflow/lite/delegates/gpu/cl:opencl_delegate_no_gl"",\r\n    ],\r\n)\r\n\r\ncc_library(\r\n    name = ""delegate_options"",\r\n    srcs = [""delegate_options.cc""],\r\n    hdrs = [""delegate_options.h""],\r\n    deps = [""//tensorflow/lite/core/c:common""],\r\n)\r\n\r\ncc_library(\r\n    name = ""delegate"",\r\n    srcs = [\r\n        # copybara:comment_begin(oss-only)\r\n        ""android_version.cc"",\r\n        # copybara:comment_end\r\n        ""delegate.cc"",\r\n    ],\r\n    hdrs = [""delegate.h""],\r\n    linkopts = gpu_delegate_linkopts(),\r\n    deps = select({\r\n        ""//tensorflow/lite/delegates/gpu/cl:opencl_delegate_no_gl"": [],\r\n        ""//conditions:default"": [\r\n            ""//tensorflow/lite/delegates/gpu/gl:api2"",\r\n        ],\r\n    }) + [\r\n        "":api"",\r\n        "":delegate_options"",\r\n        "":tflite_profile"",\r\n        ""//tensorflow/lite:kernel_api"",\r\n        ""//tensorflow/lite:minimal_logging"",\r\n        ""//tensorflow/lite/core/async:backend_async_kernel_interface"",\r\n        ""//tensorflow/lite/core/async/c:task"",\r\n        ""//tensorflow/lite/core/async/interop/c:attribute_map"",\r\n        ""//tensorflow/lite/core/async/interop/c:constants"",\r\n        ""//tensorflow/lite/core/c:common"",\r\n        ""//tensorflow/lite/delegates:serialization"",\r\n        ""//tensorflow/lite/delegates/gpu/cl:api"",\r\n        ""//tensorflow/lite/delegates/gpu/cl:util"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_builder"",\r\n        ""//tensorflow/lite/delegates/gpu/common:model_builder_helper"",\r\n        ""//tensorflow/lite/delegates/gpu/common:quantization_util"",\r\n        ""//tensorflow/lite/delegates/utils"",\r\n        ""//tensorflow/lite/delegates/utils:async_type_helpers"",\r\n        ""//tensorflow/lite/delegates/utils:ret_macros"",\r\n        ""//tensorflow/lite/delegates/utils:sync_fence"",\r\n        ""//tensorflow/lite/kernels:kernel_util"",\r\n        ""//tensorflow/lite/profiling/telemetry"",\r\n        ""//tensorflow/lite/profiling/telemetry:telemetry_status"",\r\n        ""//tensorflow/lite/profiling/telemetry/c:telemetry_setting_internal"",\r\n        ""@com_google_absl//absl/container:flat_hash_map"",\r\n        ""@com_google_absl//absl/container:flat_hash_set"",\r\n        ""@com_google_absl//absl/memory"",\r\n        ""@com_google_absl//absl/types:span"",\r\n    ],\r\n)\r\n\r\ncc_library(\r\n    name = ""tflite_profile"",\r\n    srcs = [""tflite_profile.cc""],\r\n    hdrs = [""tflite_profile.h""],\r\n    deps = [\r\n        ""//tensorflow/lite/core/api"",\r\n        ""//tensorflow/lite/delegates/gpu/common/task:profiling_info"",\r\n        ""@com_google_absl//absl/time"",\r\n    ],\r\n)\r\n```', 'created_at': datetime.datetime(2024, 8, 26, 21, 48, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313218651, 'issue_id': 2422306328, 'author': 'pkgoogle', 'body': 'We actually have a lot of differences, I\'m fairly sure the main one is not configuring the Android Workspace though, you should say yes to that.\r\n\r\n(All the `~` are actually the absolute paths, which you should use)\r\nHere are my answers to the configuration:\r\n```sh\r\n./configure\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is ~/miniforge3/envs/72281/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  ~/miniforge3/envs/72281/lib/python3.12/site-packages\r\nPlease input the desired Python library path to use.  Default is [~/miniforge3/envs/72281/lib/python3.12/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:  #Just used default\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nPlease specify the home path of the Android NDK to use. [Default is ~/library/Android/Sdk/ndk-bundle]:  ~/Library/Android/sdk/ndk/25.2.9519653 #This should be the location if you\'re using Android Studio, which you should be\r\n\r\nPlease specify the (min) Android NDK API level to use. [Available levels: [16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33]] [Default is 21]: \r\n\r\n\r\nPlease specify the home path of the Android SDK to use. [Default is ~/library/Android/Sdk]: \r\n\r\n\r\nPlease specify the Android SDK API level to use. [Available levels: [\'29\', \'30\', \'32\', \'33\', \'34\']] [Default is 34]: \r\n\r\n\r\nPlease specify an Android build tools version to use. [Available versions: [\'30.0.3\', \'33.0.1\', \'34.0.0\']] [Default is 34.0.0]: \r\n\r\n\r\nDo you wish to build TensorFlow with iOS support? [y/N]: N\r\nNo iOS support will be enabled for TensorFlow.\r\n```\r\n\r\nPlease also be sure to align your versions with this: https://www.tensorflow.org/install/source#macos, Thanks.', 'created_at': datetime.datetime(2024, 8, 27, 18, 15, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313719496, 'issue_id': 2422306328, 'author': 'poltomo', 'body': '@pkgoogle\r\nI can\'t even find an android ndk version for arm64\r\n```\r\nYou have bazel 6.1.0 installed.\r\nPlease specify the location of python. [Default is /opt/miniconda3/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /opt/miniconda3/lib/python3.11/site-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/miniconda3/lib/python3.11/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:       \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: eifjcbfrguhvcdurfrhbejvkvjhlghcfelvrcklfgvbc\r\nInvalid selection: eifjcbfrguhvcdurfrhbejvkvjhlghcfelvrcklfgvbc\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nWARNING: The NDK version in /Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125 is 26, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.\r\n\r\nTraceback (most recent call last):\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1466, in <module>\r\n    main()\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1439, in main\r\n    create_android_ndk_rule(environ_cp)\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 658, in create_android_ndk_rule\r\n    get_ndk_api_level(environ_cp, android_ndk_home_path))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 752, in get_ndk_api_level\r\n    api_levels = sorted(os.listdir(platforms))\r\n                        ^^^^^^^^^^^^^^^^^^^^^\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125/platforms\'\r\n\r\n```', 'created_at': datetime.datetime(2024, 8, 27, 23, 4, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315934789, 'issue_id': 2422306328, 'author': 'pkgoogle', 'body': 'Hi @poltomo, do you have Android studio installed? If so, please try 25c for NDK first -- then as noted in the above message, try the latest NDK=21 version. You are using 26 here. If you are having trouble installing the correct version, please let me know.', 'created_at': datetime.datetime(2024, 8, 28, 17, 47, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316946238, 'issue_id': 2422306328, 'author': 'poltomo', 'body': 'I do have android studio installed.\r\nI am getting this error even though I have the android ndk\r\n\r\nls /Users/varunnaw/Library/Android/sdk/ndk \r\n```\r\n21.4.7075529\t26.1.10909125\t27.0.11902837\r\n```\r\n\r\nHere\'s the error in ./configure\r\nI think its looking for a platforms folder\r\n```\r\n(fc1) varunnaw@80a9970d555c tensorflow_src % ./configure\r\nYou have bazel 6.1.0 installed.\r\nPlease specify the location of python. [Default is /opt/miniconda3/envs/fc1/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /opt/miniconda3/envs/fc1/lib/python3.10/site-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/miniconda3/envs/fc1/lib/python3.10/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nWARNING: The NDK version in /Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125 is 26, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.\r\n\r\nTraceback (most recent call last):\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1466, in <module>\r\n    main()\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1439, in main\r\n    create_android_ndk_rule(environ_cp)\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 658, in create_android_ndk_rule\r\n    get_ndk_api_level(environ_cp, android_ndk_home_path))\r\n  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 752, in get_ndk_api_level\r\n    api_levels = sorted(os.listdir(platforms))\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125/platforms\'\r\n```\r\n\r\n\r\nMaybe there is something that you have setup that I don\'t have setup. Or maybe I have set it up incorrectly.\r\nIs there some docker container that could streamline this? I am cross compiling anyways so build host could be whatever.\r\n\r\nMy goal is to write my own delegate for tensorflow lite to use alongside the gpu delegate. Both will target the gpu, but I wanted my delegate to hold some opencl kernels I made.\r\n\r\nIf tensorflow lite also has an opencl delegate, I also want to build that too.\r\n\r\nHere\'s a post of mine\r\nhttps://discuss.ai.google.dev/t/how-to-create-a-new-operator-with-an-opencl-kernel/37073', 'created_at': datetime.datetime(2024, 8, 29, 7, 54, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316949027, 'issue_id': 2422306328, 'author': 'poltomo', 'body': 'find /Users/varunnaw/Library/Android/sdk/ndk/ -name platforms\r\n\r\nturns up nothing', 'created_at': datetime.datetime(2024, 8, 29, 7, 55, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318967079, 'issue_id': 2422306328, 'author': 'pkgoogle', 'body': 'Hi @poltomo, yeah the NDK version sometimes have file/directory changes thats part of why we have stringent NDK requirements, if you go to tools -> SDK manager, switch to SDK Tools, and scroll down to ""NDK (Side by side)""... you should find multiple versions to install:\r\n\r\n![image](https://github.com/user-attachments/assets/4d8783bd-3330-465c-883e-f8d51be1fbd6)\r\n\r\nLatest bazel only supports NDK = 19, 20, 21 and 25.\r\n\r\nIf you have no space constraints.. I recommend go ahead and installing all versions of 19, 20, 21 and 25. I would update your bazel (perhaps install and use bazelisk so this is automatic for you?) and then try the same thing against 25b (25.1.8....) as that is the current recommended version. Please review this https://www.tensorflow.org/lite/android/lite_build#install_bazel_and_android_prerequisites . I tried against 25c (25.2.9....) and that worked but if you go w/ the recommended version you will run into less issues as that is well traveled.', 'created_at': datetime.datetime(2024, 8, 29, 20, 52, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039550, 'issue_id': 2422306328, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347894289, 'issue_id': 2422306328, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 13, 1, 58, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347894352, 'issue_id': 2422306328, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72281"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72281"">No</a>', 'created_at': datetime.datetime(2024, 9, 13, 1, 58, 58, tzinfo=datetime.timezone.utc)}]","poltomo (Issue Creator) on (2024-07-22 08:52:38 UTC): @Venkat6871 
More info:
I'm trying to build tensorflow lite for an arm64v8 android device with adreno gpu and hexagon dsp.

I wanted to use the c or c++ api. Preferably without the interpreter or runtime involved as I want to benchmark the convolution implementations.

sawantkumar (Assginee) on (2024-08-21 09:09:17 UTC): Hi @poltomo ,

I replicated your steps and was able to build successfully without any issues. I downloaded the latest sdk and ndk and installed them and ran your command.  Can you please retry with the latest android ndk and sdk and let me know if it worked for you.

```
cmake version : 3.24.2
ndk :android-ndk-r27-linux
sdk : commandlinetools-linux-11076708_latest
```

poltomo (Issue Creator) on (2024-08-21 11:28:06 UTC): @sawantkumar 
I was able to build it by following this:
https://android.googlesource.com/platform/external/tensorflow/+/6b511124eb0/tensorflow/lite/g3doc/guide/build_cmake.md#step-1-install-cmake-tool

However, I am still stuck on compiling the gpu delegate for my mobile device.

I want to benchmark my mobile gpu's performance with this.


```
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate  
```
After running this, I get this error.

```
ERROR: /private/var/tmp/_bazel_varunnaw/1da03b68673ade9f46466d0036a5c86c/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration
ERROR: /private/var/tmp/_bazel_varunnaw/1da03b68673ade9f46466d0036a5c86c/external/local_config_apple_cc/BUILD:61:23: Analysis of target '@local_config_apple_cc//:cc-compiler-darwin_arm64' failed
```
Is there a way to not use apple's toolchains?

sawantkumar (Assginee) on (2024-08-22 07:07:24 UTC): Hi @pkgoogle  ,

I ran the bazel command `bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate` on a M1 mac   and i also got the below error. Can you please take a look?

```
DEBUG: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.12
ERROR: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration
ERROR: /private/var/tmp/_bazel_sawantkumar/b1fb662fe0554513fcb6a0a68e2568bf/external/local_config_apple_cc/BUILD:61:23: Analysis of target '@local_config_apple_cc//:cc-compiler-darwin_arm64' failed
```

pkgoogle (Assginee) on (2024-08-23 19:07:16 UTC): Hi @poltomo, on release branch r2.17, in Debian Linux, Python=3.12 and ndk=25.2.9519653 (I think this is 25c) -- I was able to build successfully... this is after configuring the TF source (./configure in root github dir).

```sh
# install python=3.12 or create a venv/conda environment that has 3.12
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git switch r2.17
git pull
./configure
# answer all the questions, I built with Clang
# For android answer default for most... I used the specific ndk version up there, Android SDK Level=34
export TF_PYTHON_VERSION=3.12
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate  
```

Let me know if this works.

poltomo (Issue Creator) on (2024-08-23 21:41:36 UTC): @pkgoogle 
the Xcode problem was on a mac. On that environment, I am getting the same error as @sawantkumar.

I tried building in an x86_64 docker container.

Now the build gives this error
```
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so

Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from /root/tensorflow_src2/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=215
INFO: Reading rc options for 'build' from /root/tensorflow_src2/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow_src2/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/tensorflow_src2/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/bin/python3 --action_env PYTHON_LIB_PATH=/root/lib/python3.12/site-packages --python_path=/root/bin/python3 --action_env CLANG_COMPILER_PATH=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=CC=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --repo_env=BAZEL_COMPILER=/root/android-ndk-r26d/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /root/tensorflow_src2/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow_src2/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /root/tensorflow_src2/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /root/tensorflow_src2/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --dynamic_mode=off --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /root/tensorflow_src2/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
ERROR: /root/tensorflow_src2/tensorflow/lite/delegates/gpu/BUILD:137:10: While resolving toolchains for target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so: invalid registered toolchain '@go_sdk//:go_windows_arm64': no such package '@go_sdk//': error globbing [pkg/linux_amd64/**/*.a] - [pkg/linux_amd64/**/cmd/**] op=FILES: /root/.cache/bazel/_bazel_root/3bf8bfe2267c52b1b269a3648926f129/external/go_sdk/pkg/linux_amd64/go/build (No such file or directory)
ERROR: Analysis of target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so' failed; build aborted: 
INFO: Elapsed time: 199.354s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (45 packages loaded, 9 targets configured)
```


It was hanging here for a long time the last time I tried to build:
Fetching repository @llvm-raw; starting 83s
    Fetching /root/.cache/bazel/_bazel_root/3bf8bfe2267c52b1b269a3648926f129/external/llvm-raw; Extracting ae8627809076390dbab04e01f3bf9d384c9e124e.tar.gz 79s

pkgoogle (Assginee) on (2024-08-26 18:57:48 UTC): Hi @poltomo, I reran my instructions on my Mac M1 and I was able to build successfully ... can you check if you ran the ./configure script and the values you entered? Especially the NDK path and version? Are you using Intel or a M series processor?

Also what version of clang are you using? Here's mine:
```sh
clang --version
Homebrew clang version 18.1.4
Target: arm64-apple-darwin23.6.0
Thread model: posix
InstalledDir: /xxxxxx/llvm/bin
```

poltomo (Issue Creator) on (2024-08-26 21:37:22 UTC): Here's my ./configure answers:
```
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /opt/miniconda3/envs/fc1/bin/python3]: 


Found possible Python library paths:
  /opt/miniconda3/envs/fc1/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/opt/miniconda3/envs/fc1/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Ofast


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Do you wish to build TensorFlow with iOS support? [y/N]: n
No iOS support will be enabled for TensorFlow.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```
clang --version
```
Apple clang version 15.0.0 (clang-1500.3.9.4)
Target: arm64-apple-darwin23.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```

Do you think maybe, I shouldn't use Apple's clang distribution? I am trying to cross-compile for android arm, so should I set the toolchain to the one in the Android NDK?


Build still fails
```
ERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/local_config_apple_cc/BUILD:61:23: in apple_cc_toolchain rule @local_config_apple_cc//:cc-compiler-darwin_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, verify that ""xcode-select -p"" is correct and then try: ""bazel shutdown"" to re-run Xcode configuration
ERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/local_config_apple_cc/BUILD:61:23: Analysis of target '@local_config_apple_cc//:cc-compiler-darwin_arm64' failed
INFO: Repository vulkan_headers instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:88:19: in _initialize_third_party
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/vulkan_headers/workspace.bzl:6:20: in repo
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository eigen_archive instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:67:11: in _initialize_third_party
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/eigen3/workspace.bzl:14:20: in repo
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository gemmlowp instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:70:13: in _initialize_third_party
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/gemmlowp/workspace.bzl:14:20: in repo
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository ruy instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:85:8: in _initialize_third_party
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/ruy/workspace.bzl:6:20: in repo
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository cpuinfo instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1026:21: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:170:20: in _tf_repositories
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository farmhash_archive instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1019:28: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:68:13: in _initialize_third_party
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/farmhash/workspace.bzl:14:20: in repo
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository XNNPACK instantiated at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/WORKSPACE:80:14: in <toplevel>
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:1026:21: in workspace
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/workspace2.bzl:148:20: in _tf_repositories
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
ERROR: /private/var/tmp/_bazel_varunnaw/e6793a92b5048dbb05c5ef22851d4859/external/flatbuffers/BUILD.bazel:85:10: errors encountered resolving toolchains for @flatbuffers//:flatc
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/schema/BUILD:184:22: errors encountered resolving toolchains for //tensorflow/lite/schema:conversion_metadata_fbs
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/core/BUILD:102:11: errors encountered resolving toolchains for //tensorflow/lite/core:framework_experimental
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/core/c/BUILD:294:38: errors encountered resolving toolchains for //tensorflow/lite/core/c:c_api_experimental
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/c/BUILD:106:43: errors encountered resolving toolchains for //tensorflow/lite/c:c_api_experimental
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/delegates/utils/BUILD:103:23: errors encountered resolving toolchains for //tensorflow/lite/delegates/utils:utils
ERROR: /Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/tensorflow/lite/delegates/gpu/BUILD:225:11: errors encountered resolving toolchains for //tensorflow/lite/delegates/gpu:delegate
ERROR: Analysis of target '//tensorflow/lite/delegates/gpu:delegate' failed; build aborted: 
INFO: Elapsed time: 44.138s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (126 packages loaded, 1412 targets configured)
    Fetching repository @local_config_cc; Running xcode-locator
    Fetching https://storage.googleapis.com/.../archive/32c07c0c5334aea069e518206d75e002ccd85389.tar.gz
    Fetching https://storage.googleapis.com/.../eigen-0b51f763cbbd0ed08168f88972724329f0375498.tar.gz
    Fetching https://storage.googleapis.com/.../ruy/archive/3286a34cc8de6149ac6844107dfdffac91531e72.zip
    Fetching https://storage.googleapis.com/.../archive/e844ffd17118c1e17d94e1ba4354c075a4577b88.zip
    Fetching https://storage.googleapis.com/.../archive/0d859a811870d10f53a594927d0d0b97573ad06d.tar.gz
```

poltomo (Issue Creator) on (2024-08-26 21:48:30 UTC): Take a look at this BUILD file, should it show anything about metal? I thought the bazel build was for android_arm64
cat tensorflow/lite/delegates/gpu/BUILD
```
load(""@bazel_skylib//lib:selects.bzl"", ""selects"")
load(""//tensorflow/lite:special_rules.bzl"", ""tflite_extra_gles_deps"")
load(""//tensorflow/lite/delegates/gpu:build_defs.bzl"", ""gpu_delegate_linkopts"")
load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_static_framework"")
load(""@build_bazel_rules_apple//apple:macos.bzl"", ""macos_dylib"")

package(
    # copybara:uncomment default_applicable_licenses = [""//tensorflow:license""],
    default_visibility = [""//visibility:public""],
    licenses = [""notice""],
)

exports_files([
    ""delegate.h"",
    ""delegate_options.h"",
    ""metal_delegate.h"",
])

config_setting(
    name = ""tflite_gpu_binary_release"",
    values = {""copt"": ""-DTFLITE_GPU_BINARY_RELEASE""},
)

config_setting(
    name = ""tflite_gpu_extra_gles_deps"",
    values = {
        ""copt"": ""-DTFLITE_GPU_EXTRA_GLES_DEPS"",
        ""cpu"": ""k8"",
    },
)

cc_library(
    name = ""gl_delegate"",
    srcs = [""gl_delegate.cc""],
    hdrs = [""gl_delegate.h""],
    linkopts = gpu_delegate_linkopts(),
    deps = [
        ""//tensorflow/lite:kernel_api"",
        ""//tensorflow/lite:minimal_logging"",
        ""//tensorflow/lite/core/c:common"",
        ""//tensorflow/lite/delegates/gpu/common:convert"",
        ""//tensorflow/lite/delegates/gpu/common:model"",
        ""//tensorflow/lite/delegates/gpu/common:model_builder"",
        ""//tensorflow/lite/delegates/gpu/common:model_transformer"",
        ""//tensorflow/lite/delegates/gpu/common:shape"",
        ""//tensorflow/lite/delegates/gpu/common:status"",
        ""//tensorflow/lite/delegates/gpu/common:tensor"",
        ""//tensorflow/lite/delegates/gpu/common/transformations:model_transformations"",
        ""//tensorflow/lite/delegates/gpu/gl:api"",
        ""//tensorflow/lite/delegates/gpu/gl:command_queue"",
        ""//tensorflow/lite/delegates/gpu/gl:compiler"",
        ""//tensorflow/lite/delegates/gpu/gl:egl_environment"",
        ""//tensorflow/lite/delegates/gpu/gl:gl_call"",
        ""//tensorflow/lite/delegates/gpu/gl:request_gpu_info"",
        ""//tensorflow/lite/delegates/gpu/gl/converters:bhwc_to_phwc4"",
        ""//tensorflow/lite/delegates/gpu/gl/converters:phwc4_to_bhwc"",
        ""//tensorflow/lite/delegates/gpu/gl/kernels:registry"",
        ""//tensorflow/lite/delegates/gpu/gl/workgroups:best_effort_calculator"",
        ""@com_google_absl//absl/base:core_headers"",
        ""@com_google_absl//absl/types:span"",
    ] + select({
        ""//conditions:default"": [
            ""//tensorflow/lite/delegates/gpu/gl:common_cc_fbs"",
            ""//tensorflow/lite/delegates/gpu/gl:metadata_cc_fbs"",
            ""//tensorflow/lite/delegates/gpu/gl:workgroups_cc_fbs"",
            ""//tensorflow/lite/schema:schema_fbs"",
            ""@flatbuffers"",
        ],
        "":tflite_gpu_binary_release"": [],
    }) + tflite_extra_gles_deps(),
)

objc_library(
    name = ""metal_delegate"",
    srcs = [""metal_delegate.mm""],
    hdrs = [""metal_delegate.h""],
    copts = [""-std=c++17""],
    features = [""-layering_check""],
    module_name = ""TensorFlowLiteCMetal"",
    sdk_frameworks = [""Metal""],
    deps = [
        ""//tensorflow/lite:kernel_api"",
        ""//tensorflow/lite:minimal_logging"",
        ""//tensorflow/lite/core/c:common"",
        ""//tensorflow/lite/delegates/gpu/common:convert"",
        ""//tensorflow/lite/delegates/gpu/common:gpu_info"",
        ""//tensorflow/lite/delegates/gpu/common:model"",
        ""//tensorflow/lite/delegates/gpu/common:model_builder"",
        ""//tensorflow/lite/delegates/gpu/common:model_transformer"",
        ""//tensorflow/lite/delegates/gpu/common:precision"",
        ""//tensorflow/lite/delegates/gpu/common:quantization_util"",
        ""//tensorflow/lite/delegates/gpu/common:shape"",
        ""//tensorflow/lite/delegates/gpu/common:status"",
        ""//tensorflow/lite/delegates/gpu/common:tensor"",
        ""//tensorflow/lite/delegates/gpu/common:types"",
        ""//tensorflow/lite/delegates/gpu/metal:buffer_convert"",
        ""//tensorflow/lite/delegates/gpu/metal:inference_context"",
        ""//tensorflow/lite/delegates/gpu/metal:metal_spatial_tensor"",
        ""@com_google_absl//absl/types:span"",
    ],
)

objc_library(
    name = ""metal_delegate_internal"",
    hdrs = [""metal_delegate_internal.h""],
    copts = [""-std=c++17""],
    sdk_frameworks = [""Metal""],
    deps = [""//tensorflow/lite/delegates/gpu:metal_delegate""],
)

# build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always :libtensorflowlite_gpu_gl.so
cc_binary(
    name = ""libtensorflowlite_gpu_gl.so"",
    linkopts = [
        ""-Wl,-soname=libtensorflowlite_gpu_gl.so"",
    ] + gpu_delegate_linkopts() + select({
        ""//tensorflow:windows"": [],
        ""//conditions:default"": [
            ""-fvisibility=hidden"",
        ],
    }),
    linkshared = 1,
    linkstatic = 1,
    tags = [
        ""nobuilder"",
        ""notap"",
    ],
    deps = ["":gl_delegate""],
)

# build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always :libtensorflowlite_gpu_delegate.so
cc_binary(
    name = ""libtensorflowlite_gpu_delegate.so"",
    linkopts = [
        ""-Wl,-soname=libtensorflowlite_gpu_delegate.so"",
    ] + gpu_delegate_linkopts() + select({
        ""//tensorflow:windows"": [],
        ""//conditions:default"": [
            ""-fvisibility=hidden"",
        ],
    }),
    linkshared = 1,
    linkstatic = 1,
    tags = [
        ""nobuilder"",
        ""notap"",
    ],
    deps = ["":delegate""],
)

# bazel build -c opt --cpu ios_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :libtensorflowlite_gpu_metal --apple_platform_type=ios
ios_static_framework(
    name = ""tensorflow_lite_gpu_framework"",
    hdrs = [
        ""metal_delegate.h"",
        ""metal_delegate_internal.h"",
    ],
    minimum_os_version = ""11.4"",
    deps = ["":metal_delegate""],
)

# Note: Support for MacOS is best-effort at the moment.
# bazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :tensorflow_lite_gpu_dylib --apple_platform_type=macos
macos_dylib(
    name = ""tensorflow_lite_gpu_dylib"",
    linkopts = [
        ""-all_load"",
        ""-dead_strip"",
    ],
    minimum_os_version = ""10.13"",
    tags = [
        ""manual"",
        ""nobuilder"",
        ""notap"",
    ],
    deps = [
        "":metal_delegate"",
        "":metal_delegate_internal"",
    ],
)

cc_library(
    name = ""api"",
    srcs = [""api.cc""],
    hdrs = [""api.h""],
    deps = [
        ""//tensorflow/lite/delegates/gpu/common:data_type"",
        ""//tensorflow/lite/delegates/gpu/common:status"",
        ""//tensorflow/lite/delegates/gpu/common:util"",
        ""//tensorflow/lite/delegates/gpu/gl:portable"",
        ""@com_google_absl//absl/types:span"",
        ""@com_google_absl//absl/types:variant"",
        ""@opencl_headers"",
        ""@vulkan_headers//:vulkan_headers_no_prototypes"",
    ],
)

cc_library(
    name = ""spi"",
    hdrs = [""spi.h""],
    deps = [
        "":api"",
        ""//tensorflow/lite/delegates/gpu/common:access_type"",
        ""//tensorflow/lite/delegates/gpu/common:status"",
    ],
)

# Currently the GPU delegate needs to be built on Android (due to EGL dependency),
# or built with -DCL_DELEGATE_NO_GL (disabling OpenGL backend fallback), or both.
selects.config_setting_group(
    name = ""supports_gpu_delegate"",
    match_any = [
        ""//tensorflow:android"",
        ""//tensorflow/lite/delegates/gpu/cl:opencl_delegate_no_gl"",
    ],
)

cc_library(
    name = ""delegate_options"",
    srcs = [""delegate_options.cc""],
    hdrs = [""delegate_options.h""],
    deps = [""//tensorflow/lite/core/c:common""],
)

cc_library(
    name = ""delegate"",
    srcs = [
        # copybara:comment_begin(oss-only)
        ""android_version.cc"",
        # copybara:comment_end
        ""delegate.cc"",
    ],
    hdrs = [""delegate.h""],
    linkopts = gpu_delegate_linkopts(),
    deps = select({
        ""//tensorflow/lite/delegates/gpu/cl:opencl_delegate_no_gl"": [],
        ""//conditions:default"": [
            ""//tensorflow/lite/delegates/gpu/gl:api2"",
        ],
    }) + [
        "":api"",
        "":delegate_options"",
        "":tflite_profile"",
        ""//tensorflow/lite:kernel_api"",
        ""//tensorflow/lite:minimal_logging"",
        ""//tensorflow/lite/core/async:backend_async_kernel_interface"",
        ""//tensorflow/lite/core/async/c:task"",
        ""//tensorflow/lite/core/async/interop/c:attribute_map"",
        ""//tensorflow/lite/core/async/interop/c:constants"",
        ""//tensorflow/lite/core/c:common"",
        ""//tensorflow/lite/delegates:serialization"",
        ""//tensorflow/lite/delegates/gpu/cl:api"",
        ""//tensorflow/lite/delegates/gpu/cl:util"",
        ""//tensorflow/lite/delegates/gpu/common:model_builder"",
        ""//tensorflow/lite/delegates/gpu/common:model_builder_helper"",
        ""//tensorflow/lite/delegates/gpu/common:quantization_util"",
        ""//tensorflow/lite/delegates/utils"",
        ""//tensorflow/lite/delegates/utils:async_type_helpers"",
        ""//tensorflow/lite/delegates/utils:ret_macros"",
        ""//tensorflow/lite/delegates/utils:sync_fence"",
        ""//tensorflow/lite/kernels:kernel_util"",
        ""//tensorflow/lite/profiling/telemetry"",
        ""//tensorflow/lite/profiling/telemetry:telemetry_status"",
        ""//tensorflow/lite/profiling/telemetry/c:telemetry_setting_internal"",
        ""@com_google_absl//absl/container:flat_hash_map"",
        ""@com_google_absl//absl/container:flat_hash_set"",
        ""@com_google_absl//absl/memory"",
        ""@com_google_absl//absl/types:span"",
    ],
)

cc_library(
    name = ""tflite_profile"",
    srcs = [""tflite_profile.cc""],
    hdrs = [""tflite_profile.h""],
    deps = [
        ""//tensorflow/lite/core/api"",
        ""//tensorflow/lite/delegates/gpu/common/task:profiling_info"",
        ""@com_google_absl//absl/time"",
    ],
)
```

pkgoogle (Assginee) on (2024-08-27 18:15:09 UTC): We actually have a lot of differences, I'm fairly sure the main one is not configuring the Android Workspace though, you should say yes to that.

(All the `~` are actually the absolute paths, which you should use)
Here are my answers to the configuration:
```sh
./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is ~/miniforge3/envs/72281/bin/python3]: 


Found possible Python library paths:
  ~/miniforge3/envs/72281/lib/python3.12/site-packages
Please input the desired Python library path to use.  Default is [~/miniforge3/envs/72281/lib/python3.12/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:  #Just used default


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

Please specify the home path of the Android NDK to use. [Default is ~/library/Android/Sdk/ndk-bundle]:  ~/Library/Android/sdk/ndk/25.2.9519653 #This should be the location if you're using Android Studio, which you should be

Please specify the (min) Android NDK API level to use. [Available levels: [16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33]] [Default is 21]: 


Please specify the home path of the Android SDK to use. [Default is ~/library/Android/Sdk]: 


Please specify the Android SDK API level to use. [Available levels: ['29', '30', '32', '33', '34']] [Default is 34]: 


Please specify an Android build tools version to use. [Available versions: ['30.0.3', '33.0.1', '34.0.0']] [Default is 34.0.0]: 


Do you wish to build TensorFlow with iOS support? [y/N]: N
No iOS support will be enabled for TensorFlow.
```

Please also be sure to align your versions with this: https://www.tensorflow.org/install/source#macos, Thanks.

poltomo (Issue Creator) on (2024-08-27 23:04:27 UTC): @pkgoogle
I can't even find an android ndk version for arm64
```
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /opt/miniconda3/bin/python3]: 


Found possible Python library paths:
  /opt/miniconda3/lib/python3.11/site-packages
Please input the desired Python library path to use.  Default is [/opt/miniconda3/lib/python3.11/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:       


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: eifjcbfrguhvcdurfrhbejvkvjhlghcfelvrcklfgvbc
Invalid selection: eifjcbfrguhvcdurfrhbejvkvjhlghcfelvrcklfgvbc
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

WARNING: The NDK version in /Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125 is 26, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.

Traceback (most recent call last):
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1466, in <module>
    main()
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1439, in main
    create_android_ndk_rule(environ_cp)
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 658, in create_android_ndk_rule
    get_ndk_api_level(environ_cp, android_ndk_home_path))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 752, in get_ndk_api_level
    api_levels = sorted(os.listdir(platforms))
                        ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125/platforms'

```

pkgoogle (Assginee) on (2024-08-28 17:47:23 UTC): Hi @poltomo, do you have Android studio installed? If so, please try 25c for NDK first -- then as noted in the above message, try the latest NDK=21 version. You are using 26 here. If you are having trouble installing the correct version, please let me know.

poltomo (Issue Creator) on (2024-08-29 07:54:08 UTC): I do have android studio installed.
I am getting this error even though I have the android ndk

ls /Users/varunnaw/Library/Android/sdk/ndk 
```
21.4.7075529	26.1.10909125	27.0.11902837
```

Here's the error in ./configure
I think its looking for a platforms folder
```
(fc1) varunnaw@80a9970d555c tensorflow_src % ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /opt/miniconda3/envs/fc1/bin/python3]: 


Found possible Python library paths:
  /opt/miniconda3/envs/fc1/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/opt/miniconda3/envs/fc1/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

WARNING: The NDK version in /Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125 is 26, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.

Traceback (most recent call last):
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1466, in <module>
    main()
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 1439, in main
    create_android_ndk_rule(environ_cp)
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 658, in create_android_ndk_rule
    get_ndk_api_level(environ_cp, android_ndk_home_path))
  File ""/Users/varunnaw/FastConvolution/src/FastConvolution/tensorflow_src/./configure.py"", line 752, in get_ndk_api_level
    api_levels = sorted(os.listdir(platforms))
FileNotFoundError: [Errno 2] No such file or directory: '/Users/varunnaw/Library/Android/sdk/ndk/26.1.10909125/platforms'
```


Maybe there is something that you have setup that I don't have setup. Or maybe I have set it up incorrectly.
Is there some docker container that could streamline this? I am cross compiling anyways so build host could be whatever.

My goal is to write my own delegate for tensorflow lite to use alongside the gpu delegate. Both will target the gpu, but I wanted my delegate to hold some opencl kernels I made.

If tensorflow lite also has an opencl delegate, I also want to build that too.

Here's a post of mine
https://discuss.ai.google.dev/t/how-to-create-a-new-operator-with-an-opencl-kernel/37073

poltomo (Issue Creator) on (2024-08-29 07:55:31 UTC): find /Users/varunnaw/Library/Android/sdk/ndk/ -name platforms

turns up nothing

pkgoogle (Assginee) on (2024-08-29 20:52:23 UTC): Hi @poltomo, yeah the NDK version sometimes have file/directory changes thats part of why we have stringent NDK requirements, if you go to tools -> SDK manager, switch to SDK Tools, and scroll down to ""NDK (Side by side)""... you should find multiple versions to install:

![image](https://github.com/user-attachments/assets/4d8783bd-3330-465c-883e-f8d51be1fbd6)

Latest bazel only supports NDK = 19, 20, 21 and 25.

If you have no space constraints.. I recommend go ahead and installing all versions of 19, 20, 21 and 25. I would update your bazel (perhaps install and use bazelisk so this is automatic for you?) and then try the same thing against 25b (25.1.8....) as that is the current recommended version. Please review this https://www.tensorflow.org/lite/android/lite_build#install_bazel_and_android_prerequisites . I tried against 25c (25.2.9....) and that worked but if you go w/ the recommended version you will run into less issues as that is well traveled.

github-actions[bot] on (2024-09-06 01:57:53 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-13 01:58:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-13 01:58:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72281"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72281"">No</a>

"
2422286437,issue,closed,completed,[Android]Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:63 op_context->perm->dims->data[0] != dims (3 != 2),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.7, tf-lite 2.16.1, tensorflow-lite-select-tf-ops:2.16.1

### Custom code

Yes

### OS platform and distribution

Linux 20.04

### Mobile device

Android SDK 28

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My .tflite model works on python but it dosen't work well on android project.

It seems no difference between python and android. 
What kinds of layer or function changes demention of input?


### Standalone code to reproduce the issue

```shell
model
 python
model = Sequential()
model.add(Input([256], dtype=""int32""))
model.add(Embedding(35000, 10))
model.add(GRU(10))
# model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax', ))
model.compile(loss='binary_crossentropy', optimizer=""adam"", metrics=['accuracy'])
print(model.summary())
```
Model: ""sequential_43""

 Layer (type)                         Output Shape                 Param # 

 embedding_35 (Embedding) (None, 256, 10)                  350,000 

 gru_33 (GRU)                          (None, 10)                                 660 

 dense_27 (Dense)                   (None, 2)                                     22 

 Total params: 350,682 (1.34 MB)
 Trainable params: 350,682 (1.34 MB)
 Non-trainable params: 0 (0.00 B)

**error occur**
``` java
private AnalysisResult analyzeTextTFLite(String text) {
        // convert text to custom ids
        Feature feature = featureConverter.convert(text, ADD_SPECIAL_TOKENS);
        int curSeqLen = feature.inputIds.length;

        int[] tfInputs = new int[256];
        for (int j = 0; j < curSeqLen; j++) {
            tfInputs[j] = feature.inputIds[j];
        }

        Map<String, Object> inputsMap = new HashMap<>();
        Map<String, Object> outputMap = new HashMap<>();

        inputsMap.put(""keras_tensor_198"", tfInputs);

        float[][] logits = new float[1][2];
        outputMap.put(""output_0"", logits);

        final long moduleForwardStartTime = SystemClock.elapsedRealtime();
        tflite.runSignature(inputsMap, outputMap);
        Log.d(TAG, ""Model inference score : "" + logits[0][0] + "","" + logits[0][1]);

        float[] scores = new float[2];
        scores[0] = argmax(logits[0]);

        final long moduleForwardDuration = SystemClock.elapsedRealtime() - moduleForwardStartTime;
        return new AnalysisResult(scores, moduleForwardDuration);
    }
```

**works well**
``` python 
input_details = interpreter.get_input_details()
print(input_details)
""""""
[{'name': 'serving_default_keras_tensor_198:0',
  'index': 0,
  'shape': array([  1, 256]),
  'shape_signature': array([ -1, 256]),
  'dtype': numpy.int32,
  'quantization': (0.0, 0),
  'quantization_parameters': {'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32),
   'quantized_dimension': 0},
  'sparsity_parameters': {}}]
output_details = interpreter.get_output_details()
print(output_details)
""""""

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
""""""
[{'name': 'StatefulPartitionedCall_1:0',
  'index': 36,
  'shape': array([1, 2]),
  'shape_signature': array([-1,  2]),
  'dtype': numpy.float32,
  'quantization': (0.0, 0),
  'quantization_parameters': {'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32),
   'quantized_dimension': 0},
  'sparsity_parameters': {}}]
""""""

interpreter.set_tensor(input_details[0]['index'], encode_plus_inputs[""input_ids""])
interpreter.invoke()
```


### Relevant log output

```shell
FATAL EXCEPTION: ModuleActivity
                                                                                                    Process: org.pytorch.demo, PID: 32712
                                                                                                    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:63 op_context->perm->dims->data[0] != dims (3 != 2)
                                                                                                    Node number 10 (TRANSPOSE) failed to prepare.
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:264)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.runSignature(NativeInterpreterWrapper.java:194)
                                                                                                    	at org.tensorflow.lite.Interpreter.runSignature(Interpreter.java:271)
                                                                                                    	at org.tensorflow.lite.Interpreter.runSignature(Interpreter.java:284)
                                                                                                    	at org.pytorch.demo.nlp.NSMCPytorchActivity.analyzeTextTFLite(NSMCPytorchActivity.java:281)
                                                                                                    	at org.pytorch.demo.nlp.NSMCPytorchActivity.analyzeText(NSMCPytorchActivity.java:201)
                                                                                                    	at org.pytorch.demo.nlp.NSMCPytorchActivity.lambda$new$2$org-pytorch-demo-nlp-NSMCPytorchActivity(NSMCPytorchActivity.java:154)
                                                                                                    	at org.pytorch.demo.nlp.NSMCPytorchActivity$$ExternalSyntheticLambda2.run(Unknown Source:4)
                                                                                                    	at android.os.Handler.handleCallback(Handler.java:958)
                                                                                                    	at android.os.Handler.dispatchMessage(Handler.java:99)
                                                                                                    	at android.os.Looper.loopOnce(Looper.java:230)
                                                                                                    	at android.os.Looper.loop(Looper.java:319)
                                                                                                    	at android.os.HandlerThread.run(HandlerThread.java:67)
```
",siy415,2024-07-22 08:37:24+00:00,"['gaikwadrahul8', 'sawantkumar']",2024-09-20 02:00:13+00:00,2024-09-20 02:00:10+00:00,https://github.com/tensorflow/tensorflow/issues/72279,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2332085874, 'issue_id': 2422286437, 'author': 'gaikwadrahul8', 'body': 'Hi, @siy415 \r\n\r\nI apologize for the delayed response and if possible could you please help us with Google colab notebook in which you converted your model to TensorFlow Lite format and your github repo of android project which will help us to replicate the same behavior from our end to investigate this issue further from our end ? \r\n\r\nThank you for your cooperation and patience.', 'created_at': datetime.datetime(2024, 9, 5, 15, 48, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347894311, 'issue_id': 2422286437, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 13, 1, 58, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362558016, 'issue_id': 2422286437, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 20, 2, 0, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362558057, 'issue_id': 2422286437, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72279"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72279"">No</a>', 'created_at': datetime.datetime(2024, 9, 20, 2, 0, 12, tzinfo=datetime.timezone.utc)}]","gaikwadrahul8 (Assginee) on (2024-09-05 15:48:08 UTC): Hi, @siy415 

I apologize for the delayed response and if possible could you please help us with Google colab notebook in which you converted your model to TensorFlow Lite format and your github repo of android project which will help us to replicate the same behavior from our end to investigate this issue further from our end ? 

Thank you for your cooperation and patience.

github-actions[bot] on (2024-09-13 01:58:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-20 02:00:10 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-20 02:00:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72279"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72279"">No</a>

"
2421883878,issue,closed,completed,ValueError: Tensor data is null. Run allocate_tensors() first,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX4050 16Gb

### Current behavior?

I converted my keras model to tflite with quantization. 

When I inference the quantizated model, it has a error ""ValueError: Tensor data is null. Run allocate_tensors() first""
But there was no error when I inference it on not quantized model. 

What's difference with quantizated and not quntizated model..?

How do I fix it?

### Standalone code to reproduce the issue

```shell
model = Sequential()
model.add(Input([256], dtype=""int32""))
model.add(Embedding(35000, 10))
model.add(GRU(10,))
model.add(Dense(2, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer=""adam"", metrics=['accuracy'])

# Train
model.fit(train_input, labels, epochs=10, batch_size=128,)

# Convert
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_enable_resource_variables = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
# converter._experimental_lower_tensor_list_ops = False
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Load interpreter
interpreter = tf.lite.Interpreter(model_path=""./gru_0722_tflite/model_2.tflite"")
interpreter.allocate_tensors()

# inference
interpreter.set_tensor(input_details[0]['index'], encode_plus_inputs[""input_ids""])
interpreter.invoke()

# get result <---- Error occured 
output_data = interpreter.get_tensor(output_details[0]['index'])
output_data
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[202], line 1
----> 1 output_data = interpreter.get_tensor(output_details[0]['index'])
      2 output_data

File ~\.virtualenvs\App1G-B5e0KeSX\Lib\site-packages\tensorflow\lite\python\interpreter.py:888, in Interpreter.get_tensor(self, tensor_index, subgraph_index)
    873 def get_tensor(self, tensor_index, subgraph_index=0):
    874   """"""Gets the value of the output tensor (get a copy).
    875 
    876   If you wish to avoid the copy, use `tensor()`. This function cannot be used
   (...)
    886     a numpy array.
    887   """"""
--> 888   return self._interpreter.GetTensor(tensor_index, subgraph_index)

ValueError: Tensor data is null. Run allocate_tensors() first
```
",siy415,2024-07-22 04:21:24+00:00,['Venkat6871'],2024-09-16 05:52:29+00:00,2024-08-13 01:55:27+00:00,https://github.com/tensorflow/tensorflow/issues/72272,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2255995536, 'issue_id': 2421883878, 'author': 'sawantkumar', 'body': 'Hi @siy415 ,\r\n\r\nIs it possible that you can share a colab of your code? Because i ran your code using the tf 2.17.0 on colab TPU  using dummy training and inference data and it ran fine. Here is the [link](https://colab.research.google.com/drive/1w_BQKq-NqovlsYFvkePIDrrQNxe_gxmk?usp=sharing)', 'created_at': datetime.datetime(2024, 7, 29, 13, 46, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213384, 'issue_id': 2421883878, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196347, 'issue_id': 2421883878, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196424, 'issue_id': 2421883878, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72272"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72272"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 31, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-07-29 13:46:44 UTC): Hi @siy415 ,

Is it possible that you can share a colab of your code? Because i ran your code using the tf 2.17.0 on colab TPU  using dummy training and inference data and it ran fine. Here is the [link](https://colab.research.google.com/drive/1w_BQKq-NqovlsYFvkePIDrrQNxe_gxmk?usp=sharing)

github-actions[bot] on (2024-08-06 01:53:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-13 01:55:27 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-13 01:55:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72272"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72272"">No</a>

"
2421499740,issue,closed,completed,Why I can't install tensorflow-text2.13.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.8

### Custom code

No

### OS platform and distribution

windows11

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I intall tensorflow-text by pip install tensorflow-text==2.13.0, it's failed:
ERROR: Could not find a version that satisfies the requirement tensorflow-text==2.13.0 (from versions: 2.5.0, 2.6.0rc0, 2.6.0, 2.7.0rc0, 2.7.0rc1, 2.7.3, 2.8.0rc0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0, 2.10.0b2, 2.10.0rc0, 2.10.0)
ERROR: No matching distribution found for tensorflow-text==2.13.0
But In page https://pypi.org/project/tensorflow-text/2.13.0/, 2.13.0 is indeed exits, why I can't install it?

### Standalone code to reproduce the issue

```shell
pip install tensorflow-text==2.13.0
```


### Relevant log output

_No response_",libofei2004,2024-07-21 15:56:25+00:00,['tilakrayal'],2024-10-03 02:01:47+00:00,2024-10-03 02:01:45+00:00,https://github.com/tensorflow/tensorflow/issues/72259,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.8', '')]","[{'comment_id': 2242857246, 'issue_id': 2421499740, 'author': 'tilakrayal', 'body': '@libofei2004,\r\nAfter version 2.10, we will only be providing pip packages for Linux x86_64 and Intel-based Macs. For other systems like Windows, Aarch64, and Apple Macs, TensorFlow relies on [build collaborators](https://blog.tensorflow.org/2022/09/announcing-tensorflow-official-build-collaborators.html), and so we will not be providing packages for them. However, we will continue to accept PRs to make building for these OSs easy for users, and will try to point to community efforts related to them.\r\n\r\nhttps://github.com/tensorflow/text/blob/master/README.md#a-note-about-different-operating-system-packages\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/67465\r\n\r\nAlso this issue is related to tensorflow-text, so could you please raise the concern on the respected [repo](https://github.com/tensorflow/text/issues/) or the contribute with the PR for the quick resolution. \r\n\r\nPlease try to install using WSL/WSL2. Thank you!', 'created_at': datetime.datetime(2024, 7, 22, 12, 38, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242989779, 'issue_id': 2421499740, 'author': 'mihaimaruseac', 'body': '@libofei2004 what is your Python version? According to https://pypi.org/project/tensorflow-text/2.13.0/#files, you can only use Python 3.8, 3.9 and 3.10 for this.\r\n\r\nAlso, what operating system? Same page only shows files for Linux and MacOs, both on x86_64.', 'created_at': datetime.datetime(2024, 7, 22, 13, 39, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243392221, 'issue_id': 2421499740, 'author': 'libofei2004', 'body': ""@tilakrayal I find I can't install  mediapipe_model_maker version >0.1.0.1 in windows. I think the reason is some lib like tensorflow-texttf-models-official do not have a greater windows version. Am I right? Can mediapipe_model_maker be installed with a version greater than 0.1.0.1 on Windows"", 'created_at': datetime.datetime(2024, 7, 22, 16, 45, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243409752, 'issue_id': 2421499740, 'author': 'libofei2004', 'body': ""@mihaimaruseac my Python  version is 3.9.13. My operating system is windows 10. I can't install mediapipe_model_maker 0.2.1.4 in windows , I think the reason is some libs like  tensorflow-texttf-models-official do not have a greater windows version."", 'created_at': datetime.datetime(2024, 7, 22, 16, 56, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243452147, 'issue_id': 2421499740, 'author': 'mihaimaruseac', 'body': ""Yes, there is no support for Windows on TF-text. So you'd have to downgrade to an earlier version, or try to use WSL (on Windows) or a cloud/Colab host (Linux)."", 'created_at': datetime.datetime(2024, 7, 22, 17, 21, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375600916, 'issue_id': 2421499740, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 26, 2, 1, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390350176, 'issue_id': 2421499740, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390350221, 'issue_id': 2421499740, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72259"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72259"">No</a>', 'created_at': datetime.datetime(2024, 10, 3, 2, 1, 46, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-22 12:38:29 UTC): @libofei2004,
After version 2.10, we will only be providing pip packages for Linux x86_64 and Intel-based Macs. For other systems like Windows, Aarch64, and Apple Macs, TensorFlow relies on [build collaborators](https://blog.tensorflow.org/2022/09/announcing-tensorflow-official-build-collaborators.html), and so we will not be providing packages for them. However, we will continue to accept PRs to make building for these OSs easy for users, and will try to point to community efforts related to them.

https://github.com/tensorflow/text/blob/master/README.md#a-note-about-different-operating-system-packages

https://github.com/tensorflow/tensorflow/issues/67465

Also this issue is related to tensorflow-text, so could you please raise the concern on the respected [repo](https://github.com/tensorflow/text/issues/) or the contribute with the PR for the quick resolution. 

Please try to install using WSL/WSL2. Thank you!

mihaimaruseac on (2024-07-22 13:39:29 UTC): @libofei2004 what is your Python version? According to https://pypi.org/project/tensorflow-text/2.13.0/#files, you can only use Python 3.8, 3.9 and 3.10 for this.

Also, what operating system? Same page only shows files for Linux and MacOs, both on x86_64.

libofei2004 (Issue Creator) on (2024-07-22 16:45:59 UTC): @tilakrayal I find I can't install  mediapipe_model_maker version >0.1.0.1 in windows. I think the reason is some lib like tensorflow-texttf-models-official do not have a greater windows version. Am I right? Can mediapipe_model_maker be installed with a version greater than 0.1.0.1 on Windows

libofei2004 (Issue Creator) on (2024-07-22 16:56:25 UTC): @mihaimaruseac my Python  version is 3.9.13. My operating system is windows 10. I can't install mediapipe_model_maker 0.2.1.4 in windows , I think the reason is some libs like  tensorflow-texttf-models-official do not have a greater windows version.

mihaimaruseac on (2024-07-22 17:21:31 UTC): Yes, there is no support for Windows on TF-text. So you'd have to downgrade to an earlier version, or try to use WSL (on Windows) or a cloud/Colab host (Linux).

github-actions[bot] on (2024-09-26 02:01:23 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-03 02:01:44 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-03 02:01:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72259"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72259"">No</a>

"
2421418786,issue,closed,completed,Tensor flow issue,"import cv2
import time
import numpy as np
import HandTrackingModule as htm
import math
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume

wCam, hCam = 640, 480

cap = cv2.VideoCapture(0)
cap.set(3, wCam)
cap.set(4, hCam)

detector = htm.HandDetector(detectionCon=0.7)

devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(
    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))
volRange = volume.GetVolumeRange()
minVol = volRange[0]
maxVol = volRange[1]

while True:
    success, img = cap.read()
    img = detector.findHands(img)
    lmList = detector.findPosition(img, draw=False)
    if len(lmList) != 0:
        x1, y1 = lmList[4][1], lmList[4][2]
        x2, y2 = lmList[8][1], lmList[8][2]
        length = math.hypot(x2 - x1, y2 - y1)
        vol = np.interp(length, [50, 300], [minVol, maxVol])
        volBar = np.interp(length, [50, 300], [400, 150])
        volPer = np.interp(length, [50, 300], [0, 100])
        print(int(length), vol)
        volume.SetMasterVolumeLevel(vol, None)
        if length < 50:
            cv2.circle(img, (cx, cy), 15, (0, 255, 0), cv2.FILLED)
    
    cv2.imshow(""Img"", img)
    cv2.waitKey(1)


error is:
Python 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license()"" for more information.

================= RESTART: C:/Users/nagad/Desktop/import PIL.py ================
Traceback (most recent call last):
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/nagad/Desktop/import PIL.py"", line 4, in <module>
    import HandTrackingModule as htm
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\HandTrackingModule\__init__.py"", line 2, in <module>
    import mediapipe as mp
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\__init__.py"", line 17, in <module>
    import mediapipe.tasks.python as tasks
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\tasks\python\__init__.py"", line 17, in <module>
    from . import audio
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\tasks\python\audio\__init__.py"", line 18, in <module>
    import mediapipe.tasks.python.audio.audio_classifier
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\tasks\python\audio\audio_classifier.py"", line 26, in <module>
    from mediapipe.tasks.python.audio.core import base_audio_task_api
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\tasks\python\audio\core\base_audio_task_api.py"", line 25, in <module>
    from mediapipe.tasks.python.core.optional_dependencies import doc_controls
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\mediapipe\tasks\python\core\optional_dependencies.py"", line 20, in <module>
    from tensorflow.tools.docs import doc_controls
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\nagad\AppData\Roaming\Python\Python312\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
",Deepu777yt,2024-07-21 14:00:47+00:00,['Venkat6871'],2024-07-21 14:12:51+00:00,2024-07-21 14:12:43+00:00,https://github.com/tensorflow/tensorflow/issues/72257,[],"[{'comment_id': 2241625605, 'issue_id': 2421418786, 'author': 'mihaimaruseac', 'body': 'Duplicate of #72120', 'created_at': datetime.datetime(2024, 7, 21, 14, 12, 43, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-21 14:12:43 UTC): Duplicate of #72120

"
2421385329,issue,closed,completed,What CUDA Recommended Version for TensorFlow 2.17.0 ?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0 

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

12.2 / 8.9.6

### GPU model and memory

GTX1650

### Current behavior?

I am currently setting up TensorFlow 2.17.0 and want to ensure compatibility with CUDA. Could you please provide the recommended CUDA version for TensorFlow 2.17.0?


### Standalone code to reproduce the issue

```shell
Recommended CUDA version for TensorFlow 2.17.0?
```


### Relevant log output

_No response_",Made-Jaya,2024-07-21 12:40:05+00:00,['tilakrayal'],2024-07-23 08:26:40+00:00,2024-07-22 08:50:05+00:00,https://github.com/tensorflow/tensorflow/issues/72255,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2241920245, 'issue_id': 2421385329, 'author': '372046933', 'body': 'Possibly `>=12.3`\r\nhttps://hub.docker.com/layers/tensorflow/build/2.17-python3.11/images/sha256-70a282804c02af830f4f46ddb0d69cb852d168a9e7bba68332a30e809153ec42?context=explore\r\n\r\n```\r\nENV NVIDIA_REQUIRE_CUDA=cuda>=12.3 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\n```', 'created_at': datetime.datetime(2024, 7, 22, 2, 20, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242425507, 'issue_id': 2421385329, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72255"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72255"">No</a>', 'created_at': datetime.datetime(2024, 7, 22, 8, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242482055, 'issue_id': 2421385329, 'author': 'tilakrayal', 'body': '@Made-Jaya,\r\nCould you please take a look at this official document for the CUDA details which are compatible for latest tensorflow v2.17.0\r\nhttps://github.com/tensorflow/tensorflow/pull/71345/files\r\n\r\n```python\r\n# Set up extra packages, which are optional sets of other Python package deps.\r\n# E.g. ""pip install tensorflow[and-cuda]"" below installs the normal TF deps\r\n# plus the CUDA libraries listed.\r\nEXTRA_PACKAGES = {}\r\nEXTRA_PACKAGES[\'and-cuda\'] = [\r\n    # TODO(nluehr): set nvidia-* versions based on build components.\r\n    \'nvidia-cublas-cu12 == 12.3.4.1\',\r\n    \'nvidia-cuda-cupti-cu12 == 12.3.101\',\r\n    \'nvidia-cuda-nvcc-cu12 == 12.3.107\',\r\n    \'nvidia-cuda-nvrtc-cu12 == 12.3.107\',\r\n    \'nvidia-cuda-runtime-cu12 == 12.3.101\',\r\n```\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 22, 9, 17, 16, tzinfo=datetime.timezone.utc)}]","372046933 on (2024-07-22 02:20:24 UTC): Possibly `>=12.3`
https://hub.docker.com/layers/tensorflow/build/2.17-python3.11/images/sha256-70a282804c02af830f4f46ddb0d69cb852d168a9e7bba68332a30e809153ec42?context=explore

```
ENV NVIDIA_REQUIRE_CUDA=cuda>=12.3 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536
```

google-ml-butler[bot] on (2024-07-22 08:50:07 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72255"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72255"">No</a>

tilakrayal (Assginee) on (2024-07-22 09:17:16 UTC): @Made-Jaya,
Could you please take a look at this official document for the CUDA details which are compatible for latest tensorflow v2.17.0
https://github.com/tensorflow/tensorflow/pull/71345/files

```python
# Set up extra packages, which are optional sets of other Python package deps.
# E.g. ""pip install tensorflow[and-cuda]"" below installs the normal TF deps
# plus the CUDA libraries listed.
EXTRA_PACKAGES = {}
EXTRA_PACKAGES['and-cuda'] = [
    # TODO(nluehr): set nvidia-* versions based on build components.
    'nvidia-cublas-cu12 == 12.3.4.1',
    'nvidia-cuda-cupti-cu12 == 12.3.101',
    'nvidia-cuda-nvcc-cu12 == 12.3.107',
    'nvidia-cuda-nvrtc-cu12 == 12.3.107',
    'nvidia-cuda-runtime-cu12 == 12.3.101',
```

Thank you!

"
2421156319,issue,closed,completed,Spam removed,Spam removed,3cktorcrypto,2024-07-21 01:10:43+00:00,['Venkat6871'],2024-07-21 01:56:07+00:00,2024-07-21 01:56:07+00:00,https://github.com/tensorflow/tensorflow/issues/72237,"[('comp:lite', 'TF Lite related issues'), ('invalid', 'Hacktoberfest spam PR')]",[],
2421066059,issue,closed,not_planned,TF2 Fast Style Transfer for Arbitrary Styles running problems,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Win 10

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I try to run the Fast Style Transfer for Arbitrary Styles [https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization) on Windows 10, Python 3.12.4, TensorFlow 2.16.1.

When the copied original code was run (Style_transfer.py in this case), the following error appeared for the lines

> 79: hub_handle = 'https://kaggle.com/models/google/arbitrary-image-stylization-v1/frameworks/TensorFlow1/variations/256/versions/1'
> 
> 80: hub_module = hub.load(hub_handle)

```
Traceback (most recent call last):
  File ""C:\..\NN\Style_transfer.py"", line 80, in <module>
    hub_module = hub.load(hub_handle)
                 ^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\module_v2.py"", line 100, in load
    module_path = resolve(handle)
                  ^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\module_v2.py"", line 55, in resolve
    return registry.resolver(handle)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\registry.py"", line 49, in __call__
    return impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\compressed_module_resolver.py"", line 81, in __call__
    return resolver.atomic_download(handle, download, module_dir,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\resolver.py"", line 433, in atomic_download
    tf.compat.v1.gfile.Rename(tmp_dir, module_dir)
  File ""C:\..\Lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 606, in rename
    rename_v2(oldname, newname, overwrite)
  File ""C:\..\Lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 622, in rename_v2
    _pywrap_file_io.RenameFile(
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\Users\..\AppData\Local\Temp\tfhub_modules\887f65c3a674ba9fc31eb2b5144ccf934c65c413.fbff8afd31584e7aa3f471b6e199deb9.tmp to: C:\Users\..\AppData\Local\Temp\tfhub_modules\887f65c3a674ba9fc31eb2b5144ccf934c65c413 : Access is denied.
; Input/output error
```

What to do with `Access is denied.` I did not find. Probably, the solution is simple, but it is hidden in a ton of issues.

Next, I tried to download the model with 
> os.environ[""TFHUB_MODEL_LOAD_FORMAT""] = ""UNCOMPRESSED""

The error was

```
Traceback (most recent call last):
  File ""C:\..\NN\Style_transfer.py"", line 80, in <module>
    hub_module = hub.load(hub_handle)
                 ^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\module_v2.py"", line 100, in load
    module_path = resolve(handle)
                  ^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\module_v2.py"", line 55, in resolve
    return registry.resolver(handle)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\Lib\site-packages\tensorflow_hub\registry.py"", line 49, in __call__
    return impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\site-packages\tensorflow_hub\uncompressed_module_resolver.py"", line 34, in __call__
    return self.path_resolver(gcs_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\site-packages\tensorflow_hub\resolver.py"", line 498, in __call__
    if not tf.compat.v1.gfile.Exists(handle):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\..\site-packages\tensorflow\python\lib\io\file_io.py"", line 298, in file_exists
    return file_exists_v2(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""..\site-packages\tensorflow\python\lib\io\file_io.py"", line 290, in file_exists_v2
    _pywrap_file_io.FileExists(compat.path_to_bytes(path))
tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://kaggle-tfhub-models-uncompressed/tfhub-modules/google/magenta/arbitrary-image-stylization-v1-256/1/uncompressed')
```

There are a lot of posts with the `'gs' not implemented` issue, mainly from 2017-2018, but nothing solved this. I have no idea how to proceed.

What is needed to run [this example](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization)?

Thank you.

### Standalone code to reproduce the issue

```shell
79: hub_handle = 'https://kaggle.com/models/google/arbitrary-image-stylization-v1/frameworks/TensorFlow1/variations/256/versions/1'

80: hub_module = hub.load(hub_handle)
```


### Relevant log output

_No response_",VicB18,2024-07-20 21:42:13+00:00,['tilakrayal'],2024-07-22 06:44:53+00:00,2024-07-22 06:44:50+00:00,https://github.com/tensorflow/tensorflow/issues/72235,"[('type:support', 'Support issues')]","[{'comment_id': 2242207785, 'issue_id': 2421066059, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72235"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72235"">No</a>', 'created_at': datetime.datetime(2024, 7, 22, 6, 44, 52, tzinfo=datetime.timezone.utc)}]","google-ml-butler[bot] on (2024-07-22 06:44:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72235"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72235"">No</a>

"
2419613393,issue,closed,completed,Mmap of '41' at offset '0' failed with error '22'. on movenet,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=48kW1c2F5l1R

I amy trying to run this repo with movenet but I get error.
Mmap of '41' at offset '0' failed with error '22'. on movenet load function. No change has been made. I am running this on colab directly please help

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=48kW1c2F5l1R

I amy trying to run this repo with movenet but I get error.
Mmap of '41' at offset '0' failed with error '22'. on movenet load function. No change has been made. I am running this on colab directly please help
```


### Relevant log output

_No response_",jaskarannagi19,2024-07-19 18:42:23+00:00,['tilakrayal'],2024-07-23 16:27:55+00:00,2024-07-23 16:18:07+00:00,https://github.com/tensorflow/tensorflow/issues/72202,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2243528350, 'issue_id': 2419613393, 'author': 'Mejorarsim', 'body': 'Yes, getting same issue. Can you resolve please?', 'created_at': datetime.datetime(2024, 7, 22, 18, 7, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244335474, 'issue_id': 2419613393, 'author': 'tilakrayal', 'body': '@jaskarannagi19,\r\nI tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.\r\nhttps://github.com/tensorflow/tensorflow/issues/70841\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 23, 6, 14, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244489148, 'issue_id': 2419613393, 'author': 'jaskarannagi19', 'body': '@tilakrayal No I think the code you are refering to has a different tf load method. I am more interested in training that notebook on my own examples. I think there is a problem in model that is not compatible', 'created_at': datetime.datetime(2024, 7, 23, 7, 43, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245024521, 'issue_id': 2419613393, 'author': 'durgas4', 'body': ""I got the same error. I replaced the code - \r\n![image](https://github.com/user-attachments/assets/67c488df-6272-44d6-808f-01d3b80d2252)\r\n\r\nI highlighted the changes. The model i donwloaded from the github link-https://github.com/devfemibadmus/human-pose-estimation\r\n\r\nor even download tflite models from the kaggle link - https://www.kaggle.com/models/google/movenet/tfLite/singlepose-thunder/1?tfhub-redirect=true\r\n\r\nreplace the model movenet = Movenet('downloaded model.tflite') with the one downloaded and the code should run fine!\r\n\r\n\r\n # Load MoveNet Thunder model\r\nimport utils\r\nfrom data import BodyPart\r\nfrom ml import Movenet\r\nmovenet = Movenet('downloaded model.tflite')"", 'created_at': datetime.datetime(2024, 7, 23, 11, 51, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245675127, 'issue_id': 2419613393, 'author': 'jaskarannagi19', 'body': ""@durgas4 I think that's what we did followed by change in str() to str()_ in movenetpreprocessor class method to support numpy 1.25.2. \r\nThat notebook does need a new fresh link for the model out of the box link does not work"", 'created_at': datetime.datetime(2024, 7, 23, 16, 18, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245675202, 'issue_id': 2419613393, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72202"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72202"">No</a>', 'created_at': datetime.datetime(2024, 7, 23, 16, 18, 10, tzinfo=datetime.timezone.utc)}]","Mejorarsim on (2024-07-22 18:07:27 UTC): Yes, getting same issue. Can you resolve please?

tilakrayal (Assginee) on (2024-07-23 06:14:39 UTC): @jaskarannagi19,
I tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ca94c8afd638c92d4d0e2709ac2cb345/copy-of-movenet.ipynb) and also please have a look at the issue for the reference.
https://github.com/tensorflow/tensorflow/issues/70841

Thank you!

jaskarannagi19 (Issue Creator) on (2024-07-23 07:43:52 UTC): @tilakrayal No I think the code you are refering to has a different tf load method. I am more interested in training that notebook on my own examples. I think there is a problem in model that is not compatible

durgas4 on (2024-07-23 11:51:15 UTC): I got the same error. I replaced the code - 
![image](https://github.com/user-attachments/assets/67c488df-6272-44d6-808f-01d3b80d2252)

I highlighted the changes. The model i donwloaded from the github link-https://github.com/devfemibadmus/human-pose-estimation

or even download tflite models from the kaggle link - https://www.kaggle.com/models/google/movenet/tfLite/singlepose-thunder/1?tfhub-redirect=true

replace the model movenet = Movenet('downloaded model.tflite') with the one downloaded and the code should run fine!


 # Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('downloaded model.tflite')

jaskarannagi19 (Issue Creator) on (2024-07-23 16:18:08 UTC): @durgas4 I think that's what we did followed by change in str() to str()_ in movenetpreprocessor class method to support numpy 1.25.2. 
That notebook does need a new fresh link for the model out of the box link does not work

google-ml-butler[bot] on (2024-07-23 16:18:10 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72202"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72202"">No</a>

"
2419544044,issue,closed,completed,how to install mediapipe_model_maker0.2.1.4 in windows?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.8

### Custom code

No

### OS platform and distribution

windows11

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I run ""pip install mediapipe_model_maker""  , but I can only install version 0.1.0.1. I was trying to install mediapipe_model_maker by ""pip install mediapipe_model_maker-0.2.1.4-py3-none-any.whl"", but also failed. The error:
ERROR: Cannot install mediapipe-model-maker and mediapipe-model-maker==0.2.1.4 because these package versions have conflicting dependencies.

The conflict is caused by:
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.15.0 depends on tensorflow-text~=2.15.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.2 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.1 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.0 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.13.2 depends on tensorflow-text~=2.13.0I run ""pip install mediapipe_model_maker""  ,but I can only install version 0.1.0.1. I was trying to install mediapipe_model_maker by ""pip install mediapipe_model_maker-0.2.1.4-py3-none-any.whl"", but also failed. The error:
ERROR: Cannot install mediapipe-model-maker and mediapipe-model-maker==0.2.1.4 because these package versions have conflicting dependencies.

The conflict is caused by:
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.15.0 depends on tensorflow-text~=2.15.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.2 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.1 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.14.0 depends on tensorflow-text~=2.14.0
    mediapipe-model-maker 0.2.1.4 depends on tensorflow-text
    tf-models-official 2.13.2 depends on tensorflow-text~=2.13.0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip to attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts

Can version 0.2.1.4 be installed in windows?

### Standalone code to reproduce the issue

```shell
pip install mediapipe_model_maker-0.2.1.4-py3-none-any.whl
```


### Relevant log output

_No response_",libofei2004,2024-07-19 18:06:22+00:00,['Venkat6871'],2024-10-16 06:54:40+00:00,2024-10-16 06:54:36+00:00,https://github.com/tensorflow/tensorflow/issues/72198,"[('type:bug', 'Bug'), ('TF 2.8', '')]","[{'comment_id': 2240891481, 'issue_id': 2419544044, 'author': 'libofei2004', 'body': '@pjpratik @pkgoogle @tilakrayal @Venkat6871 @sachinprasadhs Could you please help me to solve my problem?', 'created_at': datetime.datetime(2024, 7, 20, 3, 18, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242408736, 'issue_id': 2419544044, 'author': 'Venkat6871', 'body': 'Hi **@libofei2004** ,\r\n- Could you please post this issue on [https://github.com/google-ai-edge/[mediapipe](https://github.com/google-ai-edge/mediapipe)] (https://github.com/google-ai-edge/mediapipe/issues) as this issue belongs to mediapipe.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 22, 8, 41, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246790969, 'issue_id': 2419544044, 'author': 'libofei2004', 'body': '@Venkat6871 Yes, I have post the issue on mediapipe page.', 'created_at': datetime.datetime(2024, 7, 24, 3, 27, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415889923, 'issue_id': 2419544044, 'author': 'Venkat6871', 'body': 'Hi **@libofei2004** ,\r\nClosing the issue since it is being tracked in other repo. Please feel free to re-open the issue if necessary.\r\nThank you!', 'created_at': datetime.datetime(2024, 10, 16, 6, 54, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415889981, 'issue_id': 2419544044, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72198"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72198"">No</a>', 'created_at': datetime.datetime(2024, 10, 16, 6, 54, 39, tzinfo=datetime.timezone.utc)}]","libofei2004 (Issue Creator) on (2024-07-20 03:18:41 UTC): @pjpratik @pkgoogle @tilakrayal @Venkat6871 @sachinprasadhs Could you please help me to solve my problem?

Venkat6871 (Assginee) on (2024-07-22 08:41:35 UTC): Hi **@libofei2004** ,
- Could you please post this issue on [https://github.com/google-ai-edge/[mediapipe](https://github.com/google-ai-edge/mediapipe)] (https://github.com/google-ai-edge/mediapipe/issues) as this issue belongs to mediapipe.

Thank you!

libofei2004 (Issue Creator) on (2024-07-24 03:27:05 UTC): @Venkat6871 Yes, I have post the issue on mediapipe page.

Venkat6871 (Assginee) on (2024-10-16 06:54:36 UTC): Hi **@libofei2004** ,
Closing the issue since it is being tracked in other repo. Please feel free to re-open the issue if necessary.
Thank you!

google-ml-butler[bot] on (2024-10-16 06:54:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72198"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72198"">No</a>

"
2418741496,issue,closed,completed,takeing much time to start program? why it happening,"after converting saved_model format to tflite it taking more time to start inference so plz help me with this and i posted below code so plz help with this and solve problem 

model=tf.saved_model.load(model_path)
 converter=tf.lite.TFLiteConverter.from_saved_model(model_path)
 converter.optimizations=[tf.lite.Optimize.DEFAULT]
 tflite_quant_modle=converter.convert()
tfquant_model_path=pathlib.Path(""D:/AIML/Tensorflow_FW/best_saved_model/tfquant_model"")
# tfquant_model_path.mkdir(exist_ok=True,parents=True)

tfquant_model_file=tfquant_model_path/""tfquant_model.tflite""
# tfquant_model_file.write_bytes(tflite_quant_modle)

interpreter=tf.lite.Interpreter(model_path=str(tfquant_model_file))


while True:
    frames=read_frames_in_batches(video_path=video_path)
    for frame in frames:
        if frame is not None:
            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            frame = frame.resize((640, 640))
            frame_nd_array = np.array(frame, dtype=np.float32)
            frame_nd_array = frame_nd_array / 255.0  # Normalize if required by the model
            frame_nd_array = frame_nd_array[None, ...]  # Add batch dimension
            input_tensor_frame = tf.convert_to_tensor(frame_nd_array)  # Convert numpy array to tensor
            
            interpreter.allocate_tensors()
            input_details=interpreter.get_input_details()
            output_details=interpreter.get_output_details()
            interpreter.set_tensor(input_details[0]['index'],input_tensor_frame)
            interpreter.invoke()
            predictions = interpreter.get_tensor(output_details[0]['index'])
            print(predictions)
            # predictions=predictions.numpy()
            frame_array=np.array(frame)
            output_image=postprocess(input_image=frame_array,output=predictions)
            cv2.namedWindow('Detections',cv2.WINDOW_NORMAL)
            cv2.imshow(""Detections"", cv2.cvtColor(output_image,cv2.COLOR_BGR2RGB))
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    ",Vinaygoudasp7,2024-07-19 12:10:41+00:00,['tilakrayal'],2024-08-06 01:53:36+00:00,2024-08-06 01:53:33+00:00,https://github.com/tensorflow/tensorflow/issues/72180,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2242108214, 'issue_id': 2418741496, 'author': 'tilakrayal', 'body': '@Vinaygoudasp7,\r\nCould you please provide the complete code or the colab gist & error log and the tensorflow version which you are trying to execute the code which helps to analyse the issue in a more effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 22, 5, 17, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257309329, 'issue_id': 2418741496, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 30, 1, 53, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213411, 'issue_id': 2418741496, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213456, 'issue_id': 2418741496, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72180"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72180"">No</a>', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 35, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-22 05:17:17 UTC): @Vinaygoudasp7,
Could you please provide the complete code or the colab gist & error log and the tensorflow version which you are trying to execute the code which helps to analyse the issue in a more effective way. Thank you!

github-actions[bot] on (2024-07-30 01:53:04 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-06 01:53:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-06 01:53:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72180"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72180"">No</a>

"
2418473211,issue,open,,Wheel built for wrong python version (3.11 not 3.10),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

clang version 17.0.6 (++20231209124227+6009708b4367-1~exp1~20231209124336.77)

### CUDA/cuDNN version

12.3/8.9.7.29

### GPU model and memory

NVIDIA GeForce GTX 1070 8192MiB

### Current behavior?

```
pip install bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.17.0-cp311-cp311-linux_x86_64.whl 
Defaulting to user installation because normal site-packages is not writeable
ERROR: tensorflow-2.17.0-cp311-cp311-linux_x86_64.whl is not a supported wheel on this platform.
```
Python is 3.10 but the wheel generated is for 3.11 but that's not on the system:
```
python --version
Python 3.10.12
```
I cannot find a commandline option for bazel to change the minor version to 3.10. Is there any? Or how can I force bazel to generate a whell for 3.10?

### Standalone code to reproduce the issue

```shell
./configure
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --copt=-Wno-error=unused-command-line-argument
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /home/bernd/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=161
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-12.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/bernd/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/bernd/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/bernd/tensorflow/.bazelrc: --config=cuda --action_env=TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/bernd/tensorflow/.bazelrc: --config=cuda --action_env=TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/bernd/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/bernd/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:wheel up-to-date:
  bazel-bin/tensorflow/tools/pip_package/wheel_house
INFO: Elapsed time: 1.625s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
```
",berndporr,2024-07-19 09:33:43+00:00,['Venkat6871'],2024-07-22 06:35:37+00:00,,https://github.com/tensorflow/tensorflow/issues/72166,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2238779571, 'issue_id': 2418473211, 'author': 'berndporr', 'body': ""[bazelrc.zip](https://github.com/user-attachments/files/16310214/bazelrc.zip)\r\nRemoved tensorrt from it as configure still enabeled it. Even that I didn't want it."", 'created_at': datetime.datetime(2024, 7, 19, 9, 39, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2238961969, 'issue_id': 2418473211, 'author': 'berndporr', 'body': 'Answering my own question:\r\n```\r\nbazel build //tensorflow/tools/pip_package:wheel --repo_env=TF_PYTHON_VERSION=3.10 --repo_env=WHEEL_NAME=tensorflow --config=cuda --copt=-Wno-error=unused-command-line-argument\r\n```\r\nwith the option `-repo_env=TF_PYTHON_VERSION=3.10` one can set the target python version.', 'created_at': datetime.datetime(2024, 7, 19, 11, 41, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2239344631, 'issue_id': 2418473211, 'author': 'mihaimaruseac', 'body': 'Oh, this would require a patch release :|', 'created_at': datetime.datetime(2024, 7, 19, 14, 39, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242194881, 'issue_id': 2418473211, 'author': 'Venkat6871', 'body': '@belitskiy, @learning-to-play', 'created_at': datetime.datetime(2024, 7, 22, 6, 35, 36, tzinfo=datetime.timezone.utc)}]","berndporr (Issue Creator) on (2024-07-19 09:39:06 UTC): [bazelrc.zip](https://github.com/user-attachments/files/16310214/bazelrc.zip)
Removed tensorrt from it as configure still enabeled it. Even that I didn't want it.

berndporr (Issue Creator) on (2024-07-19 11:41:35 UTC): Answering my own question:
```
bazel build //tensorflow/tools/pip_package:wheel --repo_env=TF_PYTHON_VERSION=3.10 --repo_env=WHEEL_NAME=tensorflow --config=cuda --copt=-Wno-error=unused-command-line-argument
```
with the option `-repo_env=TF_PYTHON_VERSION=3.10` one can set the target python version.

mihaimaruseac on (2024-07-19 14:39:14 UTC): Oh, this would require a patch release :|

Venkat6871 (Assginee) on (2024-07-22 06:35:36 UTC): @belitskiy, @learning-to-play

"
2418458391,issue,closed,completed,Which phones are supporting tensorflow lite GPU delegate?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have checked **compatList.isDelegateSupportedOnThisDevice** for all my own devices: Samsung S22 Ultra, Samsung A73, OPPO Reno8 T, OPPO Reno10 5G.
The values are always false for all. 
Do we have any way to get rid this issue? Please help. thanks.

### Standalone code to reproduce the issue

```shell
if(compatList.isDelegateSupportedOnThisDevice){
              // if the device has a supported GPU, add the GPU delegate
              val delegateOptions = compatList.bestOptionsForThisDevice
              this.addDelegate(GpuDelegate(delegateOptions))
          } else {
              // if the GPU is not supported, run on 4 threads
              this.setNumThreads(4)
          }
```


### Relevant log output

_No response_",tranvantungit,2024-07-19 09:26:21+00:00,['sawantkumar'],2024-08-09 01:55:07+00:00,2024-08-09 01:55:04+00:00,https://github.com/tensorflow/tensorflow/issues/72165,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)'), ('Android', '')]","[{'comment_id': 2242080960, 'issue_id': 2418458391, 'author': 'tranvantungit', 'body': 'I have tried to ignore `isDelegateSupportedOnThisDevice`, then this is the logcat\r\n\r\n```2024-07-22 11:44:16.943 22672-23473 InterpreterApi                    I  Loaded native library: tensorflowlite_jni\r\n2024-07-22 11:44:16.955 22672-23473 InterpreterApi                    I  Loaded native library: tensorflowlite_jni_gms_client\r\n2024-07-22 11:44:16.978 22672-23473 tflite                            I  Initialized TensorFlow Lite runtime.\r\n2024-07-22 11:44:16.984 22672-23473 libc                              W  Access denied finding property ""ro.mediatek.platform""\r\n2024-07-22 11:44:16.991 22672-23473 tflite                            I  Created TensorFlow Lite XNNPACK delegate for CPU.\r\n2024-07-22 11:44:16.998 22672-23473 tflite                            I  Replacing 247 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 175 partitions.', 'created_at': datetime.datetime(2024, 7, 22, 4, 48, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249906663, 'issue_id': 2418458391, 'author': 'sawantkumar', 'body': 'Hi @tranvantungit ,\r\n\r\nCan you upgrade the TFLite to the latest version and let me know if the issue persists?', 'created_at': datetime.datetime(2024, 7, 25, 9, 37, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345672, 'issue_id': 2418458391, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005059, 'issue_id': 2418458391, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005105, 'issue_id': 2418458391, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72165"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72165"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 6, tzinfo=datetime.timezone.utc)}]","tranvantungit (Issue Creator) on (2024-07-22 04:48:36 UTC): I have tried to ignore `isDelegateSupportedOnThisDevice`, then this is the logcat

```2024-07-22 11:44:16.943 22672-23473 InterpreterApi                    I  Loaded native library: tensorflowlite_jni
2024-07-22 11:44:16.955 22672-23473 InterpreterApi                    I  Loaded native library: tensorflowlite_jni_gms_client
2024-07-22 11:44:16.978 22672-23473 tflite                            I  Initialized TensorFlow Lite runtime.
2024-07-22 11:44:16.984 22672-23473 libc                              W  Access denied finding property ""ro.mediatek.platform""
2024-07-22 11:44:16.991 22672-23473 tflite                            I  Created TensorFlow Lite XNNPACK delegate for CPU.
2024-07-22 11:44:16.998 22672-23473 tflite                            I  Replacing 247 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 175 partitions.

sawantkumar (Assginee) on (2024-07-25 09:37:57 UTC): Hi @tranvantungit ,

Can you upgrade the TFLite to the latest version and let me know if the issue persists?

github-actions[bot] on (2024-08-02 01:53:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-09 01:55:04 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-09 01:55:06 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72165"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72165"">No</a>

"
2418203925,issue,closed,completed,TFLite 2.16.2 and 2.17.0 not present on Maven,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Android artifacts haven't been published on Maven

### Standalone code to reproduce the issue

```gradle
implementation(""org.tensorflow:tensorflow-lite:2.17.0"")
```


### Relevant log output

_No response_",guillaume-tgl,2024-07-19 06:54:15+00:00,"['arfaian', 'pkgoogle', 'sawantkumar']",2024-12-11 05:59:35+00:00,2024-12-11 05:59:32+00:00,https://github.com/tensorflow/tensorflow/issues/72157,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2241381791, 'issue_id': 2418203925, 'author': 'mihaimaruseac', 'body': 'TFLite releases are decoupled from main TF releases, so it would take a little while for them to get uploaded', 'created_at': datetime.datetime(2024, 7, 21, 1, 57, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261603243, 'issue_id': 2418203925, 'author': 'mihaimaruseac', 'body': 'I have not worked in TF for over 2 years, just helping with issues here and there', 'created_at': datetime.datetime(2024, 7, 31, 22, 56, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261604833, 'issue_id': 2418203925, 'author': 'pkgoogle', 'body': '@mihaimaruseac thanks! @arfaian, I believe you would have the most insight here. Thanks.', 'created_at': datetime.datetime(2024, 7, 31, 22, 58, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273119893, 'issue_id': 2418203925, 'author': 'kjlee5435', 'body': 'Please update 2.17 to maven repos.. please.', 'created_at': datetime.datetime(2024, 8, 7, 10, 14, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301990391, 'issue_id': 2418203925, 'author': 'H4dr1en', 'body': '+1, could you please upload 2.17 to maven?\r\n\r\nAll `tensorflow-lite`, `tensorflow-lite-gpu`, `tensorflow-lite-gpu-api` and corresponding `tensorflow-lite-gpu-delegate-plugin`, `tensorflow-lite-support` and `tensorflow-lite-metadata` if relevant', 'created_at': datetime.datetime(2024, 8, 21, 12, 56, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367094589, 'issue_id': 2418203925, 'author': 'SnailLiuxc', 'body': 'Could you please upload 2.17 to maven? \r\nour projects are waiting the  so library that are support 16KB page size alignment.', 'created_at': datetime.datetime(2024, 9, 23, 1, 32, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371457835, 'issue_id': 2418203925, 'author': 'asus4', 'body': 'According to [this blog](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/), TensorFlow Lite is renamed to LiteRT.\r\n\r\nI found the LiteRT 0.1.0 package on Maven, but the internal `TfLiteVersion` was `2.18.0`, assuming it is a nightly build. \r\n\r\nhttps://central.sonatype.com/artifact/io.github.google-ai-edge/litert\r\n\r\nUploading the TFLite release build to Maven would be better until the migration to LiteRT is complete.', 'created_at': datetime.datetime(2024, 9, 24, 14, 25, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475271118, 'issue_id': 2418203925, 'author': 'liqiang311', 'body': 'So, how can I get the latest 2.17 or 2.18 libtensorflowlite_jni.so? Latest version of MVN is 2.16.1.', 'created_at': datetime.datetime(2024, 11, 14, 2, 29, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475669972, 'issue_id': 2418203925, 'author': 'guillaume-tgl', 'body': ""> So, how can I get the latest 2.17 or 2.18 libtensorflowlite_jni.so? Latest version of MVN is 2.16.1.\r\n\r\nYou can use LiteRT instead: https://maven.google.com/web/index.html?q=litert#com.google.ai.edge.litert\r\nIt's the replacement for TFLite."", 'created_at': datetime.datetime(2024, 11, 14, 8, 6, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533006799, 'issue_id': 2418203925, 'author': 'pkgoogle', 'body': 'Hi @guillaume-tgl, I feel this issue is no longer relevant, do you agree?', 'created_at': datetime.datetime(2024, 12, 10, 22, 1, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533709445, 'issue_id': 2418203925, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72157"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72157"">No</a>', 'created_at': datetime.datetime(2024, 12, 11, 5, 59, 34, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-21 01:57:18 UTC): TFLite releases are decoupled from main TF releases, so it would take a little while for them to get uploaded

mihaimaruseac on (2024-07-31 22:56:18 UTC): I have not worked in TF for over 2 years, just helping with issues here and there

pkgoogle (Assginee) on (2024-07-31 22:58:06 UTC): @mihaimaruseac thanks! @arfaian, I believe you would have the most insight here. Thanks.

kjlee5435 on (2024-08-07 10:14:34 UTC): Please update 2.17 to maven repos.. please.

H4dr1en on (2024-08-21 12:56:37 UTC): +1, could you please upload 2.17 to maven?

All `tensorflow-lite`, `tensorflow-lite-gpu`, `tensorflow-lite-gpu-api` and corresponding `tensorflow-lite-gpu-delegate-plugin`, `tensorflow-lite-support` and `tensorflow-lite-metadata` if relevant

SnailLiuxc on (2024-09-23 01:32:52 UTC): Could you please upload 2.17 to maven? 
our projects are waiting the  so library that are support 16KB page size alignment.

asus4 on (2024-09-24 14:25:48 UTC): According to [this blog](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/), TensorFlow Lite is renamed to LiteRT.

I found the LiteRT 0.1.0 package on Maven, but the internal `TfLiteVersion` was `2.18.0`, assuming it is a nightly build. 

https://central.sonatype.com/artifact/io.github.google-ai-edge/litert

Uploading the TFLite release build to Maven would be better until the migration to LiteRT is complete.

liqiang311 on (2024-11-14 02:29:22 UTC): So, how can I get the latest 2.17 or 2.18 libtensorflowlite_jni.so? Latest version of MVN is 2.16.1.

guillaume-tgl (Issue Creator) on (2024-11-14 08:06:36 UTC): You can use LiteRT instead: https://maven.google.com/web/index.html?q=litert#com.google.ai.edge.litert
It's the replacement for TFLite.

pkgoogle (Assginee) on (2024-12-10 22:01:05 UTC): Hi @guillaume-tgl, I feel this issue is no longer relevant, do you agree?

google-ml-butler[bot] on (2024-12-11 05:59:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72157"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72157"">No</a>

"
2418201132,issue,open,,TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

TF2.12

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12, cuDNN 8

### GPU model and memory

_No response_

### Current behavior?

When profiling on GPU. Timestamp in Chrome trace format was incorrect due to shifting. The relevant commit is https://github.com/tensorflow/tensorflow/commit/701c1e97317ace357621b7f0bd4a2e427f16ed42#r144411330

There was a issue on  https://github.com/tensorflow/profiler/issues/238, but does not fix.

**Incorrect** timeline if `CollectData` was called:
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

**Correct** timelie if `CollectDataInternal` was called:
https://github.com/tensorflow/tensorflow/blob/a3e2c692c18649329c4210cf8df2487d2028e267/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

The shifting was called in `PostProcessSingleHostXSpace` in `CollectData`
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/third_party/xla/third_party/tsl/tsl/profiler/lib/profiler_session.cc#L81-L88
Is shifting expected behavior? If that, how to use the profiler session correctly?

### Standalone code to reproduce the issue

```shell
from absl import app
import logging

import tensorflow as tf
from tensorflow.python.client import timeline

tf.compat.v1.disable_eager_execution()

def test_profiler():
  shape = tf.constant([1000, 1000], dtype=tf.int64)
  x = tf.random.normal(shape)
  y = x ** 2
  z = y ** 2
  with tf.compat.v1.Session() as sess:
    run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
    run_metadata = tf.compat.v1.RunMetadata()
    #for i in range(3):
    sess.run(z, options=run_options, run_metadata=run_metadata)
    #logging.info(""run_metadata.step_stats:%s"", run_metadata.step_stats)
    tl = timeline.Timeline(run_metadata.step_stats)
    ctf = tl.generate_chrome_trace_format()
    with open('timeline.json', 'w') as f:
      f.write(ctf)

def main(argv):
  test_profiler()


if __name__ == ""__main__"":
  app.run(main)
```


### Relevant log output

_No response_",372046933,2024-07-19 06:52:16+00:00,['penpornk'],2024-07-23 06:31:26+00:00,,https://github.com/tensorflow/tensorflow/issues/72156,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:core', 'issues related to core part of tensorflow'), ('TF 2.12', 'For issues related to Tensorflow 2.12')]","[{'comment_id': 2242587999, 'issue_id': 2418201132, 'author': '372046933', 'body': '@tilakrayal Could take some time to look at the issue? The shift affects TF 2.12 and afterwards.', 'created_at': datetime.datetime(2024, 7, 22, 10, 9, 56, tzinfo=datetime.timezone.utc)}]","372046933 (Issue Creator) on (2024-07-22 10:09:56 UTC): @tilakrayal Could take some time to look at the issue? The shift affects TF 2.12 and afterwards.

"
2417195374,issue,closed,completed,tensorflow,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

wondows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\Bernabas\Documents\game_ai_tool\api\app.py"", line 9, in <module>
    from ai_model.model import GameAIModel
  File ""c:\Users\Bernabas\Documents\game_ai_tool\ai_model\model.py"", line 3, in <module>
    import tensorflow as tf
  File ""C:\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

### Standalone code to reproduce the issue

```shell
# api/app.py

import sys
import os

# Add the parent directory to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from ai_model.model import GameAIModel
from flask import Flask, request, jsonify

app = Flask(__name__)

# Initialize your model
model = GameAIModel()
model.load_weights('path_to_your_saved_model.h5')

@app.route('/generate_game', methods=['POST'])
def generate_game():
    data = request.json
    user_input = data['input']
    # Preprocess the input as required
    response = model(user_input)
    # Postprocess the output as required
    return jsonify(response)

if __name__ == '__main__':
    app.run(debug=True)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\Bernabas\Documents\game_ai_tool\api\app.py"", line 9, in <module>
    from ai_model.model import GameAIModel
  File ""c:\Users\Bernabas\Documents\game_ai_tool\ai_model\model.py"", line 3, in <module>
    import tensorflow as tf
  File ""C:\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
```
",bernsny24,2024-07-18 19:15:12+00:00,['Venkat6871'],2024-07-21 14:15:20+00:00,2024-07-21 14:15:18+00:00,https://github.com/tensorflow/tensorflow/issues/72120,"[('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2241170668, 'issue_id': 2417195374, 'author': 'mihaimaruseac', 'body': ""Please search for similar issues here. This shows up quite frequently and it's caused by your CPU not having support for AVX instructions."", 'created_at': datetime.datetime(2024, 7, 20, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241255764, 'issue_id': 2417195374, 'author': 'bernsny24', 'body': ""So, is there a possible way I can solve it?\r\n\r\nI'm building a software which requires a tensorflow package. Can I get a\r\npackage which can help me in replace of tensorflow?\r\n\r\nOn Sat, Jul 20, 2024, 8:06 PM Mihai Maruseac ***@***.***>\r\nwrote:\r\n\r\n> Please search for similar issues here. This shows up quite frequently and\r\n> it's caused by your CPU not having support for AVX instructions.\r\n>\r\n> \r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/72120#issuecomment-2241170668>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BHWE24ONH6VUBYVAHJCMMRTZNJYWPAVCNFSM6AAAAABLDIZ2JGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDENBRGE3TANRWHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 7, 20, 18, 32, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241626445, 'issue_id': 2417195374, 'author': 'mihaimaruseac', 'body': 'Please see #36167. Closing this as duplicate of that issue. Please search past issues in the future, instead of duplicating.', 'created_at': datetime.datetime(2024, 7, 21, 14, 15, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241626455, 'issue_id': 2417195374, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72120"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72120"">No</a>', 'created_at': datetime.datetime(2024, 7, 21, 14, 15, 20, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-20 14:36:00 UTC): Please search for similar issues here. This shows up quite frequently and it's caused by your CPU not having support for AVX instructions.

bernsny24 (Issue Creator) on (2024-07-20 18:32:59 UTC): So, is there a possible way I can solve it?

I'm building a software which requires a tensorflow package. Can I get a
package which can help me in replace of tensorflow?

On Sat, Jul 20, 2024, 8:06 PM Mihai Maruseac ***@***.***>
wrote:

mihaimaruseac on (2024-07-21 14:15:18 UTC): Please see #36167. Closing this as duplicate of that issue. Please search past issues in the future, instead of duplicating.

google-ml-butler[bot] on (2024-07-21 14:15:20 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72120"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72120"">No</a>

"
2416955338,issue,open,reopened,Error: 'class tensorflow::tools::proto_splitter::SavedModelSplitter' has no member named 'WriteToCord',"Facing below failure while executing test suite on Tensorflow master. 
Command used for test execution: 
`bazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/... `

gcc --version: 
gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
```
ERROR: /root/tensorflow/tensorflow/cc/saved_model/image_format/BUILD:21:11: Compiling tensorflow/cc/saved_model/image_format/internal_api.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/cc/saved_model/image_format:internal_api) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 165 arguments skipped)
In file included from ./tensorflow/tools/proto_splitter/cc/composable_splitter.h:20,
                 from ./tensorflow/tools/proto_splitter/cc/saved_model_splitter.h:18,
                 from tensorflow/cc/saved_model/image_format/internal_api.cc:33:
./tensorflow/tools/proto_splitter/cc/composable_splitter_base.h:33: warning: ""IS_OSS"" redefined
   33 | #define IS_OSS true
      |
In file included from tensorflow/cc/saved_model/image_format/internal_api.cc:16:
./tensorflow/cc/saved_model/image_format/internal_api.h:27: note: this is the location of the previous definition
   27 | #define IS_OSS false
      |
tensorflow/cc/saved_model/image_format/internal_api.cc:36: warning: ""IS_OSS"" redefined
   36 | #define IS_OSS false
      |
In file included from ./tensorflow/tools/proto_splitter/cc/composable_splitter.h:20,
                 from ./tensorflow/tools/proto_splitter/cc/saved_model_splitter.h:18,
                 from tensorflow/cc/saved_model/image_format/internal_api.cc:33:
./tensorflow/tools/proto_splitter/cc/composable_splitter_base.h:33: note: this is the location of the previous definition
   33 | #define IS_OSS true
      |
tensorflow/cc/saved_model/image_format/internal_api.cc: In function 'absl::lts_20230802::StatusOr<std::tuple<absl::lts_20230802::Cord, bool> > tensorflow::image_format::WriteSavedModelToCord(tensorflow::SavedModel*)':
tensorflow/cc/saved_model/image_format/internal_api.cc:126:19: error: 'class tensorflow::tools::proto_splitter::SavedModelSplitter' has no member named 'WriteToCord'; did you mean 'WriteToString'?
  126 |   return splitter.WriteToCord();
      |                   ^~~~~~~~~~~
      |                   WriteToString
INFO: Elapsed time: 2594.461s, Critical Path: 70.16s
INFO: 14278 processes: 5059 internal, 9219 local.
FAILED: Build did NOT complete successfully
```",Nayana-ibm,2024-07-18 17:11:11+00:00,['tilakrayal'],2024-12-11 13:31:42+00:00,,https://github.com/tensorflow/tensorflow/issues/72108,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues')]","[{'comment_id': 2239389606, 'issue_id': 2416955338, 'author': 'tilakrayal', 'body': '@Nayana-ibm,\r\nCould you please provide the tensorflow and compatible version you are trying and the steps followed to install the tensorflow which helps to debug the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 19, 14, 47, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242908938, 'issue_id': 2416955338, 'author': 'Nayana-ibm', 'body': '@tilakrayal Please see details below:\r\n\r\nTensorFlow version : master \r\nBazel v6.5.0\r\nPython version: 3.12.2\r\ngcc --version: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\n\r\nSteps to build:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./configure\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is /usr/local/bin/python3]:\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.12/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.12/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you want to use Clang to build TensorFlow? [Y/n]: N\r\nGCC will be used to compile TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\nbazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...\r\n\r\n\r\n```', 'created_at': datetime.datetime(2024, 7, 22, 13, 3, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251250735, 'issue_id': 2416955338, 'author': 'Nayana-ibm', 'body': '@tilakrayal Any update please', 'created_at': datetime.datetime(2024, 7, 25, 19, 28, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255605781, 'issue_id': 2416955338, 'author': 'viddya673', 'body': 'I am facing the same issue with TensorFlow version 2.16.1. Here are some additional details:\r\n\r\nx86_64\r\nbazel 6.5.0\r\ngcc --version: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0', 'created_at': datetime.datetime(2024, 7, 29, 10, 49, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255953425, 'issue_id': 2416955338, 'author': 'sawantkumar', 'body': ""Hi @pkgoogle ,\r\n\r\nI tried replicating this issue , with ubuntu 20, bazel 6.5.0 and python 3.9 and i am getting the below error, can you please take a look \r\n\r\n```\r\nroot@tflite-issue-replication:/home/sawantkumar/work/tensorflow# bazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...\r\nWARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Reading 'startup' options from /home/sawantkumar/work/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=217\r\nINFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-10/bin/clang --repo_env=CC=/usr/lib/llvm-10/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-10/bin/clang\r\nINFO: Found applicable config definition build:short_logs in file /home/sawantkumar/work/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Found applicable config definition build:linux in file /home/sawantkumar/work/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/local_xla/third_party/py/python_repo.bzl:98:14: \r\nHERMETIC_PYTHON_VERSION variable was not set correctly, using default version.\r\nPython 3.9 will be used.\r\nTo select Python version, either set HERMETIC_PYTHON_VERSION env variable in\r\nyour shell:\r\n  export HERMETIC_PYTHON_VERSION=3.12\r\nOR pass it as an argument to bazel command directly or inside your .bazelrc\r\nfile:\r\n  --repo_env=HERMETIC_PYTHON_VERSION=3.12\r\nDEBUG: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9\r\nWARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /home/sawantkumar/work/tensorflow/tensorflow/compiler/mlir/lite/stablehlo/BUILD:872:13: target '//tensorflow/compiler/mlir/lite/stablehlo:odml_to_stablehlo' is deprecated: odml_to_stablehlo is being deprecated, please use TFlite converter with flag: converter.target_spec.supported_ops =  [tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS]\r\nWARNING: /home/sawantkumar/work/tensorflow/tensorflow/core/kernels/BUILD:975:18: target '//tensorflow/core/kernels:bitcast_op' is deprecated: use //third_party/tensorflow/c/kernels:bitcast_op instead\r\nWARNING: /home/sawantkumar/work/tensorflow/tensorflow/python/eager/BUILD:714:18: target '//tensorflow/python/eager:framework_for_generated_wrappers' is deprecated: Depending on this target can cause build dependency cycles. Depend on the fine-grained sub-targets instead.\r\nWARNING: /home/sawantkumar/work/tensorflow/tensorflow/python/ops/distributions/BUILD:9:18: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found\r\nINFO: Analyzed 12399 targets (1251 packages loaded, 78333 targets configured).\r\nINFO: Found 12399 targets...\r\nERROR: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/boringssl/BUILD:146:11: Compiling src/ssl/d1_both.cc failed: (Exit 1): clang failed: error executing command (from target @boringssl//:ssl) /usr/lib/llvm-10/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 65 arguments skipped)\r\nerror: unknown warning option '-Werror=unused-but-set-variable'; did you mean '-Werror=unused-const-variable'? [-Werror,-Wunknown-warning-option]\r\nerror: unknown warning option '-Werror=unused-but-set-variable'; did you mean '-Werror=unused-const-variable'? [-Werror,-Wunknown-warning-option]\r\nINFO: Elapsed time: 189.739s, Critical Path: 112.50s\r\nINFO: 2310 processes: 1345 internal, 965 local.\r\nFAILED: Build did NOT complete successfully\r\n```"", 'created_at': datetime.datetime(2024, 7, 29, 13, 28, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264054029, 'issue_id': 2416955338, 'author': 'pkgoogle', 'body': ""I'm currently failing for different reasons ... I do want to understand better where this command is coming from. Hi @Nayana-ibm, how/where did you get this command? Did you get it from a resource/documentation?"", 'created_at': datetime.datetime(2024, 8, 1, 21, 37, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005079, 'issue_id': 2416955338, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527262, 'issue_id': 2416955338, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527330, 'issue_id': 2416955338, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72108"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72108"">No</a>', 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2319870135, 'issue_id': 2416955338, 'author': 'Nayana-ibm', 'body': ""> I'm currently failing for different reasons ... I do want to understand better where this command is coming from. Hi @Nayana-ibm, how/where did you get this command? Did you get it from a resource/documentation?\r\n\r\nGot this command from TensorFlow CI jobs.\r\nhttps://btx.cloud.google.com/invocations/ebf9f385-7d9a-4cac-ba9e-d835e4d43ff0/targets/tensorflow%2Fofficial%2Fcontinuous%2Flinux_x86_cpu_max_python;config=default/log\r\nhttps://github.com/tensorflow/tensorflow/blob/master/.bazelrc"", 'created_at': datetime.datetime(2024, 8, 30, 3, 19, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337436076, 'issue_id': 2416955338, 'author': 'gaikwadrahul8', 'body': ""Hi, @Nayana-ibm \r\n\r\nI'm really sorry for inconvenience, issue got closed by our bot after 14 days due to no response within that window period, Anyways thank you for providing the details from where you got command mentioned in the issue template so we'll have to dig more into this issue, I have re-opened this issue.\r\n\r\nThank you for you cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 9, 8, 16, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2365160821, 'issue_id': 2416955338, 'author': 'gaikwadrahul8', 'body': ""Hi, @Nayana-ibm \r\n\r\nI apologize for the delayed response, I was trying to replicate the same bahavior from my end but I'm getting below error message to confirm did you encounter the similar error while running that command ? if not, may I know which configurations did you choose after running this command `./configure`\r\n\r\n```\r\nERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/2b31a38afcac928ac333a3bda4775519/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/avx512fp16_prod_microkernels.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:avx512fp16_prod_microkernels) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 131 arguments skipped)\r\ngcc: error: unrecognized command-line option '-mavx512fp16'; did you mean '-mavx512bf16'?\r\nINFO: Elapsed time: 1722.191s, Critical Path: 92.97s\r\nINFO: 8455 processes: 2809 internal, 5646 local.\r\nFAILED: Build did NOT complete successfully\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-issue-72108/tensorflow$ \r\n```\r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 21, 12, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379180892, 'issue_id': 2416955338, 'author': 'Nayana-ibm', 'body': '@gaikwadrahul8 I have executed below command:\r\n\r\n`bazel build --define tflite_with_xnnpack=false --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/..`', 'created_at': datetime.datetime(2024, 9, 27, 12, 39, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382689294, 'issue_id': 2416955338, 'author': 'gaikwadrahul8', 'body': ""Hi, @Nayana-ibm\r\n\r\nI tried the exact command which you mentioned in the issue template initially so to confirm, is it exact same command or you made some changes in the initial command so I'll go ahead try updated command from my end ? \r\n\r\nThank you for your cooperation and patience."", 'created_at': datetime.datetime(2024, 9, 30, 10, 7, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389498330, 'issue_id': 2416955338, 'author': 'Nayana-ibm', 'body': '@gaikwadrahul8 Let me simply command which reproduces above error:\r\n\r\n git clone https://github.com/tensorflow/tensorflow\r\n cd tensorflow/\r\n git checkout v2.16.1\r\n ./configure\r\n```\r\nYou have bazel 6.5.0 installed.\r\nPlease specify the location of python. [Default is /usr/local/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.12/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.12/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you want to use Clang to build TensorFlow? [Y/n]: N\r\nGCC will be used to compile TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\nbazel build  -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...', 'created_at': datetime.datetime(2024, 10, 2, 19, 15, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393518191, 'issue_id': 2416955338, 'author': 'gaikwadrahul8', 'body': ""Hi, @Nayana-ibm\r\n\r\nThank you for providing the minimal command to replicate same behavior from our end, I tried that command from my end on `ubuntu 24.04` and build did not complete successfully got below error so we'll have to dig more into this issue.\r\n\r\nHi, @pkgoogle\r\nPlease take look into this issue. Thank you\r\n\r\n\r\n```\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-72108/tensorflow$ bazel build -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...\r\nINFO: Reading 'startup' options from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\r\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/site-packages --python_path=/home/gaikwadrahul/miniconda3/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/compiler/mlir/lite/stablehlo/BUILD:585:13: target '//tensorflow/compiler/mlir/lite/stablehlo:odml_to_stablehlo' is deprecated: odml_to_stablehlo is being deprecated, please use TFlite converter with flag: converter.target_spec.supported_ops =  [tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS]\r\nWARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/core/kernels/BUILD:973:18: target '//tensorflow/core/kernels:bitcast_op' is deprecated: use //third_party/tensorflow/c/kernels:bitcast_op instead\r\nWARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/python/eager/BUILD:713:18: target '//tensorflow/python/eager:framework_for_generated_wrappers' is deprecated: Depending on this target can cause build dependency cycles. Depend on the fine-grained sub-targets instead.\r\nWARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/python/ops/distributions/BUILD:9:18: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed 12218 targets (1 packages loaded, 8993 targets configured).\r\nINFO: Found 12218 targets...\r\nERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/36362ebe65b72c405dc49cfad486623b/external/dnnl/BUILD.bazel:38:11: Compiling src/cpu/x64/gemm/gemm_info.cpp failed: (Exit 1): gcc failed: error executing command (from target @dnnl//:dnnl_single_threaded) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 80 arguments skipped)\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = signed char; b_type = unsigned char; c_type = int]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:808:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  751 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = signed char; b_type = unsigned char; c_type = int]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:808:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  207 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  588 |         switch (data_traits<a_t>::data_type) {\r\n      |         ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = signed char; b_type = signed char; c_type = int]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:811:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  751 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = signed char; b_type = signed char; c_type = int]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:811:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  207 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  588 |         switch (data_traits<a_t>::data_type) {\r\n      |         ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = dnnl::impl::bfloat16_t; b_type = dnnl::impl::bfloat16_t; c_type = float]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:814:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  751 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = dnnl::impl::bfloat16_t; b_type = dnnl::impl::bfloat16_t; c_type = float]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:814:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  207 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  588 |         switch (data_traits<a_t>::data_type) {\r\n      |         ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = float; b_type = float; c_type = float]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:817:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  751 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = float; b_type = float; c_type = float]':\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:817:16:   required from here\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  207 |     switch (data_traits<a_t>::data_type) {\r\n      |     ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]\r\n  588 |         switch (data_traits<a_t>::data_type) {\r\n      |         ^~~~~~\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]\r\nexternal/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]\r\ncc1plus: some warnings being treated as errors\r\nINFO: Elapsed time: 1370.697s, Critical Path: 108.62s\r\nINFO: 5316 processes: 1127 internal, 4189 local.\r\nFAILED: Build did NOT complete successfully\r\n(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-72108/tensorflow$ \r\n```"", 'created_at': datetime.datetime(2024, 10, 4, 11, 45, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423345023, 'issue_id': 2416955338, 'author': 'pkgoogle', 'body': ""This doesn't seem like a lite issue... @tilakrayal, can you please route this appropriately. Thanks."", 'created_at': datetime.datetime(2024, 10, 18, 22, 56, 48, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-19 14:47:25 UTC): @Nayana-ibm,
Could you please provide the tensorflow and compatible version you are trying and the steps followed to install the tensorflow which helps to debug the issue in an effective way. Thank you!

Nayana-ibm (Issue Creator) on (2024-07-22 13:03:32 UTC): @tilakrayal Please see details below:

TensorFlow version : master 
Bazel v6.5.0
Python version: 3.12.2
gcc --version: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0


Steps to build:

```
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/local/bin/python3]:

Found possible Python library paths:
  /usr/local/lib/python3.12/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.12/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]: N
GCC will be used to compile TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

bazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...


```

Nayana-ibm (Issue Creator) on (2024-07-25 19:28:32 UTC): @tilakrayal Any update please

viddya673 on (2024-07-29 10:49:48 UTC): I am facing the same issue with TensorFlow version 2.16.1. Here are some additional details:

x86_64
bazel 6.5.0
gcc --version: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0

sawantkumar on (2024-07-29 13:28:33 UTC): Hi @pkgoogle ,

I tried replicating this issue , with ubuntu 20, bazel 6.5.0 and python 3.9 and i am getting the below error, can you please take a look 

```
root@tflite-issue-replication:/home/sawantkumar/work/tensorflow# bazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...
WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /home/sawantkumar/work/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=217
INFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/sawantkumar/work/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-10/bin/clang --repo_env=CC=/usr/lib/llvm-10/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-10/bin/clang
INFO: Found applicable config definition build:short_logs in file /home/sawantkumar/work/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:dynamic_kernels in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Found applicable config definition build:linux in file /home/sawantkumar/work/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/sawantkumar/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/local_xla/third_party/py/python_repo.bzl:98:14: 
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.9 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9
WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: /home/sawantkumar/work/tensorflow/tensorflow/compiler/mlir/lite/stablehlo/BUILD:872:13: target '//tensorflow/compiler/mlir/lite/stablehlo:odml_to_stablehlo' is deprecated: odml_to_stablehlo is being deprecated, please use TFlite converter with flag: converter.target_spec.supported_ops =  [tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS]
WARNING: /home/sawantkumar/work/tensorflow/tensorflow/core/kernels/BUILD:975:18: target '//tensorflow/core/kernels:bitcast_op' is deprecated: use //third_party/tensorflow/c/kernels:bitcast_op instead
WARNING: /home/sawantkumar/work/tensorflow/tensorflow/python/eager/BUILD:714:18: target '//tensorflow/python/eager:framework_for_generated_wrappers' is deprecated: Depending on this target can cause build dependency cycles. Depend on the fine-grained sub-targets instead.
WARNING: /home/sawantkumar/work/tensorflow/tensorflow/python/ops/distributions/BUILD:9:18: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.3.1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed 12399 targets (1251 packages loaded, 78333 targets configured).
INFO: Found 12399 targets...
ERROR: /root/.cache/bazel/_bazel_root/d4b0d2249c66f1993f1bdd593c2da087/external/boringssl/BUILD:146:11: Compiling src/ssl/d1_both.cc failed: (Exit 1): clang failed: error executing command (from target @boringssl//:ssl) /usr/lib/llvm-10/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 65 arguments skipped)
error: unknown warning option '-Werror=unused-but-set-variable'; did you mean '-Werror=unused-const-variable'? [-Werror,-Wunknown-warning-option]
error: unknown warning option '-Werror=unused-but-set-variable'; did you mean '-Werror=unused-const-variable'? [-Werror,-Wunknown-warning-option]
INFO: Elapsed time: 189.739s, Critical Path: 112.50s
INFO: 2310 processes: 1345 internal, 965 local.
FAILED: Build did NOT complete successfully
```

pkgoogle on (2024-08-01 21:37:04 UTC): I'm currently failing for different reasons ... I do want to understand better where this command is coming from. Hi @Nayana-ibm, how/where did you get this command? Did you get it from a resource/documentation?

github-actions[bot] on (2024-08-09 01:55:05 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-17 01:51:53 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-17 01:51:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72108"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72108"">No</a>

Nayana-ibm (Issue Creator) on (2024-08-30 03:19:32 UTC): Got this command from TensorFlow CI jobs.
https://btx.cloud.google.com/invocations/ebf9f385-7d9a-4cac-ba9e-d835e4d43ff0/targets/tensorflow%2Fofficial%2Fcontinuous%2Flinux_x86_cpu_max_python;config=default/log
https://github.com/tensorflow/tensorflow/blob/master/.bazelrc

gaikwadrahul8 on (2024-09-09 08:16:27 UTC): Hi, @Nayana-ibm 

I'm really sorry for inconvenience, issue got closed by our bot after 14 days due to no response within that window period, Anyways thank you for providing the details from where you got command mentioned in the issue template so we'll have to dig more into this issue, I have re-opened this issue.

Thank you for you cooperation and patience.

gaikwadrahul8 on (2024-09-21 12:00:19 UTC): Hi, @Nayana-ibm 

I apologize for the delayed response, I was trying to replicate the same bahavior from my end but I'm getting below error message to confirm did you encounter the similar error while running that command ? if not, may I know which configurations did you choose after running this command `./configure`

```
ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/2b31a38afcac928ac333a3bda4775519/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/avx512fp16_prod_microkernels.c failed: (Exit 1): gcc failed: error executing command (from target @XNNPACK//:avx512fp16_prod_microkernels) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 131 arguments skipped)
gcc: error: unrecognized command-line option '-mavx512fp16'; did you mean '-mavx512bf16'?
INFO: Elapsed time: 1722.191s, Critical Path: 92.97s
INFO: 8455 processes: 2809 internal, 5646 local.
FAILED: Build did NOT complete successfully
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-issue-72108/tensorflow$ 
```

Thank you for your cooperation and patience.

Nayana-ibm (Issue Creator) on (2024-09-27 12:39:56 UTC): @gaikwadrahul8 I have executed below command:

`bazel build --define tflite_with_xnnpack=false --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/..`

gaikwadrahul8 on (2024-09-30 10:07:58 UTC): Hi, @Nayana-ibm

I tried the exact command which you mentioned in the issue template initially so to confirm, is it exact same command or you made some changes in the initial command so I'll go ahead try updated command from my end ? 

Thank you for your cooperation and patience.

Nayana-ibm (Issue Creator) on (2024-10-02 19:15:15 UTC): @gaikwadrahul8 Let me simply command which reproduces above error:

 git clone https://github.com/tensorflow/tensorflow
 cd tensorflow/
 git checkout v2.16.1
 ./configure
```
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/local/bin/python3]:


Found possible Python library paths:
  /usr/local/lib/python3.12/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.12/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]: N
GCC will be used to compile TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

```
bazel build  -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...

gaikwadrahul8 on (2024-10-04 11:45:41 UTC): Hi, @Nayana-ibm

Thank you for providing the minimal command to replicate same behavior from our end, I tried that command from my end on `ubuntu 24.04` and build did not complete successfully got below error so we'll have to dig more into this issue.

Hi, @pkgoogle
Please take look into this issue. Thank you


```
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-72108/tensorflow$ bazel build -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...
INFO: Reading 'startup' options from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/gaikwadrahul/test-tflite-72108/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/site-packages --python_path=/home/gaikwadrahul/miniconda3/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/gaikwadrahul/test-tflite-72108/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/compiler/mlir/lite/stablehlo/BUILD:585:13: target '//tensorflow/compiler/mlir/lite/stablehlo:odml_to_stablehlo' is deprecated: odml_to_stablehlo is being deprecated, please use TFlite converter with flag: converter.target_spec.supported_ops =  [tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS]
WARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/core/kernels/BUILD:973:18: target '//tensorflow/core/kernels:bitcast_op' is deprecated: use //third_party/tensorflow/c/kernels:bitcast_op instead
WARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/python/eager/BUILD:713:18: target '//tensorflow/python/eager:framework_for_generated_wrappers' is deprecated: Depending on this target can cause build dependency cycles. Depend on the fine-grained sub-targets instead.
WARNING: /home/gaikwadrahul/test-tflite-72108/tensorflow/tensorflow/python/ops/distributions/BUILD:9:18: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
INFO: Analyzed 12218 targets (1 packages loaded, 8993 targets configured).
INFO: Found 12218 targets...
ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/36362ebe65b72c405dc49cfad486623b/external/dnnl/BUILD.bazel:38:11: Compiling src/cpu/x64/gemm/gemm_info.cpp failed: (Exit 1): gcc failed: error executing command (from target @dnnl//:dnnl_single_threaded) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 80 arguments skipped)
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = signed char; b_type = unsigned char; c_type = int]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:808:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  751 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = signed char; b_type = unsigned char; c_type = int]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:808:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  207 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  588 |         switch (data_traits<a_t>::data_type) {
      |         ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = signed char; b_type = signed char; c_type = int]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:811:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  751 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = signed char; b_type = signed char; c_type = int]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:811:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  207 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  588 |         switch (data_traits<a_t>::data_type) {
      |         ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = dnnl::impl::bfloat16_t; b_type = dnnl::impl::bfloat16_t; c_type = float]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:814:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  751 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = dnnl::impl::bfloat16_t; b_type = dnnl::impl::bfloat16_t; c_type = float]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:814:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  207 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  588 |         switch (data_traits<a_t>::data_type) {
      |         ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'bool dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::hasKernels() [with a_type = float; b_type = float; c_type = float]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:817:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  751 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:751:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp: In instantiation of 'void dnnl::impl::cpu::x64::gemm_info_t<a_type, b_type, c_type>::jit_init() [with a_type = float; b_type = float; c_type = float]':
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:817:16:   required from here
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  207 |     switch (data_traits<a_t>::data_type) {
      |     ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:207:5: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_data_type_undef' not handled in switch [-Werror=switch]
  588 |         switch (data_traits<a_t>::data_type) {
      |         ^~~~~~
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_f16' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_s32' not handled in switch [-Werror=switch]
external/dnnl/src/cpu/x64/gemm/gemm_info.cpp:588:9: error: enumeration value 'dnnl_u8' not handled in switch [-Werror=switch]
cc1plus: some warnings being treated as errors
INFO: Elapsed time: 1370.697s, Critical Path: 108.62s
INFO: 5316 processes: 1127 internal, 4189 local.
FAILED: Build did NOT complete successfully
(base) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-24:~/test-tflite-72108/tensorflow$ 
```

pkgoogle on (2024-10-18 22:56:48 UTC): This doesn't seem like a lite issue... @tilakrayal, can you please route this appropriately. Thanks.

"
2416906100,issue,closed,completed,Output says inference.so file does not exist when importing tfdf when it exists,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to use tensorflow and tensorflow decision forests. However, tfdf requires 2.15.0.
I keep getting the error saying inference.so file is not found when it exists

My tfdf version is 1.8.1.
I installed those. Tensorflow 2.15 imports fine, however when importing tfdf i get this error:
`import tensorflow_decision_forests as tfdf`
`
NotFoundError: c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\inference.so not found`
 

i tried this:
`import os
inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so'
os.environ['PATH'] += os.pathsep + inference_so_dir
import tensorflow_decision_forests as tfdf`

i still keep getting the error even when i add the files path 

### Standalone code to reproduce the issue

```shell
import os
inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so'
os.environ['PATH'] += os.pathsep + inference_so_dir
import tensorflow_decision_forests as tfdf
```


### Relevant log output

```shell
NotFoundError                             Traceback (most recent call last)
Cell In[9], line 4
      2 inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so'
      3 os.environ['PATH'] += os.pathsep + inference_so_dir
----> 4 import tensorflow_decision_forests as tfdf

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\__init__.py:64
     60 from tensorflow_decision_forests.tensorflow import check_version
     62 check_version.check_version(__version__, compatible_tf_versions)
---> 64 from tensorflow_decision_forests import keras
     65 from tensorflow_decision_forests.component import py_tree
     66 from tensorflow_decision_forests.component.builder import builder

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\keras\__init__.py:53
     15 """"""Decision Forest in a Keras Model.
     16 
     17 Usage example:
   (...)
     48 
     49 """"""
     51 from typing import Callable, List
---> 53 from tensorflow_decision_forests.keras import core
     54 from tensorflow_decision_forests.keras import wrappers
     56 # Utility classes

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\keras\core.py:62
     60 from tensorflow_decision_forests.component.inspector import inspector as inspector_lib
     61 from tensorflow_decision_forests.component.tuner import tuner as tuner_lib
---> 62 from tensorflow_decision_forests.keras import core_inference
     63 from tensorflow_decision_forests.tensorflow import cc_logging
     64 from tensorflow_decision_forests.tensorflow import core as tf_core

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\keras\core_inference.py:36
     34 from tensorflow_decision_forests.tensorflow import core_inference as tf_core
     35 from tensorflow_decision_forests.tensorflow import tf_logging
---> 36 from tensorflow_decision_forests.tensorflow.ops.inference import api as tf_op
     37 from yggdrasil_decision_forests.learner import abstract_learner_pb2
     38 from yggdrasil_decision_forests.learner.multitasker import multitasker_pb2

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\api.py:179
    177 from tensorflow_decision_forests.component.inspector import inspector as inspector_lib
    178 from tensorflow_decision_forests.tensorflow import tf1_compatibility
--> 179 from tensorflow_decision_forests.tensorflow.ops.inference import op
    180 from yggdrasil_decision_forests.dataset import data_spec_pb2
    181 from yggdrasil_decision_forests.model import abstract_model_pb2

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\op.py:15
      1 # Copyright 2021 Google LLC.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
---> 15 from tensorflow_decision_forests.tensorflow.ops.inference.op_dynamic import *

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\op_dynamic.py:24
     22 except Exception as e:
     23   check_version.info_fail_to_load_custom_op(e, ""inference.so"")
---> 24   raise e
     26 # Importing all the symbols.
     27 module = sys.modules[__name__]

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\op_dynamic.py:21
     18 import sys
     20 try:
---> 21   ops = tf.load_op_library(resource_loader.get_path_to_datafile(""inference.so""))
     22 except Exception as e:
     23   check_version.info_fail_to_load_custom_op(e, ""inference.so"")

File c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\load_library.py:54, in load_op_library(library_filename)
     31 @tf_export('load_op_library')
     32 def load_op_library(library_filename):
     33   """"""Loads a TensorFlow plugin, containing custom ops and kernels.
     34 
     35   Pass ""library_filename"" to a platform-specific mechanism for dynamically
   (...)
     52     RuntimeError: when unable to load the library or get the python wrappers.
     53   """"""
---> 54   lib_handle = py_tf.TF_LoadLibrary(library_filename)
     55   try:
     56     wrappers = _pywrap_python_op_gen.GetPythonWrappers(
     57         py_tf.TF_GetOpList(lib_handle))

NotFoundError: c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\inference.so not found
```
",HashemZn-04,2024-07-18 16:39:13+00:00,['Venkat6871'],2024-09-28 03:10:36+00:00,2024-08-10 01:54:18+00:00,https://github.com/tensorflow/tensorflow/issues/72106,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2241164148, 'issue_id': 2416906100, 'author': 'AbdulQadeer-55', 'body': 'To address the issue where the `inference.so` file is not found when importing `tensorflow_decision_forests`, despite the file existing, follow these steps:\r\n\r\n1. **Verify the File Path**: Ensure the file path is correct and that `inference.so` is actually located there.\r\n\r\n2. **Check File Permissions**: Make sure the file and the directories leading to it have the correct permissions for reading.\r\n\r\n3. **Environment Variables**: Ensure the environment variables are set correctly. Sometimes, simply appending the path might not be sufficient.\r\n\r\n4. **Reinstall the Package**: There might be a problem with the installation. Reinstalling `tensorflow_decision_forests` could resolve this.\r\n\r\n### Steps to Resolve the Issue\r\n\r\n1. **Verify File Existence and Path**\r\n   ```python\r\n   import os\r\n\r\n   file_path = \'C:\\\\Users\\\\hashe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tensorflow_decision_forests\\\\tensorflow\\\\ops\\\\inference\\\\inference.so\'\r\n   print(os.path.isfile(file_path))  # This should return True if the file exists\r\n   ```\r\n\r\n2. **Check Permissions**\r\n   Ensure that your user account has read permissions for the file and the directories.\r\n\r\n3. **Set Environment Variables Properly**\r\n   Instead of appending the path, set the `LD_LIBRARY_PATH` (for Linux) or the `PATH` environment variable directly.\r\n   ```python\r\n   import os\r\n\r\n   inference_so_dir = \'C:\\\\Users\\\\hashe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tensorflow_decision_forests\\\\tensorflow\\\\ops\\\\inference\'\r\n   os.environ[\'PATH\'] = inference_so_dir + os.pathsep + os.environ[\'PATH\']\r\n\r\n   import tensorflow_decision_forests as tfdf\r\n   ```\r\n\r\n4. **Reinstall the Package**\r\n   Uninstall and then reinstall the `tensorflow_decision_forests` package.\r\n   ```sh\r\n   pip uninstall tensorflow_decision_forests\r\n   pip install tensorflow_decision_forests\r\n   ```\r\n\r\n5. **Check for Dependencies**\r\n   Ensure all dependencies for `tensorflow_decision_forests` and `inference.so` are installed.\r\n   ```sh\r\n   pip install tensorflow==2.15.0 tensorflow_decision_forests==1.8.1\r\n   ```\r\n\r\n6. **Update TensorFlow and TensorFlow Decision Forests**\r\n   Ensure you have the latest compatible versions of TensorFlow and TensorFlow Decision Forests.\r\n   ```sh\r\n   pip install --upgrade tensorflow tensorflow_decision_forests\r\n   ```\r\n\r\n### Script to Validate and Import\r\nHere\'s an example script that includes all the above checks:\r\n\r\n```python\r\nimport os\r\n\r\n# Verify the file exists\r\nfile_path = \'C:\\\\Users\\\\hashe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tensorflow_decision_forests\\\\tensorflow\\\\ops\\\\inference\\\\inference.so\'\r\nif not os.path.isfile(file_path):\r\n    raise FileNotFoundError(f""The file {file_path} does not exist."")\r\n\r\n# Set the PATH environment variable\r\ninference_so_dir = \'C:\\\\Users\\\\hashe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tensorflow_decision_forests\\\\tensorflow\\\\ops\\\\inference\'\r\nos.environ[\'PATH\'] = inference_so_dir + os.pathsep + os.environ[\'PATH\']\r\n\r\n# Import tensorflow_decision_forests\r\ntry:\r\n    import tensorflow_decision_forests as tfdf\r\n    print(""Successfully imported tensorflow_decision_forests."")\r\nexcept Exception as e:\r\n    print(f""Error importing tensorflow_decision_forests: {e}"")\r\n```', 'created_at': datetime.datetime(2024, 7, 20, 14, 12, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244403087, 'issue_id': 2416906100, 'author': 'Venkat6871', 'body': 'Hi @**HashemZn-04** ,\r\n- I tried to run your code on Colab using TF v2.15 and faced the same issue. And i tried on 2.16.2 then it working fine. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/60ef5be2d7f9492a806ab590e51d801d/72106_2-15-0-2-16-2-v.ipynb) here for reference. Could you please refer [this](https://github.com/tensorflow/decision-forests/issues/210) issue. And please post this issue on https://github.com/tensorflow/[decision-forests](https://github.com/tensorflow/decision-forests) as this issue belongs to decision forests.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 23, 7, 2, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251577629, 'issue_id': 2416906100, 'author': 'BOT-TI', 'body': 'I have the same issue. I checked that the inference.so exists and it did. Also I have checked the tfdf is compatible with tf. Also I tried other versions. No luck.\r\n\r\n**C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found**', 'created_at': datetime.datetime(2024, 7, 25, 23, 41, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266326014, 'issue_id': 2416906100, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 3, 1, 52, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935147, 'issue_id': 2416906100, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935172, 'issue_id': 2416906100, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72106"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72106"">No</a>', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287163543, 'issue_id': 2416906100, 'author': 'benjaminlawsUNL', 'body': ""Hi!\r\n\r\n I am experiencing the exact problem shown in the initial comment and believe I successfully completed the debugging steps outlined. I have since tried to use tensor flow 2.16.2 as recommended by Venkat in this conversation, but according to the compatibility table found at https://www.tensorflow.org/decision_forests/known_issues, version requires the decision forest version 1.9.2. When I try to pip install this version I get the error 'ERROR: Could not find a version that satisfies the requirement tensorflow_decision_forests==1.9.2 (from versions: 1.8.1)'.\r\n\r\nI would be very grateful for any assistance.\r\n\r\nThank you so much,\r\n\r\nBen"", 'created_at': datetime.datetime(2024, 8, 13, 21, 24, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380381031, 'issue_id': 2416906100, 'author': 'Gpaio574', 'body': 'I tried all the above and checked everything and tried many versions of the related libs but nothing fixed the issue\r\nI even try it in new pc and new vmware and in google colab and same problem with the tf.compat.v1 error', 'created_at': datetime.datetime(2024, 9, 28, 3, 10, 35, tzinfo=datetime.timezone.utc)}]","AbdulQadeer-55 on (2024-07-20 14:12:41 UTC): To address the issue where the `inference.so` file is not found when importing `tensorflow_decision_forests`, despite the file existing, follow these steps:

1. **Verify the File Path**: Ensure the file path is correct and that `inference.so` is actually located there.

2. **Check File Permissions**: Make sure the file and the directories leading to it have the correct permissions for reading.

3. **Environment Variables**: Ensure the environment variables are set correctly. Sometimes, simply appending the path might not be sufficient.

4. **Reinstall the Package**: There might be a problem with the installation. Reinstalling `tensorflow_decision_forests` could resolve this.

### Steps to Resolve the Issue

1. **Verify File Existence and Path**
   ```python
   import os

   file_path = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so'
   print(os.path.isfile(file_path))  # This should return True if the file exists
   ```

2. **Check Permissions**
   Ensure that your user account has read permissions for the file and the directories.

3. **Set Environment Variables Properly**
   Instead of appending the path, set the `LD_LIBRARY_PATH` (for Linux) or the `PATH` environment variable directly.
   ```python
   import os

   inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference'
   os.environ['PATH'] = inference_so_dir + os.pathsep + os.environ['PATH']

   import tensorflow_decision_forests as tfdf
   ```

4. **Reinstall the Package**
   Uninstall and then reinstall the `tensorflow_decision_forests` package.
   ```sh
   pip uninstall tensorflow_decision_forests
   pip install tensorflow_decision_forests
   ```

5. **Check for Dependencies**
   Ensure all dependencies for `tensorflow_decision_forests` and `inference.so` are installed.
   ```sh
   pip install tensorflow==2.15.0 tensorflow_decision_forests==1.8.1
   ```

6. **Update TensorFlow and TensorFlow Decision Forests**
   Ensure you have the latest compatible versions of TensorFlow and TensorFlow Decision Forests.
   ```sh
   pip install --upgrade tensorflow tensorflow_decision_forests
   ```

### Script to Validate and Import
Here's an example script that includes all the above checks:

```python
import os

# Verify the file exists
file_path = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so'
if not os.path.isfile(file_path):
    raise FileNotFoundError(f""The file {file_path} does not exist."")

# Set the PATH environment variable
inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference'
os.environ['PATH'] = inference_so_dir + os.pathsep + os.environ['PATH']

# Import tensorflow_decision_forests
try:
    import tensorflow_decision_forests as tfdf
    print(""Successfully imported tensorflow_decision_forests."")
except Exception as e:
    print(f""Error importing tensorflow_decision_forests: {e}"")
```

Venkat6871 (Assginee) on (2024-07-23 07:02:05 UTC): Hi @**HashemZn-04** ,
- I tried to run your code on Colab using TF v2.15 and faced the same issue. And i tried on 2.16.2 then it working fine. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/60ef5be2d7f9492a806ab590e51d801d/72106_2-15-0-2-16-2-v.ipynb) here for reference. Could you please refer [this](https://github.com/tensorflow/decision-forests/issues/210) issue. And please post this issue on https://github.com/tensorflow/[decision-forests](https://github.com/tensorflow/decision-forests) as this issue belongs to decision forests.

Thank you!

BOT-TI on (2024-07-25 23:41:58 UTC): I have the same issue. I checked that the inference.so exists and it did. Also I have checked the tfdf is compatible with tf. Also I tried other versions. No luck.

**C:\Users\user\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow_decision_forests\tensorflow\ops\inference\inference.so not found**

github-actions[bot] on (2024-08-03 01:52:04 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-10 01:54:18 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-10 01:54:20 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72106"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72106"">No</a>

benjaminlawsUNL on (2024-08-13 21:24:37 UTC): Hi!

 I am experiencing the exact problem shown in the initial comment and believe I successfully completed the debugging steps outlined. I have since tried to use tensor flow 2.16.2 as recommended by Venkat in this conversation, but according to the compatibility table found at https://www.tensorflow.org/decision_forests/known_issues, version requires the decision forest version 1.9.2. When I try to pip install this version I get the error 'ERROR: Could not find a version that satisfies the requirement tensorflow_decision_forests==1.9.2 (from versions: 1.8.1)'.

I would be very grateful for any assistance.

Thank you so much,

Ben

Gpaio574 on (2024-09-28 03:10:35 UTC): I tried all the above and checked everything and tried many versions of the related libs but nothing fixed the issue
I even try it in new pc and new vmware and in google colab and same problem with the tf.compat.v1 error

"
2416373862,issue,closed,completed,"cublas64_11.dll,cublasLt64_11.dll,cufft64_10.dll,cusparse64_11.dll,cudnn64_8.dll not found issue","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Window

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9.2.1

### GPU model and memory

GTX 1650

### Current behavior?

GPU not found error , 


### Standalone code to reproduce the issue

```shell
tf.config.list_physical_devices('GPU')
2024-07-18 18:41:41.835861: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2024-07-18 18:41:41.836592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2024-07-18 18:41:41.837735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2024-07-18 18:41:42.846464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2024-07-18 18:41:42.847045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2024-07-18 18:41:42.847273: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```


### Relevant log output

_No response_",Tarun0000,2024-07-18 13:21:51+00:00,['tilakrayal'],2024-09-18 14:53:31+00:00,2024-08-03 01:52:06+00:00,https://github.com/tensorflow/tensorflow/issues/72097,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2239344436, 'issue_id': 2416373862, 'author': 'tilakrayal', 'body': '@Tarun0000,\r\nCould you please provide the steps which you were followed to install the tensorflow. Also tensorflow 2.13 is the old version. Could you please try to install the latest tensorflow 2.17.\r\n\r\nTensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-)\r\n\r\nhttps://www.tensorflow.org/install/pip#windows-native_1\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 19, 14, 39, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703571, 'issue_id': 2416373862, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266326029, 'issue_id': 2416373862, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 3, 1, 52, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266326062, 'issue_id': 2416373862, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72097"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72097"">No</a>', 'created_at': datetime.datetime(2024, 8, 3, 1, 52, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2358698148, 'issue_id': 2416373862, 'author': 'Tarun0000', 'body': 'hey thanks for update , i have on more counter query , by using those step suggested by you i able to activate gpu in wsl -linux , but my counter query is how i use that gpu in windows system , window conda environment', 'created_at': datetime.datetime(2024, 9, 18, 14, 53, 29, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-19 14:39:10 UTC): @Tarun0000,
Could you please provide the steps which you were followed to install the tensorflow. Also tensorflow 2.13 is the old version. Could you please try to install the latest tensorflow 2.17.

TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install [TensorFlow in WSL2](https://tensorflow.org/install/pip#windows-%5Bwsl2%5D), or install tensorflow-cpu and, optionally, try the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin#tensorflow-directml-plugin-)

https://www.tensorflow.org/install/pip#windows-native_1

Thank you!

github-actions[bot] on (2024-07-27 01:51:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-03 01:52:05 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-03 01:52:08 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72097"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72097"">No</a>

Tarun0000 (Issue Creator) on (2024-09-18 14:53:29 UTC): hey thanks for update , i have on more counter query , by using those step suggested by you i able to activate gpu in wsl -linux , but my counter query is how i use that gpu in windows system , window conda environment

"
2415792036,issue,open,,Build Error on aarch64 AWS Graviton3 with Ubuntu 22.04 for TensorFlow v2.17.0 with mkl_aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

clang version 17.0.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm encountering build errors when trying to build TensorFlow v2.17.0 from source on an AWS Graviton3 instance (aarch64 architecture) running Ubuntu 22.04. The build fails with errors related to the **MakeOneDnnStream** function.

Expected behavior: The build should complete successfully without any errors.

### Standalone code to reproduce the issue

```shell
1. Set up an AWS Graviton3 instance with Ubuntu 22.04.
2. Clone the TensorFlow repository and checkout version 2.17.0:
   
   git clone https://github.com/tensorflow/tensorflow.git
   cd tensorflow
   git checkout v2.17.0
   
3. Install Bazel and other dependencies as per the TensorFlow build documentation.
4. Run the build command:
   ```bash
   bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable --jobs=48 --local_cpu_resources=26 --verbose_failures
   ```

Any guidance or fixes to resolve this build error would be greatly appreciated.
```


### Relevant log output

```shell
(tf-venv) user@ip-xxx-xx-x-xxx:~/work_dirr/tensorflow$ taskset -c 6-31 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable  --jobs=48 --local_cpu_resources=26 --verbose_failures
INFO: Reading 'startup' options from /home/deepesh/work_dirr/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=113
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages --python_path=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --repo_env=CC=/usr/lib/llvm-17/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl_aarch64 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=build_with_mkl_aarch64=true --define=build_with_openmp=true --define=build_with_acl=true -c opt
INFO: Found applicable config definition build:opt in file /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (1 packages loaded, 3758 targets configured).
INFO: Found 1 target...
ERROR: /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/external/local_xla/xla/service/cpu/BUILD:1665:11: Compiling xla/service/cpu/onednn_matmul.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla//xla/service/cpu:onednn_matmul) 
  (cd /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/execroot/org_tensorflow && \
  exec env - \
    CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang \
    PATH=/home/deepesh/.cache/bazelisk/downloads/bazelbuild/bazel-6.5.0-linux-arm64/bin:/home/deepesh/work_dirr/tf-venv/bin:/home/deepesh/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/bin/remote-cli:/home/deepesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 \
    PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-17/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' -DHAVE_BUILTIN_THREAD_POINTER '-DLLVM_NATIVE_ARCH=""AArch64""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=19' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""19.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=""local_xla""' -iquote external/local_xla -iquote bazel-out/aarch64-opt/bin/external/local_xla -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_tsl -iquote bazel-out/aarch64-opt/bin/external/local_tsl -iquote external/ml_dtypes -iquote bazel-out/aarch64-opt/bin/external/ml_dtypes -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/llvm-project -iquote bazel-out/aarch64-opt/bin/external/llvm-project -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/mkl_dnn_acl_compatible -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible -iquote external/compute_library -iquote bazel-out/aarch64-opt/bin/external/compute_library -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/intn -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/compute_library/include/_virtual_includes/include -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/aarch64-opt/bin/external/eigen_archive/mkl_include -isystem external/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem external/mkl_dnn_acl_compatible/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include -isystem external/mkl_dnn_acl_compatible/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src -isystem external/mkl_dnn_acl_compatible/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/common -isystem external/mkl_dnn_acl_compatible/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem external/compute_library/arm_compute/runtime -isystem bazel-out/aarch64-opt/bin/external/compute_library/arm_compute/runtime -isystem external/compute_library/src/core/NEON/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/convolution/common -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common -isystem external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem external/compute_library/src/core/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/cpu/kernels/assembly -isystem external/compute_library/src/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/cpu/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/arm_conv -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv -isystem external/compute_library/src/core/NEON/kernels/arm_gemm -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-march=armv8-a+sve' '-msve-vector-bits=256' -O3 -Wno-gnu-offsetof-extensions -Wno-unused-but-set-variable '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 '-DDNNL_AARCH64_USE_ACL=1' -DENABLE_ONEDNN_OPENMP '-DXLA_CPU_USE_ACL=1' -fexceptions -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/local_xla/xla/service/cpu/onednn_matmul.cc -o bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o)
# Configuration: 389168362472d4b0f209a68c4f568adffc1a33297c68acb48d71fb4bd10724d2
# Execution platform: @local_execution_config_platform//:platform
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:199:9: warning: 'LOG' macro redefined [-Wmacro-redefined]
  199 | #define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)
      |         ^
external/local_tsl/tsl/platform/default/logging.h:165:9: note: previous definition is here
  165 | #define LOG(severity) _TF_LOG_##severity
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:237:9: warning: 'LOG_EVERY_N' macro redefined [-Wmacro-redefined]
  237 | #define LOG_EVERY_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:278:9: note: previous definition is here
  278 | #define LOG_EVERY_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:245:9: warning: 'LOG_FIRST_N' macro redefined [-Wmacro-redefined]
  245 | #define LOG_FIRST_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:284:9: note: previous definition is here
  284 | #define LOG_FIRST_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:253:9: warning: 'LOG_EVERY_POW_2' macro redefined [-Wmacro-redefined]
  253 | #define LOG_EVERY_POW_2(severity) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:290:9: note: previous definition is here
  290 | #define LOG_EVERY_POW_2(severity)                         \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:265:9: warning: 'LOG_EVERY_N_SEC' macro redefined [-Wmacro-redefined]
  265 | #define LOG_EVERY_N_SEC(severity, n_seconds) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:300:9: note: previous definition is here
  300 | #define LOG_EVERY_N_SEC(severity, n_seconds)                      \
      |         ^
external/local_xla/xla/service/cpu/onednn_matmul.cc:270:24: error: no matching function for call to 'MakeOneDnnStream'
  270 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_matmul.cc:359:24: error: no matching function for call to 'MakeOneDnnStream'
  359 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
5 warnings and 2 errors generated.
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 720.177s, Critical Path: 142.22s
INFO: 3322 processes: 82 internal, 3240 local.
FAILED: Build did NOT complete successfully
```
",deepeshfujitsu,2024-07-18 09:02:02+00:00,['Venkat6871'],2024-08-07 17:42:25+00:00,,https://github.com/tensorflow/tensorflow/issues/72081,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('comp:mkl', 'MKL related issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2238739387, 'issue_id': 2415792036, 'author': 'Venkat6871', 'body': '@learning-to-play', 'created_at': datetime.datetime(2024, 7, 19, 9, 21, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273976043, 'issue_id': 2415792036, 'author': 'snadampal', 'body': 'I see you are using `--config=mkl_aarch64`, please use this config `--config=mkl_aarch64_threadpool` instead. \r\nfor complete steps to build from sources using gcc or llvm toolchain, please refer to this user guide, these instructions are for v2.16.1, but should work for v2.17 as well. Please try and let me know if you still see issues.\r\nhttps://github.com/aws/aws-graviton-getting-started/blob/main/machinelearning/tensorflow.md#building-tensorflow-from-sources', 'created_at': datetime.datetime(2024, 8, 7, 17, 30, 43, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-19 09:21:29 UTC): @learning-to-play

snadampal on (2024-08-07 17:30:43 UTC): I see you are using `--config=mkl_aarch64`, please use this config `--config=mkl_aarch64_threadpool` instead. 
for complete steps to build from sources using gcc or llvm toolchain, please refer to this user guide, these instructions are for v2.16.1, but should work for v2.17 as well. Please try and let me know if you still see issues.
https://github.com/aws/aws-graviton-getting-started/blob/main/machinelearning/tensorflow.md#building-tensorflow-from-sources

"
2413550126,issue,open,,"Memory usage with tf.data pipeline (HDF5, TFRecords)","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

RTX A6000 Ada

### Current behavior?

Hello.

I am wondering about memory usage for the Tensorflow data API. I am running into OOM issues when I run on very large (360,000 files, >40 MB per file) files. I cannot even iterate through the dataset appropriately; forget about training a (Keras) model.

Here is memory usage (using `psutil` and `memory_info().rss`) over 5 ""epochs"" where I iterate over a dummy HDF5 dataset. The drops that create the sawtooth pattern are each ""epoch,"" where I just iterate over the dataset 5 times.

![image](https://github.com/user-attachments/assets/93e2f201-9c3a-411f-83c6-5861d378fdd1)

Is this expected behavior? I can repeat this memory curve with `TFRecordDataset` with and without `interleave`.

[Gist for creating dummy hdf5 files](https://gist.github.com/dryglicki/dec05492b73416e3829a0440c6024793)
[Gist for reading dummy hdf5 files with Dataset API](https://gist.github.com/dryglicki/cf0a52dd31af3358d5e3cd5351c51f13)

Be warned, that to get the plotting to work without wrecking Tensorflow, you need to use matplotlib <3.8, since that is when it incorporates Numpy 2.0 -- I used MPL v3.7.3.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import os ; import sys
from pathlib import Path
import glob

import tensorflow as tf

import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt

import h5py
import psutil
import gc


def unpack_hdf5(hdf5_file):
    '''
    Simple and quick unpack of an hdf5 dummy file for tensorflow.
    '''
    with h5py.File(hdf5_file.numpy().decode('utf-8'), 'r') as h5:
        priors = tf.convert_to_tensor(h5.get('priors')[...], tf.float32)
        forecast = tf.convert_to_tensor(h5.get('forecast')[...], tf.float32)
        model = tf.convert_to_tensor(h5.get('model')[...], tf.float32)

    return priors, model, forecast

def interleave_wrapper(hdf5_file,
        prior_shape,
        model_shape,
        forecast_shape):
    '''
    Wrapper for py_function to work with interleave. Interleave requires a dataset upon output.
    '''
    p, m, f = tf.py_function(unpack_hdf5, [hdf5_file],
                             Tout = [tf.float32, tf.float32, tf.float32])

    p.set_shape(prior_shape)
    m.set_shape(model_shape)
    f.set_shape(forecast_shape)

    return tf.data.Dataset.zip( (tf.data.Dataset.from_tensors((p,m)).map(
        lambda p, m: {'priors' : p, 'model' : m}, num_parallel_calls = tf.data.AUTOTUNE),
        tf.data.Dataset.from_tensors(f)) )

def create_dataset_interleave(file_list,
        priors_shape,
        model_shape,
        forecast_shape,
        batch_size = 32):
    '''
    Create a dataset using pre-globbed file list
    '''

    return tf.data.Dataset.from_tensor_slices(file_list).interleave(
            lambda x: interleave_wrapper(x, priors_shape, model_shape, forecast_shape),
            cycle_length = tf.data.AUTOTUNE,
            num_parallel_calls = tf.data.AUTOTUNE,
            deterministic = False).batch(
                    batch_size, drop_remainder=True)

def main():
    '''
    Program to monitor memory usage of HDF5 file reader.
    '''
    p = psutil.Process(os.getpid())
    hdf_directory = 'hdf_files'
    file_list = list(glob.glob(f'{hdf_directory}/*hdf5'))
    print(len(file_list))

    test_file = file_list[0]
    hdf_vars = ['priors', 'model', 'forecast']
    shapes = {}
    with h5py.File(test_file, 'r') as h5:
        for var in hdf_vars:
            dict_var = f'{var}_shape'
            shapes[dict_var] = h5[var].shape

    ds = create_dataset_interleave(file_list, **shapes)

    rssUse = []
    batch_numbers = []
    for epoch in range(5):
        for ii, batch in enumerate(ds):
            batch_numbers.append((epoch+1)*(ii+1))
            X, Y = batch
            print('Batch: ',ii)
            print(X['model'].shape)
            rssUse.append(p.memory_info().rss / (1024 ** 2))
        gc.collect()

    for b, r in zip(batch_numbers, rssUse):
        print(f'{b:04d}: {r}')

    fig = plt.figure(figsize = (4,3))
    ax = fig.add_subplot()

    ax.plot(rssUse, label = 'rss')
    ax.legend()
    ax.set_ylabel('Memory usage [MB]')
    ax.set_xlabel('Step')

    fig.savefig('memory_use_hdf5.png', dpi=200, bbox_inches='tight')


if __name__ == '__main__':
    main()
```


### Relevant log output

_No response_",dryglicki,2024-07-17 12:52:42+00:00,"['jsimsa', 'aaudiber']",2024-07-18 14:01:59+00:00,,https://github.com/tensorflow/tensorflow/issues/72014,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('comp:data', 'tf.data related issues'), ('type:performance', 'Performance Issue'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2233466743, 'issue_id': 2413550126, 'author': 'dryglicki', 'body': ""If I add `.cache()` to the end of the `tf.data` pipeline, here's what I get. I believe this is expected behavior.\r\n\r\n![image](https://github.com/user-attachments/assets/78ebc440-af6d-4680-9999-1eda85dff9ff)"", 'created_at': datetime.datetime(2024, 7, 17, 14, 28, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233514650, 'issue_id': 2413550126, 'author': 'dryglicki', 'body': ""And lastly, if I remove all instances of `tf.data.AUTOTUNE` and all `num_parallel_calls` from the pipeline:\r\n![image](https://github.com/user-attachments/assets/69bb7164-6baf-4c99-b069-5253ecf12951)\r\n\r\nI can accept there's going to be overheard for lazy loading, but if the behind-the-scenes profiler is going to rocket up the memory in the first epoch, then... I'm not sure I expected that."", 'created_at': datetime.datetime(2024, 7, 17, 14, 49, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235987656, 'issue_id': 2413550126, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow [v2.15](https://colab.research.google.com/gist/tilakrayal/88cf5316a9360bed54480472683aa6dc/untitled2013.ipynb), v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f40b2c52c5b557b3080c10877204d4a2/untitled2014.ipynb) and screenshot for the reference.\r\n**2.17:**\r\n![image](https://github.com/user-attachments/assets/a81a99c8-7813-473b-898b-b5058e02b43e)\r\n**2.15:**\r\n![image](https://github.com/user-attachments/assets/20afe1b7-c1bf-486e-9afa-cc3f6463c89e)\r\n\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 18, 8, 57, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236598452, 'issue_id': 2413550126, 'author': 'dryglicki', 'body': ""I really need to drive this home. I tried to replicate, to the best of my ability, this workflow in a Pytorch DataLoader object. [See gist here.](https://gist.github.com/dryglicki/afab460dd85ccbfef8df917665d532c2) Test was performed with Pytorch 2.3.1, Python 3.11.9.\r\n\r\nPlease note that I used the exact same HDF5 dataset in the test above with the same batch size (32). I trimmed off the last remainder batch. I followed the Torch examples [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) and [here](https://pytorch.org/docs/stable/data.html).\r\n\r\nThe memory usage, using the exact same methodology as the initial post:\r\n![image](https://github.com/user-attachments/assets/02e5d529-df79-4f80-871c-e1bf0b5fcf34)\r\n\r\nThere are oscillations. That's fine. I expect that.\r\n\r\nWhere the solutions obviously differ is that there is not memory growth with step."", 'created_at': datetime.datetime(2024, 7, 18, 13, 56, 55, tzinfo=datetime.timezone.utc)}]","dryglicki (Issue Creator) on (2024-07-17 14:28:42 UTC): If I add `.cache()` to the end of the `tf.data` pipeline, here's what I get. I believe this is expected behavior.

![image](https://github.com/user-attachments/assets/78ebc440-af6d-4680-9999-1eda85dff9ff)

dryglicki (Issue Creator) on (2024-07-17 14:49:48 UTC): And lastly, if I remove all instances of `tf.data.AUTOTUNE` and all `num_parallel_calls` from the pipeline:
![image](https://github.com/user-attachments/assets/69bb7164-6baf-4c99-b069-5253ecf12951)

I can accept there's going to be overheard for lazy loading, but if the behind-the-scenes profiler is going to rocket up the memory in the first epoch, then... I'm not sure I expected that.

tilakrayal on (2024-07-18 08:57:31 UTC): I was able to reproduce the issue on tensorflow [v2.15](https://colab.research.google.com/gist/tilakrayal/88cf5316a9360bed54480472683aa6dc/untitled2013.ipynb), v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f40b2c52c5b557b3080c10877204d4a2/untitled2014.ipynb) and screenshot for the reference.
**2.17:**
![image](https://github.com/user-attachments/assets/a81a99c8-7813-473b-898b-b5058e02b43e)
**2.15:**
![image](https://github.com/user-attachments/assets/20afe1b7-c1bf-486e-9afa-cc3f6463c89e)


Thank you!

dryglicki (Issue Creator) on (2024-07-18 13:56:55 UTC): I really need to drive this home. I tried to replicate, to the best of my ability, this workflow in a Pytorch DataLoader object. [See gist here.](https://gist.github.com/dryglicki/afab460dd85ccbfef8df917665d532c2) Test was performed with Pytorch 2.3.1, Python 3.11.9.

Please note that I used the exact same HDF5 dataset in the test above with the same batch size (32). I trimmed off the last remainder batch. I followed the Torch examples [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) and [here](https://pytorch.org/docs/stable/data.html).

The memory usage, using the exact same methodology as the initial post:
![image](https://github.com/user-attachments/assets/02e5d529-df79-4f80-871c-e1bf0b5fcf34)

There are oscillations. That's fine. I expect that.

Where the solutions obviously differ is that there is not memory growth with step.

"
2413019379,issue,closed,completed,No dashboards are active for the current data set.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.1.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA RTX

### Current behavior?

I am training a model for PPO training using RL with CARLA simulator. While training events files are generated but when I want to visualize the data the browser shows no data to show. I want to show the graph regarding reward and model training.

### Standalone code to reproduce the issue

```shell
#!/bin/env python
import gym
import macad_gym  # noqa F401
import argparse
import os
from pprint import pprint

import cv2
import ray
import ray.tune as tune
from gym.spaces import Box, Discrete
from macad_agents.rllib.env_wrappers import wrap_deepmind
from macad_agents.rllib.models import register_mnih15_net

from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy #0.8.5
from ray.rllib.models.catalog import ModelCatalog
from ray.rllib.models.preprocessors import Preprocessor
from ray.tune import register_env
import time
import tensorflow as tf
from tensorboardX import SummaryWriter

# from tensorflow.compat.v1 import ConfigProto
# from tensorflow.compat.v1 import InteractiveSession
# config = tf.ConfigProto()
# config.gpu_options.allow_growth = True
# session = InteractiveSession(config=config)
# config = tf.ConfigProto()
# config.gpu_options.per_process_gpu_memory_fraction = 0.7
# tf.keras.backend.set_session(tf.Session(config=config));

parser = argparse.ArgumentParser()
parser.add_argument(
    ""--env"",
    default=""PongNoFrameskip-v4"",
    help=""Name Gym env. Used only in debug mode. Default=PongNoFrameskip-v4"")
parser.add_argument(
    ""--disable-comet"",
    action=""store_true"",
    help=""Disables comet logging. Used for local smoke tests"")
parser.add_argument(
    ""--num-workers"",
    default=1, #2 #fix
    type=int,
    help=""Num workers (CPU cores) to use"")
parser.add_argument(
    ""--num-gpus"", default=1, type=int, help=""Number of gpus to use. Default=2"")
parser.add_argument(
    ""--sample-bs-per-worker"", #one iteration
    default=1024,
    type=int,
    help=""Number of samples in a batch per worker. Default=50"")
parser.add_argument(
    ""--train-bs"",
    default=128,
    type=int,
    help=""Train batch size. Use as per available GPU mem. Default=500"")
parser.add_argument(
    ""--envs-per-worker"",
    default=1,
    type=int,
    help=""Number of env instances per worker. Default=10"")
parser.add_argument(
    ""--notes"",
    default=None,
    help=""Custom experiment description to be added to comet logs"")
parser.add_argument(
    ""--model-arch"",
    default=""mnih15"",
    help=""Model architecture to use. Default=mnih15"")
parser.add_argument(
    ""--num-steps"",
    default=4000000,
    type=int,
    help=""Number of steps to train. Default=20M"")
parser.add_argument(
    ""--num-iters"",
    default=300,
    type=int,
    help=""Number of training iterations. Default=20"")
parser.add_argument(
    ""--log-graph"",
    action=""store_true"",
    help=""Write TF graph on Tensorboard for debugging"",default=True)
parser.add_argument(
    ""--num-framestack"",
    type=int,
    default=4,
    help=""Number of obs frames to stack"")
parser.add_argument(
    ""--debug"", action=""store_true"", help=""Run in debug-friendly mode"", default=False)
parser.add_argument(
    ""--redis-address"",
    default=None,
    help=""Address of ray head node. Be sure to start ray with""
    ""ray start --redis-address <...> --num-gpus<.> before running this script"")
parser.add_argument(
    ""--use-lstm"", action=""store_true"", help=""Append a LSTM cell to the model"",default=True)



args = parser.parse_args()

model_name = args.model_arch
if model_name == ""mnih15"":
    register_mnih15_net()  # Registers mnih15
else:
    print(""Unsupported model arch. Using default"")
    register_mnih15_net()
    model_name = ""mnih15""

# Used only in debug mode
env_name = ""HomoNcomIndePOIntrxMASS3CTWN3-v0""
env = gym.make(env_name)
# print (env.spec.max_episode_steps,""-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+"")
# env.spec.max_episode_steps=1024
# print (env.spec.max_episode_steps,""-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+"")

env_actor_configs = env.configs

num_framestack = args.num_framestack
# env_config[""env""][""render""] = False


def env_creator(env_config):
    
    import macad_gym
    env = gym.make(""HomoNcomIndePOIntrxMASS3CTWN3-v0"")

    # Apply wrappers to: convert to Grayscale, resize to 84 x 84,
    # stack frames & some more op
    env = wrap_deepmind(env, dim=84, num_framestack=num_framestack)
    return env


register_env(env_name, lambda config: env_creator(config))

# Placeholder to enable use of a custom pre-processor
class ImagePreproc(Preprocessor):
    def _init_shape(self, obs_space, options):
        self.shape = (84, 84, 3)  # Adjust third dim if stacking frames
        return self.shape

    def transform(self, observation):
        observation = cv2.resize(observation, (self.shape[0], self.shape[1]))
        # cv2.imshow(""observation"",observation)
        return observation
def transform(self, observation):
        observation = cv2.resize(observation, (self.shape[0], self.shape[1]))
        return observation

ModelCatalog.register_custom_preprocessor(""sq_im_84"", ImagePreproc)


if args.redis_address is not None:
    # num_gpus (& num_cpus) must not be provided when connecting to an
    # existing cluster
    ray.init(redis_address=args.redis_address,object_store_memory=10**10,log_to_driver=False)
else:
    ray.init(num_gpus=args.num_gpus,object_store_memory=10**10,log_to_driver=False)

config = {
    # Model and preprocessor options.
    ""model"": {
        ""custom_model"": model_name,
        ""custom_options"": {
            # Custom notes for the experiment
            ""notes"": {
                ""args"": vars(args)
            },
        },
        # NOTE:Wrappers are applied by RLlib if custom_preproc is NOT specified
        ""custom_preprocessor"": ""sq_im_84"",
        ""dim"": 84,
        ""free_log_std"": False,  # if args.discrete_actions else True,
        ""grayscale"": True,
        # conv_filters to be used with the custom CNN model.
        # ""conv_filters"": [[16, [4, 4], 2], [32, [3, 3], 2], [16, [3, 3], 2]]
    },
    # preproc_pref is ignored if custom_preproc is specified
    # ""preprocessor_pref"": ""deepmind"",

    # env_config to be passed to env_creator
    
    ""env_config"": env_actor_configs
}

def default_policy():
    env_actor_configs[""env""][""render""] = True

    config = {
    # Model and preprocessor options.
    ""model"": {
        ""custom_model"": model_name,
        ""custom_options"": {
            # Custom notes for the experiment
            ""notes"": {
                ""args"": vars(args)
            },
        },
        # NOTE:Wrappers are applied by RLlib if custom_preproc is NOT specified
        ""custom_preprocessor"": ""sq_im_84"",
        ""dim"": 84,
        ""free_log_std"": False,  # if args.discrete_actions else True,
        ""grayscale"": True,
        # conv_filters to be used with the custom CNN model.
        # ""conv_filters"": [[16, [4, 4], 2], [32, [3, 3], 2], [16, [3, 3], 2]]
    },


    # Should use a critic as a baseline (otherwise don't use value baseline;
    # required for using GAE).
    ""use_critic"": True,
    # If true, use the Generalized Advantage Estimator (GAE)
    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
    ""use_gae"": True,
    # The GAE(lambda) parameter.
    ""lambda"": 1.0,
    # Initial coefficient for KL divergence.
    ""kl_coeff"": 0.3,
    # Size of batches collected from each worker.
    ""rollout_fragment_length"": 128,
    # Number of timesteps collected for each SGD round. This defines the size
    # of each SGD epoch.
    # ""train_batch_size"": 4000,
    # Total SGD batch size across all devices for SGD. This defines the
    # minibatch size within each epoch.
    ""sgd_minibatch_size"": 64,
    # Whether to shuffle sequences in the batch when training (recommended).
    ""shuffle_sequences"": True,
    # Number of SGD iterations in each outer loop (i.e., number of epochs to
    # execute per train batch).
    ""num_sgd_iter"": 8,
    # Stepsize of SGD.
    ""lr"": 5e-5,
    # Learning rate schedule.
    # ""lr_schedule"": None,
    # Share layers for value function. If you set this to True, it's important
    # to tune vf_loss_coeff.
    ""vf_share_layers"": False,
    # Coefficient of the value function loss. IMPORTANT: you must tune this if
    # you set vf_share_layers: True.
    ""vf_loss_coeff"": 1.0,
    # Coefficient of the entropy regularizer.
    ""entropy_coeff"": 0.1,
    # Decay schedule for the entropy regularizer.
    ""entropy_coeff_schedule"": None,
    # PPO clip parameter.
    ""clip_param"": 0.3,
    # Clip param for the value function. Note that this is sensitive to the
    # scale of the rewards. If your expected V is large, increase this.
    ""vf_clip_param"": 10.0,
    # If specified, clip the global norm of gradients by this amount.
    ""grad_clip"": None,
    # Target value for KL divergence.
    ""kl_target"": 0.03,
    # Whether to rollout ""complete_episodes"" or ""truncate_episodes"".
    ""batch_mode"": ""complete_episodes"",
    # Which observation filter to apply to the observation.
    ""observation_filter"": ""NoFilter"",
    # Uses the sync samples optimizer instead of the multi-gpu one. This is
    # usually slower, but you might want to try it if you run into issues with
    # the default optimizer.
    ""simple_optimizer"": False,
    # Use PyTorch as framework?
    ""use_pytorch"": False,

    # Discount factor of the MDP.
    ""gamma"": 0.99,
    # Number of steps after which the episode is forced to terminate. Defaults
    # to `env.spec.max_episode_steps` (if present) for Gym envs.
    ""horizon"": 512,
    # Calculate rewards but don't reset the environment when the horizon is
    # hit. This allows value estimation and RNN state to span across logical
    # episodes denoted by horizon. This only has an effect if horizon != inf.
    ""soft_horizon"": True,
    # Don't set 'done' at the end of the episode. Note that you still need to
    # set this if soft_horizon=True, unless your env is actually running
    # forever without returning done=True.
    ""no_done_at_end"": True,
    ""monitor"": True,




    # System params.
    # Should be divisible by num_envs_per_worker
    ""sample_batch_size"":
     args.sample_bs_per_worker,
    ""train_batch_size"":
    args.train_bs,
    # ""rollout_fragment_length"": 128,
    ""num_workers"":
    args.num_workers,
    # Number of environments to evaluate vectorwise per worker.
    ""num_envs_per_worker"":
    args.envs_per_worker,
    ""num_cpus_per_worker"":
    1,
    ""num_gpus_per_worker"":
    1,
    # ""eager_tracing"": True,

    # # Learning params.
    # ""grad_clip"":
    # 40.0,
    # ""clip_rewards"":
    # True,
    # either ""adam"" or ""rmsprop""
    ""opt_type"":
    ""adam"",
    # ""lr"":
    # 0.003,
    ""lr_schedule"": [
        [0, 0.0006],
        [20000000, 0.000000000001],  # Anneal linearly to 0 from start 2 end
    ],
    # rmsprop considered
    ""decay"":
    0.5,
    ""momentum"":
    0.0,
    ""epsilon"":
    0.1,
    # # balancing the three losses
    # ""vf_loss_coeff"":
    # 0.5,  # Baseline loss scaling
    # ""entropy_coeff"":
    # -0.01,

    # preproc_pref is ignored if custom_preproc is specified
    # ""preprocessor_pref"": ""deepmind"",
   # ""gamma"": 0.99,

    ""use_lstm"": args.use_lstm,
    # env_config to be passed to env_creator
    ""env"":{
        ""render"": True
    },
    # ""in_evaluation"": True,
    # ""evaluation_num_episodes"": 1,
    ""env_config"": env_actor_configs
    }






    # pprint (config)
    return (PPOTFPolicy, Box(0.0, 255.0, shape=(84, 84, 3)), Discrete(9),config)

# pprint (args.checkpoint_path)
# pprint(os.path.isfile(args.checkpoint_path))


if args.debug:
    # For checkpoint loading and retraining (not used in this script)
    experiment_spec = tune.Experiment(
        ""multi-carla/"" + args.model_arch,
        ""PPO"",
        # restore=args.checkpoint_path,
        # timesteps_total is init with None (not 0) which causes issue
        # stop={""timesteps_total"": args.num_steps},
        stop={""timesteps_since_restore"": args.num_steps},
        config=config,
        # checkpoint_freq=1000, #1000
        # checkpoint_at_end=True,
        resources_per_trial={
            ""cpu"": 1,
            ""gpu"": 1
        })

    experiment_spec = tune.run_experiments({
            ""MA-Inde-PPO-SSUI3CCARLA"": {
                ""run"": ""PPO"",
                ""env"": env_name,
                ""stop"": {
                    
                    ""training_iteration"": args.num_iters,
                    ""timesteps_total"": args.num_steps,
                    ""episodes_total"": 1024,
                },
                # ""restore"":args.checkpoint_path,   
                ""config"": {

                    ""log_level"": ""DEBUG"",
                   # ""num_sgd_iter"": 10,  # Enables Experience Replay
                    ""multiagent"": {
                        ""policies"": {
                            id: default_policy()
                            for id in env_actor_configs[""actors""].keys()
                                # print()
                        },
                        ""policy_mapping_fn"":
                        tune.function(lambda agent_id: agent_id),
                        ""policies_to_train"": [""car2"",""car3""],
                    },
                    ""env_config"": env_actor_configs,
                    ""num_workers"": args.num_workers,
                    ""num_envs_per_worker"": args.envs_per_worker,
                    ""sample_batch_size"": args.sample_bs_per_worker,
                    ""train_batch_size"": args.train_bs,
                    ""horizon"": 512,

                },
                ""checkpoint_freq"": 5,
                ""checkpoint_at_end"": True,


            }
        })

  

else:

    experiment_spec = tune.Experiment(
        ""multi-carla/"" + args.model_arch,
        ""PPO"",
        stop={""timesteps_since_restore"": args.num_steps},
        config=config,
        resources_per_trial={
            ""cpu"": 1,
            ""gpu"": 1
        })

    experiment_spec = tune.run_experiments({
            ""MA-Inde-PPO-SSUI3CCARLA"": {
                ""run"": ""PPO"",
                ""env"": env_name,
                ""stop"": {
                    
                    ""training_iteration"": args.num_iters,
                    ""timesteps_total"": args.num_steps,
                    ""episodes_total"": 1024,
                    
                },

                ""config"": {

                    ""log_level"": ""DEBUG"",
                   # ""num_sgd_iter"": 10,  # Enables Experience Replay
                    ""multiagent"": {
                        ""policies"": {
                            id: default_policy()
                            for id in env_actor_configs[""actors""].keys()
                        },
                        ""policy_mapping_fn"":
                        tune.function(lambda agent_id: agent_id),
                        ""policies_to_train"": [""car2"",""car3""], 
                    },
                    ""env_config"": env_actor_configs,
                    ""num_workers"": args.num_workers,
                    ""num_envs_per_worker"": args.envs_per_worker,
                    ""sample_batch_size"": args.sample_bs_per_worker,
                    ""train_batch_size"": args.train_bs,
                    #""horizon"": 512, #yet to be fixed

                },
                ""checkpoint_freq"": 5,
                ""checkpoint_at_end"": True,


            }
        })


ray.shutdown()
```


### Relevant log output

_No response_",SExpert12,2024-07-17 08:41:16+00:00,['Venkat6871'],2024-07-24 06:20:56+00:00,2024-07-24 06:20:53+00:00,https://github.com/tensorflow/tensorflow/issues/72006,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('TF 2.1', 'for tracking issues in 2.1 release')]","[{'comment_id': 2238080139, 'issue_id': 2413019379, 'author': 'Venkat6871', 'body': 'Hi **@SExpert12** ,\r\n- It is not looking like tensorflow repo. Could you please provide more information about issue? And You are using old version(2.1.0) here, We are not supporting this version. Could you please execute your code using Latest Version (2.17) and let us know if the issue still persists? \r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 19, 4, 13, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2238196321, 'issue_id': 2413019379, 'author': 'SExpert12', 'body': 'Sure sir. Let me try and I will update you.\r\nThank you for your help.', 'created_at': datetime.datetime(2024, 7, 19, 5, 22, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2239019318, 'issue_id': 2413019379, 'author': 'SExpert12', 'body': 'Hi @Venkat6871,\r\nI did update latest tensorflow. There is a huge interdependence in the packages and it has started to show me errors regarding other packages. So I think I should stick to tensorflow 2.1.0 as suggested by that repo. There would be version of tensorboard which is compatible with the tensorflow 2.1.0. May be I should take that route.', 'created_at': datetime.datetime(2024, 7, 19, 12, 19, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244552956, 'issue_id': 2413019379, 'author': 'Venkat6871', 'body': 'Hi **@SExpert12** ,\r\n- This is not a bug or feature request, for any further queries you may open this issue in tf discussion [forum](https://discuss.tensorflow.org/) as there is a larger community there.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 23, 8, 15, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246997510, 'issue_id': 2413019379, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72006"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72006"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 6, 20, 55, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-19 04:13:14 UTC): Hi **@SExpert12** ,
- It is not looking like tensorflow repo. Could you please provide more information about issue? And You are using old version(2.1.0) here, We are not supporting this version. Could you please execute your code using Latest Version (2.17) and let us know if the issue still persists? 

Thank you!

SExpert12 (Issue Creator) on (2024-07-19 05:22:51 UTC): Sure sir. Let me try and I will update you.
Thank you for your help.

SExpert12 (Issue Creator) on (2024-07-19 12:19:23 UTC): Hi @Venkat6871,
I did update latest tensorflow. There is a huge interdependence in the packages and it has started to show me errors regarding other packages. So I think I should stick to tensorflow 2.1.0 as suggested by that repo. There would be version of tensorboard which is compatible with the tensorflow 2.1.0. May be I should take that route.

Venkat6871 (Assginee) on (2024-07-23 08:15:58 UTC): Hi **@SExpert12** ,
- This is not a bug or feature request, for any further queries you may open this issue in tf discussion [forum](https://discuss.tensorflow.org/) as there is a larger community there.

Thank you!

google-ml-butler[bot] on (2024-07-24 06:20:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72006"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72006"">No</a>

"
2412995154,issue,closed,completed,Tensorflow crash driver CUDA of GeForce RTX 4090,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

Win10x64 Pro

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

Geforce RTX 4090 24Gb

### Current behavior?

I have two Geforce RTX 4090 on one motherboard. I'm running two processes for training Tensorflow models on different maps.  After some time, the process crashes on GPU 0, but on GPU 1 it works without failures. I am attaching a screenshot of the error. How to fix it?
Driver Version : 537.34
CUDA Version : 12.2
TensorFlow version: 2.10.0
OS: Win10x64 Pro
![ Cuda](https://github.com/user-attachments/assets/b4472c10-8471-4669-b83d-52b3a1664052)



### Standalone code to reproduce the issue

```shell
for tr_opt in train_options_list:
    with tf.device('/GPU:0'):
        files = os.listdir(dir_data_path + param_dir + '/' + tr_opt)
        num_files = len(files)

        df_agg = pd.DataFrame()
        for i in range (0, num_files):
          current_file = pd.read_excel(dir_data_path + param_dir + '/' + tr_opt + '/' + files[i])
          current_file.columns = ['', param_dir]
          df_agg = pd.concat([df_agg, current_file])
    
        data = df_agg[param_dir].values.reshape(-1, 1)

        train_set_scale = scaler.transform(data)

        #  
        n_steps = 180
        n_features = 1
        train_matrix = create_matrix(train_set_scale, n_steps)

        tf.keras.backend.clear_session()
        np.random.seed(41)

        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.InputLayer(input_shape=(n_steps, n_features)))
        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)))
        model.add(tf.keras.layers.ReLU())
        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8, return_sequences=True)))
        model.add(tf.keras.layers.ReLU())
        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)))
        model.add(tf.keras.layers.ReLU())
        model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features)))

        model.compile(optimizer='adam', loss=""mse"")

        net_history = model.fit(train_matrix, train_matrix, epochs=10, batch_size=150)
        model.save(save_path + 'model.h5')
```


### Relevant log output

_No response_",333Random333,2024-07-17 08:29:13+00:00,['tilakrayal'],2024-08-09 01:55:10+00:00,2024-08-09 01:55:07+00:00,https://github.com/tensorflow/tensorflow/issues/72002,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:gpu', 'GPU related issues'), ('TF 2.10', '')]","[{'comment_id': 2235176758, 'issue_id': 2412995154, 'author': 'arthurflor23', 'body': ""Wow, I also have two Geforce RTX 4090 GPUs on a motherboard and had the same problem on Linux with Python 3.11 and TensorFlow 2.15.1.\r\n\r\nFor me, the whole system crashes and only a restart solves the problem. I don't know if this is the root issue, but for now, I'm just using one RTX 4090. In addition, I was using driver 535, and now 550 (still testing)."", 'created_at': datetime.datetime(2024, 7, 18, 2, 28, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235728425, 'issue_id': 2412995154, 'author': '333Random333', 'body': ""> Wow, I also have two Geforce RTX 4090 GPUs on a motherboard and had the same problem on Linux with Python 3.11 and TensorFlow 2.15.1.\r\n> \r\n> For me, the whole system crashes and only a restart solves the problem. I don't know if this is the root issue, but for now, I'm just using one RTX 4090. In addition, I was using driver 535, and now 550 (still testing).\r\n\r\nIs this not observed with one RTX 4090? \r\nIn my code I use tensorflow to isolate the training calls using:\r\n```\r\nwith tf.device('/GPU:0'):\r\n    model.fit(...)\r\n```\r\nAnd also for the second process, but with tf.device('/GPU:1').\r\n\r\nAt the beginning of both the scripts I use tensorflow dynamic memory allocation:\r\n```\r\ngpus = tf.config.list_physical_devices('GPU')\r\ntf.config.set_visible_devices(gpus[0], 'GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\nAnd also for the second process gpus[1].\r\n\r\nHowever, these actions do not solve the problem..."", 'created_at': datetime.datetime(2024, 7, 18, 6, 32, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236196927, 'issue_id': 2412995154, 'author': 'arthurflor23', 'body': ""> Is this not observed with one RTX 4090?\r\n\r\nI'm still testing as the issue arises randomly, but for now, no problem.\r\n \r\n> ```\r\n> gpus = tf.config.list_physical_devices('GPU')\r\n> tf.config.set_visible_devices(gpus[0], 'GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> ```\r\n\r\nI'm doing the same.\r\n\r\nActually, I think the problem might be related to the driver version (530 series). So, I'm also using the 550 series to see if it resolves the issue."", 'created_at': datetime.datetime(2024, 7, 18, 10, 46, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2250333008, 'issue_id': 2412995154, 'author': 'tilakrayal', 'body': '@333Random333,\r\nIs this crash happening with the tensorflow 2.10 only, or with the latest version tensorflow v2.17 as well. As mentioned above it is working with the 530 and 540 series, I have checked on the 530 and observed it is working. Kindly check and let us know. Thank you!', 'created_at': datetime.datetime(2024, 7, 25, 13, 32, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345736, 'issue_id': 2412995154, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005123, 'issue_id': 2412995154, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005163, 'issue_id': 2412995154, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72002"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72002"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 9, tzinfo=datetime.timezone.utc)}]","arthurflor23 on (2024-07-18 02:28:05 UTC): Wow, I also have two Geforce RTX 4090 GPUs on a motherboard and had the same problem on Linux with Python 3.11 and TensorFlow 2.15.1.

For me, the whole system crashes and only a restart solves the problem. I don't know if this is the root issue, but for now, I'm just using one RTX 4090. In addition, I was using driver 535, and now 550 (still testing).

333Random333 (Issue Creator) on (2024-07-18 06:32:17 UTC): Is this not observed with one RTX 4090? 
In my code I use tensorflow to isolate the training calls using:
```
with tf.device('/GPU:0'):
    model.fit(...)
```
And also for the second process, but with tf.device('/GPU:1').

At the beginning of both the scripts I use tensorflow dynamic memory allocation:
```
gpus = tf.config.list_physical_devices('GPU')
tf.config.set_visible_devices(gpus[0], 'GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
```
And also for the second process gpus[1].

However, these actions do not solve the problem...

arthurflor23 on (2024-07-18 10:46:43 UTC): I'm still testing as the issue arises randomly, but for now, no problem.
 

I'm doing the same.

Actually, I think the problem might be related to the driver version (530 series). So, I'm also using the 550 series to see if it resolves the issue.

tilakrayal (Assginee) on (2024-07-25 13:32:06 UTC): @333Random333,
Is this crash happening with the tensorflow 2.10 only, or with the latest version tensorflow v2.17 as well. As mentioned above it is working with the 530 and 540 series, I have checked on the 530 and observed it is working. Kindly check and let us know. Thank you!

github-actions[bot] on (2024-08-02 01:53:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-09 01:55:07 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-09 01:55:09 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72002"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/72002"">No</a>

"
2412050721,issue,closed,completed,TensorFlow embedded in esp32,"Hello,

I am looking to use TensorFlow Lite embedded in a microprocessor for a specific project. I will not be using computer vision or cameras; my goal is to identify the passage of people using a distance sensor.

We will have only one input of type float and one output of type int. We will input the measured distance and, after a series of measurements, receive information on whether one, two, or no people are passing by.

Although it is a basic application, I am facing difficulties in finding materials that can help. Most of the projects I found are outdated (the libraries have changed significantly, and many no longer work) or are aimed at camera usage. Even the courses I purchased focus exclusively on cameras.

If anyone knows where I can find supporting materials or could provide some assistance, I would greatly appreciate it.

Thank you for your attention!
",Ligeirinho00,2024-07-16 21:10:07+00:00,['Venkat6871'],2024-09-19 04:11:21+00:00,2024-09-19 02:00:19+00:00,https://github.com/tensorflow/tensorflow/issues/71965,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:micro', 'Related to TensorFlow Lite Microcontrollers')]","[{'comment_id': 2328072735, 'issue_id': 2412050721, 'author': 'sawantkumar', 'body': 'Hi @Ligeirinho00 ,\r\n\r\nYour use case is very specific so i am not sure if there are existing examples related to your case. But you can find several examples of running tflite models on embedded devices at https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples.  I will suggest you to post your issue at https://github.com/tensorflow/tflite-micro for better help.', 'created_at': datetime.datetime(2024, 9, 4, 7, 2, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102560, 'issue_id': 2412050721, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826843, 'issue_id': 2412050721, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826899, 'issue_id': 2412050721, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71965"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71965"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 21, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-09-04 07:02:20 UTC): Hi @Ligeirinho00 ,

Your use case is very specific so i am not sure if there are existing examples related to your case. But you can find several examples of running tflite models on embedded devices at https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples.  I will suggest you to post your issue at https://github.com/tensorflow/tflite-micro for better help.

github-actions[bot] on (2024-09-12 01:58:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:19 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 02:00:21 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71965"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71965"">No</a>

"
2411393111,issue,open,,`tf.data.Dataset.prefetch()` error with basic usage,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The most basic usage of `tf.data.Dataset.prefetch()` raises an error. 

The `buffer_size` argument is documented as requiring a int64 tensor:

> `buffer_size` 	
> A [tf.int64](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [tf.data.AUTOTUNE](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.

https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch

Probably related to https://github.com/tensorflow/tensorflow/issues/71744, The ""eager fallback"" codepath is broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x = np.arange(5)
y = np.arange(5)

ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)

ds.prefetch(tf.constant(1, dtype = 'int64'))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1259, in prefetch
    return prefetch_op._prefetch(  # pylint: disable=protected-access
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 28, in _prefetch
    return _PrefetchDataset(input_dataset, buffer_size, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 46, in __init__
    variant_tensor = gen_dataset_ops.prefetch_dataset(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6001, in prefetch_dataset
    return prefetch_dataset_eager_fallback(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6072, in prefetch_dataset_eager_fallback
    legacy_autotune = _execute.make_bool(legacy_autotune, ""legacy_autotune"")
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 172, in make_bool
    raise TypeError(""Expected bool for argument '%s' not %s."" %
TypeError: Expected bool for argument 'legacy_autotune' not <tf.Tensor: shape=(), dtype=bool, numpy=False>.
```
",t-kalinowski,2024-07-16 15:02:16+00:00,"['jsimsa', 'aaudiber']",2024-07-17 13:38:50+00:00,,https://github.com/tensorflow/tensorflow/issues/71937,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:data', 'tf.data related issues'), ('regression issue', 'To spot regression issues in latest version'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2233351687, 'issue_id': 2411393111, 'author': 'tilakrayal', 'body': '@t-kalinowski,\r\nThank you for reporting the issue. I tried to execute the same code and faced the same issue. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ddc3b91080f52e39b80c772d2911962f/untitled2009.ipynb).  Please allow some time to depdive into this issue and will update on the same. Thank you!', 'created_at': datetime.datetime(2024, 7, 17, 13, 38, 19, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-17 13:38:19 UTC): @t-kalinowski,
Thank you for reporting the issue. I tried to execute the same code and faced the same issue. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ddc3b91080f52e39b80c772d2911962f/untitled2009.ipynb).  Please allow some time to depdive into this issue and will update on the same. Thank you!

"
2411098064,issue,open,,Tensorflow distributed + DTensor approach for large outer product,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.9

### Custom code

Yes

### OS platform and distribution

MacOS

### Mobile device

N/A

### Python version

3.10

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

Hey,

I am computing a large matrix of size $(\mathrm{N_{samples}}, \mathrm{N_{parameters}})$ , which corresponds to a jacobian for each sample. I am obtaining this matrix by using a distributed strategy: `tf.distribute.MirroredStrategy` and calculating $N_\mathrm{samples}/N_\mathrm{devices}$ jacobians on all my devices. I then gather along the first axis to construct the full matrix $X$ from which I calculate the matrix $Y = X X^T$ of size $(\mathrm{N_{samples}} ,\mathrm{N_{samples}})$. 

I am looking for a faster way to perform this calculation by sharding along the direction of $N_\mathrm{parameters}$ and performing the calculation like this:

$$
X X^T =\sum^{\mathrm{N_gpus}1}_{g=0} X_g X_g^T
$$

See this reference for the specific layout:
<img width=""887"" alt=""image"" src=""https://github.com/user-attachments/assets/47a117c4-906a-4fa3-bf7d-832de810ed7c"">

What I tried is to calculate my jacobians with `strategy.run`, gather them and then copy to a mesh to perform the matrix multiplication. However, I feel like this approach is probably not optimal. 

What is the best way to achieve the same within tensorflow's distributed framework? Can I do something similar to MPI's all-to-all primitive to reorder the data across all my devices and perform the matmul described above?

I added a MWE below that outlines my thinking, in my actual code `train_step` involves a complicated calculation and a call to `tf.jacobians` over all my trainable variables.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.experimental import dtensor
import numpy as np

print('TensorFlow version:', tf.__version__)


def configure_virtual_cpus(ncpu):
    phy_devices = tf.config.list_physical_devices('CPU')
    tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

# Create virtual devices
number_of_devices = 6
configure_virtual_cpus(number_of_devices)
DEVICES = [f'CPU:{i}' for i in range(number_of_devices)]
# Use a mirrorer strategy
strategy = tf.distribute.MirroredStrategy(DEVICES)

number_of_samples_per_device = 4
# Make sure that the number of parameters is divisble by the number of devices
number_of_parameters = 2 * number_of_devices
# Create 1D mesh
mesh = dtensor.create_mesh([(""x"", number_of_devices), ], devices=DEVICES)
# Shard along Parameter direction
layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)
tf.random.set_seed(1000)


@tf.function()
def train_step():
    # We create a tensor of (Ns, Np,) as a fictional Jacobian
    return tf.random.uniform((number_of_samples_per_device, number_of_parameters))


@tf.function()
def distributed_step():
    params = strategy.run(train_step)
    # Gathering on axis 1 gives (Ns * N_devices, Np)
    return strategy.gather(params, axis=0)


jacobians = distributed_step()
# Calculate matmul without using a mesh
X_g_full = tf.matmul(jacobians, tf.transpose(jacobians))
# Create a mesh of size (N_devices, )
a = dtensor.copy_to_mesh(jacobians,
                         layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))
# Each device gets (Ns * N_devices, Np / N_devices) samples
my_dtensor = dtensor.relayout(a, layout=layout)
for component_tensor in dtensor.unpack(my_dtensor):
    print(""Device:"", component_tensor.device, "","", component_tensor.shape)
# Perform matmul on mesh.
X_g = tf.matmul(my_dtensor, tf.transpose(my_dtensor))
# They match
print(np.allclose(X_g_full.numpy(), X_g.numpy()))
```


### Relevant log output

```shell
Device: /job:localhost/replica:0/task:0/device:CPU:0 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:1 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:2 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:3 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:4 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:5 , (24, 2)
True
```
",therooler,2024-07-16 13:07:28+00:00,['Venkat6871'],2024-07-17 08:46:10+00:00,,https://github.com/tensorflow/tensorflow/issues/71930,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2232769784, 'issue_id': 2411098064, 'author': 'tilakrayal', 'body': 'I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d35205fcdd90d904cec4cd0ce03e568d/untitled2008.ipynb).', 'created_at': datetime.datetime(2024, 7, 17, 8, 46, 2, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-17 08:46:02 UTC): I was able to reproduce the issue on tensorflow v2.15, v2.17 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d35205fcdd90d904cec4cd0ce03e568d/untitled2008.ipynb).

"
2410970339,issue,closed,completed,IntelliSence in VS Code - Keras module not found by the IDE but is present,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi everyone, I noticed a problem when i import the keras module from tensorflow. There is no completion code with keras module in VS Code but is present. When i try to execute this code:
```python
import tensorflow as tf

print(tf.keras)
print(tf.keras.__version__)
```


**Output:**
```bash
<KerasLazyLoader>
3.4.1
```

*IntelliSense doesn't work*
![Capture d'cran 2024-07-16 134956](https://github.com/user-attachments/assets/991e78ab-3b0a-4345-a84b-7a1dca2851d9)

</br>

I didn't see any other open issues mentioning this problem. Thanks for your feedback.

### Standalone code to reproduce the issue

```shell
Code present in the ""current behavior"" section
```


### Relevant log output

_No response_",BREBION-Mathis,2024-07-16 12:05:25+00:00,['tilakrayal'],2024-09-20 20:25:00+00:00,2024-07-29 12:30:27+00:00,https://github.com/tensorflow/tensorflow/issues/71927,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2230822325, 'issue_id': 2410970339, 'author': 'BREBION-Mathis', 'body': ""I've noticed that I'm far from being the only one to have had this problem, as many bug tickets have been opened mentioning this problem. I don't understand why, after so many versions since then, the problem is still present in tensorflow version 2.17. Especially since tinkering with the installation is not a viable solution when you want to use the project in a professional context."", 'created_at': datetime.datetime(2024, 7, 16, 12, 55, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233320298, 'issue_id': 2410970339, 'author': 'tilakrayal', 'body': '@BREBION-Mathis,\r\nKeras is migrated to Keras 3 with multi backend support.\r\n\r\nCould you please install Keras separately using **pip install keras** and `import keras` directly and let us know the outcome with keras.layers etc. Thank you!', 'created_at': datetime.datetime(2024, 7, 17, 13, 24, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233425870, 'issue_id': 2410970339, 'author': 'BREBION-Mathis', 'body': ""@tilakrayal\r\n\r\nIt works, thanks ! But what is the difference between using the keras module directly in tensorflow with `tf.keras.layers...` or directly using the `keras` module by importing it alone ?\r\n\r\nI also noticed that the `keras.preprocessing` module doesn't have a **text** module, which is a drawback when you want to work with transformers. So you have to look for it directly in `tf.keras.preprocessing.text.Tokenizer` but we lose all IntelliSence..."", 'created_at': datetime.datetime(2024, 7, 17, 14, 10, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255755854, 'issue_id': 2410970339, 'author': 'tilakrayal', 'body': '@BREBION-Mathis,\r\nWhen we are importing the keras from tensorflow till version 2.15, keras 2.0 is imported. Whereas when we try to import the tensorflow 2.16 it imports keras3.0 directly.\r\n\r\n**TF 2.15: Keras2.0**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n```\r\n\r\n**TF 2.16: Keras 3.0**\r\n```python\r\nimport tensorflow as tf\r\nimport keras\r\n```\r\n\r\n\r\nKeras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch\r\nhttps://keras.io/keras_3/\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 29, 12, 7, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255804647, 'issue_id': 2410970339, 'author': 'BREBION-Mathis', 'body': ""@tilakrayal\r\n\r\nThat was a nuance I didn't have with the newer versions of TF.\r\nThanks for your time and I think we can change the status as closed."", 'created_at': datetime.datetime(2024, 7, 29, 12, 30, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255804723, 'issue_id': 2410970339, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71927"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71927"">No</a>', 'created_at': datetime.datetime(2024, 7, 29, 12, 30, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364565829, 'issue_id': 2410970339, 'author': 'RaghavTheGreat1', 'body': ""> @BREBION-Mathis, Keras is migrated to Keras 3 with multi backend support.\r\n> \r\n> Could you please install Keras separately using **pip install keras** and `import keras` directly and let us know the outcome with keras.layers etc. Thank you!\r\n\r\nThis saved my day! I was learning from a book and it imported the keras from tensorflow. Now I know, why I wasn't able to see keras anywhere when I was going through the source code of the tensorflow package."", 'created_at': datetime.datetime(2024, 9, 20, 20, 24, 59, tzinfo=datetime.timezone.utc)}]","BREBION-Mathis (Issue Creator) on (2024-07-16 12:55:41 UTC): I've noticed that I'm far from being the only one to have had this problem, as many bug tickets have been opened mentioning this problem. I don't understand why, after so many versions since then, the problem is still present in tensorflow version 2.17. Especially since tinkering with the installation is not a viable solution when you want to use the project in a professional context.

tilakrayal (Assginee) on (2024-07-17 13:24:41 UTC): @BREBION-Mathis,
Keras is migrated to Keras 3 with multi backend support.

Could you please install Keras separately using **pip install keras** and `import keras` directly and let us know the outcome with keras.layers etc. Thank you!

BREBION-Mathis (Issue Creator) on (2024-07-17 14:10:03 UTC): @tilakrayal

It works, thanks ! But what is the difference between using the keras module directly in tensorflow with `tf.keras.layers...` or directly using the `keras` module by importing it alone ?

I also noticed that the `keras.preprocessing` module doesn't have a **text** module, which is a drawback when you want to work with transformers. So you have to look for it directly in `tf.keras.preprocessing.text.Tokenizer` but we lose all IntelliSence...

tilakrayal (Assginee) on (2024-07-29 12:07:08 UTC): @BREBION-Mathis,
When we are importing the keras from tensorflow till version 2.15, keras 2.0 is imported. Whereas when we try to import the tensorflow 2.16 it imports keras3.0 directly.

**TF 2.15: Keras2.0**
```python
import tensorflow as tf
from tensorflow import keras
```

**TF 2.16: Keras 3.0**
```python
import tensorflow as tf
import keras
```


Keras 3 implements the full Keras API and makes it available with TensorFlow, JAX, and PyTorch
https://keras.io/keras_3/

Thank you!

BREBION-Mathis (Issue Creator) on (2024-07-29 12:30:27 UTC): @tilakrayal

That was a nuance I didn't have with the newer versions of TF.
Thanks for your time and I think we can change the status as closed.

google-ml-butler[bot] on (2024-07-29 12:30:29 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71927"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71927"">No</a>

RaghavTheGreat1 on (2024-09-20 20:24:59 UTC): This saved my day! I was learning from a book and it imported the keras from tensorflow. Now I know, why I wasn't able to see keras anywhere when I was going through the source code of the tensorflow package.

"
2410755862,issue,open,,wasm-ld: error: --shared-memory is disallowed by c_api.o because it was not compiled with 'atomics' or 'bulk-memory' features.,"https://github.com/emscripten-core/emsdk/issues/1424

`wasm_cc_binary`with `tensorflow/lite/examples/minimal` failed

my question is can `wasm_cc_binary` without `threads = ""emscripten""`

thank you very much in advance",Yongle-Fu,2024-07-16 10:08:45+00:00,"['mattsoulanille', 'pkgoogle', 'sawantkumar']",2025-01-05 09:44:33+00:00,,https://github.com/tensorflow/tensorflow/issues/71922,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2232331337, 'issue_id': 2410755862, 'author': 'tilakrayal', 'body': '@Yongle-Fu,\r\nCould you please let us know which tensorflow version you are trying to use for executing the mentioned code? Thank you!', 'created_at': datetime.datetime(2024, 7, 17, 3, 57, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232413731, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': '> @Yongle-Fu, Could you please let us know which tensorflow version you are trying to use for executing the mentioned code? Thank you!\r\n\r\nthank you for your respnose, i use latest branch on master', 'created_at': datetime.datetime(2024, 7, 17, 4, 51, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237201531, 'issue_id': 2410755862, 'author': 'pkgoogle', 'body': 'Hi @Yongle-Fu, can you please provide reproducible steps & please share your environment as well. Thanks.', 'created_at': datetime.datetime(2024, 7, 18, 18, 12, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237951232, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': '> Hi @Yongle-Fu, can you please provide reproducible steps & please share your environment as well. Thanks.\r\n\r\nsure, thank you for help\r\n\r\nhttps://github.com/Yongle-Fu/tensorflow/tree/feat/bazel_wasm\r\n\r\nclone above, and run `bazel build -c opt //tensorflow/lite/examples/minimal:wasm-minimal`\r\n\r\nmy env is MacBook Pro(Intel) Sonoma 14.5 with python 3.9', 'created_at': datetime.datetime(2024, 7, 19, 2, 33, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2239728014, 'issue_id': 2410755862, 'author': 'pkgoogle', 'body': ""Hi @Yongle-Fu, did you add a custom target?\r\n\r\nhttps://github.com/Yongle-Fu/tensorflow/blob/feat/bazel_wasm/tensorflow/lite/examples/minimal/BUILD\r\n\r\nDoes not have that target:\r\n\r\n```sh\r\n/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9\r\nERROR: Skipping '//tensorflow/lite/examples/minimal:wasm-minimal': no such target '//tensorflow/lite/examples/minimal:wasm-minimal': target 'wasm-minimal' not declared in package 'tensorflow/lite/examples/minimal'\r\n```"", 'created_at': datetime.datetime(2024, 7, 19, 17, 30, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240910363, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': ""> Hi @Yongle-Fu, did you add a custom target?\r\n> \r\n> https://github.com/Yongle-Fu/tensorflow/blob/feat/bazel_wasm/tensorflow/lite/examples/minimal/BUILD\r\n> \r\n> Does not have that target:\r\n> \r\n> ```shell\r\n> /external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9\r\n> ERROR: Skipping '//tensorflow/lite/examples/minimal:wasm-minimal': no such target '//tensorflow/lite/examples/minimal:wasm-minimal': target 'wasm-minimal' not declared in package 'tensorflow/lite/examples/minimal'\r\n> ```\r\n\r\nsorry, pushed now"", 'created_at': datetime.datetime(2024, 7, 20, 4, 27, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243521012, 'issue_id': 2410755862, 'author': 'pkgoogle', 'body': 'Hi @Yongle-Fu, so the fact that the binary target isn\'t there ""officially"" tells me this is something we don\'t really support... are you following an official resource that says otherwise? If you want to support it, is this really more of a feature request you would say?', 'created_at': datetime.datetime(2024, 7, 22, 18, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244062185, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': '> Hi @Yongle-Fu, so the fact that the binary target isn\'t there ""officially"" tells me this is something we don\'t really support... are you following an official resource that says otherwise? If you want to support it, is this really more of a feature request you would say?\r\n\r\nyes, it\'s a feature request, would you like provide any additional information or suggestions on how to support it. https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/src/cc/BUILD.bazel seems support `wasm_cc_binary` without `threads = ""emscripten""`', 'created_at': datetime.datetime(2024, 7, 23, 1, 4, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245957410, 'issue_id': 2410755862, 'author': 'pkgoogle', 'body': ""Hi @Yongle-Fu, there probably is a way but it will take some time to figure it all out, I'll escalate this as a feature request for now -- I would also look into why/why not TFJS may be a better fit for what you are trying to do in the bigger picture. @mattsoulanille, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 7, 23, 18, 28, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246290435, 'issue_id': 2410755862, 'author': 'mattsoulanille', 'body': ""At one point, we were trying to open-source the `@tensorflow/tfjs-tflite` wasm build, but there was some codegen in our internal repo that made it difficult. It should be possible to build a non-threaded WASM binary externally with `wasm_cc_binary`. `@tensorflow/tfjs-tflite` uses the tflite C api instead of the C++ api, which might be why we don't see this error in our build. Perhaps the C++ api needs shared memory while the C api doesn't?"", 'created_at': datetime.datetime(2024, 7, 23, 20, 53, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246927856, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': '> At one point, we were trying to open-source the `@tensorflow/tfjs-tflite` wasm build, but there was some codegen in our internal repo that made it difficult. It should be possible to build a non-threaded WASM binary externally with `wasm_cc_binary`. `@tensorflow/tfjs-tflite` uses the tflite C api instead of the C++ api, which might be why we don\'t see this error in our build. Perhaps the C++ api needs shared memory while the C api doesn\'t?\r\n\r\n```\r\ncc_binary(\r\n    name = ""minimal"",\r\n    srcs = [\r\n        ""minimal.cc"",\r\n    ],\r\n    deps = [\r\n        ""//tensorflow/lite/c:c_api"",\r\n    ],\r\n)\r\n\r\nwasm_cc_binary(\r\n    name = ""wasm-minimal"",\r\n    cc_target = "":minimal"",\r\n)\r\n```\r\n\r\nbuild wasm `c_api` without threads = ""emscripten"" has same issue', 'created_at': datetime.datetime(2024, 7, 24, 5, 24, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249108976, 'issue_id': 2410755862, 'author': 'mattsoulanille', 'body': 'I think I\'ve found the cause of the error. For some reason, the params sent to emscripten contain `-pthread` even though threading is supposed to be disabled.\r\n\r\nHere\'s how to find where `-pthread` is included and try linking without it:\r\n1. Compile with `bazel build -c opt -s --action_env=SHARED_MEMORY=0 \r\n//tensorflow/lite/examples/minimal:wasm-minimal`. This will print a bunch of stuff to the console. If you don\'t see anything, you may first need to `bazel clean`.\r\n\r\n2. Find the line that says something like this:\r\n```\r\nSUBCOMMAND: # //tensorflow/lite/examples/minimal:minimal [action \'Linking tensorflow/lite/examples/minimal/minimal\',\r\n```\r\nAfter that line is the command that Bazel ran. In my case, it was this:\r\n```bash\r\n(cd /usr/local/google/home/msoulanille/.cache/bazel/_bazel_msoulanille/7a684da12c3cd27c1d32c19ef60b7ca5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=35.0.0 \\\r\n    ANDROID_NDK_API_LEVEL=21 \\\r\n    ANDROID_NDK_HOME=/usr/local/google/home/msoulanille/Android/Sdk/ndk/27.0.11902837 \\\r\n    ANDROID_NDK_VERSION=27 \\\r\n    ANDROID_SDK_API_LEVEL=35 \\\r\n    ANDROID_SDK_HOME=/usr/local/google/home/msoulanille/Android/Sdk \\\r\n    CLANG_COMPILER_PATH=/usr/local/google/home/msoulanille/Android/Sdk/ndk/27.0.11902837/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-18 \\\r\n    EMCC_WASM_BACKEND=1 \\\r\n    EM_BIN_PATH=external/emscripten_bin_linux \\\r\n    EM_CONFIG_PATH=external/emscripten_cache/emscripten_config \\\r\n    PATH=/usr/local/google/home/msoulanille/.cache/bazelisk/downloads/sha256/a40ac69263440761199fcb8da47ad4e3f328cbe79ffbf4ecc14e5ba252857307/bin:/usr/local/google/home/msoulanille/.nvm/versions/node/v20.15.0/bin:/usr/lib/google-golang/bin:/usr/local/buildtools/java/jdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/google/home/msoulanille/Android/Sdk/cmdline-tools/latest/bin:/usr/local/google/home/msoulanille/Android/Sdk/platform-tools/ \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3.11/dist-packages \\\r\n    SHARED_MEMORY=0 \\\r\n    TF2_BEHAVIOR=1 \\\r\n  external/emsdk/emscripten_toolchain/emcc_link.sh @bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal-2.params)\r\n```\r\nRunning this command on its own should produce the same linker error you see when you run Bazel.\r\n\r\n3. This command refers to the file `bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal-2.params` (your hash will differ). Opening that file reveals this:\r\n```\r\n--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot\r\n-fdiagnostics-color\r\n-fno-strict-aliasing\r\n-funsigned-char\r\n-no-canonical-prefixes\r\n-s\r\nPRINTF_LONG_DOUBLE=1\r\n--oformat=js\r\n-O2\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/_objs/minimal/minimal.o\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/liboptional_debug_tools.a\r\n-Wl,-whole-archive\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/core/libcc_api_experimental.lo\r\n-Wl,-no-whole-archive\r\n...\r\n... (a bunch of files listed here)\r\n...\r\n-Wl,-whole-archive\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/core/c/libcommon.lo\r\n-Wl,-no-whole-archive\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/compiler/mlir/lite/schema/libschema_utils.a\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/external/ruy/ruy/libdenormal.a\r\n-Wl,-s\r\n-pthread\r\n-pthread\r\n-pthread\r\n-lpthread\r\n-lm\r\n-lm\r\n-pthread\r\n-ldl\r\n-o\r\nbazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal\r\n```\r\nYou\'ll note that there are a bunch of references to pthreads even though threading is supposed to be disabled.\r\n\r\n4. Edit this file (`chmod +w the_file` first to get write access) and remove all the `-pthread` lines (our internal build keeps `-lpthread` lines and still works, so that doesn\'t need to be removed, although I\'m not sure if it should actually be there).\r\n\r\n5. Run the linker command again, and it should work. I\'m not actually sure where the output ends up, sorry.\r\n\r\n6. You should probably also run `bazel clean` after experimenting with this, since we edited one of the files in `bazel-out`. Bazel can probably detect this, but it\'s best to be safe.\r\n\r\nI\'m not sure how `-pthread` ends up in this file. Maybe something in `deps` of the `cc_binary` `minimal` target is adding it. \r\n\r\nMaybe one of these files is doing it, although this is a full grep over the entire repo so it might not actually be included in the build:\r\n\r\n```\r\n$ find . -name ""*.bzl"" | xargs grep ""\\-pthread""\r\n./third_party/xla/xla/tsl/tsl.bzl:            clean_dep(""//xla/tsl:no_lgpl_deps""): [""-D__TENSORFLOW_NO_LGPL_DEPS__"", ""-pthread""],\r\n./third_party/xla/xla/tsl/tsl.bzl:            ""//conditions:default"": [""-pthread""],\r\n./tensorflow/tensorflow.bzl:            clean_dep(""//tensorflow:no_lgpl_deps""): [""-D__TENSORFLOW_NO_LGPL_DEPS__"", ""-pthread""],\r\n./tensorflow/tensorflow.bzl:            ""//conditions:default"": [""-pthread""],\r\n```\r\n\r\nThere\'s probably a way to get `bazel query` to tell you where it\'s coming from, but I don\'t know how.', 'created_at': datetime.datetime(2024, 7, 25, 0, 24, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249619259, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': 'Thank you so much for your help. I tried removing all the `-pthread` lines and then ran `emcc_link.sh`, and it worked. The output directory is located at `bazel-out/wasm-opt-ST-${hash}/bin/tensorflow/lite/examples/minimal/.`\r\n\r\nI attempted to run JS to call the above WebAssembly. The run was successful, but I encountered the following issue when invoking `TfLiteInterpreterInvoke(interpreter); `line.\r\n\r\n`libc++abi: terminating due to uncaught exception of type Napi::Error: Aborted (native code called abort())`\r\n\r\nI also tried removing the `-lpthread` line and ran `emcc_link.sh` successfully, but the same issue occurred', 'created_at': datetime.datetime(2024, 7, 25, 7, 13, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251067301, 'issue_id': 2410755862, 'author': 'mattsoulanille', 'body': 'The cc_binary we use for the internal build has several linkopts set. Maybe one of these is required:\r\n```\r\n            ""--bind"",\r\n            ""-Os"",\r\n            ""-fno-rtti"",\r\n            ""-s ASSERTIONS=0"",\r\n            ""-s ALLOW_BLOCKING_ON_MAIN_THREAD=1"",\r\n            ""-s ALLOW_MEMORY_GROWTH=1"",\r\n            ""-s DISABLE_EXCEPTION_CATCHING=1"",\r\n            ""-s DEFAULT_LIBRARY_FUNCS_TO_INCLUDE=[]"",\r\n            ""-s ENVIRONMENT=web,worker"",\r\n            ""-s ERROR_ON_UNDEFINED_SYMBOLS=0"",\r\n            ""-s EXIT_RUNTIME=0"",\r\n            ""-s EXPORTED_FUNCTIONS=_malloc,_free"",\r\n            ""-s FILESYSTEM=0"",\r\n            ""-s INLINING_LIMIT=1"",\r\n            ""-s MALLOC=emmalloc"",\r\n            ""-s MODULARIZE=1"",\r\n            ""-s PTHREAD_POOL_SIZE=8"",\r\n            ""-s TOTAL_MEMORY=32MB"",\r\n```\r\n\r\nI\'m not very hopeful that one of these options will fix it, but they may be worth trying.\r\n\r\nYou can also try enabling dwarf debugging to see where this error comes from, but unfortunately, that\'s a somewhat difficult process. The [Emscripten debugging documentation](https://emscripten.org/docs/porting/Debugging.html#debugging-in-the-browser) and [this blog post](https://developer.chrome.com/blog/wasm-debugging-2020/) detail how to do it, and you\'ll probably need [this chrome extension](https://goo.gle/wasm-debugging-extension). \r\n\r\nWithin Bazel, I think debug mode can be enabled by passing `--features=dwarf_debug_info -c dbg` to your `bazel build` command, but I\'m not actually sure if this will work (I haven\'t debugged external code with DWARF symbols in a while). You\'ll also need to serve the debug files (and possibly source code as well) with your test server. Something to keep in mind is that [Bazel will probably have the wrong paths](https://github.com/emscripten-core/emsdk/issues/1129) for all the sourcemaps and dwarf symbol locations, so Chrome won\'t be able to load them or the original sources for debugging. To fix this, you\'ll need to add a path substitution in the C/C++ DevTools plugin. Go to [chrome://extensions/](chrome://extensions/) and find the extension you just installed. Click details, and then click extension options. This should give you a page where you can add path substitutions. Alternatively, the issue I linked might have some details on how to fix the paths.', 'created_at': datetime.datetime(2024, 7, 25, 17, 45, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251763235, 'issue_id': 2410755862, 'author': 'mattsoulanille', 'body': ""My guess is the `-pthread` lines are a symptom of the issue rather than a cause. The build is probably still configured for threads before it gets to the `wasm_cc_binary`, which is likely causing the `-pthread` lines to appear in the emscripten config. Then, it panics at runtime because it expects threads to be available (maybe? I'm surprised the linker wouldn't complain in this case).\r\n\r\nA `-c dbg` build, even one without dwarf symbols, might reveal more."", 'created_at': datetime.datetime(2024, 7, 26, 1, 3, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2252379556, 'issue_id': 2410755862, 'author': 'Yongle-Fu', 'body': ""I found a solution to fix the `emcc_link` issue by adding `-s USE_PTHREADS=0`. \r\n\r\nHowever, the inference issue reported yesterday still exists when invoking `TfLiteInterpreterInvoke(interpreter);`.\r\n\r\nTo address this, I added a `c_api_test` example for Node.js using `wasm-minimal`. \r\nhttps://github.com/Yongle-Fu/tensorflow/blob/feat/bazel_wasm/tensorflow/lite/examples/minimal/js_example/README.md\r\n\r\nAlthough the model hasn't loaded successfully yet, once it does, this should serve as  example for reproducing the issue."", 'created_at': datetime.datetime(2024, 7, 26, 9, 50, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571565425, 'issue_id': 2410755862, 'author': 'cucibala', 'body': 'if your issue has been resolved? I have also encountered the same problem', 'created_at': datetime.datetime(2025, 1, 5, 9, 44, 31, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-17 03:57:06 UTC): @Yongle-Fu,
Could you please let us know which tensorflow version you are trying to use for executing the mentioned code? Thank you!

Yongle-Fu (Issue Creator) on (2024-07-17 04:51:58 UTC): thank you for your respnose, i use latest branch on master

pkgoogle (Assginee) on (2024-07-18 18:12:51 UTC): Hi @Yongle-Fu, can you please provide reproducible steps & please share your environment as well. Thanks.

Yongle-Fu (Issue Creator) on (2024-07-19 02:33:44 UTC): sure, thank you for help

https://github.com/Yongle-Fu/tensorflow/tree/feat/bazel_wasm

clone above, and run `bazel build -c opt //tensorflow/lite/examples/minimal:wasm-minimal`

my env is MacBook Pro(Intel) Sonoma 14.5 with python 3.9

pkgoogle (Assginee) on (2024-07-19 17:30:09 UTC): Hi @Yongle-Fu, did you add a custom target?

https://github.com/Yongle-Fu/tensorflow/blob/feat/bazel_wasm/tensorflow/lite/examples/minimal/BUILD

Does not have that target:

```sh
/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9
ERROR: Skipping '//tensorflow/lite/examples/minimal:wasm-minimal': no such target '//tensorflow/lite/examples/minimal:wasm-minimal': target 'wasm-minimal' not declared in package 'tensorflow/lite/examples/minimal'
```

Yongle-Fu (Issue Creator) on (2024-07-20 04:27:25 UTC): sorry, pushed now

pkgoogle (Assginee) on (2024-07-22 18:03:03 UTC): Hi @Yongle-Fu, so the fact that the binary target isn't there ""officially"" tells me this is something we don't really support... are you following an official resource that says otherwise? If you want to support it, is this really more of a feature request you would say?

Yongle-Fu (Issue Creator) on (2024-07-23 01:04:38 UTC): yes, it's a feature request, would you like provide any additional information or suggestions on how to support it. https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/src/cc/BUILD.bazel seems support `wasm_cc_binary` without `threads = ""emscripten""`

pkgoogle (Assginee) on (2024-07-23 18:28:33 UTC): Hi @Yongle-Fu, there probably is a way but it will take some time to figure it all out, I'll escalate this as a feature request for now -- I would also look into why/why not TFJS may be a better fit for what you are trying to do in the bigger picture. @mattsoulanille, can you please take a look? Thanks.

mattsoulanille (Assginee) on (2024-07-23 20:53:30 UTC): At one point, we were trying to open-source the `@tensorflow/tfjs-tflite` wasm build, but there was some codegen in our internal repo that made it difficult. It should be possible to build a non-threaded WASM binary externally with `wasm_cc_binary`. `@tensorflow/tfjs-tflite` uses the tflite C api instead of the C++ api, which might be why we don't see this error in our build. Perhaps the C++ api needs shared memory while the C api doesn't?

Yongle-Fu (Issue Creator) on (2024-07-24 05:24:42 UTC): ```
cc_binary(
    name = ""minimal"",
    srcs = [
        ""minimal.cc"",
    ],
    deps = [
        ""//tensorflow/lite/c:c_api"",
    ],
)

wasm_cc_binary(
    name = ""wasm-minimal"",
    cc_target = "":minimal"",
)
```

build wasm `c_api` without threads = ""emscripten"" has same issue

mattsoulanille (Assginee) on (2024-07-25 00:24:34 UTC): I think I've found the cause of the error. For some reason, the params sent to emscripten contain `-pthread` even though threading is supposed to be disabled.

Here's how to find where `-pthread` is included and try linking without it:
1. Compile with `bazel build -c opt -s --action_env=SHARED_MEMORY=0 
//tensorflow/lite/examples/minimal:wasm-minimal`. This will print a bunch of stuff to the console. If you don't see anything, you may first need to `bazel clean`.

2. Find the line that says something like this:
```
SUBCOMMAND: # //tensorflow/lite/examples/minimal:minimal [action 'Linking tensorflow/lite/examples/minimal/minimal',
```
After that line is the command that Bazel ran. In my case, it was this:
```bash
(cd /usr/local/google/home/msoulanille/.cache/bazel/_bazel_msoulanille/7a684da12c3cd27c1d32c19ef60b7ca5/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=35.0.0 \
    ANDROID_NDK_API_LEVEL=21 \
    ANDROID_NDK_HOME=/usr/local/google/home/msoulanille/Android/Sdk/ndk/27.0.11902837 \
    ANDROID_NDK_VERSION=27 \
    ANDROID_SDK_API_LEVEL=35 \
    ANDROID_SDK_HOME=/usr/local/google/home/msoulanille/Android/Sdk \
    CLANG_COMPILER_PATH=/usr/local/google/home/msoulanille/Android/Sdk/ndk/27.0.11902837/toolchains/llvm/prebuilt/linux-x86_64/bin/clang-18 \
    EMCC_WASM_BACKEND=1 \
    EM_BIN_PATH=external/emscripten_bin_linux \
    EM_CONFIG_PATH=external/emscripten_cache/emscripten_config \
    PATH=/usr/local/google/home/msoulanille/.cache/bazelisk/downloads/sha256/a40ac69263440761199fcb8da47ad4e3f328cbe79ffbf4ecc14e5ba252857307/bin:/usr/local/google/home/msoulanille/.nvm/versions/node/v20.15.0/bin:/usr/lib/google-golang/bin:/usr/local/buildtools/java/jdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/google/home/msoulanille/Android/Sdk/cmdline-tools/latest/bin:/usr/local/google/home/msoulanille/Android/Sdk/platform-tools/ \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.11/dist-packages \
    SHARED_MEMORY=0 \
    TF2_BEHAVIOR=1 \
  external/emsdk/emscripten_toolchain/emcc_link.sh @bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal-2.params)
```
Running this command on its own should produce the same linker error you see when you run Bazel.

3. This command refers to the file `bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal-2.params` (your hash will differ). Opening that file reveals this:
```
--sysroot=external/emscripten_bin_linux/emscripten/cache/sysroot
-fdiagnostics-color
-fno-strict-aliasing
-funsigned-char
-no-canonical-prefixes
-s
PRINTF_LONG_DOUBLE=1
--oformat=js
-O2
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/_objs/minimal/minimal.o
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/liboptional_debug_tools.a
-Wl,-whole-archive
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/core/libcc_api_experimental.lo
-Wl,-no-whole-archive
...
... (a bunch of files listed here)
...
-Wl,-whole-archive
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/core/c/libcommon.lo
-Wl,-no-whole-archive
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/compiler/mlir/lite/schema/libschema_utils.a
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/external/ruy/ruy/libdenormal.a
-Wl,-s
-pthread
-pthread
-pthread
-lpthread
-lm
-lm
-pthread
-ldl
-o
bazel-out/wasm-opt-ST-b4dbd4a2bfc4/bin/tensorflow/lite/examples/minimal/minimal
```
You'll note that there are a bunch of references to pthreads even though threading is supposed to be disabled.

4. Edit this file (`chmod +w the_file` first to get write access) and remove all the `-pthread` lines (our internal build keeps `-lpthread` lines and still works, so that doesn't need to be removed, although I'm not sure if it should actually be there).

5. Run the linker command again, and it should work. I'm not actually sure where the output ends up, sorry.

6. You should probably also run `bazel clean` after experimenting with this, since we edited one of the files in `bazel-out`. Bazel can probably detect this, but it's best to be safe.

I'm not sure how `-pthread` ends up in this file. Maybe something in `deps` of the `cc_binary` `minimal` target is adding it. 

Maybe one of these files is doing it, although this is a full grep over the entire repo so it might not actually be included in the build:

```
$ find . -name ""*.bzl"" | xargs grep ""\-pthread""
./third_party/xla/xla/tsl/tsl.bzl:            clean_dep(""//xla/tsl:no_lgpl_deps""): [""-D__TENSORFLOW_NO_LGPL_DEPS__"", ""-pthread""],
./third_party/xla/xla/tsl/tsl.bzl:            ""//conditions:default"": [""-pthread""],
./tensorflow/tensorflow.bzl:            clean_dep(""//tensorflow:no_lgpl_deps""): [""-D__TENSORFLOW_NO_LGPL_DEPS__"", ""-pthread""],
./tensorflow/tensorflow.bzl:            ""//conditions:default"": [""-pthread""],
```

There's probably a way to get `bazel query` to tell you where it's coming from, but I don't know how.

Yongle-Fu (Issue Creator) on (2024-07-25 07:13:06 UTC): Thank you so much for your help. I tried removing all the `-pthread` lines and then ran `emcc_link.sh`, and it worked. The output directory is located at `bazel-out/wasm-opt-ST-${hash}/bin/tensorflow/lite/examples/minimal/.`

I attempted to run JS to call the above WebAssembly. The run was successful, but I encountered the following issue when invoking `TfLiteInterpreterInvoke(interpreter); `line.

`libc++abi: terminating due to uncaught exception of type Napi::Error: Aborted (native code called abort())`

I also tried removing the `-lpthread` line and ran `emcc_link.sh` successfully, but the same issue occurred

mattsoulanille (Assginee) on (2024-07-25 17:45:34 UTC): The cc_binary we use for the internal build has several linkopts set. Maybe one of these is required:
```
            ""--bind"",
            ""-Os"",
            ""-fno-rtti"",
            ""-s ASSERTIONS=0"",
            ""-s ALLOW_BLOCKING_ON_MAIN_THREAD=1"",
            ""-s ALLOW_MEMORY_GROWTH=1"",
            ""-s DISABLE_EXCEPTION_CATCHING=1"",
            ""-s DEFAULT_LIBRARY_FUNCS_TO_INCLUDE=[]"",
            ""-s ENVIRONMENT=web,worker"",
            ""-s ERROR_ON_UNDEFINED_SYMBOLS=0"",
            ""-s EXIT_RUNTIME=0"",
            ""-s EXPORTED_FUNCTIONS=_malloc,_free"",
            ""-s FILESYSTEM=0"",
            ""-s INLINING_LIMIT=1"",
            ""-s MALLOC=emmalloc"",
            ""-s MODULARIZE=1"",
            ""-s PTHREAD_POOL_SIZE=8"",
            ""-s TOTAL_MEMORY=32MB"",
```

I'm not very hopeful that one of these options will fix it, but they may be worth trying.

You can also try enabling dwarf debugging to see where this error comes from, but unfortunately, that's a somewhat difficult process. The [Emscripten debugging documentation](https://emscripten.org/docs/porting/Debugging.html#debugging-in-the-browser) and [this blog post](https://developer.chrome.com/blog/wasm-debugging-2020/) detail how to do it, and you'll probably need [this chrome extension](https://goo.gle/wasm-debugging-extension). 

Within Bazel, I think debug mode can be enabled by passing `--features=dwarf_debug_info -c dbg` to your `bazel build` command, but I'm not actually sure if this will work (I haven't debugged external code with DWARF symbols in a while). You'll also need to serve the debug files (and possibly source code as well) with your test server. Something to keep in mind is that [Bazel will probably have the wrong paths](https://github.com/emscripten-core/emsdk/issues/1129) for all the sourcemaps and dwarf symbol locations, so Chrome won't be able to load them or the original sources for debugging. To fix this, you'll need to add a path substitution in the C/C++ DevTools plugin. Go to [chrome://extensions/](chrome://extensions/) and find the extension you just installed. Click details, and then click extension options. This should give you a page where you can add path substitutions. Alternatively, the issue I linked might have some details on how to fix the paths.

mattsoulanille (Assginee) on (2024-07-26 01:03:53 UTC): My guess is the `-pthread` lines are a symptom of the issue rather than a cause. The build is probably still configured for threads before it gets to the `wasm_cc_binary`, which is likely causing the `-pthread` lines to appear in the emscripten config. Then, it panics at runtime because it expects threads to be available (maybe? I'm surprised the linker wouldn't complain in this case).

A `-c dbg` build, even one without dwarf symbols, might reveal more.

Yongle-Fu (Issue Creator) on (2024-07-26 09:50:27 UTC): I found a solution to fix the `emcc_link` issue by adding `-s USE_PTHREADS=0`. 

However, the inference issue reported yesterday still exists when invoking `TfLiteInterpreterInvoke(interpreter);`.

To address this, I added a `c_api_test` example for Node.js using `wasm-minimal`. 
https://github.com/Yongle-Fu/tensorflow/blob/feat/bazel_wasm/tensorflow/lite/examples/minimal/js_example/README.md

Although the model hasn't loaded successfully yet, once it does, this should serve as  example for reproducing the issue.

cucibala on (2025-01-05 09:44:31 UTC): if your issue has been resolved? I have also encountered the same problem

"
2410554767,issue,closed,completed,"In the local server command line environment, TensorFlow is able to recognize and utilize the GPU. However, when attempting to use TensorFlow in a Jupyter Notebook through a remote VSCode connection to the same server, there is an issue with loading the GPU libraries.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

linux Ubuntu

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the local server command line environment, TensorFlow is able to recognize and utilize the GPU. However, when attempting to use TensorFlow in a Jupyter Notebook through a remote VSCode connection to the same server, there is an issue with loading the GPU libraries.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
print(""Is GPU available: "", tf.test.is_gpu_available())
```


### Relevant log output

```shell
In the VSCode Remote Jupyter Notebook:Num GPUs Available:  0 2024-07-16 16:15:18.670761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-16 16:15:18.691749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-16 16:15:18.698034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-16 16:15:18.713208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-16 16:15:19.758587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Num GPUs Available:  0
Is GPU available:  False
2024-07-16 16:12:35.973708: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
In terminalNum GPUs Available:  2 Is GPU available:  True
```
",ewwll,2024-07-16 08:35:00+00:00,['tilakrayal'],2024-08-01 02:31:33+00:00,2024-08-01 01:57:47+00:00,https://github.com/tensorflow/tensorflow/issues/71912,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2232697255, 'issue_id': 2410554767, 'author': 'tilakrayal', 'body': '@ewwll,\r\nCould you please let me know the steps which you followed to install the tensorflow and also the compatible versions which you are using. I tried to install it and it was able to detect the GPU. As you mentioned in the local it is able to  recognize and in the vscode it was not able to detect. Please try to re-install the vscode and try to use it again.\r\n\r\n\r\n![2 17 gpu](https://github.com/user-attachments/assets/9a88e532-16ca-498e-b6de-32e9622040a4)', 'created_at': datetime.datetime(2024, 7, 17, 8, 9, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249208818, 'issue_id': 2410554767, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 25, 1, 52, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799676, 'issue_id': 2410554767, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799728, 'issue_id': 2410554767, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71912"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71912"">No</a>', 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 49, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-17 08:09:35 UTC): @ewwll,
Could you please let me know the steps which you followed to install the tensorflow and also the compatible versions which you are using. I tried to install it and it was able to detect the GPU. As you mentioned in the local it is able to  recognize and in the vscode it was not able to detect. Please try to re-install the vscode and try to use it again.


![2 17 gpu](https://github.com/user-attachments/assets/9a88e532-16ca-498e-b6de-32e9622040a4)

github-actions[bot] on (2024-07-25 01:52:55 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-01 01:57:47 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-01 01:57:49 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71912"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71912"">No</a>

"
2409487087,issue,closed,completed,"Tensorflow Dataset API continues to be broken, list_files no longer works","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

pip install tensorflow[and-cuda]

### TensorFlow version

tf v2.17.0-rc1-2-gad6d8cc177d

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`list_files` from Tensorflow Dataset API is now broken in TF 2.17. I cannot follow the example located here: https://www.tensorflow.org/guide/data#consuming_sets_of_files

After the issues where employing TFRecords and using generators blows up memory, now `list_files` won't even work. I've attached code that will list the appropriate files using both `pathlib` and `glob` libraries. Both return lists.

What on earth is going on here.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3
import os ; import sys
from pathlib import Path
import glob

import tensorflow as tf

output_path = Path('test_text')
if not output_path.exists(): output_path.mkdir(parents=True)

num_files = 10
write_out_string = 'abcdef'
for fnum in range(num_files):
    output_file = output_path / f'sample_{fnum:03d}.txt'
    with open(output_file, 'w') as f:
        f.write(write_out_string)
        
# First, try with Python's pathlib glob, make sure it can see the files
print(""With pathlib."")
print(list(output_path.glob('*txt')))
print()

# Next, try to with Python's glob
print(""With glob"")
glob_path = output_path / '*txt'
print(list(glob.glob(str(glob_path))))

list_ds = tf.data.Dataset.list_files(str(glob_path))

for bf in list_ds.take(5):
    print(bf.numpy())
```


### Relevant log output

```shell
With pathlib.
[PosixPath('test_text/sample_009.txt'), PosixPath('test_text/sample_007.txt'), PosixPath('test_text/sample_000.txt'), PosixPath('test_text/sample_006.txt'), PosixPath('test_text/sample_004.txt'), PosixPath('test_text/sample_008.txt'), PosixPath('test_text/sample_005.txt'), PosixPath('test_text/sample_002.txt'), PosixPath('test_text/sample_001.txt'), PosixPath('test_text/sample_003.txt')]

With glob
['test_text/sample_009.txt', 'test_text/sample_007.txt', 'test_text/sample_000.txt', 'test_text/sample_006.txt', 'test_text/sample_004.txt', 'test_text/sample_008.txt', 'test_text/sample_005.txt', 'test_text/sample_002.txt', 'test_text/sample_001.txt', 'test_text/sample_003.txt']
2024-07-15 15:39:17.545697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46673 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:16:00.0, compute capability: 8.9


Traceback (most recent call last):
  File ""/home/dryglicki/code/list_files/demonstrate_list_files.py"", line 28, in <module>
    list_ds = tf.data.Dataset.list_files(str(glob_path))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/ssd0/miniforge3_2024-04/envs/tensorflow_2d17/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1318, in list_files
    string_ops.reduce_join(file_pattern, separator="", ""), name=""message"")
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/ssd0/miniforge3_2024-04/envs/tensorflow_2d17/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/ssd0/miniforge3_2024-04/envs/tensorflow_2d17/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: object __array__ method not producing an array
```
",dryglicki,2024-07-15 19:40:51+00:00,['tilakrayal'],2024-07-16 13:55:39+00:00,2024-07-16 13:55:37+00:00,https://github.com/tensorflow/tensorflow/issues/71884,"[('type:bug', 'Bug'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2229273255, 'issue_id': 2409487087, 'author': 'dryglicki', 'body': ""For what it's worth, pre-globbing and using `from_tensor_slices` does allow for one to iterate through the filenames.\r\n\r\nBut if `list_files` is indeed deprecated, somebody say so!"", 'created_at': datetime.datetime(2024, 7, 15, 19, 56, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229712640, 'issue_id': 2409487087, 'author': 'dryglicki', 'body': 'Another issue. This will also fail if `TFRecordDataset` is in the chain. If I need to open a separate ticket or issue or post more code, please let me know. But the `TFRecordDataset` issue appears both when wrapped by `interleave` or when pre-globbing and using those file names in `TFRecordDataset` directly.', 'created_at': datetime.datetime(2024, 7, 16, 0, 19, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230821370, 'issue_id': 2409487087, 'author': 'tilakrayal', 'body': '@dryglicki,\r\nI tried to execute both the official document code and mentioned code with latest tensorflow stable version 2.17.0, observed that both are executed without fail/error. Kindly find the gist of both and update if the understanding is correct. \r\n[Official doc](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb) and [code](https://colab.research.google.com/gist/tilakrayal/ebfc5bcece7c534c31cd797257e7cb4f/untitled2004.ipynb)\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 16, 12, 55, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230839502, 'issue_id': 2409487087, 'author': 'dryglicki', 'body': ""Thanks, @tilakrayal . Few more details.\r\n1. I installed TF using `pip install tensorflow[and-cuda]`. I have since updated the original post\r\n2. The Colab version of python is 3.10.12. I'm using 3.11.9.\r\n\r\nI will try another conda environment that follows what's in the Colab."", 'created_at': datetime.datetime(2024, 7, 16, 13, 3, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230846629, 'issue_id': 2409487087, 'author': 'dryglicki', 'body': 'All right, I can confirm if I use 3.10.14 and force `pip install tensorflow[and-cuda]==2.17.0` I can get it to work.', 'created_at': datetime.datetime(2024, 7, 16, 13, 7, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230882339, 'issue_id': 2409487087, 'author': 'dryglicki', 'body': ""All right, found the source of the problem. It's dependency collision.\r\n\r\nmatplotlib installed via `conda` brings numpy 2.0 which runs over the numpy 1.X version installed via pip for tensorflow."", 'created_at': datetime.datetime(2024, 7, 16, 13, 23, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230958285, 'issue_id': 2409487087, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71884"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71884"">No</a>', 'created_at': datetime.datetime(2024, 7, 16, 13, 55, 39, tzinfo=datetime.timezone.utc)}]","dryglicki (Issue Creator) on (2024-07-15 19:56:19 UTC): For what it's worth, pre-globbing and using `from_tensor_slices` does allow for one to iterate through the filenames.

But if `list_files` is indeed deprecated, somebody say so!

dryglicki (Issue Creator) on (2024-07-16 00:19:33 UTC): Another issue. This will also fail if `TFRecordDataset` is in the chain. If I need to open a separate ticket or issue or post more code, please let me know. But the `TFRecordDataset` issue appears both when wrapped by `interleave` or when pre-globbing and using those file names in `TFRecordDataset` directly.

tilakrayal (Assginee) on (2024-07-16 12:55:13 UTC): @dryglicki,
I tried to execute both the official document code and mentioned code with latest tensorflow stable version 2.17.0, observed that both are executed without fail/error. Kindly find the gist of both and update if the understanding is correct. 
[Official doc](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb) and [code](https://colab.research.google.com/gist/tilakrayal/ebfc5bcece7c534c31cd797257e7cb4f/untitled2004.ipynb)
Thank you!

dryglicki (Issue Creator) on (2024-07-16 13:03:38 UTC): Thanks, @tilakrayal . Few more details.
1. I installed TF using `pip install tensorflow[and-cuda]`. I have since updated the original post
2. The Colab version of python is 3.10.12. I'm using 3.11.9.

I will try another conda environment that follows what's in the Colab.

dryglicki (Issue Creator) on (2024-07-16 13:07:13 UTC): All right, I can confirm if I use 3.10.14 and force `pip install tensorflow[and-cuda]==2.17.0` I can get it to work.

dryglicki (Issue Creator) on (2024-07-16 13:23:26 UTC): All right, found the source of the problem. It's dependency collision.

matplotlib installed via `conda` brings numpy 2.0 which runs over the numpy 1.X version installed via pip for tensorflow.

google-ml-butler[bot] on (2024-07-16 13:55:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71884"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71884"">No</a>

"
2407872826,issue,closed,not_planned,TFlite: GPU delegate: ability to limit amount of GPU memory used by TFlite?,"Tensorflow can impose a limit on the amount of memory used by Tensorflow on a GPU:

tf.config.LogicalDeviceConfiguration(memory_limit=1024)]

Is it possible to have similar functionality for TFlite on GPU? 

Thanks",andy-tai,2024-07-15 05:11:37+00:00,['LakshmiKalaKadali'],2024-12-10 22:12:38+00:00,2024-12-10 22:12:37+00:00,https://github.com/tensorflow/tensorflow/issues/71838,"[('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2533048621, 'issue_id': 2407872826, 'author': 'pkgoogle', 'body': ""Hi @andy-tai,\r\n\r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress [here](https://github.com/google-ai-edge/LiteRT/issues/271).\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 12, 10, 22, 12, 8, tzinfo=datetime.timezone.utc)}]","pkgoogle on (2024-12-10 22:12:08 UTC): Hi @andy-tai,

Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress [here](https://github.com/google-ai-edge/LiteRT/issues/271).

Let us know if you have any questions. Thanks.

"
2407786344,issue,closed,completed,Should this be opened against Keras repo?,"              Should this be opened against Keras repo?

_Originally posted by @mihaimaruseac in https://github.com/tensorflow/tensorflow/issues/71829#issuecomment-2227494804_
            ",SsomsakTH,2024-07-15 03:25:22+00:00,['Venkat6871'],2024-07-15 04:07:28+00:00,2024-07-15 04:07:28+00:00,https://github.com/tensorflow/tensorflow/issues/71836,[],"[{'comment_id': 2227667442, 'issue_id': 2407786344, 'author': 'mihaimaruseac', 'body': ""Please don't spam"", 'created_at': datetime.datetime(2024, 7, 15, 4, 7, 28, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-15 04:07:28 UTC): Please don't spam

"
2407650734,issue,closed,completed,"A Digital Future for All
Siifsiin 2.0: A Next-Generation Platform for an Empowered Humanity","**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",Morehman27,2024-07-15 00:02:32+00:00,['tilakrayal'],2024-07-15 00:02:50+00:00,2024-07-15 00:02:50+00:00,https://github.com/tensorflow/tensorflow/issues/71835,[],[],
2407596792,issue,closed,completed,Inconsistent results from distributed training of models containing `TimeDistributed` or `SeparableConv2D` ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240710

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I trained a small model in distributed settings and detected inconsistent results. I would expect there's no difference when training the same model on the same input with different world sizes.

I first created a four-layer model defined by the following code:
```
layer_0 = layers.Input(shape=[32, 32, 3], batch_shape=None, dtype='float32', sparse=False, tensor=None, name='00_input_object')
layer_1 = layers.TimeDistributed(layer=layers.LSTM(units=4, activation='softmax', recurrent_activation='elu', use_bias=False, kernel_initializer='random_uniform', recurrent_initializer='random_uniform', bias_initializer='random_uniform', unit_forget_bias=False, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False), name='01_time_distributed')(layer_0)
layer_2 = layers.Flatten(data_format=None, name='02_flatten')(layer_1)
layer_3 = layers.Dense(units=10, activation='linear', use_bias=False, kernel_initializer='random_uniform', bias_initializer='random_uniform', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, name='03_dense')(layer_2)
layer_4 = layers.Reshape(target_shape=[10], name='04_reshape')(layer_3)
model = keras.Model(layer_0, layer_4)
```

Then I trained it using `MirroredStrategy` with 1 CPU and 2 CPU, respectively, and compared the prediction results. There was a large difference `Pred Linf: 0.06834731` between the two prediction results. Further investigation shows that the loss and gradient values are close (Linf < 3e-7).

Another piece of evidence that this should be a bug is that as I tested with the same code, there's only a small difference (~1e-6) with TensorFlow 2.15. The large inconsistency was introduced in 2.16.1 and it still exists in the current nightly version.

I detect similar inconsistencies when replacing the TimeDistributed layer with a separable convolution layer. The model definition is as following:

```
layer_0 = layers.Input(shape=[32, 32, 3], batch_shape=None, dtype='float32', sparse=False, tensor=None, name='00_input_object')
layer_1 = layers.SeparableConv2D(filters=4, kernel_size=[27, 27], strides=[1, 1], padding='same', data_format='channels_last', dilation_rate=[1, 1], depth_multiplier=5, activation='sigmoid', use_bias=True, depthwise_initializer='random_uniform', pointwise_initializer='random_uniform', bias_initializer='random_uniform', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, name='01_separable_conv2D')(layer_0)
layer_2 = layers.Flatten(data_format=None, name='02_flatten')(layer_1)
layer_3 = layers.Dense(units=10, activation='linear', use_bias=False, kernel_initializer='random_uniform', bias_initializer='random_uniform', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, name='03_dense')(layer_2)
layer_4 = layers.Reshape(target_shape=[10], name='04_reshape')(layer_3)
model = keras.Model(layer_0, layer_4)
```

The inconsistency in their results is larger: `Pred Linf: 3208.046`

Note that I use a large learning rate (10.0) deliberately in the reproduction code below to show the large inconsistencies in the nightly version. Those two inconsistencies don't exist in TF 2.15.

### Standalone code to reproduce the issue

```shell
For the first model with TimeDistributed layer:
https://colab.research.google.com/drive/1DLV5cye_BRn80IHLl8UOB_V9yeYw1lmJ?usp=sharing

For the second model with SeparableConv2D layer:
https://colab.research.google.com/drive/1AtEiKBaoAEL-U5_IFsnpm39LwoBhQqBk?usp=sharing
```


### Relevant log output

```shell
# For the first model with TimeDistributed layer:

1CPU vs 2CPU:
Pred Linf: 0.06834731
Loss Linf: 2.3841858e-07
Gradient 0 Linf: 4.656613e-09
Gradient 1 Linf: 1.4551915e-11
Gradient 2 Linf: 4.3655746e-11


# For the second model with SeparableConv2D layer:

Pred Linf: 3208.046
Loss Linf: 2.3841858e-07
Gradient 0 Linf: 8.381903e-09
Gradient 1 Linf: 1.8626451e-08
Gradient 2 Linf: 1.2777746e-06
Gradient 3 Linf: 2.9802322e-08
```
",jiannanWang,2024-07-14 21:08:50+00:00,['tilakrayal'],2024-09-04 08:06:26+00:00,2024-08-23 01:50:35+00:00,https://github.com/tensorflow/tensorflow/issues/71833,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2230768691, 'issue_id': 2407596792, 'author': 'tilakrayal', 'body': '@jiannanWang,\r\nLooks like this is an issue with the keras3.0 which by default with tensorflow v2.16. Please allow some time to deepdive into this issue and update the same. Meanwhile could you please try with the latest tensorflowv2.17 and the latest keras-nightly as well. Thank you!', 'created_at': datetime.datetime(2024, 7, 16, 12, 29, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231461838, 'issue_id': 2407596792, 'author': 'jiannanWang', 'body': 'Thank you for your reply! The reproduction code in the original post is using the nightly version `keras-nightly-3.4.1.dev2024071403` and `tf-nightly-2.18.0.dev20240710`.\r\n\r\nI then tried the latest keras-nightly as of today, which is `3.4.1.dev2024071603`. The inconsistencies become smaller, with `6.7055225e-08` in the timedistributed_lstm model and `0.0029296875` in the separableconv2d model. The colab links are copied below:\r\n\r\n`tf-nightly-2.18.0.dev20240710` + `keras-nightly-3.4.1.dev2024071603`\r\nModel with timedistributed_lstm\r\nhttps://colab.research.google.com/drive/1mqr9UMGta3utoRtF_kUha00J2KuOV2vF?usp=sharing\r\n\r\nModel with separableconv2d\r\nhttps://colab.research.google.com/drive/1812DVH33j3OlgqX3_r9zH4V5JCmoNL3T?usp=sharing\r\n\r\nI also tried tf2.17 and I can reproduce both inconsistencies with tf 2.17.0 + keras 3.4.1. Below are the links to colabs:\r\n\r\n`tf 2.17.0` + `keras 3.4.1`:\r\nModel with timedistributed_lstm\r\nhttps://colab.research.google.com/drive/19R1mmYOgYtZFP7lSDH0XczddQiMMHlNx?usp=sharing\r\n\r\nModel with separableconv2d\r\nhttps://colab.research.google.com/drive/1Yxz_y8q5oar08imbj7RDiAz5kS1SKs5a?usp=sharing', 'created_at': datetime.datetime(2024, 7, 16, 17, 34, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304116956, 'issue_id': 2407596792, 'author': 'tilakrayal', 'body': '@jiannanWang,\r\nI also tried to execute the mentioned code on tf-nightly, and observed that the inconsistencies become smaller, with 6.7055225e-08 in the timedistributed_lstm model and 0.0029296875 in the separableconv2d model. \r\n\r\nAlso looks like this issue is more related to Keras, could you please raise the new issue on the keras-team/keras repo for the quick response. Thank you!', 'created_at': datetime.datetime(2024, 8, 22, 8, 46, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306027204, 'issue_id': 2407596792, 'author': 'jiannanWang', 'body': ""Sure! Given that the original bug has been fixed in the nightly version, I raised the new issue in the Keras repo as suggested. \r\n\r\nI'll close this issue as the original bug is fixed."", 'created_at': datetime.datetime(2024, 8, 23, 1, 50, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306027228, 'issue_id': 2407596792, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71833"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71833"">No</a>', 'created_at': datetime.datetime(2024, 8, 23, 1, 50, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-16 12:29:48 UTC): @jiannanWang,
Looks like this is an issue with the keras3.0 which by default with tensorflow v2.16. Please allow some time to deepdive into this issue and update the same. Meanwhile could you please try with the latest tensorflowv2.17 and the latest keras-nightly as well. Thank you!

jiannanWang (Issue Creator) on (2024-07-16 17:34:41 UTC): Thank you for your reply! The reproduction code in the original post is using the nightly version `keras-nightly-3.4.1.dev2024071403` and `tf-nightly-2.18.0.dev20240710`.

I then tried the latest keras-nightly as of today, which is `3.4.1.dev2024071603`. The inconsistencies become smaller, with `6.7055225e-08` in the timedistributed_lstm model and `0.0029296875` in the separableconv2d model. The colab links are copied below:

`tf-nightly-2.18.0.dev20240710` + `keras-nightly-3.4.1.dev2024071603`
Model with timedistributed_lstm
https://colab.research.google.com/drive/1mqr9UMGta3utoRtF_kUha00J2KuOV2vF?usp=sharing

Model with separableconv2d
https://colab.research.google.com/drive/1812DVH33j3OlgqX3_r9zH4V5JCmoNL3T?usp=sharing

I also tried tf2.17 and I can reproduce both inconsistencies with tf 2.17.0 + keras 3.4.1. Below are the links to colabs:

`tf 2.17.0` + `keras 3.4.1`:
Model with timedistributed_lstm
https://colab.research.google.com/drive/19R1mmYOgYtZFP7lSDH0XczddQiMMHlNx?usp=sharing

Model with separableconv2d
https://colab.research.google.com/drive/1Yxz_y8q5oar08imbj7RDiAz5kS1SKs5a?usp=sharing

tilakrayal (Assginee) on (2024-08-22 08:46:48 UTC): @jiannanWang,
I also tried to execute the mentioned code on tf-nightly, and observed that the inconsistencies become smaller, with 6.7055225e-08 in the timedistributed_lstm model and 0.0029296875 in the separableconv2d model. 

Also looks like this issue is more related to Keras, could you please raise the new issue on the keras-team/keras repo for the quick response. Thank you!

jiannanWang (Issue Creator) on (2024-08-23 01:50:35 UTC): Sure! Given that the original bug has been fixed in the nightly version, I raised the new issue in the Keras repo as suggested. 

I'll close this issue as the original bug is fixed.

google-ml-butler[bot] on (2024-08-23 01:50:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71833"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71833"">No</a>

"
2407480075,issue,closed,completed,Accuracy Drop Across TensorFlow Versions When Using Keras 3 Instead of Keras 2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.1, 2.16.2, 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

As mentioned [here](https://github.com/tensorflow/tensorflow/issues/63362#issuecomment-2226996257), a drop in model results was observed in the new version of TensorFlow using the new Keras.

I'm using a CNN+BLSTM+CTC for handwriting recognition tests, and only `tf.keras.layers`, `tf.nn.ctc_loss`, and `tf.nn.ctc_beam_search_decoder` as the API base. So I expanded the tests and indeed observed the following results.

As a comment, the loss didn't reach the minimum during training with the new keras. So I also tried to find the root of the problem in default parameter differences between keras and tf.keras, but without success.

```
Version                         Results                     Comment
2.15.1                          (CER ~4.6 and WER ~12.5) -- no messages
2.16.2                          (CER ~5.7 and WER ~15.4) -- PTX messages
2.16.2    + TF_USE_LEGACY_KERAS (CER ~4.7 and WER ~12.0) -- PTX messages
2.17.0rc0                       (CER ~5.8 and WER ~15.6) -- numa and gpu_timer messages
2.17.0rc0 + TF_USE_LEGACY_KERAS (CER ~4.8 and WER ~13.0) -- numa and gpu_timer messages
2.17.0rc1                       (CER ~5.2 and WER ~14.8) -- numa and gpu_timer messages
2.17.0rc1 + TF_USE_LEGACY_KERAS (CER ~4.8 and WER ~13.2) -- numa and gpu_timer messages
2.17.0                          (CER ~5.4 and WER ~16.0) -- numa and gpu_timer messages
2.17.0    + TF_USE_LEGACY_KERAS (CER ~4.6 and WER ~12.6) -- numa and gpu_timer messages
```",arthurflor23,2024-07-14 15:21:25+00:00,['Venkat6871'],2024-07-15 17:18:16+00:00,2024-07-15 17:18:14+00:00,https://github.com/tensorflow/tensorflow/issues/71829,"[('type:bug', 'Bug'), ('comp:keras', 'Keras related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2227385856, 'issue_id': 2407480075, 'author': 'cuixue', 'body': '', 'created_at': datetime.datetime(2024, 7, 14, 15, 24, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227494804, 'issue_id': 2407480075, 'author': 'mihaimaruseac', 'body': 'Should this be opened against Keras repo?', 'created_at': datetime.datetime(2024, 7, 14, 21, 57, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229010751, 'issue_id': 2407480075, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71829"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71829"">No</a>', 'created_at': datetime.datetime(2024, 7, 15, 17, 18, 16, tzinfo=datetime.timezone.utc)}]","cuixue on (2024-07-14 15:24:19 UTC): 

mihaimaruseac on (2024-07-14 21:57:50 UTC): Should this be opened against Keras repo?

google-ml-butler[bot] on (2024-07-15 17:18:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71829"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71829"">No</a>

"
2407447499,issue,closed,completed,Module not found,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

not shown

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Module not found error. It shows tensorflow.python not found

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_",Irayanbu05,2024-07-14 13:59:44+00:00,['tilakrayal'],2024-07-31 01:45:35+00:00,2024-07-31 01:45:32+00:00,https://github.com/tensorflow/tensorflow/issues/71828,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2227360053, 'issue_id': 2407447499, 'author': 'Irayanbu05', 'body': 'what should I do now', 'created_at': datetime.datetime(2024, 7, 14, 14, 1, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2228254526, 'issue_id': 2407447499, 'author': 'tilakrayal', 'body': '@Irayanbu05,\r\nCould you please provide the steps you have followed to install the tensorflow. Also please let us know which tensorflow versions, environment and compatible configurations you are trying. \r\n\r\nPlease have a look at the [compatible](https://www.tensorflow.org/install/source_windows#tested_build_configurations) tested build configurations as well and the official document for the installation steps.\r\n\r\nhttps://www.tensorflow.org/install/pip#step-by-step_instructions\r\n Thank you!', 'created_at': datetime.datetime(2024, 7, 15, 11, 14, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244101978, 'issue_id': 2407447499, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 23, 1, 53, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259475230, 'issue_id': 2407447499, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 31, 1, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259475263, 'issue_id': 2407447499, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71828"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71828"">No</a>', 'created_at': datetime.datetime(2024, 7, 31, 1, 45, 34, tzinfo=datetime.timezone.utc)}]","Irayanbu05 (Issue Creator) on (2024-07-14 14:01:05 UTC): what should I do now

tilakrayal (Assginee) on (2024-07-15 11:14:09 UTC): @Irayanbu05,
Could you please provide the steps you have followed to install the tensorflow. Also please let us know which tensorflow versions, environment and compatible configurations you are trying. 

Please have a look at the [compatible](https://www.tensorflow.org/install/source_windows#tested_build_configurations) tested build configurations as well and the official document for the installation steps.

https://www.tensorflow.org/install/pip#step-by-step_instructions
 Thank you!

github-actions[bot] on (2024-07-23 01:53:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-31 01:45:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-31 01:45:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71828"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71828"">No</a>

"
2406814580,issue,closed,completed,GPU delegate and NNAPIDelegate results diverging significantly for a transformer model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

Android Snapdragon 855 (Samsung Fold5/S24Ultra)

### Python version

3.11
Huggingface transformer 4.21

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have converted a transformer model(https://huggingface.co/Salesforce/blip-vqa-base) as tflite for running on an Android device.
Blipvqa consists of 3 nested models vision_encoder, text_encoder and text_decoder.
I seperated the vision_encoder as seperate tensorflow model and converted it as tflite.
The model works perfectly fine on CPU.

When running the same tflite model on GPU there were few ops like Stridedslice reshape(5d) etc. Which were throwing error with GPUdelegate.
So before conversion i rewired few exiting operations to support the same on GPU.
Now the model runs fine on GPU delegate but the results of CPU(NNAPIdelegate) and GPU(GPUDelegate) are different and diverging significantly. 

I am using float32 weights and have set precisionlossallowed as false for GPU delegate to avoid any rounding off errors.
Even after running it with float32 precision the output is very different of GPUdelagte.

When i debugged the output layer by layer i found that the result starts diverging from the first Conv2D layer and the divergence grows significantly after multiple layers of vision encoder model.

I am unable to comprehend why even for a float32 weight the output of GPU delegate varies.
If this is the case then such deep networks of transformers models will always produce poor outputs on Android GPU delegates.


### Standalone code to reproduce the issue

```shell
https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_tf_blip.py

In above code refer to class TFBlipVisionEmbeddings(keras.layers.Layer)

This has a Conv2d operation which when inspected produces different results on CPU and GPU of Android.
The issue is not just with the Conv2d. Later in other layers of TFBlipVisionModel(TFBlipPreTrainedModel) the same divergence explodes for further operations.
I have used the .h5 weights provided on hugging face blipvqa for loading the model.

Any suggestions how to make sure that the output stays close for NNApiDelegate and GPUdelegate.
```


### Relevant log output

_No response_",pulkitagarawal,2024-07-13 09:16:14+00:00,['sawantkumar'],2024-08-09 01:55:13+00:00,2024-08-09 01:55:10+00:00,https://github.com/tensorflow/tensorflow/issues/71809,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2238558981, 'issue_id': 2406814580, 'author': 'sawantkumar', 'body': 'Hi @pulkitagarawal ,\r\n\r\nCan you please provide me the conversion script and also the converted tflite model if possible?', 'created_at': datetime.datetime(2024, 7, 19, 7, 35, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345778, 'issue_id': 2406814580, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005161, 'issue_id': 2406814580, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005229, 'issue_id': 2406814580, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71809"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71809"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 12, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-07-19 07:35:46 UTC): Hi @pulkitagarawal ,

Can you please provide me the conversion script and also the converted tflite model if possible?

github-actions[bot] on (2024-08-02 01:53:26 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-09 01:55:09 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-09 01:55:12 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71809"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71809"">No</a>

"
2406563240,issue,open,,CUDA Messages in TensorFlow 2.17.0 and tf-nightly,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I've updated TensorFlow to version 2.17.0 and noticed that several CUDA-related messages appear during model training.

Example messages:
```
I0000 00:00:1720821406.093527 2748382 cuda_executor.cc:821] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720821406.127659 2748382 cuda_executor.cc:821] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720821406.127833 2748382 cuda_executor.cc:821] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720821406.161631 2748382 cuda_executor.cc:821] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
```

```
I0000 00:00:1720821413.362793 2748566 cuda_dnn.cc:530] Loaded cuDNN version 8907
W0000 00:00:1720821413.423013 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.441550 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.442263 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.447093 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.447949 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.449831 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.450692 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.451745 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.452589 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
W0000 00:00:1720821413.453438 2748566 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced
[...]
```

I receive numerous messages of 'Skipping the delay kernel, measurement accuracy will be reduced' at the beginning of training. The messages don't affect the training process, however, are new and appear only in the recent version (2.17.0 and tf-nightly).

The flag `TF_CPP_MIN_LOG_LEVEL` doesn't make any difference, so any assistance in understanding or suppressing these messages would be appreciated.

### Standalone code to reproduce the issue

When importing tensorflow it is already possible to see some messages of this type.

```
python -c ""import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf;""

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1720831683.868288 3059140 cuda_dnn.cc:8458] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1720831683.872386 3059140 cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
```",arthurflor23,2024-07-13 01:03:29+00:00,['tilakrayal'],2024-10-14 23:36:44+00:00,,https://github.com/tensorflow/tensorflow/issues/71791,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:gpu', 'GPU related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2228303330, 'issue_id': 2406563240, 'author': 'tilakrayal', 'body': '@arthurflor23,\r\nThank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same. \r\n\r\nI request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/62075) and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/70947\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 15, 11, 43, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235181786, 'issue_id': 2406563240, 'author': 'tourist-C', 'body': 'Hi, same issue here with model.fit()\r\n\r\nI am also getting this log message, in addition to the ""Skipping the delay kernel"". Running on WSL \r\n\r\n```\r\n2024-07-18 10:30:32.699962: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n```\r\n\r\nIs there any implication to training speed and training accuracy?\r\n\r\nAppreciate your work thanks!', 'created_at': datetime.datetime(2024, 7, 18, 2, 32, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235292525, 'issue_id': 2406563240, 'author': 'tilakrayal', 'body': 'cc @belitskiy', 'created_at': datetime.datetime(2024, 7, 18, 4, 10, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237115569, 'issue_id': 2406563240, 'author': 'rainwoodman', 'body': 'Hi! The Local Rendezvous message is most likely benign and does not affect your actual training. I recall it shows up when the end of stream is reached on some of the member iterators of the MultiDeviceIterator, but not all of them. Under that theory, if you add drop_remainder=True to tf.data\'s batch call (or somehow otherwise ensure the data is evenly sharded to the underlying devices), I think this error most likely will get away.\r\n\r\nI cannot comment on the ""delay skipped"" warning though.', 'created_at': datetime.datetime(2024, 7, 18, 17, 19, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246394679, 'issue_id': 2406563240, 'author': 'mmtrebuchet', 'body': 'The ""skipping the delay kernel"" is printed a ridiculous number of times for me - one of my training runs spat out this message 4,428 times. I encourage my users to check their logs for warnings and this sort of behavior teaches users that it\'s safe to ignore warnings since they don\'t mean anything. This then leads people to ignore warnings from my code like \r\n`WARNING: Non-integer read count found. Data should not be normalized when training!` and `WARNING: Divide by zero in seqlet distribution!`, and these are things that indicate that something is possibly wrong.\r\n\r\nThis is after I run \r\n\r\n```\r\nimport os\r\nimport warnings\r\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\r\nimport tensorflow as tf\r\ntf.get_logger().setLevel(""ERROR"")\r\nwarnings.simplefilter(""ignore"")\r\n```\r\n\r\nThose messages still get printed, as does\r\n\r\n`WARNING: All log messages before absl::InitializeLog() is called are written to STDERR`\r\n\r\nand a few XLA messages.', 'created_at': datetime.datetime(2024, 7, 23, 22, 7, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254095053, 'issue_id': 2406563240, 'author': 'Zyrin', 'body': 'The ""skipping the delay kernel"" appears to be new with TensorFlow 2.17.0. And it even appears in some official documentation.\r\nHere is a screenshot from https://www.tensorflow.org/guide/function:\r\n![grafik](https://github.com/user-attachments/assets/5530c1e7-71ee-4484-998c-1fcfadd930ce)\r\nhttps://www.tensorflow.org/tutorials/quickstart/advanced is also affected.\r\n\r\nA warning spammed a gazillion times with no apparent way to fix it is more ore less useless.', 'created_at': datetime.datetime(2024, 7, 27, 9, 42, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412488698, 'issue_id': 2406563240, 'author': 'johndpope', 'body': 'resolved in nightly 2.19 \r\n`python3 -m pip install tf-nightly`\r\n\r\nUPDATE - while the gazzilion messages  goes away - now seeing (maybe just my config - ubuntu 22.04)\r\n**No DNN in stream executor. [Op:Conv2D]**\r\n\r\n\r\n\r\npython unit_test.py                  \r\n```shell\r\n2024-10-15 10:23:27.333813: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-10-15 10:23:27.341960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:476] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1728948207.351265  906543 cuda_dnn.cc:8508] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1728948207.354057  906543 cuda_blas.cc:1420] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-10-15 10:23:27.363760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nPython version: 3.11.7\r\nTensorFlow version: 2.19.0-dev20241014\r\nPyTorch version: 2.4.0+cu121\r\nCUDA version: 12.5\r\ncuDNN version: 9\r\nTensorFlow GPU devices: [PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\nPyTorch GPU devices: 1\r\nTensorFlow is built with CUDA\r\nPyTorch CUDA is available\r\n2024-10-15 10:23:30.450699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\nI0000 00:00:1728948210.452098  906543 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9640 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\r\nAttributeError: module \'ml_dtypes\' has no attribute \'float8_e3m4\'\r\n\r\nE0000 00:00:1728948210.914970  906543 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2024-10-15 10:23:30.915354: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at conv_ops_impl.h:1204 : INVALID_ARGUMENT: No DNN in stream executor.\r\n2024-10-15 10:23:30.915386: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: No DNN in stream executor.\r\nError in TensorFlow forward pass: Exception encountered when calling Conv2D.call().\r\n\r\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D]\r\n\r\nArguments received by Conv2D.call():\r\n   inputs=tf.Tensor(shape=(2, 3, 256, 256), dtype=float32)\r\nGPU Info:\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\nTensorFlow version: 2.19.0-dev20241014\r\nCUDA version: 12.5.1\r\ncuDNN version: 9\r\nE\r\n======================================================================\r\nERROR: test_dense_feature_encoder (__main__.TestIMFComponents.test_dense_feature_encoder)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/media/2TB/IMF/unit_test.py"", line 176, in test_dense_feature_encoder\r\n    x = tf_model.initial_conv(x)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/oem/miniconda3/envs/comfyui/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File ""/media/2TB/IMF/tf/model.py"", line 89, in call\r\n    x = self.conv(x)\r\n        ^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling Conv2D.call().\r\n\r\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D]\r\n\r\nArguments received by Conv2D.call():\r\n   inputs=tf.Tensor(shape=(2, 3, 256, 256), dtype=float32)\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.741s\r\n\r\n\r\n```\r\nI think i need cuDNN 9.3.0 (9.5 didn\'t work)\r\nhttps://developer.nvidia.com/cudnn-9-3-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local\r\n\r\ni think it needs cuda 12.6 - i could but gonna just roll back to 2.15\r\n`pip install tensorflow==2.15.0`', 'created_at': datetime.datetime(2024, 10, 14, 23, 7, 56, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-15 11:43:15 UTC): @arthurflor23,
Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same. 

I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/62075) and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue.

https://github.com/tensorflow/tensorflow/issues/70947

Thank you!

tourist-C on (2024-07-18 02:32:09 UTC): Hi, same issue here with model.fit()

I am also getting this log message, in addition to the ""Skipping the delay kernel"". Running on WSL 

```
2024-07-18 10:30:32.699962: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
```

Is there any implication to training speed and training accuracy?

Appreciate your work thanks!

tilakrayal (Assginee) on (2024-07-18 04:10:23 UTC): cc @belitskiy

rainwoodman on (2024-07-18 17:19:03 UTC): Hi! The Local Rendezvous message is most likely benign and does not affect your actual training. I recall it shows up when the end of stream is reached on some of the member iterators of the MultiDeviceIterator, but not all of them. Under that theory, if you add drop_remainder=True to tf.data's batch call (or somehow otherwise ensure the data is evenly sharded to the underlying devices), I think this error most likely will get away.

I cannot comment on the ""delay skipped"" warning though.

mmtrebuchet on (2024-07-23 22:07:23 UTC): The ""skipping the delay kernel"" is printed a ridiculous number of times for me - one of my training runs spat out this message 4,428 times. I encourage my users to check their logs for warnings and this sort of behavior teaches users that it's safe to ignore warnings since they don't mean anything. This then leads people to ignore warnings from my code like 
`WARNING: Non-integer read count found. Data should not be normalized when training!` and `WARNING: Divide by zero in seqlet distribution!`, and these are things that indicate that something is possibly wrong.

This is after I run 

```
import os
import warnings
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""
import tensorflow as tf
tf.get_logger().setLevel(""ERROR"")
warnings.simplefilter(""ignore"")
```

Those messages still get printed, as does

`WARNING: All log messages before absl::InitializeLog() is called are written to STDERR`

and a few XLA messages.

Zyrin on (2024-07-27 09:42:29 UTC): The ""skipping the delay kernel"" appears to be new with TensorFlow 2.17.0. And it even appears in some official documentation.
Here is a screenshot from https://www.tensorflow.org/guide/function:
![grafik](https://github.com/user-attachments/assets/5530c1e7-71ee-4484-998c-1fcfadd930ce)
https://www.tensorflow.org/tutorials/quickstart/advanced is also affected.

A warning spammed a gazillion times with no apparent way to fix it is more ore less useless.

johndpope on (2024-10-14 23:07:56 UTC): resolved in nightly 2.19 
`python3 -m pip install tf-nightly`

UPDATE - while the gazzilion messages  goes away - now seeing (maybe just my config - ubuntu 22.04)
**No DNN in stream executor. [Op:Conv2D]**



python unit_test.py                  
```shell
2024-10-15 10:23:27.333813: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-15 10:23:27.341960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:476] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1728948207.351265  906543 cuda_dnn.cc:8508] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1728948207.354057  906543 cuda_blas.cc:1420] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-15 10:23:27.363760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Python version: 3.11.7
TensorFlow version: 2.19.0-dev20241014
PyTorch version: 2.4.0+cu121
CUDA version: 12.5
cuDNN version: 9
TensorFlow GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
PyTorch GPU devices: 1
TensorFlow is built with CUDA
PyTorch CUDA is available
2024-10-15 10:23:30.450699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
I0000 00:00:1728948210.452098  906543 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9640 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'

E0000 00:00:1728948210.914970  906543 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2024-10-15 10:23:30.915354: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at conv_ops_impl.h:1204 : INVALID_ARGUMENT: No DNN in stream executor.
2024-10-15 10:23:30.915386: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: No DNN in stream executor.
Error in TensorFlow forward pass: Exception encountered when calling Conv2D.call().

{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D]

Arguments received by Conv2D.call():
   inputs=tf.Tensor(shape=(2, 3, 256, 256), dtype=float32)
GPU Info:
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
TensorFlow version: 2.19.0-dev20241014
CUDA version: 12.5.1
cuDNN version: 9
E
======================================================================
ERROR: test_dense_feature_encoder (__main__.TestIMFComponents.test_dense_feature_encoder)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/media/2TB/IMF/unit_test.py"", line 176, in test_dense_feature_encoder
    x = tf_model.initial_conv(x)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/oem/miniconda3/envs/comfyui/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/media/2TB/IMF/tf/model.py"", line 89, in call
    x = self.conv(x)
        ^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling Conv2D.call().

{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D]

Arguments received by Conv2D.call():
   inputs=tf.Tensor(shape=(2, 3, 256, 256), dtype=float32)

----------------------------------------------------------------------
Ran 1 test in 0.741s


```
I think i need cuDNN 9.3.0 (9.5 didn't work)
https://developer.nvidia.com/cudnn-9-3-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local

i think it needs cuda 12.6 - i could but gonna just roll back to 2.15
`pip install tensorflow==2.15.0`

"
2406149247,issue,closed,completed,import tensorflow command gives error,"C:\Users\Prasad>python
Python 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Prasad\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Prasad\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Prasad\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 85, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\Prasad\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
>>>",prasad-mali07,2024-07-12 18:40:04+00:00,['tilakrayal'],2024-07-30 11:08:13+00:00,2024-07-21 14:14:41+00:00,https://github.com/tensorflow/tensorflow/issues/71764,"[('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('subtype:windows', 'Windows Build/Installation Issues')]","[{'comment_id': 2227742878, 'issue_id': 2406149247, 'author': 'tilakrayal', 'body': '@prasad-mali07,\r\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\r\n\r\n```python\r\n- You need to install the MSVC 2019 redistributable\r\n- Your CPU does not support AVX2 instructions\r\n- Your CPU/Python is on 32 bits\r\n- There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/61887\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 15, 5, 51, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2238779747, 'issue_id': 2406149247, 'author': '44Yves', 'body': ""It seems that my CPU doesn't support AVX1 or AVX2. However my msvcp140_1.dll plugin was in a wrong directory as well. But apparently there are certain TF wheel files to install for CPU without AVX support. I appreciate your help in realizing my CPU issue. \r\nEdit: Am using python 3.11, it doesn't matter which TF version I can install, but I've got trouble finding the right pre-built tf.whl with no avx support for my python version, Thank you for the help."", 'created_at': datetime.datetime(2024, 7, 19, 9, 39, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241626250, 'issue_id': 2406149247, 'author': 'mihaimaruseac', 'body': 'Duplicate of #36167', 'created_at': datetime.datetime(2024, 7, 21, 14, 14, 41, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-15 05:51:06 UTC): @prasad-mali07,
Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:

```python
- You need to install the MSVC 2019 redistributable
- Your CPU does not support AVX2 instructions
- Your CPU/Python is on 32 bits
- There is a library that is in a different location/not installed on your system that cannot be loaded.
```

https://github.com/tensorflow/tensorflow/issues/61887

Thank you!

44Yves on (2024-07-19 09:39:12 UTC): It seems that my CPU doesn't support AVX1 or AVX2. However my msvcp140_1.dll plugin was in a wrong directory as well. But apparently there are certain TF wheel files to install for CPU without AVX support. I appreciate your help in realizing my CPU issue. 
Edit: Am using python 3.11, it doesn't matter which TF version I can install, but I've got trouble finding the right pre-built tf.whl with no avx support for my python version, Thank you for the help.

mihaimaruseac on (2024-07-21 14:14:41 UTC): Duplicate of #36167

"
2405454857,issue,open,,`tf.data.Dataset.from_tensor_slices` allocates GPU RAM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. 

This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.

To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

gpu_ram_gb = 12 # adjust for size of GPU 

gb = gpu_ram_gb+1; dtype = ""float64""
size = (gb * 1024**3) // tf.dtypes.as_dtype(dtype).size

x = np.zeros((size,), dtype = dtype)

tf.data.Dataset.from_tensor_slices(x)
```


### Relevant log output

```shell
2024-07-12 08:20:42.771788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 08:20:42.784893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 08:20:42.788889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 08:20:42.798174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 08:20:43.492876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-07-12 08:20:50.714170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5
2024-07-12 08:20:50.715606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.
2024-07-12 08:21:05.997032: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-07-12 08:21:05.997054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-07-12 08:21:05.997064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997072: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: 
2024-07-12 08:21:05.997219: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-07-12 08:21:05.997225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B
2024-07-12 08:21:05.997232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584
2024-07-12 08:21:05.997241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                        35651584
InUse:                               0
MaxInUse:                            0
NumAllocs:                           0
MaxAllocSize:                        0
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-07-12 08:21:05.997248: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] <allocator contains no memory>

Traceback (most recent call last):
  File ""/home/tomasz/github/rstudio/keras3/test.py"", line 16, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 826, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 33, in __init__
    element = structure.normalize_element(element)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py"", line 134, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 713, in convert_to_tensor
    return tensor_conversion_registry.convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 234, in convert
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py"", line 29, in _constant_tensor_conversion_function
    return constant_op.constant(v, dtype=dtype, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 289, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
```
",t-kalinowski,2024-07-12 12:24:35+00:00,"['jsimsa', 'aaudiber']",2024-08-14 12:06:41+00:00,,https://github.com/tensorflow/tensorflow/issues/71744,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:data', 'tf.data related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2225583745, 'issue_id': 2405454857, 'author': 't-kalinowski', 'body': 'Minor correction, this bug is also present in TF v2.16.2. It is *not* present in 2.13.1.', 'created_at': datetime.datetime(2024, 7, 12, 13, 23, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2228288100, 'issue_id': 2405454857, 'author': 'tilakrayal', 'body': 'When I tried to execute the mentioned code on tensorflow v2.16, v2.15 & tf-nightly, it was crashing with the failure. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/47f10ca2372cbf5be336b86343f7d11f/untitled2002.ipynb).', 'created_at': datetime.datetime(2024, 7, 15, 11, 34, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2228362117, 'issue_id': 2405454857, 'author': 't-kalinowski', 'body': ""The colab gist you linked to has a system configured where the system RAM (12 gb) is smaller than the GPU RAM, and it crashes with OOM before the `tf.data.Dataset.from_tensor_slices()` call. \r\n\r\nI'm confident that, with just a little effort, you can find 10 other ways to confirm GPU ram is being allocated when a numpy array is passed to `tf.data.Dataset.from_tensor_slices()`."", 'created_at': datetime.datetime(2024, 7, 15, 12, 14, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288566084, 'issue_id': 2405454857, 'author': '372046933', 'body': 'If eager execution is disabled. The following error occurs.\r\n```\r\nTraceback (most recent call last):                                                                                                                                           \r\n  File ""/tmp/test_dataset_v2.py"", line 12, in <module>\r\n    tf.data.Dataset.from_tensor_slices(x)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 793, in from_tensor_slices\r\n    return TensorSliceDataset(tensors, name=name)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4477, in __init__\r\n    element = structure.normalize_element(element)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py"", line 125, in normalize_element\r\n    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 1695, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 48, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py"", line 267, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl\r\n    tensor_util.make_tensor_proto(\r\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_util.py"", line 521, in make_tensor_proto\r\n    raise ValueError(\r\nValueError: Cannot create a tensor proto whose content is larger than 2GB\r\n```', 'created_at': datetime.datetime(2024, 8, 14, 12, 6, 40, tzinfo=datetime.timezone.utc)}]","t-kalinowski (Issue Creator) on (2024-07-12 13:23:13 UTC): Minor correction, this bug is also present in TF v2.16.2. It is *not* present in 2.13.1.

tilakrayal on (2024-07-15 11:34:16 UTC): When I tried to execute the mentioned code on tensorflow v2.16, v2.15 & tf-nightly, it was crashing with the failure. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/47f10ca2372cbf5be336b86343f7d11f/untitled2002.ipynb).

t-kalinowski (Issue Creator) on (2024-07-15 12:14:58 UTC): The colab gist you linked to has a system configured where the system RAM (12 gb) is smaller than the GPU RAM, and it crashes with OOM before the `tf.data.Dataset.from_tensor_slices()` call. 

I'm confident that, with just a little effort, you can find 10 other ways to confirm GPU ram is being allocated when a numpy array is passed to `tf.data.Dataset.from_tensor_slices()`.

372046933 on (2024-08-14 12:06:40 UTC): If eager execution is disabled. The following error occurs.
```
Traceback (most recent call last):                                                                                                                                           
  File ""/tmp/test_dataset_v2.py"", line 12, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 793, in from_tensor_slices
    return TensorSliceDataset(tensors, name=name)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4477, in __init__
    element = structure.normalize_element(element)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py"", line 125, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 1695, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 48, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py"", line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl
    tensor_util.make_tensor_proto(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_util.py"", line 521, in make_tensor_proto
    raise ValueError(
ValueError: Cannot create a tensor proto whose content is larger than 2GB
```

"
2405343015,issue,closed,not_planned,TensorFlow Lite with iOS MTLBuffer doesn't support dynamic shape,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

No

### OS platform and distribution

iOS

### Mobile device

iPhone

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use tflite with Metal MTLBuffer on iOS following the doc : https://www.tensorflow.org/lite/ios/delegates/gpu#inputoutput_buffers_using_c_api

My model.tflite is designed for dynamic shape usecase, so the input/output shape is saved as [1,-1,-1,1] in the model. 

I've tried to call `ResizeInputTensor` before `ModifyGraphWithDelegate` , but it causes 
` Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)` when calling `Invoke`.

If I don't call `ResizeInputTensor` before `ModifyGraphWithDelegate`, then I still got nothing from output tensor. 

I want to know if tensorflowlite metal delegate supports dynamic shape or not, and how to use it with dynamic shape correctly?

### Standalone code to reproduce the issue

```shell
tflite::ops::builtin::BuiltinOpResolver op_resolver;
        tflite::InterpreterBuilder interpreter_builder(model, op_resolver);

         // Configure and create the delegate.
        TFLGpuDelegateOptions options;
        options.enable_quantization = true;
        options.allow_precision_loss = true;
        options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeActive;
        _gpu_delegate = TFLGpuDelegateCreate(&options);

        if (interpreter_builder(&_predictor) != kTfLiteOk || !_predictor) {
            GLOGE(""Unable to prepare TfLite interpreter."");
        }
        TfLiteStatus status;
       status  = _predictor->ResizeInputTensor(0, {1, input_height, input_width, 1});
        if (status != kTfLiteOk) {
            GLOGE(""Failed to resize input tensor: {}"", status);
            return;
        }

        status = _predictor->ModifyGraphWithDelegate(_gpu_delegate);
        if (status != kTfLiteOk) {
            GLOGE(""Failed to ModifyGraphWithDelegate: {}"", status);
            return;
        }

        _predictor->SetAllowBufferHandleOutput(true);  // disable default gpu->cpu copy
        
       // id<MTLBuffer> input and  id<MTLBuffer> output from other parts of my codes
        if (!TFLGpuDelegateBindMetalBufferToTensor(
            _gpu_delegate, _predictor->inputs()[0], input)) {
            GLOGE(""Failed to TFLGpuDelegateBindMetalBufferToTensor input"");
            return false;
        }
        if (!TFLGpuDelegateBindMetalBufferToTensor(
                _gpu_delegate, _predictor->outputs()[0], output)) {
            GLOGE(""Failed to TFLGpuDelegateBindMetalBufferToTensor output"");
            return false;
        }

        id<MTLCommandBuffer> command_buffer = [_metal_queue commandBuffer];
        command_buffer.label = @""TfliteMetalRunner"";
        TFLGpuDelegateSetCommandBuffer(_gpu_delegate, command_buffer);

        if (_predictor->Invoke() != kTfLiteOk) {
            GLOGE(""metal runner invoke failed"");
            return false;
        }
            GLOGE(""metal runner invoke success"");

        [command_buffer commit];
        [command_buffer waitUntilScheduled];
```


### Relevant log output

```shell
2024-07-12 18:54:38.834941+0800 myapp[6052:2224696] Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)
2024-07-12 18:54:38.890017+0800 myapp[6052:2214145] Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)
2024-07-12 18:54:38.920375+0800 myapp[6052:2214145] Execution of the command buffer was aborted due to an error during execution. Ignored (for causing prior/excessive GPU errors) (00000004:kIOGPUCommandBufferCallbackErrorSubmissionsIgnored)
2024-07-12 18:54:38.920508+0800 myapp[6052:2214145] Execution of the command buffer was aborted due to an error during execution. Ignored (for causing prior/excessive GPU errors) (00000004:kIOGPUCommandBufferCallbackErrorSubmissionsIgnored)
```
",zhanghuicuc,2024-07-12 11:18:44+00:00,"['yishuangP', 'pkgoogle', 'sawantkumar']",2024-11-26 18:08:28+00:00,2024-11-26 18:08:28+00:00,https://github.com/tensorflow/tensorflow/issues/71740,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('comp:lite', 'TF Lite related issues'), ('iOS', ''), ('TF 2.16', '')]","[{'comment_id': 2231902881, 'issue_id': 2405343015, 'author': 'pkgoogle', 'body': ""Hi @zhanghuicuc, we definitely expect dynamic shapes for C++/Python interfaces: https://www.tensorflow.org/lite/guide/inference#run_inference_with_dynamic_shape_model ... I don't see an example for MTLBuffer ... so let's assume it's supported for now. Do you have your .tflite model/file available so that I may test/reproduce this? I'm assuming you are using xcode, if you can share as much as your project as possible, that will help us look into this. Thanks."", 'created_at': datetime.datetime(2024, 7, 16, 22, 7, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232190459, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': '@pkgoogle  could you tell me your e-mail? so I can send you the model and codes.', 'created_at': datetime.datetime(2024, 7, 17, 2, 3, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2234028763, 'issue_id': 2405343015, 'author': 'pkgoogle', 'body': 'Hi @zhanghuicuc, is there any way you can use drive.google.com? It is easier for us to share via that (you have to give me or others permission or something like that). For my own safety, I tend not to give out my email.', 'created_at': datetime.datetime(2024, 7, 17, 18, 56, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241950945, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': '@pkgoogle \r\ntflite model link: https://drive.google.com/file/d/1ZsmF1kiXlzOYNHaVR7a4h4kXPZ5_Ercc/view?usp=sharing', 'created_at': datetime.datetime(2024, 7, 22, 2, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242156694, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': '@pkgoogle \r\nxcode project demo link: https://drive.google.com/file/d/1v_osYI36D_frhMYBS1VEkjieBon0s0RT/view?usp=sharing\r\nThe related codes are in tflite_metal_runner.mm.', 'created_at': datetime.datetime(2024, 7, 22, 6, 5, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243554299, 'issue_id': 2405343015, 'author': 'pkgoogle', 'body': '@zhanghuicuc, I have requested access, please grant when appropriate.', 'created_at': datetime.datetime(2024, 7, 22, 18, 21, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243807438, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': '@pkgoogle granted', 'created_at': datetime.datetime(2024, 7, 22, 21, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251384440, 'issue_id': 2405343015, 'author': 'pkgoogle', 'body': ""I'm running into simulator issues reproducing your issue. @yishuangP, can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 7, 25, 20, 58, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2258057422, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': ""@yishuangP I'd like to know if there is any update or any infromation you want me to provide ?"", 'created_at': datetime.datetime(2024, 7, 30, 10, 50, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283296446, 'issue_id': 2405343015, 'author': 'zhanghuicuc', 'body': '@pkgoogle hiany update here?', 'created_at': datetime.datetime(2024, 8, 12, 7, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500263100, 'issue_id': 2405343015, 'author': 'gaikwadrahul8', 'body': ""Hi, @zhanghuicuc \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/46\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 39, 38, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-16 22:07:21 UTC): Hi @zhanghuicuc, we definitely expect dynamic shapes for C++/Python interfaces: https://www.tensorflow.org/lite/guide/inference#run_inference_with_dynamic_shape_model ... I don't see an example for MTLBuffer ... so let's assume it's supported for now. Do you have your .tflite model/file available so that I may test/reproduce this? I'm assuming you are using xcode, if you can share as much as your project as possible, that will help us look into this. Thanks.

zhanghuicuc (Issue Creator) on (2024-07-17 02:03:44 UTC): @pkgoogle  could you tell me your e-mail? so I can send you the model and codes.

pkgoogle (Assginee) on (2024-07-17 18:56:06 UTC): Hi @zhanghuicuc, is there any way you can use drive.google.com? It is easier for us to share via that (you have to give me or others permission or something like that). For my own safety, I tend not to give out my email.

zhanghuicuc (Issue Creator) on (2024-07-22 02:59:56 UTC): @pkgoogle 
tflite model link: https://drive.google.com/file/d/1ZsmF1kiXlzOYNHaVR7a4h4kXPZ5_Ercc/view?usp=sharing

zhanghuicuc (Issue Creator) on (2024-07-22 06:05:07 UTC): @pkgoogle 
xcode project demo link: https://drive.google.com/file/d/1v_osYI36D_frhMYBS1VEkjieBon0s0RT/view?usp=sharing
The related codes are in tflite_metal_runner.mm.

pkgoogle (Assginee) on (2024-07-22 18:21:53 UTC): @zhanghuicuc, I have requested access, please grant when appropriate.

zhanghuicuc (Issue Creator) on (2024-07-22 21:00:53 UTC): @pkgoogle granted

pkgoogle (Assginee) on (2024-07-25 20:58:30 UTC): I'm running into simulator issues reproducing your issue. @yishuangP, can you please take a look? Thanks.

zhanghuicuc (Issue Creator) on (2024-07-30 10:50:44 UTC): @yishuangP I'd like to know if there is any update or any infromation you want me to provide ?

zhanghuicuc (Issue Creator) on (2024-08-12 07:41:00 UTC): @pkgoogle hiany update here?

gaikwadrahul8 on (2024-11-26 10:39:38 UTC): Hi, @zhanghuicuc 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/46

Let us know if you have any questions. Thanks.

"
2404884907,issue,closed,completed, 'Sequential' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?,"Im using the latest version of tensorflow installed from PIP

I'm getting the error:   File ""/Users/andrewattard/Downloads/Foodity Dataset/main.py"", line 88, in <module>
    tflite_model = converter.convert()
                   ^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/lite.py"", line 1175, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/lite.py"", line 1129, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/lite.py"", line 1641, in convert
    self._freeze_keras_model()
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/lite.py"", line 1582, in _freeze_keras_model
    input_signature = _model_input_signature(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/andrewattard/miniforge3/envs/foodity/lib/python3.12/site-packages/tensorflow/lite/python/tflite_keras_util.py"", line 84, in model_input_signature
    input_specs = model._get_save_spec(  # pylint: disable=protected-access
                  ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Sequential' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?

My code is

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing import image_dataset_from_directory
import tensorflow.lite as tflite
import os

# Constants
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 1
DIRECTORY_PATH = ""./archive/Dataset""  # Update this to the path of your dataset
LABELS_FILE = ""labels.txt""

# Load the data
train_dataset = image_dataset_from_directory(
    DIRECTORY_PATH,
    validation_split=0.2,
    subset=""training"",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

validation_dataset = image_dataset_from_directory(
    DIRECTORY_PATH,
    validation_split=0.2,
    subset=""validation"",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

# Prefetching for better performance
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)

# Data augmentation
data_augmentation = tf.keras.Sequential(
    [
        tf.keras.layers.RandomFlip(""horizontal_and_vertical""),
        tf.keras.layers.RandomRotation(0.2),
    ]
)

# Manually extract class names from the directory structure
class_names = sorted(item.name for item in os.scandir(DIRECTORY_PATH) if item.is_dir())

# Save the labels to a file
with open(LABELS_FILE, ""w"") as f:
    for label in class_names:
        f.write(f""{label}\n"")

print(f""Labels saved to {LABELS_FILE}"")

# Build the model
model = tf.keras.Sequential([
    data_augmentation,
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(len(class_names), activation='softmax')  # Number of classes
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=EPOCHS
)

# Convert the model to TensorFlow Lite format with quantization
converter = tflite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the TensorFlow Lite model
tflite_model_path = ""model_quantized.tflite""
with open(tflite_model_path, ""wb"") as f:
    f.write(tflite_model)

print(f""Quantized model saved to {tflite_model_path}"")
```
",Leli1024,2024-07-12 06:52:05+00:00,['sawantkumar'],2024-08-13 01:55:35+00:00,2024-08-13 01:55:30+00:00,https://github.com/tensorflow/tensorflow/issues/71726,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2224947498, 'issue_id': 2404884907, 'author': 'Leli1024', 'body': 'UPDATE:\r\n\r\nAdding the following code works as a workaround\r\n\r\n```\r\nimport os\r\n\r\nos.environ[""TF_USE_LEGACY_KERAS""]=""1""\r\n```', 'created_at': datetime.datetime(2024, 7, 12, 6, 57, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254989893, 'issue_id': 2404884907, 'author': 'sawantkumar', 'body': 'Hi @Leli1024 ,\r\n\r\nThis issue is resolved in tf 2.17, can you please retry your code with tf version 2.17 and let me know if it works for you.', 'created_at': datetime.datetime(2024, 7, 29, 5, 42, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213458, 'issue_id': 2404884907, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196397, 'issue_id': 2404884907, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196479, 'issue_id': 2404884907, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71726"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71726"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 34, tzinfo=datetime.timezone.utc)}]","Leli1024 (Issue Creator) on (2024-07-12 06:57:23 UTC): UPDATE:

Adding the following code works as a workaround

```
import os

os.environ[""TF_USE_LEGACY_KERAS""]=""1""
```

sawantkumar (Assginee) on (2024-07-29 05:42:40 UTC): Hi @Leli1024 ,

This issue is resolved in tf 2.17, can you please retry your code with tf version 2.17 and let me know if it works for you.

github-actions[bot] on (2024-08-06 01:53:35 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-13 01:55:29 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-13 01:55:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71726"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71726"">No</a>

"
2404305433,issue,closed,completed,The Flower_Classification_with_TFLite_Model_Maker.ipynb runs low on memory,"While executing the following line `!pip install tflite-model-maker`, the T4 environment runs low on memory, and so no labeled date nor model can be downloaded",benjaminreynoso,2024-07-11 22:52:04+00:00,['tilakrayal'],2024-08-02 05:04:51+00:00,2024-08-01 01:57:49+00:00,https://github.com/tensorflow/tensorflow/issues/71701,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('type:performance', 'Performance Issue'), ('TFLiteModelMaker', 'TFLite Model Maker related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2227899368, 'issue_id': 2404305433, 'author': 'tilakrayal', 'body': ""@benjaminreynoso,\r\nCould you please let me know if this is happening only on the colab or on the virtual environment as well. I tried on the other ubuntu environment and haven't faced the similar issue. Thank you!"", 'created_at': datetime.datetime(2024, 7, 15, 7, 55, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244102002, 'issue_id': 2404305433, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 23, 1, 53, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799710, 'issue_id': 2404305433, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799803, 'issue_id': 2404305433, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71701"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71701"">No</a>', 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 52, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-15 07:55:25 UTC): @benjaminreynoso,
Could you please let me know if this is happening only on the colab or on the virtual environment as well. I tried on the other ubuntu environment and haven't faced the similar issue. Thank you!

github-actions[bot] on (2024-07-23 01:53:15 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-01 01:57:48 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-01 01:57:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71701"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71701"">No</a>

"
2403211835,issue,closed,completed,Cannot create BertQA model with  tflite-model-maker==0.4.2 (or 4.3),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.8.4

### Custom code

Yes

### OS platform and distribution

Fedora 40

### Mobile device

_No response_

### Python version

3.8.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CPU

### GPU model and memory

_No response_

### Current behavior?

I'm trying to train a BERTQA model (Squad v2 format) with tflite-model-maker in order to use it on Android

Here is the code:

```python
from tflite_model_maker import model_spec
from tflite_model_maker import question_answer
from tflite_model_maker.question_answer import DataLoader

import tensorflow as tf
assert tf.__version__.startswith('2')
tf.get_logger().setLevel('ERROR')

spec = model_spec.get('mobilebert_qa_squad')

train_data = DataLoader.from_squad(filename='final.json',model_spec=spec,is_training=True,version_2_with_negative=True)
test_data = DataLoader.from_squad(filename='final_val.json',model_spec=spec,is_training=False,version_2_with_negative=True)

model = question_answer.create(train_data, model_spec=spec, epochs=2)

print(model.summary())

res = model.evaluate(test_data) # <==== RES IS NONE

print('Loss:', res[0])
print('Accuracy:', res[1])

model.export(export_dir='mobilebert',
    tflite_filename='ingredients.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None)
```


[final_val.json](https://github.com/user-attachments/files/16177865/final_val.json)
[final.json](https://github.com/user-attachments/files/16177866/final.json)

### Standalone code to reproduce the issue

```shell
pip install tflite-model-maker==0.4.2
pip install tensorflow-addons==0.17.1
pip install tensorflow-cpu==2.8.4
```

Here is the output of the previous script:

```
(.venv)  main   python train.py
WARNING:absl:Could not find answer: 'd'ail,' vs. 'd' ail,'
2024-07-11 09:07:50.409590: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/2
80/80 [==============================] - 528s 6s/step - loss: 0.6571 - start_positions_loss: 0.5835 - end_positions_loss: 0.7307
Epoch 2/2
80/80 [==============================] - 505s 6s/step - loss: 0.1952 - start_positions_loss: 0.1642 - end_positions_loss: 0.2262
Model: ""model""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_word_ids (InputLayer)    [(None, 384)]        0           []                               
                                                                                                  
 input_mask (InputLayer)        [(None, 384)]        0           []                               
                                                                                                  
 input_type_ids (InputLayer)    [(None, 384)]        0           []                               
                                                                                                  
 hub_keras_layer_v1v2 (HubKeras  {'end_logits': (Non  24582914   ['input_word_ids[0][0]',         
 LayerV1V2)                     e, 384),                          'input_mask[0][0]',             
                                 'start_logits': (N               'input_type_ids[0][0]']         
                                one, 384)}                                                        
                                                                                                  
 start_positions (Lambda)       (None, 384)          0           ['hub_keras_layer_v1v2[0][1]']   
                                                                                                  
 end_positions (Lambda)         (None, 384)          0           ['hub_keras_layer_v1v2[0][0]']   
                                                                                                  
==================================================================================================
Total params: 24,582,914
Trainable params: 24,582,914
Non-trainable params: 0
__________________________________________________________________________________________________
None
Traceback (most recent call last):
  File ""train.py"", line 25, in <module>
    print('Loss:', res[0])
KeyError: 0
```


### Relevant log output

_No response_",AmarOk1412,2024-07-11 13:35:48+00:00,"['pkgoogle', 'sawantkumar']",2024-08-09 01:55:16+00:00,2024-08-09 01:55:11+00:00,https://github.com/tensorflow/tensorflow/issues/71666,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.8', ''), ('TFLiteModelMaker', 'TFLite Model Maker related issues')]","[{'comment_id': 2224408281, 'issue_id': 2403211835, 'author': 'sushreebarsa', 'body': '@AmarOk1412 Creating a BERT QA model with a TensorFlow Lite Model Maker can sometimes be tricky due to compatibility issues. Could you please upgrade to the latest TF version and let us know the outcome?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 12, 3, 55, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2225782602, 'issue_id': 2403211835, 'author': 'AmarOk1412', 'body': 'I can\'t because I can\'t install it with pip (tried python 3.8, 3.9, 3.10, 3.11 and 3.12)\r\n\r\n```\r\nERROR: pip\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nscann 1.2.6 requires tensorflow~=2.8.0, but you have tensorflow 2.11.0 which is incompatible.\r\n\r\n\r\n# Then running:\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File ""train.py"", line 1, in <module>\r\n    from tflite_model_maker import model_spec\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tflite_model_maker/__init__.py"", line 51, in <module>\r\n    from tflite_model_maker import searcher\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tflite_model_maker/searcher/__init__.py"", line 25, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task.searcher import ExportFormat\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/searcher.py"", line 30, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.utils import ondevice_scann_builder\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/utils/ondevice_scann_builder.py"", line 17, in <module>\r\n    from scann.proto import scann_pb2\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/__init__.py"", line 2, in <module>\r\n    from scann.scann_ops.py import scann_ops\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/scann_ops/py/scann_ops.py"", line 23, in <module>\r\n    _scann_ops_so = tf.load_op_library(\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so: undefined symbol: _ZN4absl12lts_2021032416numbers_internal9kHexTableE\r\n```\r\n\r\nBut here is a version with 2.13 (Scann 1.2.6 doesn\'t like Python 3.10, so I used 3.8 and 2.13 is the max)\r\n\r\n```\r\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python  python train.py\r\n2024-07-12 10:29:57.563757: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-07-12 10:29:57.565045: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 10:29:57.590564: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 10:29:57.590860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-12 10:29:58.233409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \r\n\r\nTensorFlow Addons (TFA) has ended development and introduction of new features.\r\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\r\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \r\n\r\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \r\n\r\n  warnings.warn(\r\nEpoch 1/2\r\n2024-07-12 10:38:04.953281: W tensorflow/core/framework/op_kernel.cc:1805] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\r\n2024-07-12 10:38:05.100766: W tensorflow/core/framework/op_kernel.cc:1805] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\r\nTraceback (most recent call last):\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 19, in <module>\r\n    model = question_answer.create(train_data, model_spec=spec, epochs=2, steps_per_epoch=50)\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 228, in create\r\n    model.train(train_data, epochs, batch_size, steps_per_epoch)\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 79, in train\r\n    self.model = self.model_spec.train(\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/text_spec.py"", line 944, in train\r\n    bert_model.fit(x=train_ds, epochs=epochs, **kwargs)\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\r\n\r\nDetected at node \'Adam/Cast\' defined at (most recent call last):\r\n    File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 19, in <module>\r\n      model = question_answer.create(train_data, model_spec=spec, epochs=2, steps_per_epoch=50)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 228, in create\r\n      model.train(train_data, epochs, batch_size, steps_per_epoch)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 79, in train\r\n      self.model = self.model_spec.train(\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/text_spec.py"", line 944, in train\r\n      bert_model.fit(x=train_ds, epochs=epochs, **kwargs)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1742, in fit\r\n      tmp_logs = self.train_function(iterator)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1338, in train_function\r\n      return step_function(self, iterator)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1322, in step_function\r\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1303, in run_step\r\n      outputs = model.train_step(data)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1084, in train_step\r\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 544, in minimize\r\n      self.apply_gradients(grads_and_vars)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/official/nlp/optimization.py"", line 175, in apply_gradients\r\n      return super(AdamWeightDecay, self).apply_gradients(\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1230, in apply_gradients\r\n      return super().apply_gradients(grads_and_vars, name=name)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 650, in apply_gradients\r\n      self._apply_weight_decay(trainable_variables)\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1249, in _apply_weight_decay\r\n      tf.__internal__.distribute.interim.maybe_merge_call(\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1245, in distributed_apply_weight_decay\r\n      distribution.extended.update(\r\n    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1241, in weight_decay_fn\r\n      wd = tf.cast(self.weight_decay, variable.dtype)\r\nNode: \'Adam/Cast\'\r\nCast string to float is not supported\r\n\t [[{{node Adam/Cast}}]] [Op:__inference_train_function_181018]\r\n(.venv3.9)   amarok@L-SBLIN \ue0b0 ~/Projects/PlanEat/models/ingredients \ue0b0 \ue0a0 main  \ue0b0 pip freeze             \r\nabsl-py==1.4.0\r\narray_record==0.5.1\r\nastunparse==1.6.3\r\nattrs==23.2.0\r\naudioread==3.0.1\r\nbleach==6.1.0\r\ncachetools==5.3.3\r\ncertifi==2024.7.4\r\ncffi==1.16.0\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncycler==0.12.1\r\nCython==3.0.10\r\ndataclasses==0.6\r\ndecorator==5.1.1\r\ndm-tree==0.1.8\r\netils==1.5.2\r\nfire==0.6.0\r\nflatbuffers==24.3.25\r\nfsspec==2024.6.1\r\ngast==0.4.0\r\ngin-config==0.5.0\r\ngoogle-api-core==2.19.1\r\ngoogle-api-python-client==2.137.0\r\ngoogle-auth==2.32.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-auth-oauthlib==1.0.0\r\ngoogle-cloud-bigquery==3.25.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-crc32c==1.5.0\r\ngoogle-pasta==0.2.0\r\ngoogle-resumable-media==2.7.1\r\ngoogleapis-common-protos==1.63.1\r\ngrpcio==1.64.1\r\ngrpcio-status==1.48.2\r\nh5py==3.11.0\r\nhttplib2==0.22.0\r\nidna==3.7\r\nimportlib_metadata==8.0.0\r\nimportlib_resources==6.4.0\r\njoblib==1.4.2\r\nkaggle==1.6.14\r\nkeras==2.13.1\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.4.5\r\nlibclang==18.1.1\r\nlibrosa==0.8.1\r\nllvmlite==0.43.0\r\nlxml==5.2.2\r\nMarkdown==3.6\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmatplotlib==3.4.3\r\nmdurl==0.1.2\r\nml-dtypes==0.4.0\r\nnamex==0.0.8\r\nneural-structured-learning==1.4.0\r\nnumba==0.60.0\r\nnumpy==1.23.0\r\noauthlib==3.2.2\r\nopencv-python-headless==4.10.0.84\r\nopt-einsum==3.3.0\r\noptree==0.12.1\r\npackaging==20.9\r\npandas==2.2.2\r\npillow==10.4.0\r\nplatformdirs==4.2.2\r\npooch==1.8.2\r\npromise==2.3\r\nproto-plus==1.24.0\r\nprotobuf==4.25.3\r\npsutil==6.0.0\r\npy-cpuinfo==9.0.0\r\npyasn1==0.6.0\r\npyasn1_modules==0.4.0\r\npybind11==2.13.1\r\npycparser==2.22\r\nPygments==2.18.0\r\npyparsing==3.1.2\r\npython-dateutil==2.9.0.post0\r\npython-slugify==8.0.4\r\npytz==2024.1\r\nPyYAML==6.0.1\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nresampy==0.4.3\r\nrich==13.7.1\r\nrsa==4.9\r\nscann==1.2.10\r\nscikit-learn==1.5.1\r\nscipy==1.13.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsounddevice==0.4.7\r\nsoundfile==0.12.1\r\ntensorboard==2.13.0\r\ntensorboard-data-server==0.7.2\r\ntensorboard-plugin-wit==1.8.1\r\ntensorflow==2.13.1\r\ntensorflow-addons==0.23.0\r\ntensorflow-datasets==4.9.0\r\ntensorflow-estimator==2.13.0\r\ntensorflow-hub==0.12.0\r\ntensorflow-io-gcs-filesystem==0.37.1\r\ntensorflow-metadata==1.13.0\r\ntensorflow-model-optimization==0.8.0\r\ntensorflowjs==3.18.0\r\ntermcolor==2.4.0\r\ntext-unidecode==1.3\r\ntf-models-official==2.3.0\r\ntf-slim==1.1.0\r\ntflite-model-maker==0.4.3\r\ntflite-support==0.4.4\r\nthreadpoolctl==3.5.0\r\ntoml==0.10.2\r\ntqdm==4.66.4\r\ntypeguard==2.13.3\r\ntyping_extensions==4.5.0\r\ntzdata==2024.1\r\nuritemplate==4.1.1\r\nurllib3==1.25.11\r\nwebencodings==0.5.1\r\nWerkzeug==3.0.3\r\nwrapt==1.16.0\r\nzipp==3.19.2\r\n```\r\n\r\nThen with tensorflow 2.17:\r\n\r\n```\r\n python train.py                           \r\n2024-07-12 10:59:02.240329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-07-12 10:59:02.240648: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 10:59:02.243211: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 10:59:02.249174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-07-12 10:59:02.258851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-07-12 10:59:02.261519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-07-12 10:59:02.269337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-12 10:59:02.878013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 1, in <module>\r\n    from tflite_model_maker import model_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/__init__.py"", line 44, in <module>\r\n    from tflite_model_maker import audio_classifier\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/audio_classifier/__init__.py"", line 24, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataLoader\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/audio_dataloader.py"", line 27, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/__init__.py"", line 20, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/audio_spec.py"", line 30, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task import model_util\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_util.py"", line 28, in <module>\r\n    from tensorflowjs.converters import converter as tfjs_converter\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/__init__.py"", line 21, in <module>\r\n    from tensorflowjs import converters\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>\r\n    from tensorflowjs.converters.converter import convert\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>\r\n    from tensorflowjs.converters import tf_saved_model_conversion_v2\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 42, in <module>\r\n    import tensorflow_hub as hub\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/__init__.py"", line 88, in <module>\r\n    from tensorflow_hub.estimator import LatestModuleExporter\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/estimator.py"", line 62, in <module>\r\n    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow/python/util/module_wrapper.py"", line 232, in _getattr\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\nAttributeError: module \'tensorflow.compat.v1\' has no attribute \'estimator\'\r\n```\r\n\r\nAnd 2.15.0\r\n\r\n```\r\n2024-07-12 11:01:56.289875: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-07-12 11:01:56.291246: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 11:01:56.313500: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-07-12 11:01:56.313534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-07-12 11:01:56.314475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-07-12 11:01:56.318960: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-07-12 11:01:56.319116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-12 11:01:56.967406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 1, in <module>\r\n    from tflite_model_maker import model_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/__init__.py"", line 44, in <module>\r\n    from tflite_model_maker import audio_classifier\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/audio_classifier/__init__.py"", line 24, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataLoader\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/audio_dataloader.py"", line 27, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/__init__.py"", line 20, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/audio_spec.py"", line 30, in <module>\r\n    from tensorflow_examples.lite.model_maker.core.task import model_util\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_util.py"", line 28, in <module>\r\n    from tensorflowjs.converters import converter as tfjs_converter\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/__init__.py"", line 21, in <module>\r\n    from tensorflowjs import converters\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>\r\n    from tensorflowjs.converters.converter import convert\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>\r\n    from tensorflowjs.converters import tf_saved_model_conversion_v2\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 42, in <module>\r\n    import tensorflow_hub as hub\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/__init__.py"", line 93, in <module>\r\n    from tensorflow_hub.feature_column_v2 import text_embedding_column_v2\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/feature_column_v2.py"", line 24, in <module>\r\n    from tensorflow_hub import keras_layer\r\n  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/keras_layer.py"", line 27, in <module>\r\n    from tensorflow.python.training.tracking import data_structures\r\nModuleNotFoundError: No module named \'tensorflow.python.training.tracking\'\r\n(.venv3.9)   amarok@L-SBLIN \ue0b0 ~/Projects/PlanEat/models/ingredients \ue0b0 \ue0a0 main  \ue0b0 pip freeze                  \r\nabsl-py==1.4.0\r\narray_record==0.5.1\r\nastunparse==1.6.3\r\nattrs==23.2.0\r\naudioread==3.0.1\r\nbleach==6.1.0\r\ncachetools==5.3.3\r\ncertifi==2024.7.4\r\ncffi==1.16.0\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncycler==0.12.1\r\nCython==3.0.10\r\ndataclasses==0.6\r\ndecorator==5.1.1\r\ndm-tree==0.1.8\r\netils==1.5.2\r\nfire==0.6.0\r\nflatbuffers==24.3.25\r\nfsspec==2024.6.1\r\ngast==0.4.0\r\ngin-config==0.5.0\r\ngoogle-api-core==2.19.1\r\ngoogle-api-python-client==2.137.0\r\ngoogle-auth==2.32.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-auth-oauthlib==1.0.0\r\ngoogle-cloud-bigquery==3.25.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-crc32c==1.5.0\r\ngoogle-pasta==0.2.0\r\ngoogle-resumable-media==2.7.1\r\ngoogleapis-common-protos==1.63.1\r\ngrpcio==1.64.1\r\ngrpcio-status==1.48.2\r\nh5py==3.11.0\r\nhttplib2==0.22.0\r\nidna==3.7\r\nimportlib_metadata==8.0.0\r\nimportlib_resources==6.4.0\r\njoblib==1.4.2\r\nkaggle==1.6.14\r\nkeras==3.4.1\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.4.5\r\nlibclang==18.1.1\r\nlibrosa==0.8.1\r\nllvmlite==0.43.0\r\nlxml==5.2.2\r\nMarkdown==3.6\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmatplotlib==3.4.3\r\nmdurl==0.1.2\r\nml-dtypes==0.2.0\r\nnamex==0.0.8\r\nneural-structured-learning==1.4.0\r\nnumba==0.60.0\r\nnumpy==1.23.4\r\noauthlib==3.2.2\r\nopencv-python-headless==4.10.0.84\r\nopt-einsum==3.3.0\r\noptree==0.12.1\r\npackaging==20.9\r\npandas==2.2.2\r\npillow==10.4.0\r\nplatformdirs==4.2.2\r\npooch==1.8.2\r\npromise==2.3\r\nproto-plus==1.24.0\r\nprotobuf==4.25.3\r\npsutil==6.0.0\r\npy-cpuinfo==9.0.0\r\npyasn1==0.6.0\r\npyasn1_modules==0.4.0\r\npybind11==2.13.1\r\npycparser==2.22\r\nPygments==2.18.0\r\npyparsing==3.1.2\r\npython-dateutil==2.9.0.post0\r\npython-slugify==8.0.4\r\npytz==2024.1\r\nPyYAML==6.0.1\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nresampy==0.4.3\r\nrich==13.7.1\r\nrsa==4.9\r\nscann==1.2.10\r\nscikit-learn==1.5.1\r\nscipy==1.13.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsounddevice==0.4.7\r\nsoundfile==0.12.1\r\ntensorboard==2.15.2\r\ntensorboard-data-server==0.7.2\r\ntensorboard-plugin-wit==1.8.1\r\ntensorflow==2.15.0\r\ntensorflow-addons==0.23.0\r\ntensorflow-datasets==4.9.0\r\ntensorflow-estimator==2.15.0\r\ntensorflow-hub==0.12.0\r\ntensorflow-io-gcs-filesystem==0.37.1\r\ntensorflow-metadata==1.13.0\r\ntensorflow-model-optimization==0.8.0\r\ntensorflowjs==3.18.0\r\ntermcolor==2.4.0\r\ntext-unidecode==1.3\r\ntf-models-official==2.3.0\r\ntf-slim==1.1.0\r\ntflite-model-maker==0.4.3\r\ntflite-support==0.4.4\r\nthreadpoolctl==3.5.0\r\ntoml==0.10.2\r\ntqdm==4.66.4\r\ntypeguard==2.13.3\r\ntyping_extensions==4.5.0\r\ntzdata==2024.1\r\nuritemplate==4.1.1\r\nurllib3==1.25.11\r\nwebencodings==0.5.1\r\nWerkzeug==3.0.3\r\nwrapt==1.14.1\r\nzipp==3.19.2\r\n```', 'created_at': datetime.datetime(2024, 7, 12, 15, 2, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2225784879, 'issue_id': 2403211835, 'author': 'AmarOk1412', 'body': 'So the question is, is there a tuple python/tensorflow/tflite-model-maker/numpy/keras I can try cause compatibility seems a clusterfuck', 'created_at': datetime.datetime(2024, 7, 12, 15, 3, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227412396, 'issue_id': 2403211835, 'author': 'AmarOk1412', 'body': 'I also tried https://www.tensorflow.org/lite/models/modify/model_maker/question_answer that give the same result (because my model is Squad v2.0, so I tried Squad v1).', 'created_at': datetime.datetime(2024, 7, 14, 16, 54, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2238550598, 'issue_id': 2403211835, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle ,\r\n\r\nI have also been running into tflite model maker installation issues, where it just goes in a loop of downloading bunch of files. \r\n\r\n`!pip install -q tflite-model-maker`\r\n\r\n` Preparing metadata (setup.py) ... done\r\n      5.2/5.2 MB 48.1 MB/s eta 0:00:00\r\n      1.7/1.7 MB 74.9 MB/s eta 0:00:00\r\n      80.3/80.3 kB 7.8 MB/s eta 0:00:00\r\n      632.0/632.0 MB 2.1 MB/s eta 0:00:00\r\n      621.0/621.0 MB 2.2 MB/s eta 0:00:00\r\n      620.6/620.6 MB 1.6 MB/s eta 0:00:00\r\n      620.6/620.6 MB 2.2 MB/s eta 0:00:00\r\n      620.6/620.6 MB 2.2 MB/s eta 0:00:00\r\n      620.6/620.6 MB 1.9 MB/s eta 0:00:00\r\n      620.6/620.6 MB 1.6 MB/s eta 0:00:00\r\n      620.5/620.5 MB 2.2 MB/s eta 0:00:00\r\n      607.5/607.5 MB 1.7 MB/s eta 0:00:00\r\n      607.5/607.5 MB 2.3 MB/s eta 0:00:00\r\n      606.2/606.2 MB 2.0 MB/s eta 0:00:00\r\n      606.2/606.2 MB 940.0 kB/s eta 0:00:00\r\n      606.2/606.2 MB 2.5 MB/s eta 0:00:00\r\n      605.8/605.8 MB 2.5 MB/s eta 0:00:00\r\n      604.7/604.7 MB 2.0 MB/s eta 0:00:00\r\n      604.5/604.5 MB 2.1 MB/s eta 0:00:00\r\n      604.6/604.6 MB 2.0 MB/s eta 0:00:00\r\n      604.5/604.5 MB 2.1 MB/s eta 0:00:00\r\n      604.4/604.4 MB 2.3 MB/s eta 0:00:00\r\n      604.4/604.4 MB 1.2 MB/s eta 0:00:00\r\n      604.4/604.4 MB 2.0 MB/s eta 0:00:00\r\n      600.4/600.4 MB 1.3 MB/s eta 0:00:00\r\n      600.3/600.3 MB 2.2 MB/s eta 0:00:00\r\n      600.0/600.0 MB 1.4 MB/s eta 0:00:00\r\n      590.9/590.9 MB 2.0 MB/s eta 0:00:00\r\n      590.9/590.9 MB 1.9 MB/s eta 0:00:00\r\n      591.0/591.0 MB 1.3 MB/s eta 0:00:00\r\n      591.0/591.0 MB 1.6 MB/s eta 0:00:00\r\n      591.0/591.0 MB 1.8 MB/s eta 0:00:00\r\n      591.0/591.0 MB 1.6 MB/s eta 0:00:00\r\n      590.9/590.9 MB 1.2 MB/s eta 0:00:00\r\n      590.9/590.9 MB 2.5 MB/s eta 0:00:00\r\n      590.7/590.7 MB 2.4 MB/s eta 0:00:00\r\n      590.7/590.7 MB 1.8 MB/s eta 0:00:00\r\n      175.1/590.7 MB 51.7 MB/s eta 0:00:09\r\nERROR: Operation cancelled by user`\r\nI am not sure if this issue can be resolved through mediapipe model maker , can you please take a look ?', 'created_at': datetime.datetime(2024, 7, 19, 7, 30, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2239739356, 'issue_id': 2403211835, 'author': 'pkgoogle', 'body': 'Hi @AmarOk1412, please either use mediapipe or please use a modern LLM for your use case, BertQA is quite outdated so the tools and infrastructure to help it generally are less stable, may I recommend https://huggingface.co/google/gemma-2b-it for your use case?', 'created_at': datetime.datetime(2024, 7, 19, 17, 38, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241537692, 'issue_id': 2403211835, 'author': 'AmarOk1412', 'body': ""I'll, converting the models may take a while as i'm in vacations. But will try. For now, I tried llama2 but it wasn't good, will try gemma"", 'created_at': datetime.datetime(2024, 7, 21, 9, 8, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243542367, 'issue_id': 2403211835, 'author': 'pkgoogle', 'body': ""Hi @AmarOk1412, awesome, let us know if somehow you feel Gemma isn't up to par as well so that we may be able to debug whether it is a use issue or not."", 'created_at': datetime.datetime(2024, 7, 22, 18, 15, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247136998, 'issue_id': 2403211835, 'author': 'SsomsakTH', 'body': ""Hi @AmarOk1412  It appears that there might be some issues with the installation process. Let's explore potential solutions:\r\n\r\n**1.Dependency Resolution:**\r\n    - The error you encountered might be related to dependency conflicts. To address this, consider the following steps:\r\n        - Loosen Version Constraints: Modify your package version constraints to allow more flexibility. Sometimes, specifying a narrower range can lead to conflicts.\r\n        - Remove Specific Versions: Remove specific package versions to let pip attempt to resolve the dependency conflict algorithmically.\r\n\r\nhttps://stackoverflow.com/questions/77537307/error-cannot-install-tflite-model-maker-the-conflict-is-caused-by-other-module\r\n\r\n**2.Fallback Runtime Version in Colab**:\r\n    - If you're using Colab, try using the fallback runtime version option. Choose Python 3.9 and install tflite-model-maker. You might encounter a runtime error, but it can be safely ignored.\r\n\r\nhttps://discuss.tensorflow.org/t/tflite-model-maker-installation/16577/26\r\n\r\n**3.Nightly Version:**\r\n    - While you mentioned concerns about stability, consider using the nightly version of tflite-model-maker. It might provide a workaround until the issue is resolved.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/71666"", 'created_at': datetime.datetime(2024, 7, 24, 7, 49, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345815, 'issue_id': 2403211835, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005205, 'issue_id': 2403211835, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005310, 'issue_id': 2403211835, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71666"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71666"">No</a>', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 15, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-12 03:55:08 UTC): @AmarOk1412 Creating a BERT QA model with a TensorFlow Lite Model Maker can sometimes be tricky due to compatibility issues. Could you please upgrade to the latest TF version and let us know the outcome?
Thank you!

AmarOk1412 (Issue Creator) on (2024-07-12 15:02:27 UTC): I can't because I can't install it with pip (tried python 3.8, 3.9, 3.10, 3.11 and 3.12)

```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
scann 1.2.6 requires tensorflow~=2.8.0, but you have tensorflow 2.11.0 which is incompatible.


# Then running:
  warnings.warn(
Traceback (most recent call last):
  File ""train.py"", line 1, in <module>
    from tflite_model_maker import model_spec
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tflite_model_maker/__init__.py"", line 51, in <module>
    from tflite_model_maker import searcher
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tflite_model_maker/searcher/__init__.py"", line 25, in <module>
    from tensorflow_examples.lite.model_maker.core.task.searcher import ExportFormat
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/searcher.py"", line 30, in <module>
    from tensorflow_examples.lite.model_maker.core.utils import ondevice_scann_builder
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/utils/ondevice_scann_builder.py"", line 17, in <module>
    from scann.proto import scann_pb2
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/__init__.py"", line 2, in <module>
    from scann.scann_ops.py import scann_ops
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/scann_ops/py/scann_ops.py"", line 23, in <module>
    _scann_ops_so = tf.load_op_library(
  File ""/home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/amarok/Projects/PlanEat/models/ingredients/.venv/lib64/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so: undefined symbol: _ZN4absl12lts_2021032416numbers_internal9kHexTableE
```

But here is a version with 2.13 (Scann 1.2.6 doesn't like Python 3.10, so I used 3.8 and 2.13 is the max)

```
PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python  python train.py
2024-07-12 10:29:57.563757: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-12 10:29:57.565045: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 10:29:57.590564: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 10:29:57.590860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 10:29:58.233409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
Epoch 1/2
2024-07-12 10:38:04.953281: W tensorflow/core/framework/op_kernel.cc:1805] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported
2024-07-12 10:38:05.100766: W tensorflow/core/framework/op_kernel.cc:1805] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported
Traceback (most recent call last):
  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 19, in <module>
    model = question_answer.create(train_data, model_spec=spec, epochs=2, steps_per_epoch=50)
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 228, in create
    model.train(train_data, epochs, batch_size, steps_per_epoch)
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 79, in train
    self.model = self.model_spec.train(
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/text_spec.py"", line 944, in train
    bert_model.fit(x=train_ds, epochs=epochs, **kwargs)
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:

Detected at node 'Adam/Cast' defined at (most recent call last):
    File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 19, in <module>
      model = question_answer.create(train_data, model_spec=spec, epochs=2, steps_per_epoch=50)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 228, in create
      model.train(train_data, epochs, batch_size, steps_per_epoch)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/question_answer.py"", line 79, in train
      self.model = self.model_spec.train(
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/text_spec.py"", line 944, in train
      bert_model.fit(x=train_ds, epochs=epochs, **kwargs)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 544, in minimize
      self.apply_gradients(grads_and_vars)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/official/nlp/optimization.py"", line 175, in apply_gradients
      return super(AdamWeightDecay, self).apply_gradients(
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1230, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 650, in apply_gradients
      self._apply_weight_decay(trainable_variables)
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1249, in _apply_weight_decay
      tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1245, in distributed_apply_weight_decay
      distribution.extended.update(
    File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/keras/src/optimizers/optimizer.py"", line 1241, in weight_decay_fn
      wd = tf.cast(self.weight_decay, variable.dtype)
Node: 'Adam/Cast'
Cast string to float is not supported
	 [[{{node Adam/Cast}}]] [Op:__inference_train_function_181018]
(.venv3.9)   amarok@L-SBLIN  ~/Projects/PlanEat/models/ingredients   main   pip freeze             
absl-py==1.4.0
array_record==0.5.1
astunparse==1.6.3
attrs==23.2.0
audioread==3.0.1
bleach==6.1.0
cachetools==5.3.3
certifi==2024.7.4
cffi==1.16.0
charset-normalizer==3.3.2
click==8.1.7
cycler==0.12.1
Cython==3.0.10
dataclasses==0.6
decorator==5.1.1
dm-tree==0.1.8
etils==1.5.2
fire==0.6.0
flatbuffers==24.3.25
fsspec==2024.6.1
gast==0.4.0
gin-config==0.5.0
google-api-core==2.19.1
google-api-python-client==2.137.0
google-auth==2.32.0
google-auth-httplib2==0.2.0
google-auth-oauthlib==1.0.0
google-cloud-bigquery==3.25.0
google-cloud-core==2.4.1
google-crc32c==1.5.0
google-pasta==0.2.0
google-resumable-media==2.7.1
googleapis-common-protos==1.63.1
grpcio==1.64.1
grpcio-status==1.48.2
h5py==3.11.0
httplib2==0.22.0
idna==3.7
importlib_metadata==8.0.0
importlib_resources==6.4.0
joblib==1.4.2
kaggle==1.6.14
keras==2.13.1
Keras-Preprocessing==1.1.2
kiwisolver==1.4.5
libclang==18.1.1
librosa==0.8.1
llvmlite==0.43.0
lxml==5.2.2
Markdown==3.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.4.3
mdurl==0.1.2
ml-dtypes==0.4.0
namex==0.0.8
neural-structured-learning==1.4.0
numba==0.60.0
numpy==1.23.0
oauthlib==3.2.2
opencv-python-headless==4.10.0.84
opt-einsum==3.3.0
optree==0.12.1
packaging==20.9
pandas==2.2.2
pillow==10.4.0
platformdirs==4.2.2
pooch==1.8.2
promise==2.3
proto-plus==1.24.0
protobuf==4.25.3
psutil==6.0.0
py-cpuinfo==9.0.0
pyasn1==0.6.0
pyasn1_modules==0.4.0
pybind11==2.13.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.1.2
python-dateutil==2.9.0.post0
python-slugify==8.0.4
pytz==2024.1
PyYAML==6.0.1
requests==2.32.3
requests-oauthlib==2.0.0
resampy==0.4.3
rich==13.7.1
rsa==4.9
scann==1.2.10
scikit-learn==1.5.1
scipy==1.13.1
sentencepiece==0.2.0
six==1.16.0
sounddevice==0.4.7
soundfile==0.12.1
tensorboard==2.13.0
tensorboard-data-server==0.7.2
tensorboard-plugin-wit==1.8.1
tensorflow==2.13.1
tensorflow-addons==0.23.0
tensorflow-datasets==4.9.0
tensorflow-estimator==2.13.0
tensorflow-hub==0.12.0
tensorflow-io-gcs-filesystem==0.37.1
tensorflow-metadata==1.13.0
tensorflow-model-optimization==0.8.0
tensorflowjs==3.18.0
termcolor==2.4.0
text-unidecode==1.3
tf-models-official==2.3.0
tf-slim==1.1.0
tflite-model-maker==0.4.3
tflite-support==0.4.4
threadpoolctl==3.5.0
toml==0.10.2
tqdm==4.66.4
typeguard==2.13.3
typing_extensions==4.5.0
tzdata==2024.1
uritemplate==4.1.1
urllib3==1.25.11
webencodings==0.5.1
Werkzeug==3.0.3
wrapt==1.16.0
zipp==3.19.2
```

Then with tensorflow 2.17:

```
 python train.py                           
2024-07-12 10:59:02.240329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-12 10:59:02.240648: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 10:59:02.243211: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 10:59:02.249174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 10:59:02.258851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 10:59:02.261519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 10:59:02.269337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 10:59:02.878013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 1, in <module>
    from tflite_model_maker import model_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/__init__.py"", line 44, in <module>
    from tflite_model_maker import audio_classifier
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/audio_classifier/__init__.py"", line 24, in <module>
    from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataLoader
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/audio_dataloader.py"", line 27, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/__init__.py"", line 20, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/audio_spec.py"", line 30, in <module>
    from tensorflow_examples.lite.model_maker.core.task import model_util
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_util.py"", line 28, in <module>
    from tensorflowjs.converters import converter as tfjs_converter
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/__init__.py"", line 21, in <module>
    from tensorflowjs import converters
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>
    from tensorflowjs.converters.converter import convert
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>
    from tensorflowjs.converters import tf_saved_model_conversion_v2
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 42, in <module>
    import tensorflow_hub as hub
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/__init__.py"", line 88, in <module>
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/estimator.py"", line 62, in <module>
    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow/python/util/module_wrapper.py"", line 232, in _getattr
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'tensorflow.compat.v1' has no attribute 'estimator'
```

And 2.15.0

```
2024-07-12 11:01:56.289875: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-12 11:01:56.291246: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 11:01:56.313500: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 11:01:56.313534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 11:01:56.314475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 11:01:56.318960: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-12 11:01:56.319116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 11:01:56.967406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/home/amarok/Projects/PlanEat/models/ingredients/train.py"", line 1, in <module>
    from tflite_model_maker import model_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/__init__.py"", line 44, in <module>
    from tflite_model_maker import audio_classifier
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tflite_model_maker/audio_classifier/__init__.py"", line 24, in <module>
    from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataLoader
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/audio_dataloader.py"", line 27, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/__init__.py"", line 20, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/audio_spec.py"", line 30, in <module>
    from tensorflow_examples.lite.model_maker.core.task import model_util
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_util.py"", line 28, in <module>
    from tensorflowjs.converters import converter as tfjs_converter
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/__init__.py"", line 21, in <module>
    from tensorflowjs import converters
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>
    from tensorflowjs.converters.converter import convert
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>
    from tensorflowjs.converters import tf_saved_model_conversion_v2
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 42, in <module>
    import tensorflow_hub as hub
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/__init__.py"", line 93, in <module>
    from tensorflow_hub.feature_column_v2 import text_embedding_column_v2
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/feature_column_v2.py"", line 24, in <module>
    from tensorflow_hub import keras_layer
  File ""/home/amarok/Projects/PlanEat/.venv3.9/lib64/python3.9/site-packages/tensorflow_hub/keras_layer.py"", line 27, in <module>
    from tensorflow.python.training.tracking import data_structures
ModuleNotFoundError: No module named 'tensorflow.python.training.tracking'
(.venv3.9)   amarok@L-SBLIN  ~/Projects/PlanEat/models/ingredients   main   pip freeze                  
absl-py==1.4.0
array_record==0.5.1
astunparse==1.6.3
attrs==23.2.0
audioread==3.0.1
bleach==6.1.0
cachetools==5.3.3
certifi==2024.7.4
cffi==1.16.0
charset-normalizer==3.3.2
click==8.1.7
cycler==0.12.1
Cython==3.0.10
dataclasses==0.6
decorator==5.1.1
dm-tree==0.1.8
etils==1.5.2
fire==0.6.0
flatbuffers==24.3.25
fsspec==2024.6.1
gast==0.4.0
gin-config==0.5.0
google-api-core==2.19.1
google-api-python-client==2.137.0
google-auth==2.32.0
google-auth-httplib2==0.2.0
google-auth-oauthlib==1.0.0
google-cloud-bigquery==3.25.0
google-cloud-core==2.4.1
google-crc32c==1.5.0
google-pasta==0.2.0
google-resumable-media==2.7.1
googleapis-common-protos==1.63.1
grpcio==1.64.1
grpcio-status==1.48.2
h5py==3.11.0
httplib2==0.22.0
idna==3.7
importlib_metadata==8.0.0
importlib_resources==6.4.0
joblib==1.4.2
kaggle==1.6.14
keras==3.4.1
Keras-Preprocessing==1.1.2
kiwisolver==1.4.5
libclang==18.1.1
librosa==0.8.1
llvmlite==0.43.0
lxml==5.2.2
Markdown==3.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.4.3
mdurl==0.1.2
ml-dtypes==0.2.0
namex==0.0.8
neural-structured-learning==1.4.0
numba==0.60.0
numpy==1.23.4
oauthlib==3.2.2
opencv-python-headless==4.10.0.84
opt-einsum==3.3.0
optree==0.12.1
packaging==20.9
pandas==2.2.2
pillow==10.4.0
platformdirs==4.2.2
pooch==1.8.2
promise==2.3
proto-plus==1.24.0
protobuf==4.25.3
psutil==6.0.0
py-cpuinfo==9.0.0
pyasn1==0.6.0
pyasn1_modules==0.4.0
pybind11==2.13.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.1.2
python-dateutil==2.9.0.post0
python-slugify==8.0.4
pytz==2024.1
PyYAML==6.0.1
requests==2.32.3
requests-oauthlib==2.0.0
resampy==0.4.3
rich==13.7.1
rsa==4.9
scann==1.2.10
scikit-learn==1.5.1
scipy==1.13.1
sentencepiece==0.2.0
six==1.16.0
sounddevice==0.4.7
soundfile==0.12.1
tensorboard==2.15.2
tensorboard-data-server==0.7.2
tensorboard-plugin-wit==1.8.1
tensorflow==2.15.0
tensorflow-addons==0.23.0
tensorflow-datasets==4.9.0
tensorflow-estimator==2.15.0
tensorflow-hub==0.12.0
tensorflow-io-gcs-filesystem==0.37.1
tensorflow-metadata==1.13.0
tensorflow-model-optimization==0.8.0
tensorflowjs==3.18.0
termcolor==2.4.0
text-unidecode==1.3
tf-models-official==2.3.0
tf-slim==1.1.0
tflite-model-maker==0.4.3
tflite-support==0.4.4
threadpoolctl==3.5.0
toml==0.10.2
tqdm==4.66.4
typeguard==2.13.3
typing_extensions==4.5.0
tzdata==2024.1
uritemplate==4.1.1
urllib3==1.25.11
webencodings==0.5.1
Werkzeug==3.0.3
wrapt==1.14.1
zipp==3.19.2
```

AmarOk1412 (Issue Creator) on (2024-07-12 15:03:39 UTC): So the question is, is there a tuple python/tensorflow/tflite-model-maker/numpy/keras I can try cause compatibility seems a clusterfuck

AmarOk1412 (Issue Creator) on (2024-07-14 16:54:31 UTC): I also tried https://www.tensorflow.org/lite/models/modify/model_maker/question_answer that give the same result (because my model is Squad v2.0, so I tried Squad v1).

sawantkumar (Assginee) on (2024-07-19 07:30:33 UTC): Hi @pkgoogle ,

I have also been running into tflite model maker installation issues, where it just goes in a loop of downloading bunch of files. 

`!pip install -q tflite-model-maker`

` Preparing metadata (setup.py) ... done
      5.2/5.2 MB 48.1 MB/s eta 0:00:00
      1.7/1.7 MB 74.9 MB/s eta 0:00:00
      80.3/80.3 kB 7.8 MB/s eta 0:00:00
      632.0/632.0 MB 2.1 MB/s eta 0:00:00
      621.0/621.0 MB 2.2 MB/s eta 0:00:00
      620.6/620.6 MB 1.6 MB/s eta 0:00:00
      620.6/620.6 MB 2.2 MB/s eta 0:00:00
      620.6/620.6 MB 2.2 MB/s eta 0:00:00
      620.6/620.6 MB 1.9 MB/s eta 0:00:00
      620.6/620.6 MB 1.6 MB/s eta 0:00:00
      620.5/620.5 MB 2.2 MB/s eta 0:00:00
      607.5/607.5 MB 1.7 MB/s eta 0:00:00
      607.5/607.5 MB 2.3 MB/s eta 0:00:00
      606.2/606.2 MB 2.0 MB/s eta 0:00:00
      606.2/606.2 MB 940.0 kB/s eta 0:00:00
      606.2/606.2 MB 2.5 MB/s eta 0:00:00
      605.8/605.8 MB 2.5 MB/s eta 0:00:00
      604.7/604.7 MB 2.0 MB/s eta 0:00:00
      604.5/604.5 MB 2.1 MB/s eta 0:00:00
      604.6/604.6 MB 2.0 MB/s eta 0:00:00
      604.5/604.5 MB 2.1 MB/s eta 0:00:00
      604.4/604.4 MB 2.3 MB/s eta 0:00:00
      604.4/604.4 MB 1.2 MB/s eta 0:00:00
      604.4/604.4 MB 2.0 MB/s eta 0:00:00
      600.4/600.4 MB 1.3 MB/s eta 0:00:00
      600.3/600.3 MB 2.2 MB/s eta 0:00:00
      600.0/600.0 MB 1.4 MB/s eta 0:00:00
      590.9/590.9 MB 2.0 MB/s eta 0:00:00
      590.9/590.9 MB 1.9 MB/s eta 0:00:00
      591.0/591.0 MB 1.3 MB/s eta 0:00:00
      591.0/591.0 MB 1.6 MB/s eta 0:00:00
      591.0/591.0 MB 1.8 MB/s eta 0:00:00
      591.0/591.0 MB 1.6 MB/s eta 0:00:00
      590.9/590.9 MB 1.2 MB/s eta 0:00:00
      590.9/590.9 MB 2.5 MB/s eta 0:00:00
      590.7/590.7 MB 2.4 MB/s eta 0:00:00
      590.7/590.7 MB 1.8 MB/s eta 0:00:00
      175.1/590.7 MB 51.7 MB/s eta 0:00:09
ERROR: Operation cancelled by user`
I am not sure if this issue can be resolved through mediapipe model maker , can you please take a look ?

pkgoogle (Assginee) on (2024-07-19 17:38:39 UTC): Hi @AmarOk1412, please either use mediapipe or please use a modern LLM for your use case, BertQA is quite outdated so the tools and infrastructure to help it generally are less stable, may I recommend https://huggingface.co/google/gemma-2b-it for your use case?

AmarOk1412 (Issue Creator) on (2024-07-21 09:08:50 UTC): I'll, converting the models may take a while as i'm in vacations. But will try. For now, I tried llama2 but it wasn't good, will try gemma

pkgoogle (Assginee) on (2024-07-22 18:15:01 UTC): Hi @AmarOk1412, awesome, let us know if somehow you feel Gemma isn't up to par as well so that we may be able to debug whether it is a use issue or not.

SsomsakTH on (2024-07-24 07:49:10 UTC): Hi @AmarOk1412  It appears that there might be some issues with the installation process. Let's explore potential solutions:

**1.Dependency Resolution:**
    - The error you encountered might be related to dependency conflicts. To address this, consider the following steps:
        - Loosen Version Constraints: Modify your package version constraints to allow more flexibility. Sometimes, specifying a narrower range can lead to conflicts.
        - Remove Specific Versions: Remove specific package versions to let pip attempt to resolve the dependency conflict algorithmically.

https://stackoverflow.com/questions/77537307/error-cannot-install-tflite-model-maker-the-conflict-is-caused-by-other-module

**2.Fallback Runtime Version in Colab**:
    - If you're using Colab, try using the fallback runtime version option. Choose Python 3.9 and install tflite-model-maker. You might encounter a runtime error, but it can be safely ignored.

https://discuss.tensorflow.org/t/tflite-model-maker-installation/16577/26

**3.Nightly Version:**
    - While you mentioned concerns about stability, consider using the nightly version of tflite-model-maker. It might provide a workaround until the issue is resolved.

https://github.com/tensorflow/tensorflow/issues/71666

github-actions[bot] on (2024-08-02 01:53:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-09 01:55:11 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-09 01:55:15 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71666"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71666"">No</a>

"
2402844915,issue,closed,completed,epoch is getting stuck,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Mac OS

### Mobile device

_No response_

### Python version

3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

epoch is getting stuck when try to train lstm code

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
from tensorflow.keras import backend as K
import numpy as np
import os

# Clear session
tf.keras.backend.clear_session()
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

# Optionally, force use of CPU for debugging

# Define the LSTM model
model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(20, 20), unroll=True, use_bias=False, recurrent_activation='sigmoid'),
    Dropout(0.2),
    LSTM(100, unroll=True, use_bias=False, recurrent_activation='sigmoid'),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train_seq,y_train_seq, epochs=100, batch_size=64, verbose=1)
# Make predictions
train_predict = model.predict(X_train_seq)
test_predict = model.predict(X_test_seq)
# Invert predictions to original scale
```


### Relevant log output

_No response_",nvn234,2024-07-11 10:34:19+00:00,['tilakrayal'],2024-08-02 05:03:12+00:00,2024-07-27 01:51:35+00:00,https://github.com/tensorflow/tensorflow/issues/71654,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.16', '')]","[{'comment_id': 2225595754, 'issue_id': 2402844915, 'author': 'tilakrayal', 'body': '@nvn234,\r\nI was facing a different error while executing the above mentioned code. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/27d712e135b7ffcc561b1a8a09718d9e/untitled2000.ipynb) and provide the complete dependencies which helps us to debug the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 12, 13, 29, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240829714, 'issue_id': 2402844915, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 20, 1, 50, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703610, 'issue_id': 2402844915, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703633, 'issue_id': 2402844915, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71654"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71654"">No</a>', 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 37, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-12 13:29:37 UTC): @nvn234,
I was facing a different error while executing the above mentioned code. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/27d712e135b7ffcc561b1a8a09718d9e/untitled2000.ipynb) and provide the complete dependencies which helps us to debug the issue in an effective way. Thank you!

github-actions[bot] on (2024-07-20 01:50:39 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-27 01:51:34 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-27 01:51:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71654"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71654"">No</a>

"
2402307648,issue,closed,completed,Binary cross entropy produces negative results if input is more than 1,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.6.12

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

Linux Ubuntu 22.04

### Python version

3.11.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Output: `tf.Tensor(-1.7812711e-06, shape=(), dtype=float32)`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

a = tf.Variable([1.0000000597], dtype=tf.float32)
b = tf.constant([1.0], dtype=tf.float32)

loss = tf.keras.losses.binary_crossentropy(a, b)
print(loss)
```


### Relevant log output

_No response_",hguandl,2024-07-11 05:50:42+00:00,['tilakrayal'],2024-07-27 01:51:40+00:00,2024-07-27 01:51:36+00:00,https://github.com/tensorflow/tensorflow/issues/71638,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('2.6.0', '')]","[{'comment_id': 2225604190, 'issue_id': 2402307648, 'author': 'tilakrayal', 'body': ""@hguandl,\r\nThe loss is just a scalar that you are trying to minimize. It's not supposed to be positive! For instance a cosine proximity loss will usually be negative (trying to make proximity as high as possible by minimizing a negative scalar).\r\n\r\nhttps://stackoverflow.com/questions/42264649/keras-binary-crossentropy-has-negative-values/47503934#47503934\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 12, 13, 34, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240829730, 'issue_id': 2402307648, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 20, 1, 50, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703625, 'issue_id': 2402307648, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703675, 'issue_id': 2402307648, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71638"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71638"">No</a>', 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 39, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-12 13:34:14 UTC): @hguandl,
The loss is just a scalar that you are trying to minimize. It's not supposed to be positive! For instance a cosine proximity loss will usually be negative (trying to make proximity as high as possible by minimizing a negative scalar).

https://stackoverflow.com/questions/42264649/keras-binary-crossentropy-has-negative-values/47503934#47503934

Thank you!

github-actions[bot] on (2024-07-20 01:50:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-27 01:51:36 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-27 01:51:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71638"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71638"">No</a>

"
2402255405,issue,closed,completed,TFlite on ARM64,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

Linux, ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,
I tried to build TFlite for ARM64 using the instructions provided in https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_aarch64_arm64 . I was able to successfully configure, but while building using `make -j4 ` build errors are produced. Please provide instructions on how to build.

### Standalone code to reproduce the issue

```shell
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains

ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-
ARMCC_FLAGS=""-funsafe-math-optimizations""
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=aarch64 \
  ../tensorflow/lite/

Alternatively, I have tried adding a few more flags as the following:

cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc  
      -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++   
      -DCMAKE_C_FLAGS=""${ARMCC_FLAGS} -march=armv8.2-a""  
      -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS} -march=armv8.2-a""   
      -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   
      -DCMAKE_SYSTEM_NAME=Linux   
      -DCMAKE_SYSTEM_PROCESSOR=aarch64   
      -DTFLITE_HOST_TOOLS_DIR=/arm_build_backup_1/flatbuffers   
      -DXNN_ENABLE_ARM_BF16=0   
      -DXNNPACK_ENABLE_ARM_BF16=0   
      /arm_build_backup_1/tensorflow_src/tensorflow/lite/
```


### Relevant log output

```shell
Line  886: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:39:9: error: unknown type name bfloat16_t
	Line  889: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:39:33: error: unknown type name bfloat16_t
	Line  892: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:40:3: error: unknown type name bfloat16_t; did you mean float16_t?
	Line  898: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:40:21: error: bfloat16_t undeclared (first use in this function); did you mean float16_t?
	Line  903: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:40:32: error: expected expression before ) token
	Line  906: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:42:19: error: expected =, ,, ;, asm or __attribute__ before * token
	Line  909: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:42:21: error: w undeclared (first use in this function)
	Line  912: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:42:31: error: expected ) before bfloat16_t
	Line  935: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:51:13: error: unknown type name bfloat16x8_t
	Line  942: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:53:13: error: unknown type name bfloat16x8_t
	Line  945: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:54:13: error: unknown type name bfloat16x8_t
	Line  948: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:55:13: error: unknown type name bfloat16x8_t
	Line  951: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:56:13: error: unknown type name bfloat16x8_t
	Line  958: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:58:15: error: incompatible types when assigning to type float32x4_t from type int
	Line  961: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:59:15: error: incompatible types when assigning to type float32x4_t from type int
	Line  964: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:60:15: error: incompatible types when assigning to type float32x4_t from type int
	Line  967: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:61:15: error: incompatible types when assigning to type float32x4_t from type int
	Line  971: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:64:13: error: unknown type name bfloat16x8_t
	Line  974: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:64:59: error: expected ) before bfloat16_t
	Line  984: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:66:13: error: unknown type name bfloat16x8_t
	Line  987: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:67:13: error: unknown type name bfloat16x8_t
	Line  990: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:68:13: error: unknown type name bfloat16x8_t
	Line  993: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:69:13: error: unknown type name bfloat16x8_t
	Line 1001: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:71:40: error: incompatible type for argument 1 of vceqq_u16
	Line 1008: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:72:40: error: incompatible type for argument 1 of vceqq_u16
	Line 1015: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:73:40: error: incompatible type for argument 1 of vceqq_u16
	Line 1022: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:74:40: error: incompatible type for argument 1 of vceqq_u16
	Line 1029: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:76:13: error: unknown type name bfloat16x8_t
	Line 1036: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:76:67: error: incompatible type for argument 1 of vbicq_u16
	Line 1043: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:77:15: error: incompatible types when assigning to type float32x4_t from type int
	Line 1046: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:78:13: error: unknown type name bfloat16x8_t
	Line 1049: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:78:67: error: incompatible type for argument 1 of vbicq_u16
	Line 1056: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:79:15: error: incompatible types when assigning to type float32x4_t from type int
	Line 1059: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:80:13: error: unknown type name bfloat16x8_t
	Line 1062: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:80:67: error: incompatible type for argument 1 of vbicq_u16
	Line 1069: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:81:15: error: incompatible types when assigning to type float32x4_t from type int
	Line 1072: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:82:13: error: unknown type name bfloat16x8_t
	Line 1075: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:82:67: error: incompatible type for argument 1 of vbicq_u16
	Line 1082: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:83:15: error: incompatible types when assigning to type float32x4_t from type int
	Line 1085: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:106:5: error: unknown type name bfloat16x4_t; did you mean float16x4_t?
	Line 1097: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:110:24: error: expected expression before ) token
	Line 1100: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:112:18: error: expected ) before bfloat16_t
	Line 1114: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:117:35: error: incompatible type for argument 2 of vst1_lane_u32
	Line 1125: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:119:53: error: incompatible type for argument 1 of vext_u16
	Line 1132: /arm_build/build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c:119:88: error: incompatible type for argument 2 of vext_u16
	Line 1143: make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:46951: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o] Error 1
	Line 1145: make[1]: *** [CMakeFiles/Makefile2:6761: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2
	Line 1169: make: *** [Makefile:139: all] Error 2
```
",devapriyas2001,2024-07-11 05:03:28+00:00,"['pkgoogle', 'sawantkumar']",2024-08-28 12:44:13+00:00,2024-07-24 08:09:15+00:00,https://github.com/tensorflow/tensorflow/issues/71636,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2224835450, 'issue_id': 2402255405, 'author': 'sawantkumar', 'body': ""Hi @pkgoogle ,\r\n\r\nI replicated the issues with tf 2.16.2 and ubuntu 20, can you please take a look ?\r\n\r\n`[ 73%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o\r\ncd /home/sawantkumar/work/tensorflow/minimal_build/_deps/xnnpack-build && /root/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=1 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_AVXVNNI=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -DXNN_ENABLE_VSX=1 -I/home/sawantkumar/work/tensorflow/third_party/xla/third_party/tsl -I/home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src -I/home/sawantkumar/work/tensorflow/minimal_build/pthreadpool-source/include -I/home/sawantkumar/work/tensorflow/minimal_build/FXdiv-source/include -I/home/sawantkumar/work/tensorflow/minimal_build/FP16-source/include  -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -O2 -pthread -std=c99  -fno-math-errno  -march=armv8.2-a+bf16  -o CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o   -c /home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c\r\ncc1: error: invalid feature modifier in -march=armv8.2-a+bf16\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:43525: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[2]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build'\r\nmake[1]: *** [CMakeFiles/Makefile2:7148: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2\r\nmake[1]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build'\r\nmake: *** [Makefile:133: all] Error 2`"", 'created_at': datetime.datetime(2024, 7, 12, 6, 13, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2226196378, 'issue_id': 2402255405, 'author': 'pkgoogle', 'body': 'I\'m essentially getting the same error as @sawantkumar.\r\n\r\nHere are my reproduce steps:\r\n\r\n```sh\r\ncurl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz\r\nmkdir -p ${HOME}/toolchains\r\ntar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains\r\n```\r\nNote, I had to point to where flatc is installed with `-DTFLITE_HOST_TOOLS_DIR=/usr/local/bin`, I also tried this with nightly and got similar results\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout tags/v2.17.0\r\nmkdir aarch64_build\r\ncd aarch64_build\r\nARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-\r\nARMCC_FLAGS=""-funsafe-math-optimizations""\r\ncmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \\\r\n  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \\\r\n  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n  -DCMAKE_SYSTEM_NAME=Linux \\\r\n  -DCMAKE_SYSTEM_PROCESSOR=aarch64 \\\r\n  -DTFLITE_HOST_TOOLS_DIR=/usr/local/bin \\\r\n  ../tensorflow/lite/\r\ncmake --build . -j\r\n```\r\n\r\nabbreviated output:\r\n```\r\ncc1: error: invalid feature modifier in -march=armv8.2-a+bf16\r\ncc1: error: invalid feature modifier in -march=armv8.2-a+bf16\r\ngmake[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:46951: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-6x8c2-minmax-neonbf16-bfdot-lane-ld128.c.o] Error 1\r\ncc1: error: invalid feature modifier in -march=armv8.2-a+bf16\r\ngmake[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:46965: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-vunary/gen/bf16-vabs-neonbf16-u8.c.o] Error 1\r\ncc1: error: invalid feature modifier in -march=armv8.2-a+bf16\r\n```\r\n\r\nHi @terryheo, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 7, 12, 19, 2, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247175472, 'issue_id': 2402255405, 'author': 'devapriyas2001', 'body': 'Hi, I got the program compiled by changing the toolchain.\r\nThanks', 'created_at': datetime.datetime(2024, 7, 24, 8, 9, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247175632, 'issue_id': 2402255405, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71636"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71636"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 8, 9, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315220291, 'issue_id': 2402255405, 'author': 'hailangWu-Aval', 'body': '> Hi, I got the program compiled by changing the toolchain. Thanks\r\n\r\nHow to change the toolchain? Can you show some tips?', 'created_at': datetime.datetime(2024, 8, 28, 12, 44, 12, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-07-12 06:13:25 UTC): Hi @pkgoogle ,

I replicated the issues with tf 2.16.2 and ubuntu 20, can you please take a look ?

`[ 73%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o
cd /home/sawantkumar/work/tensorflow/minimal_build/_deps/xnnpack-build && /root/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=1 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_AVXVNNI=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -DXNN_ENABLE_VSX=1 -I/home/sawantkumar/work/tensorflow/third_party/xla/third_party/tsl -I/home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src -I/home/sawantkumar/work/tensorflow/minimal_build/pthreadpool-source/include -I/home/sawantkumar/work/tensorflow/minimal_build/FXdiv-source/include -I/home/sawantkumar/work/tensorflow/minimal_build/FP16-source/include  -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -O2 -pthread -std=c99  -fno-math-errno  -march=armv8.2-a+bf16  -o CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o   -c /home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c
cc1: error: invalid feature modifier in -march=armv8.2-a+bf16
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:43525: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-1x4c8-minmax-neonbf16-bfdot.c.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[2]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build'
make[1]: *** [CMakeFiles/Makefile2:7148: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2
make[1]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build'
make: *** [Makefile:133: all] Error 2`

pkgoogle (Assginee) on (2024-07-12 19:02:10 UTC): I'm essentially getting the same error as @sawantkumar.

Here are my reproduce steps:

```sh
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains
```
Note, I had to point to where flatc is installed with `-DTFLITE_HOST_TOOLS_DIR=/usr/local/bin`, I also tried this with nightly and got similar results
```sh
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout tags/v2.17.0
mkdir aarch64_build
cd aarch64_build
ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-
ARMCC_FLAGS=""-funsafe-math-optimizations""
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=aarch64 \
  -DTFLITE_HOST_TOOLS_DIR=/usr/local/bin \
  ../tensorflow/lite/
cmake --build . -j
```

abbreviated output:
```
cc1: error: invalid feature modifier in -march=armv8.2-a+bf16
cc1: error: invalid feature modifier in -march=armv8.2-a+bf16
gmake[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:46951: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-gemm/gen/bf16-gemm-6x8c2-minmax-neonbf16-bfdot-lane-ld128.c.o] Error 1
cc1: error: invalid feature modifier in -march=armv8.2-a+bf16
gmake[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:46965: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/bf16-vunary/gen/bf16-vabs-neonbf16-u8.c.o] Error 1
cc1: error: invalid feature modifier in -march=armv8.2-a+bf16
```

Hi @terryheo, can you please take a look? Thanks.

devapriyas2001 (Issue Creator) on (2024-07-24 08:09:12 UTC): Hi, I got the program compiled by changing the toolchain.
Thanks

google-ml-butler[bot] on (2024-07-24 08:09:17 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71636"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71636"">No</a>

hailangWu-Aval on (2024-08-28 12:44:12 UTC): How to change the toolchain? Can you show some tips?

"
2402231803,issue,closed,completed,"""tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor"" Error","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v1.15.0-rc3-22-g590d6ee

### Custom code

Yes

### OS platform and distribution

Red Hat Enterprise Linux 8.4 (Ootpa)

### Mobile device

_No response_

### Python version

Python 3.6.8 :: Anaconda, Inc.

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'model_1/lambda_1/Variable/replica_1/Initializer/Identity:0' shape=(64, 196) dtype=float32> because it depends transitively on placeholder <tf.Operation 'model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable' type=Placeholder> via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp (ReadVariableOp) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable (Placeholder)

### Standalone code to reproduce the issue

```shell
I am implementing a code from https://github.com/aspuru-guzik-group/chemical_vae GitHub. I am successfully able to run the same. However, I am using multiple GPUs and to distribute the work among devices, I am using custom training for tf.distribute.MirroredStrategy(). I am pasting the code that I have converted for the same for no_main_prop method in chemvae/train_vae.py file in this GitHub repository. 


=================================================================
def main_no_prop(params):
    start_time = time.time()

    # Create a MirroredStrategy.
    strategy = tf.distribute.MirroredStrategy()

    X_train, X_test = vectorize_data(params)

    # Define the model within the strategy scope
    with strategy.scope():
        AE_only_model, encoder, decoder, kl_loss_var = load_models(params)

        # compile models
        if params['optim'] == 'adam':
            optim = Adam(lr=params['lr'], beta_1=params['momentum'])
        elif params['optim'] == 'rmsprop':
            optim = RMSprop(learning_rate=params['lr'], rho=params['momentum'])
        elif params['optim'] == 'sgd':
            optim = SGD(learning_rate=params['lr'], momentum=params['momentum'])
        else:
            raise NotImplemented(""Please define valid optimizer"")
    
        model_losses = {'x_pred': params['loss'],
                        'z_mean_log_var': kl_loss}
    
        # vae metrics, callbacks
        vae_sig_schedule = partial(mol_cb.sigmoid_schedule, slope=params['anneal_sigmod_slope'],
                                   start=params['vae_annealer_start'])
        vae_anneal_callback = mol_cb.WeightAnnealer_epoch(
            vae_sig_schedule, kl_loss_var, params['kl_loss_weight'], 'vae')
    
        csv_clb = CSVLogger(params[""history_file""], append=False)
        callbacks = [vae_anneal_callback, csv_clb]
    
        def vae_anneal_metric(y_true, y_pred):
            return kl_loss_var
    
        xent_loss_weight = tf.keras.backend.variable(params['xent_loss_weight'])
    
        # Distribute the dataset
        batch_size = params['batch_size']*4
        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, {
            'x_pred': X_train,
            'z_mean_log_var': np.ones((np.shape(X_train)[0], params['hidden_dim'] * 2))
        }))
        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
        distributed_train_dataset = strategy.experimental_distribute_dataset(train_dataset)
        
        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, {
            'x_pred': X_test,
            'z_mean_log_var': np.ones((np.shape(X_test)[0], params['hidden_dim'] * 2))
        }))
        test_dataset = test_dataset.batch(batch_size)
        distributed_test_dataset = strategy.experimental_distribute_dataset(test_dataset)

        AE_only_model.compile(
            loss=model_losses,
            loss_weights=[xent_loss_weight, kl_loss_var],
            optimizer=optim,
            metrics={'x_pred': ['categorical_accuracy', vae_anneal_metric]}
        )
    
        # Custom training loop
        epochs = params['epochs']
        initial_epoch = params['prev_epochs']

        @tf.function
        def training_steps():
            for epoch in range(initial_epoch, epochs):
                total_loss = 0.0
                num_batches = 0

                for inputs in distributed_train_dataset:
                    print(""=""*100)
                    print(""Input (Train): "", inputs)
                    print(""=""*100)

                    def train_step(inputs):
                        with tf.GradientTape() as tape:
                            y_pred = AE_only_model(inputs[0])
                            loss = AE_only_model.compute_loss(inputs[1], y_pred)

                        gradients = tape.gradient(loss, AE_only_model.trainable_variables)
                        optim.apply_gradients(zip(gradients, AE_only_model.trainable_variables))
                        return loss

                    @tf.function
                    def distributed_train_step(inputs):
                        per_replica_losses = strategy.experimental_run_v2(train_step,  args=(inputs,))
                        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

                    total_loss += distributed_train_step(inputs)
                    num_batches += 1

                train_loss = total_loss / num_batches
                print(f'Epoch {epoch + 1}, Loss: {train_loss:.4f}')

                for inputs in distributed_test_dataset:
                    print(""=""*100)
                    print(""Input (Test): "", inputs)
                    print(""=""*100)
                    def test_step(inputs):
                        y_pred = AE_only_model(inputs[0])
                        return AE_only_model.compute_loss(inputs[1], y_pred)

                    @tf.function
                    def distributed_test_step(inputs):
                        per_replica_losses = strategy.experimental_run(test_step,  args=(inputs,))
                        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

                    test_loss = distributed_test_step(inputs)

                print(f'Epoch {epoch + 1}, Validation Loss: {test_loss:.4f}')

        training_steps()
    
    encoder.save(params['encoder_weights_file'])
    decoder.save(params['decoder_weights_file'])
    print('Time of run: ', time.time() - start_time)
    print('**FINISHED**')
    return

=================================================================
```


### Relevant log output

```shell
Error reported to Coordinator: Unable to lift tensor <tf.Tensor 'model_1/lambda_1/Variable/replica_1/Initializer/Identity:0' shape=(64, 196) dtype=float32> because it depends transitively on placeholder <tf.Operation 'model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable' type=Placeholder> via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp (ReadVariableOp) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable (Placeholder)
Traceback (most recent call last):
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 880, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/tmp/tmpd4qk3dv8.py"", line 46, in train_step
    y_pred = ag__.converted_call(AE_only_model, train_step_scope.callopts, (inputs[0],), None, train_step_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 541, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/tmpg_1oddri.py"", line 192, in tf____call__
    output = ag__.converted_call(self.call, __call___scope.callopts, (inputs,), dict(kwargs, **{}), __call___scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmppws4cnkd.py"", line 68, in tf__call
    do_return, retval_ = ag__.if_stmt(cond_1, if_true_1, if_false_1, get_state_1, set_state_1, ('do_return', 'retval_'), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmppws4cnkd.py"", line 63, in if_false_1
    output_tensors, _, _ = ag__.converted_call(self.run_internal_graph, call_scope.callopts, (inputs, masks), None, call_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 541, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/tmpupurh3pr.py"", line 298, in tf__run_internal_graph
    ag__.for_stmt(depth_keys, None, loop_body_5, get_state_17, set_state_17, (), (), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 339, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 350, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmpupurh3pr.py"", line 296, in loop_body_5
    ag__.for_stmt(nodes, None, loop_body_4, get_state_16, set_state_16, (), (), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 339, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 350, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmpupurh3pr.py"", line 294, in loop_body_4
    ag__.if_stmt(cond_11, if_true_11, if_false_11, get_state_15, set_state_15, (), ('x._keras_shape', 'x._uses_learning_phase', ""kwargs['mask']""))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpupurh3pr.py"", line 211, in if_true_11
    computed_tensors, output_tensors, output_masks = ag__.if_stmt(cond_7, if_true_7, if_false_7, get_state_9, set_state_9, ('computed_tensors', 'output_tensors', 'output_masks'), (""kwargs['mask']"",))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpupurh3pr.py"", line 207, in if_false_7
    output_tensors = ag__.converted_call(_to_list, run_internal_graph_scope.callopts, (ag__.converted_call(layer.call, run_internal_graph_scope.callopts, (computed_tensors,), dict(kwargs, **{}), run_internal_graph_scope),), None, run_internal_graph_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp4nuqr_3n.py"", line 31, in tf__call
    retval_ = call_scope.mark_return_value(ag__.converted_call(self.function, call_scope.callopts, (inputs,), dict(arguments, **{}), call_scope))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmphkvehh5i.py"", line 13, in tf__sampling
    epsilon = ag__.converted_call(K.random_normal_variable, sampling_scope.callopts, (), {'shape': (params['batch_size'], params['hidden_dim']), 'mean': 0.0, 'scale': 1.0}, sampling_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp2dyogoo7.py"", line 68, in tf__random_normal_variable
    retval_ = random_normal_variable_scope.mark_return_value(ag__.converted_call(variable, random_normal_variable_scope.callopts, (value,), {'dtype': dtype, 'name': name}, random_normal_variable_scope))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpqaev2zhu.py"", line 103, in tf__variable
    do_return, retval_ = ag__.if_stmt(cond_3, if_true_3, if_false_3, get_state_3, set_state_3, ('do_return', 'retval_'), ('v._keras_shape', 'v._uses_learning_phase'))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpqaev2zhu.py"", line 67, in if_false_3
    v = ag__.converted_call(tf.Variable, variable_scope.callopts, (value,), {'dtype': ag__.converted_call(_convert_string_dtype, variable_scope.callopts, (dtype,), None, variable_scope), 'name': name}, variable_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 422, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 258, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 219, in _variable_v1_call
    shape=shape)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/shared_variable_creator.py"", line 69, in create_new_variable
    v = next_creator(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1345, in creator_with_resource_vars
    return self._create_variable(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 527, in _create_variable
    values.SyncOnReadVariable, *args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2311, in create_mirrored_variable
    value_list = real_mirrored_creator(devices, *args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 519, in _real_mirrored_creator
    v = next_creator(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 380, in variable_capturing_scope
    lifted_initializer_graph=lifted_initializer_graph, **kwds)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 175, in __init__
    disallowed_placeholders=placeholder_ops)[initial_value]
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/lift_to_graph.py"", line 260, in lift_to_graph
    add_sources=add_sources))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/op_selector.py"", line 413, in map_subgraph
    % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))
tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'model_1/lambda_1/Variable/replica_1/Initializer/Identity:0' shape=(64, 196) dtype=float32> because it depends transitively on placeholder <tf.Operation 'model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable' type=Placeholder> via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp (ReadVariableOp) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable (Placeholder)
Traceback (most recent call last):
  File ""chemvae/train_vae.py"", line 462, in <module>
    main_no_prop(params)
  File ""chemvae/train_vae.py"", line 339, in main_no_prop
    training_steps()
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 449, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 392, in _initialize
    *args, **kwds))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 335, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 902, in wrapper
    ), args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpd4qk3dv8.py"", line 112, in tf__training_steps
    test_loss, = ag__.for_stmt(ag__.converted_call(range, training_steps_scope.callopts, (initial_epoch, epochs), None, training_steps_scope), None, loop_body_2, get_state_2, set_state_2, (test_loss,), ('test_loss',), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 339, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 350, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmpd4qk3dv8.py"", line 69, in loop_body_2
    total_loss, num_batches = ag__.for_stmt(distributed_train_dataset, None, loop_body, get_state, set_state, (total_loss, num_batches), ('total_loss', 'num_batches'), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 337, in for_stmt
    return custom_handler(extra_test, body, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 408, in _autograph_for_loop
    return self.reduce(init_state, reduce_body)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 445, in reduce
    cond, loop_body, [has_data, data, initial_state], parallel_iterations=1)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2675, in while_loop
    back_prop=back_prop)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py"", line 198, in while_loop
    add_control_dependencies=add_control_dependencies)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py"", line 176, in wrapped_body
    outputs = body(*_pack_sequence_as(orig_loop_vars, args))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 440, in loop_body
    state = reduce_fn(state, per_replica_data)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 404, in reduce_body
    new_state = body(iterate, *state)
  File ""/tmp/tmpd4qk3dv8.py"", line 66, in loop_body
    total_loss += ag__.converted_call(distributed_train_step, training_steps_scope.callopts, (inputs,), None, training_steps_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 401, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 332, in _call_unconverted
    return f(*args)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 449, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 392, in _initialize
    *args, **kwds))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 335, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 902, in wrapper
    ), args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 401, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File ""/tmp/tmpd4qk3dv8.py"", line 61, in distributed_train_step
    per_replica_losses = ag__.converted_call(strategy.experimental_run_v2, distributed_train_step_scope.callopts, (train_step,), {'args': (inputs,)}, distributed_train_step_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 764, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1810, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 662, in _call_for_each_replica
    fn, args, kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 196, in _call_for_each_replica
    coord.join(threads)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/svc-ds/anaconda3/envs/chemvae/lib/python3.6/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 880, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/tmp/tmpd4qk3dv8.py"", line 46, in train_step
    y_pred = ag__.converted_call(AE_only_model, train_step_scope.callopts, (inputs[0],), None, train_step_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 541, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/tmpg_1oddri.py"", line 192, in tf____call__
    output = ag__.converted_call(self.call, __call___scope.callopts, (inputs,), dict(kwargs, **{}), __call___scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmppws4cnkd.py"", line 68, in tf__call
    do_return, retval_ = ag__.if_stmt(cond_1, if_true_1, if_false_1, get_state_1, set_state_1, ('do_return', 'retval_'), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmppws4cnkd.py"", line 63, in if_false_1
    output_tensors, _, _ = ag__.converted_call(self.run_internal_graph, call_scope.callopts, (inputs, masks), None, call_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 541, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/tmpupurh3pr.py"", line 298, in tf__run_internal_graph
    ag__.for_stmt(depth_keys, None, loop_body_5, get_state_17, set_state_17, (), (), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 339, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 350, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmpupurh3pr.py"", line 296, in loop_body_5
    ag__.for_stmt(nodes, None, loop_body_4, get_state_16, set_state_16, (), (), ())
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 339, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 350, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmpupurh3pr.py"", line 294, in loop_body_4
    ag__.if_stmt(cond_11, if_true_11, if_false_11, get_state_15, set_state_15, (), ('x._keras_shape', 'x._uses_learning_phase', ""kwargs['mask']""))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpupurh3pr.py"", line 211, in if_true_11
    computed_tensors, output_tensors, output_masks = ag__.if_stmt(cond_7, if_true_7, if_false_7, get_state_9, set_state_9, ('computed_tensors', 'output_tensors', 'output_masks'), (""kwargs['mask']"",))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpupurh3pr.py"", line 207, in if_false_7
    output_tensors = ag__.converted_call(_to_list, run_internal_graph_scope.callopts, (ag__.converted_call(layer.call, run_internal_graph_scope.callopts, (computed_tensors,), dict(kwargs, **{}), run_internal_graph_scope),), None, run_internal_graph_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp4nuqr_3n.py"", line 31, in tf__call
    retval_ = call_scope.mark_return_value(ag__.converted_call(self.function, call_scope.callopts, (inputs,), dict(arguments, **{}), call_scope))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmphkvehh5i.py"", line 13, in tf__sampling
    epsilon = ag__.converted_call(K.random_normal_variable, sampling_scope.callopts, (), {'shape': (params['batch_size'], params['hidden_dim']), 'mean': 0.0, 'scale': 1.0}, sampling_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp2dyogoo7.py"", line 68, in tf__random_normal_variable
    retval_ = random_normal_variable_scope.mark_return_value(ag__.converted_call(variable, random_normal_variable_scope.callopts, (value,), {'dtype': dtype, 'name': name}, random_normal_variable_scope))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 539, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpqaev2zhu.py"", line 103, in tf__variable
    do_return, retval_ = ag__.if_stmt(cond_3, if_true_3, if_false_3, get_state_3, set_state_3, ('do_return', 'retval_'), ('v._keras_shape', 'v._uses_learning_phase'))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 895, in if_stmt
    return _py_if_stmt(cond, body, orelse)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 1004, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/tmpqaev2zhu.py"", line 67, in if_false_3
    v = ag__.converted_call(tf.Variable, variable_scope.callopts, (value,), {'dtype': ag__.converted_call(_convert_string_dtype, variable_scope.callopts, (dtype,), None, variable_scope), 'name': name}, variable_scope)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 422, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 258, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 219, in _variable_v1_call
    shape=shape)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/shared_variable_creator.py"", line 69, in create_new_variable
    v = next_creator(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1345, in creator_with_resource_vars
    return self._create_variable(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 527, in _create_variable
    values.SyncOnReadVariable, *args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2311, in create_mirrored_variable
    value_list = real_mirrored_creator(devices, *args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 519, in _real_mirrored_creator
    v = next_creator(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 380, in variable_capturing_scope
    lifted_initializer_graph=lifted_initializer_graph, **kwds)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 175, in __init__
    disallowed_placeholders=placeholder_ops)[initial_value]
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/lift_to_graph.py"", line 260, in lift_to_graph
    add_sources=add_sources))
  File ""/home/svc-ds/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/op_selector.py"", line 413, in map_subgraph
    % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))
tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'model_1/lambda_1/Variable/replica_1/Initializer/Identity:0' shape=(64, 196) dtype=float32> because it depends transitively on placeholder <tf.Operation 'model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable' type=Placeholder> via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp (ReadVariableOp) <- model_1/lambda_1/Variable/replica_1/Initializer/ReadVariableOp/model_1/lambda_1/Variable (Placeholder)
```
",dhruv2103,2024-07-11 04:39:49+00:00,['tilakrayal'],2024-08-01 11:21:21+00:00,2024-07-28 01:56:41+00:00,https://github.com/tensorflow/tensorflow/issues/71632,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 1.15', 'for issues seen on TF 1.15')]","[{'comment_id': 2222018169, 'issue_id': 2402231803, 'author': 'dhruv2103', 'body': 'I am looking for a solution to this error. I have never seen this error before and could not understand it through online sources. There is very less resources online about this error.', 'created_at': datetime.datetime(2024, 7, 11, 4, 43, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2225066898, 'issue_id': 2402231803, 'author': 'tilakrayal', 'body': '@dhruv2103,\r\nCould you please try to check in the latest tensorflow version and let us know if you are facing the same issue. Also Some apis are deprecated in [TF v2.x](https://colab.research.google.com/gist/sushreebarsa/e89efe21ca7feac3bc31c479b1c585a5/19390.ipynb). TF v1.x is not actively supported, please upgrade to the latest TF versions and refer to the[migration](https://www.tensorflow.org/guide/migrate) doc to know more on this. Thank you!', 'created_at': datetime.datetime(2024, 7, 12, 8, 18, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240829747, 'issue_id': 2402231803, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 20, 1, 50, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254310987, 'issue_id': 2402231803, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 28, 1, 56, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254311013, 'issue_id': 2402231803, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71632"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71632"">No</a>', 'created_at': datetime.datetime(2024, 7, 28, 1, 56, 43, tzinfo=datetime.timezone.utc)}]","dhruv2103 (Issue Creator) on (2024-07-11 04:43:21 UTC): I am looking for a solution to this error. I have never seen this error before and could not understand it through online sources. There is very less resources online about this error.

tilakrayal (Assginee) on (2024-07-12 08:18:22 UTC): @dhruv2103,
Could you please try to check in the latest tensorflow version and let us know if you are facing the same issue. Also Some apis are deprecated in [TF v2.x](https://colab.research.google.com/gist/sushreebarsa/e89efe21ca7feac3bc31c479b1c585a5/19390.ipynb). TF v1.x is not actively supported, please upgrade to the latest TF versions and refer to the[migration](https://www.tensorflow.org/guide/migrate) doc to know more on this. Thank you!

github-actions[bot] on (2024-07-20 01:50:41 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-28 01:56:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-28 01:56:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71632"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71632"">No</a>

"
2401489014,issue,closed,completed,tensorflow 2.18.0 build error: Compiling xla/service/cpu/runtime/thunk_executor.cc failed,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

CentOS 9 Stream

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

13.3.1

### CUDA/cuDNN version

12.5

### GPU model and memory

GeForce RTX 2080 Ti 11GB

### Current behavior?

I am trying to build the latest TF 2.18 and getting following error: Compiling xla/service/cpu/runtime/thunk_executor.cc failed. I use GCC 13.3.1 from the gcc-toolset-13.

Here is an initial part of output log. After ~20 minutes of compiling it faiils with the **Compiling xla/service/cpu/runtime/thunk_executor.cc failed**.

```
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /usr/src/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=154
INFO: Reading rc options for 'build' from /usr/src/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /usr/src/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /usr/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-12.5 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/rh/gcc-toolset-13/root/usr/lib64:/opt/rh/gcc-toolset-13/root/usr/lib:/usr/local/cuda/lib64:/usr/local/cuDNN/cuda/lib64:/usr/geos38/lib64:/usr/proj72/lib:/usr/local/cuda/lib64/:/usr/local/cuDNN/cuda/lib64:/usr/geos38/lib64:/usr/proj72/lib:/usr/local/cuda/lib64/ --action_env GCC_HOST_COMPILER_PATH=/opt/rh/gcc-toolset-13/root/usr/bin/gcc --config=cuda
INFO: Found applicable config definition build:short_logs in file /usr/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /usr/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /usr/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /usr/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /usr/src/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /usr/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/local_xla/third_party/py/python_repo.bzl:109:10: Using hermetic Python 3.9
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (721 packages loaded, 51840 targets configured).
INFO: Found 1 target...

```

Relevant log output placed below.


### Standalone code to reproduce the issue

```shell
bazel build --config=cuda --local_cpu_resources=HOST_CPUS*.8 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
```


### Relevant log output

```shell
ERROR: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/local_xla/xla/service/cpu/runtime/BUILD:118:11: Compiling xla/service/cpu/runtime/thunk_executor.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target @local_xla//xla/service/cpu/runtime:thunk_executor) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/local_xla/xla/service/cpu/runtime/_objs/thunk_executor/thunk_executor.pic.d ... (remaining 123 arguments skipped)
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:30,
                 from external/local_xla/xla/service/buffer_assignment.h:36,
                 from external/local_xla/xla/runtime/buffer_use.h:21,
                 from external/local_xla/xla/service/cpu/runtime/thunk.h:36,
                 from external/local_xla/xla/service/cpu/runtime/thunk_executor.h:34,
                 from external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:16:
external/com_google_absl/absl/log/check.h:57: warning: ""CHECK"" redefined
   57 | #define CHECK(condition) ABSL_LOG_INTERNAL_CHECK_IMPL((condition), #condition)
      |
In file included from external/local_tsl/tsl/platform/logging.h:26,
                 from external/local_xla/xla/ffi/execution_context.h:30,
                 from external/local_xla/xla/service/cpu/runtime/thunk.h:35:
external/local_tsl/tsl/platform/default/logging.h:308: note: this is the location of the previous definition
  308 | #define CHECK(condition)              \
      |
external/com_google_absl/absl/log/check.h:65: warning: ""QCHECK"" redefined
   65 | #define QCHECK(condition) ABSL_LOG_INTERNAL_QCHECK_IMPL((condition), #condition)
      |
external/local_tsl/tsl/platform/default/logging.h:542: note: this is the location of the previous definition
  542 | #define QCHECK(condition) CHECK(condition)
      |
external/com_google_absl/absl/log/check.h:88: warning: ""DCHECK"" redefined
   88 | #define DCHECK(condition) ABSL_LOG_INTERNAL_DCHECK_IMPL((condition), #condition)
      |
external/local_tsl/tsl/platform/default/logging.h:521: note: this is the location of the previous definition
  521 | #define DCHECK(condition) \
      |
external/com_google_absl/absl/log/check.h:116: warning: ""CHECK_EQ"" redefined
  116 | #define CHECK_EQ(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:499: note: this is the location of the previous definition
  499 | #define CHECK_EQ(val1, val2) CHECK_OP(Check_EQ, ==, val1, val2)
      |
external/com_google_absl/absl/log/check.h:118: warning: ""CHECK_NE"" redefined
  118 | #define CHECK_NE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:500: note: this is the location of the previous definition
  500 | #define CHECK_NE(val1, val2) CHECK_OP(Check_NE, !=, val1, val2)
      |
external/com_google_absl/absl/log/check.h:120: warning: ""CHECK_LE"" redefined
  120 | #define CHECK_LE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:501: note: this is the location of the previous definition
  501 | #define CHECK_LE(val1, val2) CHECK_OP(Check_LE, <=, val1, val2)
      |
external/com_google_absl/absl/log/check.h:122: warning: ""CHECK_LT"" redefined
  122 | #define CHECK_LT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:502: note: this is the location of the previous definition
  502 | #define CHECK_LT(val1, val2) CHECK_OP(Check_LT, <, val1, val2)
      |
external/com_google_absl/absl/log/check.h:124: warning: ""CHECK_GE"" redefined
  124 | #define CHECK_GE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:503: note: this is the location of the previous definition
  503 | #define CHECK_GE(val1, val2) CHECK_OP(Check_GE, >=, val1, val2)
      |
external/com_google_absl/absl/log/check.h:126: warning: ""CHECK_GT"" redefined
  126 | #define CHECK_GT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:504: note: this is the location of the previous definition
  504 | #define CHECK_GT(val1, val2) CHECK_OP(Check_GT, >, val1, val2)
      |
external/com_google_absl/absl/log/check.h:128: warning: ""QCHECK_EQ"" redefined
  128 | #define QCHECK_EQ(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:543: note: this is the location of the previous definition
  543 | #define QCHECK_EQ(x, y) CHECK_EQ(x, y)
      |
external/com_google_absl/absl/log/check.h:130: warning: ""QCHECK_NE"" redefined
  130 | #define QCHECK_NE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:544: note: this is the location of the previous definition
  544 | #define QCHECK_NE(x, y) CHECK_NE(x, y)
      |
external/com_google_absl/absl/log/check.h:132: warning: ""QCHECK_LE"" redefined
  132 | #define QCHECK_LE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:545: note: this is the location of the previous definition
  545 | #define QCHECK_LE(x, y) CHECK_LE(x, y)
      |
external/com_google_absl/absl/log/check.h:134: warning: ""QCHECK_LT"" redefined
  134 | #define QCHECK_LT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:546: note: this is the location of the previous definition
  546 | #define QCHECK_LT(x, y) CHECK_LT(x, y)
      |
external/com_google_absl/absl/log/check.h:136: warning: ""QCHECK_GE"" redefined
  136 | #define QCHECK_GE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:547: note: this is the location of the previous definition
  547 | #define QCHECK_GE(x, y) CHECK_GE(x, y)
      |
external/com_google_absl/absl/log/check.h:138: warning: ""QCHECK_GT"" redefined
  138 | #define QCHECK_GT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:548: note: this is the location of the previous definition
  548 | #define QCHECK_GT(x, y) CHECK_GT(x, y)
      |
external/com_google_absl/absl/log/check.h:140: warning: ""DCHECK_EQ"" redefined
  140 | #define DCHECK_EQ(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:531: note: this is the location of the previous definition
  531 | #define DCHECK_EQ(x, y) _TF_DCHECK_NOP(x, y)
      |
external/com_google_absl/absl/log/check.h:142: warning: ""DCHECK_NE"" redefined
  142 | #define DCHECK_NE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:532: note: this is the location of the previous definition
  532 | #define DCHECK_NE(x, y) _TF_DCHECK_NOP(x, y)
      |
external/com_google_absl/absl/log/check.h:144: warning: ""DCHECK_LE"" redefined
  144 | #define DCHECK_LE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:533: note: this is the location of the previous definition
  533 | #define DCHECK_LE(x, y) _TF_DCHECK_NOP(x, y)
      |
external/com_google_absl/absl/log/check.h:146: warning: ""DCHECK_LT"" redefined
  146 | #define DCHECK_LT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:534: note: this is the location of the previous definition
  534 | #define DCHECK_LT(x, y) _TF_DCHECK_NOP(x, y)
      |
external/com_google_absl/absl/log/check.h:148: warning: ""DCHECK_GE"" redefined
  148 | #define DCHECK_GE(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:535: note: this is the location of the previous definition
  535 | #define DCHECK_GE(x, y) _TF_DCHECK_NOP(x, y)
      |
external/com_google_absl/absl/log/check.h:150: warning: ""DCHECK_GT"" redefined
  150 | #define DCHECK_GT(val1, val2) \
      |
external/local_tsl/tsl/platform/default/logging.h:536: note: this is the location of the previous definition
  536 | #define DCHECK_GT(x, y) _TF_DCHECK_NOP(x, y)
      |
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:199: warning: ""LOG"" redefined
  199 | #define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)
      |
external/local_tsl/tsl/platform/default/logging.h:165: note: this is the location of the previous definition
  165 | #define LOG(severity) _TF_LOG_##severity
      |
external/com_google_absl/absl/log/log.h:237: warning: ""LOG_EVERY_N"" redefined
  237 | #define LOG_EVERY_N(severity, n) \
      |
external/local_tsl/tsl/platform/default/logging.h:278: note: this is the location of the previous definition
  278 | #define LOG_EVERY_N(severity, n)                       \
      |
external/com_google_absl/absl/log/log.h:245: warning: ""LOG_FIRST_N"" redefined
  245 | #define LOG_FIRST_N(severity, n) \
      |
external/local_tsl/tsl/platform/default/logging.h:284: note: this is the location of the previous definition
  284 | #define LOG_FIRST_N(severity, n)                       \
      |
external/com_google_absl/absl/log/log.h:253: warning: ""LOG_EVERY_POW_2"" redefined
  253 | #define LOG_EVERY_POW_2(severity) \
      |
external/local_tsl/tsl/platform/default/logging.h:290: note: this is the location of the previous definition
  290 | #define LOG_EVERY_POW_2(severity)                         \
      |
external/com_google_absl/absl/log/log.h:265: warning: ""LOG_EVERY_N_SEC"" redefined
  265 | #define LOG_EVERY_N_SEC(severity, n_seconds) \
      |
external/local_tsl/tsl/platform/default/logging.h:300: note: this is the location of the previous definition
  300 | #define LOG_EVERY_N_SEC(severity, n_seconds)                      \
      |
In file included from external/local_xla/xla/shape.h:28,
                 from external/local_xla/xla/index_util.h:24,
                 from external/local_xla/xla/literal.h:45,
                 from external/local_xla/xla/hlo/ir/dfs_hlo_visitor.h:27,
                 from external/local_xla/xla/hlo/ir/hlo_computation.h:36:
external/local_xla/xla/layout.h:464:18: warning: xla::Layout::DimInfo::dim_level_type is too small to hold all values of enum xla::DimLevelType
  464 |     DimLevelType dim_level_type : 6;
      |                  ^~~~~~~~~~~~~~
external/local_xla/xla/layout.h:476:17: warning: xla::Layout::index_primitive_type_ is too small to hold all values of enum xla::PrimitiveType
  476 |   PrimitiveType index_primitive_type_ : 8;
      |                 ^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/layout.h:477:17: warning: xla::Layout::pointer_primitive_type_ is too small to hold all values of enum xla::PrimitiveType
  477 |   PrimitiveType pointer_primitive_type_ : 8;
      |                 ^~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/runtime/thunk_executor.h:105:14: warning: use of std::hardware_destructive_interference_size [-Winterference-size]
  105 |         std::hardware_destructive_interference_size;
      |         ~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/runtime/thunk_executor.h:105:14: note: its value can vary between compiler versions or with different -mtune or -mcpu flags
external/local_xla/xla/service/cpu/runtime/thunk_executor.h:105:14: note: if this use is part of a public ABI, change it to instead use a constant variable you define
external/local_xla/xla/service/cpu/runtime/thunk_executor.h:105:14: note: the default value for the current CPU tuning is 64 bytes
external/local_xla/xla/service/cpu/runtime/thunk_executor.h:105:14: note: you can stabilize this value with --param hardware_destructive_interference_size=64, or disable this warning with -Wno-interference-size
In file included from /usr/include/unistd.h:226,
                 from external/com_google_absl/absl/base/internal/thread_identity.h:27,
                 from external/com_google_absl/absl/synchronization/mutex.h:70,
                 from external/com_google_absl/absl/strings/internal/cordz_info.h:31,
                 from external/com_google_absl/absl/strings/cord.h:91,
                 from external/com_google_absl/absl/status/internal/status_internal.h:23,
                 from external/com_google_absl/absl/status/status.h:59,
                 from external/local_xla/xla/service/cpu/runtime/thunk_executor.h:30:
external/local_xla/xla/tsl/concurrency/async_value.h: In instantiation of static void tsl::internal::ConcreteAsyncValue<T>::VerifyOffsets() [with T = tsl::DummyValueForErrorAsyncValue]:
external/local_xla/xla/tsl/concurrency/async_value.h:547:18:   required from tsl::internal::ConcreteAsyncValue<T>::ConcreteAsyncValue(absl::lts_20230802::Status) [with T = tsl::DummyValueForErrorAsyncValue]
external/local_xla/xla/tsl/concurrency/async_value.h:738:30:   required from here
external/local_xla/xla/tsl/concurrency/async_value.h:717:28: warning: offsetof within non-standard-layout type tsl::internal::ConcreteAsyncValue<tsl::DummyValueForErrorAsyncValue> is conditionally-supported [-Winvalid-offsetof]
  717 |     static_assert(offsetof(ConcreteAsyncValue<T>, data_store_.data_) ==
      |                            ^
external/local_xla/xla/tsl/concurrency/async_value.h: In instantiation of static void tsl::internal::ConcreteAsyncValue<T>::VerifyOffsets() [with T = tsl::Chain]:
external/local_xla/xla/tsl/concurrency/async_value.h:556:18:   required from tsl::internal::ConcreteAsyncValue<T>::ConcreteAsyncValue(ConstructedPayload, Args&& ...) [with Args = {}; T = tsl::Chain]
external/local_xla/xla/tsl/concurrency/async_value_ref.h:862:10:   required from T* tsl::internal::PlacementConstruct(void*, Args&& ...) [with T = ConcreteAsyncValue<tsl::Chain>; Args = {ConcreteAsyncValue<tsl::Chain>::ConstructedPayload}]
external/local_xla/xla/tsl/concurrency/async_value_ref.h:868:40:   required from T* tsl::internal::AllocateAndConstruct(Args&& ...) [with T = ConcreteAsyncValue<tsl::Chain>; Args = {ConcreteAsyncValue<tsl::Chain>::ConstructedPayload}]
external/local_xla/xla/tsl/concurrency/async_value_ref.h:895:70:   required from tsl::AsyncValueRef<T> tsl::MakeConstructedAsyncValueRef(Args&& ...) [with T = Chain; Args = {}]
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:120:68:   required from here
external/local_xla/xla/tsl/concurrency/async_value.h:717:28: warning: offsetof within non-standard-layout type tsl::internal::ConcreteAsyncValue<tsl::Chain> is conditionally-supported [-Winvalid-offsetof]
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:295:35: warning: always_inline function might not be inlinable [-Wattributes]
  295 | void ABSL_ATTRIBUTE_ALWAYS_INLINE ThunkExecutor::SplitReadyQueue(
      |                                   ^~~~~~~~~~~~~
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc: In member function void xla::cpu::ThunkExecutor::Execute(ExecuteState*, const xla::cpu::Thunk::ExecuteParams&, ReadyQueue, xla::cpu::Thunk::ExecuteSession::Lock):
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:295:35: error: inlining failed in call to always_inline void xla::cpu::ThunkExecutor::SplitReadyQueue(ExecuteState*, const xla::cpu::Thunk::ExecuteParams&, int64_t, ReadyQueue&): function body can be overwritten at link time
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:254:22: note: called from here
  254 |       SplitReadyQueue(state, params, /*start_index=*/i + 1, ready_queue);
      |       ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:295:35: error: inlining failed in call to always_inline void xla::cpu::ThunkExecutor::SplitReadyQueue(ExecuteState*, const xla::cpu::Thunk::ExecuteParams&, int64_t, ReadyQueue&): function body can be overwritten at link time
  295 | void ABSL_ATTRIBUTE_ALWAYS_INLINE ThunkExecutor::SplitReadyQueue(
      |                                   ^~~~~~~~~~~~~
external/local_xla/xla/service/cpu/runtime/thunk_executor.cc:254:22: note: called from here
  254 |       SplitReadyQueue(state, params, /*start_index=*/i + 1, ready_queue);
      |       ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2413.829s, Critical Path: 338.89s
INFO: 29431 processes: 9681 internal, 19750 local.
FAILED: Build did NOT complete successfully
```
",regularRandom,2024-07-10 19:22:44+00:00,['tilakrayal'],2024-07-31 01:45:37+00:00,2024-07-31 01:45:34+00:00,https://github.com/tensorflow/tensorflow/issues/71593,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:xla', 'XLA'), ('subtype:centos', 'Centos Build/Installation issues')]","[{'comment_id': 2224440984, 'issue_id': 2401489014, 'author': 'sushreebarsa', 'body': '@maximd1 Could you please make sure to use the stable TF version and let us know?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 12, 4, 2, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2226726097, 'issue_id': 2401489014, 'author': 'regularRandom', 'body': '@sushreebarsa 2.17.0 built and installed successfully.', 'created_at': datetime.datetime(2024, 7, 13, 2, 25, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227734763, 'issue_id': 2401489014, 'author': 'tilakrayal', 'body': '@maximd1,\r\nTensorflow 2.18.0(nightly) is not the stable version. We request to use the stable tensorflow v2.17 for the usage. Once the  stable tensorflow v2.18.0 released please test and provide the update on the same. Thank you!', 'created_at': datetime.datetime(2024, 7, 15, 5, 41, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244102041, 'issue_id': 2401489014, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 23, 1, 53, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259475253, 'issue_id': 2401489014, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 31, 1, 45, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259475316, 'issue_id': 2401489014, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71593"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71593"">No</a>', 'created_at': datetime.datetime(2024, 7, 31, 1, 45, 37, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-12 04:02:39 UTC): @maximd1 Could you please make sure to use the stable TF version and let us know?
Thank you!

regularRandom (Issue Creator) on (2024-07-13 02:25:56 UTC): @sushreebarsa 2.17.0 built and installed successfully.

tilakrayal (Assginee) on (2024-07-15 05:41:14 UTC): @maximd1,
Tensorflow 2.18.0(nightly) is not the stable version. We request to use the stable tensorflow v2.17 for the usage. Once the  stable tensorflow v2.18.0 released please test and provide the update on the same. Thank you!

github-actions[bot] on (2024-07-23 01:53:18 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-31 01:45:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-31 01:45:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71593"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71593"">No</a>

"
2401346789,issue,closed,completed,TFLite-Model-Maker running error: AttributeError: module 'numpy' has no attribute 'object'.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.9.3

### Custom code

Yes

### OS platform and distribution

windows11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I installed TFLite-Model-Maker with command : pip3 install tflite-model-maker , 
and run my code:

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import numpy as np
print(""numpy version:"", np.__version__)
import os
import tensorflow as tf
from tflite_model_maker import model_spec
from tflite_model_maker import object_detector
from tflite_model_maker.object_detector import DataLoader

data_dir = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot'

data_loader = DataLoader.from_pascal_voc(data_dir)

train_data, validation_data, test_data = data_loader.load()

model_spec = model_spec.get('efficientdet_lite0')

model = object_detector.create(train_data, model_spec=model_spec, validation_data=validation_data, epochs=50, batch_size=8, train_whole_model=True)

print(model.evaluate(test_data))

model.export(export_dir='./exported_model')
```

The error is reported:
FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  np.uint8, np.uint16, np.object, np.bool]
Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""C:\Users\libof\myenv1\lib\site-packages\tflite_model_maker\__init__.py"", line 44, in <module>
    from tflite_model_maker import audio_classifier
  File ""C:\Users\libof\myenv1\lib\site-packages\tflite_model_maker\audio_classifier\__init__.py"", line 24, in <module>
    from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataLoader
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflow_examples\lite\model_maker\core\data_util\audio_dataloader.py"", line 27, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\__init__.py"", line 20, in <module>
    from tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py"", line 29, in <module>
    from tensorflow_examples.lite.model_maker.core.task import model_util
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_util.py"", line 28, in <module>
    from tensorflowjs.converters import converter as tfjs_converter
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\__init__.py"", line 21, in <module>
    from tensorflowjs import converters
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\converters\__init__.py"", line 21, in <module>
    from tensorflowjs.converters.converter import convert
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\converters\converter.py"", line 35, in <module>
    from tensorflowjs.converters import keras_h5_conversion as conversion
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\converters\keras_h5_conversion.py"", line 33, in <module>
    from tensorflowjs import write_weights  # pylint: disable=import-error
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\write_weights.py"", line 25, in <module>
    from tensorflowjs import read_weights
  File ""C:\Users\libof\myenv1\lib\site-packages\tensorflowjs\read_weights.py"", line 28, in <module>
    np.uint8, np.uint16, np.object, np.bool]
  File ""C:\Users\libof\myenv1\lib\site-packages\numpy\__init__.py"", line 353, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'object'.
`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. 
The aliases was originally deprecated in NumPy 1.20;

![image](https://github.com/tensorflow/tensorflow/assets/10001548/a4184a14-8bda-45a6-a1c5-c211eafb0959)
![image](https://github.com/tensorflow/tensorflow/assets/10001548/b6c2b355-4b7c-458d-9e8b-59f7418b09ee)




### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import numpy as np
print(""numpy version:"", np.__version__)
import os
import tensorflow as tf
from tflite_model_maker import model_spec
from tflite_model_maker import object_detector
from tflite_model_maker.object_detector import DataLoader

data_dir = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot'

data_loader = DataLoader.from_pascal_voc(data_dir)

train_data, validation_data, test_data = data_loader.load()

model_spec = model_spec.get('efficientdet_lite0')

model = object_detector.create(train_data, model_spec=model_spec, validation_data=validation_data, epochs=50, batch_size=8, train_whole_model=True)

print(model.evaluate(test_data))

model.export(export_dir='./exported_model')
```


### Relevant log output

_No response_",libofei2004,2024-07-10 17:51:55+00:00,"['pkgoogle', 'sawantkumar']",2024-08-07 01:54:25+00:00,2024-08-07 01:54:21+00:00,https://github.com/tensorflow/tensorflow/issues/71587,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)'), ('TFLiteModelMaker', 'TFLite Model Maker related issues')]","[{'comment_id': 2222042482, 'issue_id': 2401346789, 'author': 'tilakrayal', 'body': ""@libofei2004,\r\nThis issue is the known issue, Could you please use [mediapipe model maker](https://developers.google.com/mediapipe/solutions/model_maker) instead for now. Here is also a [gist](https://colab.sandbox.google.com/gist/pkgoogle/93fb7581fab1ea14728c61adf584ca13/media_pipe_example.ipynb) that runs through some image classification examples with mediapipe model maker, including quantization: gist. If you can't accomplish your goals with mediapipe-model-maker please let us know and we'll see if there is a way to accomplish your goals.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/60431\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 11, 5, 7, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231475645, 'issue_id': 2401346789, 'author': 'libofei2004', 'body': '@tilakrayal Thank you, now I have installed mediapipe_model_maker, but when I run my code, the error occured:\r\nTraceback (most recent call last):\r\n  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked\r\n  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\mediapipe_model_maker\\__init__.py"", line 17, in <module>\r\n    from mediapipe_model_maker.python.vision import image_classifier\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\mediapipe_model_maker\\python\\vision\\image_classifier\\__init__.py"", line 16, in <module>\r\n    from mediapipe_model_maker.python.vision.image_classifier import dataset\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\mediapipe_model_maker\\python\\vision\\image_classifier\\dataset.py"", line 21, in <module>\r\n    import tensorflow_datasets as tfds\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\__init__.py"", line 43, in <module>\r\n    import tensorflow_datasets.core.logging as _tfds_logging\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py"", line 22, in <module>\r\n    from tensorflow_datasets.core import community\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py"", line 18, in <module>\r\n    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py"", line 31, in <module>\r\n    from tensorflow_datasets.core import dataset_builder\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py"", line 34, in <module>\r\n    from tensorflow_datasets.core import dataset_info\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py"", line 47, in <module>\r\n    from tensorflow_datasets.core import file_adapters\r\n  File ""C:\\Users\\libof\\env1\\lib\\site-packages\\tensorflow_datasets\\core\\file_adapters.py"", line 29, in <module>\r\n    from array_record.python import array_record_module\r\nImportError: cannot import name \'array_record_module\' from \'array_record.python\' (C:\\Users\\libof\\env1\\lib\\site-packages\\array_record\\python\\__init__.py)\r\n\r\nmy code is:\r\n`import os\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\r\n \r\nfrom mediapipe_model_maker import object_detector\r\n\r\ntrain_dataset_path = \'C:\\\\workspace\\\\imgupload\\\\img\\\\selected1\\\\bt3\\\\train\\\\shot\'\r\nvalidation_dataset_path = \'C:\\\\workspace\\\\imgupload\\\\img\\\\selected1\\\\bt3\\\\train\\\\shot\'\r\ncache_dir = \'C:\\\\workspace\\\\imgupload\\\\img\\\\selected1\\\\tmp\'\r\n\r\ntrain_data = object_detector.Dataset.from_pascal_voc_folder(\r\n    train_dataset_path,\r\n    cache_dir=cache_dir)\r\n \r\nvalidate_data = object_detector.Dataset.from_pascal_voc_folder(\r\n    validation_dataset_path,\r\n    cache_dir=cache_dir)\r\n\r\nhparams = object_detector.HParams(batch_size=8, learning_rate=0.3, epochs=50, export_dir=\'exported_model\')\r\noptions = object_detector.ObjectDetectorOptions(\r\n    supported_model=object_detector.SupportedModels.MOBILENET_V2,\r\n    hparams=hparams)\r\n\r\nmodel = object_detector.ObjectDetector.create(\r\n    train_data=train_data,\r\n    validation_data=validate_data,\r\n    options=options)\r\n\r\nloss, coco_metrics = model.evaluate(validate_data, batch_size=4)\r\nprint(f""Validation loss: {loss}"")\r\nprint(f""Validation coco metrics: {coco_metrics}"")\r\n\r\nmodel.export_model(\'dogs.tflite\')`\r\n\r\n![image](https://github.com/user-attachments/assets/b4efb490-ef13-41e3-8c4c-f8f36e419aa6)', 'created_at': datetime.datetime(2024, 7, 16, 17, 43, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235758079, 'issue_id': 2401346789, 'author': 'sawantkumar', 'body': 'Hi @libofei2004 ,\r\n\r\nI am not able to replicate this issue since i keep getting the below error, can you please upload a jupyter notebook with the code and the dataset so that i will be able to replicate this issue ?\r\n\r\n`    return cache_files.TFRecordCacheFiles(\r\n  File ""<string>"", line 6, in __init__\r\n  File ""/home/sawantkumar/work/python_wor/myenv/lib/python3.9/site-packages/mediapipe_model_maker/python/core/data/cache_files.py"", line 53, in __post_init__\r\n    raise ValueError(\r\nValueError: num_shards must be greater than 0, got 0`', 'created_at': datetime.datetime(2024, 7, 18, 6, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236589450, 'issue_id': 2401346789, 'author': 'libofei2004', 'body': '@sawantkumar  my code is shown above. I think the problem is caused by conflicting versions of dependencies.', 'created_at': datetime.datetime(2024, 7, 18, 13, 52, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236600343, 'issue_id': 2401346789, 'author': 'libofei2004', 'body': '@tilakrayal I use ""pip3 install mediapipe-model-maker to install mediapipe-model-maker,  my version is 0.1.0.1,  but the latest version is 0.2.1.4 in https://pypi.org/project/mediapipe-model-maker/, if I use  pip3 install mediapipe_model_maker==0.2.1.4  to update, the error occured:\r\nERROR: Could not find a version that satisfies the requirement mediapipe_model_maker==0.2.1.4 (from versions: none)\r\nERROR: No matching distribution found for mediapipe_model_maker==0.2.1.4\r\n\r\nI don\'t know why?', 'created_at': datetime.datetime(2024, 7, 18, 13, 57, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245979687, 'issue_id': 2401346789, 'author': 'pkgoogle', 'body': 'Hi @libofei2004, there is probably some weird conflict w/ your previous installation.. can you try with a fresh conda environment or a fresh venv python environment? Let us know what happens.', 'created_at': datetime.datetime(2024, 7, 23, 18, 41, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259475266, 'issue_id': 2401346789, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 31, 1, 45, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272470072, 'issue_id': 2401346789, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 7, 1, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272470117, 'issue_id': 2401346789, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71587"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71587"">No</a>', 'created_at': datetime.datetime(2024, 8, 7, 1, 54, 23, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-11 05:07:27 UTC): @libofei2004,
This issue is the known issue, Could you please use [mediapipe model maker](https://developers.google.com/mediapipe/solutions/model_maker) instead for now. Here is also a [gist](https://colab.sandbox.google.com/gist/pkgoogle/93fb7581fab1ea14728c61adf584ca13/media_pipe_example.ipynb) that runs through some image classification examples with mediapipe model maker, including quantization: gist. If you can't accomplish your goals with mediapipe-model-maker please let us know and we'll see if there is a way to accomplish your goals.

https://github.com/tensorflow/tensorflow/issues/60431

Thank you!

libofei2004 (Issue Creator) on (2024-07-16 17:43:26 UTC): @tilakrayal Thank you, now I have installed mediapipe_model_maker, but when I run my code, the error occured:
Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 790, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""C:\Users\libof\env1\lib\site-packages\mediapipe_model_maker\__init__.py"", line 17, in <module>
    from mediapipe_model_maker.python.vision import image_classifier
  File ""C:\Users\libof\env1\lib\site-packages\mediapipe_model_maker\python\vision\image_classifier\__init__.py"", line 16, in <module>
    from mediapipe_model_maker.python.vision.image_classifier import dataset
  File ""C:\Users\libof\env1\lib\site-packages\mediapipe_model_maker\python\vision\image_classifier\dataset.py"", line 21, in <module>
    import tensorflow_datasets as tfds
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\__init__.py"", line 43, in <module>
    import tensorflow_datasets.core.logging as _tfds_logging
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\__init__.py"", line 22, in <module>
    from tensorflow_datasets.core import community
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\community\__init__.py"", line 18, in <module>
    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\community\huggingface_wrapper.py"", line 31, in <module>
    from tensorflow_datasets.core import dataset_builder
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 34, in <module>
    from tensorflow_datasets.core import dataset_info
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\dataset_info.py"", line 47, in <module>
    from tensorflow_datasets.core import file_adapters
  File ""C:\Users\libof\env1\lib\site-packages\tensorflow_datasets\core\file_adapters.py"", line 29, in <module>
    from array_record.python import array_record_module
ImportError: cannot import name 'array_record_module' from 'array_record.python' (C:\Users\libof\env1\lib\site-packages\array_record\python\__init__.py)

my code is:
`import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
 
from mediapipe_model_maker import object_detector

train_dataset_path = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot'
validation_dataset_path = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot'
cache_dir = 'C:\\workspace\\imgupload\\img\\selected1\\tmp'

train_data = object_detector.Dataset.from_pascal_voc_folder(
    train_dataset_path,
    cache_dir=cache_dir)
 
validate_data = object_detector.Dataset.from_pascal_voc_folder(
    validation_dataset_path,
    cache_dir=cache_dir)

hparams = object_detector.HParams(batch_size=8, learning_rate=0.3, epochs=50, export_dir='exported_model')
options = object_detector.ObjectDetectorOptions(
    supported_model=object_detector.SupportedModels.MOBILENET_V2,
    hparams=hparams)

model = object_detector.ObjectDetector.create(
    train_data=train_data,
    validation_data=validate_data,
    options=options)

loss, coco_metrics = model.evaluate(validate_data, batch_size=4)
print(f""Validation loss: {loss}"")
print(f""Validation coco metrics: {coco_metrics}"")

model.export_model('dogs.tflite')`

![image](https://github.com/user-attachments/assets/b4efb490-ef13-41e3-8c4c-f8f36e419aa6)

sawantkumar (Assginee) on (2024-07-18 06:51:53 UTC): Hi @libofei2004 ,

I am not able to replicate this issue since i keep getting the below error, can you please upload a jupyter notebook with the code and the dataset so that i will be able to replicate this issue ?

`    return cache_files.TFRecordCacheFiles(
  File ""<string>"", line 6, in __init__
  File ""/home/sawantkumar/work/python_wor/myenv/lib/python3.9/site-packages/mediapipe_model_maker/python/core/data/cache_files.py"", line 53, in __post_init__
    raise ValueError(
ValueError: num_shards must be greater than 0, got 0`

libofei2004 (Issue Creator) on (2024-07-18 13:52:40 UTC): @sawantkumar  my code is shown above. I think the problem is caused by conflicting versions of dependencies.

libofei2004 (Issue Creator) on (2024-07-18 13:57:47 UTC): @tilakrayal I use ""pip3 install mediapipe-model-maker to install mediapipe-model-maker,  my version is 0.1.0.1,  but the latest version is 0.2.1.4 in https://pypi.org/project/mediapipe-model-maker/, if I use  pip3 install mediapipe_model_maker==0.2.1.4  to update, the error occured:
ERROR: Could not find a version that satisfies the requirement mediapipe_model_maker==0.2.1.4 (from versions: none)
ERROR: No matching distribution found for mediapipe_model_maker==0.2.1.4

I don't know why?

pkgoogle (Assginee) on (2024-07-23 18:41:58 UTC): Hi @libofei2004, there is probably some weird conflict w/ your previous installation.. can you try with a fresh conda environment or a fresh venv python environment? Let us know what happens.

github-actions[bot] on (2024-07-31 01:45:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-07 01:54:20 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-07 01:54:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71587"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71587"">No</a>

"
2401130396,issue,closed,completed,How to fix this issue ? Please help ??,"<img width=""745"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/37790823/8af593a2-75e5-4acc-b599-551a0ea40ef5"">",wscJayasooriya,2024-07-10 15:54:57+00:00,['tilakrayal'],2024-07-23 09:53:53+00:00,2024-07-23 09:53:50+00:00,https://github.com/tensorflow/tensorflow/issues/71578,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2222020808, 'issue_id': 2401130396, 'author': 'tilakrayal', 'body': '@wscJayasooriya,\r\nCould you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment and the tensorflow version you are using. It helps us in localizing the issue faster. Thank you!', 'created_at': datetime.datetime(2024, 7, 11, 4, 46, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2222258172, 'issue_id': 2401130396, 'author': 'wscJayasooriya', 'body': 'import tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model(""../mymodel.keras"")\r\nmodel.compile(optimizer=\'adam\', loss=\'sparse_categorical_crossentropy\', metrics=[\'accuracy\'])\r\nmodel.save(\'saved_model.keras\')\r\n\r\nmodel = tf.keras.models.load_model(\'saved_model.keras\')\r\nmodel.compile(optimizer=\'adam\', loss=\'sparse_categorical_crossentropy\', metrics=[\'accuracy\'])\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open(""../mymodel.tflite"", ""wb"") as f:\r\n    f.write(tflite_model)\r\n\r\n-----------------------------------------------------ERROR-------------------------------------------\r\nAttributeError: \'Sequential\' object has no attribute \'_get_save_spec\'', 'created_at': datetime.datetime(2024, 7, 11, 7, 45, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2228568325, 'issue_id': 2401130396, 'author': 'tilakrayal', 'body': '@wscJayasooriya,\r\nYeah. This is the known issue in tensorflow v2.16. Could you please try using Legacy Keras is the preferred workaround for now:\r\n\r\n```python\r\nimport os\r\nos.environ[""TF_USE_LEGACY_KERAS""]=""1""\r\n```\r\n\r\nhttps://blog.tensorflow.org/2024/03/whats-new-in-tensorflow-216.html\r\nhttps://github.com/tensorflow/tensorflow/issues/63867\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 15, 13, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244102061, 'issue_id': 2401130396, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 23, 1, 53, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244642154, 'issue_id': 2401130396, 'author': 'wscJayasooriya', 'body': 'Sorry for the late response. Your answer helped me in my issue. \r\nThank you', 'created_at': datetime.datetime(2024, 7, 23, 8, 56, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244719331, 'issue_id': 2401130396, 'author': 'tilakrayal', 'body': '@wscJayasooriya,\r\nGlad the suggestion worked for you. Could you please feel free to move this issue to closed status? Thank you!', 'created_at': datetime.datetime(2024, 7, 23, 9, 32, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244766132, 'issue_id': 2401130396, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71578"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71578"">No</a>', 'created_at': datetime.datetime(2024, 7, 23, 9, 53, 52, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-11 04:46:14 UTC): @wscJayasooriya,
Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment and the tensorflow version you are using. It helps us in localizing the issue faster. Thank you!

wscJayasooriya (Issue Creator) on (2024-07-11 07:45:06 UTC): import tensorflow as tf

model = tf.keras.models.load_model(""../mymodel.keras"")
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.save('saved_model.keras')

model = tf.keras.models.load_model('saved_model.keras')
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()

with open(""../mymodel.tflite"", ""wb"") as f:
    f.write(tflite_model)

-----------------------------------------------------ERROR-------------------------------------------
AttributeError: 'Sequential' object has no attribute '_get_save_spec'

tilakrayal (Assginee) on (2024-07-15 13:56:00 UTC): @wscJayasooriya,
Yeah. This is the known issue in tensorflow v2.16. Could you please try using Legacy Keras is the preferred workaround for now:

```python
import os
os.environ[""TF_USE_LEGACY_KERAS""]=""1""
```

https://blog.tensorflow.org/2024/03/whats-new-in-tensorflow-216.html
https://github.com/tensorflow/tensorflow/issues/63867

Thank you!

github-actions[bot] on (2024-07-23 01:53:19 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

wscJayasooriya (Issue Creator) on (2024-07-23 08:56:25 UTC): Sorry for the late response. Your answer helped me in my issue. 
Thank you

tilakrayal (Assginee) on (2024-07-23 09:32:34 UTC): @wscJayasooriya,
Glad the suggestion worked for you. Could you please feel free to move this issue to closed status? Thank you!

google-ml-butler[bot] on (2024-07-23 09:53:52 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71578"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71578"">No</a>

"
2401053553,issue,closed,completed,Tensorflow Metal Plugin produces different results with CPU for Conv1D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow 2.16.1
tensorflow-metal 1.1.0

### Custom code

Yes

### OS platform and distribution

macOS 14.5

### Mobile device

macOS 14.5

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If we create Conv1D layer with 65537 layers, the results are different.

On macOS (`pip install tensorflow-metal`), the results are:
```
y_cpu: [[[ 0.00041509 -0.00370452 -0.00212137 ... -0.00374832 -0.00753052
   -0.00895449]]]
y_gpu: [[[-0.00895449 -0.00370452 -0.00212137 ... -0.00374832 -0.00753052
    0.        ]]]
Equal: False
```

On Linux (`pip install tensorflow-gpu`), the results are:
```
y_cpu: [[[ 0.00225602  0.01136317  0.00391667 ... -0.0028112   0.00243109
   -0.00492344]]]
y_gpu: [[[ 0.00225602  0.01136317  0.00391667 ... -0.0028112   0.00243109
   -0.00492344]]]
Equal: True
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tf.random.set_seed(0)

conv = tf.keras.layers.Conv1D(filters=65537, kernel_size=3, padding='same')

x = tf.ones([1, 1, 3])

with tf.device('/CPU:0'):
    y_cpu = conv(x)

with tf.device('/GPU:0'):
    y_gpu = conv(x)

print(""y_cpu:"", y_cpu.numpy())
print(""y_gpu:"", y_gpu.numpy())
print(""Equal:"", np.allclose(y_cpu.numpy(), y_gpu.numpy()))
```


### Relevant log output

_No response_",hguandl,2024-07-10 15:18:35+00:00,['tilakrayal'],2024-09-06 01:57:58+00:00,2024-09-06 01:57:55+00:00,https://github.com/tensorflow/tensorflow/issues/71577,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('TF 2.16', '')]","[{'comment_id': 2251965067, 'issue_id': 2401053553, 'author': 'tilakrayal', 'body': '@hguandl,\r\nWhen I tried to check in the colab which was having Linux in the background which was executed and observed that the results are also similar. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6907eed04ea3b9cec404e0a753cb74a4/untitled2024.ipynb). I will check in Macos as well and provide more info on this. Meanwhile could you please check with tf-nightly and provide the update if you are facing the same. Thank you!', 'created_at': datetime.datetime(2024, 7, 26, 4, 44, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262101761, 'issue_id': 2401053553, 'author': 'hguandl', 'body': '@tilakrayal \r\nThank you for the information! When I tried tf-nightly, the program reported a library error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/Users/hguandl/Desktop/Workspace/ml-transfer/pt-129207-tf-report.py"", line 1, in <module>\r\n    import tensorflow as tf\r\n  File ""/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/__init__.py"", line 434, in <module>\r\n    _ll.load_library(_plugin_dir)\r\n  File ""/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\r\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib\r\n  Expected in:     <FE39B172-55AE-3F24-B912-DE28E355BE70> /Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\nI guess tf-nightly does not work with the [PluggableDevice](https://developer.apple.com/metal/tensorflow-plugin/) well.', 'created_at': datetime.datetime(2024, 8, 1, 5, 57, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304010468, 'issue_id': 2401053553, 'author': 'tilakrayal', 'body': ""@hguandl,\r\nApologies for the delay. I tried to execute the mentioned code using multiple environments like windows, wsl2, linux and others as well and couldn't find the different results with the CPU for Conv1D. So please raise the issue in the Apple forum for the quick resolution. Thank you!"", 'created_at': datetime.datetime(2024, 8, 22, 7, 50, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2319671550, 'issue_id': 2401053553, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 30, 1, 57, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039590, 'issue_id': 2401053553, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039634, 'issue_id': 2401053553, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71577"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71577"">No</a>', 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 57, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-26 04:44:14 UTC): @hguandl,
When I tried to check in the colab which was having Linux in the background which was executed and observed that the results are also similar. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6907eed04ea3b9cec404e0a753cb74a4/untitled2024.ipynb). I will check in Macos as well and provide more info on this. Meanwhile could you please check with tf-nightly and provide the update if you are facing the same. Thank you!

hguandl (Issue Creator) on (2024-08-01 05:57:12 UTC): @tilakrayal 
Thank you for the information! When I tried tf-nightly, the program reported a library error:
```
Traceback (most recent call last):
  File ""/Users/hguandl/Desktop/Workspace/ml-transfer/pt-129207-tf-report.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/__init__.py"", line 434, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <FE39B172-55AE-3F24-B912-DE28E355BE70> /Users/hguandl/Library/micromamba/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```
I guess tf-nightly does not work with the [PluggableDevice](https://developer.apple.com/metal/tensorflow-plugin/) well.

tilakrayal (Assginee) on (2024-08-22 07:50:17 UTC): @hguandl,
Apologies for the delay. I tried to execute the mentioned code using multiple environments like windows, wsl2, linux and others as well and couldn't find the different results with the CPU for Conv1D. So please raise the issue in the Apple forum for the quick resolution. Thank you!

github-actions[bot] on (2024-08-30 01:57:25 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-06 01:57:55 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-06 01:57:57 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71577"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71577"">No</a>

"
2400681527,issue,closed,completed,ValueError: Trying to load a model of incompatible/unknown type.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

Widows

### Mobile device

_No response_

### Python version

3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Although my code was work perfectly for months, but yesterday suddenly I get this error when I run my code: ValueError: Trying to load a model of incompatible/unknown type. 'C:\Users\me\AppData\Local\Temp\tfhub_modules\230e8287a3b3f30e3824b066c9ee9c839533b009' contains neither 'saved_model.pb' nor 'saved_model.pbtxt

### Standalone code to reproduce the issue

```shell
bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' 
#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'

map_name_to_handle = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_base/2',
    'electra_small':
        'https://tfhub.dev/google/electra_small/2',
    'electra_base':
        'https://tfhub.dev/google/electra_base/2',
    'experts_pubmed':
        'https://tfhub.dev/google/experts/bert/pubmed/2',
    'experts_wiki_books':
        'https://tfhub.dev/google/experts/bert/wiki_books/2',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
}

map_model_to_preprocess = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',
    'electra_small':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'electra_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'experts_pubmed':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'experts_wiki_books':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',
}

tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[290], line 1
----> 1 bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

File ~\Anaconda\lib\site-packages\tensorflow_hub\keras_layer.py:157, in KerasLayer.__init__(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)
    153   self._output_shape = data_structures.NoDependency(
    154       _convert_nest_to_shapes(output_shape))
    156 self._load_options = load_options
--> 157 self._func = load_module(handle, tags, self._load_options)
    158 self._is_hub_module_v1 = getattr(self._func, ""_is_hub_module_v1"", False)
    160 # Update with the defaults when using legacy TF1 Hub format.

File ~\Anaconda\lib\site-packages\tensorflow_hub\keras_layer.py:459, in load_module(handle, tags, load_options)
    457     except ImportError:  # Expected before TF2.4.
    458       set_load_options = load_options
--> 459 return module_v2.load(handle, tags=tags, options=set_load_options)

File ~\Anaconda\lib\site-packages\tensorflow_hub\module_v2.py:107, in load(handle, tags, options)
    102 saved_model_pbtxt_path = os.path.join(
    103     tf.compat.as_bytes(module_path),
    104     tf.compat.as_bytes(tf.saved_model.SAVED_MODEL_FILENAME_PBTXT))
    105 if (not tf.io.gfile.exists(saved_model_path) and
    106     not tf.io.gfile.exists(saved_model_pbtxt_path)):
--> 107   raise ValueError(""Trying to load a model of incompatible/unknown type. ""
    108                    ""'%s' contains neither '%s' nor '%s'."" %
    109                    (module_path, tf.saved_model.SAVED_MODEL_FILENAME_PB,
    110                     tf.saved_model.SAVED_MODEL_FILENAME_PBTXT))
    112 if options:
    113   if not hasattr(getattr(tf, ""saved_model"", None), ""LoadOptions""):

ValueError: Trying to load a model of incompatible/unknown type. 'C:\Users\me\AppData\Local\Temp\tfhub_modules\230e8287a3b3f30e3824b066c9ee9c839533b009' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.
```
",hanan454,2024-07-10 12:51:58+00:00,['tilakrayal'],2024-08-01 11:20:40+00:00,2024-07-26 01:52:45+00:00,https://github.com/tensorflow/tensorflow/issues/71570,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 2.16', '')]","[{'comment_id': 2222030511, 'issue_id': 2400681527, 'author': 'tilakrayal', 'body': '@hanan454,\r\nTo avoid the issue of TF-Hub looking in temp directory for cached models, you can customise the download location to home directory by by setting the environment variable TFHUB_CACHE_DIR (recommended) or by passing the command-line flag --tfhub_cache_dir. Users who prefer persistent caching across system reboots can instead set TFHUB_CACHE_DIR to a location in their home directory. When using a persistent location, be aware that there is no automatic cleanup.\r\n\r\nI would recommend you to download the model from tfhub.dev with assets, variables and .pb checkpoint file and save it to your home directory and specify the downloaded model folder path in the hub.load() to load the model from local storage instead of looking in temp directory.\r\n\r\n```python\r\nmodel = hub.load(""/Users/tilak/Downloads/universal-sentence-encoder-multilingual-large_3/"")\r\n```\r\nThe other way would be to instruct the tensorflow_hub library to directly read models from remote storage (GCS) instead of downloading the models locally. This way, no caching directory is needed.\r\n\r\nRef: [Caching model downloads from TF Hub](https://www.tensorflow.org/hub/caching). \r\n\r\nAlso [tfhub.dev](http://tfhub.dev/) has been converged with Kaggle Model hub. You can refer [this](https://www.kaggle.com/discussions/product-feedback/448425) for update. Future improvements will be driven by Kaggle team. Thank you!', 'created_at': datetime.datetime(2024, 7, 11, 4, 56, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237879462, 'issue_id': 2400681527, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 19, 1, 52, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251813339, 'issue_id': 2400681527, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 26, 1, 52, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251813413, 'issue_id': 2400681527, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71570"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71570"">No</a>', 'created_at': datetime.datetime(2024, 7, 26, 1, 52, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-11 04:56:08 UTC): @hanan454,
To avoid the issue of TF-Hub looking in temp directory for cached models, you can customise the download location to home directory by by setting the environment variable TFHUB_CACHE_DIR (recommended) or by passing the command-line flag --tfhub_cache_dir. Users who prefer persistent caching across system reboots can instead set TFHUB_CACHE_DIR to a location in their home directory. When using a persistent location, be aware that there is no automatic cleanup.

I would recommend you to download the model from tfhub.dev with assets, variables and .pb checkpoint file and save it to your home directory and specify the downloaded model folder path in the hub.load() to load the model from local storage instead of looking in temp directory.

```python
model = hub.load(""/Users/tilak/Downloads/universal-sentence-encoder-multilingual-large_3/"")
```
The other way would be to instruct the tensorflow_hub library to directly read models from remote storage (GCS) instead of downloading the models locally. This way, no caching directory is needed.

Ref: [Caching model downloads from TF Hub](https://www.tensorflow.org/hub/caching). 

Also [tfhub.dev](http://tfhub.dev/) has been converged with Kaggle Model hub. You can refer [this](https://www.kaggle.com/discussions/product-feedback/448425) for update. Future improvements will be driven by Kaggle team. Thank you!

github-actions[bot] on (2024-07-19 01:52:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-26 01:52:45 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-26 01:52:47 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71570"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71570"">No</a>

"
2400655132,issue,closed,completed,Error building custom operator,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm following the tutorial here: https://www.tensorflow.org/guide/create_op

When trying to compile the cc file as described in the tutorial:
g++  GPUInstrumentOp.cc -fPIC ""-I/usr/local/lib/python3.10/dist-packages/tensorflow/include"" ""-L/usr/local/lib/python3.10/dist-packages/tensorflow"" -O2

I'm getting the following error:
----------------------------------------

`GPUInstrumentOp.cc: In lambda function:
GPUInstrumentOp.cc:12:22: error: 'OK' is not a member of 'tsl::Status' {aka 'absl::lts_20230802::Status'}
   12 |       return Status::OK();
      |                      ^~
GPUInstrumentOp.cc: At global scope:
GPUInstrumentOp.cc:10:16: error: cannot convert '<lambda(tensorflow::shape_inference::InferenceContext*)>' to 'tensorflow::OpShapeInferenceFn' {aka 'std::function<absl::lts_20230802::Status(tensorflow::shape_inference::InferenceContext*)>'}
   10 |     .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
In file included from GPUInstrumentOp.cc:1:
/usr/local/lib/python3.10/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:272:54: note:   initializing argument 1 of 'tensorflow::register_op::OpDefBuilderWrapper& tensorflow::register_op::OpDefBuilderWrapper::SetShapeFn(tensorflow::OpShapeInferenceFn)'
  272 |   OpDefBuilderWrapper& SetShapeFn(OpShapeInferenceFn fn) {
      |                                   ~~~~~~~~~~~~~~~~~~~^~
`


Any assistance is very apperciated

### Standalone code to reproduce the issue

```shell
The code from https://www.tensorflow.org/guide/create_op
```


### Relevant log output

_No response_",eyalhir74,2024-07-10 12:40:31+00:00,['tilakrayal'],2024-08-01 01:57:56+00:00,2024-08-01 01:57:50+00:00,https://github.com/tensorflow/tensorflow/issues/71568,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues'), ('TF 2.16', '')]","[{'comment_id': 2222795135, 'issue_id': 2400655132, 'author': 'tilakrayal', 'body': '@eyalhir74,\r\nCan you check if your op file is following [this](https://github.com/tensorflow/custom-op#template-overview) folder structure under respective kernels.\r\nAlso did you try building the sample custom op zero_out_ops mentioned in the above link.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/55148\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 11, 12, 22, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2222988783, 'issue_id': 2400655132, 'author': 'eyalhir74', 'body': 'Hi @tilakrayal ,\r\n  So seems like the zero_out_ops is somewhat out of date as far as I can understand..\r\n  I\'m working with Python 3.6 and a NVIDIA docker (nvcr.io/nvidia/tritonserver:24.05-py3)\r\n\r\n  I had to use ""return Status();"" instead of ""return Status::OK();""\r\n  \r\n  Also if someone stumbles on this and get the same/similar issues, this is the makefile I\'ve used to compile it all\r\n\r\n`\r\nTF_CFLAGS=-I/usr/local/lib/python3.10/dist-packages/tensorflow/include\r\nTF_LFLAGS=-L/usr/local/lib/python3.10/dist-packages/tensorflow -l:libtensorflow_framework.so.2\r\nCUDA_FLAGS=--expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90\r\n\r\nCFLAGS=-m64 -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64 -O2\r\nNVCC_CFLAGS=--compiler-options ""-fPIC -pthread""\r\n\r\nall: build\r\n\r\nbuild: GPUInstrumentOp\r\n\r\nGPUInstrumentOp.o:GPUInstrumentOp.cc\r\n\tnvcc -ccbin g++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} -o GPUInstrumentOp.o -c GPUInstrumentOp.cc\r\n\r\nGPUInstrumentKernels.o: GPUInstrumentKernels.cu\r\n\tnvcc -ccbin g++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} ${CUDA_FLAGS} -o GPUInstrumentKernels.o -c GPUInstrumentKernels.cu\r\n\r\nGPUInstrumentOp: GPUInstrumentOp.o GPUInstrumentKernels.o\r\n\tg++ ${CFLAGS} -Wl,--no-as-needed ${TF_CFLAGS} ${TF_LFLAGS} --shared  -o GPUInstrument.so GPUInstrumentOp.o GPUInstrumentKernels.o\r\n\r\nclean:\r\n\trm -f GPUInstrumentOp.o GPUInstrumentKernels.o GPUInstrumentOp.so\r\n`', 'created_at': datetime.datetime(2024, 7, 11, 13, 47, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233765653, 'issue_id': 2400655132, 'author': 'tilakrayal', 'body': '@eyalhir74,\r\npython 3.6 is not compatible with the latest tensorflow 2.16. Could you please try with the python 3.9-3.12 and the latest stable tensorflow v2.17.0 and update the same. Thank you!', 'created_at': datetime.datetime(2024, 7, 17, 16, 55, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249208912, 'issue_id': 2400655132, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 25, 1, 52, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799749, 'issue_id': 2400655132, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799881, 'issue_id': 2400655132, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71568"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71568"">No</a>', 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 55, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-11 12:22:51 UTC): @eyalhir74,
Can you check if your op file is following [this](https://github.com/tensorflow/custom-op#template-overview) folder structure under respective kernels.
Also did you try building the sample custom op zero_out_ops mentioned in the above link.

https://github.com/tensorflow/tensorflow/issues/55148

Thank you!

eyalhir74 (Issue Creator) on (2024-07-11 13:47:35 UTC): Hi @tilakrayal ,
  So seems like the zero_out_ops is somewhat out of date as far as I can understand..
  I'm working with Python 3.6 and a NVIDIA docker (nvcr.io/nvidia/tritonserver:24.05-py3)

  I had to use ""return Status();"" instead of ""return Status::OK();""
  
  Also if someone stumbles on this and get the same/similar issues, this is the makefile I've used to compile it all

`
TF_CFLAGS=-I/usr/local/lib/python3.10/dist-packages/tensorflow/include
TF_LFLAGS=-L/usr/local/lib/python3.10/dist-packages/tensorflow -l:libtensorflow_framework.so.2
CUDA_FLAGS=--expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90

CFLAGS=-m64 -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64 -O2
NVCC_CFLAGS=--compiler-options ""-fPIC -pthread""

all: build

build: GPUInstrumentOp

GPUInstrumentOp.o:GPUInstrumentOp.cc
	nvcc -ccbin g++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} -o GPUInstrumentOp.o -c GPUInstrumentOp.cc

GPUInstrumentKernels.o: GPUInstrumentKernels.cu
	nvcc -ccbin g++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} ${CUDA_FLAGS} -o GPUInstrumentKernels.o -c GPUInstrumentKernels.cu

GPUInstrumentOp: GPUInstrumentOp.o GPUInstrumentKernels.o
	g++ ${CFLAGS} -Wl,--no-as-needed ${TF_CFLAGS} ${TF_LFLAGS} --shared  -o GPUInstrument.so GPUInstrumentOp.o GPUInstrumentKernels.o

clean:
	rm -f GPUInstrumentOp.o GPUInstrumentKernels.o GPUInstrumentOp.so
`

tilakrayal (Assginee) on (2024-07-17 16:55:14 UTC): @eyalhir74,
python 3.6 is not compatible with the latest tensorflow 2.16. Could you please try with the python 3.9-3.12 and the latest stable tensorflow v2.17.0 and update the same. Thank you!

github-actions[bot] on (2024-07-25 01:52:59 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-01 01:57:50 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-01 01:57:55 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71568"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71568"">No</a>

"
2400562400,issue,closed,completed,"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. #62097","**System information**
- MacOS Sonoma 14.5:
- TensorFlow installed from https://github.com/tensorflow/tensorflow.git:


**Error output from tflite model in the Flutter Application**

Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select

**Standalone code to reproduce the issue** 
Link to Google Colab: [Expenses.ipynb](https://colab.research.google.com/drive/1n-cALnogLn4fblDQYaXzNA05XWihkv1e?usp=sharing)

**Any other info / logs**

I am using a TensorflowLite Ops model in Flutter application and whenever I am trying to use it the interpreter is returning a null value, I have tried creating an .aar file of the same using bazel and also the flex delegate using the code https://github.com/tensorflow/tensorflow/issues/62097#issuecomment-1767622437 mentioned here: 
`bazel build -c opt --config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex `
and created the aar file using the code : 

```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```
Then I installed it to my local Maven and used the mavelLocal() repository:
```
repositories {
        google()
        mavenCentral()
        maven {         // add this repo to use snapshots
            name 'ossrh-snapshot'
            url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
        mavenLocal()
    }
```
and also the dependency: `implementation 'org.tensorflow:tensorflow-lite:0.1.100'`
After all this I am still getting the same response from the application. Kindly look into it.",risheek-mittal,2024-07-10 11:55:05+00:00,"['gaikwadrahul8', 'sawantkumar']",2024-09-20 02:00:17+00:00,2024-09-20 02:00:14+00:00,https://github.com/tensorflow/tensorflow/issues/71564,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2224486164, 'issue_id': 2400562400, 'author': 'sushreebarsa', 'body': '@risheek-mittal Could you please let us know the TF version you are using here.  We are unable to access your colab link so could you share the colab gist with us which will help to analyze the issue in a better way?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 12, 4, 11, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2224895032, 'issue_id': 2400562400, 'author': 'risheek-mittal', 'body': ""I am using  \r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1'\r\nimplementation 'org.tensorflow:tensorflow-lite:2.16.1'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.12.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\r\n```\r\nPlease try again with this link [Expense Colab](https://colab.research.google.com/drive/1n-cALnogLn4fblDQYaXzNA05XWihkv1e?usp=sharing)"", 'created_at': datetime.datetime(2024, 7, 12, 6, 35, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235627556, 'issue_id': 2400562400, 'author': 'risheek-mittal', 'body': 'Hey, there has been no updates on this ticket lately. May I know the progress for this?', 'created_at': datetime.datetime(2024, 7, 18, 5, 55, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245710716, 'issue_id': 2400562400, 'author': 'sawantkumar', 'body': ""Hi @risheek-mittal ,\r\n\r\nCan you use the below dependencies and let me know if it works? I just changed the version of tflite-gpu to 2.16.1 to match the other two dependencies .\r\n\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1'\r\nimplementation 'org.tensorflow:tensorflow-lite:2.16.1'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.16.1'\r\nimplementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\r\n```"", 'created_at': datetime.datetime(2024, 7, 23, 16, 32, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249443077, 'issue_id': 2400562400, 'author': 'risheek-mittal', 'body': 'Yeah hi @sawantkumar I tried it but it is still giving me the same response\r\n`Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select`', 'created_at': datetime.datetime(2024, 7, 25, 5, 7, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331641729, 'issue_id': 2400562400, 'author': 'gaikwadrahul8', 'body': 'Hi, @risheek-mittal\r\n\r\nCould you please give it try with below mentioned command and see is it working as expected or not ? if issue still persists please let us know with error log for further investigation ? Thank you.\r\n\r\n`bazel build -c opt --config=monolithic --config=android_arm64 tensorflow/lite/delegates/flex:tensorflowlite_flex`', 'created_at': datetime.datetime(2024, 9, 5, 13, 9, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347894391, 'issue_id': 2400562400, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 13, 1, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362558090, 'issue_id': 2400562400, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 20, 2, 0, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362558132, 'issue_id': 2400562400, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71564"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71564"">No</a>', 'created_at': datetime.datetime(2024, 9, 20, 2, 0, 16, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-12 04:11:05 UTC): @risheek-mittal Could you please let us know the TF version you are using here.  We are unable to access your colab link so could you share the colab gist with us which will help to analyze the issue in a better way?
Thank you!

risheek-mittal (Issue Creator) on (2024-07-12 06:35:11 UTC): I am using  
```
implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1'
implementation 'org.tensorflow:tensorflow-lite:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-gpu:2.12.0'
implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'
```
Please try again with this link [Expense Colab](https://colab.research.google.com/drive/1n-cALnogLn4fblDQYaXzNA05XWihkv1e?usp=sharing)

risheek-mittal (Issue Creator) on (2024-07-18 05:55:22 UTC): Hey, there has been no updates on this ticket lately. May I know the progress for this?

sawantkumar (Assginee) on (2024-07-23 16:32:08 UTC): Hi @risheek-mittal ,

Can you use the below dependencies and let me know if it works? I just changed the version of tflite-gpu to 2.16.1 to match the other two dependencies .

```
implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.16.1'
implementation 'org.tensorflow:tensorflow-lite:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-gpu:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'
```

risheek-mittal (Issue Creator) on (2024-07-25 05:07:31 UTC): Yeah hi @sawantkumar I tried it but it is still giving me the same response
`Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select`

gaikwadrahul8 (Assginee) on (2024-09-05 13:09:24 UTC): Hi, @risheek-mittal

Could you please give it try with below mentioned command and see is it working as expected or not ? if issue still persists please let us know with error log for further investigation ? Thank you.

`bazel build -c opt --config=monolithic --config=android_arm64 tensorflow/lite/delegates/flex:tensorflowlite_flex`

github-actions[bot] on (2024-09-13 01:59:00 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-20 02:00:13 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-20 02:00:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71564"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71564"">No</a>

"
2399651982,issue,closed,completed,Tensorflow Lite: Using a custom GPU external delegate in conjunction with the official GPU delegate,"Hi,

we would like to implement some custom op running on GPU in a custom external delegate. If we use this delegate with the official GPU delegate as fallback for the standard ops, can we expect that the OpenCL buffers will be used between both delegates, or will the data go back and forth between GPU - CPU - GPU ? (which is what I would expect looking at the  delegate API).",BGigotSDS,2024-07-10 03:11:05+00:00,['tilakrayal'],2024-07-15 05:07:17+00:00,2024-07-15 04:53:49+00:00,https://github.com/tensorflow/tensorflow/issues/71362,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue')]","[{'comment_id': 2222595559, 'issue_id': 2399651982, 'author': 'tilakrayal', 'body': '@BGigotSDS,\r\n\r\nA custom delegate can create and manage its own OpenCL buffers for performing operations the same as a GPU delegate. However, the tensorflow lite delegate API does not provide a direct mechanism for delegates to share GPU buffers directly. Obviously, the data in GPU buffers need to be transferred back to the CPU before handing off to the other delegate. \r\n\r\nBut the data transfer hence forth between GPU and CPU might lead to performance slowdown while inference. Here is the implementation of custom delegate [reference](https://www.tensorflow.org/lite/performance/implementing_delegate).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 11, 10, 38, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227671842, 'issue_id': 2399651982, 'author': 'BGigotSDS', 'body': ""Thank you for your answer, I was expecting this, but wanted to be sure I didn't miss something. Thanks again for taking the time to answer."", 'created_at': datetime.datetime(2024, 7, 15, 4, 13, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227689777, 'issue_id': 2399651982, 'author': 'tilakrayal', 'body': '@BGigotSDS,\r\nCould you please feel free to move this issue to closed status in that case. Thank you!', 'created_at': datetime.datetime(2024, 7, 15, 4, 41, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227698535, 'issue_id': 2399651982, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71362"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71362"">No</a>', 'created_at': datetime.datetime(2024, 7, 15, 4, 53, 51, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-11 10:38:09 UTC): @BGigotSDS,

A custom delegate can create and manage its own OpenCL buffers for performing operations the same as a GPU delegate. However, the tensorflow lite delegate API does not provide a direct mechanism for delegates to share GPU buffers directly. Obviously, the data in GPU buffers need to be transferred back to the CPU before handing off to the other delegate. 

But the data transfer hence forth between GPU and CPU might lead to performance slowdown while inference. Here is the implementation of custom delegate [reference](https://www.tensorflow.org/lite/performance/implementing_delegate).

Thank you!

BGigotSDS (Issue Creator) on (2024-07-15 04:13:55 UTC): Thank you for your answer, I was expecting this, but wanted to be sure I didn't miss something. Thanks again for taking the time to answer.

tilakrayal (Assginee) on (2024-07-15 04:41:20 UTC): @BGigotSDS,
Could you please feel free to move this issue to closed status in that case. Thank you!

google-ml-butler[bot] on (2024-07-15 04:53:51 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71362"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71362"">No</a>

"
2398795391,issue,closed,completed,tf.data.Dataset.from_generator: TypeError: '>' not supported between instances of 'NoneType' and 'int',"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TF dataset created from numpy array and tensorf slices works fine while TF dataset created from generator passes Nonetype object to custom metric function and results in following error -

`    TypeError: '>' not supported between instances of 'NoneType' and 'int'`


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Input, Conv2D
from tensorflow.keras.models import Model
from tensorflow.keras.metrics import MeanIoU

np.random.seed(1)

NUM_OF_CLASSES = 3

NUM_SAMPLES = 1000
BATCH_SIZE = 32
IMAGE_SIZE = (224, 224)

# Example dataset generation
input_images = np.random.rand(NUM_SAMPLES, IMAGE_SIZE[0], IMAGE_SIZE[1], 3).astype(np.float32)
masks = np.random.randint(0, NUM_OF_CLASSES, size=(NUM_SAMPLES, IMAGE_SIZE[0], IMAGE_SIZE[1]), dtype=np.int32)

class TestMeanIou(MeanIoU):
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        return super().update_state(y_true, tf.argmax(y_pred, axis=-1), sample_weight)

# Example dataset generation function
def generate_dataset(num_samples):
    for i in range(num_samples):
        input_image = input_images[i]
        mask = masks[i]

        yield input_image, mask

def from_generator():
    print(""\ntf.data.Dataset.from_generator"")

    dataset = tf.data.Dataset.from_generator(generate_dataset, args=[NUM_SAMPLES],
                    output_types=(tf.float32, tf.int32, tf.float32))
    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

    return dataset

def process_data(input_image, mask):
    # example process function
    return input_image, mask

def from_numpy_array():
    print(""\ntf.data.Dataset.from_tensor_slices"")

    dataset = tf.data.Dataset.from_tensor_slices((input_images, masks))
    dataset = dataset.map(process_data)
    dataset = dataset.shuffle(NUM_SAMPLES).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

    return dataset

if __name__=='__main__':
    input_image = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
    output = Conv2D(NUM_OF_CLASSES, 1, activation='softmax', name='output')(input_image)

    model = Model(inputs=input_image, outputs=[output])
    model.compile(optimizer='adam',
                  loss={'output': 'sparse_categorical_crossentropy'},
                  metrics={'output': [TestMeanIou(num_classes=NUM_OF_CLASSES)]})
    model.summary()


    '''
    tf.data.Dataset.from_tensor_slices
    This works
    '''
    dataset = from_numpy_array()

    model.fit(dataset, epochs=2)    # train


    '''
    tf.data.Dataset.from_generator
    This results in following error -

    File ""./venv/lib/python3.8/site-packages/keras/src/metrics/iou_metrics.py"", line 123, in update_state  **
        if y_true.shape.ndims > 1:

    TypeError: '>' not supported between instances of 'NoneType' and 'int'
    '''
    # get tf dataset
    dataset = from_generator()

    model.fit(dataset, epochs=2)    # train
```


### Relevant log output

```shell
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 output (Conv2D)             (None, 224, 224, 3)       12        
                                                                 
=================================================================
Total params: 12 (48.00 Byte)
Trainable params: 12 (48.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

tf.data.Dataset.from_tensor_slices
Epoch 1/2
32/32 [==============================] - 2s 43ms/step - loss: 1.1880 - test_mean_iou: 0.1375
Epoch 2/2
32/32 [==============================] - 1s 42ms/step - loss: 1.1675 - test_mean_iou: 0.1479

tf.data.Dataset.from_generator
Epoch 1/2
Traceback (most recent call last):
  File ""tf_dataset_test.py"", line 86, in <module>
    model.fit(dataset, epochs=2)    # train
  File ""./venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/7993776.1.aib4.q/__autograph_generated_filevzr2jken.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
  File ""/tmp/7993776.1.aib4.q/__autograph_generated_fileqrp0encq.py"", line 14, in tf__update_state
    retval_ = ag__.converted_call(ag__.converted_call(ag__.ld(super), (), None, fscope).update_state, (ag__.ld(y_true), ag__.converted_call(ag__.ld(tf).argmax, (ag__.ld(y_pred),), dict(axis=(- 1)), fscope), ag__.ld(sample_weight)), None, fscope)
TypeError: in user code:

    File ""./venv/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""tf_dataset_test.py"", line 22, in update_state  *
        return super().update_state(y_true, tf.argmax(y_pred, axis=-1), sample_weight)
    File ""./venv/lib/python3.8/site-packages/keras/src/metrics/iou_metrics.py"", line 123, in update_state  **
        if y_true.shape.ndims > 1:

    TypeError: '>' not supported between instances of 'NoneType' and 'int'
```
",dauso,2024-07-09 17:42:46+00:00,['tilakrayal'],2024-08-13 09:40:05+00:00,2024-07-11 15:52:24+00:00,https://github.com/tensorflow/tensorflow/issues/71330,"[('type:support', 'Support issues'), ('comp:data', 'tf.data related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2222018303, 'issue_id': 2398795391, 'author': 'tilakrayal', 'body': '@dauso,\r\nYou should use `sample_weight` instead. `class_weight` is not supported for 3+ dimensional targets because the concept of class is ambiguous in that case.\r\n\r\nAlso please try to take a look at the similar issues for the reference.\r\nhttps://github.com/tensorflow/tensorflow/issues/47032\r\nhttps://github.com/keras-team/keras/issues/3653\r\nhttps://github.com/tensorflow/tensorflow/issues/43248\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 11, 4, 43, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2223268750, 'issue_id': 2398795391, 'author': 'dauso', 'body': '- This is not a weights issue, right? There are no weights provided, only input image and mask arrays. The error is about **y_true** in the IoU metric function.\r\n- The example code given above also shows 2D arrays, not 3D. If you change input image images from 3 channel to a grayscale, we still get the same behavior.\r\n\r\nChanging the IoU metric function above to print arguments shows the difference in `y_true`.\r\n\r\n```\r\nclass TestMeanIou(MeanIoU):\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        print(""y_true"", type(y_true), y_true)\r\n        print(""y_pred"", type(y_pred), y_pred)\r\n        print(""sample_weight"", type(sample_weight), sample_weight)\r\n\r\n        y_true = tf.cast(y_true, tf.int32)\r\n        return super().update_state(y_true, tf.argmax(y_pred, axis=-1), sample_weight)\r\n ```\r\n \r\n**Output logs:**\r\n\r\nNumpy (`tf.data.Dataset.from_tensor_slices`) -\r\n**y_true: Tensor(""IteratorGetNext:1"", shape=(None, 224, 224), dtype=int32)**\r\n```\r\ny_true <class \'tensorflow.python.framework.ops.Tensor\'> Tensor(""IteratorGetNext:1"", shape=(None, 224, 224), dtype=int32)\r\ny_pred <class \'tensorflow.python.framework.ops.Tensor\'> Tensor(""model/output/Softmax:0"", shape=(None, 224, 224, 3), dtype=float32)\r\nsample_weight <class \'NoneType\'> None\r\n```\r\n\r\nGenerator (`tf.data.Dataset.from_generator`) -\r\n**y_true: Tensor(""IteratorGetNext:1"", dtype=int32)**\r\n```\r\ny_true <class \'tensorflow.python.framework.ops.Tensor\'> Tensor(""IteratorGetNext:1"", dtype=int32)\r\ny_pred <class \'tensorflow.python.framework.ops.Tensor\'> Tensor(""model/output/Softmax:0"", shape=(None, None, None, 3), dtype=float32)\r\nsample_weight <class \'NoneType\'> None\r\n```', 'created_at': datetime.datetime(2024, 7, 11, 15, 37, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2223301341, 'issue_id': 2398795391, 'author': 'dauso', 'body': 'Based on [@yufengma reply](https://github.com/tensorflow/tensorflow/issues/43248#issuecomment-804594808) in #43248, adding `output_shapes` fixed the issue.\r\n\r\n```\r\ndef from_generator():\r\n    print(""\\ntf.data.Dataset.from_generator"")\r\n\r\n    dataset = tf.data.Dataset.from_generator(generate_dataset, args=[NUM_SAMPLES],\r\n                    output_types=(tf.float32, tf.int32),\r\n                    output_shapes=((IMAGE_SIZE[0], IMAGE_SIZE[1], 3), (IMAGE_SIZE[0], IMAGE_SIZE[1]))\r\n                    )\r\n    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\r\n```', 'created_at': datetime.datetime(2024, 7, 11, 15, 52, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2223301399, 'issue_id': 2398795391, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71330"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71330"">No</a>', 'created_at': datetime.datetime(2024, 7, 11, 15, 52, 26, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-11 04:43:29 UTC): @dauso,
You should use `sample_weight` instead. `class_weight` is not supported for 3+ dimensional targets because the concept of class is ambiguous in that case.

Also please try to take a look at the similar issues for the reference.
https://github.com/tensorflow/tensorflow/issues/47032
https://github.com/keras-team/keras/issues/3653
https://github.com/tensorflow/tensorflow/issues/43248

Thank you!

dauso (Issue Creator) on (2024-07-11 15:37:31 UTC): - This is not a weights issue, right? There are no weights provided, only input image and mask arrays. The error is about **y_true** in the IoU metric function.
- The example code given above also shows 2D arrays, not 3D. If you change input image images from 3 channel to a grayscale, we still get the same behavior.

Changing the IoU metric function above to print arguments shows the difference in `y_true`.

```
class TestMeanIou(MeanIoU):
    def update_state(self, y_true, y_pred, sample_weight=None):
        print(""y_true"", type(y_true), y_true)
        print(""y_pred"", type(y_pred), y_pred)
        print(""sample_weight"", type(sample_weight), sample_weight)

        y_true = tf.cast(y_true, tf.int32)
        return super().update_state(y_true, tf.argmax(y_pred, axis=-1), sample_weight)
 ```
 
**Output logs:**

Numpy (`tf.data.Dataset.from_tensor_slices`) -
**y_true: Tensor(""IteratorGetNext:1"", shape=(None, 224, 224), dtype=int32)**
```
y_true <class 'tensorflow.python.framework.ops.Tensor'> Tensor(""IteratorGetNext:1"", shape=(None, 224, 224), dtype=int32)
y_pred <class 'tensorflow.python.framework.ops.Tensor'> Tensor(""model/output/Softmax:0"", shape=(None, 224, 224, 3), dtype=float32)
sample_weight <class 'NoneType'> None
```

Generator (`tf.data.Dataset.from_generator`) -
**y_true: Tensor(""IteratorGetNext:1"", dtype=int32)**
```
y_true <class 'tensorflow.python.framework.ops.Tensor'> Tensor(""IteratorGetNext:1"", dtype=int32)
y_pred <class 'tensorflow.python.framework.ops.Tensor'> Tensor(""model/output/Softmax:0"", shape=(None, None, None, 3), dtype=float32)
sample_weight <class 'NoneType'> None
```

dauso (Issue Creator) on (2024-07-11 15:52:24 UTC): Based on [@yufengma reply](https://github.com/tensorflow/tensorflow/issues/43248#issuecomment-804594808) in #43248, adding `output_shapes` fixed the issue.

```
def from_generator():
    print(""\ntf.data.Dataset.from_generator"")

    dataset = tf.data.Dataset.from_generator(generate_dataset, args=[NUM_SAMPLES],
                    output_types=(tf.float32, tf.int32),
                    output_shapes=((IMAGE_SIZE[0], IMAGE_SIZE[1], 3), (IMAGE_SIZE[0], IMAGE_SIZE[1]))
                    )
    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)
```

google-ml-butler[bot] on (2024-07-11 15:52:26 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71330"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71330"">No</a>

"
2397722015,issue,closed,completed,Missing C headers in AAR,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using the approach described here to integrate C Api for tensorflow lite: https://www.tensorflow.org/lite/android/development#tools_for_building_with_c_and_c

According to the description I should do the following:

> Using this API is the recommended approach for developers using the NDK. Download the TensorFlow Lite AAR hosted at MavenCentral file, rename to tensorflow-lite-*.zip, and unzip it. You must include the four header files in the headers/tensorflow/lite/ and headers/tensorflow/lite/c/ folders and the relevant libtensorflowlite_jni.so dynamic library in the jni/ folder in your NDK project.
> 
> The c_api.h header file contains basic documentation about using the TFLite C API.

This approach works for previous versions of TFLite. When I try to upgrade to a v2.16.1 I get the following error:

```
In file included from ***/tflite/include/tensorflow/lite/c/c_api.h:24:
***/tflite/include/tensorflow/lite/core/c/c_api.h:29:10: fatal error: 'tensorflow/lite/core/async/c/types.h' file not found
```

The AAR is missing the following two headers:
""tensorflow/lite/core/async/c/types.h""
""tensorflow/lite/core/c/registration_external.h""

After manually adding these 2 headers, I was able to compile my project.

### Standalone code to reproduce the issue

```shell
Follow the guidelines described in the TFLite C API and use the 2.16.1 version AAR from Maven Central.
```


### Relevant log output

_No response_",guillaume-tgl,2024-07-09 09:57:38+00:00,"['pkgoogle', 'sawantkumar']",2024-07-18 18:25:36+00:00,2024-07-18 07:20:58+00:00,https://github.com/tensorflow/tensorflow/issues/71184,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('Android', ''), ('TF 2.16', '')]","[{'comment_id': 2226208205, 'issue_id': 2397722015, 'author': 'pkgoogle', 'body': 'Hi @guillaume-tgl, in the documentation it states: \r\n> You must include the four header files in the `headers/tensorflow/lite/`\r\n\r\nour interpretation of this is that you would manually add all these headers. So I believe it is working as intended. Thanks.', 'created_at': datetime.datetime(2024, 7, 12, 19, 10, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227856558, 'issue_id': 2397722015, 'author': 'guillaume-tgl', 'body': ""> our interpretation of this is that you would manually add all these headers. So I believe it is working as intended. Thanks.\r\n\r\nSorry, maybe my description was not clear enough. The headers embedded in the AAR are not enough to be able to compile a project depending on TFLite. I had to copy the missing headers directly from the Tensorflow repository for my project to compile.\r\nA similar issue was reported a few months ago and it was resolved by including the missing headers in the AAR: https://github.com/tensorflow/tensorflow/issues/59026\r\nDon't you think a similar solution can be implemented?"", 'created_at': datetime.datetime(2024, 7, 15, 7, 27, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229428592, 'issue_id': 2397722015, 'author': 'pkgoogle', 'body': 'Hi @guillaume-tgl, apologies for the misunderstanding, so if I\'m understanding correctly there are more than the 4 header files documented needed to use the C API now and those 2 files are: \r\n> ""tensorflow/lite/core/async/c/types.h""\r\n> ""tensorflow/lite/core/c/registration_external.h""\r\n\r\nWould you say these headers are needed more generally i.e. for most cases or do you feel they are specific for your project use cases? For either... can you provide any example project that shows this so that I may test it (Like a hello world version of what you did)? Thanks.', 'created_at': datetime.datetime(2024, 7, 15, 21, 1, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2230262987, 'issue_id': 2397722015, 'author': 'guillaume-tgl', 'body': '@pkgoogle These two headers are needed in the general case since they are included by one of the 4 public headers: https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/core/c/c_api.h', 'created_at': datetime.datetime(2024, 7, 16, 7, 58, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231879347, 'issue_id': 2397722015, 'author': 'pkgoogle', 'body': 'Hi @guillaume-tgl, master/nightly is actually synchronous for their requirements: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/BUILD#L142, that being said, you are correct branch/release 2.16 is not consistent, let me look into see how I can update that branch. Thanks for your help.', 'created_at': datetime.datetime(2024, 7, 16, 21, 46, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232951550, 'issue_id': 2397722015, 'author': 'Moon4u', 'body': '@guillaume-tgl Did you add the ``TFLITE_IN_GMSCORE`` define? I am assuming that you have not, since this define changes the include paths, ie\r\n```c\r\n#if TFLITE_IN_GMSCORE\r\n#include ""tensorflow/lite/abi/c/c_api_types.h""  // IWYU pragma: export\r\n#else\r\n#include ""tensorflow/lite/core/c/c_api_types.h""  // IWYU pragma: export\r\n#endif\r\n```\r\n\r\nI assume GMS stands for google mobile services, since tensorflow is now part of google play services thingy.\r\n\r\nAlso, you may stumble with the initialization of tensorflow (I sure did). Tensorflow will crash upon any function call and will output a message saying ""You need to initialize Tensorflow with a call to GmsTfLiteInitialize"" or something like that. This function (GmsTfLiteInitialize) expects some weird  \'internal native handle\' of type jobject. Absolutely no idea how to obtain that, or what it is. \r\nThe only solution (that I\'ve found) is to call `TfLiteNative.initialize` in java/kotlin. And the entire TfLiteNative class is not available in 2.16.1. You need to move to 2.16.2-beta02 to have it available. \r\n\r\nThis is the extend of my knowledge, I stumbled upon this same problem last week and this is all I\'ve got. Most of it is mentioned in the [docs](https://www.tensorflow.org/lite/android/native), but there is no explanation for what GMS is or why TfLiteNative is not available in 2.16.1, and how to do thing prior to 2.16.2. Or why is the official documentation advising the use of a beta release...', 'created_at': datetime.datetime(2024, 7, 17, 10, 16, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233078387, 'issue_id': 2397722015, 'author': 'guillaume-tgl', 'body': ""> @guillaume-tgl Did you add the TFLITE_IN_GMSCORE define?\r\n\r\nNo, I don't have this flag enabled. I use the regular way to embed TensorflowLite."", 'created_at': datetime.datetime(2024, 7, 17, 11, 27, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2233080667, 'issue_id': 2397722015, 'author': 'guillaume-tgl', 'body': '> Hi @guillaume-tgl, master/nightly is actually synchronous for their requirements: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/BUILD#L142, that being said, you are correct branch/release 2.16 is not consistent, let me look into see how I can update that branch. Thanks for your help.\r\n\r\nI wanted to try v2.17 to see if the problem is still there but it looks like this version is not present on [Maven](https://central.sonatype.com/artifact/org.tensorflow/tensorflow-lite)  Any idea why?', 'created_at': datetime.datetime(2024, 7, 17, 11, 28, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2234275298, 'issue_id': 2397722015, 'author': 'pkgoogle', 'body': ""Hey All. Apologies @guillaume-tgl, there's no more expected patches for 2.16 and unfortunately this issue is not big enough to warrant a new patch unless there's a security issue or such things. Please use the work around for now until a new version is out. I don't know exactly why v2.17 is not out yet on Maven, I'm guessing there are issues around the release process (maybe a similar issue like this for another platform). Thanks for your help."", 'created_at': datetime.datetime(2024, 7, 17, 21, 0, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235803897, 'issue_id': 2397722015, 'author': 'guillaume-tgl', 'body': ""Thanks.\r\n\r\n> I don't know exactly why v2.17 is not out yet on Maven, I'm guessing there are issues around the release process (maybe a similar issue like this for another platform).\r\n\r\nShould I open another ticket for this?"", 'created_at': datetime.datetime(2024, 7, 18, 7, 20, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235803953, 'issue_id': 2397722015, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71184"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71184"">No</a>', 'created_at': datetime.datetime(2024, 7, 18, 7, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237230093, 'issue_id': 2397722015, 'author': 'pkgoogle', 'body': 'Hi @guillaume-tgl, hmm yeah it seems reasonable, I suppose it will create some pressure to release it -- but largely I think they already know that. It will provide me more data/information to create more pressure for it if we feel there is a large demand.', 'created_at': datetime.datetime(2024, 7, 18, 18, 24, 56, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-12 19:10:34 UTC): Hi @guillaume-tgl, in the documentation it states: 

our interpretation of this is that you would manually add all these headers. So I believe it is working as intended. Thanks.

guillaume-tgl (Issue Creator) on (2024-07-15 07:27:38 UTC): Sorry, maybe my description was not clear enough. The headers embedded in the AAR are not enough to be able to compile a project depending on TFLite. I had to copy the missing headers directly from the Tensorflow repository for my project to compile.
A similar issue was reported a few months ago and it was resolved by including the missing headers in the AAR: https://github.com/tensorflow/tensorflow/issues/59026
Don't you think a similar solution can be implemented?

pkgoogle (Assginee) on (2024-07-15 21:01:53 UTC): Hi @guillaume-tgl, apologies for the misunderstanding, so if I'm understanding correctly there are more than the 4 header files documented needed to use the C API now and those 2 files are: 

Would you say these headers are needed more generally i.e. for most cases or do you feel they are specific for your project use cases? For either... can you provide any example project that shows this so that I may test it (Like a hello world version of what you did)? Thanks.

guillaume-tgl (Issue Creator) on (2024-07-16 07:58:17 UTC): @pkgoogle These two headers are needed in the general case since they are included by one of the 4 public headers: https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/core/c/c_api.h

pkgoogle (Assginee) on (2024-07-16 21:46:11 UTC): Hi @guillaume-tgl, master/nightly is actually synchronous for their requirements: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/BUILD#L142, that being said, you are correct branch/release 2.16 is not consistent, let me look into see how I can update that branch. Thanks for your help.

Moon4u on (2024-07-17 10:16:27 UTC): @guillaume-tgl Did you add the ``TFLITE_IN_GMSCORE`` define? I am assuming that you have not, since this define changes the include paths, ie
```c
#if TFLITE_IN_GMSCORE
#include ""tensorflow/lite/abi/c/c_api_types.h""  // IWYU pragma: export
#else
#include ""tensorflow/lite/core/c/c_api_types.h""  // IWYU pragma: export
#endif
```

I assume GMS stands for google mobile services, since tensorflow is now part of google play services thingy.

Also, you may stumble with the initialization of tensorflow (I sure did). Tensorflow will crash upon any function call and will output a message saying ""You need to initialize Tensorflow with a call to GmsTfLiteInitialize"" or something like that. This function (GmsTfLiteInitialize) expects some weird  'internal native handle' of type jobject. Absolutely no idea how to obtain that, or what it is. 
The only solution (that I've found) is to call `TfLiteNative.initialize` in java/kotlin. And the entire TfLiteNative class is not available in 2.16.1. You need to move to 2.16.2-beta02 to have it available. 

This is the extend of my knowledge, I stumbled upon this same problem last week and this is all I've got. Most of it is mentioned in the [docs](https://www.tensorflow.org/lite/android/native), but there is no explanation for what GMS is or why TfLiteNative is not available in 2.16.1, and how to do thing prior to 2.16.2. Or why is the official documentation advising the use of a beta release...

guillaume-tgl (Issue Creator) on (2024-07-17 11:27:18 UTC): No, I don't have this flag enabled. I use the regular way to embed TensorflowLite.

guillaume-tgl (Issue Creator) on (2024-07-17 11:28:40 UTC): I wanted to try v2.17 to see if the problem is still there but it looks like this version is not present on [Maven](https://central.sonatype.com/artifact/org.tensorflow/tensorflow-lite)  Any idea why?

pkgoogle (Assginee) on (2024-07-17 21:00:43 UTC): Hey All. Apologies @guillaume-tgl, there's no more expected patches for 2.16 and unfortunately this issue is not big enough to warrant a new patch unless there's a security issue or such things. Please use the work around for now until a new version is out. I don't know exactly why v2.17 is not out yet on Maven, I'm guessing there are issues around the release process (maybe a similar issue like this for another platform). Thanks for your help.

guillaume-tgl (Issue Creator) on (2024-07-18 07:20:58 UTC): Thanks.


Should I open another ticket for this?

google-ml-butler[bot] on (2024-07-18 07:21:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71184"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71184"">No</a>

pkgoogle (Assginee) on (2024-07-18 18:24:56 UTC): Hi @guillaume-tgl, hmm yeah it seems reasonable, I suppose it will create some pressure to release it -- but largely I think they already know that. It will provide me more data/information to create more pressure for it if we feel there is a large demand.

"
2397700446,issue,closed,not_planned,TFLite Podspecs haven't been updated after v2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.2

### Custom code

No

### OS platform and distribution

iOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorflowLiteC podspec still uses the 2.14 binary and hasn't been updated since v2.14. It's necessary to be able to integrate the latest versions with the C API for iOS.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_",guillaume-tgl,2024-07-09 09:48:42+00:00,"['yishuangP', 'pkgoogle', 'sawantkumar']",2024-11-26 18:08:48+00:00,2024-11-26 18:08:44+00:00,https://github.com/tensorflow/tensorflow/issues/71180,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2226213388, 'issue_id': 2397700446, 'author': 'pkgoogle', 'body': 'This seems correct: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/TensorFlowLiteC.podspec\r\n\r\n@yishuangP, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 7, 12, 19, 14, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500268856, 'issue_id': 2397700446, 'author': 'gaikwadrahul8', 'body': ""Hi, @guillaume-tgl \r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/47\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 26, 10, 42, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501619086, 'issue_id': 2397700446, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71180"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71180"">No</a>', 'created_at': datetime.datetime(2024, 11, 26, 18, 8, 46, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-12 19:14:05 UTC): This seems correct: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/TensorFlowLiteC.podspec

@yishuangP, can you please take a look? Thanks.

gaikwadrahul8 on (2024-11-26 10:42:18 UTC): Hi, @guillaume-tgl 
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/47

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-26 18:08:46 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71180"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/71180"">No</a>

"
2397330357,issue,closed,completed,"please add sparsemax function, tabnet need this, but hard to code","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

please add sparsemax function

### Standalone code to reproduce the issue

```shell
please add sparsemax function
```


### Relevant log output

```shell
please add sparsemax function
```
",coco-boy,2024-07-09 07:06:10+00:00,['tilakrayal'],2024-07-26 01:52:48+00:00,2024-07-26 01:52:48+00:00,https://github.com/tensorflow/tensorflow/issues/71125,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:ops', 'OPs related issues')]","[{'comment_id': 2219830695, 'issue_id': 2397330357, 'author': 'tilakrayal', 'body': '@coco-boy,\r\nHave you got the chance to have a look at tfa.layers.Sparsemax and tfa.activations.Sparsemax which will be used as layers and the activations. Kindly refer to the official document and let us know if you are looking for the similar feature.\r\n\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/layers/Sparsemax\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/activations/sparsemax\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 10, 8, 7, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2219958310, 'issue_id': 2397330357, 'author': 'coco-boy', 'body': '@tilakrayal\r\n> @coco-boy, Have you got the chance to have a look at tfa.layers.Sparsemax and tfa.activations.Sparsemax which will be used as layers and the activations. Kindly refer to the official document and let us know if you are looking for the similar feature.\r\n> \r\n> https://www.tensorflow.org/addons/api_docs/python/tfa/layers/Sparsemax https://www.tensorflow.org/addons/api_docs/python/tfa/activations/sparsemax\r\n> \r\n> Thank you!\r\n\r\ntfa project, This project is deprecated. TensorFlow Addons has stopped development, so hope to add the new feature', 'created_at': datetime.datetime(2024, 7, 10, 9, 3, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2222051671, 'issue_id': 2397330357, 'author': 'tilakrayal', 'body': '@coco-boy,\r\n [Relu Activation Function](https://arxiv.org/pdf/1505.00853.pdf%C3%A3%E2%82%AC%E2%80%9AReLU) seems to be good for handling the problem of Vanishing and Exploding Gradients.\r\n\r\nCan you please let us know how efficient are the activation functions that you proposed compared to the existing non-saturated activation functions like [Relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu), [Elu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu), [Selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)? Thank you!', 'created_at': datetime.datetime(2024, 7, 11, 5, 16, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237879691, 'issue_id': 2397330357, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 19, 1, 52, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251813428, 'issue_id': 2397330357, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 26, 1, 52, 47, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-10 08:07:04 UTC): @coco-boy,
Have you got the chance to have a look at tfa.layers.Sparsemax and tfa.activations.Sparsemax which will be used as layers and the activations. Kindly refer to the official document and let us know if you are looking for the similar feature.

https://www.tensorflow.org/addons/api_docs/python/tfa/layers/Sparsemax
https://www.tensorflow.org/addons/api_docs/python/tfa/activations/sparsemax

Thank you!

coco-boy (Issue Creator) on (2024-07-10 09:03:56 UTC): @tilakrayal

tfa project, This project is deprecated. TensorFlow Addons has stopped development, so hope to add the new feature

tilakrayal (Assginee) on (2024-07-11 05:16:18 UTC): @coco-boy,
 [Relu Activation Function](https://arxiv.org/pdf/1505.00853.pdf%C3%A3%E2%82%AC%E2%80%9AReLU) seems to be good for handling the problem of Vanishing and Exploding Gradients.

Can you please let us know how efficient are the activation functions that you proposed compared to the existing non-saturated activation functions like [Relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu), [Elu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu), [Selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)? Thank you!

github-actions[bot] on (2024-07-19 01:52:24 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-26 01:52:47 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2396950967,issue,closed,completed,tflite input setting after post-training quantization,"### 1. System information

- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): tensorflow 2.13
- TensorFlow library (version, if pip package or github SHA, if built from source): n/a

### 2. Code
I have a question on how tflite input works for post-training-quantized model (INT8).
I made a tflite wrapper class as below:

```
class tflite_instance:
    def __init__(self, model_name, qat=True):
        self.interpreter = tf.lite.Interpreter(model_path=os.path.join(TFLITE_MODELS_DIR, model_name + '.tflite'))
        self.interpreter.allocate_tensors()
        self.tflite_input_details = self.interpreter.get_input_details()
        self.tflite_output_details = self.interpreter.get_output_details()

    def inference(self, x, num_out=1):
        input_details = self.tflite_input_details[0]
        tensor_index = input_details['index']
        input_tensor = self.interpreter.tensor(tensor_index)()
        scale, zero_point = input_details['quantization']        
        quantized_input = np.uint8(x / scale + zero_point)                    
        input_tensor = quantized_input

    
        self.interpreter.invoke()
        
        output = self.interpreter.get_tensor(self.tflite_output_details[0]['index'])
                   
        scale, zero_point = self.tflite_output_details[0]['quantization']    
        output = scale * (output.astype(np.float32) - zero_point)                    
        
        return output
```

This code runs, but it looks like tflite cannot accept the correct input value.
I imitated a sample code from the official document and modified my wrapper as below and now it works fine. But I do not correctly understand how tflite is working behind the scene, because for me both code look same. Especially, Is there any magic behind this line of code? `input_tensor[:, :, :, :] = quantized_input`

```
class tflite_instance:
    def __init__(self, model_name, qat=True):
        self.interpreter = tf.lite.Interpreter(model_path=os.path.join(TFLITE_MODELS_DIR, model_name + '.tflite'))
        self.interpreter.allocate_tensors()
        self.tflite_input_details = self.interpreter.get_input_details()
        self.tflite_output_details = self.interpreter.get_output_details()
    

    def set_input_tensor(self, x):
        input_details = self.interpreter.get_input_details()[0]
        tensor_index = input_details['index']
        input_tensor = self.interpreter.tensor(tensor_index)()
        # Inputs for the TFLite model must be uint8, so we quantize our input data.
        scale, zero_point = input_details['quantization']
        quantized_input = np.uint8(x / scale + zero_point)
        input_tensor[:, :, :, :] = quantized_input

    def inference(self, x, num_out=1):

        self.set_input_tensor(x)
    
        self.interpreter.invoke()
        
        output = self.interpreter.get_tensor(self.tflite_output_details[0]['index'])
                   
        scale, zero_point = self.tflite_output_details[0]['quantization']    
        output = scale * (output.astype(np.float32) - zero_point)                    
        
        return output
```


### 3. Failure after conversion
The code works fine, but cannot understand the behavior. Some explanation would be grateful.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",KeondoPark,2024-07-09 02:30:19+00:00,['sawantkumar'],2024-07-19 01:59:33+00:00,2024-07-19 01:59:33+00:00,https://github.com/tensorflow/tensorflow/issues/71074,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('comp:lite', 'TF Lite related issues'), ('TFLiteConverter', 'For issues related to TFLite converter')]","[{'comment_id': 2227751087, 'issue_id': 2396950967, 'author': 'sawantkumar', 'body': 'Hi @KeondoPark ,\r\n\r\nIn this code, you\'re assigning quantized_input to input_tensor. However, input_tensor is just a reference to the numpy array returned by self.interpreter.tensor(tensor_index)(). By assigning quantized_input directly to input_tensor, you\'re changing the reference rather than updating the actual memory of the tensor that the TFLite interpreter uses for input.\r\nWhen you use slicing [:, :, :, :] = quantized_input, you\'re modifying the existing buffer in place, ensuring the interpreter uses the updated values. It uses NumPy\'s indexing to modify the entire content of the input tensor in-place. The [:, :, :, :] notation means ""select all elements in all dimensions,"" effectively updating the entire tensor with the new quantized data.\r\nLet me know if you have any problems understanding this explanation.', 'created_at': datetime.datetime(2024, 7, 15, 6, 0, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229752917, 'issue_id': 2396950967, 'author': 'KeondoPark', 'body': 'Hi @sawantkumar \r\nThank you for your response. To my understanding, the `input_tensor` is something like a pointer to the input buffer.\r\nI have two following questions:\r\n1. If `input_tensor` is a pointer to the input buffer, its value should be memory address. When I assigned `quantized_input` directly to `input_tensor`, why did it run without error? `quantized_input` just happened to be a valid memory address?\r\n2. Other than using slicing, can I use different syntax? For example, changing `input_tensor` to reference to `quantized_input`, instead of changing the input buffer in-place (I assume the contents are copied).\r\n\r\nThank you for your kind response!', 'created_at': datetime.datetime(2024, 7, 16, 0, 43, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231241645, 'issue_id': 2396950967, 'author': 'sawantkumar', 'body': ""Hi @KeondoPark ,\r\n\r\n The TFLite interpreter has pre-allocated memory for its tensors. The original input_tensor is a view into this pre-allocated memory. When you reassign input_tensor, you are not changing the contents of that pre-allocated memory, you are just changing what the Python variable input_tensor refers to. \r\nThis didn't raise an error because you're not actually modifying the memory that TFLite is using. Instead, you're just reassigning the local variable input_tensor to point to the new quantized_input array. This operation is valid in Python, but it doesn't affect the TFLite interpreter's internal memory. That's why your model didn't work correctly, even though there was no error. Let me know if you have any further questions."", 'created_at': datetime.datetime(2024, 7, 16, 15, 32, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2234780327, 'issue_id': 2396950967, 'author': 'KeondoPark', 'body': '@sawantkumar \r\nI understand. Thank you!', 'created_at': datetime.datetime(2024, 7, 17, 23, 51, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2235598028, 'issue_id': 2396950967, 'author': 'sawantkumar', 'body': 'Hi @KeondoPark ,\r\n\r\nIf you think your issue is resolved , can you please close this issue ?', 'created_at': datetime.datetime(2024, 7, 18, 5, 51, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237916714, 'issue_id': 2396950967, 'author': 'KeondoPark', 'body': 'Ok I will close.', 'created_at': datetime.datetime(2024, 7, 19, 1, 59, 33, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-07-15 06:00:10 UTC): Hi @KeondoPark ,

In this code, you're assigning quantized_input to input_tensor. However, input_tensor is just a reference to the numpy array returned by self.interpreter.tensor(tensor_index)(). By assigning quantized_input directly to input_tensor, you're changing the reference rather than updating the actual memory of the tensor that the TFLite interpreter uses for input.
When you use slicing [:, :, :, :] = quantized_input, you're modifying the existing buffer in place, ensuring the interpreter uses the updated values. It uses NumPy's indexing to modify the entire content of the input tensor in-place. The [:, :, :, :] notation means ""select all elements in all dimensions,"" effectively updating the entire tensor with the new quantized data.
Let me know if you have any problems understanding this explanation.

KeondoPark (Issue Creator) on (2024-07-16 00:43:26 UTC): Hi @sawantkumar 
Thank you for your response. To my understanding, the `input_tensor` is something like a pointer to the input buffer.
I have two following questions:
1. If `input_tensor` is a pointer to the input buffer, its value should be memory address. When I assigned `quantized_input` directly to `input_tensor`, why did it run without error? `quantized_input` just happened to be a valid memory address?
2. Other than using slicing, can I use different syntax? For example, changing `input_tensor` to reference to `quantized_input`, instead of changing the input buffer in-place (I assume the contents are copied).

Thank you for your kind response!

sawantkumar (Assginee) on (2024-07-16 15:32:02 UTC): Hi @KeondoPark ,

 The TFLite interpreter has pre-allocated memory for its tensors. The original input_tensor is a view into this pre-allocated memory. When you reassign input_tensor, you are not changing the contents of that pre-allocated memory, you are just changing what the Python variable input_tensor refers to. 
This didn't raise an error because you're not actually modifying the memory that TFLite is using. Instead, you're just reassigning the local variable input_tensor to point to the new quantized_input array. This operation is valid in Python, but it doesn't affect the TFLite interpreter's internal memory. That's why your model didn't work correctly, even though there was no error. Let me know if you have any further questions.

KeondoPark (Issue Creator) on (2024-07-17 23:51:49 UTC): @sawantkumar 
I understand. Thank you!

sawantkumar (Assginee) on (2024-07-18 05:51:28 UTC): Hi @KeondoPark ,

If you think your issue is resolved , can you please close this issue ?

KeondoPark (Issue Creator) on (2024-07-19 01:59:33 UTC): Ok I will close.

"
2394147267,issue,closed,completed,tensorflow not getting cuda drivers,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.1

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS (WSL)

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 11.4 

### GPU model and memory

_No response_

### Current behavior?

I had created the WSL setup on my Windows 11 23H2. I had installed Nvidia GPU drivers on Windows and Cuda Toolkit on my WSL (Ubuntu 22.04) as described [here](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)
everythings working fine. the GPU drivers installed on Windows host are successfully got by WSL see the `# CUDA Device Query` logs in the logs section.

when i import tensorflow it say can't found any cuda libraries even though they are on the path.
```
Python 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2024-07-07 22:04:49.005473: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-07 22:04:49.048212: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-07 22:04:49.048305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-07 22:04:49.049681: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-07 22:04:49.056859: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2024-07-07 22:04:49.057206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-07 22:04:50.354998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
>>>
```

# My Setup:
## Tensorflow Installation
1. Install `conda` and create a `env` and install `python=3.11`.
2. Install tensorflow from pip `tensorflow==2.15.1`

## WSL setup for enable GPU in tensorflow
1. Installed GPU drivers for the device `NVIDIA GeForce GTX 660`
2. Installed Cuda Toolkit 11.8 from [here](https://developer.nvidia.com/cuda-11-8-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local). I installed Cuda Toolkit 12.2 and it gives me error because Cuda Toolkit 12.2 is not compatible with my GPU derivers.
checked the compatiblity from here.
    - [tensorflow comaptibility with cuda](https://www.tensorflow.org/install/source#gpu)
    - [cuda toolkit compatibility with drivers](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#id1)

_EDIT: I don't have any chance to use `tf-nightly`  or any other versions. because I am using **Tensorflow Object Detection API** and it gives lot of errors with latest tensorflow codebases like `AttributeError: module 'keras.layers' has no attribute 'experimental'` . I think the code is outdated in Object Detection API and the only solution to this error is to downgrade the tensorflow version. After spending lot of days i came with this version of tensorflow._

### Standalone code to reproduce the issue

```shell
No Code
```


### Relevant log output

```shell
$ nvidia-smi
Sun Jul  7 21:59:03 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.255      Driver Version: 475.06       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:07:00.0 N/A |                  N/A |
|ERR!    0C    P0    N/A /  N/A |    639MiB /  3072MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

$ /usr/local/cuda-11.8/extras/demo_suite/deviceQuery
/usr/local/cuda-11.8/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""NVIDIA GeForce GTX 660""
  CUDA Driver Version / Runtime Version          11.4 / 11.8
  CUDA Capability Major/Minor version number:    3.0
  Total amount of global memory:                 3072 MBytes (3221225472 bytes)
  ( 5) Multiprocessors, (192) CUDA Cores/MP:     960 CUDA Cores
  GPU Max Clock rate:                            1032 MHz (1.03 GHz)
  Memory Clock rate:                             3004 Mhz
  Memory Bus Width:                              192-bit
  L2 Cache Size:                                 393216 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            No
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 7 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.4, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce GTX 660
Result = PASS


>>> tf.sysconfig.get_build_info()
OrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'), ('cuda_compute_capabilities', ['sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])
>>>
```
",a-sajjad72,2024-07-07 17:35:08+00:00,['Venkat6871'],2025-01-31 02:00:26+00:00,2025-01-31 02:00:22+00:00,https://github.com/tensorflow/tensorflow/issues/70960,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2212611955, 'issue_id': 2394147267, 'author': 'mihaimaruseac', 'body': 'Please use TF 2.17 (RC) or 2.16, as 2.15 is out of support. Also, see #63362', 'created_at': datetime.datetime(2024, 7, 7, 23, 18, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2212673118, 'issue_id': 2394147267, 'author': 'a-sajjad72', 'body': ""> Please use TF 2.17 (RC) or 2.16, as 2.15 is out of support. Also, see #63362\r\n\r\nAs I mentioned earlier, I am trying to use the Object Detection API, it throws error with the latest releases. That's why I downgraded it. I also tested it on Google Colab recently, I also got error there.\r\n`AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental`\r\nI search along the github and the solution suggests that downgrade the TF version.  I have been stucked for many days. I had to submit my project for ML application. Already the deadline is passed. I got and an extension in the submission."", 'created_at': datetime.datetime(2024, 7, 8, 0, 31, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2212676034, 'issue_id': 2394147267, 'author': 'mihaimaruseac', 'body': 'That makes sense. For Keras error, please look at `pip list` and make sure you are using the same as TF (until TF 2.16, after that note that Keras multiplatform and TF-Keras are 2 separate packages and only one is supposed to be tied with TF -- I have to learn exactly which and what)', 'created_at': datetime.datetime(2024, 7, 8, 0, 33, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2220998285, 'issue_id': 2394147267, 'author': 'a-sajjad72', 'body': ""keras 3.4.1\r\ntf_keras==2.16.0\r\ntensorflow-2.16.2\r\n\r\n@mihaimaruseac i have these versions of libraries installed in my conda environment with `python 3.11.9`. I didn't installed any of these manually. all of the libraries are installed by the `object_detection` library while installation."", 'created_at': datetime.datetime(2024, 7, 10, 16, 43, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2221629084, 'issue_id': 2394147267, 'author': 'mihaimaruseac', 'body': 'You have keras 2.16 and TF 2.15. They have to be the same.', 'created_at': datetime.datetime(2024, 7, 10, 22, 24, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2221969168, 'issue_id': 2394147267, 'author': 'a-sajjad72', 'body': ""> You have keras 2.16 and TF 2.15. They have to be the same.\r\n\r\nsorry, but the TF version on my machine is 2.16. btw this is the new setup now. because you mentioned that the version TF 2.15 is out of support, that's why I switched to to the 2.16 version and tested on it."", 'created_at': datetime.datetime(2024, 7, 11, 3, 54, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2222935081, 'issue_id': 2394147267, 'author': 'mihaimaruseac', 'body': 'Oh, I was replying based on previous info.\r\n\r\nNow you need to resolve the conflict of having 2 keras installed, I think. @sampathweb, can you advise on which version of Keras should stay? This is a pip list after a fresh install:\r\n\r\n> keras 3.4.1 tf_keras==2.16.0 tensorflow-2.16.2\r\n\r\nFrom my understanding, there should be only one of `keras` and `tf_keras`.', 'created_at': datetime.datetime(2024, 7, 11, 13, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611384395, 'issue_id': 2394147267, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2025, 1, 24, 2, 0, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626129135, 'issue_id': 2394147267, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2025, 1, 31, 2, 0, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626129389, 'issue_id': 2394147267, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70960"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70960"">No</a>', 'created_at': datetime.datetime(2025, 1, 31, 2, 0, 24, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-07 23:18:54 UTC): Please use TF 2.17 (RC) or 2.16, as 2.15 is out of support. Also, see #63362

a-sajjad72 (Issue Creator) on (2024-07-08 00:31:39 UTC): As I mentioned earlier, I am trying to use the Object Detection API, it throws error with the latest releases. That's why I downgraded it. I also tested it on Google Colab recently, I also got error there.
`AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental`
I search along the github and the solution suggests that downgrade the TF version.  I have been stucked for many days. I had to submit my project for ML application. Already the deadline is passed. I got and an extension in the submission.

mihaimaruseac on (2024-07-08 00:33:59 UTC): That makes sense. For Keras error, please look at `pip list` and make sure you are using the same as TF (until TF 2.16, after that note that Keras multiplatform and TF-Keras are 2 separate packages and only one is supposed to be tied with TF -- I have to learn exactly which and what)

a-sajjad72 (Issue Creator) on (2024-07-10 16:43:35 UTC): keras 3.4.1
tf_keras==2.16.0
tensorflow-2.16.2

@mihaimaruseac i have these versions of libraries installed in my conda environment with `python 3.11.9`. I didn't installed any of these manually. all of the libraries are installed by the `object_detection` library while installation.

mihaimaruseac on (2024-07-10 22:24:34 UTC): You have keras 2.16 and TF 2.15. They have to be the same.

a-sajjad72 (Issue Creator) on (2024-07-11 03:54:16 UTC): sorry, but the TF version on my machine is 2.16. btw this is the new setup now. because you mentioned that the version TF 2.15 is out of support, that's why I switched to to the 2.16 version and tested on it.

mihaimaruseac on (2024-07-11 13:24:29 UTC): Oh, I was replying based on previous info.

Now you need to resolve the conflict of having 2 keras installed, I think. @sampathweb, can you advise on which version of Keras should stay? This is a pip list after a fresh install:


From my understanding, there should be only one of `keras` and `tf_keras`.

github-actions[bot] on (2025-01-24 02:00:21 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2025-01-31 02:00:22 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2025-01-31 02:00:24 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70960"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70960"">No</a>

"
2394043112,issue,closed,completed,2.16.2 doesn't see my gpu,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.4

### Bazel version

_No response_

### GCC/compiler version

VSC 1.91.0 (user setup)

### CUDA/cuDNN version

12.5/9.2

### GPU model and memory

RTX 3050 Laptop GPU

### Current behavior?

i followed the steps to install CUDA and cuDNN but tensorflow won't see my gpu
nvidia-smi
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 556.12                 Driver Version: 556.12         CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id               Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf        Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                                            |                                     |                      MIG M. |
|========================+==============+==============|
|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   56C    P0                         7W /   30W |         0MiB /   4096MiB |      0%      Default |
|                                                                    |                                      |                    N/A |
+-----------------------------------------+------------------------+----------------------+


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Print TensorFlow version
print(""TensorFlow version:"", tf.__version__)

# Check for GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(""GPUs are available"")
    for gpu in gpus:
        print(gpu)
else:
    print(""No GPUs found"")

output:
TensorFlow version: 2.16.2
No GPUs found
```


### Relevant log output

_No response_",Yulee3542,2024-07-07 12:52:53+00:00,['tilakrayal'],2024-07-07 13:54:43+00:00,2024-07-07 13:54:39+00:00,https://github.com/tensorflow/tensorflow/issues/70958,"[('type:support', 'Support issues')]","[{'comment_id': 2212459088, 'issue_id': 2394043112, 'author': 'mihaimaruseac', 'body': 'Please see #63362', 'created_at': datetime.datetime(2024, 7, 7, 13, 54, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2212459100, 'issue_id': 2394043112, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70958"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70958"">No</a>', 'created_at': datetime.datetime(2024, 7, 7, 13, 54, 41, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2024-07-07 13:54:39 UTC): Please see #63362

google-ml-butler[bot] on (2024-07-07 13:54:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70958"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70958"">No</a>

"
2393631644,issue,closed,completed,"Unable to register cuDNN factory, cuFFT factory, and cuBLAS factory","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TensorFlow Version: 2.15.0

### Custom code

No

### OS platform and distribution

OS Version: #46~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX4000

### Current behavior?

Cannot utilize my GPU to run TF

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2024-07-06 15:47:43.180939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-06 15:47:43.181095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-06 15:47:43.345276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-06 15:47:43.613629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-06 15:47:45.645686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
",MohannadAbuIssa,2024-07-06 17:05:31+00:00,['Venkat6871'],2024-07-29 14:44:54+00:00,2024-07-29 14:44:51+00:00,https://github.com/tensorflow/tensorflow/issues/70947,"[('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2213218474, 'issue_id': 2393631644, 'author': 'sushreebarsa', 'body': '@MohannadAbuIssa Could you ensure that the correct versions of CUDA and cuDNN compatible with your TensorFlow version are installed? Also please make sure that your NVIDIA drivers are up to date.\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 8, 7, 17, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2218419834, 'issue_id': 2393631644, 'author': 'vladbelit', 'body': 'This is a duplicate of https://github.com/tensorflow/tensorflow/issues/62075\r\n\r\nHowever, it should not prevent  the GPU from being detected/used', 'created_at': datetime.datetime(2024, 7, 9, 18, 52, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251813463, 'issue_id': 2393631644, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 26, 1, 52, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2254484623, 'issue_id': 2393631644, 'author': 'anilbey', 'body': 'This issue is better not closed because it still exists', 'created_at': datetime.datetime(2024, 7, 28, 11, 32, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256138402, 'issue_id': 2393631644, 'author': 'belitskiy', 'body': ""Closing as it's a duplicate of https://github.com/tensorflow/tensorflow/issues/62075"", 'created_at': datetime.datetime(2024, 7, 29, 14, 44, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256138506, 'issue_id': 2393631644, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70947"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70947"">No</a>', 'created_at': datetime.datetime(2024, 7, 29, 14, 44, 53, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-08 07:17:14 UTC): @MohannadAbuIssa Could you ensure that the correct versions of CUDA and cuDNN compatible with your TensorFlow version are installed? Also please make sure that your NVIDIA drivers are up to date.
Thank you!

vladbelit on (2024-07-09 18:52:20 UTC): This is a duplicate of https://github.com/tensorflow/tensorflow/issues/62075

However, it should not prevent  the GPU from being detected/used

github-actions[bot] on (2024-07-26 01:52:48 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

anilbey on (2024-07-28 11:32:45 UTC): This issue is better not closed because it still exists

belitskiy on (2024-07-29 14:44:51 UTC): Closing as it's a duplicate of https://github.com/tensorflow/tensorflow/issues/62075

google-ml-butler[bot] on (2024-07-29 14:44:53 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70947"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70947"">No</a>

"
2392946870,issue,closed,completed,install tensorflow 1.12.0 (python 3.5),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.12.0

### Custom code

No

### OS platform and distribution

Mac

### Mobile device

_No response_

### Python version

python 3.5.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

install tf 1.12 under python 3.5

### Standalone code to reproduce the issue

```shell
pip install tensorflow-1.12.0
```


### Relevant log output

```shell
python --version
Python 3.5.10

pip install tensorflow-1.12.0

Collecting tensorflow-1.12.0
  Could not fetch URL https://pypi.python.org/simple/tensorflow-1-12-0/: There was a problem confirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:728) - skipping
  Could not find a version that satisfies the requirement tensorflow-1.12.0 (from versions: )
No matching distribution found for tensorflow-1.12.0
```
",yayale1,2024-07-05 17:34:56+00:00,['Venkat6871'],2024-08-02 01:53:35+00:00,2024-08-02 01:53:32+00:00,https://github.com/tensorflow/tensorflow/issues/70901,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('TF 1.12', 'Issues related to TF 1.12'), ('subtype:macOS', 'macOS Build/Installation issues')]","[{'comment_id': 2213153135, 'issue_id': 2392946870, 'author': 'sushreebarsa', 'body': '@yayale1 Older version of TF(1.x) is not actively supported so please upgrade to the latest TF version. Please refer to this tested build [configuration](https://www.tensorflow.org/install/source#macos) and let us know?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 8, 6, 37, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2217661194, 'issue_id': 2392946870, 'author': 'yayale1', 'body': '> @yayale1 Older version of TF(1.x) is not actively supported so please upgrade to the latest TF version. Please refer to this tested build [configuration](https://www.tensorflow.org/install/source#macos) and let us know? Thank you!\r\ngot it! thanks for your help!', 'created_at': datetime.datetime(2024, 7, 9, 12, 55, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2224508500, 'issue_id': 2392946870, 'author': 'sushreebarsa', 'body': '@yayale1 Could you please let us know if there is any update on this issue after upgrading?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 12, 4, 15, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2251813526, 'issue_id': 2392946870, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 26, 1, 52, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345903, 'issue_id': 2392946870, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264345944, 'issue_id': 2392946870, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70901"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70901"">No</a>', 'created_at': datetime.datetime(2024, 8, 2, 1, 53, 34, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-08 06:37:20 UTC): @yayale1 Older version of TF(1.x) is not actively supported so please upgrade to the latest TF version. Please refer to this tested build [configuration](https://www.tensorflow.org/install/source#macos) and let us know?
Thank you!

yayale1 (Issue Creator) on (2024-07-09 12:55:23 UTC): got it! thanks for your help!

sushreebarsa on (2024-07-12 04:15:38 UTC): @yayale1 Could you please let us know if there is any update on this issue after upgrading?
Thank you!

github-actions[bot] on (2024-07-26 01:52:50 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-02 01:53:32 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-02 01:53:34 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70901"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70901"">No</a>

"
2391902435,issue,closed,completed,Inaccurate Shoulder Width and Height Measurements on iOS using React Native and Expo,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

4.10.0

### Custom code

Yes

### OS platform and distribution

Mac Os Version 14.5

### Mobile device

iOS Platform 17.5.1

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The code below, which I provided, is used to measure the width of the shoulder and the height of the user of this application, built on React Native (0.72.10) and Expo (49.0.8). While the code works correctly on the Android application, it does not produce the expected output on iOS. The measurement of shoulder width and height is not accurate, and the captured image's detected human keypoints, indicated by red lines, are also incorrect.

It would be very helpful to know what considerations I should take into account when working with iOS platform-specific code to make things work correctly.

The Below Code Is Correctly working in Android Application.

import React, { useEffect, useState, useRef } from 'react';
import { StyleSheet, Text, View, Dimensions, Platform, TouchableOpacity, Image, ActivityIndicator, Button, PixelRatio } from 'react-native';
import { Camera } from 'expo-camera';
import * as posenet from '@tensorflow-models/posenet';
import * as tf from '@tensorflow/tfjs';
import { decodeJpeg } from '@tensorflow/tfjs-react-native';
import Svg, { Circle, Ellipse, Line } from 'react-native-svg';
import { CameraType } from 'expo-camera/build/Camera.types';
import { Icon } from '@react-native-material/core';


const IS_ANDROID = Platform.OS === 'android';
const IS_IOS = Platform.OS === 'ios';
const pixelDensity = PixelRatio.get() * 160;
const CAM_PREVIEW_WIDTH = Dimensions.get('window').width;
//const CAM_PREVIEW_HEIGHT = 600;
const CAM_PREVIEW_HEIGHT = CAM_PREVIEW_WIDTH / (IS_IOS ? 9 / 16 : 3 / 4);
const MIN_KEYPOINT_SCORE = 0.3;
const MIN_KEYPOINT_SCORE_IOS = 0.1;
const AUTO_RENDER = false;
const KNOWN_DETAILS = {
    DEVICE_HEIGHT: 1600,
    DEVICE_WIDTH: 1200,
    FOCAL_LENGTH_MM: 2.216,
    DISTANCE_CM: 122,

    HEIGHT_CM: 172,
    HEIGHT_PIXELS: 700,
    SHOULDER_DISTANCE_CM: 42,
    SHOULDER_DISTANCE_PIXELS: 150,
}
const KNOWN_DETAILS_IOS = {
    DEVICE_HEIGHT: 1300,
    DEVICE_WIDTH: 900,
    FOCAL_LENGTH_MM: 2.216,
    DISTANCE_CM: 122,

    HEIGHT_CM: 172,
    HEIGHT_PIXELS: 700,
    SHOULDER_DISTANCE_CM: 42,
    SHOULDER_DISTANCE_PIXELS: 150,
}

export default function CameraMatSelectionAfterTaken({ afterMeasured }) {
    const cameraRef = useRef(null);
    const [tfReady, setTfReady] = useState(false);
    const [model, setModel] = useState(null);
    const [poses, setPoses] = useState(null);
    const [fps, setFps] = useState(0);
    const [orientation, setOrientation] = useState();
    const [cameraType, setCameraType] = useState(CameraType.front);
    const [timeLeft, setTimeLeft] = useState(null);
    const [capturedImage, setCapturedImage] = useState(null);
    const [processing, setProcessing] = useState(true);
    const [keyPoints, setKeyPoints] = useState(null);
    const [height, setHeight] = useState(null);
    const [shoulderWidth, setShoulderWidth] = useState(null);
    const [imageNotProper, setImageNotProper] = useState(false);
    const [outputTensorHeight, setOutputTensorHeight] = useState(0);
    const [outputTensorWidth, setOutputTensorWidth] = useState(0);
    console.log(CAM_PREVIEW_WIDTH)

    useEffect(() => {
        async function prepare() {
            try {
                await Camera.requestCameraPermissionsAsync();
                await tf.ready();
                const model = await posenet.load();
                setModel(model);
                setTfReady(true);
                console.log('Model loaded successfully');
            } catch (error) {
                console.error('Error loading model:', error);
            }
        }
        prepare();
    }, []);
    
    useEffect(() => {
        if (timeLeft === 0) {
            takePhotoAsync();
            setTimeLeft(null)
        }
        if (!timeLeft) return;
        const intervalId = setInterval(() => {
            setTimeLeft(timeLeft - 1);
        }, 1000);
        return () => clearInterval(intervalId);
    }, [timeLeft]);
    useEffect(() => {
        if (capturedImage && tfReady) {
            processImage();
        }
    }, [capturedImage]);
    async function takePhotoAsync() {
        if (cameraRef.current) {
            let takenPhoto = await cameraRef.current.takePictureAsync({ base64: true, exif: true, });
            setCapturedImage(takenPhoto);
        }
    }
    const processImage = async () => {
        console.log('Processing Image')
        //console.log(capturedImage.exif);

        if (capturedImage) {
            try {
                console.log('Captured Image:', capturedImage.exif);
                setProcessing(true);
                const response = capturedImage.base64;
                const imageData = new Uint8Array(Buffer.from(response, 'base64'));
                const imageTensor = decodeJpeg(imageData);
                console.log({ image_hight: capturedImage.height, image_width: capturedImage.width });
                console.log(""imageTensor:"", imageTensor);
                const shape = imageTensor.shape;
                if (shape) {
                    setOutputTensorHeight(shape[0])
                    setOutputTensorWidth(shape[1]);
                }
                const dtposes = await model.estimateSinglePose(imageTensor);
                console.log(""dtposes"", dtposes)
                if (dtposes != null && dtposes.keypoints != null && dtposes.keypoints.length > 0) {
                    const keypoints = dtposes.keypoints
                        .filter((k) => (k.score ?? 0) > IS_ANDROID ? MIN_KEYPOINT_SCORE : MIN_KEYPOINT_SCORE_IOS);
                    const head = keypoints.find(point => point.part === 'nose');
                    const leftAnkle = keypoints.find(point => point.part === 'leftAnkle');
                    const rightAnkle = keypoints.find(point => point.part === 'rightAnkle');

                    const leftShoulder = keypoints.find(point => point.part === 'leftShoulder');
                    const rightShoulder = keypoints.find(point => point.part === 'rightShoulder');
                    setKeyPoints(keypoints); 
                    if (head && leftAnkle && rightAnkle && leftShoulder && rightShoulder) {
                        const feet = getFeetPosition(leftAnkle, rightAnkle);
                        const pixelDistance = calculateDistance(head.position, feet.position);

                        const known_pixels = (KNOWN_DETAILS.HEIGHT_PIXELS / KNOWN_DETAILS.DEVICE_HEIGHT) * shape[0];

                        const knwonFloor = shape[0] - known_pixels;
                        const currentFloor = shape[0] - pixelDistance;
                        const conversionFactor = KNOWN_DETAILS.HEIGHT_CM / knwonFloor;
                        const realHeight = ((currentFloor * conversionFactor)).toFixed(2);
                        console.log({
                            known_pixels: known_pixels,
                            knwonFloor: knwonFloor,
                            currentFloor: currentFloor,
                            conversionFactor: conversionFactor,
                            Pixel: pixelDistance,
                            realHeight: realHeight,
                            pixelDensity: pixelDensity,
                            pixelDensityCM: (pixelDensity / 2.54)
                        });
                        setHeight(realHeight);

                        const pixelShoulder = calculateDistance(leftShoulder.position, rightShoulder.position);
                        const know_sholder_pixels = (KNOWN_DETAILS.SHOULDER_DISTANCE_PIXELS / KNOWN_DETAILS.DEVICE_WIDTH) * shape[1];
                        const shoulder_conversionFactor = KNOWN_DETAILS.SHOULDER_DISTANCE_CM / know_sholder_pixels;
                        const realShoulderWidth = (pixelShoulder * shoulder_conversionFactor).toFixed(2);
                        setShoulderWidth(realShoulderWidth);

                        console.log({
                            know_sholder_pixels: know_sholder_pixels,
                            pixelShoulder: pixelShoulder,
                            shoulder_conversionFactor: shoulder_conversionFactor,
                            realShoulderWidth: realShoulderWidth,
                            pixelDensity: pixelDensity,
                            pixelDensityCM: (pixelDensity / 2.54)
                        });
                    }
                    else {
                        setImageNotProper(true);
                    }
                }
                setProcessing(false);
            } catch (error) {
                console.error('Error loading PoseNet model:', error);
                setProcessing(false);
            }
        }
    };
    function calculateDistance(point1, point2) {
        const dx = point2.x - point1.x;
        const dy = point2.y - point1.y;
        return Math.sqrt(dx * dx + dy * dy);
    }
    const getFeetPosition = (leftFeet, rightFeet) => {
        const feet = { position: {} }
        feet.position.x = (leftFeet.position.x + rightFeet.position.x) / 2;
        feet.position.y = (leftFeet.position.y + rightFeet.position.y) / 2;
        return feet;
    }
    const renderPose = () => {
        if (keyPoints != null && keyPoints.length > 0) {

            const leftShoulder = keyPoints.find(point => point.part === 'leftShoulder');
            const rightShoulder = keyPoints.find(point => point.part === 'rightShoulder');
            let shoulderLine = null;
            let heightLine = null;
            if (leftShoulder && rightShoulder) {
                const rcx = (rightShoulder.position.x / outputTensorWidth) * CAM_PREVIEW_WIDTH
                const rcy = (rightShoulder.position.y / outputTensorHeight) * CAM_PREVIEW_HEIGHT
                const lcx = (leftShoulder.position.x / outputTensorWidth) * CAM_PREVIEW_WIDTH
                const lcy = (leftShoulder.position.y / outputTensorHeight) * CAM_PREVIEW_HEIGHT
                shoulderLine = <Line
                    x1={rcx}
                    y1={rcy}
                    x2={lcx}
                    y2={lcy}
                    r='4'
                    strokeWidth='2'
                    stroke='red'
                />
            }
            const head = keyPoints.find(point => point.part === 'nose');
            const leftAnkle = keyPoints.find(point => point.part === 'leftAnkle');
            const rightAnkle = keyPoints.find(point => point.part === 'rightAnkle');    
            if (head && rightAnkle && leftAnkle) {
                const feet = getFeetPosition(leftAnkle, rightAnkle);
                const rcx = (feet.position.x / outputTensorWidth) * CAM_PREVIEW_WIDTH
                const rcy = (feet.position.y / outputTensorHeight) * CAM_PREVIEW_HEIGHT
                const lcx = (head.position.x / outputTensorWidth) * CAM_PREVIEW_WIDTH
                const lcy = (head.position.y / outputTensorHeight) * CAM_PREVIEW_HEIGHT
                heightLine = <Line
                    x1={rcx}
                    y1={rcy}
                    x2={lcx}
                    y2={lcy}
                    r='4'
                    strokeWidth='2'
                    stroke='red'
                />
            }
            const keypoints = keyPoints.map((point) => {

                let k = point.position;
                const x = k.x;
                const y = k.y;
                const cx = (x / outputTensorWidth) * CAM_PREVIEW_WIDTH;
                const cy = (y / outputTensorHeight) * CAM_PREVIEW_HEIGHT;

                return (
                    <Circle
                        key={`skeletonkp_${point.part}`}
                        cx={cx}
                        cy={cy}
                        r='4'
                        strokeWidth='2'
                        fill='red'
                        stroke='white'
                    />
                );
            });
            return <Svg style={[styles.svg, (cameraType == CameraType.front ? styles.svgMirror : {})]}>
                {shoulderLine && shoulderLine}
                {heightLine && heightLine}
                {keypoints}
            </Svg>;
        } else {
            return <View></View>;
        }
    };
    const onTakePhoto = () => {
        setTimeLeft(10);
    }

    const handleSwitchCameraType = () => {
        if (cameraType === CameraType.front) {
            setCameraType(CameraType.back);
        } else {
            setCameraType(CameraType.front);
        }
    };
    const reCapture = () => {
        setCapturedImage(null);
        setHeight(null);
        setShoulderWidth(null);
        setKeyPoints(null);
    }
    const onContinue = () => {
        if (height && shoulderWidth) {
            afterMeasured(Math.round(height), Math.round(shoulderWidth));
        }
    }   
    if (!tfReady) {
        return (
            <View style={styles.loadingMsg}>
                <Text>Loading...</Text>
            </View>
        );
    }
    else {
        return (
            <View
                style={styles.containerPortrait}
            >
                <View>

                    {capturedImage ?
                        <View>
                            <Image
                                source={{ uri: capturedImage.uri }}
                                style={[styles.camera, (cameraType == CameraType.front ? styles.imageMirror : {})]}
                            />
                            {renderPose()}
                            {processing && <View style={styles.alignCenter}>
                                <ActivityIndicator /><Text style={{ color: ""black"" }}>Please wait we are proccesing...</Text></View>}

                            {imageNotProper && <View style={styles.alignCenter}>
                                <Text style={{ color: ""black"" }}>Can't capture your data</Text>
                                <Button
                                    title={""Retry""}
                                    onPress={reCapture}
                                />
                            </View>
                            }
                            {(shoulderWidth || height) && <View style={styles.alignCenter}>
                                <Text style={{ color: ""black"" }}>Data captured successfully</Text>
                                <Text style={{ color: ""black"" }}>Your Height: {height} CM</Text>
                                <Text style={{ color: ""black"" }}>Your Shoulder Width: {shoulderWidth} CM</Text>
                                <Button
                                    title={""Continue""}
                                    onPress={onContinue}
                                />
                            </View>
                            }
                        </View> :

                        <View>
                            <Camera
                                ref={cameraRef}
                                style={styles.camera}
                                autorender={AUTO_RENDER}
                                type={cameraType}
                            >
                            </Camera>
                            <Svg style={[styles.svg, (cameraType == CameraType.front ? styles.svgMirror : {})]}>
                                <Line
                                    x1={0}
                                    y1={60}
                                    x2={CAM_PREVIEW_WIDTH}
                                    y2={60}
                                    r='4'
                                    strokeWidth='2'
                                    stroke='red'
                                />
                                <Line
                                    x1={CAM_PREVIEW_WIDTH / 2}
                                    y1={0}
                                    x2={CAM_PREVIEW_WIDTH / 2}
                                    y2={CAM_PREVIEW_HEIGHT}
                                    r='4'
                                    strokeWidth='2'
                                    stroke='red'
                                />
                                <Line
                                    x1={0}
                                    y1={CAM_PREVIEW_HEIGHT - 50}
                                    x2={CAM_PREVIEW_WIDTH}
                                    y2={CAM_PREVIEW_HEIGHT - 50}
                                    r='4'
                                    strokeWidth='2'
                                    stroke='red'
                                />
                                <Ellipse
                                    cx={CAM_PREVIEW_WIDTH / 2}
                                    cy={60}
                                    rx={50}
                                    ry={50}
                                    stroke={""red""}
                                    strokeWidth=""2""
                                    fill={""transparent""}
                                />
                            </Svg>
                            <View style={styles.fpsContainer}>
                                <TouchableOpacity onPress={onTakePhoto} style={styles.camera_take_photo_container}>
                                    <Icon style={[styles.camera_icons, styles.camera_take_photo_icon]} name=""camera"" />
                                </TouchableOpacity>
                            </View>
                            <View style={styles.cameraTypeSwitcher}>
                                <TouchableOpacity onPress={handleSwitchCameraType} style={styles.camera_flip_camera_container}>
                                    <Icon style={[styles.camera_icons, styles.camera_flip_camera_icon]} name=""camera-flip"" />
                                </TouchableOpacity>
                            </View>
                            <View style={styles.timer}>
                                <Text style={{ color: 'white', fontSize: 56 }}>{timeLeft}</Text>
                            </View>
                        </View>}
                </View>
            </View>
        );  
    }
}

const styles = StyleSheet.create({
    containerPortrait: {
        position: 'relative',
        width: CAM_PREVIEW_WIDTH,
        height: CAM_PREVIEW_HEIGHT,
        marginTop: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2,
    },
    containerLandscape: {
        position: 'relative',
        width: CAM_PREVIEW_HEIGHT,
        height: CAM_PREVIEW_WIDTH,
        marginLeft: Dimensions.get('window').height / 2 - CAM_PREVIEW_HEIGHT / 2,
    },
    loadingMsg: {
        position: 'absolute',
        width: '100%',
        height: '100%',
        alignItems: 'center',
        justifyContent: 'center',
    },
    camera: {
        width: '100%',
        height: '100%',
        zIndex: 1,
    },
    imageMirror: {
        transform: [
            { scaleX: -1 }
        ]
    },
    svgMirror: {
        transform: [
            { scaleX: -1 }
        ]
    },
    svg: {
        width: '100%',
        height: '100%',
        position: 'absolute',
        zIndex: 2,
    },
    timer: {
        position: 'absolute',
        bottom: CAM_PREVIEW_HEIGHT / 2,
        left: 0,
        justifyContent: 'center',
        alignItems: 'center',
        width: ""100%"",
        height: 100,
        zIndex: 3,
    },
    fpsContainer: {
        backgroundColor: 'black',
        position: 'absolute',
        top: 10,
        left: 10,
        width: 80,
        alignItems: 'center',
        borderRadius: 2,
        padding: 8,
        zIndex: 3,
    },
    cameraTypeSwitcher: {
        backgroundColor: 'black',
        position: 'absolute',
        top: 10,
        right: 10,
        width: 70,
        alignItems: 'center',
        borderRadius: 2,
        padding: 8,
        zIndex: 3,
    },
    camera_icons: {
        color: 'white',
    },
    camera_take_photo_icon: {
        fontSize: 56,
    },
    camera_flip_camera_container: {
        backgroundColor: 'transparent',
        padding: 10,
        borderRadius: 50,
        marginBottom: 26,
        justifyContent: ""center""
    },
    camera_take_photo_container: {
        marginBottom: 28,
    },
    camera_flip_camera_icon: {
        fontSize: 36,
    },
    alignCenter: {
        position: 'absolute',
        bottom: CAM_PREVIEW_HEIGHT / 2,
        left: 0,
        justifyContent: 'center',
        alignItems: 'center',
        width: '100%',
        height: 100,
        zIndex: 3,
        backgroundColor: 'rgba(255, 255, 255, .7)',
    }
});

And My Package.json file for reference: 

{
  ""name"": ""camdetect.mobileapp"",
  ""version"": ""1.0.0"",
  ""main"": ""index.js"",
  ""scripts"": {
    ""start"": ""expo start --dev-client"",
    ""android"": ""expo run:android"",
    ""ios"": ""expo run:ios"",
    ""web"": ""expo start --web"",
    ""postinstall"": ""npx patch-package""
  },
  ""dependencies"": {
    ""@expo/vector-icons"": ""^13.0.0"",
    ""@mediapipe/pose"": ""^0.5.1675469404"",
    ""@react-native-async-storage/async-storage"": ""1.18.2"",
    ""@react-native-material/core"": ""^1.3.7"",
    ""@react-navigation/bottom-tabs"": ""^6.5.20"",
    ""@react-navigation/native"": ""^6.1.7"",
    ""@react-navigation/native-stack"": ""^6.9.13"",
    ""@reduxjs/toolkit"": ""^2.2.4"",
    ""@rneui/base"": ""^4.0.0-rc.8"",
    ""@rneui/themed"": ""^4.0.0-rc.8"",
    ""@tensorflow-models/pose-detection"": ""^2.1.3"",
    ""@tensorflow-models/posenet"": ""^2.2.2"",
    ""@tensorflow/tfjs"": ""4.10.0"",
    ""@tensorflow/tfjs-react-native"": ""0.8.0"",
    ""axios"": ""^1.6.8"",
    ""depcheck"": ""^1.4.7"",
    ""expo"": ""~49.0.8"",
    ""expo-camera"": ""~13.4.4"",
    ""expo-dev-client"": ""~2.4.13"",
    ""expo-file-system"": ""~15.4.5"",
    ""expo-font"": ""~11.4.0"",
    ""expo-gl"": ""~13.0.1"",
    ""expo-image-manipulator"": ""~11.3.0"",
    ""expo-permissions"": ""~14.2.1"",
    ""expo-screen-orientation"": ""~6.0.6"",
    ""expo-splash-screen"": ""~0.20.5"",
    ""expo-status-bar"": ""~1.6.0"",
    ""react"": ""18.2.0"",
    ""react-native"": ""0.72.10"",
    ""react-native-fs"": ""^2.20.0"",
    ""react-native-gesture-handler"": ""~2.12.1"",
    ""react-native-image-slider-box"": ""^2.0.7"",
    ""react-native-safe-area-context"": ""4.6.3"",
    ""react-native-screens"": ""~3.22.0"",
    ""react-native-svg"": ""13.9.0"",
    ""react-native-toast-message"": ""^2.2.0"",
    ""react-redux"": ""^9.1.2"",
    ""redux-persist"": ""^6.0.0""
  },
  ""devDependencies"": {
    ""@babel/core"": ""^7.20.0""
  },
  ""private"": true
}






### Standalone code to reproduce the issue

```shell
Expecting A Suggestion Or To Say To An Guide How To Work With Tensorflow in iOS Device.
```


### Relevant log output

```shell
Android Console output:
Model loaded successfully
 LOG  Processing Image
 LOG  {""image_hight"": 1440, ""image_width"": 1080}
 LOG  imageTensor: {""dataId"": {""id"": 62}, ""dtype"": ""int32"", ""id"": 62, ""isDisposedInternal"": false, ""kept"": false, ""rankType"": ""3"", ""shape"": [1440, 1080, 3], ""size"": 4665600, ""strides"": [3240, 3]}
 LOG  dtposes [{""part"": ""nose"", ""position"": {""x"": 545.1062059216925, ""y"": 191.49965041342412}, ""score"": 0.9810086488723755}, {""part"": ""leftEye"", ""position"": {""x"": 538.8779410870623, ""y"": 172.17822598112232}, ""score"": 0.9725439548492432}, {""part"": ""rightEye"", ""position"": {""x"": 532.0788146735165, ""y"": 160.94038788910504}, ""score"": 0.984584629535675}, {""part"": ""leftEar"", ""position"": {""x"": 584.5300245470573, ""y"": 179.49073880562986}, ""score"": 0.7300240397453308}, {""part"": ""rightEar"", ""position"": {""x"": 484.447311223249, ""y"": 178.01409294633086}, ""score"": 0.9097193479537964}, {""part"": ""leftShoulder"", ""position"": {""x"": 620.3027913728113, ""y"": 297.7782039419686}, ""score"": 0.997944176197052}, {""part"": ""rightShoulder"", ""position"": {""x"": 439.7834415658439, ""y"": 306.0812389804231}, ""score"": 0.9973758459091187}, {""part"": ""leftElbow"", ""position"": {""x"": 670.8984659989056, ""y"": 441.28194252340705}, ""score"": 0.9933682084083557}, {""part"": ""rightElbow"", ""position"": {""x"": 413.39004932210594, ""y"": 451.5963092017266}, ""score"": 0.992526650428772}, {""part"": ""leftWrist"", ""position"": {""x"": 675.0317192819795, ""y"": 540.9335386521158}, ""score"": 0.9703831076622009}, {""part"": ""rightWrist"", ""position"": {""x"": 404.9840976106517, ""y"": 531.7778234815783}, ""score"": 0.8649968504905701}, {""part"": ""leftHip"", ""position"": {""x"": 591.9822603812013, ""y"": 552.5793884514834}, ""score"": 0.9943028092384338}, {""part"": ""rightHip"", ""position"": {""x"": 483.0766706058487, ""y"": 554.6393463491002}, ""score"": 0.9938344359397888}, {""part"": ""leftKnee"", ""position"": {""x"": 583.9515537603355, ""y"": 790.9039662876945}, ""score"": 0.9841167330741882}, {""part"": ""rightKnee"", ""position"": {""x"": 506.98266411569796, ""y"": 807.5756550948443}, ""score"": 0.983876645565033}, {""part"": ""leftAnkle"", ""position"": {""x"": 581.4712085055934, ""y"": 1031.1535882022129}, ""score"": 0.5877557992935181}, {""part"": ""rightAnkle"", ""position"": {""x"": 546.8396518573686, ""y"": 1038.7305637463521}, ""score"": 0.7883061170578003}]
 LOG  {""Pixel"": 843.6575123715096, ""conversionFactor"": 0.2123456790123457, ""currentFloor"": 596.3424876284904, ""known_pixels"": 630, ""knwonFloor"": 810, ""pixelDensity"": 320, ""pixelDensityCM"": 125.98425196850394, ""realHeight"": ""126.63""}
 LOG  {""know_sholder_pixels"": 135, ""pixelDensity"": 320, ""pixelDensityCM"": 125.98425196850394, ""pixelShoulder"": 180.7101990635284, ""realShoulderWidth"": ""56.22"", ""shoulder_conversionFactor"": 0.3111111111111111}

iOS Output: 
Model loaded successfully
 LOG  Processing Image
 LOG  {""image_hight"": 4032, ""image_width"": 2268}
 LOG  imageTensor: {""dataId"": {""id"": 62}, ""dtype"": ""int32"", ""id"": 62, ""isDisposedInternal"": false, ""kept"": false, ""rankType"": ""3"", ""shape"": [2268, 4032, 3], ""size"": 27433728, ""strides"": [12096, 3]}
 LOG  dtposes [{""part"": ""nose"", ""position"": {""x"": 1597.000972762646, ""y"": 783.8229571984434}, ""score"": 0.01142120361328125}, {""part"": ""leftEye"", ""position"": {""x"": 1803.6201361867704, ""y"": 593.8270914396887}, ""score"": 0.023651123046875}, {""part"": ""rightEye"", ""position"": {""x"": 1933.0826848249026, ""y"": 452.52140077821014}, ""score"": 0.0143890380859375}, {""part"": ""leftEar"", ""position"": {""x"": 1825.4066147859921, ""y"": 604.2453793774318}, ""score"": 0.0218658447265625}, {""part"": ""rightEar"", ""position"": {""x"": 1742.0679717898831, ""y"": 568.9610894941634}, ""score"": 0.016021728515625}, {""part"": ""leftShoulder"", ""position"": {""x"": 1845.3239299610893, ""y"": 609.7992461089493}, ""score"": 0.041778564453125}, {""part"": ""rightShoulder"", ""position"": {""x"": 1801.1381322957197, ""y"": 538.135214007782}, ""score"": 0.0360107421875}, {""part"": ""leftElbow"", ""position"": {""x"": 1734.231274319066, ""y"": 769.727626459144}, ""score"": 0.021453857421875}, {""part"": ""rightElbow"", ""position"": {""x"": 1710.6215953307392, ""y"": 742.3336575875485}, ""score"": 0.0287628173828125}, {""part"": ""leftWrist"", ""position"": {""x"": 1738.6284046692606, ""y"": 1068.2422178988327}, ""score"": 0.019683837890625}, {""part"": ""rightWrist"", ""position"": {""x"": 1388.2062256809338, ""y"": 800.4922178988327}, ""score"": 0.01934814453125}, {""part"": ""leftHip"", ""position"": {""x"": 1799.7285992217899, ""y"": 1123.7464129377431}, ""score"": 0.03594970703125}, {""part"": ""rightHip"", ""position"": {""x"": 1812.567607003891, ""y"": 1129.0072653210116}, ""score"": 0.0274810791015625}, {""part"": ""leftKnee"", ""position"": {""x"": 1783.6875, ""y"": 1560.046692607004}, ""score"": 0.02386474609375}, {""part"": ""rightKnee"", ""position"": {""x"": 1799.7898832684825, ""y"": 1554.1634241245133}, ""score"": 0.0305023193359375}, {""part"": ""leftAnkle"", ""position"": {""x"": 1820.8715953307392, ""y"": 2132.062408803502}, ""score"": 0.03436279296875}, {""part"": ""rightAnkle"", ""position"": {""x"": 1005.2048729328793, ""y"": 2338.783073929961}, ""score"": 0.0352783203125}]
 LOG  {""Pixel"": 1463.2102455224874, ""conversionFactor"": 0.13482265334117186, ""currentFloor"": 804.7897544775126, ""known_pixels"": 992.25, ""knwonFloor"": 1275.75, ""pixelDensity"": 480, ""pixelDensityCM"": 188.9763779527559, ""realHeight"": ""108.50""}
 LOG  {""know_sholder_pixels"": 504, ""pixelDensity"": 480, ""pixelDensityCM"": 188.9763779527559, ""pixelShoulder"": 84.19096277108441, ""realShoulderWidth"": ""7.02"", ""shoulder_conversionFactor"": 0.08333333333333333}
```
",babvijayb,2024-07-05 06:14:55+00:00,['tilakrayal'],2024-07-08 12:52:21+00:00,2024-07-08 12:52:04+00:00,https://github.com/tensorflow/tensorflow/issues/70883,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('type:others', 'issues not falling in  bug, perfromance, support, build and install or feature')]","[{'comment_id': 2213955553, 'issue_id': 2391902435, 'author': 'tilakrayal', 'body': '@babvijayb,\r\nLooks like this issue is not related to tensorflow and more related to the tensorflow JS. Could you please feel free to close this issue and raise the new issue in that repo from [here](https://github.com/tensorflow/tfjs/issues) for the quick response. Thank you!', 'created_at': datetime.datetime(2024, 7, 8, 12, 33, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213995859, 'issue_id': 2391902435, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70883"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70883"">No</a>', 'created_at': datetime.datetime(2024, 7, 8, 12, 52, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213996312, 'issue_id': 2391902435, 'author': 'babvijayb', 'body': 'Thanks For Your Response', 'created_at': datetime.datetime(2024, 7, 8, 12, 52, 20, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-08 12:33:49 UTC): @babvijayb,
Looks like this issue is not related to tensorflow and more related to the tensorflow JS. Could you please feel free to close this issue and raise the new issue in that repo from [here](https://github.com/tensorflow/tfjs/issues) for the quick response. Thank you!

google-ml-butler[bot] on (2024-07-08 12:52:06 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70883"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70883"">No</a>

babvijayb (Issue Creator) on (2024-07-08 12:52:20 UTC): Thanks For Your Response

"
2390750213,issue,closed,completed,"Error Running TFLite Models on ESP32C3: Failed to get registration from op code CUSTOM, AllocateTensors() failed","I am encountering an error when running two TensorFlow Lite models on my ESP32C3. Below are the details of the error, the models, and the conversion code used.

**Arduino code:** https://drive.google.com/file/d/15as7adLykxOzsaAxV6l9UTe_EHS_WqXw/view?usp=sharing

**Tflite models**

![state_model tflite](https://github.com/tensorflow/tensorflow/assets/73652707/3ec950fc-a334-47e7-a243-cf5b811fc9f7)

![temp_model tflite](https://github.com/tensorflow/tensorflow/assets/73652707/0d35e4fb-c31a-49eb-80f7-dec7e31a260d)


**Error Message**

```
Failed to get registration from op code CUSTOM
AllocateTensors() failed
```

**Models**

1. **State Model**
    ```python
    def create_state_model(input_dim):
        model = Sequential()
        model.add(Reshape((input_dim, 1), input_shape=(input_dim,)))
        model.add(LSTM(64, return_sequences=False))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model
    ```

2. **Temperature Model**
    ```python
    def create_temp_model(input_dim, output_dim):
        model = Sequential()
        model.add(Reshape((input_dim, 1), input_shape=(input_dim,)))
        model.add(LSTM(64, return_sequences=False))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(output_dim, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model
    ```
    
**Conversion Code**

```python
# State Model Conversion
converter = tf.lite.TFLiteConverter.from_keras_model(state_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
state_model_tflite = converter.convert()
with open('state_model.tflite', 'wb') as f:
    f.write(state_model_tflite)

# Temperature Model Conversion
converter = tf.lite.TFLiteConverter.from_keras_model(temp_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
temp_model_tflite = converter.convert()
with open('temp_model.tflite', 'wb') as f:
    f.write(temp_model_tflite)
```
",AdityaB-01,2024-07-04 12:25:09+00:00,"['pkgoogle', 'sawantkumar']",2024-08-01 01:57:58+00:00,2024-08-01 01:57:53+00:00,https://github.com/tensorflow/tensorflow/issues/70868,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2210308868, 'issue_id': 2390750213, 'author': 'sushreebarsa', 'body': '@AdityaB-01 The ESP32C3 requires the TFLite Micro interpreter, which is optimized for microcontrollers. Ensure that you have the correct version of the TFLite library. Could you please provide the TF version you are using?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 5, 6, 59, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210504839, 'issue_id': 2390750213, 'author': 'AdityaB-01', 'body': 'Hello, thank you for your response. I am using Arduino IDE to upload the code and It has a library in its library manager. The Git link for the same is https://github.com/spaziochirale/Chirale_TensorFlowLite.git, \r\n\r\nAlso to add, I removed the LSTM layer and used simple Dense layers to train the model in Python, then the converted model is running correctly on esp. But again this is a work around and the actual accurate model is not able to run on that.', 'created_at': datetime.datetime(2024, 7, 5, 9, 11, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231932851, 'issue_id': 2390750213, 'author': 'pkgoogle', 'body': 'Hi @AdityaB-01, I believe you will have more luck posting this issue to: https://github.com/tensorflow/tflite-micro. They are more knowledgeable on Arduino and Arduino code.', 'created_at': datetime.datetime(2024, 7, 16, 22, 36, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710270, 'issue_id': 2390750213, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799832, 'issue_id': 2390750213, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261799960, 'issue_id': 2390750213, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70868"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70868"">No</a>', 'created_at': datetime.datetime(2024, 8, 1, 1, 57, 58, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-05 06:59:45 UTC): @AdityaB-01 The ESP32C3 requires the TFLite Micro interpreter, which is optimized for microcontrollers. Ensure that you have the correct version of the TFLite library. Could you please provide the TF version you are using?
Thank you!

AdityaB-01 (Issue Creator) on (2024-07-05 09:11:35 UTC): Hello, thank you for your response. I am using Arduino IDE to upload the code and It has a library in its library manager. The Git link for the same is https://github.com/spaziochirale/Chirale_TensorFlowLite.git, 

Also to add, I removed the LSTM layer and used simple Dense layers to train the model in Python, then the converted model is running correctly on esp. But again this is a work around and the actual accurate model is not able to run on that.

pkgoogle (Assginee) on (2024-07-16 22:36:51 UTC): Hi @AdityaB-01, I believe you will have more luck posting this issue to: https://github.com/tensorflow/tflite-micro. They are more knowledgeable on Arduino and Arduino code.

github-actions[bot] on (2024-07-24 01:53:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-01 01:57:52 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-01 01:57:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70868"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70868"">No</a>

"
2390743891,issue,closed,completed,Can't load model from model 2.15 in tensorflow 2.15?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I use model.save(""model.keras"") in kaggle using tf 2.15

then I load model in ubuntu 22.0 with python 3.10.12 using tensorflow 2.15
`tf.keras.models.load_model(
        os.path.join('model', 'model.keras'),
        safe_mode=False
    )`

result
`TypeError: Could not deserialize class 'Functional' because its parent module keras.src.models.functional cannot be imported. Full object config: {'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {'name': 'functional_1', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 16, 256, 1], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None, 'name': 'input_layer', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 1]}, 'name': 'conv2d', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 1], 'dtype': 'float32', 'keras_history': ['input_layer', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'batch_normalization', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['conv2d', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 're_lu', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['batch_normalization', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_1', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'conv2d_1', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['re_lu', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'batch_normalization_1', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['conv2d_1', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_1', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 're_lu_1', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['batch_normalization_1', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'max_pooling2d', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['re_lu_1', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_2', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 64]}, 'name': 'conv2d_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 64], 'dtype': 'float32', 'keras_history': ['max_pooling2d', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_2', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'batch_normalization_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['conv2d_2', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_2', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 're_lu_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['batch_normalization_2', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_3', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'conv2d_3', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['re_lu_2', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_3', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'batch_normalization_3', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['conv2d_3', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_3', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 're_lu_3', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['batch_normalization_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_1', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'max_pooling2d_1', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['re_lu_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_4', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 128]}, 'name': 'conv2d_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 128], 'dtype': 'float32', 'keras_history': ['max_pooling2d_1', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'batch_normalization_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['conv2d_4', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_4', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 're_lu_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['batch_normalization_4', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_5', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'conv2d_5', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['re_lu_4', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'batch_normalization_5', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['conv2d_5', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_5', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 're_lu_5', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['batch_normalization_5', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_2', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'max_pooling2d_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['re_lu_5', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_6', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 256]}, 'name': 'conv2d_6', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 256], 'dtype': 'float32', 'keras_history': ['max_pooling2d_2', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_6', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'batch_normalization_6', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['conv2d_6', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_6', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 're_lu_6', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['batch_normalization_6', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_7', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'conv2d_7', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['re_lu_6', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_7', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'batch_normalization_7', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['conv2d_7', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_7', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 're_lu_7', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['batch_normalization_7', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_3', 'trainable': True, 'dtype': 'float32', 'pool_size': [2, 2], 'padding': 'valid', 'strides': [2, 2], 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'max_pooling2d_3', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['re_lu_7', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_8', 'trainable': True, 'dtype': 'float32', 'filters': 1024, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 512]}, 'name': 'conv2d_8', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 512], 'dtype': 'float32', 'keras_history': ['max_pooling2d_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_8', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 'batch_normalization_8', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['conv2d_8', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_8', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 're_lu_8', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['batch_normalization_8', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_9', 'trainable': True, 'dtype': 'float32', 'filters': 1024, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 'conv2d_9', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['re_lu_8', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_9', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 'batch_normalization_9', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['conv2d_9', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_9', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 're_lu_9', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['batch_normalization_9', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2DTranspose', 'config': {'name': 'conv2d_transpose', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': [2, 2], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1, 16, 1024]}, 'name': 'conv2d_transpose', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 1, 16, 1024], 'dtype': 'float32', 'keras_history': ['re_lu_9', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Concatenate', 'config': {'name': 'concatenate', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': [[None, 2, 32, 512], [None, 2, 32, 512]]}, 'name': 'concatenate', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['conv2d_transpose', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['re_lu_7', 0, 0]}}]], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_10', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 1024]}, 'name': 'conv2d_10', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 1024], 'dtype': 'float32', 'keras_history': ['concatenate', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_10', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'batch_normalization_10', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['conv2d_10', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_10', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 're_lu_10', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['batch_normalization_10', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_11', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'conv2d_11', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['re_lu_10', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_11', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'batch_normalization_11', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['conv2d_11', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_11', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 're_lu_11', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['batch_normalization_11', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2DTranspose', 'config': {'name': 'conv2d_transpose_1', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [2, 2], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2, 32, 512]}, 'name': 'conv2d_transpose_1', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2, 32, 512], 'dtype': 'float32', 'keras_history': ['re_lu_11', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Concatenate', 'config': {'name': 'concatenate_1', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': [[None, 4, 64, 256], [None, 4, 64, 256]]}, 'name': 'concatenate_1', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['conv2d_transpose_1', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['re_lu_5', 0, 0]}}]], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_12', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 512]}, 'name': 'conv2d_12', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 512], 'dtype': 'float32', 'keras_history': ['concatenate_1', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_12', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'batch_normalization_12', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['conv2d_12', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_12', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 're_lu_12', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['batch_normalization_12', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_13', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'conv2d_13', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['re_lu_12', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_13', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'batch_normalization_13', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['conv2d_13', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_13', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 're_lu_13', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['batch_normalization_13', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2DTranspose', 'config': {'name': 'conv2d_transpose_2', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': [2, 2], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 64, 256]}, 'name': 'conv2d_transpose_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 4, 64, 256], 'dtype': 'float32', 'keras_history': ['re_lu_13', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Concatenate', 'config': {'name': 'concatenate_2', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': [[None, 8, 128, 128], [None, 8, 128, 128]]}, 'name': 'concatenate_2', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['conv2d_transpose_2', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['re_lu_3', 0, 0]}}]], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_14', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 256]}, 'name': 'conv2d_14', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 256], 'dtype': 'float32', 'keras_history': ['concatenate_2', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_14', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'batch_normalization_14', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['conv2d_14', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_14', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 're_lu_14', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['batch_normalization_14', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_15', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'conv2d_15', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['re_lu_14', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_15', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'batch_normalization_15', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['conv2d_15', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_15', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 're_lu_15', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['batch_normalization_15', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2DTranspose', 'config': {'name': 'conv2d_transpose_3', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [2, 2], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 8, 128, 128]}, 'name': 'conv2d_transpose_3', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 8, 128, 128], 'dtype': 'float32', 'keras_history': ['re_lu_15', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Concatenate', 'config': {'name': 'concatenate_3', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': [[None, 16, 256, 64], [None, 16, 256, 64]]}, 'name': 'concatenate_3', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['conv2d_transpose_3', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['re_lu_1', 0, 0]}}]], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_16', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 128]}, 'name': 'conv2d_16', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 128], 'dtype': 'float32', 'keras_history': ['concatenate_3', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_16', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'batch_normalization_16', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['conv2d_16', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_16', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 're_lu_16', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['batch_normalization_16', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_17', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'conv2d_17', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['re_lu_16', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_17', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'synchronized': False}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'batch_normalization_17', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['conv2d_17', 0, 0]}}], 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'ReLU', 'config': {'name': 're_lu_17', 'trainable': True, 'dtype': 'float32', 'max_value': None, 'negative_slope': 0.0, 'threshold': 0.0}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 're_lu_17', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['batch_normalization_17', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_18', 'trainable': True, 'dtype': 'float32', 'filters': 1, 'kernel_size': [1, 1], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16, 256, 64]}, 'name': 'conv2d_18', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16, 256, 64], 'dtype': 'float32', 'keras_history': ['re_lu_17', 0, 0]}}], 'kwargs': {}}]}], 'input_layers': [['input_layer', 0, 0]], 'output_layers': [['conv2d_18', 0, 0]]}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 9.999999747378752e-05, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'builtins', 'class_name': 'function', 'config': 'sp_custom_loss', 'registered_name': 'function'}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'CosineSimilarity', 'config': {'name': 'cosine', 'dtype': 'float32'}, 'registered_name': None}, {'module': 'keras.metrics', 'class_name': 'MeanAbsoluteError', 'config': {'name': 'MAE', 'dtype': 'float32'}, 'registered_name': None}, {'module': 'keras.metrics', 'class_name': 'MeanSquaredError', 'config': {'name': 'MSE', 'dtype': 'float32'}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}`

### Standalone code to reproduce the issue

```shell
model.save
tf.keras.models.load_model(os.path.join('model', 'model.keras'),safe_mode=False)
```


### Relevant log output

_No response_",hotamago,2024-07-04 12:21:48+00:00,['Venkat6871'],2024-08-13 09:50:24+00:00,2024-07-05 09:37:09+00:00,https://github.com/tensorflow/tensorflow/issues/70866,"[('type:bug', 'Bug'), ('comp:model', 'Model related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2210477250, 'issue_id': 2390743891, 'author': 'Venkat6871', 'body': 'Hi **@hotamago** ,\r\n- Could please provide a full code snippet to reproduce the issue reported here. I tried with another example, It works fine for me. Here i providing [gist](https://colab.research.google.com/gist/Venkat6871/59a99843d1abcfc2cd62686afd401fbc/70866_nightly-v.ipynb) for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 5, 8, 55, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210530028, 'issue_id': 2390743891, 'author': 'hotamago', 'body': '> Hi **@hotamago** ,\r\n> \r\n> * Could please provide a full code snippet to reproduce the issue reported here. I tried with another example, It works fine for me. Here i providing [gist](https://colab.research.google.com/gist/Venkat6871/59a99843d1abcfc2cd62686afd401fbc/70866_nightly-v.ipynb) for reference.\r\n> \r\n> Thank you!\r\n\r\nOh, I tried my code in tf-nightly and get this error:\r\n![image](https://github.com/tensorflow/tensorflow/assets/24850901/38db6037-95c1-49d5-bb84-e3da2d1cc0c1)\r\n\r\nThank you for your help!', 'created_at': datetime.datetime(2024, 7, 5, 9, 27, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210546375, 'issue_id': 2390743891, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70866"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70866"">No</a>', 'created_at': datetime.datetime(2024, 7, 5, 9, 37, 11, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-05 08:55:25 UTC): Hi **@hotamago** ,
- Could please provide a full code snippet to reproduce the issue reported here. I tried with another example, It works fine for me. Here i providing [gist](https://colab.research.google.com/gist/Venkat6871/59a99843d1abcfc2cd62686afd401fbc/70866_nightly-v.ipynb) for reference.

Thank you!

hotamago (Issue Creator) on (2024-07-05 09:27:03 UTC): Oh, I tried my code in tf-nightly and get this error:
![image](https://github.com/tensorflow/tensorflow/assets/24850901/38db6037-95c1-49d5-bb84-e3da2d1cc0c1)

Thank you for your help!

google-ml-butler[bot] on (2024-07-05 09:37:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70866"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70866"">No</a>

"
2390194942,issue,closed,completed,Cannot build the TensorFlow Lite installation package (C++),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.16.2

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

Ubuntu 22.04

### Python version

3.10.12

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'd like to build the installable package of TensorFlow so as to be able to build standalone C++ applications using TensorFlow. They would be built using cmake. I followed the instructions provided [here](https://www.tensorflow.org/lite/guide/build_cmake#build_installable_package). Cmake fails.

Please notice that building the TensorFlow lib itself (not installable) succeeds. The issue only is applicable to the installable package.

I would expect the cmake to complete so as I would be able to execute 'cmake --install .' and install the TensorFlow library along with all necessary packages. If I am wrong here with my assumptions please advise me on installing the Tensorflow C++ runtime to be able to build standalone cmake based applications (x86 and ARM targets).

### Standalone code to reproduce the issue

```shell
cmake ../tensorflow/tensorflow/lite -DTFLITE_ENABLE_INSTALL=ON \
  -DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \
  -DSYSTEM_FARMHASH=ON \
  -DSYSTEM_PTHREADPOOL=ON \
  -Dabsl_DIR=/usr/lib/cmake/absl \
  -DEigen3_DIR=/usr/share/eigen3/cmake \
  -DFlatBuffers_DIR=/usr/lib/cmake/flatbuffers \
  -Dgemmlowp_DIR=/usr/lib/cmake/gemmlowp \
  -DNEON_2_SSE_DIR=/usr/lib/cmake/NEON_2_SSE \
  -Dcpuinfo_DIR=/usr/share/cpuinfo \
  -Druy_DIR=/usr/lib/cmake/ruy
```


### Relevant log output

```shell
-- Configuring done (125.8s)
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_flags"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_hash"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_status"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_strings"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_synchronization"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""absl_variant"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""ruy"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""xnnpack-delegate"" that is not in any export set.
CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""XNNPACK"" that is not in any export set.
CMake Error at /usr/local/lib/cmake/gemmlowp/gemmlowp-config.cmake:69 (set_target_properties):
  The link interface of target ""gemmlowp::eight_bit_int_gemm"" contains:

    Threads::Threads

  but the target was not found.  Possible reasons include:

    * There is a typo in the target name.
    * A find_package call is missing for an IMPORTED target.
    * An ALIAS target is missing.

Call Stack (most recent call first):
  CMakeLists.txt:171 (find_package)


-- Generating done (1.1s)
CMake Generate step failed.  Build files cannot be regenerated correctly.
```
",WojciechRynczuk,2024-07-04 07:51:03+00:00,"['pkgoogle', 'sawantkumar']",2024-07-27 01:51:45+00:00,2024-07-27 01:51:42+00:00,https://github.com/tensorflow/tensorflow/issues/70851,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2226294307, 'issue_id': 2390194942, 'author': 'pkgoogle', 'body': ""Hi @WojciechRynczuk those instructions are supposed to be used with find_package in another cmake project. So after building, your project which integrates TFLite should include the `find_package(tensorflow-lite CONFIG)` line in your project's CMakeLists.txt file. I do not believe these instructions are meant to be used with `cmake --install`. Your CMakeLists.txt file will probably need more to work properly, but that is likely project dependent. Does that answer your question? Thanks."", 'created_at': datetime.datetime(2024, 7, 12, 20, 16, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240829808, 'issue_id': 2390194942, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 20, 1, 50, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703698, 'issue_id': 2390194942, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2253703723, 'issue_id': 2390194942, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70851"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70851"">No</a>', 'created_at': datetime.datetime(2024, 7, 27, 1, 51, 43, tzinfo=datetime.timezone.utc)}]","pkgoogle (Assginee) on (2024-07-12 20:16:05 UTC): Hi @WojciechRynczuk those instructions are supposed to be used with find_package in another cmake project. So after building, your project which integrates TFLite should include the `find_package(tensorflow-lite CONFIG)` line in your project's CMakeLists.txt file. I do not believe these instructions are meant to be used with `cmake --install`. Your CMakeLists.txt file will probably need more to work properly, but that is likely project dependent. Does that answer your question? Thanks.

github-actions[bot] on (2024-07-20 01:50:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-27 01:51:41 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-27 01:51:43 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70851"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70851"">No</a>

"
2390053135,issue,closed,completed,"TensorFlow is not detecting the GPU, whereas PyTorch is successfully identifying it.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA version: 11.8, cuDNN version: 8.7.0.0

### GPU model and memory

NVIDIA GeForce RTX 4060,  8188MiB

### Current behavior?

tensorflow unable to detect GPU, 


### Standalone code to reproduce the issue

```shell
tf.config.list_physical_devices()
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
print(f""Using device: {device}"")
Using device: cuda
```


### Relevant log output

_No response_",buttaRahul,2024-07-04 06:27:38+00:00,['Venkat6871'],2024-07-24 13:59:04+00:00,2024-07-24 13:59:01+00:00,https://github.com/tensorflow/tensorflow/issues/70845,"[('type:bug', 'Bug'), ('comp:gpu', 'GPU related issues'), ('TF 2.16', '')]","[{'comment_id': 2208360145, 'issue_id': 2390053135, 'author': 'sushreebarsa', 'body': '@buttaRahul I was able to detect the GPU in Google colab, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3ee5f24e7acfdcd8c21c83e0b92a85bc/70845.ipynb).\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 4, 8, 5, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208433729, 'issue_id': 2390053135, 'author': 'buttaRahul', 'body': 'Thank you for the response, but tesorflow is unable to detect GPU in my local device\r\nmy tensorflow version : 2.14.0, CUDA version: 11.8, cuDNN version 8.7.0.0 These versions are compatible as per [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu) I would like to know what might be the issue in my device.', 'created_at': datetime.datetime(2024, 7, 4, 8, 43, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2209202540, 'issue_id': 2390053135, 'author': 'FrejaThoresen', 'body': 'I have the same issue on my device with cuDNN 9 (also tested with version 8.7), CUDA 12.3 and tensorflow 2.16.1. Installation works fine with pytorch, but tensorflow can not detect the GPU. \r\n\r\nI see that the installation with pip is installing nvidia libraries, including some related to cuDNN, which may not have the same version as the one installed on the device, could this be a lead?', 'created_at': datetime.datetime(2024, 7, 4, 15, 8, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210306072, 'issue_id': 2390053135, 'author': 'sushreebarsa', 'body': ""@buttaRahul  Could you please verify if your system recognizes the GPU using the NVIDIA System Management Interface:\r\n\r\n```\r\nnvidia-smi\r\n```\r\nPlease ensure that CUDA is correctly installed and the paths are set correctly:\r\n```\r\nnvcc --version\r\n\r\n```\r\nalso verify the cudnn version:\r\n```\r\ncat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\r\n\r\n```\r\n\r\n```\r\npip uninstall tensorflow tensorflow-gpu\r\npip install tensorflow==2.14.0\r\n```\r\nthen install TF GPU and let us know?\r\n\r\n@FrejaThoresen Yes, the installation of NVIDIA libraries via pip that differ from your system's installed versions of CUDA and cuDNN could be causing conflicts. This is a common issue when there are mismatched versions. Please refer [TensorFlow compatibility guide](https://www.tensorflow.org/install/source#gpu) for the same. \r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 5, 6, 57, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2212459281, 'issue_id': 2390053135, 'author': 'mihaimaruseac', 'body': 'Please use TF 2.17, see #63362', 'created_at': datetime.datetime(2024, 7, 7, 13, 55, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229847380, 'issue_id': 2390053135, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 16, 1, 53, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247900053, 'issue_id': 2390053135, 'author': 'FrejaThoresen', 'body': 'I can confirm it works with Python 3.12, Tensorflow 2.17, Cuda 12.3, and cuDNN 8.9.7. I suspect the issue was caused by a mismatch between the pip versions of Nvidia cudnn installations and the cuDNN installation.\r\n\r\nTensorflow finding the GPU\r\n```\r\n>> python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))""\r\n2024-07-24 15:06:12.214574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-07-24 15:06:12.223961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-07-24 15:06:12.226778: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-07-24 15:06:12.233782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-24 15:06:12.794225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1721826373.167169  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1721826373.187865  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1721826373.188006  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\r\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\r\n(tf_py12) freya@agora-linux-ai:~/Documents/Code/thin-slice-classifier$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Wed_Nov_22_10:17:15_PST_2023\r\nCuda compilation tools, release 12.3, V12.3.107\r\nBuild cuda_12.3.r12.3/compiler.33567101_0\r\n```\r\n\r\ncuDNN version\r\n```\r\n>> cat /usr/include/x86_64-linux-gnu/cudnn_v*.h | \r\ngrep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 8\r\n#define CUDNN_MINOR 9\r\n#define CUDNN_PATCHLEVEL 7\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n/* cannot use constexpr here since this is a C-only file */\r\n```\r\n\r\npip nvidia libraries\r\n```\r\n>> pip list | grep nvidia\r\nnvidia-cublas-cu12       12.3.4.1\r\nnvidia-cuda-cupti-cu12   12.3.101\r\nnvidia-cuda-nvcc-cu12    12.3.107\r\nnvidia-cuda-nvrtc-cu12   12.3.107\r\nnvidia-cuda-runtime-cu12 12.3.101\r\nnvidia-cudnn-cu12        8.9.7.29\r\nnvidia-cufft-cu12        11.0.12.1\r\nnvidia-curand-cu12       10.3.4.107\r\nnvidia-cusolver-cu12     11.5.4.101\r\nnvidia-cusparse-cu12     12.2.0.103\r\nnvidia-nccl-cu12         2.19.3\r\nnvidia-nvjitlink-cu12    12.3.101\r\n```\r\n\r\npip tensorflow\r\n```\r\npip list | grep tensor\r\ntensorboard              2.17.0\r\ntensorboard-data-server  0.7.2\r\ntensorflow               2.17.0\r\n```\r\n\r\nDriver version\r\n```\r\n>> nvidia-smi\r\nWed Jul 24 15:11:31 2024       \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA GeForce GTX 1660 ...    On  |   00000000:01:00.0  On |                  N/A |\r\n| 29%   31C    P8              6W /  125W |     126MiB /   6144MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n                                                                                         \r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|    0   N/A  N/A      1255      G   /usr/lib/xorg/Xorg                            116MiB |\r\n|    0   N/A  N/A      1303      G   /usr/bin/gnome-shell                            6MiB |\r\n+-----------------------------------------------------------------------------------------+\r\n```', 'created_at': datetime.datetime(2024, 7, 24, 13, 14, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2248040685, 'issue_id': 2390053135, 'author': 'mihaimaruseac', 'body': 'Since it works on 2.17, moving towards closing this issue. Thank you @FrejaThoresen', 'created_at': datetime.datetime(2024, 7, 24, 13, 59, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2248040816, 'issue_id': 2390053135, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70845"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70845"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 13, 59, 3, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-04 08:05:14 UTC): @buttaRahul I was able to detect the GPU in Google colab, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3ee5f24e7acfdcd8c21c83e0b92a85bc/70845.ipynb).
Thank you!

buttaRahul (Issue Creator) on (2024-07-04 08:43:52 UTC): Thank you for the response, but tesorflow is unable to detect GPU in my local device
my tensorflow version : 2.14.0, CUDA version: 11.8, cuDNN version 8.7.0.0 These versions are compatible as per [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu) I would like to know what might be the issue in my device.

FrejaThoresen on (2024-07-04 15:08:57 UTC): I have the same issue on my device with cuDNN 9 (also tested with version 8.7), CUDA 12.3 and tensorflow 2.16.1. Installation works fine with pytorch, but tensorflow can not detect the GPU. 

I see that the installation with pip is installing nvidia libraries, including some related to cuDNN, which may not have the same version as the one installed on the device, could this be a lead?

sushreebarsa on (2024-07-05 06:57:28 UTC): @buttaRahul  Could you please verify if your system recognizes the GPU using the NVIDIA System Management Interface:

```
nvidia-smi
```
Please ensure that CUDA is correctly installed and the paths are set correctly:
```
nvcc --version

```
also verify the cudnn version:
```
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2

```

```
pip uninstall tensorflow tensorflow-gpu
pip install tensorflow==2.14.0
```
then install TF GPU and let us know?

@FrejaThoresen Yes, the installation of NVIDIA libraries via pip that differ from your system's installed versions of CUDA and cuDNN could be causing conflicts. This is a common issue when there are mismatched versions. Please refer [TensorFlow compatibility guide](https://www.tensorflow.org/install/source#gpu) for the same. 
Thank you!

mihaimaruseac on (2024-07-07 13:55:29 UTC): Please use TF 2.17, see #63362

github-actions[bot] on (2024-07-16 01:53:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

FrejaThoresen on (2024-07-24 13:14:26 UTC): I can confirm it works with Python 3.12, Tensorflow 2.17, Cuda 12.3, and cuDNN 8.9.7. I suspect the issue was caused by a mismatch between the pip versions of Nvidia cudnn installations and the cuDNN installation.

Tensorflow finding the GPU
```
2024-07-24 15:06:12.214574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-24 15:06:12.223961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-24 15:06:12.226778: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-24 15:06:12.233782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-24 15:06:12.794225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1721826373.167169  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1721826373.187865  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1721826373.188006  141358 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
(tf_py12) freya@agora-linux-ai:~/Documents/Code/thin-slice-classifier$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.3, V12.3.107
Build cuda_12.3.r12.3/compiler.33567101_0
```

cuDNN version
```
grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 8
#define CUDNN_MINOR 9
#define CUDNN_PATCHLEVEL 7
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

/* cannot use constexpr here since this is a C-only file */
```

pip nvidia libraries
```
nvidia-cublas-cu12       12.3.4.1
nvidia-cuda-cupti-cu12   12.3.101
nvidia-cuda-nvcc-cu12    12.3.107
nvidia-cuda-nvrtc-cu12   12.3.107
nvidia-cuda-runtime-cu12 12.3.101
nvidia-cudnn-cu12        8.9.7.29
nvidia-cufft-cu12        11.0.12.1
nvidia-curand-cu12       10.3.4.107
nvidia-cusolver-cu12     11.5.4.101
nvidia-cusparse-cu12     12.2.0.103
nvidia-nccl-cu12         2.19.3
nvidia-nvjitlink-cu12    12.3.101
```

pip tensorflow
```
pip list | grep tensor
tensorboard              2.17.0
tensorboard-data-server  0.7.2
tensorflow               2.17.0
```

Driver version
```
Wed Jul 24 15:11:31 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1660 ...    On  |   00000000:01:00.0  On |                  N/A |
| 29%   31C    P8              6W /  125W |     126MiB /   6144MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1255      G   /usr/lib/xorg/Xorg                            116MiB |
|    0   N/A  N/A      1303      G   /usr/bin/gnome-shell                            6MiB |
+-----------------------------------------------------------------------------------------+
```

mihaimaruseac on (2024-07-24 13:59:01 UTC): Since it works on 2.17, moving towards closing this issue. Thank you @FrejaThoresen

google-ml-butler[bot] on (2024-07-24 13:59:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70845"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70845"">No</a>

"
2389965883,issue,closed,completed,TFlite on xtensa lx7,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux, ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,
I need to compile tensorflow lite on xtensa lx7 platform. Currently, I used cross compilation information from tflite documentation but build errors (file not found errors, undefined references, syntax errors) are generated.
Please provide instruction on the build steps.




### Standalone code to reproduce the issue

```shell
cross compilation :
cmake   -DCMAKE_C_COMPILER=${XTENSA_PREFIX}/clang   -DCMAKE_CXX_COMPILER=${XTENSA_PREFIX}/clang++   -DCMAKE_C_FLAGS=""${XTENSA_FLAGS}""   -DCMAKE_CXX_FLAGS=""${XTENSA_FLAGS}""   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   -DCMAKE_SYSTEM_NAME=Linux   -DCMAKE_SYSTEM_PROCESSOR=xtensa   -DCMAKE_THREAD_LIBS_INIT=""-lpthread""   -DCMAKE_HAVE_THREADS_LIBRARY=1   -DCMAKE_USE_PTHREADS_INIT=1   -DTHREADS_PREFER_PTHREAD_FLAG=ON   
../tensorflow_src/tensorflow/lite

build:
cmake --build . -j
```


### Relevant log output

_No response_",devapriyas2001,2024-07-04 05:22:18+00:00,['pkgoogle'],2024-08-17 01:51:59+00:00,2024-08-17 01:51:56+00:00,https://github.com/tensorflow/tensorflow/issues/70842,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('comp:micro', 'Related to TensorFlow Lite Microcontrollers'), ('TF 2.16', '')]","[{'comment_id': 2210308784, 'issue_id': 2389965883, 'author': 'Venkat6871', 'body': ""Hi **@devapriyas2001** ,\r\nThe error you are facing is due to compatibility issue between EDK version of your device and TFLite. Could you please make sure the EDK version is compatible with the TFLite version you're targeting and the build includes the necessary header files in the path from Xtensa libraries and TFLite. For faster resolution could you please post this issue on [tflite/micro](https://github.com/tensorflow/tflite-micro).\r\n\r\nThank You!"", 'created_at': datetime.datetime(2024, 7, 5, 6, 59, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2219587955, 'issue_id': 2389965883, 'author': 'devapriyas2001', 'body': ""Hi @Venkat6871 ,\r\nHow to find the compatible EDK version for the device and TFlite?\r\nI posted the query on https://github.com/tensorflow/tflite-micro but didn't get the desired response.\r\nThanks!"", 'created_at': datetime.datetime(2024, 7, 10, 5, 24, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263642995, 'issue_id': 2389965883, 'author': 'pkgoogle', 'body': 'Hi @devapriyas2001, do you have a link to your query & response so that we may gain better context on what the issue may be?', 'created_at': datetime.datetime(2024, 8, 1, 17, 56, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277005295, 'issue_id': 2389965883, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 9, 1, 55, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527351, 'issue_id': 2389965883, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294527409, 'issue_id': 2389965883, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70842"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70842"">No</a>', 'created_at': datetime.datetime(2024, 8, 17, 1, 51, 58, tzinfo=datetime.timezone.utc)}]","Venkat6871 on (2024-07-05 06:59:41 UTC): Hi **@devapriyas2001** ,
The error you are facing is due to compatibility issue between EDK version of your device and TFLite. Could you please make sure the EDK version is compatible with the TFLite version you're targeting and the build includes the necessary header files in the path from Xtensa libraries and TFLite. For faster resolution could you please post this issue on [tflite/micro](https://github.com/tensorflow/tflite-micro).

Thank You!

devapriyas2001 (Issue Creator) on (2024-07-10 05:24:10 UTC): Hi @Venkat6871 ,
How to find the compatible EDK version for the device and TFlite?
I posted the query on https://github.com/tensorflow/tflite-micro but didn't get the desired response.
Thanks!

pkgoogle (Assginee) on (2024-08-01 17:56:08 UTC): Hi @devapriyas2001, do you have a link to your query & response so that we may gain better context on what the issue may be?

github-actions[bot] on (2024-08-09 01:55:14 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-17 01:51:56 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-17 01:51:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70842"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70842"">No</a>

"
2389918850,issue,closed,completed,movenet = Movenet('movenet_thunder') not working,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

google ocllab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When am trying to use the Movnet model, it is showing some error - 
movenet = Movenet('movenet_thunder')

### Standalone code to reproduce the issue

```shell
Used movnet for pose estimation , but the model is failing -
# Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('movenet_thunder')
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
<ipython-input-16-85ae69c878c7> in <cell line: 11>()
      9 from data import BodyPart
     10 from ml import Movenet
---> 11 movenet = Movenet('movenet_thunder')
     12 
     13 # Define function to run pose estimation using MoveNet Thunder.

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering)
    462           x for x in self._custom_op_registerers if not isinstance(x, str)
    463       ]
--> 464       self._interpreter = _interpreter_wrapper.CreateWrapperFromFile(
    465           model_path,
    466           op_resolver_id,

ValueError: Mmap of '41' at offset '0' failed with error '22'.
```
",durgas4,2024-07-04 04:36:05+00:00,['sawantkumar'],2024-07-18 10:55:36+00:00,2024-07-18 10:55:33+00:00,https://github.com/tensorflow/tensorflow/issues/70841,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.15', 'For issues related to 2.15.x')]","[{'comment_id': 2210254299, 'issue_id': 2389918850, 'author': 'tilakrayal', 'body': '@durgas4,\r\nCould you please provide the complete code or the colab gist to reproduce the issue which helps to debug the issue in an effective way. Thank you!', 'created_at': datetime.datetime(2024, 7, 5, 6, 20, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210337068, 'issue_id': 2389918850, 'author': 'durgas4', 'body': 'The code is here-\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/movenet.ipynb#scrollTo=SYFdK-JHYhrv\r\n\r\nThis part-\r\nLoad Model from TF hub', 'created_at': datetime.datetime(2024, 7, 5, 7, 21, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232625746, 'issue_id': 2389918850, 'author': 'sawantkumar', 'body': 'Hi @durgas4 ,\r\n\r\nI ran the notebook from this same link but it worked fine. Can you please retry and let me know if it works?', 'created_at': datetime.datetime(2024, 7, 17, 7, 31, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236212157, 'issue_id': 2389918850, 'author': 'durgas4', 'body': ""Working fine, thanks!  i forgot the following code line -\r\n# Download model from TF Hub and check out inference code from GitHub\r\n!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\r\n!git clone https://github.com/tensorflow/examples.git\r\npose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\r\nsys.path.append(pose_sample_rpi_path)\r\n\r\nAnd directly went to this :)\r\n# Load MoveNet Thunder model\r\nimport utils\r\nfrom data import BodyPart\r\nfrom ml import Movenet\r\nmovenet = Movenet('movenet_thunder')"", 'created_at': datetime.datetime(2024, 7, 18, 10, 55, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2236212226, 'issue_id': 2389918850, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70841"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70841"">No</a>', 'created_at': datetime.datetime(2024, 7, 18, 10, 55, 35, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-05 06:20:10 UTC): @durgas4,
Could you please provide the complete code or the colab gist to reproduce the issue which helps to debug the issue in an effective way. Thank you!

durgas4 (Issue Creator) on (2024-07-05 07:21:47 UTC): The code is here-
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/movenet.ipynb#scrollTo=SYFdK-JHYhrv

This part-
Load Model from TF hub

sawantkumar (Assginee) on (2024-07-17 07:31:53 UTC): Hi @durgas4 ,

I ran the notebook from this same link but it worked fine. Can you please retry and let me know if it works?

durgas4 (Issue Creator) on (2024-07-18 10:55:33 UTC): Working fine, thanks!  i forgot the following code line -
# Download model from TF Hub and check out inference code from GitHub
!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
!git clone https://github.com/tensorflow/examples.git
pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')
sys.path.append(pose_sample_rpi_path)

And directly went to this :)
# Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('movenet_thunder')

google-ml-butler[bot] on (2024-07-18 10:55:35 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70841"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70841"">No</a>

"
2389428704,issue,closed,completed,TPU unresolvable on google colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when trying to run:
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()

I get:
ValueError: Please provide a TPU Name to connect to.

Im getting this while running on a colab TPU instance.
documentation states:
""A string corresponding to the TPU to use. It can be the TPU name or TPU worker gRPC address. If not set, it will try automatically resolve the TPU address on Cloud TPUs. If set to ""local"", it will assume that the TPU is directly connected to the VM instead of over the network.""
- https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver



P.S. Please help me Ive been dealing with OOMs for the last 5 months because of things like this please end my suffering


### Standalone code to reproduce the issue

```shell
!pip install datasets huggingface_hub transformers tf-models-official

import pandas as pd
import numpy as np
import tensorflow as tf
#import keras_nlp as knlp
import tensorflow_models as tfm

tpu = tf.distribute.cluster_resolver.TPUClusterResolver()

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

strategy = tf.distribute.TPUStrategy(tpu)


print(f""Available number of replicas: {strategy.num_replicas_in_sync}"")
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-ae74ec612ba0> in <cell line: 1>()
----> 1 tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
      2 
      3 tf.config.experimental_connect_to_cluster(tpu)
      4 tf.tpu.experimental.initialize_tpu_system(tpu)
      5 

1 frames
/usr/local/lib/python3.10/dist-packages/cloud_tpu_client/client.py in __init__(self, tpu, zone, project, credentials, service, discovery_url)
    137 
    138     if tpu is None:
--> 139       raise ValueError('Please provide a TPU Name to connect to.')
    140 
    141     self._tpu = _as_text(tpu)

ValueError: Please provide a TPU Name to connect to.
```
",Leo-Lifeblood,2024-07-03 20:38:11+00:00,['tilakrayal'],2024-12-26 14:02:21+00:00,2024-08-06 01:53:39+00:00,https://github.com/tensorflow/tensorflow/issues/70827,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:tpus', 'tpu, tpuestimator'), ('TF 2.16', '')]","[{'comment_id': 2208341682, 'issue_id': 2389428704, 'author': 'sushreebarsa', 'body': '@Leo-Lifeblood Please make sure to run this code in an environment that supports TPU, such as Google Colab with TPU runtime enabled?  I tried to run the code by modifying it, please have a look at this [gist](https://colab.research.google.com/gist/sushreebarsa/6ec53a5b175b0fe441087b43094d304b/70827.ipynb).\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 4, 7, 54, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210483902, 'issue_id': 2389428704, 'author': 'Leo-Lifeblood', 'body': 'I did run it on a colander runtime with TPU enabled thats how I got the error in the first place, Im writing this now on my phone so I cant check your code but what did you change to make it work? Any obvious thing I did wrong?', 'created_at': datetime.datetime(2024, 7, 5, 8, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213133327, 'issue_id': 2389428704, 'author': 'sushreebarsa', 'body': '@Leo-Lifeblood Please try restarting the runtime and let us know?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 8, 6, 28, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2214102126, 'issue_id': 2389428704, 'author': 'apehex', 'body': ""I have the same issue:\r\n\r\n- on Google Colab TPU (yes it is selected, restarted etc)\r\n- listing devices with both `tf.config.list_logical_devices('TPU')` and `tf.config.list_physical_devices('TPU')`\r\n\r\nThe notebook you linked cannot find any logical nor physical TPU either.\r\n\r\nAlso installing `tensorflow-tpu` fails because of a dependency conflic:\r\n\r\n```\r\nThe conflict is caused by:\r\n    tensorflow-tpu 2.16.2 depends on libtpu==2.16.0rc0\r\n    tensorflow-tpu 2.16.1 depends on libtpu==2.16.0rc0\r\n```\r\n\r\nI have run many successful model training with TF 2.15 on TPUs in Google Colab.\r\nNow that I turn to Keras 3 which requires tensorflow 2.16 it fails."", 'created_at': datetime.datetime(2024, 7, 8, 13, 37, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242104126, 'issue_id': 2389428704, 'author': 'tilakrayal', 'body': '@Leo-Lifeblood \r\nLooks like the issue is caused by ""pip install tf-models-official"". TPUs are found without a problem if not installing the package. This doesn\'t seem to be an issue with tf.distribute. Kindly recheck and raise the issue on tensorflow/models repo. Thank you!', 'created_at': datetime.datetime(2024, 7, 22, 5, 13, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243384662, 'issue_id': 2389428704, 'author': 'poulsbo', 'body': '@Leo-Lifeblood please see the previous comment -- it was actually meant for you, not for me.', 'created_at': datetime.datetime(2024, 7, 22, 16, 41, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257309480, 'issue_id': 2389428704, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 30, 1, 53, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213530, 'issue_id': 2389428704, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213568, 'issue_id': 2389428704, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70827"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70827"">No</a>', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562779794, 'issue_id': 2389428704, 'author': 'okbalefthanded', 'body': 'In Colab the TPU runtimes are still using Tensorflow 2.15 and Keras 2.15, any attempt to use Keras 3 will fail.\r\nPer this issue listing in colab repo, the last native upgrade for both TF and Keras supports only CPU and GPU runtimes.\r\nhttps://github.com/googlecolab/colabtools/issues/4744', 'created_at': datetime.datetime(2024, 12, 26, 14, 2, 20, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-04 07:54:46 UTC): @Leo-Lifeblood Please make sure to run this code in an environment that supports TPU, such as Google Colab with TPU runtime enabled?  I tried to run the code by modifying it, please have a look at this [gist](https://colab.research.google.com/gist/sushreebarsa/6ec53a5b175b0fe441087b43094d304b/70827.ipynb).
Thank you!

Leo-Lifeblood (Issue Creator) on (2024-07-05 08:59:27 UTC): I did run it on a colander runtime with TPU enabled thats how I got the error in the first place, Im writing this now on my phone so I cant check your code but what did you change to make it work? Any obvious thing I did wrong?

sushreebarsa on (2024-07-08 06:28:08 UTC): @Leo-Lifeblood Please try restarting the runtime and let us know?
Thank you!

apehex on (2024-07-08 13:37:41 UTC): I have the same issue:

- on Google Colab TPU (yes it is selected, restarted etc)
- listing devices with both `tf.config.list_logical_devices('TPU')` and `tf.config.list_physical_devices('TPU')`

The notebook you linked cannot find any logical nor physical TPU either.

Also installing `tensorflow-tpu` fails because of a dependency conflic:

```
The conflict is caused by:
    tensorflow-tpu 2.16.2 depends on libtpu==2.16.0rc0
    tensorflow-tpu 2.16.1 depends on libtpu==2.16.0rc0
```

I have run many successful model training with TF 2.15 on TPUs in Google Colab.
Now that I turn to Keras 3 which requires tensorflow 2.16 it fails.

tilakrayal (Assginee) on (2024-07-22 05:13:12 UTC): @Leo-Lifeblood 
Looks like the issue is caused by ""pip install tf-models-official"". TPUs are found without a problem if not installing the package. This doesn't seem to be an issue with tf.distribute. Kindly recheck and raise the issue on tensorflow/models repo. Thank you!

poulsbo on (2024-07-22 16:41:30 UTC): @Leo-Lifeblood please see the previous comment -- it was actually meant for you, not for me.

github-actions[bot] on (2024-07-30 01:53:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-06 01:53:39 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-06 01:53:41 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70827"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70827"">No</a>

okbalefthanded on (2024-12-26 14:02:20 UTC): In Colab the TPU runtimes are still using Tensorflow 2.15 and Keras 2.15, any attempt to use Keras 3 will fail.
Per this issue listing in colab repo, the last native upgrade for both TF and Keras supports only CPU and GPU runtimes.
https://github.com/googlecolab/colabtools/issues/4744

"
2389372791,issue,closed,completed,Importing tensorflow_model_optimization causes error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100

### Current behavior?

on running:

import pandas as pd
import numpy as np
import tensorflow as tf
import keras_nlp as knlp
from tensorflow_model_optimization.quantization.keras import quantize_model

I get:

AttributeError: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'

### Standalone code to reproduce the issue

```shell
!pip install tensorflow
!pip install keras-nlp datasets huggingface_hub transformers tensorflow-model-optimization

import pandas as pd
import numpy as np
import tensorflow as tf
import keras_nlp as knlp
from tensorflow_model_optimization.quantization.keras import quantize_model
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-1ccfaf353d00> in <cell line: 6>()
      4 import keras_nlp as knlp
      5 from keras.preprocessing.sequence import pad_sequences
----> 6 from tensorflow_model_optimization.quantization.keras import quantize_model

20 frames
/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/__init__.py in <module>
     84 from tensorflow_model_optimization.python.core import version
     85 
---> 86 from tensorflow_model_optimization.python.core.api import clustering
     87 from tensorflow_model_optimization.python.core.api import experimental
     88 from tensorflow_model_optimization.python.core.api import quantization

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/api/__init__.py in <module>
     14 # ==============================================================================
     15 """"""Import API modules for Tensorflow Model Optimization.""""""
---> 16 from tensorflow_model_optimization.python.core.api import clustering
     17 from tensorflow_model_optimization.python.core.api import experimental
     18 from tensorflow_model_optimization.python.core.api import quantization

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py in <module>
     14 # ==============================================================================
     15 """"""Module containing code for clustering.""""""
---> 16 from tensorflow_model_optimization.python.core.api.clustering import keras

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py in <module>
     17 from tensorflow_model_optimization.python.core.clustering.keras import experimental
     18 
---> 19 from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_scope
     20 from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_weights
     21 from tensorflow_model_optimization.python.core.clustering.keras.cluster import strip_clustering

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py in <module>
     20 
     21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config
---> 22 from tensorflow_model_optimization.python.core.clustering.keras import cluster_wrapper
     23 from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids
     24 from tensorflow_model_optimization.python.core.keras.compat import keras

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py in <module>
     21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config
     22 from tensorflow_model_optimization.python.core.clustering.keras import clusterable_layer
---> 23 from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids
     24 from tensorflow_model_optimization.python.core.clustering.keras import clustering_registry
     25 from tensorflow_model_optimization.python.core.keras.compat import keras

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_centroids.py in <module>
     20 from tensorflow.python.ops import clustering_ops
     21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config
---> 22 from tensorflow_model_optimization.python.core.keras.compat import keras
     23 
     24 

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/keras/compat.py in <module>
     39 
     40 
---> 41 keras = _get_keras_instance()
     42 
     43 def assign(ref, value, name=None):

/usr/local/lib/python3.10/dist-packages/tensorflow_model_optimization/python/core/keras/compat.py in _get_keras_instance()
     33   version_fn = getattr(tf.keras, 'version', None)
     34   if version_fn and version_fn().startswith('3.'):
---> 35     import tf_keras as keras_internal  # pylint: disable=g-import-not-at-top,unused-import
     36   else:
     37     keras_internal = tf.keras

/usr/local/lib/python3.10/dist-packages/tf_keras/__init__.py in <module>
      1 """"""AUTOGENERATED. DO NOT EDIT.""""""
      2 
----> 3 from tf_keras import __internal__
      4 from tf_keras import activations
      5 from tf_keras import applications

/usr/local/lib/python3.10/dist-packages/tf_keras/__internal__/__init__.py in <module>
      4 from tf_keras.__internal__ import layers
      5 from tf_keras.__internal__ import losses
----> 6 from tf_keras.__internal__ import models
      7 from tf_keras.__internal__ import optimizers
      8 from tf_keras.__internal__ import utils

/usr/local/lib/python3.10/dist-packages/tf_keras/__internal__/models/__init__.py in <module>
      1 """"""AUTOGENERATED. DO NOT EDIT.""""""
      2 
----> 3 from tf_keras.src.models.cloning import clone_and_build_model
      4 from tf_keras.src.models.cloning import in_place_subclassed_model_state_restoration

/usr/local/lib/python3.10/dist-packages/tf_keras/src/__init__.py in <module>
     19 """"""
     20 
---> 21 from tf_keras.src import applications
     22 from tf_keras.src import distribute
     23 from tf_keras.src import models

/usr/local/lib/python3.10/dist-packages/tf_keras/src/applications/__init__.py in <module>
     16 
     17 
---> 18 from tf_keras.src.applications.convnext import ConvNeXtBase
     19 from tf_keras.src.applications.convnext import ConvNeXtLarge
     20 from tf_keras.src.applications.convnext import ConvNeXtSmall

/usr/local/lib/python3.10/dist-packages/tf_keras/src/applications/convnext.py in <module>
     31 from tf_keras.src import utils
     32 from tf_keras.src.applications import imagenet_utils
---> 33 from tf_keras.src.engine import sequential
     34 from tf_keras.src.engine import training as training_lib
     35 

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/sequential.py in <module>
     22 from tf_keras.src import layers as layer_module
     23 from tf_keras.src.engine import base_layer
---> 24 from tf_keras.src.engine import functional
     25 from tf_keras.src.engine import input_layer
     26 from tf_keras.src.engine import training

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/functional.py in <module>
     31 from tf_keras.src.engine import input_spec
     32 from tf_keras.src.engine import node as node_module
---> 33 from tf_keras.src.engine import training as training_lib
     34 from tf_keras.src.engine import training_utils
     35 from tf_keras.src.saving import serialization_lib

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py in <module>
     46 from tf_keras.src.optimizers import optimizer_v1
     47 from tf_keras.src.saving import pickle_utils
---> 48 from tf_keras.src.saving import saving_api
     49 from tf_keras.src.saving import saving_lib
     50 from tf_keras.src.saving import serialization_lib

/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_api.py in <module>
     23 
     24 from tf_keras.src.saving import saving_lib
---> 25 from tf_keras.src.saving.legacy import save as legacy_sm_saving_lib
     26 from tf_keras.src.utils import io_utils
     27 

/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/legacy/save.py in <module>
     25 from tf_keras.src.saving.legacy import serialization
     26 from tf_keras.src.saving.legacy.saved_model import load as saved_model_load
---> 27 from tf_keras.src.saving.legacy.saved_model import load_context
     28 from tf_keras.src.saving.legacy.saved_model import save as saved_model_save
     29 from tf_keras.src.saving.legacy.saved_model.utils import keras_option_scope

/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/legacy/saved_model/load_context.py in <module>
     66 
     67 
---> 68 tf.__internal__.register_load_context_function(in_load_context)
     69 

AttributeError: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'
```
",Leo-Lifeblood,2024-07-03 20:04:27+00:00,['tilakrayal'],2024-07-24 01:53:18+00:00,2024-07-24 01:53:14+00:00,https://github.com/tensorflow/tensorflow/issues/70825,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('ModelOptimizationToolkit', 'TF Model Optimization Toolkit'), ('TF 2.16', '')]","[{'comment_id': 2213096427, 'issue_id': 2389372791, 'author': 'tilakrayal', 'body': '@Leo-Lifeblood,\r\nI tried with the alternative approach using the latest tensorflow version and it was working as expected without any error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/e38f358afad1d9ca788988a0e20a38e5/untitled1992.ipynb) for the reference. Thank you!', 'created_at': datetime.datetime(2024, 7, 8, 6, 3, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229847460, 'issue_id': 2389372791, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 16, 1, 53, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710330, 'issue_id': 2389372791, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710383, 'issue_id': 2389372791, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70825"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70825"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 16, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-08 06:03:38 UTC): @Leo-Lifeblood,
I tried with the alternative approach using the latest tensorflow version and it was working as expected without any error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/e38f358afad1d9ca788988a0e20a38e5/untitled1992.ipynb) for the reference. Thank you!

github-actions[bot] on (2024-07-16 01:53:33 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-24 01:53:14 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-24 01:53:16 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70825"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70825"">No</a>

"
2388820824,issue,closed,completed,TFLite initializes Coral device but still runs inference on CPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a model which I wish to run on RPi-4 with Coral USB device connected to it. I've set the context manager verbosity to max. I can see that the library is communicating with the device, however, in the end, it still runs inference on CPU. I have verified this by measuring time and checking the CPU utilization.

Notice: ""INFO: Created TensorFlow Lite XNNPACK delegate for CPU."" in the middle of the logs.

### Standalone code to reproduce the issue

```shell
#include <memory>
#include <iostream>
#include ""tensorflow/lite/core/interpreter.h""
#include ""tensorflow/lite/core/model_builder.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tflite/public/edgetpu.h""
#include ""flatbuffers/flatbuffers.h""
#include <chrono>

using namespace std::chrono;

std::unique_ptr<tflite::Interpreter> BuildEdgeTpuInterpreter(
    const tflite::FlatBufferModel &model,
    edgetpu::EdgeTpuContext *edgetpu_context)
{
    tflite::ops::builtin::BuiltinOpResolver resolver;
    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());
    std::unique_ptr<tflite::Interpreter> interpreter;
    if (tflite::InterpreterBuilder(model, resolver)(&interpreter) != kTfLiteOk)
    {
        std::cerr << ""Failed to build interpreter."" << std::endl;
    }
    // Bind given context with interpreter.
    interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);
    interpreter->SetNumThreads(-1);
    if (interpreter->AllocateTensors() != kTfLiteOk)
    {
        std::cerr << ""Failed to allocate tensors."" << std::endl;
    }
    return interpreter;
}

std::unique_ptr<tflite::Interpreter> Initialize()
{
    const std::string model_path = ""converted_model.tflite"";
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(model_path.c_str());

    edgetpu::EdgeTpuManager::GetSingleton()->SetVerbosity(10);

    std::shared_ptr<edgetpu::EdgeTpuContext> edgetpu_context =
        edgetpu::EdgeTpuManager::GetSingleton()->OpenDevice();

    std::unique_ptr<tflite::Interpreter> model_interpreter =
        BuildEdgeTpuInterpreter(*model, edgetpu_context.get());

    return model_interpreter;
}

int main()
{
    std::cout << ""Hello world!"" << std::endl;
    std::unique_ptr<tflite::Interpreter> interpreter = Initialize();
    // Fill `input`

    auto start = high_resolution_clock::now();

    interpreter->Invoke();

    auto stop = high_resolution_clock::now();
    auto duration = duration_cast<microseconds>(stop - start);
    std::cout << duration.count() << std::endl;
    float *output = interpreter->typed_output_tensor<float>(0);
    std::cout << output[0] << std::endl;

    return 0;
}
```


### Relevant log output

```shell
Hello world!
I tflite/edgetpu_manager_direct.cc:453] No matching device is already opened for shared ownership.
I driver/driver_factory_default.cc:31] Failed to open /sys/class/apex: No such file or directory
I driver/usb/local_usb_device.cc:944] EnumerateDevices: vendor:0x1a6e, product:0x89a
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[2] port[0]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[4]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[0]
I driver/usb/local_usb_device.cc:944] EnumerateDevices: vendor:0x18d1, product:0x9302
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[2] port[0]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[4]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:998] EnumerateDevices: found [/sys/bus/usb/devices/1-1.3.3]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:979] EnumerateDevices: checking bus[1] port[0]
I driver/beagle/beagle_usb_driver_provider.cc:225] Enumerate: adding path [/sys/bus/usb/devices/1-1.3.3]
I tflite/edgetpu_manager_direct.cc:471] No device of type Apex (PCIe) is available.
I tflite/edgetpu_context_direct.cc:106] USB always DFU: False (default)
I tflite/edgetpu_context_direct.cc:147] USB bulk-in queue capacity: 8
I tflite/edgetpu_context_direct.cc:63] Performance expectation: High when USB connected EdgeTpu is throttled
I driver/usb/usb_driver.cc:1383] Open device and check if DFU is needed
I driver/usb/local_usb_device.cc:1013] OpenDevice: [/sys/bus/usb/devices/1-1.3.3]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[2] port[0]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[4]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:1081] OpenDevice: device opened 0x55b429dd60
I driver/usb/local_usb_device.cc:184] LocalUsbDevice
I driver/usb/usb_standard_commands.cc:36] UsbStandardCommands
I driver/usb/usb_dfu_commands.cc:37] UsbDfuCommands
I driver/usb/usb_standard_commands.cc:43] GetDeviceDescriptor
I driver/usb/local_usb_device.cc:398] GetDescriptor
I driver/usb/usb_standard_commands.cc:78] Vender ID: 0x18d1
I driver/usb/usb_standard_commands.cc:79] Product ID: 0x9302
I driver/usb/usb_driver.cc:1410] Device is already in application mode, skipping DFU
I driver/usb/usb_driver.cc:1422] Resetting device
I driver/usb/local_usb_device.cc:243] Close: closing device 0x55b429dd60 
I driver/usb/local_usb_device.cc:216] DoCancelAllTransfers: cancelling 0 async transfers
I driver/usb/local_usb_device.cc:224] DoCancelAllTransfers: waiting for all async transfers to complete
I driver/usb/local_usb_device.cc:234] DoCancelAllTransfers: all async transfers have completed
I driver/usb/local_usb_device.cc:276] Close: releasing 0 transfer buffers
I driver/usb/local_usb_device.cc:289] Close: performing graceful reset
I driver/usb/local_usb_device.cc:322] Close: final clean up completed
I driver/usb/usb_driver.cc:1364] Opening device expecting application mode
I driver/usb/local_usb_device.cc:1013] OpenDevice: [/sys/bus/usb/devices/1-1.3.3]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[2] port[0]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[2]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[1]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[4]
I driver/usb/local_usb_device.cc:1050] OpenDevice: checking bus[1] port[3]
I driver/usb/local_usb_device.cc:1081] OpenDevice: device opened 0x55b42ba410
I driver/usb/local_usb_device.cc:184] LocalUsbDevice
I driver/usb/usb_standard_commands.cc:36] UsbStandardCommands
I driver/usb/usb_ml_commands.cc:47] UsbMlCommands
I driver/usb/usb_dfu_commands.cc:40] ~UsbDfuCommands
I driver/usb/usb_standard_commands.cc:39] ~UsbStandardCommands
I driver/usb/local_usb_device.cc:196] ~LocalUsbDevice
I driver/usb/local_usb_device.cc:243] Close: closing device (nil) 
I driver/usb/local_usb_device.cc:352] ClaimInterface
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a30c
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A30C] == 0xF0059
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A30C] := 0xF0059
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a314
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A314] == 0x110000
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a318
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A318] == 0x50C50258
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a318
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A318] == 0x50C50258
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A318] := 0x50850258
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a318
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A318] == 0x50850000
I driver/usb/usb_ml_commands.cc:117] ReadRegister64 offset 0x44018
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:147] ReadRegister64 [0x44018] == 0x0
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4A000] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x48788] := 0x7F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:117] ReadRegister64 offset 0x48788
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:147] ReadRegister64 [0x48788] == 0x7F
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40020] := 0x1E02
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a314
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A314] == 0x110000
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A314] := 0x150000
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a000
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A000] == 0x219089A
I driver/usb/usb_driver.cc:321] e-fuse programming revision: 2
I driver/usb/usb_driver.cc:328] InitializeChip Enabling only sc host interrupt descriptors
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C148] := 0xF0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_driver.cc:341] InitializeChip Enabling single EP mode
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C160] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_driver.cc:355] InitializeChip Setting 256B chunk for USB 2 High Speed
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C058] := 0x20
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44018] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44158] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44198] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x441D8] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44218] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x48788] := 0x7F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:117] ReadRegister64 offset 0x48788
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:147] ReadRegister64 [0x48788] == 0x7F
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x400C0] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40150] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40110] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40250] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40298] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x402E0] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40328] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40190] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x401D0] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40210] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C060] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C070] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C080] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C090] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C0A0] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a0d4
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A0D4] == 0x1
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A0D4] := 0x80000001
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a704
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A704] == 0x70007F
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A704] := 0x7F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a33c
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A33C] == 0xC003F
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A33C] := 0x3F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A500] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A600] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A558] := 0x3
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A658] := 0x3
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a0d8
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A0D8] == 0x0
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A0D8] := 0x80000000
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_driver.cc:1575] Reducing bulk-in request size to 256 bytes for USB2
I tflite/edgetpu_context_direct.cc:174] Opening device at /sys/bus/usb/devices/1-1.3.3
I driver/usb/usb_driver.cc:1155] WorkerThreadFunc starting worker thread
I driver/usb/usb_driver.cc:1174] WorkerThreadFunc dispatching 0 callback events in worker thread
I driver/usb/usb_driver.cc:1210] WorkerThreadFunc Re-installing event reader
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 2 begin
I driver/usb/usb_driver.cc:1232] WorkerThreadFunc Re-installing interrupt reader
I driver/usb/local_usb_device.cc:785] AsyncInterruptInTransfer
I driver/usb/local_usb_device.cc:798] ASYNC IN 3 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [0]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [1]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [2]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [3]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [4]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [5]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [6]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1260] WorkerThreadFunc Installing bulk-in reader. buffer index [7]
I driver/usb/local_usb_device.cc:748] AsyncBulkInTransfer
I driver/usb/local_usb_device.cc:761] ASYNC IN 1 begin
I driver/usb/usb_driver.cc:1317] WorkerThreadFunc waiting on state change
I driver/usb/usb_driver.cc:91] Unlocks both mutex
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
I tflite/edgetpu_manager_direct.cc:226] Releasing Edge TPU device at /sys/bus/usb/devices/1-1.3.3
I tflite/edgetpu_context_direct.cc:180] Closing Edge TPU device at /sys/bus/usb/devices/1-1.3.3
I driver/usb/local_usb_device.cc:216] DoCancelAllTransfers: cancelling 10 async transfers
I driver/usb/usb_driver.cc:86] lock (does nothing)
I driver/usb/usb_driver.cc:1322] WorkerThreadFunc driver state change detected
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:224] DoCancelAllTransfers: waiting for all async transfers to complete
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 3 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 1 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:672] ASYNC IN 2 end
I driver/usb/local_usb_device.cc:97] ConvertLibUsbTransferStatus: USB transfer error 3 [LibUsbDataInCallback]
I driver/usb/local_usb_device.cc:642] UnregisterCompletedTransfer
I driver/usb/local_usb_device.cc:234] DoCancelAllTransfers: all async transfers have completed
I driver/usb/usb_driver.cc:1174] WorkerThreadFunc dispatching 10 callback events in worker thread
I driver/usb/usb_driver.cc:466] HandleInterrupt cancelled, ignore.
I driver/usb/usb_driver.cc:404] HandleEvent cancelled, ignore.
I driver/usb/usb_driver.cc:1194] All bulk-in buffers are available
I driver/usb/usb_driver.cc:1201] Driver is closing, and all async operations have completed.
I driver/usb/usb_driver.cc:1330] WorkerThreadFunc leaving worker thread
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C070] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C080] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C090] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C0A0] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a0d4
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A0D4] == 0x80000001
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A0D4] := 0x1
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a704
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A704] == 0x7F
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A704] := 0x70007F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a33c
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A33C] == 0x3F
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A33C] := 0xC003F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A500] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A600] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A558] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A658] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a0d8
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A0D8] == 0x80000000
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A0D8] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x4C060] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44018] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44158] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44198] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x441D8] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x44218] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x48788] := 0x7F
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:117] ReadRegister64 offset 0x48788
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:147] ReadRegister64 [0x48788] == 0x7F
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x400C0] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40150] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40110] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40250] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40298] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x402E0] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40328] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40190] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x401D0] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:178] WriteRegister64 [0x40210] := 0x2
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a318
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A318] == 0x50850008
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1A318] := 0x50C50008
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:80] ReadRegister32 offset 0x1a318
I driver/usb/local_usb_device.cc:514] SendControlCommandWithDataIn
I driver/usb/local_usb_device.cc:521] SYNC CTRL WITH DATA IN begin
I driver/usb/local_usb_device.cc:538] SYNC CTRL WITH DATA IN end
I driver/usb/usb_ml_commands.cc:110] ReadRegister32 [0x1A318] == 0x50C50258
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1907C] := 0xF
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:153] WriteRegister32 [0x1907C] := 0x0
I driver/usb/local_usb_device.cc:475] SendControlCommandWithDataOut
I driver/usb/local_usb_device.cc:482] SYNC CTRL WITH DATA OUT begin
I driver/usb/local_usb_device.cc:499] SYNC CTRL WITH DATA OUT end
I driver/usb/usb_ml_commands.cc:50] ~UsbMlCommands
I driver/usb/usb_standard_commands.cc:39] ~UsbStandardCommands
I driver/usb/local_usb_device.cc:196] ~LocalUsbDevice
I driver/usb/local_usb_device.cc:243] Close: closing device 0x55b42ba410 
I driver/usb/local_usb_device.cc:263] Close: releasing claimed interface 0
I driver/usb/local_usb_device.cc:216] DoCancelAllTransfers: cancelling 0 async transfers
I driver/usb/local_usb_device.cc:224] DoCancelAllTransfers: waiting for all async transfers to complete
I driver/usb/local_usb_device.cc:234] DoCancelAllTransfers: all async transfers have completed
I driver/usb/local_usb_device.cc:276] Close: releasing 0 transfer buffers
I driver/usb/local_usb_device.cc:322] Close: final clean up completed
37329476
0.001
```
",damjandakic93,2024-07-03 14:43:23+00:00,['tilakrayal'],2024-07-04 12:42:38+00:00,2024-07-04 12:42:35+00:00,https://github.com/tensorflow/tensorflow/issues/70803,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('TF 2.13', 'For issues related to Tensorflow 2.13')]","[{'comment_id': 2208416094, 'issue_id': 2388820824, 'author': 'tilakrayal', 'body': '@damjandakic93,\r\nCould you please check and confirm if you are facing the same issue with the latest tensorflow versions 2.15 and 2.16?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 4, 8, 34, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208464896, 'issue_id': 2388820824, 'author': 'damjandakic93', 'body': ""I actually made a mistake, I ran the edgetpu-compiler but copied the wrong file (so no ops were intended for TPU).\r\nI have another issue where I get segfault upon calling Invoke, I guess I'll open a new Issue for that?"", 'created_at': datetime.datetime(2024, 7, 4, 8, 59, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208763701, 'issue_id': 2388820824, 'author': 'tilakrayal', 'body': '@damjandakic93,\r\nIn that scenario, could you please feel free to move this issue to closed status? Thank you!', 'created_at': datetime.datetime(2024, 7, 4, 11, 44, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208883925, 'issue_id': 2388820824, 'author': 'damjandakic93', 'body': 'Agreed, thanks.', 'created_at': datetime.datetime(2024, 7, 4, 12, 42, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208883998, 'issue_id': 2388820824, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70803"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70803"">No</a>', 'created_at': datetime.datetime(2024, 7, 4, 12, 42, 38, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-04 08:34:12 UTC): @damjandakic93,
Could you please check and confirm if you are facing the same issue with the latest tensorflow versions 2.15 and 2.16?
Thank you!

damjandakic93 (Issue Creator) on (2024-07-04 08:59:29 UTC): I actually made a mistake, I ran the edgetpu-compiler but copied the wrong file (so no ops were intended for TPU).
I have another issue where I get segfault upon calling Invoke, I guess I'll open a new Issue for that?

tilakrayal (Assginee) on (2024-07-04 11:44:32 UTC): @damjandakic93,
In that scenario, could you please feel free to move this issue to closed status? Thank you!

damjandakic93 (Issue Creator) on (2024-07-04 12:42:35 UTC): Agreed, thanks.

google-ml-butler[bot] on (2024-07-04 12:42:38 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70803"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70803"">No</a>

"
2388643236,issue,closed,completed,TFLite in C++ causes Segmentation Fault for MobileNet,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.1
2.15.1 behaves similarly
2.16.1 cannot even produce the tflite model (something similar to this issue: https://github.com/tensorflow/tensorflow/issues/65012)
tf-nightly cannot build libtensorflowlite.so (gcc: error: unrecognized command-line option '-mavx512fp16'; did you mean '-mavx512bf16'?)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I make a simple MobileNet model in TF, convert it to TFLite and attempt to run it in C++. It causes Segmentation Fault. Running TFLite Interpreter in Python works properly. I have created a minimal repro case:

```
import tensorflow as tf

def main():
    model = tf.keras.Sequential([
        tf.keras.Input(batch_shape=[1, 224, 224, 3]),
        tf.keras.applications.MobileNetV3Small(
            input_shape=[224, 224, 3],
            alpha=1.0,
            minimalistic=False,
            include_top=False,
            weights=""imagenet"",
            pooling=""max"",
            dropout_rate=0.2,
            classifier_activation=None,
            include_preprocessing=True,
        )
    ])

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()
    with open(""converted_model.tflite"", ""wb"") as f:
        f.write(tflite_model)


if __name__ == ""__main__"":
    main()
```

### Standalone code to reproduce the issue

```shell
#include <memory>
#include <iostream>
#include ""tensorflow/lite/core/interpreter.h""
#include ""tensorflow/lite/core/model_builder.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/core/c/common.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""flatbuffers/flatbuffers.h""

std::unique_ptr<tflite::Interpreter> Initialize()
{
    const std::string model_path = ""converted_model.tflite"";
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(model_path.c_str());

    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> model_interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&model_interpreter);
    if (model_interpreter->AllocateTensors() != kTfLiteOk)
    {
        std::cerr << ""Failed to allocate tensors."" << std::endl;
    }

    return model_interpreter;
}

int main()
{
    std::unique_ptr<tflite::Interpreter> interpreter = Initialize();

    std::cout << ""Acquiring input buffer"" << std::endl;

    float *input_buff = interpreter->typed_input_tensor<float>(0);

    for (int i = 0; i < 224*224*3; i++)
    {
        input_buff[i] = 0.2;
    }

    std::cout << ""Filled input buffer"" << std::endl;

    interpreter->Invoke();

    std::cout << ""Done with Invoke"" << std::endl;

    float *output = interpreter->typed_output_tensor<float>(0);
    std::cout << ""Output: "" << output[0] << std::endl;

    return 0;
}
```


### Relevant log output

```shell
INFO: Initialized TensorFlow Lite runtime.
INFO: Applying 1 TensorFlow Lite delegate(s) lazily.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 109 out of 110 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 2 partitions for the whole graph.
Note (XNNPACK): fuse Constant Pad Node #5 into Depthwise Convolution 2D Node #4
Note (XNNPACK): fuse Constant Pad Node #14 into Depthwise Convolution 2D Node #13
Note (XNNPACK): fuse Constant Pad Node #23 into Depthwise Convolution 2D Node #22
Note (XNNPACK): fuse Constant Pad Node #77 into Depthwise Convolution 2D Node #76
INFO: Successfully applied the default TensorFlow Lite delegate indexed at 0.
 *NOTE*: because a delegate has been applied, the precision of computations should be unchanged, but the exact output tensor values may have changed. If such output values are checked in your code, like in your tests etc., please consider increasing error tolerance for the check.
Acquiring input buffer
Filled input buffer
Segmentation fault (core dumped)
```
",damjandakic93,2024-07-03 13:30:53+00:00,"['pkgoogle', 'sawantkumar']",2024-09-19 02:00:28+00:00,2024-09-19 02:00:26+00:00,https://github.com/tensorflow/tensorflow/issues/70802,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2231265298, 'issue_id': 2388643236, 'author': 'sawantkumar', 'body': 'Hi @damjandakic93 ,\r\n\r\nCan you please provide me the tflite file if possible ?', 'created_at': datetime.datetime(2024, 7, 16, 15, 42, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232981805, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': 'Sure, though you have the code to generate it above.\r\nHere it is attached.\r\n[converted_model.tflite.zip](https://github.com/user-attachments/files/16264096/converted_model.tflite.zip)\r\nThanks!', 'created_at': datetime.datetime(2024, 7, 17, 10, 33, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255849493, 'issue_id': 2388643236, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle ,\r\n\r\nI tried several times but i keep getting the below error where it says it cannot find the interpreter.h even though its present at that location, , can you please take a look?\r\n\r\n```\r\nCMake Generate step failed.  Build files cannot be regenerated correctly.\r\nroot@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# cd ..\r\nroot@tflite-issue-replication:/home/sawantkumar/work/tflite_program# nano CMakeLists.txt \r\nroot@tflite-issue-replication:/home/sawantkumar/work/tflite_program# cd build/\r\nroot@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# cmake ..\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/sawantkumar/work/tflite_program/build\r\nroot@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# make\r\nScanning dependencies of target tflite_program\r\n[ 50%] Building CXX object CMakeFiles/tflite_program.dir/model_runner.cpp.o\r\n/home/sawantkumar/work/tflite_program/model_runner.cpp:3:10: fatal error: tensorflow/lite/core/interpreter.h: No such file or directory\r\n    3 | #include ""tensorflow/lite/core/interpreter.h""\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/tflite_program.dir/build.make:63: CMakeFiles/tflite_program.dir/model_runner.cpp.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:76: CMakeFiles/tflite_program.dir/all] Error 2\r\nmake: *** [Makefile:84: all] Error 2\r\n```', 'created_at': datetime.datetime(2024, 7, 29, 12, 51, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257025377, 'issue_id': 2388643236, 'author': 'pkgoogle', 'body': 'Hi @damjandakic93, are you willing to switch to a different workflow? Using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) and our [minimal C++ example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal), I was able to successfully run this model:\r\n\r\nconversion:\r\n```py\r\nimport ai_edge_torch\r\nimport torch\r\nimport torchvision\r\n\r\n\r\norig_model = torchvision.models.mobilenet_v3_small()\r\nsample_input = (torch.randn(1, 3, 224, 224),)\r\n\r\nedge_model = ai_edge_torch.convert(orig_model.eval(), sample_input)\r\nedge_model.export(""mobilenet_v3_small.tflite"")\r\n```\r\n\r\nexecution:\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\nmkdir minimal_build\r\ncd minimal_build\r\ncmake ../tensorflow_src/tensorflow/lite/examples/minimal\r\ncmake --build . -j\r\n./minimal <path/to/mobilenet_v3_small.tflite> #wherever you saved it\r\n```\r\n\r\nexample output:\r\n```\r\n./minimal xxxxxxxx/issues/tflite/70802/mobilenet_v3_small.tflite\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n=== Pre-invoke Interpreter State ===\r\nInterpreter has 1 subgraphs.\r\n\r\n-----------Subgraph-0 has 316 tensors and 132 nodes------------\r\n1 Inputs: [0] -> 602112B (0.57MB)\r\n1 Outputs: [222] -> 4000B (0.00MB)\r\n\r\nTensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  \r\nTensor   0 serving_default_args_0:0  kTfLiteFloat32  kTfLiteArenaRw     602112   / 0.57 [1,3,224,224] [0, 602112)\r\nTensor   1 arith.constant            kTfLiteFloat32  kTfLiteMmapRo      2359296  / 2.25 [1024,576] [7758680, 10117976)\r\nTensor   2 arith.constant1           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [7537476, 7758660)\r\nTensor   3 arith.constant2           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [96,1,1,576] [7316280, 7537464)\r\nTensor   4 arith.constant3           kTfLiteFloat32  kTfLiteMmapRo      57600    / 0.05 [1,5,5,576] [7258668, 7316268)\r\nTensor   5 arith.constant4           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [7037472, 7258656)\r\nTensor   6 arith.constant5           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [96,1,1,576] [6816276, 7037460)\r\nTensor   7 arith.constant6           kTfLiteFloat32  kTfLiteMmapRo      57600    / 0.05 [1,5,5,576] [6758664, 6816264)\r\nTensor   8 arith.constant7           kTfLiteFloat32  kTfLiteMmapRo      2304     / 0.00 [576] [6756348, 6758652)\r\nTensor   9 arith.constant8           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [6535152, 6756336)\r\nTensor  10 arith.constant9           kTfLiteFloat32  kTfLiteMmapRo      110592   / 0.11 [96,1,1,288] [6424548, 6535140)\r\nTensor  11 arith.constant10          kTfLiteFloat32  kTfLiteMmapRo      28800    / 0.03 [1,5,5,288] [6395736, 6424536)\r\n...\r\n...\r\n...\r\nNode 127 Operator Builtin Code  74 SUM (delegated by node 131)\r\n  2 Input Tensors:[218,59] -> 0B (0.00MB)\r\n  1 Output Tensors:[219] -> 0B (0.00MB)\r\n  4 Temporary Tensors:[259-262] -> 0B (0.00MB)\r\nNode 128 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 131)\r\n  3 Input Tensors:[219,1,-1] -> 0B (0.00MB)\r\n  1 Output Tensors:[220] -> 0B (0.00MB)\r\nNode 129 Operator Builtin Code 117 HARD_SWISH (delegated by node 131)\r\n  1 Input Tensors:[220] -> 0B (0.00MB)\r\n  1 Output Tensors:[221] -> 0B (0.00MB)\r\nNode 130 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 131)\r\n  3 Input Tensors:[221,58,-1] -> 0B (0.00MB)\r\n  1 Output Tensors:[222] -> 0B (0.00MB)\r\nNode 131 Operator Custom Name TfLiteXNNPackDelegate \r\n  92 Input Tensors:[0-91] -> 10719000B (10.22MB)\r\n  1 Output Tensors:[222] -> 4000B (0.00MB)\r\n\r\nExecution plan as the list of 1 nodes invoked in-order: [131]\r\nAmong these nodes in the execution plan:\r\n  Node 131 is a TfLiteXNNPackDelegate node (0x555e556d2f20), which has delegated 131 nodes: [0-130]\r\n--------------Subgraph-0 dump has completed--------------\r\n\r\n--------------Memory Arena Status Start--------------\r\nTotal memory usage: 606112 bytes (0.578 MB)\r\n- Total arena memory usage: 606112 bytes (0.578 MB)\r\n- Total dynamic memory usage: 0 bytes (0.000 MB)\r\n\r\nSubgraph#0   Arena (Normal)         606112 (100.00%)\r\n--------------Memory Arena Status End--------------\r\n\r\n```', 'created_at': datetime.datetime(2024, 7, 29, 21, 17, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2257141632, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': ""@pkgoogle Wow, this seems interesting, thanks! I actually had a significant amount of work to switch from PyTorch I was working with to TensorFlow as I was unaware of this tool. The reason I had to switch was because ultimately I wish to run the model on a Coral device. Does this workflow support running models on Coral?\r\n\r\nAlso, my model is not a simple mobilenet like in this example but a custom model which relies on the mobilenet as a backbone (so mobilenet + some fully connected layers in several branches with regression outputs at the end), does this sound like something I'd be able to get running on Coral with AI-Edge-Torch?"", 'created_at': datetime.datetime(2024, 7, 29, 22, 50, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2259139355, 'issue_id': 2388643236, 'author': 'pkgoogle', 'body': ""Hi @damjandakic93, ideally it should work with any model you can define in PyTorch, in practice this is probably not 100% true -- if it isn't, that team would love to hear from you :) so we can make the product even better. The produced tflite model is not particularly special/not-special so the produced model should run on Coral.\r\n\r\nExample custom model conversion: https://github.com/tensorflow/tensorflow/issues/65769#issuecomment-2159237894"", 'created_at': datetime.datetime(2024, 7, 30, 20, 18, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264838292, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': ""Managed to convert the model (it seems v2.functional.rotate isn't supported but I've removed it for the sake of getting anything to work).\r\nHowever, once I try to run it in tflite (C++, natively without TPU) it causes SegmentationFault on Invoke (same inference code as above, I've just commented-out the filling of the input buffer).\r\nNow I cannot share with you this specific model (due to NDA). I can tell you that it contains MobileNet, slicing, concatenation, torch.nn.Linear and torch.nn.functional.silu which are all fairly simple operators.\r\n\r\nAny hints on how to debug this futher? Thanks!"", 'created_at': datetime.datetime(2024, 8, 2, 8, 21, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2264844946, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': 'Btw, custom matmul (""my_mat_mul.tflite"") from the example conversion you linked above works properly so the issue is model-specific.', 'created_at': datetime.datetime(2024, 8, 2, 8, 24, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265181529, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': 'Actually, let me rephrase my question.\r\nSince I cannot give you the whole model, how do you suggest I proceed with the debug?\r\nI an start from simple mobilenet and butcher my model until it starts working to see which op causes the segfault, or is there a better way? Thanks!', 'created_at': datetime.datetime(2024, 8, 2, 11, 44, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265905832, 'issue_id': 2388643236, 'author': 'pkgoogle', 'body': ""Hi @damjandakic93 What you are suggesting will work to isolate where the situation is happening though there is a risk that the system together is what causing the issue, but you will answer that as you try to isolate the issue. From there you will have a minimally reproducible model which will help you in the next step.\r\n\r\nI'm guessing you have a program that is running the model in C++. You have to compile TF from source with debug symbols, so follow https://www.tensorflow.org/install/source and add this option when using Bazel: `--config=dbg`, install the version of TF you want to debug (I recommend starting with nightly as that represents the latest code, in case your issue is actually resolved already)\r\n\r\nChoose your favorite debugger (usually this means gdb or lldb), use it with your program i.e. if you would execute your program like: `./your_program`, do `gdb ./your_program`\r\n\r\nIf you are unfamiliar with a debugger, well then it's time to start but I would look up a cheat sheet online and just start stepping through the code."", 'created_at': datetime.datetime(2024, 8, 2, 18, 13, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278935190, 'issue_id': 2388643236, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 10, 1, 54, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284073556, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': ""I'll need some time (other priorities) before I dive into this debug (as it will take some time most likely).\r\nI'll leave the issue as stale (and then closed) and will reopen it once I have an update."", 'created_at': datetime.datetime(2024, 8, 12, 14, 1, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297815999, 'issue_id': 2388643236, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 20, 1, 53, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419688, 'issue_id': 2388643236, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 27, 1, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311419742, 'issue_id': 2388643236, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">No</a>', 'created_at': datetime.datetime(2024, 8, 27, 1, 56, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324324132, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': 'Hi @pkgoogle,\r\n\r\nSo, I came back back to this issue. I\'ve relaxed my requirements to supporting Python as well (so no need for C++ anymore, it\'s too much of a hassle, I\'ll do it inter-process).\r\nI start with a simple vanilla Mobilenet model:\r\n\r\n`model = torchvision.models.get_model(""mobilenet_v3_small"", weights=""DEFAULT"")`\r\n\r\nI convert it via aiedge:\r\n\r\n```\r\nmodel = ai_edge_torch.to_channel_last_io(model, args=[0])\r\n_args = (\r\n    torch.randn((1, 224, 224, 3), dtype=torch.float32),\r\n)\r\n\r\nedge_model = ai_edge_torch.convert(model, _args)\r\nedge_model.export(""edge_model.tflite"")\r\n```\r\n\r\nWhen I try to run it through edgetpu_compiler via CLI I get the following:\r\n\r\n> edgetpu_compiler edge_model.tflite \r\n> /bin/bash: /home/ddakic/anaconda3/envs/aiedge/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n> Edge TPU Compiler version 16.0.384591198\r\n> ERROR: Op builtin_code out of range: 204. Are you using old TFLite binary with newer model?\r\n> ERROR: Registration failed.\r\n> \r\n> Invalid model: edge_model.tflite\r\n> Model could not be parsed\r\n\r\nIs there a way to control the opset version during the conversion (I guess that would be a reasonable path to start debugging)?\r\n\r\nMy TF version is 2.17.0 (also tried with tf-nightly, with it I get ""Didn\'t find op for builtin opcode \'MUL\' version \'7\'."").\r\nPyTorch version is 2.4.0.\r\nPython version 3.11 (also tried with 3.9).\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 9, 2, 9, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327193739, 'issue_id': 2388643236, 'author': 'pkgoogle', 'body': 'Hi @damjandakic93, this looks like an edgetpu_compiler issue... is there a more relevant repo for that particular piece? Perhaps this? https://github.com/google-coral/libedgetpu/issues', 'created_at': datetime.datetime(2024, 9, 3, 18, 48, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328385222, 'issue_id': 2388643236, 'author': 'damjandakic93', 'body': ""I'll try to debug it with them as well.\r\nThought this might also be the place to mention this as the same model implemented in TensorFlow and converted directly through TFLite converter works properly with the edgetpu_compiler. Let me try to work it out with them first then, in case you get some ideas on your side let me know please. Thanks!"", 'created_at': datetime.datetime(2024, 9, 4, 9, 37, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329899641, 'issue_id': 2388643236, 'author': 'pkgoogle', 'body': ""Hi @damjandakic93, then .. it may be an ai-edge-torch issue: https://github.com/google-ai-edge/ai-edge-torch/issues, I do think they can perhaps identify root cause better, if there's an issue w/ the converted .tflite file then route it to AI-Edge-Torch (i.e. it converted something improperly). But digging into why edgetpu_compiler is failing with it will be better handled by that repo for now."", 'created_at': datetime.datetime(2024, 9, 4, 20, 20, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345102648, 'issue_id': 2388643236, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 12, 1, 58, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359826998, 'issue_id': 2388643236, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2359827064, 'issue_id': 2388643236, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">No</a>', 'created_at': datetime.datetime(2024, 9, 19, 2, 0, 27, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-07-16 15:42:28 UTC): Hi @damjandakic93 ,

Can you please provide me the tflite file if possible ?

damjandakic93 (Issue Creator) on (2024-07-17 10:33:19 UTC): Sure, though you have the code to generate it above.
Here it is attached.
[converted_model.tflite.zip](https://github.com/user-attachments/files/16264096/converted_model.tflite.zip)
Thanks!

sawantkumar (Assginee) on (2024-07-29 12:51:19 UTC): Hi @pkgoogle ,

I tried several times but i keep getting the below error where it says it cannot find the interpreter.h even though its present at that location, , can you please take a look?

```
CMake Generate step failed.  Build files cannot be regenerated correctly.
root@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# cd ..
root@tflite-issue-replication:/home/sawantkumar/work/tflite_program# nano CMakeLists.txt 
root@tflite-issue-replication:/home/sawantkumar/work/tflite_program# cd build/
root@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# cmake ..
-- Configuring done
-- Generating done
-- Build files have been written to: /home/sawantkumar/work/tflite_program/build
root@tflite-issue-replication:/home/sawantkumar/work/tflite_program/build# make
Scanning dependencies of target tflite_program
[ 50%] Building CXX object CMakeFiles/tflite_program.dir/model_runner.cpp.o
/home/sawantkumar/work/tflite_program/model_runner.cpp:3:10: fatal error: tensorflow/lite/core/interpreter.h: No such file or directory
    3 | #include ""tensorflow/lite/core/interpreter.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
make[2]: *** [CMakeFiles/tflite_program.dir/build.make:63: CMakeFiles/tflite_program.dir/model_runner.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:76: CMakeFiles/tflite_program.dir/all] Error 2
make: *** [Makefile:84: all] Error 2
```

pkgoogle (Assginee) on (2024-07-29 21:17:42 UTC): Hi @damjandakic93, are you willing to switch to a different workflow? Using [AI-Edge-Torch](https://github.com/google-ai-edge/ai-edge-torch) and our [minimal C++ example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal), I was able to successfully run this model:

conversion:
```py
import ai_edge_torch
import torch
import torchvision


orig_model = torchvision.models.mobilenet_v3_small()
sample_input = (torch.randn(1, 3, 224, 224),)

edge_model = ai_edge_torch.convert(orig_model.eval(), sample_input)
edge_model.export(""mobilenet_v3_small.tflite"")
```

execution:
```sh
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir minimal_build
cd minimal_build
cmake ../tensorflow_src/tensorflow/lite/examples/minimal
cmake --build . -j
./minimal <path/to/mobilenet_v3_small.tflite> #wherever you saved it
```

example output:
```
./minimal xxxxxxxx/issues/tflite/70802/mobilenet_v3_small.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
=== Pre-invoke Interpreter State ===
Interpreter has 1 subgraphs.

-----------Subgraph-0 has 316 tensors and 132 nodes------------
1 Inputs: [0] -> 602112B (0.57MB)
1 Outputs: [222] -> 4000B (0.00MB)

Tensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  
Tensor   0 serving_default_args_0:0  kTfLiteFloat32  kTfLiteArenaRw     602112   / 0.57 [1,3,224,224] [0, 602112)
Tensor   1 arith.constant            kTfLiteFloat32  kTfLiteMmapRo      2359296  / 2.25 [1024,576] [7758680, 10117976)
Tensor   2 arith.constant1           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [7537476, 7758660)
Tensor   3 arith.constant2           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [96,1,1,576] [7316280, 7537464)
Tensor   4 arith.constant3           kTfLiteFloat32  kTfLiteMmapRo      57600    / 0.05 [1,5,5,576] [7258668, 7316268)
Tensor   5 arith.constant4           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [7037472, 7258656)
Tensor   6 arith.constant5           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [96,1,1,576] [6816276, 7037460)
Tensor   7 arith.constant6           kTfLiteFloat32  kTfLiteMmapRo      57600    / 0.05 [1,5,5,576] [6758664, 6816264)
Tensor   8 arith.constant7           kTfLiteFloat32  kTfLiteMmapRo      2304     / 0.00 [576] [6756348, 6758652)
Tensor   9 arith.constant8           kTfLiteFloat32  kTfLiteMmapRo      221184   / 0.21 [576,1,1,96] [6535152, 6756336)
Tensor  10 arith.constant9           kTfLiteFloat32  kTfLiteMmapRo      110592   / 0.11 [96,1,1,288] [6424548, 6535140)
Tensor  11 arith.constant10          kTfLiteFloat32  kTfLiteMmapRo      28800    / 0.03 [1,5,5,288] [6395736, 6424536)
...
...
...
Node 127 Operator Builtin Code  74 SUM (delegated by node 131)
  2 Input Tensors:[218,59] -> 0B (0.00MB)
  1 Output Tensors:[219] -> 0B (0.00MB)
  4 Temporary Tensors:[259-262] -> 0B (0.00MB)
Node 128 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 131)
  3 Input Tensors:[219,1,-1] -> 0B (0.00MB)
  1 Output Tensors:[220] -> 0B (0.00MB)
Node 129 Operator Builtin Code 117 HARD_SWISH (delegated by node 131)
  1 Input Tensors:[220] -> 0B (0.00MB)
  1 Output Tensors:[221] -> 0B (0.00MB)
Node 130 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 131)
  3 Input Tensors:[221,58,-1] -> 0B (0.00MB)
  1 Output Tensors:[222] -> 0B (0.00MB)
Node 131 Operator Custom Name TfLiteXNNPackDelegate 
  92 Input Tensors:[0-91] -> 10719000B (10.22MB)
  1 Output Tensors:[222] -> 4000B (0.00MB)

Execution plan as the list of 1 nodes invoked in-order: [131]
Among these nodes in the execution plan:
  Node 131 is a TfLiteXNNPackDelegate node (0x555e556d2f20), which has delegated 131 nodes: [0-130]
--------------Subgraph-0 dump has completed--------------

--------------Memory Arena Status Start--------------
Total memory usage: 606112 bytes (0.578 MB)
- Total arena memory usage: 606112 bytes (0.578 MB)
- Total dynamic memory usage: 0 bytes (0.000 MB)

Subgraph#0   Arena (Normal)         606112 (100.00%)
--------------Memory Arena Status End--------------

```

damjandakic93 (Issue Creator) on (2024-07-29 22:50:15 UTC): @pkgoogle Wow, this seems interesting, thanks! I actually had a significant amount of work to switch from PyTorch I was working with to TensorFlow as I was unaware of this tool. The reason I had to switch was because ultimately I wish to run the model on a Coral device. Does this workflow support running models on Coral?

Also, my model is not a simple mobilenet like in this example but a custom model which relies on the mobilenet as a backbone (so mobilenet + some fully connected layers in several branches with regression outputs at the end), does this sound like something I'd be able to get running on Coral with AI-Edge-Torch?

pkgoogle (Assginee) on (2024-07-30 20:18:50 UTC): Hi @damjandakic93, ideally it should work with any model you can define in PyTorch, in practice this is probably not 100% true -- if it isn't, that team would love to hear from you :) so we can make the product even better. The produced tflite model is not particularly special/not-special so the produced model should run on Coral.

Example custom model conversion: https://github.com/tensorflow/tensorflow/issues/65769#issuecomment-2159237894

damjandakic93 (Issue Creator) on (2024-08-02 08:21:01 UTC): Managed to convert the model (it seems v2.functional.rotate isn't supported but I've removed it for the sake of getting anything to work).
However, once I try to run it in tflite (C++, natively without TPU) it causes SegmentationFault on Invoke (same inference code as above, I've just commented-out the filling of the input buffer).
Now I cannot share with you this specific model (due to NDA). I can tell you that it contains MobileNet, slicing, concatenation, torch.nn.Linear and torch.nn.functional.silu which are all fairly simple operators.

Any hints on how to debug this futher? Thanks!

damjandakic93 (Issue Creator) on (2024-08-02 08:24:40 UTC): Btw, custom matmul (""my_mat_mul.tflite"") from the example conversion you linked above works properly so the issue is model-specific.

damjandakic93 (Issue Creator) on (2024-08-02 11:44:05 UTC): Actually, let me rephrase my question.
Since I cannot give you the whole model, how do you suggest I proceed with the debug?
I an start from simple mobilenet and butcher my model until it starts working to see which op causes the segfault, or is there a better way? Thanks!

pkgoogle (Assginee) on (2024-08-02 18:13:53 UTC): Hi @damjandakic93 What you are suggesting will work to isolate where the situation is happening though there is a risk that the system together is what causing the issue, but you will answer that as you try to isolate the issue. From there you will have a minimally reproducible model which will help you in the next step.

I'm guessing you have a program that is running the model in C++. You have to compile TF from source with debug symbols, so follow https://www.tensorflow.org/install/source and add this option when using Bazel: `--config=dbg`, install the version of TF you want to debug (I recommend starting with nightly as that represents the latest code, in case your issue is actually resolved already)

Choose your favorite debugger (usually this means gdb or lldb), use it with your program i.e. if you would execute your program like: `./your_program`, do `gdb ./your_program`

If you are unfamiliar with a debugger, well then it's time to start but I would look up a cheat sheet online and just start stepping through the code.

github-actions[bot] on (2024-08-10 01:54:22 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

damjandakic93 (Issue Creator) on (2024-08-12 14:01:21 UTC): I'll need some time (other priorities) before I dive into this debug (as it will take some time most likely).
I'll leave the issue as stale (and then closed) and will reopen it once I have an update.

github-actions[bot] on (2024-08-20 01:53:56 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-27 01:56:00 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-27 01:56:03 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">No</a>

damjandakic93 (Issue Creator) on (2024-09-02 09:58:00 UTC): Hi @pkgoogle,

So, I came back back to this issue. I've relaxed my requirements to supporting Python as well (so no need for C++ anymore, it's too much of a hassle, I'll do it inter-process).
I start with a simple vanilla Mobilenet model:

`model = torchvision.models.get_model(""mobilenet_v3_small"", weights=""DEFAULT"")`

I convert it via aiedge:

```
model = ai_edge_torch.to_channel_last_io(model, args=[0])
_args = (
    torch.randn((1, 224, 224, 3), dtype=torch.float32),
)

edge_model = ai_edge_torch.convert(model, _args)
edge_model.export(""edge_model.tflite"")
```

When I try to run it through edgetpu_compiler via CLI I get the following:


Is there a way to control the opset version during the conversion (I guess that would be a reasonable path to start debugging)?

My TF version is 2.17.0 (also tried with tf-nightly, with it I get ""Didn't find op for builtin opcode 'MUL' version '7'."").
PyTorch version is 2.4.0.
Python version 3.11 (also tried with 3.9).

Thanks!

pkgoogle (Assginee) on (2024-09-03 18:48:10 UTC): Hi @damjandakic93, this looks like an edgetpu_compiler issue... is there a more relevant repo for that particular piece? Perhaps this? https://github.com/google-coral/libedgetpu/issues

damjandakic93 (Issue Creator) on (2024-09-04 09:37:30 UTC): I'll try to debug it with them as well.
Thought this might also be the place to mention this as the same model implemented in TensorFlow and converted directly through TFLite converter works properly with the edgetpu_compiler. Let me try to work it out with them first then, in case you get some ideas on your side let me know please. Thanks!

pkgoogle (Assginee) on (2024-09-04 20:20:50 UTC): Hi @damjandakic93, then .. it may be an ai-edge-torch issue: https://github.com/google-ai-edge/ai-edge-torch/issues, I do think they can perhaps identify root cause better, if there's an issue w/ the converted .tflite file then route it to AI-Edge-Torch (i.e. it converted something improperly). But digging into why edgetpu_compiler is failing with it will be better handled by that repo for now.

github-actions[bot] on (2024-09-12 01:58:45 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-19 02:00:25 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-19 02:00:27 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70802"">No</a>

"
2388642956,issue,open,,Updgraded manylinux support?,"As per my understanding, manylinux 2014 (infact CentOS Linux 7) went EOL on June 30 2024.

I still see the tf-nightly-cpu wheels and latest 2.17 prerelease wheels use manylinux 2014.
Do you plan to support updated manylinux (probably 2_28). ?

I also see some notes for the same here: https://docs.google.com/document/d/1l6q1qzEyCzEwXloKZqwP7bmVO1SxeVRhPW3A0BeB4KE/edit#heading=h.u7w1oawlfu32

@MichaelHudgins , any thoughts on this? or any relevant contacts to this?

Refer : https://github.com/mayeut/pep600_compliance/blob/master/EOL.rst
Refer : https://www.centos.org/",kiriti-pendyala,2024-07-03 13:30:47+00:00,"['MichaelHudgins', 'Venkat6871']",2024-07-11 09:06:25+00:00,,https://github.com/tensorflow/tensorflow/issues/70801,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:feature', 'Feature requests'), ('type:build/install', 'Build and install issues'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues')]","[{'comment_id': 2214045154, 'issue_id': 2388642956, 'author': 'MichaelHudgins', 'body': 'Hi @kiriti-pendyala , moving to a newer manylinux is something that is actively being considered.  We have not landed on which exact standard we will change to or when but it will likely be soon.  2_28 is a likely candidate but there are still ongoing discussions.  CC: @vam-google', 'created_at': datetime.datetime(2024, 7, 8, 13, 13, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2222411249, 'issue_id': 2388642956, 'author': 'kiriti-pendyala', 'body': 'Can you let us know any approximate timeline that this migration may happen? @vam-google @MichaelHudgins', 'created_at': datetime.datetime(2024, 7, 11, 9, 6, tzinfo=datetime.timezone.utc)}]","MichaelHudgins (Assginee) on (2024-07-08 13:13:47 UTC): Hi @kiriti-pendyala , moving to a newer manylinux is something that is actively being considered.  We have not landed on which exact standard we will change to or when but it will likely be soon.  2_28 is a likely candidate but there are still ongoing discussions.  CC: @vam-google

kiriti-pendyala (Issue Creator) on (2024-07-11 09:06:00 UTC): Can you let us know any approximate timeline that this migration may happen? @vam-google @MichaelHudgins

"
2388515632,issue,closed,completed,cannot import name 'mean_absolute_error' from 'tensorflow.keras.losses',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.16.1-19-g810f233968c 2.16.2

### Custom code

No

### OS platform and distribution

macOS Sonoma 14.5

### Mobile device

_No response_

### Python version

Python 3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When i import neurite or voxelmorph it spits out

ImportError: cannot import name 'mean_absolute_error' from 'tensorflow.keras.losses' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/_tf_keras/keras/losses/__init__.py)


I see someone had a similar issue here but the answers did not help me: https://github.com/tensorflow/tensorflow/issues/69851




### Standalone code to reproduce the issue

```shell
import voxelmorph as vxm
```


### Relevant log output

```shell
----> 6 import voxelmorph as vxm
      7 import neurite as ne

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/voxelmorph/__init__.py:12
      9 from packaging import version
     11 # ensure valid neurite version is available
---> 12 import neurite
     13 minv = '0.2'
     14 curv = getattr(neurite, '__version__', None)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/neurite/__init__.py:50
     47 except ImportError:
     48     raise ImportError('Please install tensorflow to use this neurite backend')
---> 50 from . import tf
     51 from .tf import *

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/neurite/tf/__init__.py:5
      3 from . import generators
      4 from . import callbacks
----> 5 from . import metrics
      6 from . import losses
      7 from . import models

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/neurite/tf/metrics.py:33
     31 from tensorflow.keras import losses
     32 # simple metrics renamed mae -> l1, mse -> l2
---> 33 from tensorflow.keras.losses import mean_absolute_error as l1
     34 from tensorflow.keras.losses import mean_squared_error as l2
     36 # local

ImportError: cannot import name 'mean_absolute_error' from 'tensorflow.keras.losses' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/_tf_keras/keras/losses/__init__.py)
```
",dwang6524,2024-07-03 12:34:06+00:00,['tilakrayal'],2024-07-21 01:56:00+00:00,2024-07-21 01:55:57+00:00,https://github.com/tensorflow/tensorflow/issues/70796,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues'), ('TF 2.16', '')]","[{'comment_id': 2208410100, 'issue_id': 2388515632, 'author': 'tilakrayal', 'body': ""@dwang6524,\r\nAFAIK the issue is tensorflow v2.16 contains keras3.0 which might be the reason for the issue/error. As a workaround could you please try to below code before executing the import statement.\r\n\r\n```python\r\n!pip install tf-keras\r\nimport os\r\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\r\n```\r\n\r\nKindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/efa6769300cb9365313856205da78e7f/untitled1990.ipynb)\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 4, 8, 31, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2209462295, 'issue_id': 2388515632, 'author': 'just-sabyr', 'body': 'This issue can be solved by changing two files (both files need to be changed): site-packages/neurite/tf/metrics.py (lines 33 and 34) and site-packages/neurite/tf/losses.py (lines 32 and 33). Tensorflow version==2.16.1, voxelmorph version==0.2\r\n\r\nChange from these: \r\n\r\n```python\r\nfrom tensorflow.keras.losses import mean_absolute_error as l1\r\nfrom tensorflow.keras.losses import mean_squared_error as l2\r\n```\r\nTo these: \r\n```python\r\nfrom tensorflow.keras.losses import MAE as l1\r\nfrom tensorflow.keras.losses import MSE as l2\r\n```', 'created_at': datetime.datetime(2024, 7, 4, 18, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2226702257, 'issue_id': 2388515632, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 13, 1, 51, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241380889, 'issue_id': 2388515632, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 21, 1, 55, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2241380903, 'issue_id': 2388515632, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70796"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70796"">No</a>', 'created_at': datetime.datetime(2024, 7, 21, 1, 55, 58, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-04 08:31:03 UTC): @dwang6524,
AFAIK the issue is tensorflow v2.16 contains keras3.0 which might be the reason for the issue/error. As a workaround could you please try to below code before executing the import statement.

```python
!pip install tf-keras
import os
os.environ['TF_USE_LEGACY_KERAS'] = '1'
```

Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/efa6769300cb9365313856205da78e7f/untitled1990.ipynb)

Thank you!

just-sabyr on (2024-07-04 18:57:48 UTC): This issue can be solved by changing two files (both files need to be changed): site-packages/neurite/tf/metrics.py (lines 33 and 34) and site-packages/neurite/tf/losses.py (lines 32 and 33). Tensorflow version==2.16.1, voxelmorph version==0.2

Change from these: 

```python
from tensorflow.keras.losses import mean_absolute_error as l1
from tensorflow.keras.losses import mean_squared_error as l2
```
To these: 
```python
from tensorflow.keras.losses import MAE as l1
from tensorflow.keras.losses import MSE as l2
```

github-actions[bot] on (2024-07-13 01:51:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-21 01:55:56 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-21 01:55:58 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70796"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70796"">No</a>

"
2386777948,issue,closed,completed,Issue with Loading Sequential Models,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

Windows 10 Home

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to train a Sequential model based off the Single Shot CNN model found in the Tensorflow [time series forecasting tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series).

The compiling, testing, predicting, and saving operations all work fine; however, loading the model gives me this error - `ValueError: Sequential model 'sequential' has already been configured to use input shape (None, 10, 4). You cannot build it with input_shape [None, 10, 4]`.

This happens with version 2.16.2, but if I were to use version 2.15.0 from a Google Colab notebook, I would get no errors. This difference between versions and the oddity between two identical shapes not equaling causes me to believe that this is a bug.

I also [posted on the Tensorflow Forums](https://discuss.tensorflow.org/t/issue-with-identical-input-shapes-not-equalling/25585) about this issue, and another user replied that they too have similar issues.


### Standalone code to reproduce the issue

This is a link to a Colab notebook that reproduces the error:
https://colab.research.google.com/drive/1ivDScvFR9oJOKVg31oIc0BbGux7WGEEM?usp=sharing

This is a link to the csv file that I used (place it in the content folder):
https://drive.google.com/file/d/1gpG0SN6ayCnemssI9XdnLB5VOxfV-xzc/view?usp=sharing


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py in build(self, input_shape)
    164         if isinstance(self._layers[0], InputLayer):
    165             if self._layers[0].batch_shape != input_shape:
--> 166                 raise ValueError(
    167                     f""Sequential model '{self.name}' has already been ""
    168                     ""configured to use input shape ""

ValueError: Sequential model 'sequential' has already been configured to use input shape (None, 10, 4). You cannot build it with input_shape [None, 10, 4]
```
",RaulCastillo547,2024-07-02 17:23:24+00:00,['tilakrayal'],2024-08-20 14:34:58+00:00,2024-08-03 01:57:17+00:00,https://github.com/tensorflow/tensorflow/issues/70757,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('comp:model', 'Model related issues'), ('TF 2.16', '')]","[{'comment_id': 2205000524, 'issue_id': 2386777948, 'author': 'GarmischWg', 'body': 'Experiencing the same issue in tensorflow 2.16.1.', 'created_at': datetime.datetime(2024, 7, 3, 3, 22, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208380197, 'issue_id': 2386777948, 'author': 'sushreebarsa', 'body': '@tilakrayal I was able to replicate the issue in TF[v2.16](https://colab.research.google.com/gist/sushreebarsa/0c319b7455efb0deb9f2e79d99dd18f7/notebook-for-error.ipynb#scrollTo=DdP3-O33ypGO) and tf-[nightly](https://colab.research.google.com/gist/sushreebarsa/35d0ea64c077c3fa55a6976eea53279a/notebook-for-error.ipynb#scrollTo=r84tJyfxykjb) but it is not reproducible in TF [2.15](https://colab.research.google.com/gist/sushreebarsa/e38f5797c21a185db2ac276d7994bef3/notebook-for-error.ipynb#scrollTo=kPo0jdaRx_D0).\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 4, 8, 16, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213162438, 'issue_id': 2386777948, 'author': 'tilakrayal', 'body': '@RaulCastillo547,\r\nI tried to execute the mentioned using **!pip install tf-keras==2.17.0rc0**, and executed the official time series forecasting document and it was executed without fail. By default Tensorflow v2.16 contains the keras3.0 version which might be the reason for the issue/error/fail. Kindly find the gist of it [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb).\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 8, 6, 40, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2214524989, 'issue_id': 2386777948, 'author': 'RaulCastillo547', 'body': '@tilakrayal,\r\nI tried using !pip install tf-keras==2.17.0rc0 and tried to save and load the CNN multi-step model on both the official time series forecasting document and my google collab notebook. However, the same error pops up when I load the model.', 'created_at': datetime.datetime(2024, 7, 8, 15, 57, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255789960, 'issue_id': 2386777948, 'author': 'tilakrayal', 'body': '@RaulCastillo547,\r\nThank you for reporting the issue. I was able to reproduce the issue on TensorFlow 2.15, keras2.0 and tf-nightly keras3.0 as well. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/e1188571488b381ebce4ab6708593739/notebook_for_error.ipynb).\r\n\r\nAs this issue is more related to keras, could you please try to raise the issue in the keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!', 'created_at': datetime.datetime(2024, 7, 29, 12, 23, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261560207, 'issue_id': 2386777948, 'author': 'xchen99sdr', 'body': 'Hello, reporting the same issue with pip version 2.17. Thanks', 'created_at': datetime.datetime(2024, 7, 31, 22, 24, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262203859, 'issue_id': 2386777948, 'author': 'tilakrayal', 'body': 'Could you please feel free to move this issue to closed status, since it is already being tracked in the Keras repo? Thank you!', 'created_at': datetime.datetime(2024, 8, 1, 7, 4, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266328094, 'issue_id': 2386777948, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70757"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70757"">No</a>', 'created_at': datetime.datetime(2024, 8, 3, 1, 57, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2299017201, 'issue_id': 2386777948, 'author': 'IHATEWRITINGCODE', 'body': ""In my case, the issue was caused by my use of the `tf.keras.layers.Lambda` class within the `tf.keras.Sequential` model. The lambda function I was using was `lambda x: tf.reshape(x, [...])` which I suppose changed the tuple shape representation to a list representation. I believe this comparison subsequently failed downstream when loading the model. My solution was to use the `tf.keras.layers.Reshape` class instead of the Lambda class as you're supposed to. P.S. This might not be the exact reason the tutorial code is failing."", 'created_at': datetime.datetime(2024, 8, 20, 14, 34, 30, tzinfo=datetime.timezone.utc)}]","GarmischWg on (2024-07-03 03:22:41 UTC): Experiencing the same issue in tensorflow 2.16.1.

sushreebarsa on (2024-07-04 08:16:15 UTC): @tilakrayal I was able to replicate the issue in TF[v2.16](https://colab.research.google.com/gist/sushreebarsa/0c319b7455efb0deb9f2e79d99dd18f7/notebook-for-error.ipynb#scrollTo=DdP3-O33ypGO) and tf-[nightly](https://colab.research.google.com/gist/sushreebarsa/35d0ea64c077c3fa55a6976eea53279a/notebook-for-error.ipynb#scrollTo=r84tJyfxykjb) but it is not reproducible in TF [2.15](https://colab.research.google.com/gist/sushreebarsa/e38f5797c21a185db2ac276d7994bef3/notebook-for-error.ipynb#scrollTo=kPo0jdaRx_D0).
Thank you!

tilakrayal (Assginee) on (2024-07-08 06:40:45 UTC): @RaulCastillo547,
I tried to execute the mentioned using **!pip install tf-keras==2.17.0rc0**, and executed the official time series forecasting document and it was executed without fail. By default Tensorflow v2.16 contains the keras3.0 version which might be the reason for the issue/error/fail. Kindly find the gist of it [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb).

Thank you!

RaulCastillo547 (Issue Creator) on (2024-07-08 15:57:08 UTC): @tilakrayal,
I tried using !pip install tf-keras==2.17.0rc0 and tried to save and load the CNN multi-step model on both the official time series forecasting document and my google collab notebook. However, the same error pops up when I load the model.

tilakrayal (Assginee) on (2024-07-29 12:23:50 UTC): @RaulCastillo547,
Thank you for reporting the issue. I was able to reproduce the issue on TensorFlow 2.15, keras2.0 and tf-nightly keras3.0 as well. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/e1188571488b381ebce4ab6708593739/notebook_for_error.ipynb).

As this issue is more related to keras, could you please try to raise the issue in the keras-team/keras [repo](https://github.com/keras-team/keras/issues) for the quick resolution. Thank you!

xchen99sdr on (2024-07-31 22:24:30 UTC): Hello, reporting the same issue with pip version 2.17. Thanks

tilakrayal (Assginee) on (2024-08-01 07:04:12 UTC): Could you please feel free to move this issue to closed status, since it is already being tracked in the Keras repo? Thank you!

google-ml-butler[bot] on (2024-08-03 01:57:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70757"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70757"">No</a>

IHATEWRITINGCODE on (2024-08-20 14:34:30 UTC): In my case, the issue was caused by my use of the `tf.keras.layers.Lambda` class within the `tf.keras.Sequential` model. The lambda function I was using was `lambda x: tf.reshape(x, [...])` which I suppose changed the tuple shape representation to a list representation. I believe this comparison subsequently failed downstream when loading the model. My solution was to use the `tf.keras.layers.Reshape` class instead of the Lambda class as you're supposed to. P.S. This might not be the exact reason the tutorial code is failing.

"
2386516403,issue,closed,completed,TFLite in C++ causes Segmentation Fault,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No (different bug)

### Source

source

### TensorFlow version

2.13.1 (described below)
2.15.1 behaves similarly
2.16.1 cannot even produce the tflite model (something similar to this issue: https://github.com/tensorflow/tensorflow/issues/65012)
tf-nightly cannot build libtensorflowlite.so (gcc: error: unrecognized command-line option '-mavx512fp16'; did you mean '-mavx512bf16'?)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a model which I wish to run on RPi-4 in C++, however, I am currently facing issues with converting the model to TFLite and running it in C++ on the native machine. I made a dummy model which demonstrates the bug. Here's the code for generating the model:

```
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation=""relu""),
    tf.keras.layers.Dense(5, activation=""softmax"")
])

model.build(input_shape=(1, 224, 449, 3))

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open(""converted_model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

Note however that THIS solves the issue for this dummy model:
```
class WrappedModel(tf.keras.Model):
    def __init__(self, baseModel):
        super().__init__()
        self.baseModel = baseModel

    def call(self, x, training=False):
        x = tf.reshape(x, [1, 224, 449, 3])
        return self.baseModel(x)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation=""relu""),
    tf.keras.layers.Dense(5, activation=""softmax"")
])

model.build(input_shape=(1, 224, 449, 3))

wrapped_model = WrappedModel(model)

# Calling wrapped_model.build(...) doesn't work here for some reason, looks like another bug.
wrapped_model.compute_output_shape(
    input_shape=[1, 224 * 449 * 3]
)

converter = tf.lite.TFLiteConverter.from_keras_model(wrapped_model)
tflite_model = converter.convert()
with open(""converted_model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```
but it doesn't work with my real model (which includes some pretrained models, several branched layers, etc.).
When I run TFLite interpreter from Python everything works as expected.

Thanks!

### Standalone code to reproduce the issue

```shell
#include <memory>
#include <iostream>
#include ""tensorflow/lite/core/interpreter.h""
#include ""tensorflow/lite/core/model_builder.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/core/c/common.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""flatbuffers/flatbuffers.h""

std::unique_ptr<tflite::Interpreter> Initialize()
{
    const std::string model_path = ""converted_model.tflite"";
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(model_path.c_str());

    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> model_interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&model_interpreter);
    if (model_interpreter->AllocateTensors() != kTfLiteOk)
    {
        std::cerr << ""Failed to allocate tensors."" << std::endl;
    }

    return model_interpreter;
}

int main()
{
    std::unique_ptr<tflite::Interpreter> interpreter = Initialize();

    std::cout << ""Acquiring input buffer"" << std::endl;

    float *input_buff = interpreter->typed_input_tensor<float>(0);

    for (int i = 0; i < 224*449*3; i++)
    {
        input_buff[i] = 0.2;
    }

    std::cout << ""Filled input buffer"" << std::endl;

    interpreter->Invoke();

    std::cout << ""Done with Invoke"" << std::endl;

    float *output = interpreter->typed_output_tensor<float>(0);
    std::cout << ""Output: "" << output[0] << std::endl;

    return 0;
}
```


### Relevant log output
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Acquiring input buffer
Filled input buffer
Segmentation fault (core dumped)
```",damjandakic93,2024-07-02 15:08:12+00:00,['Venkat6871'],2024-07-03 13:32:34+00:00,2024-07-03 13:32:31+00:00,https://github.com/tensorflow/tensorflow/issues/70747,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues')]","[{'comment_id': 2206089522, 'issue_id': 2386516403, 'author': 'damjandakic93', 'body': ""Static dimensions (including batch size) are required apparently, though I didn't see it anywhere in documentation?\r\ntf.keras.Sequential([tf.keras.Input(batch_shape=[1, 224, 449, 3]), model])\r\n\r\nClosing the issue as I've opened a new one with the rest of the bugs."", 'created_at': datetime.datetime(2024, 7, 3, 13, 32, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2206089594, 'issue_id': 2386516403, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70747"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70747"">No</a>', 'created_at': datetime.datetime(2024, 7, 3, 13, 32, 33, tzinfo=datetime.timezone.utc)}]","damjandakic93 (Issue Creator) on (2024-07-03 13:32:31 UTC): Static dimensions (including batch size) are required apparently, though I didn't see it anywhere in documentation?
tf.keras.Sequential([tf.keras.Input(batch_shape=[1, 224, 449, 3]), model])

Closing the issue as I've opened a new one with the rest of the bugs.

google-ml-butler[bot] on (2024-07-03 13:32:33 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70747"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70747"">No</a>

"
2386437695,issue,closed,completed,Issue: TensorFlow API Installation and Integration with Website Failing on Laptop,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.16.0-rc0-18-g5bc9d26649c 2.16.1

### Custom code

Yes

### OS platform and distribution

 Microsoft Windows 11 Home Single Language

### Mobile device

_No response_

### Python version

Python 3.9.18

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am facing issues installing the TensorFlow API and integrating TensorFlow models into my website. Despite following various guides and tutorials available on YouTube and Google, I have been unable to resolve the installation issues on my laptop.

### Standalone code to reproduce the issue

```shell
I currently do not have specific code to reproduce the issue as I have been running commands in the bash shell and facing installation problems. However, I have attempted every possible solution and guide available online through Google and YouTube, and none have resolved the installation issues Im experiencing.
```


### Relevant log output

_No response_",SakshiFadnavis2003,2024-07-02 14:34:20+00:00,['tilakrayal'],2024-07-24 01:53:20+00:00,2024-07-24 01:53:16+00:00,https://github.com/tensorflow/tensorflow/issues/70744,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.16', '')]","[{'comment_id': 2206000636, 'issue_id': 2386437695, 'author': 'tilakrayal', 'body': '@SakshiFadnavis2003,\r\nCould you please provide the steps you have followed to install the tensorflow and also provide the error log which you are facing during the process.\r\n\r\nAlso please try to follow the steps mentioned which are available in the official document.\r\nhttps://www.tensorflow.org/install/pip#step-by-step_instructions\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 3, 12, 48, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2206009707, 'issue_id': 2386437695, 'author': 'SakshiFadnavis2003', 'body': '@tilakrayal ,\r\nTensorflow is installed but the API needed to integrating the tensorflow models into the website are not installing like the  one tensorflow API', 'created_at': datetime.datetime(2024, 7, 3, 12, 53, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213282394, 'issue_id': 2386437695, 'author': 'tilakrayal', 'body': '@SakshiFadnavis2003,\r\nCould you please let me know the details of the tensorflow API which you are trying to install the website which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 7, 8, 7, 53, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229847581, 'issue_id': 2386437695, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 16, 1, 53, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710360, 'issue_id': 2386437695, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710463, 'issue_id': 2386437695, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70744"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70744"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 19, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-03 12:48:46 UTC): @SakshiFadnavis2003,
Could you please provide the steps you have followed to install the tensorflow and also provide the error log which you are facing during the process.

Also please try to follow the steps mentioned which are available in the official document.
https://www.tensorflow.org/install/pip#step-by-step_instructions

Thank you!

SakshiFadnavis2003 (Issue Creator) on (2024-07-03 12:53:22 UTC): @tilakrayal ,
Tensorflow is installed but the API needed to integrating the tensorflow models into the website are not installing like the  one tensorflow API

tilakrayal (Assginee) on (2024-07-08 07:53:15 UTC): @SakshiFadnavis2003,
Could you please let me know the details of the tensorflow API which you are trying to install the website which helps to debug the issue. Thank you!

github-actions[bot] on (2024-07-16 01:53:34 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-24 01:53:15 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-24 01:53:19 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70744"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70744"">No</a>

"
2385679221,issue,open,reopened,Build error in tensorflow lite minimal example,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.2

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following guide at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal

Did not modify any files. The build step (5) fails with an undefined reference error (see log outputs).
Building standalone tensorflow lite with cmake following https://www.tensorflow.org/lite/guide/build_cmake works just fine.


### Standalone code to reproduce the issue

```shell
sudo apt-get install cmake
wget http://es.archive.ubuntu.com/ubuntu/pool/main/libf/libffi/libffi7_3.3-4_amd64.deb
sudo dpkg -i libffi7_3.3-4_amd64.deb
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir minimal_build
cd minimal_build
cmake ../tensorflow_src/tensorflow/lite/examples/minimal
cmake --build . -j
```


### Relevant log output

```shell
[100%] Built target tensorflow-lite
[100%] Building CXX object CMakeFiles/minimal.dir/minimal.cc.o
[100%] Linking CXX executable minimal
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
fully_connected.cc:(.text+0x3d02): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'
/usr/bin/ld: fully_connected.cc:(.text+0x3d50): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0x3db9): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0x3ded): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0x3ff6): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'
collect2: error: ld returned 1 exit status
gmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1
gmake[1]: *** [CMakeFiles/Makefile2:1372: CMakeFiles/minimal.dir/all] Error 2
gmake: *** [Makefile:136: all] Error 2
```
",bossebandowski,2024-07-02 09:03:56+00:00,"['pkgoogle', 'sawantkumar']",2024-12-09 19:32:10+00:00,,https://github.com/tensorflow/tensorflow/issues/70730,"[('stat:contribution welcome', 'Status - Contributions welcome'), ('awaiting review', 'Pull request awaiting review'), ('type:build/install', 'Build and install issues'), ('comp:lite', 'TF Lite related issues'), ('TF 2.16', '')]","[{'comment_id': 2222048657, 'issue_id': 2385679221, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle ,\r\n\r\nWhile replicating this issue I keep getting compilation error like below and the vm freezes, can you please take a look below\r\n\r\n`[100%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/compiler/mlir/lite/experimental/remat/metadata_util.cc.o\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In constructor tflite::delegates::NnapiPlugin::NnapiPlugin(const tflite::TFLiteSettings&):\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:74:52: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]\r\n   74 |         nnapi_settings->accelerator_name()->Length() != 0) {\r\n      |                                                    ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function void tflite::delegates::NnapiPlugin::SetCompilationCacheDir(const tflite::TFLiteSettings&):\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:103:77: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  103 |         tflite_settings.compilation_caching_settings()->cache_dir()->Length() !=\r\n      |                                                                             ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:110:76: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  110 |                tflite_settings.nnapi_settings()->cache_directory()->Length() !=\r\n      |                                                                            ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function void tflite::delegates::NnapiPlugin::SetModelToken(const tflite::TFLiteSettings&):\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:122:26: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  122 |                 ->Length() != 0) {\r\n      |                          ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:127:72: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  127 |                tflite_settings.nnapi_settings()->model_token()->Length() != 0) {\r\n      |                                                                        ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }`', 'created_at': datetime.datetime(2024, 7, 11, 5, 13, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2223873939, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': ""Hi @bossebandowski, I was able to build with these steps on the nightly branch, will that work for you?\r\n\r\n\r\nMy OS and gcc:\r\n```sh\r\n$ gcc --version\r\ngcc (Debian 13.2.0-13) 13.2.0\r\nCopyright (C) 2023 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n\r\n```sh\r\n$ python --version\r\nPython 3.10.12\r\n```\r\n\r\nI already have cmake installed and I don't need the optional steps so here are my steps:\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\ncd tensorflow_src\r\ngit switch nightly\r\ncd ..\r\nmkdir minimal_build\r\ncd minimal_build\r\ncmake ../tensorflow_src/tensorflow/lite/examples/minimal\r\ncmake --build . -j\r\n```"", 'created_at': datetime.datetime(2024, 7, 11, 20, 32, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2225286375, 'issue_id': 2385679221, 'author': 'bossebandowski', 'body': ""Hey,\r\n\r\nthanks for the reply. I updated to gcc-13 but I am still getting the same error.\r\n\r\n```\r\ngcc --version\r\ngcc (Ubuntu 13.1.0-8ubuntu1~22.04) 13.1.0\r\nCopyright (C) 2023 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n```\r\n/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nfully_connected.cc:(.text+0xc919): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc967): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc9cf): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xca02): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xcc2d): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:1307: CMakeFiles/minimal.dir/all] Error 2\r\ngmake: *** [Makefile:136: all] Error 2\r\n```"", 'created_at': datetime.datetime(2024, 7, 12, 10, 28, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2226306865, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': 'Hi @bossebandowski, can you let me know these pieces of information?\r\n\r\n- which branch tensorflow is on (From your information, it seems you are building from master branch, I would try with nightly and see if that changes anything)\r\n- cmake version (mine is 3.28.3)\r\n\r\nThanks.', 'created_at': datetime.datetime(2024, 7, 12, 20, 27, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2240829830, 'issue_id': 2385679221, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 20, 1, 50, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2242279735, 'issue_id': 2385679221, 'author': 'bossebandowski', 'body': ""```\r\n!cmake --version\r\ncmake version 3.22.1\r\n```\r\n\r\nI just reproduced the issue from the nightly branch, same error message:\r\n\r\n```\r\n[100%] Linking CXX executable minimal\r\n/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nfully_connected.cc:(.text+0xc85e): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc8ab): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc913): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc947): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xcb76): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:1372: CMakeFiles/minimal.dir/all] Error 2\r\ngmake: *** [Makefile:136: all] Error 2\r\n```"", 'created_at': datetime.datetime(2024, 7, 22, 7, 31, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2243746509, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': ""Hi @bossebandowski, I tried with cmake==3.22.1, it worked fine on my end... I have a Debian system but they're pretty similar to Ubuntu. Are you following the optional steps or my steps exactly? Did you pull the latest files in nightly? Is there any way you can create a fresh environment? (A new VM and/or do this in a docker?)"", 'created_at': datetime.datetime(2024, 7, 22, 20, 20, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2244651898, 'issue_id': 2385679221, 'author': 'bossebandowski', 'body': 'Yes, still getting the same error after a fresh pull. Following your steps exactly. I have managed to get it to work on other machines though.', 'created_at': datetime.datetime(2024, 7, 23, 9, 0, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2245963245, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': ""Hmm ok that means all the data points to me a particular environmental issue with that machine.. is there any way you can reinstall everything there? or uninstall most things and reinstall? It's not likely to be productive if we can't reproduce it either. Thanks."", 'created_at': datetime.datetime(2024, 7, 23, 18, 32, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247101338, 'issue_id': 2385679221, 'author': 'bossebandowski', 'body': 'Sure. Closing this issue now as there does not seem to be a way forward.', 'created_at': datetime.datetime(2024, 7, 24, 7, 28, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2247101415, 'issue_id': 2385679221, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70730"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70730"">No</a>', 'created_at': datetime.datetime(2024, 7, 24, 7, 28, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455164755, 'issue_id': 2385679221, 'author': 'pasweistorz', 'body': '@bossebandowski @pkgoogle :\r\n\r\nHi,\r\n\r\nI stumbled accross the same error today. I also found an explanation for the linker errors in my case. The working copy resided in a path that contained the string `sse`. Therefore, the regular expression `(.*neon.*|.*sse.*)\\\\.(cc|h)` from `tensorflow/lite/CMakeLists.txt` matched and filtered the file `fully_connected_reference.cc`. Thus, the functions implemented in it were missing in `libtensorflow-lite.a` and I got linker errors.\r\n\r\nOne idea would be to improve the regular expression and use `(.*neon_.*|.*sse_.*)\\\\.(cc|h)`, instead. This solved the issue on my machine.', 'created_at': datetime.datetime(2024, 11, 4, 16, 29, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455583288, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': 'Hi @pasweistorz, would you like to submit a PR for your fix? Thanks.', 'created_at': datetime.datetime(2024, 11, 4, 19, 56, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457716139, 'issue_id': 2385679221, 'author': 'pasweistorz', 'body': 'Hi @pkgoogle, if we can establish a corporate CLA I will create a PR.', 'created_at': datetime.datetime(2024, 11, 5, 17, 1, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527156398, 'issue_id': 2385679221, 'author': 'pasweistorz', 'body': 'Hi @pkgoogle, I submitted the PR #81920 to fix this issue. But the sanity checks for the PR seem to make no progress. Is there anything missing from my side?', 'created_at': datetime.datetime(2024, 12, 9, 7, 37, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529205190, 'issue_id': 2385679221, 'author': 'pkgoogle', 'body': 'Hi @pasweistorz. Thanks! As far as I can tell no, there are ~5000 PRs, so we will get to it.', 'created_at': datetime.datetime(2024, 12, 9, 19, 31, 42, tzinfo=datetime.timezone.utc)}]","sawantkumar (Assginee) on (2024-07-11 05:13:16 UTC): Hi @pkgoogle ,

While replicating this issue I keep getting compilation error like below and the vm freezes, can you please take a look below

`[100%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/compiler/mlir/lite/experimental/remat/metadata_util.cc.o
In file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In constructor tflite::delegates::NnapiPlugin::NnapiPlugin(const tflite::TFLiteSettings&):
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:74:52: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]
   74 |         nnapi_settings->accelerator_name()->Length() != 0) {
      |                                                    ^
In file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,
                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here
  168 |   SizeT Length() const { return size(); }
      |         ^~~~~~
In file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function void tflite::delegates::NnapiPlugin::SetCompilationCacheDir(const tflite::TFLiteSettings&):
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:103:77: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]
  103 |         tflite_settings.compilation_caching_settings()->cache_dir()->Length() !=
      |                                                                             ^
In file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,
                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here
  168 |   SizeT Length() const { return size(); }
      |         ^~~~~~
In file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:110:76: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]
  110 |                tflite_settings.nnapi_settings()->cache_directory()->Length() !=
      |                                                                            ^
In file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,
                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here
  168 |   SizeT Length() const { return size(); }
      |         ^~~~~~
In file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function void tflite::delegates::NnapiPlugin::SetModelToken(const tflite::TFLiteSettings&):
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:122:26: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]
  122 |                 ->Length() != 0) {
      |                          ^
In file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,
                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here
  168 |   SizeT Length() const { return size(); }
      |         ^~~~~~
In file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:127:72: warning: SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int] is deprecated: use size() instead [-Wdeprecated-declarations]
  127 |                tflite_settings.nnapi_settings()->model_token()->Length() != 0) {
      |                                                                        ^
In file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,
                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,
                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:
/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here
  168 |   SizeT Length() const { return size(); }`

pkgoogle (Assginee) on (2024-07-11 20:32:13 UTC): Hi @bossebandowski, I was able to build with these steps on the nightly branch, will that work for you?


My OS and gcc:
```sh
$ gcc --version
gcc (Debian 13.2.0-13) 13.2.0
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```


```sh
$ python --version
Python 3.10.12
```

I already have cmake installed and I don't need the optional steps so here are my steps:
```sh
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
cd tensorflow_src
git switch nightly
cd ..
mkdir minimal_build
cd minimal_build
cmake ../tensorflow_src/tensorflow/lite/examples/minimal
cmake --build . -j
```

bossebandowski (Issue Creator) on (2024-07-12 10:28:23 UTC): Hey,

thanks for the reply. I updated to gcc-13 but I am still getting the same error.

```
gcc --version
gcc (Ubuntu 13.1.0-8ubuntu1~22.04) 13.1.0
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

```
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
fully_connected.cc:(.text+0xc919): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'
/usr/bin/ld: fully_connected.cc:(.text+0xc967): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xc9cf): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xca02): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xcc2d): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'
collect2: error: ld returned 1 exit status
gmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1
gmake[1]: *** [CMakeFiles/Makefile2:1307: CMakeFiles/minimal.dir/all] Error 2
gmake: *** [Makefile:136: all] Error 2
```

pkgoogle (Assginee) on (2024-07-12 20:27:14 UTC): Hi @bossebandowski, can you let me know these pieces of information?

- which branch tensorflow is on (From your information, it seems you are building from master branch, I would try with nightly and see if that changes anything)
- cmake version (mine is 3.28.3)

Thanks.

github-actions[bot] on (2024-07-20 01:50:47 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

bossebandowski (Issue Creator) on (2024-07-22 07:31:41 UTC): ```
!cmake --version
cmake version 3.22.1
```

I just reproduced the issue from the nightly branch, same error message:

```
[100%] Linking CXX executable minimal
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
fully_connected.cc:(.text+0xc85e): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'
/usr/bin/ld: fully_connected.cc:(.text+0xc8ab): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xc913): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xc947): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'
/usr/bin/ld: fully_connected.cc:(.text+0xcb76): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'
collect2: error: ld returned 1 exit status
gmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1
gmake[1]: *** [CMakeFiles/Makefile2:1372: CMakeFiles/minimal.dir/all] Error 2
gmake: *** [Makefile:136: all] Error 2
```

pkgoogle (Assginee) on (2024-07-22 20:20:39 UTC): Hi @bossebandowski, I tried with cmake==3.22.1, it worked fine on my end... I have a Debian system but they're pretty similar to Ubuntu. Are you following the optional steps or my steps exactly? Did you pull the latest files in nightly? Is there any way you can create a fresh environment? (A new VM and/or do this in a docker?)

bossebandowski (Issue Creator) on (2024-07-23 09:00:59 UTC): Yes, still getting the same error after a fresh pull. Following your steps exactly. I have managed to get it to work on other machines though.

pkgoogle (Assginee) on (2024-07-23 18:32:08 UTC): Hmm ok that means all the data points to me a particular environmental issue with that machine.. is there any way you can reinstall everything there? or uninstall most things and reinstall? It's not likely to be productive if we can't reproduce it either. Thanks.

bossebandowski (Issue Creator) on (2024-07-24 07:28:40 UTC): Sure. Closing this issue now as there does not seem to be a way forward.

google-ml-butler[bot] on (2024-07-24 07:28:42 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70730"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70730"">No</a>

pasweistorz on (2024-11-04 16:29:16 UTC): @bossebandowski @pkgoogle :

Hi,

I stumbled accross the same error today. I also found an explanation for the linker errors in my case. The working copy resided in a path that contained the string `sse`. Therefore, the regular expression `(.*neon.*|.*sse.*)\\.(cc|h)` from `tensorflow/lite/CMakeLists.txt` matched and filtered the file `fully_connected_reference.cc`. Thus, the functions implemented in it were missing in `libtensorflow-lite.a` and I got linker errors.

One idea would be to improve the regular expression and use `(.*neon_.*|.*sse_.*)\\.(cc|h)`, instead. This solved the issue on my machine.

pkgoogle (Assginee) on (2024-11-04 19:56:14 UTC): Hi @pasweistorz, would you like to submit a PR for your fix? Thanks.

pasweistorz on (2024-11-05 17:01:31 UTC): Hi @pkgoogle, if we can establish a corporate CLA I will create a PR.

pasweistorz on (2024-12-09 07:37:10 UTC): Hi @pkgoogle, I submitted the PR #81920 to fix this issue. But the sanity checks for the PR seem to make no progress. Is there anything missing from my side?

pkgoogle (Assginee) on (2024-12-09 19:31:42 UTC): Hi @pasweistorz. Thanks! As far as I can tell no, there are ~5000 PRs, so we will get to it.

"
2385504088,issue,closed,completed,Tflite(C++) for saving fine tuned models,"I have used the reference [here](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training) to enable on-device learning by adding four signatures (infer, train, restore and save) same as given in the reference. I now use C++ TFLite API, which takes as input <tflite_model_path> and <input_data> and performs inference and can also train the model using the tflite::SignatureRunner. I am unable to perform saving and restoring of weights. 

Following is my attempt. I get Error: Memory Allocation failed. 

`tflite::SignatureRunner* save_runner = interpreter->GetSignatureRunner(signature_defs[2]->c_str());`
`if(!save_runner){
  std::cerr << ""Cannot obtain save signature"" << std::endl; 
}`

`TFLITE_MINIMAL_CHECK(save_runner->AllocateTensors() == kTfLiteOk);`
`TfLiteTensor* input_tensor_ = save_runner->input_tensor(""checkpoint_path"");`
`TFLITE_MINIMAL_CHECK(input_tensor_ != nullptr);`
`std::string* input_ = tflite::GetTensorData<std::string>(input_tensor_);`
`if(input_){
  *input_ = ""model.ckpt"";
}`
`else{
  std::cerr << ""Error: Memory allocation failed"" << std::endl; exit(1);
}`

`TFLITE_MINIMAL_CHECK(save_runner->Invoke() == kTfLiteOk);`
`TfLiteTensor* output_tensor_ = save_runner->input_tensor(""checkpoint_path"");`
`std::string* output = tflite::GetTensorData<std::string>(output_tensor_);`
`std::cout << ""OUTPUT checkpoint path: ""<< *output << std::endl;`

`interpreter.reset();`
`return 0;`

Why is the input_ variable not allocated any memory. Following is the function signature corresponding to ""save"".
`@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])`
`  def save(self, checkpoint_path):`
`    tensor_names = [weight.name for weight in self.model.weights]`
`    tensors_to_save = [weight.read_value() for weight in self.model.weights]`
`    tf.raw_ops.Save(`
`        filename=checkpoint_path, tensor_names=tensor_names,`
`        data=tensors_to_save, name='save')`
`    return {`
`        ""checkpoint_path"": checkpoint_path`
`    }`",Sachi-27,2024-07-02 07:49:11+00:00,"['haozha111', 'pkgoogle', 'sawantkumar']",2024-10-24 02:02:06+00:00,2024-10-24 02:02:04+00:00,https://github.com/tensorflow/tensorflow/issues/70726,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:bug', 'Bug'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:lite', 'TF Lite related issues'), ('TF 2.9', 'Issues found in the TF 2.9 release (or RCs)')]","[{'comment_id': 2202712278, 'issue_id': 2385504088, 'author': 'tilakrayal', 'body': '@Sachi-27,\r\nCould you please provide the complete code to reproduce and the tensorflow version you are trying  which helps to debug the issue. Thank you!', 'created_at': datetime.datetime(2024, 7, 2, 10, 45, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2204908939, 'issue_id': 2385504088, 'author': 'Sachi-27', 'body': 'Tensorflow version : 2.9.1\r\nI have used the same code as in [here](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training) to convert the .h5 model to .tflite with signatures.\r\n\r\nI have made changes to the tensorflow/tlite/examples/minimal/minimal.cc code.\r\n```\r\n#include <cstdio>\r\n#include <iostream>\r\n#include <fstream>\r\n#include <sys/time.h>\r\n#include ""tensorflow/lite/interpreter.h""\r\n#include ""tensorflow/lite/kernels/register.h""\r\n#include ""tensorflow/lite/model.h""\r\n#include ""tensorflow/lite/optional_debug_tools.h""\r\n#include ""tensorflow/lite/signature_runner.h""\r\n#include ""tensorflow/lite/kernels/internal/tensor_ctypes.h""\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n  if (!(x)) {                                                \\\r\n    fprintf(stderr, ""Error at %s:%d\\n"", __FILE__, __LINE__); \\\r\n    exit(1);                                                 \\\r\n  }\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, ""minimal <tflite model>\\n"");\r\n     return 1;\r\n  }\r\n\r\n  const char* filename = argv[1];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter with the InterpreterBuilder.\r\n  // Note: all Interpreters should be built with the InterpreterBuilder,\r\n  // which allocates memory for the Interpreter and does various set up\r\n  // tasks so that the Interpreter can read the provided model.\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n  \r\nint batch_sz = 10;\r\n  std::vector<const std::string*> signature_defs = interpreter->signature_keys();\r\n tflite::SignatureRunner* save_runner = interpreter->GetSignatureRunner(signature_defs[2]->c_str());\r\n  if(!save_runner){\r\n    std::cerr << ""Cannot obtain save signature"" << std::endl; \r\n  }\r\nconst std::vector<const char*>& input_names_ = save_runner->input_names();\r\n  const std::vector<const char*>& output_names_ = save_runner->output_names();\r\n  std::cout << ""input_names.size() = "" << input_names_.size() << std::endl;\r\n  std::cout << input_names_[0] << std::endl;\r\n  std::cout << ""output_names.size() = "" << output_names_.size() << std::endl;\r\n  std::cout << output_names_[0] << std::endl;\r\n\r\n  TFLITE_MINIMAL_CHECK(save_runner->AllocateTensors() == kTfLiteOk);\r\n  TfLiteTensor* input_tensor_ = save_runner->input_tensor(""checkpoint_path"");\r\n  TFLITE_MINIMAL_CHECK(input_tensor_ != nullptr);\r\n  std::string* input_ = tflite::GetTensorData<std::string>(input_tensor_);\r\n  std::cout << ""About to set input_ value"" << std::endl;\r\n  std::cout << ""TFLITE TYPE: "" << input_tensor_->type << std::endl;\r\n    *input_ = ""model.ckpt"";\r\n  \r\n  std::cout << ""Setting input_ to: "" << input_tensor->data.raw << std::endl;\r\n\r\n  TFLITE_MINIMAL_CHECK(save_runner->Invoke() == kTfLiteOk);\r\n  std::cout << ""Invoked"" << std::endl;\r\n  const TfLiteTensor* output_tensor_ = save_runner->output_tensor(""checkpoint_path"");\r\n  std::cout << ""Obtained output_tensor_"" << std::endl;\r\n  std::cout << ""OUTPUT checkpoint path: ""<< output_tensor->data.raw << std::endl;\r\n\r\n  interpreter.reset();\r\n  return 0;\r\n}\r\n```\r\n\r\n\r\n\r\nCOMMAND TO RUN:\r\n`./minimal demo.tflite`\r\n\r\nOUTPUT:\r\n1\r\ncheckpoint_path\r\n1\r\ncheckpoint_path\r\nAbout to set input_ value\r\nTFLITE TYPE: 5\r\nSegmentation Fault\r\n\r\n\r\n\r\nQUERY:\r\nI would like to basically know how to pass in tf.string input correctly. I have also tried doing the following instead:\r\n`input_tensor_->data.raw = ""model.ckpt"";`\r\nBut i get Segmentation fault when `save_runner->Invoke()` is called', 'created_at': datetime.datetime(2024, 7, 3, 2, 11, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255155668, 'issue_id': 2385504088, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle ,\r\n\r\nI also got the same error, can you please look into it \r\n\r\n```\r\nroot@tflite-issue-replication:/home/sawantkumar/work/c_plus/tensorflow/bazel-bin/tensorflow/lite/examples/minimal# ./minimal /home/sawantkumar/work/c_plus/tensorflow/tensorflow/lite/examples/minimal/demo1.tflite\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\ninput_names.size() = 1\r\ncheckpoint_path\r\noutput_names.size() = 1\r\ncheckpoint_path\r\nAbout to set input_ value\r\nTFLITE TYPE: 5\r\nSegmentation fault (core dumped)\r\n```', 'created_at': datetime.datetime(2024, 7, 29, 7, 25, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2256848180, 'issue_id': 2385504088, 'author': 'pkgoogle', 'body': 'Hi @Sachi-27, do you absolutely need to do on device training? -- if not I can recommend other methods which may work better.', 'created_at': datetime.datetime(2024, 7, 29, 20, 30, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262406952, 'issue_id': 2385504088, 'author': 'Sachi-27', 'body': 'Yes @pkgoogle. I am working on developing on-device real-time training.', 'created_at': datetime.datetime(2024, 8, 1, 8, 45, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263752054, 'issue_id': 2385504088, 'author': 'pkgoogle', 'body': ""So that tutorial is very old, I think we should at the very least update everything to work with the latest version. Here's a [gist](https://colab.sandbox.google.com/gist/pkgoogle/12c0c38408514a5b840c920373754cb1/odt_tutorial.ipynb) showing how far I got on nightly. @haozha111 can you please take a look? Thanks."", 'created_at': datetime.datetime(2024, 8, 1, 18, 54, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263780325, 'issue_id': 2385504088, 'author': 'Sachi-27', 'body': ""@pkgoogle It seems you haven't given the access rights. Can you change the share permissions? Thankyou"", 'created_at': datetime.datetime(2024, 8, 1, 19, 10, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263884038, 'issue_id': 2385504088, 'author': 'pkgoogle', 'body': ""> @pkgoogle It seems you haven't given the access rights. Can you change the share permissions? Thankyou\r\n\r\nAhh wrong link, it should be updated now."", 'created_at': datetime.datetime(2024, 8, 1, 20, 4, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277297493, 'issue_id': 2385504088, 'author': 'haozha111', 'body': ""@Sachi-27 i think the way you set string tensor for the tflite save signature is a bit problematic. TF Lite's string tensor has a different encoding format with the std::string, so we can't just copy the `std:string` into TF Lite. Can you try something like:\r\n\r\n`def run_save():\r\n    save_signature = interpreter.get_signature_runner('save')\r\n    ckpt = '/tmp/model.ckpt'\r\n    output = save_signature(checkpoint_path=np.array(ckpt, dtype=np.string_))\r\n`"", 'created_at': datetime.datetime(2024, 8, 9, 7, 13, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399800782, 'issue_id': 2385504088, 'author': 'gaikwadrahul8', 'body': 'Hi, @Sachi-27 \r\n\r\nCould you please try above [workaround](https://github.com/tensorflow/tensorflow/issues/70726#issuecomment-2277297493) and see is it resolving your issue or not ? Thank you', 'created_at': datetime.datetime(2024, 10, 8, 13, 8, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415574012, 'issue_id': 2385504088, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 10, 16, 2, 3, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434074316, 'issue_id': 2385504088, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 10, 24, 2, 2, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434074389, 'issue_id': 2385504088, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70726"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70726"">No</a>', 'created_at': datetime.datetime(2024, 10, 24, 2, 2, 5, tzinfo=datetime.timezone.utc)}]","tilakrayal on (2024-07-02 10:45:40 UTC): @Sachi-27,
Could you please provide the complete code to reproduce and the tensorflow version you are trying  which helps to debug the issue. Thank you!

Sachi-27 (Issue Creator) on (2024-07-03 02:11:48 UTC): Tensorflow version : 2.9.1
I have used the same code as in [here](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training) to convert the .h5 model to .tflite with signatures.

I have made changes to the tensorflow/tlite/examples/minimal/minimal.cc code.
```
#include <cstdio>
#include <iostream>
#include <fstream>
#include <sys/time.h>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""tensorflow/lite/signature_runner.h""
#include ""tensorflow/lite/kernels/internal/tensor_ctypes.h""

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }
int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
     return 1;
  }

  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter with the InterpreterBuilder.
  // Note: all Interpreters should be built with the InterpreterBuilder,
  // which allocates memory for the Interpreter and does various set up
  // tasks so that the Interpreter can read the provided model.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);
  
int batch_sz = 10;
  std::vector<const std::string*> signature_defs = interpreter->signature_keys();
 tflite::SignatureRunner* save_runner = interpreter->GetSignatureRunner(signature_defs[2]->c_str());
  if(!save_runner){
    std::cerr << ""Cannot obtain save signature"" << std::endl; 
  }
const std::vector<const char*>& input_names_ = save_runner->input_names();
  const std::vector<const char*>& output_names_ = save_runner->output_names();
  std::cout << ""input_names.size() = "" << input_names_.size() << std::endl;
  std::cout << input_names_[0] << std::endl;
  std::cout << ""output_names.size() = "" << output_names_.size() << std::endl;
  std::cout << output_names_[0] << std::endl;

  TFLITE_MINIMAL_CHECK(save_runner->AllocateTensors() == kTfLiteOk);
  TfLiteTensor* input_tensor_ = save_runner->input_tensor(""checkpoint_path"");
  TFLITE_MINIMAL_CHECK(input_tensor_ != nullptr);
  std::string* input_ = tflite::GetTensorData<std::string>(input_tensor_);
  std::cout << ""About to set input_ value"" << std::endl;
  std::cout << ""TFLITE TYPE: "" << input_tensor_->type << std::endl;
    *input_ = ""model.ckpt"";
  
  std::cout << ""Setting input_ to: "" << input_tensor->data.raw << std::endl;

  TFLITE_MINIMAL_CHECK(save_runner->Invoke() == kTfLiteOk);
  std::cout << ""Invoked"" << std::endl;
  const TfLiteTensor* output_tensor_ = save_runner->output_tensor(""checkpoint_path"");
  std::cout << ""Obtained output_tensor_"" << std::endl;
  std::cout << ""OUTPUT checkpoint path: ""<< output_tensor->data.raw << std::endl;

  interpreter.reset();
  return 0;
}
```



COMMAND TO RUN:
`./minimal demo.tflite`

OUTPUT:
1
checkpoint_path
1
checkpoint_path
About to set input_ value
TFLITE TYPE: 5
Segmentation Fault



QUERY:
I would like to basically know how to pass in tf.string input correctly. I have also tried doing the following instead:
`input_tensor_->data.raw = ""model.ckpt"";`
But i get Segmentation fault when `save_runner->Invoke()` is called

sawantkumar (Assginee) on (2024-07-29 07:25:18 UTC): Hi @pkgoogle ,

I also got the same error, can you please look into it 

```
root@tflite-issue-replication:/home/sawantkumar/work/c_plus/tensorflow/bazel-bin/tensorflow/lite/examples/minimal# ./minimal /home/sawantkumar/work/c_plus/tensorflow/tensorflow/lite/examples/minimal/demo1.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
input_names.size() = 1
checkpoint_path
output_names.size() = 1
checkpoint_path
About to set input_ value
TFLITE TYPE: 5
Segmentation fault (core dumped)
```

pkgoogle (Assginee) on (2024-07-29 20:30:55 UTC): Hi @Sachi-27, do you absolutely need to do on device training? -- if not I can recommend other methods which may work better.

Sachi-27 (Issue Creator) on (2024-08-01 08:45:29 UTC): Yes @pkgoogle. I am working on developing on-device real-time training.

pkgoogle (Assginee) on (2024-08-01 18:54:15 UTC): So that tutorial is very old, I think we should at the very least update everything to work with the latest version. Here's a [gist](https://colab.sandbox.google.com/gist/pkgoogle/12c0c38408514a5b840c920373754cb1/odt_tutorial.ipynb) showing how far I got on nightly. @haozha111 can you please take a look? Thanks.

Sachi-27 (Issue Creator) on (2024-08-01 19:10:12 UTC): @pkgoogle It seems you haven't given the access rights. Can you change the share permissions? Thankyou

pkgoogle (Assginee) on (2024-08-01 20:04:47 UTC): Ahh wrong link, it should be updated now.

haozha111 (Assginee) on (2024-08-09 07:13:15 UTC): @Sachi-27 i think the way you set string tensor for the tflite save signature is a bit problematic. TF Lite's string tensor has a different encoding format with the std::string, so we can't just copy the `std:string` into TF Lite. Can you try something like:

`def run_save():
    save_signature = interpreter.get_signature_runner('save')
    ckpt = '/tmp/model.ckpt'
    output = save_signature(checkpoint_path=np.array(ckpt, dtype=np.string_))
`

gaikwadrahul8 on (2024-10-08 13:08:05 UTC): Hi, @Sachi-27 

Could you please try above [workaround](https://github.com/tensorflow/tensorflow/issues/70726#issuecomment-2277297493) and see is it resolving your issue or not ? Thank you

github-actions[bot] on (2024-10-16 02:03:10 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-10-24 02:02:03 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-10-24 02:02:05 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70726"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70726"">No</a>

"
2385496029,issue,open,,Advisory GHSA-84mw-34w6-2q43 contains wrong POC codes,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

N/A

### Custom code

No

### OS platform and distribution

N/A

### Mobile device

N/A

### Python version

N/A

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It seems that description of Gtihub advisory [GHSA-84mw-34w6-2q43](https://github.com/advisories/GHSA-84mw-34w6-2q43) contains POC codes from another advisory [GHSA-772p-x54p-hjrv](https://github.com/advisories/GHSA-772p-x54p-hjrv). This causes issues in understanding the vulnerability.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_",Non1187,2024-07-02 07:45:33+00:00,['tilakrayal'],2024-07-29 12:24:20+00:00,,https://github.com/tensorflow/tensorflow/issues/70724,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:core', 'issues related to core part of tensorflow')]","[{'comment_id': 2216853760, 'issue_id': 2385496029, 'author': 'tilakrayal', 'body': '@mihaimaruseac', 'created_at': datetime.datetime(2024, 7, 9, 7, 49, 35, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-09 07:49:35 UTC): @mihaimaruseac

"
2383832019,issue,closed,completed,Data augmentation for object detection with bounding boxes,"Hello everyone,
I need advice for an effective data augmentation for an object detection problem (YOLO neural network) in tensorflow/keras. 
My first option was to use tf keras layers, but not all of them support bounding boxes. For example: keras_cv.layers.JitteredResize() supports bounding boxes but tf.keras.layers.RandomZoom() does not. 
What would be your advice? 
Additionally, I would be interested in the possibility of using ImageDataGenerator, is there a way to include my bounding boxes into the generator? 
Thank you a lot.",tirk999,2024-07-01 13:17:02+00:00,['Venkat6871'],2024-07-19 01:52:32+00:00,2024-07-19 01:52:29+00:00,https://github.com/tensorflow/tensorflow/issues/70687,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:keras', 'Keras related issues')]","[{'comment_id': 2208076195, 'issue_id': 2383832019, 'author': 'Venkat6871', 'body': 'Hi **@tirk999** ,\r\n- Data augmentation is crucial for improving the performance of object detection models like YOLO. For your use case with TensorFlow/Keras, here i will provide some recommendations and strategies to handle data augmentation while considering bounding boxes:\r\n- Supported Layers: Utilize layers such as keras_cv.layers.JitteredResize() that support bounding boxes.\r\n- ImageDataGenerator in TensorFlow does not directly support bounding boxes. However, you can create a custom generator to include bounding boxes.\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 4, 3, 57, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2224272614, 'issue_id': 2383832019, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 12, 1, 51, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237880218, 'issue_id': 2383832019, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 19, 1, 52, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237880478, 'issue_id': 2383832019, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70687"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70687"">No</a>', 'created_at': datetime.datetime(2024, 7, 19, 1, 52, 31, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-04 03:57:49 UTC): Hi **@tirk999** ,
- Data augmentation is crucial for improving the performance of object detection models like YOLO. For your use case with TensorFlow/Keras, here i will provide some recommendations and strategies to handle data augmentation while considering bounding boxes:
- Supported Layers: Utilize layers such as keras_cv.layers.JitteredResize() that support bounding boxes.
- ImageDataGenerator in TensorFlow does not directly support bounding boxes. However, you can create a custom generator to include bounding boxes.
Thank you!

github-actions[bot] on (2024-07-12 01:51:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-19 01:52:29 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-19 01:52:31 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70687"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70687"">No</a>

"
2383379342,issue,closed,not_planned,Tflite inference with multiple inputs and outputs on Android,,panhu,2024-07-01 09:53:11+00:00,['pkgoogle'],2024-09-27 17:44:25+00:00,2024-09-27 17:44:22+00:00,https://github.com/tensorflow/tensorflow/issues/70682,"[('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('2.6.0', ''), ('Android', '')]","[{'comment_id': 2231273143, 'issue_id': 2383379342, 'author': 'sawantkumar', 'body': 'Hi @panhu ,\r\nPlease take a look at this [link](https://www.tensorflow.org/lite/guide/inference#android_platform) and let me know if this what you are looking for.', 'created_at': datetime.datetime(2024, 7, 16, 15, 46, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246710394, 'issue_id': 2383379342, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 24, 1, 53, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379441928, 'issue_id': 2383379342, 'author': 'gaikwadrahul8', 'body': 'Hi, @pkgoogle \r\nPlease take look into this issue. Thank you', 'created_at': datetime.datetime(2024, 9, 27, 14, 41, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379771770, 'issue_id': 2383379342, 'author': 'pkgoogle', 'body': ""No description, I'm assuming this is spam, if there is a real issue please create a new one with a description. Thanks."", 'created_at': datetime.datetime(2024, 9, 27, 17, 44, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379771809, 'issue_id': 2383379342, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70682"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70682"">No</a>', 'created_at': datetime.datetime(2024, 9, 27, 17, 44, 23, tzinfo=datetime.timezone.utc)}]","sawantkumar on (2024-07-16 15:46:27 UTC): Hi @panhu ,
Please take a look at this [link](https://www.tensorflow.org/lite/guide/inference#android_platform) and let me know if this what you are looking for.

github-actions[bot] on (2024-07-24 01:53:16 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

gaikwadrahul8 on (2024-09-27 14:41:22 UTC): Hi, @pkgoogle 
Please take look into this issue. Thank you

pkgoogle (Assginee) on (2024-09-27 17:44:22 UTC): No description, I'm assuming this is spam, if there is a real issue please create a new one with a description. Thanks.

google-ml-butler[bot] on (2024-09-27 17:44:23 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70682"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70682"">No</a>

"
2382954183,issue,closed,completed,Indexing error (graph execution error) by tf.keras.metrics.OneHotIoU metric,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I get the following error while fine-tuning a vgg16 model for multiclass object detection. The model has 2 output heads for class label and bounding boxes regression. The error seems to be while the evaluation by the IoU metric for bounding boxes. The ground truth labels are one-hot encoded. The shape of the images is 512*512, output tensor shapes are (6,4) and (6,3). The actual labels and annotations have the same shapes. What could be the possible reason for the error? 

```
`Traceback (most recent call last):
  File ""C:\Users\user\model\train.py"", line 164, in <module>
    history = model.fit(
  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\utils\traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\user\model\env1\lib\site-packages\tensorflow\python\eager\execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node ScatterNd defined at (most recent call last):
  File ""C:\Users\user\model\V_tf_model_train.py"", line 164, in <module>

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\utils\traceback_utils.py"", line 117, in error_handler

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 318, in fit

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 121, in one_step_on_iterator

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 108, in one_step_on_data  

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\backend\tensorflow\trainer.py"", line 77, in train_step

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\trainers\trainer.py"", line 444, in compute_metrics

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\trainers\compile_utils.py"", line 330, in update_state

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\trainers\compile_utils.py"", line 17, in update_state

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\metrics\iou_metrics.py"", line 129, in update_state

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\metrics\metrics_utils.py"", line 682, in confusion_matrix       

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\ops\core.py"", line 237, in scatter

  File ""C:\Users\user\model\env1\lib\site-packages\keras\src\backend\tensorflow\core.py"", line 354, in scatter

indices[0] = [280, 0] does not index into shape [3,3]
         [[{{node ScatterNd}}]] [Op:__inference_one_step_on_iterator_4213]`
```
### Standalone code to reproduce the issue

```shell
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(512, 512, 3))

# Freeze convolutional layers
for layer in base_model.layers:
    layer.trainable = False

flatten = base_model.output
flatten = Flatten()(flatten)

# FC layer for bounding box prediction
bboxHead = Dense(128, activation=""relu"")(flatten)
bboxHead = Dense(64, activation=""relu"")(bboxHead)
bboxHead = Dense(32, activation=""relu"")(bboxHead)
bboxHead = Dense(bbox_dim1*bbox_dim2, activation=""sigmoid"")(bboxHead)
bboxHead = Reshape((bbox_dim1, bbox_dim2), name=""bounding_box"")(bboxHead)

# Second fully-connected layer head to predict the class label
softmaxHead = Dense(512, activation=""relu"")(flatten)
softmaxHead = Dropout(0.5)(softmaxHead)
softmaxHead = Dense(512, activation=""relu"")(softmaxHead)
softmaxHead = Dropout(0.5)(softmaxHead)
softmaxHead = Dense(label_dim1*label_dim2, activation=""softmax"")(softmaxHead)
softmaxHead = Reshape((label_dim1, label_dim2), name=""class_label"")(softmaxHead)

# Create the model
model = Model(inputs=base_model.input, outputs=[bboxHead, softmaxHead])

losses = {
    ""class_label"": ""categorical_crossentropy"",
    ""bounding_box"": ""mean_squared_error"",
}

metrics = {
    ""class_label"": ""categorical_accuracy"",
    ""bounding_box"": tf.keras.metrics.IoU(num_classes, target_class_ids = [0, 1])
}

model.compile(optimizer= Adam(learning_rate=0.001), loss=losses, metrics=metrics)

history = model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=1,
    steps_per_epoch=len(train_images) // 10,
    validation_steps=len(test_images) // 10,
    verbose=1
)
```


### Relevant log output

_No response_",beatsea20,2024-07-01 06:25:54+00:00,['tilakrayal'],2024-08-13 01:55:38+00:00,2024-08-13 01:55:33+00:00,https://github.com/tensorflow/tensorflow/issues/70673,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('comp:apis', 'Highlevel API related issues'), ('type:performance', 'Performance Issue'), ('TF 2.16', '')]","[{'comment_id': 2199650374, 'issue_id': 2382954183, 'author': 'sushreebarsa', 'body': '@beatsea20 Could you please provide standalone code to replicate the issue reported here?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 1, 9, 19, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199911326, 'issue_id': 2382954183, 'author': 'beatsea20', 'body': '@sushreebarsa I have provided the model training code in the question, for the data generator part the code is:\r\n\r\n```\r\nclass Dataloader():\r\n    def __init__(self, annotation_path, image_path) -> None:\r\n        self.annotation_path = annotation_path\r\n        self.image_path = image_path\r\n\r\n    def load(self):\r\n        images = []\r\n        labels = []\r\n        bboxes = []\r\n\r\n        #annotation loader\r\n        for file in os.listdir(self.annotation_path):\r\n            annotation = load_annotations(self.annotation_path, file)\r\n            labels.append(tuple([d[\'class\'] for d in annotation]))\r\n            bboxes.append(tuple((d[\'x1\'], d[\'y1\'], d[\'x2\'], d[\'y2\']) for d in annotation))\r\n\r\n        #image loader\r\n        for file in os.listdir(self.image_path):\r\n            if file.endswith(\'.jpg\'):\r\n                img = cv2.imread(os.path.join(self.image_path, file))\r\n                img = cv2.resize(img, (512,512))\r\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n                images.append(img)\r\n\r\n        categories = [\'Hotspots\', \'Hotstring\', \'Bypass Diode active\']\r\n        labels = tf.ragged.constant(labels)\r\n        layer = StringLookup(vocabulary=categories)\r\n        indices = layer(labels)\r\n        enc_layer = CategoryEncoding(num_tokens=3, output_mode=""one_hot"")\r\n        indices = enc_layer(indices)\r\n        labels = indices.to_tensor()\r\n        bboxes = tf.ragged.constant(bboxes, dtype=tf.float32)\r\n        bboxes = bboxes.to_tensor()\r\n\r\n        return images, labels, bboxes\r\n\r\n\r\ndef data_generator(images, targets, batch_size=10):\r\n    while True:\r\n        for i in range(0, len(images), batch_size):\r\n            batch_images = images[i:i+batch_size]\r\n            batch_labels = targets[1][i:i+batch_size]\r\n            batch_bboxes = targets[0][i:i+batch_size]\r\n            yield np.array(batch_images), {""bounding_box"": np.array(batch_bboxes), ""class_label"": np.array(batch_labels)}\r\n\r\ndef create_dataset(images, targets, batch_size=10):\r\n    n, m = targets[0][0].shape\r\n    p, q = targets[1][0].shape\r\n    dataset = tf.data.Dataset.from_generator(\r\n        lambda: data_generator(images, targets, batch_size),\r\n        output_signature=(\r\n            tf.TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32), \r\n            {\r\n                ""bounding_box"": tf.TensorSpec(shape=(batch_size, n, m), dtype=tf.float32),\r\n                ""class_label"": tf.TensorSpec(shape=(batch_size, p, q), dtype=tf.float32)\r\n            }\r\n        )\r\n    )\r\n    return dataset\r\n\r\ntrain_dataset = create_dataset(train_images, train_targets, batch_size=10)\r\ntest_dataset = create_dataset(test_images, test_targets, batch_size=10)\r\n```', 'created_at': datetime.datetime(2024, 7, 1, 11, 36, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2201926932, 'issue_id': 2382954183, 'author': 'sushreebarsa', 'body': ""@beatsea20 Please ensure the order of loading and processing images and annotations is consistent as an example provided in this [gist](https://colab.research.google.com/gist/sushreebarsa/39fb0882b138a6dfba5c14d14766477e/70673.ipynb).\r\nI am facing `NameError: name 'train_images' is not defined `while trying to replicate the issue?\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 2, 4, 58, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2202015331, 'issue_id': 2382954183, 'author': 'beatsea20', 'body': 'Hey, I have added you as a collaborator in a repo. kindly look at the code and sample files.', 'created_at': datetime.datetime(2024, 7, 2, 6, 9, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208263356, 'issue_id': 2382954183, 'author': 'beatsea20', 'body': '@sushreebarsa  kindly accept the invitation request and please help me!', 'created_at': datetime.datetime(2024, 7, 4, 7, 5, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2213345047, 'issue_id': 2382954183, 'author': 'tilakrayal', 'body': '@beatsea20,\r\nCOuld you please provide the link where you provided the invitation or the colab gist with the code, so that it will be easy to analyse the issue. Thank you!', 'created_at': datetime.datetime(2024, 7, 8, 8, 23, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2225158303, 'issue_id': 2382954183, 'author': 'beatsea20', 'body': 'Sent you the invite @tilakrayal', 'created_at': datetime.datetime(2024, 7, 12, 9, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2255804680, 'issue_id': 2382954183, 'author': 'tilakrayal', 'body': ""@beatsea20,\r\nApologies for the delay. I tried to find the invite and couldn't find the same. Is it possible to post the code or the colab gist for easy access. And from the error it looks like this is due to keras. Thank you!"", 'created_at': datetime.datetime(2024, 7, 29, 12, 30, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270213553, 'issue_id': 2382954183, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 6, 1, 53, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196465, 'issue_id': 2382954183, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285196548, 'issue_id': 2382954183, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70673"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70673"">No</a>', 'created_at': datetime.datetime(2024, 8, 13, 1, 55, 37, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-01 09:19:20 UTC): @beatsea20 Could you please provide standalone code to replicate the issue reported here?
Thank you!

beatsea20 (Issue Creator) on (2024-07-01 11:36:19 UTC): @sushreebarsa I have provided the model training code in the question, for the data generator part the code is:

```
class Dataloader():
    def __init__(self, annotation_path, image_path) -> None:
        self.annotation_path = annotation_path
        self.image_path = image_path

    def load(self):
        images = []
        labels = []
        bboxes = []

        #annotation loader
        for file in os.listdir(self.annotation_path):
            annotation = load_annotations(self.annotation_path, file)
            labels.append(tuple([d['class'] for d in annotation]))
            bboxes.append(tuple((d['x1'], d['y1'], d['x2'], d['y2']) for d in annotation))

        #image loader
        for file in os.listdir(self.image_path):
            if file.endswith('.jpg'):
                img = cv2.imread(os.path.join(self.image_path, file))
                img = cv2.resize(img, (512,512))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                images.append(img)

        categories = ['Hotspots', 'Hotstring', 'Bypass Diode active']
        labels = tf.ragged.constant(labels)
        layer = StringLookup(vocabulary=categories)
        indices = layer(labels)
        enc_layer = CategoryEncoding(num_tokens=3, output_mode=""one_hot"")
        indices = enc_layer(indices)
        labels = indices.to_tensor()
        bboxes = tf.ragged.constant(bboxes, dtype=tf.float32)
        bboxes = bboxes.to_tensor()

        return images, labels, bboxes


def data_generator(images, targets, batch_size=10):
    while True:
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            batch_labels = targets[1][i:i+batch_size]
            batch_bboxes = targets[0][i:i+batch_size]
            yield np.array(batch_images), {""bounding_box"": np.array(batch_bboxes), ""class_label"": np.array(batch_labels)}

def create_dataset(images, targets, batch_size=10):
    n, m = targets[0][0].shape
    p, q = targets[1][0].shape
    dataset = tf.data.Dataset.from_generator(
        lambda: data_generator(images, targets, batch_size),
        output_signature=(
            tf.TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32), 
            {
                ""bounding_box"": tf.TensorSpec(shape=(batch_size, n, m), dtype=tf.float32),
                ""class_label"": tf.TensorSpec(shape=(batch_size, p, q), dtype=tf.float32)
            }
        )
    )
    return dataset

train_dataset = create_dataset(train_images, train_targets, batch_size=10)
test_dataset = create_dataset(test_images, test_targets, batch_size=10)
```

sushreebarsa on (2024-07-02 04:58:15 UTC): @beatsea20 Please ensure the order of loading and processing images and annotations is consistent as an example provided in this [gist](https://colab.research.google.com/gist/sushreebarsa/39fb0882b138a6dfba5c14d14766477e/70673.ipynb).
I am facing `NameError: name 'train_images' is not defined `while trying to replicate the issue?
Thank you!

beatsea20 (Issue Creator) on (2024-07-02 06:09:40 UTC): Hey, I have added you as a collaborator in a repo. kindly look at the code and sample files.

beatsea20 (Issue Creator) on (2024-07-04 07:05:14 UTC): @sushreebarsa  kindly accept the invitation request and please help me!

tilakrayal (Assginee) on (2024-07-08 08:23:01 UTC): @beatsea20,
COuld you please provide the link where you provided the invitation or the colab gist with the code, so that it will be easy to analyse the issue. Thank you!

beatsea20 (Issue Creator) on (2024-07-12 09:11:33 UTC): Sent you the invite @tilakrayal

tilakrayal (Assginee) on (2024-07-29 12:30:28 UTC): @beatsea20,
Apologies for the delay. I tried to find the invite and couldn't find the same. Is it possible to post the code or the colab gist for easy access. And from the error it looks like this is due to keras. Thank you!

github-actions[bot] on (2024-08-06 01:53:40 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-08-13 01:55:33 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-08-13 01:55:37 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70673"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70673"">No</a>

"
2382925477,issue,open,,tf.matmul gives inconsistent results for same data,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.8.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 11.8

### GPU model and memory

_No response_

### Current behavior?

I want to do matmul(A,transpose(A)) and matmul(B,transpose(B)). Here, B is just matrix A with additional rows. A is 8 by 512 and b is 64 by 512 with first 8 rows exactly as that of A.

lets call
A_out = A matmul transpose(A), this is 8 by 8,
B_out = B matmul transpose(B), this is 64 by 64.

So the above matmul just dots each row of a matrix with all the other rows of the same matrix. So if the first 8 rows in B are same as A then the top-left, 8 by 8 sub matrix of B_out should exactly be same as A_out.

tf.matmul operation doesnt produce this. however numpy does.

Please look into this

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.layers import Embedding

# Set up
embedding = Embedding(36711,512,mask_zero=True)
example_sequence = tf.constant([36710,  5095,   466, 16678,     5,     3,  5152, 36711] + [0]*(64-8))

pad_embed    = example_sequence # input with padding       # has shape (64,)
no_pad_embed = example_sequence[:8] # input without padding # has shape (8,)

no_pad_embed = embedding(no_pad_embed) # shape (8,512)  # A
pad_embed    = embedding(pad_embed)    # shape (64,512) # B : has same first 8 rows as A


# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_out    = tf.matmul(pad_embed,pad_embed, transpose_b=True)
no_pad_out = tf.matmul(no_pad_embed,no_pad_embed, transpose_b=True)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # False, meaning that the upper left 8 by 8 submatrix of pad_out is not equal to no_pad_out

import numpy as np

# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_embed   = pad_embed.numpy()
no_pad_embed= no_pad_embed.numpy()

pad_out    = np.matmul(pad_embed,pad_embed.T)
no_pad_out = np.matmul(no_pad_embed,no_pad_embed.T)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # True, meaning 8 by 8 submatrix of pad_out equals no_pad_out
```


### Relevant log output

```shell
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor(True, shape=(), dtype=bool)
```
",syedhamzamohiuddin,2024-07-01 06:09:26+00:00,['Venkat6871'],2024-07-03 06:20:21+00:00,,https://github.com/tensorflow/tensorflow/issues/70672,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:ops', 'OPs related issues'), ('TF 2.8', '')]","[{'comment_id': 2203399812, 'issue_id': 2382925477, 'author': 'Venkat6871', 'body': 'Hi **@syedhamzamohiuddin** ,\r\n- I tried to run your code on Colab using TF v2.16.1, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/d871e6b5c38a040993a2d2bcdf580299/70672_2-16-1-nightly-v.ipynb) here for reference.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 2, 14, 40, 9, tzinfo=datetime.timezone.utc)}]","Venkat6871 (Assginee) on (2024-07-02 14:40:09 UTC): Hi **@syedhamzamohiuddin** ,
- I tried to run your code on Colab using TF v2.16.1, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/d871e6b5c38a040993a2d2bcdf580299/70672_2-16-1-nightly-v.ipynb) here for reference.

Thank you!

"
2382710994,issue,closed,completed,how to assign tensorflow operation running-time device,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

20.04

### Mobile device

ubantu 20.04

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

 CUDA Version: 11.8 

### GPU model and memory

A6000  46G

### Current behavior?

I am trying to optimize the inference time of the TF model, which is a DeepFM model with feature columns. The costs time of singal inference of GPU and CPU device are 30ms and 3ms. The difference between them is mainly because of the H2D and D2H conversions (CPU->GPU and GPU->CPU).
So, what I want to do is force the operation of feature column subgraph runing in CPU, and the operation of neural network subgraph running in GPU.
As far as I know, the running time operation device is dynamic and aligned by tensorflow automatically.
Is there some way I can align the running time device of operations, either in the training part or loading the saved model in inference?
The best way I prefer is to load the saved model and then align the running time device of the OP and save it to the savedmodel to be served by TFS.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
N/A
```
",immusferr,2024-07-01 03:20:20+00:00,['tilakrayal'],2024-07-19 01:52:30+00:00,2024-07-19 01:52:30+00:00,https://github.com/tensorflow/tensorflow/issues/70670,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:feature', 'Feature requests'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('type:docs-feature', 'Doc issues for new feature, or clarifications about functionality')]","[{'comment_id': 2205881553, 'issue_id': 2382710994, 'author': 'tilakrayal', 'body': '@immusferr,\r\nHave you got the chance to have a look at the Custom training loop.\r\n\r\nhttps://www.tensorflow.org/tutorials/customization/custom_layers\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nAlso If you have control over the training process, the approach below offers the solution. Try to wrap the creation of your feature column processing logic within a **tf.device(""/cpu:0"")** scope.\r\n\r\n```python\r\ndef feature_processing(features):\r\n    with tf.device(""/cpu:0""):\r\n        processed_features = \r\n    return processed_features\r\n\r\n\r\nmodel = (DeepFM model definition using processed_features)\r\ntf.saved_model.save(model, export_dir=""path/to/save"")\r\n\r\n```\r\n\r\nThe above try the  feature column operations are explicitly placed on the CPU during training, and this placement is saved within the model definition. Thank you!', 'created_at': datetime.datetime(2024, 7, 3, 11, 43, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2221837973, 'issue_id': 2382710994, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 11, 1, 52, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237880344, 'issue_id': 2382710994, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 19, 1, 52, 30, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-03 11:43:15 UTC): @immusferr,
Have you got the chance to have a look at the Custom training loop.

https://www.tensorflow.org/tutorials/customization/custom_layers
https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch

Also If you have control over the training process, the approach below offers the solution. Try to wrap the creation of your feature column processing logic within a **tf.device(""/cpu:0"")** scope.

```python
def feature_processing(features):
    with tf.device(""/cpu:0""):
        processed_features = 
    return processed_features


model = (DeepFM model definition using processed_features)
tf.saved_model.save(model, export_dir=""path/to/save"")

```

The above try the  feature column operations are explicitly placed on the CPU during training, and this placement is saved within the model definition. Thank you!

github-actions[bot] on (2024-07-11 01:52:46 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-19 01:52:30 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

"
2382348778,issue,closed,not_planned,Tflite model fails on Android GPU delegate due to 'null pointer dereference',"Hi there,

My tflite model fails on GPU delegate, using Android. Without the GPU delegate the model works fine using CPU.

I cut the model until I found out at which point the model crashes. It seems to be underneath node:

<img width=""699"" alt=""afbeelding"" src=""https://github.com/tensorflow/tensorflow/assets/20172454/0d4808ef-b648-4e38-9889-07ee44292447"">

**Models used:**
Model suceeding: https://www.dropbox.com/scl/fi/iz96vmzrr99wpl40mr5pv/till480_7_7_float32-works.tflite?rlkey=ofqjleukotz1wnbmzovb5cddm&st=ww59mctf&dl=0
Model failing: https://www.dropbox.com/scl/fi/2ne8owmufj7ks3mcki818/960_7_7_float32-crashes.tflite?rlkey=ij49ion6d3xjt5lp0fapp2n2r&st=x6ftxmbo&dl=0

I'm using tensorflow 2.16.1 and onnx2tf 1.22.4:
implementation 'org.tensorflow:tensorflow-lite:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-gpu:2.16.1'
implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.16.1'

Any idea what could be wrong or how I can debug this further?

Thanks for your help!
Best regards,
Ramon

**Full stacktrace:**
I/GPU     (10563): org.tensorflow.lite.gpu.GpuDelegate$Options@3e97741
I/GPU     (10563): GPU is supported and will be used for inference.
I/tflite  (10563): Replacing 299 out of 299 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
F/libc    (10563): Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x18 in tid 11103 (background), pid 10563 (g_app)
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'samsung/a52qnseea/a52q:13/TP1A.220624.014/A525FXXS6DWG2:user/release-keys'
Revision: '8'
ABI: 'arm64'
Processor: '7'
Timestamp: 2024-06-30 17:23:08.630221214+0200
Process uptime: 137s
Cmdline: com.example.debug_app
pid: 10563, tid: 11103, name: background  >>> com.example.debug_app <<<
uid: 10448
signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0000000000000018
Cause: null pointer dereference
    x0  b400007a9f865700  x1  0000000000000079  x2  0000000000000000  x3  0000000000000004
    x4  0000000000000000  x5  0000000000000000  x6  0000000000000000  x7  0000000000000000
    x8  0000000000000004  x9  0000000000000001  x10 b400007a5983a370  x11 380c88acc82776a5
    x12 000000000000001f  x13 0000000000000001  x14 0000007a6a05251e  x15 0000007a6ad1f0f4
    x16 0000000000000000  x17 00000000000000ff  x18 0000007a64a28000  x19 0000007b021f6000
    x20 0000000000000000  x21 b400007a9f865700  x22 0000000000000079  x23 0000000000000000
    x24 0000007b021f17f0  x25 0000000000000002  x26 0000000000000004  x27 0000007b021f6000
    x28 0000000000000079  x29 0000007b021f1410
    lr  0000000000000000  sp  0000007b021f12f0  pc  0000007a6acd49f0  pst 0000000080001000
backtrace:
      #00 pc 0000000000dcc9f0  /vendor/lib64/libllvm-qcom.so (llvm::SelectionDAG::getNode(unsigned int, llvm::DebugLoc, llvm::EVT, llvm::SDValue)+64) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #01 pc 0000000000e172a0  /vendor/lib64/libllvm-qcom.so (getCopyFromParts(llvm::SelectionDAG&, llvm::DebugLoc, llvm::SDValue const*, unsigned int, llvm::EVT, llvm::EVT, llvm::ISD::NodeType)+2952) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #02 pc 0000000000e1a1a0  /vendor/lib64/libllvm-qcom.so (llvm::SelectionDAGISel::LowerArguments(llvm::BasicBlock const*)+3840) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #03 pc 0000000000e251e8  /vendor/lib64/libllvm-qcom.so (llvm::SelectionDAGISel::SelectAllBasicBlocks(llvm::Function const&)+1336) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #04 pc 0000000000e2449c  /vendor/lib64/libllvm-qcom.so (llvm::SelectionDAGISel::runOnMachineFunction(llvm::MachineFunction&)+1420) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #05 pc 000000000092942c  /vendor/lib64/libllvm-qcom.so (llvm::FPPassManager::runOnFunction(llvm::Function&)+884) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #06 pc 0000000000928bc8  /vendor/lib64/libllvm-qcom.so (llvm::FunctionPassManagerImpl::run(llvm::Function&)+192) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #07 pc 00000000009289f8  /vendor/lib64/libllvm-qcom.so (llvm::FunctionPassManager::run(llvm::Function&)+104) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #08 pc 0000000000f1a1cc  /vendor/lib64/libllvm-qcom.so (llvm::llclib::CompileInComplexPipeline(llvm::Module&, llvm::Triple const&, llvm::TargetMachine&, void* (*)(unsigned int), llvm::PassOverrides&, llvm::RunnableListType<llvm::RunnableListType<llvm::OptManager> >*, llvm::CLPrintfInterpreter const*, llvm::TimeRegion*, llvm::Module*, llvm::formatted_raw_ostream&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, char**, unsigned int&)+2164) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #09 pc 0000000000f1b05c  /vendor/lib64/libllvm-qcom.so (llvm::llclib::Compile(llvm::Module*, void* (*)(unsigned int), char**, unsigned int&, llvm::Module*, llvm::CLPrintfInterpreter const*)+2460) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #10 pc 0000000001ac905c  /vendor/lib64/libllvm-qcom.so (clang::clanglib::Codegen(llvm::MemoryBuffer*, cl_compiler_target, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, llvm::OwningArrayPtr<char>&, unsigned int&, cl_rs_compiler_info*)+1052) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #11 pc 0000000001aeb554  /vendor/lib64/libllvm-qcom.so ((anonymous namespace)::BasicCompilation::link()+4044) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #12 pc 0000000001ae5dfc  /vendor/lib64/libllvm-qcom.so (cl_compiler_link_program+492) (BuildId: 90db7b70a33a3223117d01259e5d89fd)
      #13 pc 000000000027b52c  /vendor/lib64/libCB.so (cl_program_link_immediate+956) (BuildId: 4a1aa94f6bc5d4e8fb2d539d1411636d)
      #14 pc 000000000027a5d0  /vendor/lib64/libCB.so (cl_program_build_immediate+272) (BuildId: 4a1aa94f6bc5d4e8fb2d539d1411636d)
      #15 pc 0000000000280448  /vendor/lib64/libCB.so (cb_build_program+1072) (BuildId: 4a1aa94f6bc5d4e8fb2d539d1411636d)
      #16 pc 0000000000013360  /vendor/lib64/libOpenCL.so (qCLDrvAPI_clBuildProgram+112) (BuildId: c2a2acac9160da02966c1071cb85246e)
      #17 pc 000000000013c1b8  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #18 pc 000000000013c0e4  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #19 pc 000000000013782c  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #20 pc 00000000000e6e7c  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #21 pc 00000000000dd4a0  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #22 pc 00000000000dcf88  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #23 pc 00000000000dcd60  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #24 pc 00000000000d7304  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #25 pc 00000000000ad7b8  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #26 pc 00000000000ad22c  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #27 pc 00000000000ae204  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #28 pc 00000000002febd8  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #29 pc 00000000002fe5bc  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #30 pc 00000000002fe1d8  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #31 pc 00000000000aa7f0  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_gpu_jni.so (BuildId: 3b36f80b867134f597c198e81d9bec61)
      #32 pc 0000000000302f98  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #33 pc 000000000030353c  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #34 pc 00000000002f6370  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #35 pc 00000000002f914c  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #36 pc 00000000002f9668  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #37 pc 000000000007f4d8  /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_createInterpreter+692) (BuildId: 385d6d92f29b4b4cb3ffca33758d3471)
      #38 pc 0000000000355830  /apex/com.android.art/lib64/libart.so (art_quick_generic_jni_trampoline+144) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #39 pc 000000000033f080  /apex/com.android.art/lib64/libart.so (art_quick_invoke_static_stub+640) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #40 pc 000000000037dde8  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+416) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #41 pc 000000000037d598  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<true>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+1960) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #42 pc 000000000049a6d8  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+14012) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #43 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #44 pc 000000000006e220  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes7.dex] (org.tensorflow.lite.NativeInterpreterWrapper.init+0)
      #45 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #46 pc 000000000037db04  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+100) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #47 pc 000000000037d534  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<true>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+1860) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #48 pc 0000000000499b48  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+11052) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #49 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #50 pc 000000000006df38  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes7.dex] (org.tensorflow.lite.NativeInterpreterWrapper.<init>+0)
      #51 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #52 pc 0000000000511d1c  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+5252) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #53 pc 0000000000497814  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+2040) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #54 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #55 pc 000000000006d8f8  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes7.dex] (org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>+0)
      #56 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #57 pc 0000000000511d1c  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+5252) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #58 pc 0000000000497814  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+2040) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #59 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #60 pc 000000000006d728  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes7.dex] (org.tensorflow.lite.Interpreter.<init>+0)
      #61 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #62 pc 0000000000511d1c  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+5252) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #63 pc 0000000000497814  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+2040) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #64 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #65 pc 0000000000006fc4  [anon:dalvik-classes5.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes5.dex] (com.example.debug_app.detection.TfliteEdgeyoloRunner.<init>+0)
      #66 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #67 pc 0000000000511d1c  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, bool, art::JValue*)+5252) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #68 pc 0000000000497814  /apex/com.android.art/lib64/libart.so (void art::interpreter::ExecuteSwitchImplCpp<false>(art::interpreter::SwitchImplContext*)+2040) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #69 pc 0000000000357fd8  /apex/com.android.art/lib64/libart.so (ExecuteSwitchImplAsm+8) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #70 pc 00000000000049a0  [anon:dalvik-classes5.dex extracted in memory from /data/app/~~sh8Q2LYw9jz26oygi1RiqQ==/com.example.debug_app-0VmdEVI_oHQgAjGD37PnJQ==/base.apk!classes5.dex] (com.example.debug_app.detection.Detector$detectInImage$backgroundThread$1.run+0)
      #71 pc 0000000000374120  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.__uniq.112435418011751916792819755956732575238.llvm.420609892041422114)+232) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #72 pc 0000000000373a18  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+964) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #73 pc 0000000000355968  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #74 pc 000000000033eda4  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+612) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #75 pc 0000000000239d54  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+144) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #76 pc 000000000053a1b0  /apex/com.android.art/lib64/libart.so (art::Thread::CreateCallback(void*)+1600) (BuildId: 735f12f804f88d62a2cb437261076ff7)
      #77 pc 00000000000f5298  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+208) (BuildId: 1bcad8bca80d38bceb9089f70d394e33)
      #78 pc 000000000008ebdc  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+68) (BuildId: 1bcad8bca80d38bceb9089f70d394e33)
Lost connection to device.

Exited.

",ramonhollands,2024-06-30 17:22:18+00:00,"['arfaian', 'pkgoogle']",2024-12-24 12:09:58+00:00,2024-11-27 18:03:58+00:00,https://github.com/tensorflow/tensorflow/issues/70664,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:support', 'Support issues'), ('comp:lite', 'TF Lite related issues'), ('TFLiteGpuDelegate', 'TFLite Gpu delegate issue'), ('TF 2.16', '')]","[{'comment_id': 2199642926, 'issue_id': 2382348778, 'author': 'sushreebarsa', 'body': '@ramonhollands Please ensure that the GPU delegate is configured correctly in your Android application and you are using TF latest version as well?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 1, 9, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199702554, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': ""Thanks for your reply.\r\n\r\nI'm using the latest tensorflow version ([2.16.1](https://mvnrepository.com/artifact/org.tensorflow/tensorflow-lite/2.16.1))\r\n\r\nThe GPU delegate looks correctly configured because it works on one model and I'm using the exact same configuration for the other model, right?"", 'created_at': datetime.datetime(2024, 7, 1, 9, 45, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2200246288, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': '@sushreebarsa, just to be sure, I am tagging your name. Thanks in advance for your response.', 'created_at': datetime.datetime(2024, 7, 1, 14, 4, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208513440, 'issue_id': 2382348778, 'author': 'sawantkumar', 'body': 'Hi @ramonhollands ,\r\n\r\nI tried running your model on the dimensity 9000 gpu, and both the models ran successfully using the GPU Delegate, can you tell me which device are you running your model. If possible can you also give me the inference script ?', 'created_at': datetime.datetime(2024, 7, 4, 9, 23, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208562977, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': ""Hi @sawantkumar,\r\n\r\nThanks for checking out! \r\nI'm running on a Galaxy A52 device.\r\n\r\nAttached, the code I'm using (striped to the essentials).\r\n\r\nRegards, Ramon\r\n\r\n```package com.example.debug_gpu.detection;\r\n\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.RectF;\r\nimport android.os.Build;\r\n\r\nimport java.io.BufferedReader;\r\nimport java.io.IOException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.util.ArrayList;\r\nimport java.util.Comparator;\r\nimport java.util.HashMap;\r\nimport java.util.Map;\r\nimport java.util.PriorityQueue;\r\nimport java.util.Vector;\r\n\r\nimport android.util.Log;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.Tensor;\r\nimport org.tensorflow.lite.gpu.CompatibilityList;\r\nimport org.tensorflow.lite.gpu.GpuDelegate;\r\n\r\npublic class TfliteEdgeyoloRunner extends AbstractBoundingBoxRunner {\r\n    public float getObjThresh() {\r\n        return 0.5f;\r\n    }\r\n    private MappedByteBuffer tfliteModel;\r\n    private final Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n    private Vector<String> labels = new Vector<String>();\r\n    private int[] intValues;\r\n    private int outputBoxes;\r\n\r\n    private ByteBuffer imgDataBuffer;\r\n    private ByteBuffer outDataBuffer;\r\n\r\n    private Interpreter tfLite;\r\n    private float inp_scale;\r\n    private int inp_zero_point;\r\n    private float oup_scale;\r\n    private int oup_zero_point;\r\n    private int numClass;\r\n\r\n    private int preInf;\r\n    private int postInf;\r\n    private int inf;\r\n\r\n    public int getPreInf() {\r\n        return preInf;\r\n    }\r\n\r\n    public int getPostInf() {\r\n        return postInf;\r\n    }\r\n\r\n    public int getInf() {\r\n        return inf;\r\n    }\r\n\r\n    /**\r\n     * Initializes the detector\r\n     *\r\n     * @param tfliteModel MappedByteBuffer TfLite model\r\n     * @param labels_br BufferedReader with labels\r\n     * @param isQuantized   Boolean representing model is quantized or not\r\n     */\r\n    public TfliteEdgeyoloRunner (\r\n            final MappedByteBuffer tfliteModel\r\n    ) throws IOException {\r\n        try {\r\n            Interpreter.Options options = new Interpreter.Options();\r\n            CompatibilityList compatList = new CompatibilityList();\r\n\r\n            if(compatList.isDelegateSupportedOnThisDevice()){\r\n                GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();\r\n                GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);\r\n                options.addDelegate(gpuDelegate);\r\n\r\n            }\r\n            else {\r\n                options.setNumThreads();\r\n            }\r\n\r\n            this.tfliteModel = tfliteModel;\r\n            this.tfLite = new Interpreter(this.tfliteModel, options);\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        int numBytesPerChannel = 4; // Floating point\r\n        int[] inputShape = this.tfLite.getInputTensor(0).shape();\r\n        int[] outputShape = this.tfLite.getOutputTensor(0).shape();\r\n\r\n        int modelWidth = inputShape[1];\r\n        int modelHeight = inputShape[2];\r\n\r\n        this.INPUT_HEIGHT = modelHeight;\r\n        this.INPUT_WIDTH = modelWidth;\r\n        this.imgDataBuffer = ByteBuffer.allocateDirect(1 * this.INPUT_WIDTH * this.INPUT_HEIGHT * 3 * numBytesPerChannel);\r\n        this.imgDataBuffer.order(ByteOrder.nativeOrder());\r\n        this.intValues = new int[this.INPUT_WIDTH * this.INPUT_HEIGHT];\r\n        this.outputBoxes = outputShape[outputShape.length - 2];\r\n        this.numClass = outputShape[outputShape.length - 1] - 5;\r\n        this.outDataBuffer = ByteBuffer.allocateDirect(this.outputBoxes * (this.numClass + 5) * numBytesPerChannel);\r\n        this.outDataBuffer.order(ByteOrder.nativeOrder());\r\n    }\r\n\r\n    public void close() {\r\n        tfLite.close();\r\n        tfLite = null;\r\n        tfliteModel = null;\r\n    }\r\n    private void recreateInterpreter() {\r\n        if (tfLite != null) {\r\n            tfLite.close();\r\n            tfLite = new Interpreter(this.tfliteModel, tfliteOptions);\r\n        }\r\n    }\r\n\r\n    /**\r\n     * Writes Image data into a {@code ByteBuffer}.\r\n     */\r\n    protected void convertBitmapToByteBuffer(Bitmap bitmap) {\r\n        int[] pixels = new int[bitmap.getWidth() * bitmap.getHeight()];\r\n        bitmap.getPixels(pixels, 0, transposedWidth, 0, 0, transposedWidth, transposedHeight);\r\n        imgDataBuffer.rewind();\r\n        for (int i = 0; i < pixels.length; i++) {\r\n            int pixelValue = pixels[i];\r\n            imgDataBuffer.putFloat((pixelValue & 0xFF));\r\n            imgDataBuffer.putFloat(((pixelValue >> 8) & 0xFF));\r\n            imgDataBuffer.putFloat(((pixelValue >> 16) & 0xFF));\r\n        }\r\n    }\r\n\r\n    public ArrayList<BoundingBox> recognizeImage(Bitmap bitmap) {\r\n        int width = bitmap.getWidth();\r\n        int height = bitmap.getHeight();\r\n        \r\n        convertBitmapToByteBuffer(bitmap);\r\n        // float[][][] outputData = new float[1][this.outputBoxes][this.numClass + 5];\r\n        // Different output for GPU tests\r\n        float[][][][] outputData = new float[1][28][28][120];\r\n        tfLite.run(imgDataBuffer, outputData);\r\n\r\n        // No nms etc for debugging\r\n        ArrayList<BoundingBox> detectedElements = new ArrayList<BoundingBox>();\r\n        return detectedElements;\r\n    }\r\n}\r\n```"", 'created_at': datetime.datetime(2024, 7, 4, 9, 46, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2208565703, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': ""PS: I'm using Android 13"", 'created_at': datetime.datetime(2024, 7, 4, 9, 48, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2219886242, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @sawantkumar,\r\n\r\nAny updates on this issue?\r\n\r\nThanks so much for you reply!', 'created_at': datetime.datetime(2024, 7, 10, 8, 30, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231203604, 'issue_id': 2382348778, 'author': 'sawantkumar', 'body': 'Hi @ramonhollands ,\r\n\r\nI have been running into some problems while replicating your issue. Meanwhile can you please use the tflite benchmark tool to benchmark the model and send me the logs .  You can find the link [here](https://www.tensorflow.org/lite/performance/measurement).', 'created_at': datetime.datetime(2024, 7, 16, 15, 17, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2231550150, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @sawantkumar \r\n\r\nThanks for your message!\r\n\r\nI did run these commands:\r\n```adb push 960_7_7_float32-crashes.tflite /data/local/tmp\r\nadb shell am start -S  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  --es args \'""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite  --num_threads=1 --use_gpu=true --num_runs=100""\'\r\nadb logcat | grep ""tflite""\r\n```\r\n\r\nResulting in this log:\r\n```\r\n07-16 20:27:49.171 27343 27343 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite  --num_threads=1 --use_gpu=true --num_runs=100\r\n07-16 20:27:49.179 27343 27343 I tflite  : Log parameter values verbosely: [0]\r\n07-16 20:27:49.179 27343 27343 I tflite  : Min num runs: [100]\r\n07-16 20:27:49.179 27343 27343 I tflite  : Num threads: [1]\r\n07-16 20:27:49.179 27343 27343 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n07-16 20:27:49.179 27343 27343 I tflite  : Signature to run: []\r\n07-16 20:27:49.180 27343 27343 I tflite  : #threads used for CPU inference: [1]\r\n07-16 20:27:49.180 27343 27343 I tflite  : Use gpu: [1]\r\n07-16 20:27:49.180 27343 27343 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n07-16 20:27:49.183 27343 27343 I tflite  : Initialized TensorFlow Lite runtime.\r\n07-16 20:27:49.183 27343 27343 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n07-16 20:27:49.183 27343 27343 I tflite  : GPU delegate created.\r\n07-16 20:27:49.187 27343 27343 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.\r\n07-16 20:27:49.189 27343 27343 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\n07-16 20:27:49.250 27343 27343 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.\r\n07-16 20:27:49.253 27343 27343 E tflite  : Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR\r\n07-16 20:27:49.253 27343 27343 E tflite  : Falling back to OpenGL\r\n07-16 20:27:49.273 27343 27343 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.\r\n07-16 20:27:49.273 27343 27343 I tflite  : Created 0 GPU delegate kernels.\r\n07-16 20:27:49.273 27343 27343 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n07-16 20:27:49.273 27343 27343 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.\r\n07-16 20:27:49.273 27343 27343 E tflite  : Restored original execution plan after delegate application failure.\r\n07-16 20:27:49.274 27343 27343 E tflite  : Failed to apply GPU delegate.```', 'created_at': datetime.datetime(2024, 7, 16, 18, 29, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232547276, 'issue_id': 2382348778, 'author': 'sawantkumar', 'body': 'Hi @ramonhollands ,\r\n\r\nI ran both the models using the tflite benchmark profiler and i was able to run both the models. Please check the logs below for the model which crashed on your device. From what i see, it looks like device specific issue. Can you try it on other devices and see if it works, because it works fine on my mediatek phone.\r\n\r\n`07-17 12:07:28.122 22646 22646 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=1 --use_gpu=true --num_runs=100\r\n07-17 12:07:28.132 22646 22646 I tflite: Log parameter values verbosely: [0]\r\n07-17 12:07:28.132 22646 22646 I tflite: Min num runs: [100]\r\n07-17 12:07:28.133 22646 22646 I tflite: Num threads: [1]\r\n07-17 12:07:28.133 22646 22646 I tflite: Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n07-17 12:07:28.133 22646 22646 I tflite: Signature to run: []\r\n07-17 12:07:28.134 22646 22646 I tflite: #threads used for CPU inference: [1]\r\n07-17 12:07:28.134 22646 22646 I tflite: Use gpu: [1]\r\n07-17 12:07:28.136 22646 22646 I tflite: Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n07-17 12:07:28.137 22646 22646 I tflite: Initialized TensorFlow Lite runtime.\r\n07-17 12:07:28.139 22646 22646 I tflite: Created TensorFlow Lite delegate for GPU.\r\n07-17 12:07:28.140 22646 22646 I tflite: GPU delegate created.\r\n07-17 12:07:28.145 22646 22646 I tflite: Loaded OpenCL library with dlopen.\r\n07-17 12:07:28.148 22646 22646 I tflite: Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partition for the whole graph.\r\n07-17 12:07:28.224 22646 22646 I tflite: Loaded OpenCL library with dlopen.\r\n07-17 12:07:34.038 22646 22646 I tflite: Initialized OpenCL-based API.\r\n07-17 12:07:34.216 22646 22646 I tflite: Created 1 GPU delegate kernels.\r\n07-17 12:07:34.218 22646 22646 I tflite: Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.\r\n07-17 12:07:34.218 22646 22646 I tflite: The input model file size (MB): 27.109\r\n07-17 12:07:34.218 22646 22646 I tflite: Initialized session in 6083.21ms.\r\n07-17 12:07:34.224 22646 22646 I tflite: Running benchmark for at least 1 iteration and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n07-17 12:07:34.760 22646 22646 I tflite: count=10 first=90579 curr=46180 min=40198 max=90579 avg=53008.9 std=14561\r\n07-17 12:07:34.761 22646 22646 I tflite: Running benchmark for at least 100 iterations and at least 1 second but terminate if exceeding 150 seconds.\r\n07-17 12:07:39.370 22646 22646 I tflite: count=100 first=55128 curr=45268 min=28197 max=61757 avg=45770.2 std=6209\r\n07-17 12:07:39.372 22646 22646 I tflite: Inference timings in us: Init: 6083206, First inference: 90579, Warmup (avg): 53008.9, Inference (avg): 45770.2\r\n07-17 12:07:39.373 22646 22646 I tflite: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n07-17 12:07:39.373 22646 22646 I tflite: Memory footprint delta from the start of the tool (MB): init=177.977 overall=177.977\r\n\r\n`', 'created_at': datetime.datetime(2024, 7, 17, 6, 43, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2232636280, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': ""Hi @sawantkumar,\r\n\r\nI tried it on the Samsung Galaxy S10e as well. Same error. \r\n\r\nIt might have something to do with the openGl tflite implementation? I see 'your' phone is using openCL. Can you also force your phone to fallback on openGl to be able to debug it further?\r\n```\r\n07-17 09:30:22.367 28155 28155 E tflite  : Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR\r\n07-17 09:30:22.367 28155 28155 E tflite  : Falling back to OpenGL\r\n```\r\n\r\nWhat's the reason the OpenCL library cannot be opened?\r\n\r\nThe problem is that the Android app is crashing when using GPU delegate. Is there some way to prevent the crash and fallback on CPU in this case?\r\n\r\nNext to that it would be helpfull to find out why this error message appears:\r\n```\r\n07-17 09:30:25.016 28155 28155 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.\r\n```\r\n\r\nLooking forward to your reply, thanks in advance!"", 'created_at': datetime.datetime(2024, 7, 17, 7, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2238677338, 'issue_id': 2382348778, 'author': 'sawantkumar', 'body': 'Hi @ramonhollands ,\r\n\r\nYou were right, the model fails to execute when using the ""opengl"" backend. Below logs show that model is running on opencl but not on opengl. @pkgoogle , can you please take a look ?\r\n\r\nOPENCL BACKEND\r\n```\r\n\r\n07-19 14:10:10.844  7937  7937 I tflite  : Log parameter values verbosely: [0]\r\n07-19 14:10:10.844  7937  7937 I tflite  : Min num runs: [100]\r\n07-19 14:10:10.844  7937  7937 I tflite  : Num threads: [1]\r\n07-19 14:10:10.844  7937  7937 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n07-19 14:10:10.844  7937  7937 I tflite  : Signature to run: []\r\n07-19 14:10:10.845  7937  7937 I tflite  : #threads used for CPU inference: [1]\r\n07-19 14:10:10.845  7937  7937 I tflite  : Use gpu: [1]\r\n07-19 14:10:10.845  7937  7937 I tflite  : GPU backend: [cl]\r\n07-19 14:10:10.847  7937  7937 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n07-19 14:10:10.848  7937  7937 I tflite  : Initialized TensorFlow Lite runtime.\r\n07-19 14:10:10.850  7937  7937 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n07-19 14:10:10.850  7937  7937 I tflite  : GPU delegate created.\r\n07-19 14:10:10.858  7937  7937 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\n07-19 14:10:23.306  7937  7937 I tflite  : Initialized OpenCL-based API.\r\n07-19 14:10:23.627  7937  7937 I tflite  : Created 1 GPU delegate kernels.\r\n07-19 14:10:23.630  7937  7937 I tflite  : Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.\r\n07-19 14:10:23.630  7937  7937 I tflite  : The input model file size (MB): 27.109\r\n07-19 14:10:23.630  7937  7937 I tflite  : Initialized session in 12784.8ms.\r\n07-19 14:10:23.643  7937  7937 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n07-19 14:10:24.159  7937  7937 I tflite  : count=17 first=52840 curr=22225 min=22225 max=52840 avg=29988.8 std=6356\r\n07-19 14:10:24.159  7937  7937 I tflite  : Running benchmark for at least 100 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n07-19 14:10:27.110  7937  7937 I tflite  : count=100 first=25048 curr=29957 min=21306 max=40857 avg=29247.1 std=2662\r\n07-19 14:10:27.111  7937  7937 I tflite  : Inference timings in us: Init: 12784815, First inference: 52840, Warmup (avg): 29988.8, Inference (avg): 29247.1\r\n07-19 14:10:27.111  7937  7937 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n07-19 14:10:27.111  7937  7937 I tflite  : Memory footprint delta from the start of the tool (MB): init=180.105 overall=180.105\r\n07-19 14:10:42.939  8054  8054 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=1 --use_gpu=true --gpu_backend=gl --num_runs=100\r\n```\r\n\r\n\r\nOPENGL BACKEND\r\n```\r\n07-19 14:10:42.948  8054  8054 I tflite  : Log parameter values verbosely: [0]\r\n07-19 14:10:42.948  8054  8054 I tflite  : Min num runs: [100]\r\n07-19 14:10:42.948  8054  8054 I tflite  : Num threads: [1]\r\n07-19 14:10:42.948  8054  8054 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n07-19 14:10:42.948  8054  8054 I tflite  : Signature to run: []\r\n07-19 14:10:42.949  8054  8054 I tflite  : #threads used for CPU inference: [1]\r\n07-19 14:10:42.949  8054  8054 I tflite  : Use gpu: [1]\r\n07-19 14:10:42.949  8054  8054 I tflite  : GPU backend: [gl]\r\n07-19 14:10:42.951  8054  8054 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n07-19 14:10:42.952  8054  8054 I tflite  : Initialized TensorFlow Lite runtime.\r\n07-19 14:10:42.954  8054  8054 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n07-19 14:10:42.954  8054  8054 I tflite  : GPU delegate created.\r\n07-19 14:10:42.964  8054  8054 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\n07-19 14:10:43.188  8054  8054 I tflite  : Initialized OpenGL-based API.\r\n07-19 14:10:44.384  8054  8054 E tflite  : TfLiteGpuDelegate Init: No shader implementation for split\r\n07-19 14:10:44.420  8054  8054 I tflite  : Created 0 GPU delegate kernels.\r\n07-19 14:10:44.420  8054  8054 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n07-19 14:10:44.420  8054  8054 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.\r\n07-19 14:10:44.424  8054  8054 E tflite  : Restored original execution plan after delegate application failure.\r\n07-19 14:10:44.426  8054  8054 E tflite  : Failed to apply GPU delegate.\r\n```', 'created_at': datetime.datetime(2024, 7, 19, 8, 43, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263104395, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @pkgoogle \nAny updates on this issue? Thanks in advance for your reply!\nBest regards,\nRamon', 'created_at': datetime.datetime(2024, 8, 1, 13, 46, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2265927199, 'issue_id': 2382348778, 'author': 'pkgoogle', 'body': 'I\'m running into different issues for both cases, but I am using an emulator (Pixel 7 Pro API 34).\r\n\r\nMy log:\r\nwith OpenGL:\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \\\r\n--es args \'""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite \\\r\n--num_threads=4 \\\r\n--use_gpu=true \\\r\n--gpu_backend=gl""\'\r\n\r\n08-02 11:20:15.279  6390  6390 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=4 --use_gpu=true --gpu_backend=gl\r\n08-02 11:20:15.303  6390  6390 I tflite  : Log parameter values verbosely: [0]\r\n08-02 11:20:15.303  6390  6390 I tflite  : Num threads: [4]\r\n08-02 11:20:15.303  6390  6390 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n08-02 11:20:15.303  6390  6390 I tflite  : Signature to run: []\r\n08-02 11:20:15.303  6390  6390 I tflite  : #threads used for CPU inference: [4]\r\n08-02 11:20:15.303  6390  6390 I tflite  : Use gpu: [1]\r\n08-02 11:20:15.303  6390  6390 I tflite  : GPU backend: [gl]\r\n08-02 11:20:15.367  6390  6390 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n08-02 11:20:15.380  6390  6390 I tflite  : Initialized TensorFlow Lite runtime.\r\n08-02 11:20:15.381  6390  6390 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n08-02 11:20:15.381  6390  6390 I tflite  : GPU delegate created.\r\n08-02 11:20:15.382  6390  6390 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.\r\n08-02 11:20:15.384  6390  6390 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\n08-02 11:20:15.501  6390  6390 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.\r\n08-02 11:20:15.501  6390  6390 I tflite  : Created 0 GPU delegate kernels.\r\n08-02 11:20:15.501  6390  6390 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n08-02 11:20:15.501  6390  6390 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.\r\n08-02 11:20:15.502  6390  6390 E tflite  : Restored original execution plan after delegate application failure.\r\n08-02 11:20:15.503  6390  6390 E tflite  : Failed to apply GPU delegate.\r\n```\r\nWith OpenCL:\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \\\r\n--es args \'""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite \\\r\n--num_threads=4 \\\r\n--use_gpu=true \\\r\n--gpu_backend=cl""\'\r\n\r\n08-02 11:22:11.631  6437  6437 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=4 --use_gpu=true --gpu_backend=cl\r\n08-02 11:22:11.643  6437  6437 I tflite  : Log parameter values verbosely: [0]\r\n08-02 11:22:11.643  6437  6437 I tflite  : Num threads: [4]\r\n08-02 11:22:11.643  6437  6437 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]\r\n08-02 11:22:11.644  6437  6437 I tflite  : Signature to run: []\r\n08-02 11:22:11.644  6437  6437 I tflite  : #threads used for CPU inference: [4]\r\n08-02 11:22:11.645  6437  6437 I tflite  : Use gpu: [1]\r\n08-02 11:22:11.645  6437  6437 I tflite  : GPU backend: [cl]\r\n08-02 11:22:11.649  6437  6437 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite\r\n08-02 11:22:11.651  6437  6437 I tflite  : Initialized TensorFlow Lite runtime.\r\n08-02 11:22:11.654  6437  6437 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n08-02 11:22:11.654  6437  6437 I tflite  : GPU delegate created.\r\n08-02 11:22:11.655  6437  6437 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.\r\n08-02 11:22:11.656  6437  6437 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.\r\n08-02 11:22:11.712  6437  6437 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.\r\n08-02 11:22:11.715  6437  6437 E tflite  : TfLiteGpuDelegate Init: Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR\r\n08-02 11:22:11.715  6437  6437 I tflite  : Created 0 GPU delegate kernels.\r\n08-02 11:22:11.715  6437  6437 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n08-02 11:22:11.715  6437  6437 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.\r\n08-02 11:22:11.715  6437  6437 E tflite  : Restored original execution plan after delegate application failure.\r\n08-02 11:22:11.715  6437  6437 E tflite  : Failed to apply GPU delegate.\r\n```\r\n\r\nHi @arfaian, can you please take a look? Thanks.', 'created_at': datetime.datetime(2024, 8, 2, 18, 28, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276660186, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @arfaian \nAny updates on this issue? Thanks in advance for your reply!\nBest regards,\nRamon', 'created_at': datetime.datetime(2024, 8, 8, 21, 12, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304975907, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @arfaian \n\nDo you have an idea when this issue will be looked at? Thanks in advance for your reply!', 'created_at': datetime.datetime(2024, 8, 22, 15, 19, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340224400, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Hi @arfaian, @pkgoogle ,\r\n\r\nAny news on this one?\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 9, 10, 10, 3, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382878966, 'issue_id': 2382348778, 'author': 'ramonhollands', 'body': 'Any updates @arfaian, @pkgoogle ?\r\n\r\nThanks in advance!', 'created_at': datetime.datetime(2024, 9, 30, 11, 6, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412827310, 'issue_id': 2382348778, 'author': 'microstudent', 'body': 'Any updates? \r\n\r\nI also encountered this crash when I used my OnePlus 6T to load the YOLO11 model with TFLites GpuDelegate enabled. but Xiaomi 14 Pro works fine.', 'created_at': datetime.datetime(2024, 10, 15, 3, 35, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442127970, 'issue_id': 2382348778, 'author': 'pavlosharhan2', 'body': '+1\r\nHaving the same error with Samsung S22, trying to run pose_estimation from tensorflow/examples, which results in a crash once i switch the delegate to GPU', 'created_at': datetime.datetime(2024, 10, 28, 16, 54, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502681477, 'issue_id': 2382348778, 'author': 'gaikwadrahul8', 'body': ""Hi, @ramonhollands\r\nThanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/56\r\n\r\nLet us know if you have any questions. Thanks."", 'created_at': datetime.datetime(2024, 11, 27, 4, 8, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504512801, 'issue_id': 2382348778, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70664"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70664"">No</a>', 'created_at': datetime.datetime(2024, 11, 27, 18, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561052832, 'issue_id': 2382348778, 'author': 'prilaga', 'body': 'Are there any updates? When will the bug be fixed?', 'created_at': datetime.datetime(2024, 12, 24, 12, 7, 49, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-01 09:15:32 UTC): @ramonhollands Please ensure that the GPU delegate is configured correctly in your Android application and you are using TF latest version as well?
Thank you!

ramonhollands (Issue Creator) on (2024-07-01 09:45:20 UTC): Thanks for your reply.

I'm using the latest tensorflow version ([2.16.1](https://mvnrepository.com/artifact/org.tensorflow/tensorflow-lite/2.16.1))

The GPU delegate looks correctly configured because it works on one model and I'm using the exact same configuration for the other model, right?

ramonhollands (Issue Creator) on (2024-07-01 14:04:52 UTC): @sushreebarsa, just to be sure, I am tagging your name. Thanks in advance for your response.

sawantkumar on (2024-07-04 09:23:11 UTC): Hi @ramonhollands ,

I tried running your model on the dimensity 9000 gpu, and both the models ran successfully using the GPU Delegate, can you tell me which device are you running your model. If possible can you also give me the inference script ?

ramonhollands (Issue Creator) on (2024-07-04 09:46:40 UTC): Hi @sawantkumar,

Thanks for checking out! 
I'm running on a Galaxy A52 device.

Attached, the code I'm using (striped to the essentials).

Regards, Ramon

```package com.example.debug_gpu.detection;

import android.graphics.Bitmap;
import android.graphics.RectF;
import android.os.Build;

import java.io.BufferedReader;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.MappedByteBuffer;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Map;
import java.util.PriorityQueue;
import java.util.Vector;

import android.util.Log;

import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.Tensor;
import org.tensorflow.lite.gpu.CompatibilityList;
import org.tensorflow.lite.gpu.GpuDelegate;

public class TfliteEdgeyoloRunner extends AbstractBoundingBoxRunner {
    public float getObjThresh() {
        return 0.5f;
    }
    private MappedByteBuffer tfliteModel;
    private final Interpreter.Options tfliteOptions = new Interpreter.Options();
    private Vector<String> labels = new Vector<String>();
    private int[] intValues;
    private int outputBoxes;

    private ByteBuffer imgDataBuffer;
    private ByteBuffer outDataBuffer;

    private Interpreter tfLite;
    private float inp_scale;
    private int inp_zero_point;
    private float oup_scale;
    private int oup_zero_point;
    private int numClass;

    private int preInf;
    private int postInf;
    private int inf;

    public int getPreInf() {
        return preInf;
    }

    public int getPostInf() {
        return postInf;
    }

    public int getInf() {
        return inf;
    }

    /**
     * Initializes the detector
     *
     * @param tfliteModel MappedByteBuffer TfLite model
     * @param labels_br BufferedReader with labels
     * @param isQuantized   Boolean representing model is quantized or not
     */
    public TfliteEdgeyoloRunner (
            final MappedByteBuffer tfliteModel
    ) throws IOException {
        try {
            Interpreter.Options options = new Interpreter.Options();
            CompatibilityList compatList = new CompatibilityList();

            if(compatList.isDelegateSupportedOnThisDevice()){
                GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
                GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
                options.addDelegate(gpuDelegate);

            }
            else {
                options.setNumThreads();
            }

            this.tfliteModel = tfliteModel;
            this.tfLite = new Interpreter(this.tfliteModel, options);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
        int numBytesPerChannel = 4; // Floating point
        int[] inputShape = this.tfLite.getInputTensor(0).shape();
        int[] outputShape = this.tfLite.getOutputTensor(0).shape();

        int modelWidth = inputShape[1];
        int modelHeight = inputShape[2];

        this.INPUT_HEIGHT = modelHeight;
        this.INPUT_WIDTH = modelWidth;
        this.imgDataBuffer = ByteBuffer.allocateDirect(1 * this.INPUT_WIDTH * this.INPUT_HEIGHT * 3 * numBytesPerChannel);
        this.imgDataBuffer.order(ByteOrder.nativeOrder());
        this.intValues = new int[this.INPUT_WIDTH * this.INPUT_HEIGHT];
        this.outputBoxes = outputShape[outputShape.length - 2];
        this.numClass = outputShape[outputShape.length - 1] - 5;
        this.outDataBuffer = ByteBuffer.allocateDirect(this.outputBoxes * (this.numClass + 5) * numBytesPerChannel);
        this.outDataBuffer.order(ByteOrder.nativeOrder());
    }

    public void close() {
        tfLite.close();
        tfLite = null;
        tfliteModel = null;
    }
    private void recreateInterpreter() {
        if (tfLite != null) {
            tfLite.close();
            tfLite = new Interpreter(this.tfliteModel, tfliteOptions);
        }
    }

    /**
     * Writes Image data into a {@code ByteBuffer}.
     */
    protected void convertBitmapToByteBuffer(Bitmap bitmap) {
        int[] pixels = new int[bitmap.getWidth() * bitmap.getHeight()];
        bitmap.getPixels(pixels, 0, transposedWidth, 0, 0, transposedWidth, transposedHeight);
        imgDataBuffer.rewind();
        for (int i = 0; i < pixels.length; i++) {
            int pixelValue = pixels[i];
            imgDataBuffer.putFloat((pixelValue & 0xFF));
            imgDataBuffer.putFloat(((pixelValue >> 8) & 0xFF));
            imgDataBuffer.putFloat(((pixelValue >> 16) & 0xFF));
        }
    }

    public ArrayList<BoundingBox> recognizeImage(Bitmap bitmap) {
        int width = bitmap.getWidth();
        int height = bitmap.getHeight();
        
        convertBitmapToByteBuffer(bitmap);
        // float[][][] outputData = new float[1][this.outputBoxes][this.numClass + 5];
        // Different output for GPU tests
        float[][][][] outputData = new float[1][28][28][120];
        tfLite.run(imgDataBuffer, outputData);

        // No nms etc for debugging
        ArrayList<BoundingBox> detectedElements = new ArrayList<BoundingBox>();
        return detectedElements;
    }
}
```

ramonhollands (Issue Creator) on (2024-07-04 09:48:05 UTC): PS: I'm using Android 13

ramonhollands (Issue Creator) on (2024-07-10 08:30:44 UTC): Hi @sawantkumar,

Any updates on this issue?

Thanks so much for you reply!

sawantkumar on (2024-07-16 15:17:49 UTC): Hi @ramonhollands ,

I have been running into some problems while replicating your issue. Meanwhile can you please use the tflite benchmark tool to benchmark the model and send me the logs .  You can find the link [here](https://www.tensorflow.org/lite/performance/measurement).

ramonhollands (Issue Creator) on (2024-07-16 18:29:43 UTC): Hi @sawantkumar 

Thanks for your message!

I did run these commands:
```adb push 960_7_7_float32-crashes.tflite /data/local/tmp
adb shell am start -S  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  --es args '""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite  --num_threads=1 --use_gpu=true --num_runs=100""'
adb logcat | grep ""tflite""
```

Resulting in this log:
```
07-16 20:27:49.171 27343 27343 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite  --num_threads=1 --use_gpu=true --num_runs=100
07-16 20:27:49.179 27343 27343 I tflite  : Log parameter values verbosely: [0]
07-16 20:27:49.179 27343 27343 I tflite  : Min num runs: [100]
07-16 20:27:49.179 27343 27343 I tflite  : Num threads: [1]
07-16 20:27:49.179 27343 27343 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
07-16 20:27:49.179 27343 27343 I tflite  : Signature to run: []
07-16 20:27:49.180 27343 27343 I tflite  : #threads used for CPU inference: [1]
07-16 20:27:49.180 27343 27343 I tflite  : Use gpu: [1]
07-16 20:27:49.180 27343 27343 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
07-16 20:27:49.183 27343 27343 I tflite  : Initialized TensorFlow Lite runtime.
07-16 20:27:49.183 27343 27343 I tflite  : Created TensorFlow Lite delegate for GPU.
07-16 20:27:49.183 27343 27343 I tflite  : GPU delegate created.
07-16 20:27:49.187 27343 27343 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.
07-16 20:27:49.189 27343 27343 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
07-16 20:27:49.250 27343 27343 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.
07-16 20:27:49.253 27343 27343 E tflite  : Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR
07-16 20:27:49.253 27343 27343 E tflite  : Falling back to OpenGL
07-16 20:27:49.273 27343 27343 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.
07-16 20:27:49.273 27343 27343 I tflite  : Created 0 GPU delegate kernels.
07-16 20:27:49.273 27343 27343 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
07-16 20:27:49.273 27343 27343 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.
07-16 20:27:49.273 27343 27343 E tflite  : Restored original execution plan after delegate application failure.
07-16 20:27:49.274 27343 27343 E tflite  : Failed to apply GPU delegate.```

sawantkumar on (2024-07-17 06:43:07 UTC): Hi @ramonhollands ,

I ran both the models using the tflite benchmark profiler and i was able to run both the models. Please check the logs below for the model which crashed on your device. From what i see, it looks like device specific issue. Can you try it on other devices and see if it works, because it works fine on my mediatek phone.

`07-17 12:07:28.122 22646 22646 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=1 --use_gpu=true --num_runs=100
07-17 12:07:28.132 22646 22646 I tflite: Log parameter values verbosely: [0]
07-17 12:07:28.132 22646 22646 I tflite: Min num runs: [100]
07-17 12:07:28.133 22646 22646 I tflite: Num threads: [1]
07-17 12:07:28.133 22646 22646 I tflite: Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
07-17 12:07:28.133 22646 22646 I tflite: Signature to run: []
07-17 12:07:28.134 22646 22646 I tflite: #threads used for CPU inference: [1]
07-17 12:07:28.134 22646 22646 I tflite: Use gpu: [1]
07-17 12:07:28.136 22646 22646 I tflite: Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
07-17 12:07:28.137 22646 22646 I tflite: Initialized TensorFlow Lite runtime.
07-17 12:07:28.139 22646 22646 I tflite: Created TensorFlow Lite delegate for GPU.
07-17 12:07:28.140 22646 22646 I tflite: GPU delegate created.
07-17 12:07:28.145 22646 22646 I tflite: Loaded OpenCL library with dlopen.
07-17 12:07:28.148 22646 22646 I tflite: Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partition for the whole graph.
07-17 12:07:28.224 22646 22646 I tflite: Loaded OpenCL library with dlopen.
07-17 12:07:34.038 22646 22646 I tflite: Initialized OpenCL-based API.
07-17 12:07:34.216 22646 22646 I tflite: Created 1 GPU delegate kernels.
07-17 12:07:34.218 22646 22646 I tflite: Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
07-17 12:07:34.218 22646 22646 I tflite: The input model file size (MB): 27.109
07-17 12:07:34.218 22646 22646 I tflite: Initialized session in 6083.21ms.
07-17 12:07:34.224 22646 22646 I tflite: Running benchmark for at least 1 iteration and at least 0.5 seconds but terminate if exceeding 150 seconds.
07-17 12:07:34.760 22646 22646 I tflite: count=10 first=90579 curr=46180 min=40198 max=90579 avg=53008.9 std=14561
07-17 12:07:34.761 22646 22646 I tflite: Running benchmark for at least 100 iterations and at least 1 second but terminate if exceeding 150 seconds.
07-17 12:07:39.370 22646 22646 I tflite: count=100 first=55128 curr=45268 min=28197 max=61757 avg=45770.2 std=6209
07-17 12:07:39.372 22646 22646 I tflite: Inference timings in us: Init: 6083206, First inference: 90579, Warmup (avg): 53008.9, Inference (avg): 45770.2
07-17 12:07:39.373 22646 22646 I tflite: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
07-17 12:07:39.373 22646 22646 I tflite: Memory footprint delta from the start of the tool (MB): init=177.977 overall=177.977

`

ramonhollands (Issue Creator) on (2024-07-17 07:37:55 UTC): Hi @sawantkumar,

I tried it on the Samsung Galaxy S10e as well. Same error. 

It might have something to do with the openGl tflite implementation? I see 'your' phone is using openCL. Can you also force your phone to fallback on openGl to be able to debug it further?
```
07-17 09:30:22.367 28155 28155 E tflite  : Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR
07-17 09:30:22.367 28155 28155 E tflite  : Falling back to OpenGL
```

What's the reason the OpenCL library cannot be opened?

The problem is that the Android app is crashing when using GPU delegate. Is there some way to prevent the crash and fallback on CPU in this case?

Next to that it would be helpfull to find out why this error message appears:
```
07-17 09:30:25.016 28155 28155 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.
```

Looking forward to your reply, thanks in advance!

sawantkumar on (2024-07-19 08:43:41 UTC): Hi @ramonhollands ,

You were right, the model fails to execute when using the ""opengl"" backend. Below logs show that model is running on opencl but not on opengl. @pkgoogle , can you please take a look ?

OPENCL BACKEND
```

07-19 14:10:10.844  7937  7937 I tflite  : Log parameter values verbosely: [0]
07-19 14:10:10.844  7937  7937 I tflite  : Min num runs: [100]
07-19 14:10:10.844  7937  7937 I tflite  : Num threads: [1]
07-19 14:10:10.844  7937  7937 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
07-19 14:10:10.844  7937  7937 I tflite  : Signature to run: []
07-19 14:10:10.845  7937  7937 I tflite  : #threads used for CPU inference: [1]
07-19 14:10:10.845  7937  7937 I tflite  : Use gpu: [1]
07-19 14:10:10.845  7937  7937 I tflite  : GPU backend: [cl]
07-19 14:10:10.847  7937  7937 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
07-19 14:10:10.848  7937  7937 I tflite  : Initialized TensorFlow Lite runtime.
07-19 14:10:10.850  7937  7937 I tflite  : Created TensorFlow Lite delegate for GPU.
07-19 14:10:10.850  7937  7937 I tflite  : GPU delegate created.
07-19 14:10:10.858  7937  7937 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
07-19 14:10:23.306  7937  7937 I tflite  : Initialized OpenCL-based API.
07-19 14:10:23.627  7937  7937 I tflite  : Created 1 GPU delegate kernels.
07-19 14:10:23.630  7937  7937 I tflite  : Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
07-19 14:10:23.630  7937  7937 I tflite  : The input model file size (MB): 27.109
07-19 14:10:23.630  7937  7937 I tflite  : Initialized session in 12784.8ms.
07-19 14:10:23.643  7937  7937 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
07-19 14:10:24.159  7937  7937 I tflite  : count=17 first=52840 curr=22225 min=22225 max=52840 avg=29988.8 std=6356
07-19 14:10:24.159  7937  7937 I tflite  : Running benchmark for at least 100 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
07-19 14:10:27.110  7937  7937 I tflite  : count=100 first=25048 curr=29957 min=21306 max=40857 avg=29247.1 std=2662
07-19 14:10:27.111  7937  7937 I tflite  : Inference timings in us: Init: 12784815, First inference: 52840, Warmup (avg): 29988.8, Inference (avg): 29247.1
07-19 14:10:27.111  7937  7937 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
07-19 14:10:27.111  7937  7937 I tflite  : Memory footprint delta from the start of the tool (MB): init=180.105 overall=180.105
07-19 14:10:42.939  8054  8054 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=1 --use_gpu=true --gpu_backend=gl --num_runs=100
```


OPENGL BACKEND
```
07-19 14:10:42.948  8054  8054 I tflite  : Log parameter values verbosely: [0]
07-19 14:10:42.948  8054  8054 I tflite  : Min num runs: [100]
07-19 14:10:42.948  8054  8054 I tflite  : Num threads: [1]
07-19 14:10:42.948  8054  8054 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
07-19 14:10:42.948  8054  8054 I tflite  : Signature to run: []
07-19 14:10:42.949  8054  8054 I tflite  : #threads used for CPU inference: [1]
07-19 14:10:42.949  8054  8054 I tflite  : Use gpu: [1]
07-19 14:10:42.949  8054  8054 I tflite  : GPU backend: [gl]
07-19 14:10:42.951  8054  8054 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
07-19 14:10:42.952  8054  8054 I tflite  : Initialized TensorFlow Lite runtime.
07-19 14:10:42.954  8054  8054 I tflite  : Created TensorFlow Lite delegate for GPU.
07-19 14:10:42.954  8054  8054 I tflite  : GPU delegate created.
07-19 14:10:42.964  8054  8054 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
07-19 14:10:43.188  8054  8054 I tflite  : Initialized OpenGL-based API.
07-19 14:10:44.384  8054  8054 E tflite  : TfLiteGpuDelegate Init: No shader implementation for split
07-19 14:10:44.420  8054  8054 I tflite  : Created 0 GPU delegate kernels.
07-19 14:10:44.420  8054  8054 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
07-19 14:10:44.420  8054  8054 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.
07-19 14:10:44.424  8054  8054 E tflite  : Restored original execution plan after delegate application failure.
07-19 14:10:44.426  8054  8054 E tflite  : Failed to apply GPU delegate.
```

ramonhollands (Issue Creator) on (2024-08-01 13:46:55 UTC): Hi @pkgoogle 
Any updates on this issue? Thanks in advance for your reply!
Best regards,
Ramon

pkgoogle (Assginee) on (2024-08-02 18:28:37 UTC): I'm running into different issues for both cases, but I am using an emulator (Pixel 7 Pro API 34).

My log:
with OpenGL:
```
adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
--es args '""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite \
--num_threads=4 \
--use_gpu=true \
--gpu_backend=gl""'

08-02 11:20:15.279  6390  6390 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=4 --use_gpu=true --gpu_backend=gl
08-02 11:20:15.303  6390  6390 I tflite  : Log parameter values verbosely: [0]
08-02 11:20:15.303  6390  6390 I tflite  : Num threads: [4]
08-02 11:20:15.303  6390  6390 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
08-02 11:20:15.303  6390  6390 I tflite  : Signature to run: []
08-02 11:20:15.303  6390  6390 I tflite  : #threads used for CPU inference: [4]
08-02 11:20:15.303  6390  6390 I tflite  : Use gpu: [1]
08-02 11:20:15.303  6390  6390 I tflite  : GPU backend: [gl]
08-02 11:20:15.367  6390  6390 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
08-02 11:20:15.380  6390  6390 I tflite  : Initialized TensorFlow Lite runtime.
08-02 11:20:15.381  6390  6390 I tflite  : Created TensorFlow Lite delegate for GPU.
08-02 11:20:15.381  6390  6390 I tflite  : GPU delegate created.
08-02 11:20:15.382  6390  6390 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.
08-02 11:20:15.384  6390  6390 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
08-02 11:20:15.501  6390  6390 E tflite  : TfLiteGpuDelegate Init: [GL_INVALID_VALUE]: A numeric argument is out of range.
08-02 11:20:15.501  6390  6390 I tflite  : Created 0 GPU delegate kernels.
08-02 11:20:15.501  6390  6390 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
08-02 11:20:15.501  6390  6390 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.
08-02 11:20:15.502  6390  6390 E tflite  : Restored original execution plan after delegate application failure.
08-02 11:20:15.503  6390  6390 E tflite  : Failed to apply GPU delegate.
```
With OpenCL:
```
adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
--es args '""--graph=/data/local/tmp/960_7_7_float32-crashes.tflite \
--num_threads=4 \
--use_gpu=true \
--gpu_backend=cl""'

08-02 11:22:11.631  6437  6437 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/960_7_7_float32-crashes.tflite --num_threads=4 --use_gpu=true --gpu_backend=cl
08-02 11:22:11.643  6437  6437 I tflite  : Log parameter values verbosely: [0]
08-02 11:22:11.643  6437  6437 I tflite  : Num threads: [4]
08-02 11:22:11.643  6437  6437 I tflite  : Graph: [/data/local/tmp/960_7_7_float32-crashes.tflite]
08-02 11:22:11.644  6437  6437 I tflite  : Signature to run: []
08-02 11:22:11.644  6437  6437 I tflite  : #threads used for CPU inference: [4]
08-02 11:22:11.645  6437  6437 I tflite  : Use gpu: [1]
08-02 11:22:11.645  6437  6437 I tflite  : GPU backend: [cl]
08-02 11:22:11.649  6437  6437 I tflite  : Loaded model /data/local/tmp/960_7_7_float32-crashes.tflite
08-02 11:22:11.651  6437  6437 I tflite  : Initialized TensorFlow Lite runtime.
08-02 11:22:11.654  6437  6437 I tflite  : Created TensorFlow Lite delegate for GPU.
08-02 11:22:11.654  6437  6437 I tflite  : GPU delegate created.
08-02 11:22:11.655  6437  6437 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.
08-02 11:22:11.656  6437  6437 I tflite  : Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph.
08-02 11:22:11.712  6437  6437 I tflite  : Failed to load OpenCL library with dlopen: dlopen failed: library ""libvndksupport.so"" not found. Trying ICD loader.
08-02 11:22:11.715  6437  6437 E tflite  : TfLiteGpuDelegate Init: Can not open OpenCL library on this device - undefined symbol: clEnqueueReleaseEGLObjectsKHR
08-02 11:22:11.715  6437  6437 I tflite  : Created 0 GPU delegate kernels.
08-02 11:22:11.715  6437  6437 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
08-02 11:22:11.715  6437  6437 E tflite  : Node number 290 (TfLiteGpuDelegateV2) failed to prepare.
08-02 11:22:11.715  6437  6437 E tflite  : Restored original execution plan after delegate application failure.
08-02 11:22:11.715  6437  6437 E tflite  : Failed to apply GPU delegate.
```

Hi @arfaian, can you please take a look? Thanks.

ramonhollands (Issue Creator) on (2024-08-08 21:12:51 UTC): Hi @arfaian 
Any updates on this issue? Thanks in advance for your reply!
Best regards,
Ramon

ramonhollands (Issue Creator) on (2024-08-22 15:19:17 UTC): Hi @arfaian 

Do you have an idea when this issue will be looked at? Thanks in advance for your reply!

ramonhollands (Issue Creator) on (2024-09-10 10:03:46 UTC): Hi @arfaian, @pkgoogle ,

Any news on this one?

Thanks!

ramonhollands (Issue Creator) on (2024-09-30 11:06:11 UTC): Any updates @arfaian, @pkgoogle ?

Thanks in advance!

microstudent on (2024-10-15 03:35:51 UTC): Any updates? 

I also encountered this crash when I used my OnePlus 6T to load the YOLO11 model with TFLites GpuDelegate enabled. but Xiaomi 14 Pro works fine.

pavlosharhan2 on (2024-10-28 16:54:43 UTC): +1
Having the same error with Samsung S22, trying to run pose_estimation from tensorflow/examples, which results in a crash once i switch the delegate to GPU

gaikwadrahul8 on (2024-11-27 04:08:56 UTC): Hi, @ramonhollands
Thanks for raising this issue. Are you aware of the migration to [LiteRT](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/)? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/google-ai-edge/LiteRT/issues/56

Let us know if you have any questions. Thanks.

google-ml-butler[bot] on (2024-11-27 18:04:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70664"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70664"">No</a>

prilaga on (2024-12-24 12:07:49 UTC): Are there any updates? When will the bug be fixed?

"
2382244491,issue,closed,completed,TensorFlow Lite label_image fails to build with cmake,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0-rc0

### Custom code

No

### OS platform and distribution

Linux (Raspberry Pi OS)

### Mobile device

Raspberry Pi 4

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

12.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I get the error 
> tensorflow/lite/profiling/proto/profiling_info.pb.h: No such file or directory.


### Standalone code to reproduce the issue

```shell
$ git clone -b v2.17.0-rc0 https://github.com/tensorflow/tensorflow
$ mkdir tflite_build && tflite_build
$ cmake ../tensorflow/tensorflow/lite/
$ cmake --build . -j3
$ cmake --build . -t label_image
In file included from /home/pi/tensorflow/tensorflow/lite/profiling/profile_summarizer.h:28,
                 from /home/pi/tensorflow/tensorflow/lite/profiling/profile_summarizer.cc:16:
/home/pi/tensorflow/tensorflow/lite/profiling/profile_summary_formatter.h:31:10: fatal error: tensorflow/lite/profiling/proto/profiling_info.pb.h: No such file or directory
   31 | #include ""tensorflow/lite/profiling/proto/profiling_info.pb.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

```
```


### Relevant log output

_No response_",NobuoTsukamoto,2024-06-30 12:59:31+00:00,"['pkgoogle', 'sawantkumar']",2024-08-07 06:48:03+00:00,2024-08-07 06:48:00+00:00,https://github.com/tensorflow/tensorflow/issues/70659,"[('awaiting review', 'Pull request awaiting review'), ('type:bug', 'Bug'), ('comp:lite', 'TF Lite related issues'), ('2.17', 'Issues related to 2.17 release')]","[{'comment_id': 2198564086, 'issue_id': 2382244491, 'author': 'NobuoTsukamoto', 'body': ""The problem is that `tensorflow/lite/profiling/proto/profiling_info.pb.h` is generated under `CMAKE_BINARY_DIR`, but there is no include_directories setting.\r\n\r\nAdding the following modification to `tensorflow/lite/examples/label_image/CMakeLists.txt` will resolve the error, \r\n```bash\r\n$ git diff tensorflow/lite/examples/label_image/CMakeLists.txt\r\ndiff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\nindex 9874801f34f..5acbae31565 100644\r\n--- a/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n@@ -61,6 +61,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)\r\n           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)\r\n endif()\r\n \r\n+include_directories(label_image\r\n+  PUBLIC\r\n+  ${CMAKE_BINARY_DIR}\r\n+)\r\n+\r\n add_executable(label_image\r\n   ${TFLITE_LABEL_IMAGE_SRCS}\r\n )\r\n```\r\n\r\nbut a new error will occur.\r\n```bash\r\n/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateOpProfileDataFromDetail(tsl::StatsCalculator::Detail const*, tsl::StatsCalculator const*, tflite::profiling::OpProfileData*) const [clone .part.0]':\r\nprofile_summary_formatter.cc:(.text+0x9c): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x218): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x2e8): undefined reference to `tflite::profiling::OpProfilingStat* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfilingStat>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x304): undefined reference to `tflite::profiling::OpProfilingStat* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfilingStat>(google::protobuf::Arena*)'\r\n/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateSubGraphProfilingData(tsl::StatsCalculator const*, int, std::map<unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, tflite::profiling::SubGraphProfilingData*) const':\r\nprofile_summary_formatter.cc:(.text+0x3748): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x384c): undefined reference to `tflite::profiling::OpProfileData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfileData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3858): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'\r\n/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateDelegateProfilingData(tsl::StatsCalculator const*, tflite::profiling::DelegateProfilingData*) const':\r\nprofile_summary_formatter.cc:(.text+0x3a24): undefined reference to `tflite::profiling::OpProfileData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfileData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3a30): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'\r\n/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GetOutputString(std::map<unsigned int, std::unique_ptr<tsl::StatsCalculator, std::default_delete<tsl::StatsCalculator> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::unique_ptr<tsl::StatsCalculator, std::default_delete<tsl::StatsCalculator> > > > > const&, tsl::StatsCalculator const&, std::map<unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&) const':\r\nprofile_summary_formatter.cc:(.text+0x3ab8): undefined reference to `tflite::profiling::ModelProfilingData::ModelProfilingData(google::protobuf::Arena*, bool)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b2c): undefined reference to `tflite::profiling::SubGraphProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::SubGraphProfilingData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b38): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b8c): undefined reference to `google::protobuf::MessageLite::SerializeAsString[abi:cxx11]() const'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b94): undefined reference to `tflite::profiling::ModelProfilingData::~ModelProfilingData()'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bbc): undefined reference to `tflite::profiling::DelegateProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::DelegateProfilingData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bc8): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bdc): undefined reference to `tflite::profiling::ModelProfilingData::~ModelProfilingData()'\r\n/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::HandleOutput(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const':\r\nprofile_summary_formatter.cc:(.text+0x3cf4): undefined reference to `tflite::profiling::BenchmarkProfilingData::BenchmarkProfilingData(google::protobuf::Arena*, bool)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3d18): undefined reference to `google::protobuf::MessageLite::SerializeToOstream(std::ostream*) const'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3d28): undefined reference to `tflite::profiling::BenchmarkProfilingData::~BenchmarkProfilingData()'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3dc0): undefined reference to `google::protobuf::MessageLite::ParseFromString(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3de4): undefined reference to `google::protobuf::MessageLite::ParseFromString(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f28): undefined reference to `google::protobuf::Message::DebugString[abi:cxx11]() const'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f7c): undefined reference to `tflite::profiling::ModelProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::ModelProfilingData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f94): undefined reference to `tflite::profiling::ModelProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::ModelProfilingData>(google::protobuf::Arena*)'\r\n/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3fc4): undefined reference to `tflite::profiling::BenchmarkProfilingData::~BenchmarkProfilingData()'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[3]: *** [examples/label_image/CMakeFiles/label_image.dir/build.make:409: examples/label_image/label_image] Error 1\r\ngmake[2]: *** [CMakeFiles/Makefile2:7751: examples/label_image/CMakeFiles/label_image.dir/all] Error 2\r\ngmake[1]: *** [CMakeFiles/Makefile2:7758: examples/label_image/CMakeFiles/label_image.dir/rule] Error 2\r\ngmake: *** [Makefile:2600: label_image] Error 2"", 'created_at': datetime.datetime(2024, 6, 30, 13, 28, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2198569057, 'issue_id': 2382244491, 'author': 'NobuoTsukamoto', 'body': ""`tensorflow/lite/profiling/proto/profiling_info.pb.cc` and the link of the `libprotobuf` library are missing from label_image's `CMakeLists.txt`.\r\n\r\nI was able to confirm that label_image can be built successfully with the following corrections.\r\n```bash\r\n git diff tensorflow/lite/examples/label_image/CMakeLists.txt\r\ndiff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\nindex 9874801f34f..4e3cf86daf6 100644\r\n--- a/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n@@ -31,6 +31,7 @@ list(APPEND TFLITE_LABEL_IMAGE_SRCS\r\n   ${TFLITE_SOURCE_DIR}/tools/delegates/delegate_provider.cc\r\n   ${TFLITE_SOURCE_DIR}/tools/evaluation/utils.cc\r\n   ${TFLITE_SOURCE_DIR}/tools/tool_params.cc\r\n+  ${CMAKE_BINARY_DIR}/tensorflow/lite/profiling/proto/profiling_info.pb.cc\r\n )\r\n \r\n if(TFLITE_ENABLE_XNNPACK)\r\n@@ -61,6 +62,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)\r\n           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)\r\n endif()\r\n \r\n+include_directories(label_image\r\n+  PUBLIC\r\n+  ${CMAKE_BINARY_DIR}\r\n+)\r\n+\r\n add_executable(label_image\r\n   ${TFLITE_LABEL_IMAGE_SRCS}\r\n )\r\n@@ -78,4 +84,5 @@ target_compile_options(label_image\r\n )\r\n target_link_libraries(label_image\r\n   tensorflow-lite\r\n+  ${CMAKE_BINARY_DIR}/_deps/protobuf-build/libprotobuf.a\r\n )\r\n\r\n```"", 'created_at': datetime.datetime(2024, 6, 30, 13, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199784330, 'issue_id': 2382244491, 'author': 'NobuoTsukamoto', 'body': ""I'm not sure if modifying label_image's CMakeLists.txt is the correct approach, but I have confirmed that it can be built with the following modifications.\r\n```bash\r\n$ git diff tensorflow/lite/examples/label_image/CMakeLists.txt\r\ndiff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\nindex 9874801f34f..2fcb09ce96e 100644\r\n--- a/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt\r\n@@ -61,6 +61,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)\r\n           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)\r\n endif()\r\n \r\n+include_directories(label_image\r\n+  PUBLIC\r\n+  ${CMAKE_BINARY_DIR}\r\n+)\r\n+\r\n add_executable(label_image\r\n   ${TFLITE_LABEL_IMAGE_SRCS}\r\n )\r\n@@ -78,4 +83,6 @@ target_compile_options(label_image\r\n )\r\n target_link_libraries(label_image\r\n   tensorflow-lite\r\n+  profiling_info_proto\r\n+  protobuf\r\n )\r\n```"", 'created_at': datetime.datetime(2024, 7, 1, 10, 27, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199932557, 'issue_id': 2382244491, 'author': 'NobuoTsukamoto', 'body': 'The latest commit (775c84d11596da38120a9443dca87ca5d7930ead) still gives the error I reported.', 'created_at': datetime.datetime(2024, 7, 1, 11, 47, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2210318495, 'issue_id': 2382244491, 'author': 'sawantkumar', 'body': 'Hi @pkgoogle , \r\n\r\nI replicate the issue and i got the same error , can you please take a look ?\r\n\r\n`Scanning dependencies of target label_image\r\n[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/bitmap_helpers.cc.o\r\n[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/label_image.cc.o\r\n[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/home/sawantkumar/tensorflow/third_party/xla/xla/tsl/util/stats_calculator.cc.o\r\n[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/memory_info.cc.o\r\n[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/profile_summarizer.cc.o\r\nIn file included from /home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summarizer.h:28,\r\n                 from /home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summarizer.cc:16:\r\n/home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summary_formatter.h:31:10: fatal error: tensorflow/lite/profiling/proto/profiling_info.pb.h: No such file or directory\r\n   31 | #include ""tensorflow/lite/profiling/proto/profiling_info.pb.h""\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[3]: *** [examples/label_image/CMakeFiles/label_image.dir/build.make:115: examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/profile_summarizer.cc.o] Error 1\r\nmake[2]: *** [CMakeFiles/Makefile2:8102: examples/label_image/CMakeFiles/label_image.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/Makefile2:8109: examples/label_image/CMakeFiles/label_image.dir/rule] Error 2\r\nmake: *** [Makefile:2595: label_image] Error 2`', 'created_at': datetime.datetime(2024, 7, 5, 7, 7, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2214947194, 'issue_id': 2382244491, 'author': 'pkgoogle', 'body': ""The PR looks correct to me, let's wait for a review."", 'created_at': datetime.datetime(2024, 7, 8, 18, 56, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272743702, 'issue_id': 2382244491, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70659"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70659"">No</a>', 'created_at': datetime.datetime(2024, 8, 7, 6, 48, 2, tzinfo=datetime.timezone.utc)}]","NobuoTsukamoto (Issue Creator) on (2024-06-30 13:28:33 UTC): The problem is that `tensorflow/lite/profiling/proto/profiling_info.pb.h` is generated under `CMAKE_BINARY_DIR`, but there is no include_directories setting.

Adding the following modification to `tensorflow/lite/examples/label_image/CMakeLists.txt` will resolve the error, 
```bash
$ git diff tensorflow/lite/examples/label_image/CMakeLists.txt
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 9874801f34f..5acbae31565 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -61,6 +61,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)
 endif()
 
+include_directories(label_image
+  PUBLIC
+  ${CMAKE_BINARY_DIR}
+)
+
 add_executable(label_image
   ${TFLITE_LABEL_IMAGE_SRCS}
 )
```

but a new error will occur.
```bash
/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateOpProfileDataFromDetail(tsl::StatsCalculator::Detail const*, tsl::StatsCalculator const*, tflite::profiling::OpProfileData*) const [clone .part.0]':
profile_summary_formatter.cc:(.text+0x9c): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x218): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x2e8): undefined reference to `tflite::profiling::OpProfilingStat* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfilingStat>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x304): undefined reference to `tflite::profiling::OpProfilingStat* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfilingStat>(google::protobuf::Arena*)'
/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateSubGraphProfilingData(tsl::StatsCalculator const*, int, std::map<unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, tflite::profiling::SubGraphProfilingData*) const':
profile_summary_formatter.cc:(.text+0x3748): undefined reference to `google::protobuf::internal::ArenaStringPtr::Set(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x384c): undefined reference to `tflite::profiling::OpProfileData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfileData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3858): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'
/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GenerateDelegateProfilingData(tsl::StatsCalculator const*, tflite::profiling::DelegateProfilingData*) const':
profile_summary_formatter.cc:(.text+0x3a24): undefined reference to `tflite::profiling::OpProfileData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::OpProfileData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3a30): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'
/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::GetOutputString(std::map<unsigned int, std::unique_ptr<tsl::StatsCalculator, std::default_delete<tsl::StatsCalculator> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::unique_ptr<tsl::StatsCalculator, std::default_delete<tsl::StatsCalculator> > > > > const&, tsl::StatsCalculator const&, std::map<unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&) const':
profile_summary_formatter.cc:(.text+0x3ab8): undefined reference to `tflite::profiling::ModelProfilingData::ModelProfilingData(google::protobuf::Arena*, bool)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b2c): undefined reference to `tflite::profiling::SubGraphProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::SubGraphProfilingData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b38): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b8c): undefined reference to `google::protobuf::MessageLite::SerializeAsString[abi:cxx11]() const'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3b94): undefined reference to `tflite::profiling::ModelProfilingData::~ModelProfilingData()'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bbc): undefined reference to `tflite::profiling::DelegateProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::DelegateProfilingData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bc8): undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3bdc): undefined reference to `tflite::profiling::ModelProfilingData::~ModelProfilingData()'
/usr/bin/ld: CMakeFiles/label_image.dir/__/__/profiling/profile_summary_formatter.cc.o: in function `tflite::profiling::ProfileSummaryProtoFormatter::HandleOutput(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const':
profile_summary_formatter.cc:(.text+0x3cf4): undefined reference to `tflite::profiling::BenchmarkProfilingData::BenchmarkProfilingData(google::protobuf::Arena*, bool)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3d18): undefined reference to `google::protobuf::MessageLite::SerializeToOstream(std::ostream*) const'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3d28): undefined reference to `tflite::profiling::BenchmarkProfilingData::~BenchmarkProfilingData()'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3dc0): undefined reference to `google::protobuf::MessageLite::ParseFromString(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3de4): undefined reference to `google::protobuf::MessageLite::ParseFromString(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f28): undefined reference to `google::protobuf::Message::DebugString[abi:cxx11]() const'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f7c): undefined reference to `tflite::profiling::ModelProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::ModelProfilingData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3f94): undefined reference to `tflite::profiling::ModelProfilingData* google::protobuf::Arena::CreateMaybeMessage<tflite::profiling::ModelProfilingData>(google::protobuf::Arena*)'
/usr/bin/ld: profile_summary_formatter.cc:(.text+0x3fc4): undefined reference to `tflite::profiling::BenchmarkProfilingData::~BenchmarkProfilingData()'
collect2: error: ld returned 1 exit status
gmake[3]: *** [examples/label_image/CMakeFiles/label_image.dir/build.make:409: examples/label_image/label_image] Error 1
gmake[2]: *** [CMakeFiles/Makefile2:7751: examples/label_image/CMakeFiles/label_image.dir/all] Error 2
gmake[1]: *** [CMakeFiles/Makefile2:7758: examples/label_image/CMakeFiles/label_image.dir/rule] Error 2
gmake: *** [Makefile:2600: label_image] Error 2

NobuoTsukamoto (Issue Creator) on (2024-06-30 13:45:32 UTC): `tensorflow/lite/profiling/proto/profiling_info.pb.cc` and the link of the `libprotobuf` library are missing from label_image's `CMakeLists.txt`.

I was able to confirm that label_image can be built successfully with the following corrections.
```bash
 git diff tensorflow/lite/examples/label_image/CMakeLists.txt
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 9874801f34f..4e3cf86daf6 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -31,6 +31,7 @@ list(APPEND TFLITE_LABEL_IMAGE_SRCS
   ${TFLITE_SOURCE_DIR}/tools/delegates/delegate_provider.cc
   ${TFLITE_SOURCE_DIR}/tools/evaluation/utils.cc
   ${TFLITE_SOURCE_DIR}/tools/tool_params.cc
+  ${CMAKE_BINARY_DIR}/tensorflow/lite/profiling/proto/profiling_info.pb.cc
 )
 
 if(TFLITE_ENABLE_XNNPACK)
@@ -61,6 +62,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)
 endif()
 
+include_directories(label_image
+  PUBLIC
+  ${CMAKE_BINARY_DIR}
+)
+
 add_executable(label_image
   ${TFLITE_LABEL_IMAGE_SRCS}
 )
@@ -78,4 +84,5 @@ target_compile_options(label_image
 )
 target_link_libraries(label_image
   tensorflow-lite
+  ${CMAKE_BINARY_DIR}/_deps/protobuf-build/libprotobuf.a
 )

```

NobuoTsukamoto (Issue Creator) on (2024-07-01 10:27:14 UTC): I'm not sure if modifying label_image's CMakeLists.txt is the correct approach, but I have confirmed that it can be built with the following modifications.
```bash
$ git diff tensorflow/lite/examples/label_image/CMakeLists.txt
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 9874801f34f..2fcb09ce96e 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -61,6 +61,11 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
           ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)
 endif()
 
+include_directories(label_image
+  PUBLIC
+  ${CMAKE_BINARY_DIR}
+)
+
 add_executable(label_image
   ${TFLITE_LABEL_IMAGE_SRCS}
 )
@@ -78,4 +83,6 @@ target_compile_options(label_image
 )
 target_link_libraries(label_image
   tensorflow-lite
+  profiling_info_proto
+  protobuf
 )
```

NobuoTsukamoto (Issue Creator) on (2024-07-01 11:47:49 UTC): The latest commit (775c84d11596da38120a9443dca87ca5d7930ead) still gives the error I reported.

sawantkumar (Assginee) on (2024-07-05 07:07:05 UTC): Hi @pkgoogle , 

I replicate the issue and i got the same error , can you please take a look ?

`Scanning dependencies of target label_image
[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/bitmap_helpers.cc.o
[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/label_image.cc.o
[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/home/sawantkumar/tensorflow/third_party/xla/xla/tsl/util/stats_calculator.cc.o
[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/memory_info.cc.o
[100%] Building CXX object examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/profile_summarizer.cc.o
In file included from /home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summarizer.h:28,
                 from /home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summarizer.cc:16:
/home/sawantkumar/tensorflow/tensorflow/lite/profiling/profile_summary_formatter.h:31:10: fatal error: tensorflow/lite/profiling/proto/profiling_info.pb.h: No such file or directory
   31 | #include ""tensorflow/lite/profiling/proto/profiling_info.pb.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
make[3]: *** [examples/label_image/CMakeFiles/label_image.dir/build.make:115: examples/label_image/CMakeFiles/label_image.dir/__/__/profiling/profile_summarizer.cc.o] Error 1
make[2]: *** [CMakeFiles/Makefile2:8102: examples/label_image/CMakeFiles/label_image.dir/all] Error 2
make[1]: *** [CMakeFiles/Makefile2:8109: examples/label_image/CMakeFiles/label_image.dir/rule] Error 2
make: *** [Makefile:2595: label_image] Error 2`

pkgoogle (Assginee) on (2024-07-08 18:56:21 UTC): The PR looks correct to me, let's wait for a review.

google-ml-butler[bot] on (2024-08-07 06:48:02 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70659"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70659"">No</a>

"
2382197351,issue,closed,completed,Python interpreter set from ./configure does not go with py_strict_test,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

15

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I'm trying to run the python tests `bazel test $BAZEL_FLAGS  //tensorflow/python/...` with my pre-built python library.
In ` third_party/py/BUILD.tpl`, it seems like the python tests must be running on the library we set in the ./configure.

However, I observed that tensorflow brings its own python runtime binary from somewhere, and use it for the test.

For example, if I run 
`bazel test $BAZEL_FLAGS  //tensorflow/python/kernel_tests:benchmark_test_cpu`

Following python runtime is used which is not the python binary I specified in the configure.
`bazel-bin/tensorflow/python/kernel_tests/benchmark_test_cpu.runfiles/python_x86_64-unknown-linux-gnu/bin/python`

Could you let me know where was this python interpreter from and how to change it to my python interpeter?

Thank you:)

### Standalone code to reproduce the issue

```shell
`bazel test $BAZEL_FLAGS  //tensorflow/python/kernel_tests:benchmark_test_cpu`
```


### Relevant log output

_No response_",junwha,2024-06-30 10:47:37+00:00,['tilakrayal'],2024-09-06 01:58:02+00:00,2024-09-06 01:57:58+00:00,https://github.com/tensorflow/tensorflow/issues/70658,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('type:support', 'Support issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity')]","[{'comment_id': 2200251093, 'issue_id': 2382197351, 'author': 'tilakrayal', 'body': '@junwha0511,\r\nAFAIK the ./configure is redirecting to the ${CONFIGURE_DIR}/configure.py""  \r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure\r\n\r\n\r\n```python\r\nCONFIGURE_DIR=$(dirname ""$0"")\r\n""$PYTHON_BIN_PATH"" ""${CONFIGURE_DIR}/configure.py"" ""$@""\r\n```\r\n\r\n\r\nCould you please try to configure the below file and try to test.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 1, 14, 6, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2200540403, 'issue_id': 2382197351, 'author': 'junwha', 'body': 'Thank you for the response!\r\n\r\nI already set the env variable PYTHON_BIN_PATH with my binary, but bazel automatically hard-codes its own python binary (built by Clang 17, at 2023) for the test in the test file.\r\n\r\nBazel inserted code (PYTHON_BINARY)\r\n![image](https://github.com/tensorflow/tensorflow/assets/17183234/edaf2842-8dca-49a7-bfc3-daf8aa0d7e5c)\r\n\r\n`objdump -s -j .comment bazel-bin/tensorflow/python/kernel_tests/benchmark_test_cpu.runfiles/python_x86_64-unknown-linux-gnu/bin/python3`\r\n![image](https://github.com/tensorflow/tensorflow/assets/17183234/836d67d3-330f-4b0d-b925-b3f2a52572c5)\r\n\r\nI have no clang-17 on my host and the python3 version is also different.\r\nand the file was created in 2023, thus we can presume it was pulled from an external repo.', 'created_at': datetime.datetime(2024, 7, 1, 16, 4, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2200559830, 'issue_id': 2382197351, 'author': 'junwha', 'body': 'https://github.com/bazelbuild/bazel/blob/master/tools/python/python_bootstrap_template.txt#L73\r\n\r\nThe python binary is set by `python_bootstrap_template.txt` and the binary seems to be from `bazel-tensorflow/external/python_x86_64-unknown-linux-gnu/bin/`', 'created_at': datetime.datetime(2024, 7, 1, 16, 14, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304060913, 'issue_id': 2382197351, 'author': 'tilakrayal', 'body': '@junwha0511,\r\nCould you please test whether you are facing the same issue with the latest tensorflow v2.17 and also Could you try bazel clean --expunge followed by bazel sync. Thank you!', 'created_at': datetime.datetime(2024, 8, 22, 8, 17, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2319671632, 'issue_id': 2382197351, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 8, 30, 1, 57, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039647, 'issue_id': 2382197351, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 6, 1, 57, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333039692, 'issue_id': 2382197351, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70658"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70658"">No</a>', 'created_at': datetime.datetime(2024, 9, 6, 1, 58, tzinfo=datetime.timezone.utc)}]","tilakrayal (Assginee) on (2024-07-01 14:06:59 UTC): @junwha0511,
AFAIK the ./configure is redirecting to the ${CONFIGURE_DIR}/configure.py""  
https://github.com/tensorflow/tensorflow/blob/master/configure


```python
CONFIGURE_DIR=$(dirname ""$0"")
""$PYTHON_BIN_PATH"" ""${CONFIGURE_DIR}/configure.py"" ""$@""
```


Could you please try to configure the below file and try to test.

https://github.com/tensorflow/tensorflow/blob/master/configure.py

Thank you!

junwha (Issue Creator) on (2024-07-01 16:04:35 UTC): Thank you for the response!

I already set the env variable PYTHON_BIN_PATH with my binary, but bazel automatically hard-codes its own python binary (built by Clang 17, at 2023) for the test in the test file.

Bazel inserted code (PYTHON_BINARY)
![image](https://github.com/tensorflow/tensorflow/assets/17183234/edaf2842-8dca-49a7-bfc3-daf8aa0d7e5c)

`objdump -s -j .comment bazel-bin/tensorflow/python/kernel_tests/benchmark_test_cpu.runfiles/python_x86_64-unknown-linux-gnu/bin/python3`
![image](https://github.com/tensorflow/tensorflow/assets/17183234/836d67d3-330f-4b0d-b925-b3f2a52572c5)

I have no clang-17 on my host and the python3 version is also different.
and the file was created in 2023, thus we can presume it was pulled from an external repo.

junwha (Issue Creator) on (2024-07-01 16:14:47 UTC): https://github.com/bazelbuild/bazel/blob/master/tools/python/python_bootstrap_template.txt#L73

The python binary is set by `python_bootstrap_template.txt` and the binary seems to be from `bazel-tensorflow/external/python_x86_64-unknown-linux-gnu/bin/`

tilakrayal (Assginee) on (2024-08-22 08:17:46 UTC): @junwha0511,
Could you please test whether you are facing the same issue with the latest tensorflow v2.17 and also Could you try bazel clean --expunge followed by bazel sync. Thank you!

github-actions[bot] on (2024-08-30 01:57:28 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-06 01:57:58 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-06 01:58:00 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70658"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70658"">No</a>

"
2381311143,issue,closed,completed,tensorflow uninstallable,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4.1/9.1.1.17

### GPU model and memory

_No response_

### Current behavior?

ImportError                               Traceback (most recent call last)
File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.


### Standalone code to reproduce the issue

```shell
ImportError                               Traceback (most recent call last)
File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Relevant log output

_No response_",hiwothadush,2024-06-28 23:23:56+00:00,['sushreebarsa'],2024-07-16 01:53:40+00:00,2024-07-16 01:53:37+00:00,https://github.com/tensorflow/tensorflow/issues/70617,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype:windows', 'Windows Build/Installation Issues'), ('TF 2.10', '')]","[{'comment_id': 2199635441, 'issue_id': 2381311143, 'author': 'sushreebarsa', 'body': '@hiwothadush Could you please refer to these [steps](https://www.tensorflow.org/install/source_windows) also try to upgrade to the latest and let us know if it helps?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 1, 9, 11, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2216166787, 'issue_id': 2381311143, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 9, 1, 52, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229847794, 'issue_id': 2381311143, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 7, 16, 1, 53, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2229847986, 'issue_id': 2381311143, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70617"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70617"">No</a>', 'created_at': datetime.datetime(2024, 7, 16, 1, 53, 39, tzinfo=datetime.timezone.utc)}]","sushreebarsa (Assginee) on (2024-07-01 09:11:46 UTC): @hiwothadush Could you please refer to these [steps](https://www.tensorflow.org/install/source_windows) also try to upgrade to the latest and let us know if it helps?
Thank you!

github-actions[bot] on (2024-07-09 01:52:31 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-07-16 01:53:37 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-07-16 01:53:39 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70617"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70617"">No</a>

"
2380994554,issue,closed,completed,"Tried multiple builds, Bazel, pip, file. non of it is working","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.2

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

## Bazel

venv1)   tensorflow git:(master)  bazel build --config=opt --config=cuda
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=100
INFO: Reading rc options for 'build' from /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/bin/python3 --action_env PYTHON_LIB_PATH=/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages --python_path=/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/bin/python3 --config=tensorrt --action_env TF_CUDA_VERSION=12 --action_env TF_CUDNN_VERSION=9 --action_env TF_TENSORRT_VERSION=10 --action_env TF_NCCL_VERSION= --action_env TF_CUDA_PATHS=/opt/cuda/targets/x86_64-linux/include,/opt/cuda/targets/x86_64-linux/lib/,/opt/cuda,/usr/include,/usr/include,/usr/lib --action_env CUDA_TOOLKIT_PATH=/opt/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.9 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda
INFO: Found applicable config definition build:short_logs in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.tf_configure.bazelrc: --copt=-O3 --host_copt=-O3 --copt=-march=native --host_copt=-march=native --copt=-mtune=native --host_copt=-mtune=native --copt=-ffast-math --host_copt=-ffast-math --copt=-funroll-loops --host_copt=-funroll-loops --copt=-fopenmp --host_copt=-fopenmp --copt=-flto --host_copt=-flto --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:cuda in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Repository local_config_cuda instantiated at:
  /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/WORKSPACE:95:14: in <toplevel>
  /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/tensorflow/workspace2.bzl:928:19: in workspace
  /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/tensorflow/workspace2.bzl:106:19: in _tf_toolchains
Repository rule cuda_configure defined at:
  /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl:1542:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1145, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 679, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 574, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
sh: line 1: /home/8kesar/Filez/MathematicsProjects: No such file or directory
sh: line 1: Modeling/venv1/bin/python3: No such file or directory
ERROR: /home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/WORKSPACE:95:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1145, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 679, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/gpus/cuda_configure.bzl"", line 574, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
sh: line 1: /home/8kesar/Filez/MathematicsProjects: No such file or directory
sh: line 1: Modeling/venv1/bin/python3: No such file or directory
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
sh: line 1: /home/8kesar/Filez/MathematicsProjects: No such file or directory
sh: line 1: Modeling/venv1/bin/python3: No such file or directory
Loading: 

## pip
(venv1)   MathematicsProjects&Modeling pip install tf-nightly-gpu
Collecting tf-nightly-gpu
  Downloading tf-nightly-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
   python setup.py egg_info did not run successfully.
   exit code: 1
  > [44 lines of output]
      Traceback (most recent call last):
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/requirements.py"", line 35, in __init__
          parsed = _parse_requirement(requirement_string)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py"", line 64, in parse_requirement
          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py"", line 82, in _parse_requirement
          url, specifier, marker = _parse_requirement_details(tokenizer)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py"", line 126, in _parse_requirement_details
          marker = _parse_requirement_marker(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py"", line 147, in _parse_requirement_marker
          tokenizer.raise_syntax_error(
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py"", line 165, in raise_syntax_error
          raise ParserSyntaxError(
      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      
      The above exception was the direct cause of the following exception:
      
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-b9fpugve/tf-nightly-gpu_47887266de004f9f9805cd57c7fcaf13/setup.py"", line 40, in <module>
          setuptools.setup()
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/__init__.py"", line 102, in setup
          _install_setup_requires(attrs)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/__init__.py"", line 73, in _install_setup_requires
          dist.parse_config_files(ignore_option_errors=True)
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/dist.py"", line 636, in parse_config_files
          self._finalize_requires()
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/dist.py"", line 370, in _finalize_requires
          self._normalize_requires()
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/dist.py"", line 385, in _normalize_requires
          self.install_requires = list(map(str, _reqs.parse(install_requires)))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/8kesar/Filez/MathematicsProjects&Modeling/venv1/lib/python3.12/site-packages/setuptools/_vendor/packaging/requirements.py"", line 37, in __init__
          raise InvalidRequirement(str(e)) from e
      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

 Encountered error while generating package metadata.
> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

### Standalone code to reproduce the issue

```shell
I am on an arch linux system
```


### Relevant log output

_No response_",asp616848,2024-06-28 18:54:05+00:00,['tilakrayal'],2024-09-18 01:59:12+00:00,2024-09-18 01:59:08+00:00,https://github.com/tensorflow/tensorflow/issues/70603,"[('stat:awaiting response', 'Status  - Awaiting response from author'), ('type:build/install', 'Build and install issues'), ('stale', 'This label marks the issue/pr stale - to be closed automatically if no activity'), ('subtype: ubuntu/linux', 'Ubuntu/Linux Build/Installation Issues'), ('TF 2.16', '')]","[{'comment_id': 2199446539, 'issue_id': 2380994554, 'author': 'sushreebarsa', 'body': '@asp616848 Could you please confirm if you have followed the steps [here](https://www.tensorflow.org/install/source)?\r\nThank you!', 'created_at': datetime.datetime(2024, 7, 1, 7, 36, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199483615, 'issue_id': 2380994554, 'author': 'asp616848', 'body': ""> @asp616848 Could you please confirm if you have followed the steps [here](https://www.tensorflow.org/install/source)?\n> Thank you!\n\nYeah, so after following it I got the first error which was couldn't find Local Config part..."", 'created_at': datetime.datetime(2024, 7, 1, 7, 55, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199586332, 'issue_id': 2380994554, 'author': 'sushreebarsa', 'body': ""@asp616848 The error you encountered indicates an issue with the syntax of the requirements.txt file or the install_requires parameter in the setup.py file. Specifically, it appears there's an issue with the version specifier for python_version. Please check for any syntax errors in your requirements.txt or setup.py file. Even a small typo can cause issues.\r\n\r\nThank you!"", 'created_at': datetime.datetime(2024, 7, 1, 8, 47, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199919988, 'issue_id': 2380994554, 'author': 'asp616848', 'body': ""But that's for the pip part which I later figured out that the new version of TensorFlow just doesn't have its -GPU version, and that estimator is deprecated as well. \r\nBut for the bazel, in the install from the source page. I followed each step yet I got the error with local_config mentioned in it.\r\nI think something is wrong with my cuda as I have installed it using pip so it didn't went to the default location."", 'created_at': datetime.datetime(2024, 7, 1, 11, 40, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2199920038, 'issue_id': 2380994554, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">No</a>', 'created_at': datetime.datetime(2024, 7, 1, 11, 40, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2216166915, 'issue_id': 2380994554, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 7, 9, 1, 52, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275770687, 'issue_id': 2380994554, 'author': 'tilakrayal', 'body': '@asp616848,\r\nCould you please provide the complete environment details which you are trying to install. Also try to install the latest tensorflow v2.17 where the GPU issue has been resolved. Thank you!', 'created_at': datetime.datetime(2024, 8, 8, 13, 2, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2290767430, 'issue_id': 2380994554, 'author': 'asp616848', 'body': ""Package                   Version\r\n------------------------- --------------\r\nabsl-py                   2.1.0\r\nanyio                     4.4.0\r\nargon2-cffi               23.1.0\r\nargon2-cffi-bindings      21.2.0\r\narray_record              0.5.1\r\narrow                     1.3.0\r\nasttokens                 2.4.1\r\nastunparse                1.6.3\r\nasync-lru                 2.0.4\r\nattrs                     24.2.0\r\nbabel                     2.16.0\r\nbeautifulsoup4            4.12.3\r\nbleach                    6.1.0\r\ncertifi                   2024.6.2\r\ncffi                      1.17.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncomm                      0.2.2\r\ncontourpy                 1.2.1\r\ncuda-python               12.5.0\r\ncycler                    0.12.1\r\ndebugpy                   1.8.1\r\ndecorator                 5.1.1\r\ndefusedxml                0.7.1\r\ndm-tree                   0.1.8\r\ndocstring_parser          0.16\r\netils                     1.9.2\r\nexecuting                 2.0.1\r\nez_setup                  0.9\r\nfastjsonschema            2.20.0\r\nflatbuffers               24.3.25\r\nfonttools                 4.53.0\r\nfqdn                      1.5.1\r\nfsspec                    2024.6.1\r\ngast                      0.5.5\r\ngoogle-pasta              0.2.0\r\ngoogleapis-common-protos  1.63.2\r\ngrpcio                    1.64.1\r\nh11                       0.14.0\r\nh5py                      3.11.0\r\nhttpcore                  1.0.5\r\nhttpx                     0.27.0\r\nidna                      3.7\r\nimmutabledict             4.2.0\r\nimportlib_resources       6.4.0\r\nipykernel                 6.29.4\r\nipython                   8.25.0\r\nipywidgets                8.1.3\r\nisoduration               20.11.0\r\njedi                      0.19.1\r\nJinja2                    3.1.4\r\njoblib                    1.4.2\r\njson5                     0.9.25\r\njsonpointer               3.0.0\r\njsonschema                4.23.0\r\njsonschema-specifications 2023.12.1\r\njupyter                   1.0.0\r\njupyter_client            8.6.2\r\njupyter-console           6.6.3\r\njupyter_core              5.7.2\r\njupyter-events            0.10.0\r\njupyter-lsp               2.2.5\r\njupyter_server            2.14.2\r\njupyter_server_terminals  0.5.3\r\njupyterlab                4.2.4\r\njupyterlab_pygments       0.3.0\r\njupyterlab_server         2.27.3\r\njupyterlab_widgets        3.0.11\r\nkeras                     3.4.1\r\nkiwisolver                1.4.5\r\nlibclang                  18.1.1\r\nMarkdown                  3.6\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.5\r\nmatplotlib                3.9.0\r\nmatplotlib-inline         0.1.7\r\nmdurl                     0.1.2\r\nmistune                   3.0.2\r\nml-dtypes                 0.3.2\r\nmpmath                    1.3.0\r\nnamex                     0.0.8\r\nnbclient                  0.10.0\r\nnbconvert                 7.16.4\r\nnbformat                  5.10.4\r\nnest-asyncio              1.6.0\r\nnotebook                  7.2.1\r\nnotebook_shim             0.2.4\r\nnumpy                     1.26.4\r\nopt-einsum                3.3.0\r\noptree                    0.11.0\r\noverrides                 7.7.0\r\npackaging                 24.1\r\npaho-mqtt                 2.1.0\r\npandas                    2.2.2\r\npandocfilters             1.5.1\r\nparso                     0.8.4\r\npexpect                   4.9.0\r\npillow                    10.3.0\r\npip                       24.1.2\r\nplatformdirs              4.2.2\r\nprometheus_client         0.20.0\r\npromise                   2.3\r\nprompt_toolkit            3.0.47\r\nprotobuf                  4.25.3\r\npsutil                    6.0.0\r\nptyprocess                0.7.0\r\npure-eval                 0.2.2\r\npyarrow                   16.1.0\r\npycparser                 2.22\r\nPygments                  2.18.0\r\npyparsing                 3.1.2\r\nPyQt6                     6.7.0\r\nPyQt6-Qt6                 6.7.1\r\nPyQt6-sip                 13.6.0\r\npython-dateutil           2.9.0.post0\r\npython-json-logger        2.0.7\r\npytz                      2024.1\r\nPyYAML                    6.0.2\r\npyzmq                     26.0.3\r\nqtconsole                 5.5.2\r\nQtPy                      2.4.1\r\nreferencing               0.35.1\r\nrequests                  2.32.3\r\nrfc3339-validator         0.1.4\r\nrfc3986-validator         0.1.1\r\nrich                      13.7.1\r\nrpds-py                   0.20.0\r\nscikit-learn              1.5.0\r\nscipy                     1.13.1\r\nSend2Trash                1.8.3\r\nsetuptools                70.1.1\r\nsimple_parsing            0.1.5\r\nsix                       1.16.0\r\nsniffio                   1.3.1\r\nsoupsieve                 2.5\r\nstack-data                0.6.3\r\nsympy                     1.13.1\r\ntensorboard               2.17.0\r\ntensorboard-data-server   0.7.2\r\ntensorflow                2.17.0\r\ntensorflow-datasets       4.9.6\r\ntensorflow-estimator      2.15.0\r\ntensorflow-metadata       1.15.0\r\ntensorrt                  10.1.0\r\ntensorrt-dispatch         10.1.0\r\ntensorrt-lean             10.1.0\r\ntermcolor                 2.4.0\r\nterminado                 0.18.1\r\nthreadpoolctl             3.5.0\r\ntinycss2                  1.3.0\r\ntoml                      0.10.2\r\ntornado                   6.4.1\r\ntqdm                      4.66.4\r\ntraitlets                 5.14.3\r\ntypes-python-dateutil     2.9.0.20240316\r\ntyping_extensions         4.12.2\r\ntzdata                    2024.1\r\nuri-template              1.3.0\r\nurllib3                   2.2.2\r\nwcwidth                   0.2.13\r\nwebcolors                 24.8.0\r\nwebencodings              0.5.1\r\nwebsocket-client          1.8.0\r\nWerkzeug                  3.0.3\r\nwheel                     0.43.0\r\nwidgetsnbextension        4.0.11\r\nwrapt                     1.16.0\r\nzipp                      3.19.2\r\n\r\n\r\nenvironment details, yet it doesn't utilize gpu and show some warnings while importing as follows:\r\n\r\n2024-08-15 12:14:46.166633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-08-15 12:14:46.395008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-08-15 12:14:46.466695: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-08-15 12:14:46.494153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-15 12:14:46.612817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-15 12:14:47.395056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"", 'created_at': datetime.datetime(2024, 8, 15, 6, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324267609, 'issue_id': 2380994554, 'author': 'tilakrayal', 'body': '@asp616848,\r\nApologies for the delay. I suspect there was an issue with the tensorflow v2.16, could you please try to upgrade the tensorflow v2.17 and check if you are facing the same issue. Also could you try **bazel clean --expunge** followed by bazel sync.\r\n\r\nThank you!', 'created_at': datetime.datetime(2024, 9, 2, 9, 30, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339460557, 'issue_id': 2380994554, 'author': 'github-actions[bot]', 'body': 'This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.', 'created_at': datetime.datetime(2024, 9, 10, 1, 59, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357336958, 'issue_id': 2380994554, 'author': 'github-actions[bot]', 'body': ""This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."", 'created_at': datetime.datetime(2024, 9, 18, 1, 59, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357337035, 'issue_id': 2380994554, 'author': 'google-ml-butler[bot]', 'body': 'Are you satisfied with the resolution of your issue?\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">Yes</a>\n<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">No</a>', 'created_at': datetime.datetime(2024, 9, 18, 1, 59, 11, tzinfo=datetime.timezone.utc)}]","sushreebarsa on (2024-07-01 07:36:05 UTC): @asp616848 Could you please confirm if you have followed the steps [here](https://www.tensorflow.org/install/source)?
Thank you!

asp616848 (Issue Creator) on (2024-07-01 07:55:35 UTC): Yeah, so after following it I got the first error which was couldn't find Local Config part...

sushreebarsa on (2024-07-01 08:47:59 UTC): @asp616848 The error you encountered indicates an issue with the syntax of the requirements.txt file or the install_requires parameter in the setup.py file. Specifically, it appears there's an issue with the version specifier for python_version. Please check for any syntax errors in your requirements.txt or setup.py file. Even a small typo can cause issues.

Thank you!

asp616848 (Issue Creator) on (2024-07-01 11:40:38 UTC): But that's for the pip part which I later figured out that the new version of TensorFlow just doesn't have its -GPU version, and that estimator is deprecated as well. 
But for the bazel, in the install from the source page. I followed each step yet I got the error with local_config mentioned in it.
I think something is wrong with my cuda as I have installed it using pip so it didn't went to the default location.

google-ml-butler[bot] on (2024-07-01 11:40:40 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">No</a>

github-actions[bot] on (2024-07-09 01:52:32 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

tilakrayal (Assginee) on (2024-08-08 13:02:29 UTC): @asp616848,
Could you please provide the complete environment details which you are trying to install. Also try to install the latest tensorflow v2.17 where the GPU issue has been resolved. Thank you!

asp616848 (Issue Creator) on (2024-08-15 06:45:18 UTC): Package                   Version
------------------------- --------------
absl-py                   2.1.0
anyio                     4.4.0
argon2-cffi               23.1.0
argon2-cffi-bindings      21.2.0
array_record              0.5.1
arrow                     1.3.0
asttokens                 2.4.1
astunparse                1.6.3
async-lru                 2.0.4
attrs                     24.2.0
babel                     2.16.0
beautifulsoup4            4.12.3
bleach                    6.1.0
certifi                   2024.6.2
cffi                      1.17.0
charset-normalizer        3.3.2
click                     8.1.7
comm                      0.2.2
contourpy                 1.2.1
cuda-python               12.5.0
cycler                    0.12.1
debugpy                   1.8.1
decorator                 5.1.1
defusedxml                0.7.1
dm-tree                   0.1.8
docstring_parser          0.16
etils                     1.9.2
executing                 2.0.1
ez_setup                  0.9
fastjsonschema            2.20.0
flatbuffers               24.3.25
fonttools                 4.53.0
fqdn                      1.5.1
fsspec                    2024.6.1
gast                      0.5.5
google-pasta              0.2.0
googleapis-common-protos  1.63.2
grpcio                    1.64.1
h11                       0.14.0
h5py                      3.11.0
httpcore                  1.0.5
httpx                     0.27.0
idna                      3.7
immutabledict             4.2.0
importlib_resources       6.4.0
ipykernel                 6.29.4
ipython                   8.25.0
ipywidgets                8.1.3
isoduration               20.11.0
jedi                      0.19.1
Jinja2                    3.1.4
joblib                    1.4.2
json5                     0.9.25
jsonpointer               3.0.0
jsonschema                4.23.0
jsonschema-specifications 2023.12.1
jupyter                   1.0.0
jupyter_client            8.6.2
jupyter-console           6.6.3
jupyter_core              5.7.2
jupyter-events            0.10.0
jupyter-lsp               2.2.5
jupyter_server            2.14.2
jupyter_server_terminals  0.5.3
jupyterlab                4.2.4
jupyterlab_pygments       0.3.0
jupyterlab_server         2.27.3
jupyterlab_widgets        3.0.11
keras                     3.4.1
kiwisolver                1.4.5
libclang                  18.1.1
Markdown                  3.6
markdown-it-py            3.0.0
MarkupSafe                2.1.5
matplotlib                3.9.0
matplotlib-inline         0.1.7
mdurl                     0.1.2
mistune                   3.0.2
ml-dtypes                 0.3.2
mpmath                    1.3.0
namex                     0.0.8
nbclient                  0.10.0
nbconvert                 7.16.4
nbformat                  5.10.4
nest-asyncio              1.6.0
notebook                  7.2.1
notebook_shim             0.2.4
numpy                     1.26.4
opt-einsum                3.3.0
optree                    0.11.0
overrides                 7.7.0
packaging                 24.1
paho-mqtt                 2.1.0
pandas                    2.2.2
pandocfilters             1.5.1
parso                     0.8.4
pexpect                   4.9.0
pillow                    10.3.0
pip                       24.1.2
platformdirs              4.2.2
prometheus_client         0.20.0
promise                   2.3
prompt_toolkit            3.0.47
protobuf                  4.25.3
psutil                    6.0.0
ptyprocess                0.7.0
pure-eval                 0.2.2
pyarrow                   16.1.0
pycparser                 2.22
Pygments                  2.18.0
pyparsing                 3.1.2
PyQt6                     6.7.0
PyQt6-Qt6                 6.7.1
PyQt6-sip                 13.6.0
python-dateutil           2.9.0.post0
python-json-logger        2.0.7
pytz                      2024.1
PyYAML                    6.0.2
pyzmq                     26.0.3
qtconsole                 5.5.2
QtPy                      2.4.1
referencing               0.35.1
requests                  2.32.3
rfc3339-validator         0.1.4
rfc3986-validator         0.1.1
rich                      13.7.1
rpds-py                   0.20.0
scikit-learn              1.5.0
scipy                     1.13.1
Send2Trash                1.8.3
setuptools                70.1.1
simple_parsing            0.1.5
six                       1.16.0
sniffio                   1.3.1
soupsieve                 2.5
stack-data                0.6.3
sympy                     1.13.1
tensorboard               2.17.0
tensorboard-data-server   0.7.2
tensorflow                2.17.0
tensorflow-datasets       4.9.6
tensorflow-estimator      2.15.0
tensorflow-metadata       1.15.0
tensorrt                  10.1.0
tensorrt-dispatch         10.1.0
tensorrt-lean             10.1.0
termcolor                 2.4.0
terminado                 0.18.1
threadpoolctl             3.5.0
tinycss2                  1.3.0
toml                      0.10.2
tornado                   6.4.1
tqdm                      4.66.4
traitlets                 5.14.3
types-python-dateutil     2.9.0.20240316
typing_extensions         4.12.2
tzdata                    2024.1
uri-template              1.3.0
urllib3                   2.2.2
wcwidth                   0.2.13
webcolors                 24.8.0
webencodings              0.5.1
websocket-client          1.8.0
Werkzeug                  3.0.3
wheel                     0.43.0
widgetsnbextension        4.0.11
wrapt                     1.16.0
zipp                      3.19.2


environment details, yet it doesn't utilize gpu and show some warnings while importing as follows:

2024-08-15 12:14:46.166633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-15 12:14:46.395008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-15 12:14:46.466695: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-15 12:14:46.494153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-15 12:14:46.612817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-15 12:14:47.395056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

tilakrayal (Assginee) on (2024-09-02 09:30:22 UTC): @asp616848,
Apologies for the delay. I suspect there was an issue with the tensorflow v2.16, could you please try to upgrade the tensorflow v2.17 and check if you are facing the same issue. Also could you try **bazel clean --expunge** followed by bazel sync.

Thank you!

github-actions[bot] on (2024-09-10 01:59:11 UTC): This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.

github-actions[bot] on (2024-09-18 01:59:08 UTC): This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.

google-ml-butler[bot] on (2024-09-18 01:59:11 UTC): Are you satisfied with the resolution of your issue?
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">Yes</a>
<a href=""https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/70603"">No</a>

"
2380234430,issue,open,,Kubernetes cluster resolver fails when running from within a K8S cluster.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If trying to create a cluster spec from a pod running within a K8s cluster, [this](https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py#L90) try block fails because it can't find the kubectl config file.

The quick is rather straightforward:

```python

if override_client is None:
    try:
      from kubernetes import config as k8sconfig  # pylint: disable=g-import-not-at-top

      k8sconfig.load_kube_config()
    except ImportError:
      if not override_client:
        raise ImportError('The Kubernetes Python client must be installed '
                          'before using the Kubernetes Cluster Resolver. '
                          'To install the Kubernetes Python client, run '
                          '`pip install kubernetes` on your command line.')

...


```

Happy to open a MR for this.

### Standalone code to reproduce the issue

main.py
```python
import os
import tensorflow as tf

from absl import logging
from kubernetes import client, config

logging.set_verbosity(logging.DEBUG)
logging.info(""TF version: %s"", tf.__version__)

config.load_incluster_config()
k8s_cli = client.CoreV1Api()

# Fails here despite providing an override client for talking with the k8s APIs.
cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
    {""worker"": [""job-name=mobileye-0"", ""job-name=mobileye-1""]}, override_client=k8s_cli
)
task_index = int(os.environ.get(""TASK_INDEX""))
cluster_resolver.task_type = ""worker""
cluster_resolver.task_id = task_index

logging.info(""Cluster spec: %s"", cluster_resolver.cluster_spec().as_dict())
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    cluster_resolver=cluster_resolver
)
```

job.yaml
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-0
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""0""
      restartPolicy: Never
  parallelism: 1
---
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-1
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""1""
      restartPolicy: Never
  parallelism: 1
```
```


### Relevant log output

```shell
2024-06-28 13:23:10.980 CEST
2024-06-28 11:23:10.979832: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:10.991 CEST
2024-06-28 11:23:10.991166: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:11.109 CEST
2024-06-28 11:23:11.108950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2024-06-28 13:23:11.109 CEST
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-28 13:23:14.057 CEST
2024-06-28 11:23:14.056964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-28 13:23:18.311 CEST
INFO:absl:TF version: 2.16.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PATH: /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2024-06-28 13:23:18.312 CEST
INFO:absl:HOSTNAME: mobileye-0-v7lkr
2024-06-28 13:23:18.312 CEST
INFO:absl:LANG: C.UTF-8
2024-06-28 13:23:18.312 CEST
INFO:absl:GPG_KEY: A035C8C19219BA821ECEA86B64E628F8D684696D
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_VERSION: 3.11.8
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_PIP_VERSION: 24.0
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_SETUPTOOLS_VERSION: 65.5.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_URL: https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_SHA256: dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9
2024-06-28 13:23:18.312 CEST
INFO:absl:TASK_INDEX: 0
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT_HTTPS: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PROTO: tcp
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_ADDR: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_HOST: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:HOME: /root
2024-06-28 13:23:18.313 CEST
INFO:absl:TF2_BEHAVIOR: 1
2024-06-28 13:23:18.313 CEST
INFO:absl:TPU_ML_PLATFORM: Tensorflow
2024-06-28 13:23:18.315 CEST
Traceback (most recent call last):   File ""//src/main.py"", line 20, in <module>     cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
2024-06-28 13:23:18.316 CEST
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.316 CEST
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py"", line 93, in __init__
2024-06-28 13:23:18.317 CEST
    k8sconfig.load_kube_config()
2024-06-28 13:23:18.317 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 819, in load_kube_config
2024-06-28 13:23:18.318 CEST
    loader = _get_kube_config_loader(
2024-06-28 13:23:18.318 CEST
             ^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.318 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 776, in _get_kube_config_loader
2024-06-28 13:23:18.319 CEST
    raise ConfigException(
2024-06-28 13:23:18.319 CEST
kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
```
",msteiner-google,2024-06-28 11:41:32+00:00,['Venkat6871'],2024-07-01 07:15:43+00:00,,https://github.com/tensorflow/tensorflow/issues/70581,"[('stat:awaiting tensorflower', 'Status  - Awaiting response from tensorflower'), ('type:bug', 'Bug'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('TF 2.16', '')]",[],
