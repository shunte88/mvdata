id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2787979304,pull_request,closed,,Move various headers to `xla/tsl`,"Move various headers to `xla/tsl`

Moves
- byte_order.h
- crash_analysis.h
- dynamic_annotations.h
- grpc_credentials.h
- intrusive_ptr.h
- prefetch.h
- ram_file_system.h
- resource.h
- resource_loader.h
- rocm_rocdl_path.h
- stack_frame.h
",copybara-service[bot],2025-01-14 18:17:56+00:00,['ddunl'],2025-01-15 17:35:42+00:00,2025-01-15 17:35:41+00:00,https://github.com/tensorflow/tensorflow/pull/84875,[],[],
2787940490,pull_request,open,,[oneDNN] Add Infer after last allow to be added to Allow list,"In the current implementation of auto-mixed precision pass for FP16_CPU, the infer node is added to allow set only if both its upstream and downstream nodes are in the allow list. This could cause cast node being inserted in between fuse-able nodes. To address this problem, a sub-pass is added to place an infer node to allow set if its direct upstream is in allow set. This feature was enabled for BF16, and now is being enabled for FP16_CPU to keep the bf16/fp16 optimizations same on CPU.

eg: if we have this sequence
MatMul -> BiasAdd -> Relu -> Identity
Then for FP16_CPU with current implementation, Cast op will be inserted between MM & BiasAdd and the fusion won't happen
MatMul (FP16_CPU) -> Cast(Fp16 ->FP32) -> BiasAdd -> Relu -> Identity
With implementation in this PR, Cast op will be inserted after the Infer nodes so that the last fusion is possible
MatMul -> BiasAdd -> Relu -> Cast (FP16_CPU -> FP32) -> Identity
FusedMatMul -> Cast -> Identity",gaurides,2025-01-14 17:59:57+00:00,['gbaned'],2025-01-20 05:02:10+00:00,,https://github.com/tensorflow/tensorflow/pull/84874,"[('awaiting review', 'Pull request awaiting review'), ('size:M', 'CL Change Size: Medium'), ('comp:core', 'issues related to core part of tensorflow')]",[],
2787900063,pull_request,closed,,"Updates the Evaluator to calculate a maximum memory lower bound (i.e., the amount of memory that even the ""smallest"" possible solution is guaranteed to consume).","Updates the Evaluator to calculate a maximum memory lower bound (i.e., the amount of memory that even the ""smallest"" possible solution is guaranteed to consume).
",copybara-service[bot],2025-01-14 17:42:43+00:00,[],2025-01-14 19:46:10+00:00,2025-01-14 19:46:07+00:00,https://github.com/tensorflow/tensorflow/pull/84873,[],[],
2787853563,pull_request,closed,,Reverts fd41705e0ad7a123a9d01b8be2a3b34b3266493e,"Reverts fd41705e0ad7a123a9d01b8be2a3b34b3266493e
",copybara-service[bot],2025-01-14 17:27:02+00:00,[],2025-01-14 18:06:58+00:00,2025-01-14 18:06:57+00:00,https://github.com/tensorflow/tensorflow/pull/84872,[],[],
2787768343,pull_request,closed,,[XLA:GPU] drop unused gpu_emitter in ir_emitter_unnested,"[XLA:GPU] drop unused gpu_emitter in ir_emitter_unnested
",copybara-service[bot],2025-01-14 16:56:06+00:00,['metaflow'],2025-01-15 10:53:20+00:00,2025-01-15 10:53:20+00:00,https://github.com/tensorflow/tensorflow/pull/84871,[],[],
2787764441,pull_request,closed,,[XLA:GPU] Delete GPU IrEmitter::HandleFusion,"[XLA:GPU] Delete GPU IrEmitter::HandleFusion

it is not used anywhere, and we can remove a use of GpuElementalIrEmitter
",copybara-service[bot],2025-01-14 16:54:44+00:00,['metaflow'],2025-01-15 14:49:00+00:00,2025-01-15 14:49:00+00:00,https://github.com/tensorflow/tensorflow/pull/84870,[],[],
2787660709,pull_request,open,,Integrate LLVM at llvm/llvm-project@19032bfe87fa,"Integrate LLVM at llvm/llvm-project@19032bfe87fa

Updates LLVM usage to match
[19032bfe87fa](https://github.com/llvm/llvm-project/commit/19032bfe87fa)
",copybara-service[bot],2025-01-14 16:21:23+00:00,[],2025-01-14 16:21:23+00:00,,https://github.com/tensorflow/tensorflow/pull/84869,[],[],
2787640213,pull_request,open,,[WIP] Remove the need to run the default scheduler before LHS.,"[WIP] Remove the need to run the default scheduler before LHS.
",copybara-service[bot],2025-01-14 16:15:27+00:00,[],2025-01-14 16:15:27+00:00,,https://github.com/tensorflow/tensorflow/pull/84868,[],[],
2787248311,pull_request,closed,,Give meaningful names to HLO modules in `triton_fusion_emitter_int4_device_test.cc`.,"Give meaningful names to HLO modules in `triton_fusion_emitter_int4_device_test.cc`.

This will make it easier to identify failing tests among the dumps.
",copybara-service[bot],2025-01-14 14:12:42+00:00,[],2025-01-14 15:35:26+00:00,2025-01-14 15:35:25+00:00,https://github.com/tensorflow/tensorflow/pull/84867,[],[],
2787205042,pull_request,closed,,[XLA:GPU] Set --set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default.,"[XLA:GPU] Set --set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default.

If your model started to OOM as a result of this, do this in order of preference.
1. (most preferred) -- Layout (sub-byte typed) weights of your model in such a way that the contracting (K) dimension of dots they are used in is minor.
2. Use AUTO layout (from JAX side)
3. Set --set_xla_gpu_experimental_pack_dot_operands_along_k_dimension=false
",copybara-service[bot],2025-01-14 13:58:19+00:00,[],2025-01-20 11:42:07+00:00,2025-01-20 11:42:05+00:00,https://github.com/tensorflow/tensorflow/pull/84866,[],[],
2787173412,pull_request,closed,,[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.,"[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.
",copybara-service[bot],2025-01-14 13:45:59+00:00,[],2025-01-14 18:54:23+00:00,2025-01-14 18:54:23+00:00,https://github.com/tensorflow/tensorflow/pull/84865,[],[],
2787071640,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 12:59:39+00:00,[],2025-01-14 12:59:39+00:00,,https://github.com/tensorflow/tensorflow/pull/84861,[],[],
2787021826,pull_request,closed,,Update code links in documentation.,"Update code links in documentation.
",copybara-service[bot],2025-01-14 12:33:33+00:00,['akuegel'],2025-01-14 13:15:28+00:00,2025-01-14 13:15:27+00:00,https://github.com/tensorflow/tensorflow/pull/84859,[],[],
2786953913,pull_request,closed,,[XLA] do not log warnings from hlo pass,"[XLA] do not log warnings from hlo pass
",copybara-service[bot],2025-01-14 11:57:47+00:00,['metaflow'],2025-01-14 12:40:52+00:00,2025-01-14 12:40:51+00:00,https://github.com/tensorflow/tensorflow/pull/84855,[],[],
2786939589,pull_request,closed,,Move triton codegen to xla/backends/gpu/codegen/triton,"Move triton codegen to xla/backends/gpu/codegen/triton
",copybara-service[bot],2025-01-14 11:50:10+00:00,['akuegel'],2025-01-14 16:27:06+00:00,2025-01-14 16:27:05+00:00,https://github.com/tensorflow/tensorflow/pull/84854,[],[],
2786904681,pull_request,closed,,[XLA:GPU] Require packed dot operands to be packed along contracting dimension.,"[XLA:GPU] Require packed dot operands to be packed along contracting dimension.

For now, only do that if `--xla_gpu_experimental_pack_dot_operands_along_k_dimension` is set.
",copybara-service[bot],2025-01-14 11:31:19+00:00,[],2025-01-14 14:45:15+00:00,2025-01-14 14:45:15+00:00,https://github.com/tensorflow/tensorflow/pull/84851,[],[],
2786727882,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 10:03:01+00:00,[],2025-01-14 13:47:51+00:00,,https://github.com/tensorflow/tensorflow/pull/84845,[],[],
2786726051,pull_request,closed,,[XLA:GPU] IWYU in hlo_pass_fix.h NFC,"[XLA:GPU] IWYU in hlo_pass_fix.h NFC
",copybara-service[bot],2025-01-14 10:02:06+00:00,['metaflow'],2025-01-15 13:11:49+00:00,2025-01-15 13:11:48+00:00,https://github.com/tensorflow/tensorflow/pull/84844,[],[],
2786721985,pull_request,closed,,Qualcomm AI Engine Direct - Add dispatch options for QC,"Summary:
- Add htp runtime options
- Add log level settings
- Fix event BUILD",jiunkaiy,2025-01-14 10:00:09+00:00,['gbaned'],2025-01-14 10:06:43+00:00,2025-01-14 10:06:43+00:00,https://github.com/tensorflow/tensorflow/pull/84843,"[('size:L', 'CL Change Size: Large')]",[],
2786714847,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:56:36+00:00,[],2025-01-14 09:56:36+00:00,,https://github.com/tensorflow/tensorflow/pull/84842,[],[],
2786711039,pull_request,closed,,[XLA] typo and comment formatting NFC,"[XLA] typo and comment formatting NFC
",copybara-service[bot],2025-01-14 09:54:45+00:00,['metaflow'],2025-01-15 09:50:29+00:00,2025-01-15 09:50:28+00:00,https://github.com/tensorflow/tensorflow/pull/84841,[],[],
2786633455,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:16:35+00:00,[],2025-01-14 09:16:35+00:00,,https://github.com/tensorflow/tensorflow/pull/84837,[],[],
2786633078,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:16:24+00:00,[],2025-01-15 05:01:09+00:00,2025-01-15 05:01:08+00:00,https://github.com/tensorflow/tensorflow/pull/84836,[],[],
2786632570,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:16:08+00:00,[],2025-01-14 09:16:08+00:00,,https://github.com/tensorflow/tensorflow/pull/84835,[],[],
2786632514,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:16:06+00:00,[],2025-01-14 10:57:57+00:00,,https://github.com/tensorflow/tensorflow/pull/84834,[],[],
2786628554,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:14:10+00:00,[],2025-01-17 07:52:04+00:00,2025-01-17 07:52:03+00:00,https://github.com/tensorflow/tensorflow/pull/84833,[],[],
2786625337,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 09:12:31+00:00,[],2025-01-15 05:10:42+00:00,2025-01-15 05:10:41+00:00,https://github.com/tensorflow/tensorflow/pull/84832,[],[],
2786554945,pull_request,open,,PR #21223: [nfc] Cleanup build files for expander transforms,"PR #21223: [nfc] Cleanup build files for expander transforms

Imported from GitHub PR https://github.com/openxla/xla/pull/21223

This is part-3 of #18785 and #20595.

Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in #18785.
Copybara import of the project:

--
7832f8483091b70214d17789c05c38959d67a515 by Shraiysh Vaishay <svaishay@nvidia.com>:

[nfc] Cleanup build files for expander transforms

This is part-3 of #18785 and #20595.

Motivation: Smaller build files, fewer merge conflicts, and convinience
in development (more intuitive targets). For discussion surrounding
this, check thread in #18785.

Merging this change closes #21223

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515
",copybara-service[bot],2025-01-14 08:36:44+00:00,[],2025-01-14 08:36:44+00:00,,https://github.com/tensorflow/tensorflow/pull/84828,[],[],
2786554657,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515
",copybara-service[bot],2025-01-14 08:36:33+00:00,[],2025-01-14 08:36:33+00:00,,https://github.com/tensorflow/tensorflow/pull/84827,[],[],
2786493162,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 08:00:11+00:00,[],2025-01-14 08:00:11+00:00,,https://github.com/tensorflow/tensorflow/pull/84824,[],[],
2786485427,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 07:55:05+00:00,[],2025-01-15 05:45:59+00:00,2025-01-15 05:45:58+00:00,https://github.com/tensorflow/tensorflow/pull/84822,[],[],
2786484020,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 07:54:08+00:00,[],2025-01-14 07:54:08+00:00,,https://github.com/tensorflow/tensorflow/pull/84821,[],[],
2786483201,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 07:53:36+00:00,[],2025-01-14 07:53:36+00:00,,https://github.com/tensorflow/tensorflow/pull/84820,[],[],
2786482813,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 07:53:21+00:00,[],2025-01-14 07:53:21+00:00,,https://github.com/tensorflow/tensorflow/pull/84819,[],[],
2786449490,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 07:30:52+00:00,[],2025-01-15 08:57:53+00:00,2025-01-15 08:57:51+00:00,https://github.com/tensorflow/tensorflow/pull/84817,[],[],
2786370501,pull_request,open,,PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR #21375: [ds-fusion] Get While loop analysis with copy fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/21375

In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.
Copybara import of the project:

--
3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay <svaishay@nvidia.com>:

[ds-fusion] Get While loop analysis with copy fusion

In later stages of optimization, there are instances of copy fusion on
the parameter of the while body. With this, we need to allow inlining of
fusions while getting the induction variable index, otherwise we cannot
deduce the tuple index.

--
a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay <svaishay@nvidia.com>:

Relay control dependencies while converting fusion to call (extractor)

Merging this change closes #21375

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8
",copybara-service[bot],2025-01-14 06:30:41+00:00,[],2025-01-27 14:11:19+00:00,,https://github.com/tensorflow/tensorflow/pull/84815,[],[],
2786277270,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-14 05:07:47+00:00,[],2025-01-14 05:07:47+00:00,,https://github.com/tensorflow/tensorflow/pull/84814,[],[],
2786192432,pull_request,open,,Enable support for DenseResourceElementsAttr in TFL Op folders.,"Enable support for DenseResourceElementsAttr in TFL Op folders.
",copybara-service[bot],2025-01-14 04:08:26+00:00,['vamsimanchala'],2025-01-14 18:03:17+00:00,,https://github.com/tensorflow/tensorflow/pull/84813,[],[],
2786190927,pull_request,closed,,Internal only change for instrumentation,"Internal only change for instrumentation
",copybara-service[bot],2025-01-14 04:06:49+00:00,[],2025-01-15 07:18:25+00:00,2025-01-15 07:18:25+00:00,https://github.com/tensorflow/tensorflow/pull/84812,[],[],
2786050020,pull_request,closed,,PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose,"PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose

Imported from GitHub PR https://github.com/openxla/xla/pull/21104

Transpose folding pass doesn't preserve backend config when creating the new dot with transpose folded. Changing the behavior to copy the old dot's config to the new dot.
Copybara import of the project:

--
d2d6b628af1cab777a210e4ac62184e52fe9f4a9 by TJ Xu <tjx@nvidia.com>:

Preserve backend config when folding transpose

--
6b5fa3a1cb70a790803e3ac57ff8329690e88e5e by TJ Xu <tjx@nvidia.com>:

use SetupDerivedInstruction instead of just copying the backend config

Merging this change closes #21104

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21104 from Tixxx:tixxx/transpose_folding 6b5fa3a1cb70a790803e3ac57ff8329690e88e5e
",copybara-service[bot],2025-01-14 02:30:34+00:00,[],2025-01-14 04:05:09+00:00,2025-01-14 04:05:09+00:00,https://github.com/tensorflow/tensorflow/pull/84811,[],[],
2786029931,pull_request,closed,,Moving the logic of making a copy for rhs from `PartitionDot` to `HandleDotHelper`.,"Moving the logic of making a copy for rhs from `PartitionDot` to `HandleDotHelper`.

`HandleDotHelper` is called once for a single dot operation, while `PartitionDot` can be called many times. We need to consider adding a copy only once.
",copybara-service[bot],2025-01-14 02:21:37+00:00,[],2025-01-14 02:57:42+00:00,2025-01-14 02:57:41+00:00,https://github.com/tensorflow/tensorflow/pull/84810,[],[],
2785966480,pull_request,closed,,PR #21380: Add F4E2M1FN and F8E8M0FNU types,"PR #21380: Add F4E2M1FN and F8E8M0FNU types

Imported from GitHub PR https://github.com/openxla/xla/pull/21380

Previous PR https://github.com/openxla/xla/pull/19096 was rolled back, re-trying.

This PR adds F4E2M1FN primitive type (4-bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8-bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented.

This will enable using microscaling (MX) formats ([RFC](https://github.com/openxla/xla/discussions/18085)), such as MXFP4.

```c
F4E2M1FN
- Exponent bias: 1
- Maximum stored exponent value: 3 (binary 11)
- Maximum unbiased exponent value: 3 - 1 = 2
- Minimum stored exponent value: 1 (binary 01)
- Minimum unbiased exponent value: 1 − 1 = 0
- Has Positive and Negative zero
- Doesn't have infinity
- Doesn't have NaNs

Additional details:
- Zeros (+/-): S.00.0
- Max normal number: S.11.1 = ±2^(2) x (1 + 0.5) = ±6.0
- Min normal number: S.01.0 = ±2^(0) = ±1.0
- Min subnormal number: S.00.1 = ±2^(0) x 0.5 = ±0.5

F8E8M0FNU
- Exponent bias: 127
- Maximum stored exponent value: 254 (binary 1111'1110)
- Maximum unbiased exponent value: 254 - 127 = 127
- Minimum stored exponent value: 0 (binary 0000'0000)
- Minimum unbiased exponent value: 0 − 127 = -127
- Doesn't have zero
- Doesn't have infinity
- NaN is encoded as binary 1111'1111

Additional details:
- Zeros cannot be represented
- Negative values cannot be represented
- Mantissa is always 1
```

Related PRs:
- https://github.com/openxla/stablehlo/pull/2582
- https://github.com/jax-ml/ml_dtypes/pull/181
- https://github.com/llvm/llvm-project/pull/95392
- https://github.com/llvm/llvm-project/pull/108877
- https://github.com/jax-ml/ml_dtypes/pull/166
- https://github.com/llvm/llvm-project/pull/107127
- https://github.com/llvm/llvm-project/pull/111028
Copybara import of the project:

--
d7e00c49a4b4f26c06266d6bb941275e67464c01 by Sergey Kozub <skozub@nvidia.com>:

Add F4E2M1FN and F8E8M0FNU types

Merging this change closes #21380

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21380 from openxla:skozub/e2m1_e8m0 d7e00c49a4b4f26c06266d6bb941275e67464c01
",copybara-service[bot],2025-01-14 01:51:38+00:00,[],2025-01-14 18:34:59+00:00,2025-01-14 18:34:59+00:00,https://github.com/tensorflow/tensorflow/pull/84809,[],[],
2785958979,pull_request,open,,Fix the converter input reshaping failure when the XLA call chain has a PartitionedCall instead of StatefulPartitionedCall,"Fix the converter input reshaping failure when the XLA call chain has a PartitionedCall instead of StatefulPartitionedCall
",copybara-service[bot],2025-01-14 01:48:19+00:00,[],2025-01-14 01:48:19+00:00,,https://github.com/tensorflow/tensorflow/pull/84808,[],[],
2785895009,pull_request,closed,,Remove TFL Interpreter deprecation notice from LiteRT Interpreter.,"Remove TFL Interpreter deprecation notice from LiteRT Interpreter.
",copybara-service[bot],2025-01-14 01:15:26+00:00,['pak-laura'],2025-01-15 19:14:16+00:00,2025-01-15 19:14:15+00:00,https://github.com/tensorflow/tensorflow/pull/84807,[],[],
2785813171,pull_request,closed,,Fix a typo in the TFG dialect's GraphFuncOp::getCalledFunction,"Fix a typo in the TFG dialect's GraphFuncOp::getCalledFunction
",copybara-service[bot],2025-01-14 00:31:51+00:00,[],2025-01-14 19:26:02+00:00,2025-01-14 19:25:57+00:00,https://github.com/tensorflow/tensorflow/pull/84806,[],[],
2785722042,pull_request,closed,,Create copy if the operands of gather/scatter instructions overlap.,"Create copy if the operands of gather/scatter instructions overlap.

A gather has two operands, input and indices. If they point to the same instruction, create a copy for indices.

A scatter has n inputs, 1 indices, and n updates (2n+1 operands in total). We allow overlap between n inputs. We also allow overlap between n updates. We need to create a copy if
* indices overlap with any input or update
* update overlap with any input

The added copy will be removed if it is redundant in the following memory related passes (e.g., CopyInsertion).
",copybara-service[bot],2025-01-13 23:26:00+00:00,[],2025-01-14 01:57:05+00:00,2025-01-14 01:57:05+00:00,https://github.com/tensorflow/tensorflow/pull/84805,[],[],
2785705941,pull_request,closed,,Breaking internal tests,"Breaking internal tests

Reverts 5e78ccd69eca07a84d315e9384f2d69c8e23e26c
",copybara-service[bot],2025-01-13 23:19:48+00:00,[],2025-01-14 01:31:27+00:00,2025-01-14 01:31:27+00:00,https://github.com/tensorflow/tensorflow/pull/84804,[],[],
2785675512,pull_request,open,,In progress experimention. Add StringDType to JAX's supported types.,"In progress experimention. Add StringDType to JAX's supported types.
",copybara-service[bot],2025-01-13 23:04:47+00:00,[],2025-01-15 22:49:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84803,[],[],
2785660707,pull_request,closed,,Make dot thunk capable of running without a thread pool.,"Make dot thunk capable of running without a thread pool.
",copybara-service[bot],2025-01-13 22:57:41+00:00,[],2025-01-14 01:06:50+00:00,2025-01-14 01:06:49+00:00,https://github.com/tensorflow/tensorflow/pull/84802,[],[],
2785638555,pull_request,closed,,Extracts a util function `MakeACopyAndReturnItsPartitionedHlo` from dot_handler.cc.,"Extracts a util function `MakeACopyAndReturnItsPartitionedHlo` from dot_handler.cc.

This function creates a copy for the HloInstruction in the given PartitionedHlo and returns a new PartitionedHlo for the copy. This can be reused by other operators (like gather/scatter).
",copybara-service[bot],2025-01-13 22:45:46+00:00,[],2025-01-14 00:41:28+00:00,2025-01-14 00:41:27+00:00,https://github.com/tensorflow/tensorflow/pull/84801,[],[],
2785595352,pull_request,closed,,Disable failed test.,"Disable failed test.
",copybara-service[bot],2025-01-13 22:25:52+00:00,['rtg0795'],2025-01-14 00:32:49+00:00,2025-01-14 00:32:49+00:00,https://github.com/tensorflow/tensorflow/pull/84800,[],[],
2785536166,pull_request,closed,,Format one constraint for stablehlo.scatter operation.,"Format one constraint for stablehlo.scatter operation.
",copybara-service[bot],2025-01-13 21:57:29+00:00,[],2025-01-13 22:39:10+00:00,2025-01-13 22:39:07+00:00,https://github.com/tensorflow/tensorflow/pull/84799,[],[],
2785517353,pull_request,closed,,add PJRT runtime profiling library,"add PJRT runtime profiling library
",copybara-service[bot],2025-01-13 21:49:57+00:00,[],2025-01-28 23:55:51+00:00,2025-01-28 23:55:50+00:00,https://github.com/tensorflow/tensorflow/pull/84798,[],[],
2785509985,pull_request,closed,,Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`,"Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`
",copybara-service[bot],2025-01-13 21:46:32+00:00,['ddunl'],2025-01-14 15:55:10+00:00,2025-01-14 15:55:09+00:00,https://github.com/tensorflow/tensorflow/pull/84797,[],[],
2785476843,pull_request,closed,,[xla:gpu] Move XLA:GPU runtime to xla/backends/gpu,"[xla:gpu] Move XLA:GPU runtime to xla/backends/gpu
",copybara-service[bot],2025-01-13 21:32:50+00:00,['ezhulenev'],2025-01-14 01:22:37+00:00,2025-01-14 01:22:37+00:00,https://github.com/tensorflow/tensorflow/pull/84796,[],[],
2785423913,pull_request,closed,,Pass stablehlo-ext-prepare-for-hlo-export : Migrate from MHLO to StableHLO,"Pass stablehlo-ext-prepare-for-hlo-export : Migrate from MHLO to StableHLO
",copybara-service[bot],2025-01-13 21:12:32+00:00,[],2025-01-16 23:24:27+00:00,2025-01-16 23:24:27+00:00,https://github.com/tensorflow/tensorflow/pull/84795,[],[],
2785403576,pull_request,closed,,"In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRt-IFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.","In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRt-IFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.
",copybara-service[bot],2025-01-13 21:04:41+00:00,[],2025-01-13 22:50:57+00:00,2025-01-13 22:50:55+00:00,https://github.com/tensorflow/tensorflow/pull/84794,[],[],
2785306759,pull_request,closed,,Use buffer offset when the mlir module size is greater than 2GB,"Use buffer offset when the mlir module size is greater than 2GB

flatbuffer_export.cc currently implements a brute force approach to decide
whether to use buffer offset or not, this is expensive on HWM usage. We can
do better by analyzing the module size beforehand.
",copybara-service[bot],2025-01-13 20:25:14+00:00,['vamsimanchala'],2025-01-15 01:42:29+00:00,2025-01-15 01:42:29+00:00,https://github.com/tensorflow/tensorflow/pull/84793,[],[],
2785293042,pull_request,closed,,Decrease Linux CPU wheel limit size to 260M.,"Decrease Linux CPU wheel limit size to 260M.
",copybara-service[bot],2025-01-13 20:19:20+00:00,[],2025-01-13 21:15:55+00:00,2025-01-13 21:15:54+00:00,https://github.com/tensorflow/tensorflow/pull/84792,[],[],
2785278352,pull_request,closed,,PR #21223: [nfc] Cleanup build files for expander transforms,"PR #21223: [nfc] Cleanup build files for expander transforms

Imported from GitHub PR https://github.com/openxla/xla/pull/21223

This is part-3 of #18785 and #20595.

Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in #18785.
Copybara import of the project:

--
7832f8483091b70214d17789c05c38959d67a515 by Shraiysh Vaishay <svaishay@nvidia.com>:

[nfc] Cleanup build files for expander transforms

This is part-3 of #18785 and #20595.

Motivation: Smaller build files, fewer merge conflicts, and convinience
in development (more intuitive targets). For discussion surrounding
this, check thread in #18785.

Merging this change closes #21223

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515
",copybara-service[bot],2025-01-13 20:14:08+00:00,[],2025-01-14 08:37:45+00:00,2025-01-14 08:37:45+00:00,https://github.com/tensorflow/tensorflow/pull/84791,[],[],
2785276249,pull_request,closed,,Increase wheel limit size up to 270M for a temporary nightlies fix.,"Increase wheel limit size up to 270M for a temporary nightlies fix.
",copybara-service[bot],2025-01-13 20:13:29+00:00,['rtg0795'],2025-01-13 21:07:25+00:00,2025-01-13 21:07:24+00:00,https://github.com/tensorflow/tensorflow/pull/84790,[],[],
2785251759,pull_request,open,,Generate a build identifier for external tensor rearragement files,"Generate a build identifier for external tensor rearragement files
",copybara-service[bot],2025-01-13 20:05:53+00:00,['tf-marissaw'],2025-01-15 21:52:46+00:00,,https://github.com/tensorflow/tensorflow/pull/84789,[],[],
2785233032,pull_request,closed,,[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers,"[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers
",copybara-service[bot],2025-01-13 20:00:11+00:00,['ezhulenev'],2025-01-13 20:36:30+00:00,2025-01-13 20:36:29+00:00,https://github.com/tensorflow/tensorflow/pull/84788,[],[],
2785220404,pull_request,open,,Integrate LLVM at llvm/llvm-project@7aebacbee965,"Integrate LLVM at llvm/llvm-project@7aebacbee965

Updates LLVM usage to match
[7aebacbee965](https://github.com/llvm/llvm-project/commit/7aebacbee965)
",copybara-service[bot],2025-01-13 19:56:12+00:00,[],2025-01-13 19:56:12+00:00,,https://github.com/tensorflow/tensorflow/pull/84787,[],[],
2785190663,pull_request,closed,,PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR #21375: [ds-fusion] Get While loop analysis with copy fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/21375

In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.
Copybara import of the project:

--
ae85690876a106c4d74715fed299779e29e8e641 by Shraiysh Vaishay <svaishay@nvidia.com>:

[ds-fusion] Get While loop analysis with copy fusion

In later stages of optimization, there are instances of copy fusion on
the parameter of the while body. With this, we need to allow inlining of
fusions while getting the induction variable index, otherwise we cannot
deduce the tuple index.

Merging this change closes #21375

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis ae85690876a106c4d74715fed299779e29e8e641
",copybara-service[bot],2025-01-13 19:47:29+00:00,[],2025-01-13 21:27:04+00:00,2025-01-13 21:27:03+00:00,https://github.com/tensorflow/tensorflow/pull/84786,[],[],
2785146643,pull_request,open,,Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,"Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247
",copybara-service[bot],2025-01-13 19:35:44+00:00,['rtg0795'],2025-01-13 19:35:45+00:00,,https://github.com/tensorflow/tensorflow/pull/84785,[],[],
2785091950,pull_request,closed,,CompiledModel: Refactor lock usage,"CompiledModel: Refactor lock usage

Instead of using misleading ScopedLock, maintain vector of LiteRtTensorBuffer
and absl::Cleanup to unlock explicitly.
",copybara-service[bot],2025-01-13 19:19:10+00:00,['terryheo'],2025-01-15 03:36:03+00:00,2025-01-15 03:36:02+00:00,https://github.com/tensorflow/tensorflow/pull/84784,[],[],
2784908033,pull_request,open,,Regenerate pyi stubs with absl::Span imports included,"Regenerate pyi stubs with absl::Span imports included
",copybara-service[bot],2025-01-13 18:24:35+00:00,[],2025-01-13 18:24:35+00:00,,https://github.com/tensorflow/tensorflow/pull/84782,[],[],
2784876579,pull_request,closed,,Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,"Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247
",copybara-service[bot],2025-01-13 18:16:42+00:00,['rtg0795'],2025-01-13 19:42:48+00:00,2025-01-13 19:42:47+00:00,https://github.com/tensorflow/tensorflow/pull/84781,[],[],
2784853132,pull_request,closed,,Fix an overflow issue in TransposePlan,"Fix an overflow issue in TransposePlan
",copybara-service[bot],2025-01-13 18:10:46+00:00,[],2025-01-14 01:45:46+00:00,2025-01-14 01:45:45+00:00,https://github.com/tensorflow/tensorflow/pull/84780,[],[],
2784785738,pull_request,closed,,[XLA:GPU] Enable vectorization of the indices operand for scatter.,"[XLA:GPU] Enable vectorization of the indices operand for scatter.
",copybara-service[bot],2025-01-13 17:52:56+00:00,['pifon2a'],2025-01-14 12:53:15+00:00,2025-01-14 12:53:15+00:00,https://github.com/tensorflow/tensorflow/pull/84779,[],[],
2784773505,pull_request,closed,,[XLA:CPU] Implement deserialization from proto to thunks,"[XLA:CPU] Implement deserialization from proto to thunks
",copybara-service[bot],2025-01-13 17:48:40+00:00,[],2025-01-17 11:48:32+00:00,2025-01-17 11:48:31+00:00,https://github.com/tensorflow/tensorflow/pull/84778,[],[],
2784742594,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-01-13 17:37:15+00:00,['ddunl'],2025-01-13 20:42:39+00:00,,https://github.com/tensorflow/tensorflow/pull/84777,[],[],
2784735474,pull_request,closed,,Implement CHLO->StableHLO ragged_dot mode 1 decomposition.,"Implement CHLO->StableHLO ragged_dot mode 1 decomposition.
",copybara-service[bot],2025-01-13 17:34:35+00:00,['ghpvnist'],2025-01-16 01:51:30+00:00,2025-01-16 01:51:29+00:00,https://github.com/tensorflow/tensorflow/pull/84776,[],[],
2784723409,pull_request,closed,,[XLA] HLO Pass Unit Testing: Add `RunAndFilecheckHloRewrite` API version that takes HLO module with interleaved // CEHCK lines as an input.,"[XLA] HLO Pass Unit Testing: Add `RunAndFilecheckHloRewrite` API version that takes HLO module with interleaved // CEHCK lines as an input.
",copybara-service[bot],2025-01-13 17:30:17+00:00,[],2025-01-23 03:39:04+00:00,2025-01-23 03:39:03+00:00,https://github.com/tensorflow/tensorflow/pull/84775,[],[],
2784666826,pull_request,closed,,[XLA:Python] Fix bug introduced in https://github.com/openxla/xla/pull/21265,"[XLA:Python] Fix bug introduced in https://github.com/openxla/xla/pull/21265
which meant that we never actually populated C++ pmap cache entries.
",copybara-service[bot],2025-01-13 17:10:35+00:00,[],2025-01-13 18:35:28+00:00,2025-01-13 18:35:27+00:00,https://github.com/tensorflow/tensorflow/pull/84774,[],[],
2784655073,pull_request,closed,,Add the Windows 2022 Dockerfile.,"Add the Windows 2022 Dockerfile.
",copybara-service[bot],2025-01-13 17:06:28+00:00,['belitskiy'],2025-01-13 19:00:06+00:00,2025-01-13 19:00:05+00:00,https://github.com/tensorflow/tensorflow/pull/84773,[],[],
2784644820,pull_request,closed,,Integrate StableHLO at openxla/stablehlo@b2d36c56,"Integrate StableHLO at openxla/stablehlo@b2d36c56
",copybara-service[bot],2025-01-13 17:02:55+00:00,['GleasonK'],2025-01-14 19:16:48+00:00,2025-01-14 19:16:47+00:00,https://github.com/tensorflow/tensorflow/pull/84772,[],[],
2784633792,pull_request,closed,,[XLA:GPU] Fix the unpack dim calculation for I4 rewrite with non major_2_minor layouts,"[XLA:GPU] Fix the unpack dim calculation for I4 rewrite with non major_2_minor layouts
",copybara-service[bot],2025-01-13 16:59:22+00:00,[],2025-01-14 10:07:48+00:00,2025-01-14 10:07:48+00:00,https://github.com/tensorflow/tensorflow/pull/84771,[],[],
2784561063,pull_request,closed,,Fix a bug where creating a TFRT buffer may access an unset byte_strides object when the layout is packed.,"Fix a bug where creating a TFRT buffer may access an unset byte_strides object when the layout is packed.
",copybara-service[bot],2025-01-13 16:37:02+00:00,[],2025-01-13 17:39:10+00:00,2025-01-13 17:39:10+00:00,https://github.com/tensorflow/tensorflow/pull/84770,[],[],
2784518300,pull_request,closed,,Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.,"Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.

Only when coordination_agent_recoverable is set, it tries to reconnect to the cluster and would lead to AlreadyExists error. In this case the already_existing error can be handled by checking the existing topology is same as the new one.
",copybara-service[bot],2025-01-13 16:22:52+00:00,['ishark'],2025-01-17 20:05:57+00:00,2025-01-17 20:05:56+00:00,https://github.com/tensorflow/tensorflow/pull/84769,[],[],
2784509045,pull_request,closed,,Reverts 7869999086e70e24c0d1bda491d80cd28b127866,"Reverts 7869999086e70e24c0d1bda491d80cd28b127866
",copybara-service[bot],2025-01-13 16:19:49+00:00,['ishark'],2025-01-15 20:30:06+00:00,2025-01-15 20:30:05+00:00,https://github.com/tensorflow/tensorflow/pull/84768,[],[],
2784383599,pull_request,closed,,[XLA:GPU] Enable int4 matmul rewriting with Triton MLIR rewriter by default.,"[XLA:GPU] Enable int4 matmul rewriting with Triton MLIR rewriter by default.
",copybara-service[bot],2025-01-13 15:41:23+00:00,[],2025-01-14 13:41:22+00:00,2025-01-14 13:41:21+00:00,https://github.com/tensorflow/tensorflow/pull/84767,[],[],
2784106984,pull_request,closed,,[xla:cpu:nanort] Add microbenchmark for NanoRt IFRT Client,"[xla:cpu:nanort] Add microbenchmark for NanoRt IFRT Client

------------------------------------------------------------
Benchmark                  Time             CPU   Iterations
------------------------------------------------------------
BM_IfRtAddScalars        307 ns          307 ns      2334921

~120 ns out of 307 spent in constructing ifrt::Arrays from host buffers.
",copybara-service[bot],2025-01-13 14:19:17+00:00,['ezhulenev'],2025-01-13 14:55:22+00:00,2025-01-13 14:55:20+00:00,https://github.com/tensorflow/tensorflow/pull/84766,[],[],
2784045724,pull_request,closed,,PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,"PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute

Imported from GitHub PR https://github.com/openxla/xla/pull/19699

This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId.

This PR is another part of breaking up PR #17982.
Copybara import of the project:

--
db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts <chaser@nvidia.com>:

Explicit streams are picked up in stream assignment.

Merging this change closes #19699

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8
",copybara-service[bot],2025-01-13 13:59:17+00:00,[],2025-01-13 15:32:25+00:00,2025-01-13 15:32:25+00:00,https://github.com/tensorflow/tensorflow/pull/84765,[],[],
2784036142,pull_request,closed,,PR #20633: Improve the error message of the host out-of-memory,"PR #20633: Improve the error message of the host out-of-memory

Imported from GitHub PR https://github.com/openxla/xla/pull/20633

When working on weight offloading and activation offloading for MaxText Llama2-7B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit.
Copybara import of the project:

--
e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu <janeliu@nvidia.com>:

Improve the error message of the host out-of-memory.

Merging this change closes #20633

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenying-liu:host-OOM e38fac1d00c13185cbb96972814ddc45b0508cd8
",copybara-service[bot],2025-01-13 13:56:18+00:00,[],2025-01-13 14:26:57+00:00,2025-01-13 14:26:56+00:00,https://github.com/tensorflow/tensorflow/pull/84764,[],[],
2783917915,pull_request,closed,,Move emitters to directory xla/backends/gpu/codegen/emitters.,"Move emitters to directory xla/backends/gpu/codegen/emitters.
",copybara-service[bot],2025-01-13 13:08:11+00:00,['akuegel'],2025-01-13 14:05:53+00:00,2025-01-13 14:05:52+00:00,https://github.com/tensorflow/tensorflow/pull/84763,[],[],
2783863443,pull_request,closed,,[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.,"[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.
",copybara-service[bot],2025-01-13 12:43:38+00:00,[],2025-01-16 15:34:55+00:00,2025-01-16 15:34:54+00:00,https://github.com/tensorflow/tensorflow/pull/84762,[],[],
2783858323,pull_request,closed,,Support loading unoptimized HLO snapshot with arguments.,"Support loading unoptimized HLO snapshot with arguments.

This should allow us to reproduce hlo runs with the same arguments and, specifically, run benchamkrs that are arguments dependent.
",copybara-service[bot],2025-01-13 12:41:17+00:00,[],2025-01-16 14:46:35+00:00,2025-01-16 14:46:34+00:00,https://github.com/tensorflow/tensorflow/pull/84761,[],[],
2783781390,pull_request,closed,,PR #21271: [ds-fusion] Move the resource requests class into a dedicated header,"PR #21271: [ds-fusion] Move the resource requests class into a dedicated header

Imported from GitHub PR https://github.com/openxla/xla/pull/21271

This patch moves the resource requests class outside the gpu_executable. This is required because it will be used for thunk initializations in testing thunks. Also renamed the virtual class to
`ResourceRequestsInterface` to avoid confusion with the concrete class.
Copybara import of the project:

--
fd0c23223388dd455d7302f5a556c8838f6f9e21 by Shraiysh Vaishay <svaishay@nvidia.com>:

[ds-fusion] Move the resource requests class into a dedicated header

This patch moves the resource requests class outside the gpu_executable.
This is required because it will be used for thunk initializations in
testing thunks. Also renamed the virtual class to
`ResourceRequestsInterface` to avoid confusion with the concrete class.

Merging this change closes #21271

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21271 from shraiysh:resource_requests_refactor fd0c23223388dd455d7302f5a556c8838f6f9e21
",copybara-service[bot],2025-01-13 12:07:04+00:00,[],2025-01-13 14:16:37+00:00,2025-01-13 14:16:36+00:00,https://github.com/tensorflow/tensorflow/pull/84759,[],[],
2783774502,pull_request,closed,,Integrate LLVM at llvm/llvm-project@b270525f730b,"Integrate LLVM at llvm/llvm-project@b270525f730b

Updates LLVM usage to match
[b270525f730b](https://github.com/llvm/llvm-project/commit/b270525f730b)
",copybara-service[bot],2025-01-13 12:03:56+00:00,[],2025-01-14 14:25:02+00:00,2025-01-14 14:25:01+00:00,https://github.com/tensorflow/tensorflow/pull/84758,[],[],
2783762387,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 11:58:16+00:00,[],2025-01-14 07:22:58+00:00,2025-01-14 07:22:57+00:00,https://github.com/tensorflow/tensorflow/pull/84756,[],[],
2783740521,pull_request,closed,,[XLA:GPU] Delete no-op logic for constructing collective combiner keys.,"[XLA:GPU] Delete no-op logic for constructing collective combiner keys.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8
",copybara-service[bot],2025-01-13 11:48:10+00:00,[],2025-01-13 15:57:53+00:00,2025-01-13 15:57:52+00:00,https://github.com/tensorflow/tensorflow/pull/84754,[],[],
2783662269,pull_request,closed,,PR #21343: Re-apply PR #21213: [GPU] Fix mutex locking of a cuDNN handle.,"PR #21343: Re-apply PR #21213: [GPU] Fix mutex locking of a cuDNN handle.

Imported from GitHub PR https://github.com/openxla/xla/pull/21343

PR #21213 got accidentally overwritten due to a merge conflict with PR #20965.
Copybara import of the project:

--
f04617f665a6a3c61e76fd4c9e9ebbfb720741bb by Ilia Sergachev <isergachev@nvidia.com>:

Re-apply PR #21213: [GPU] Fix mutex locking of a cuDNN handle.

PR #21213 got accidentally overwritten due to a merge conflict with PR
#20965.

Merging this change closes #21343

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21343 from openxla:fix_cudnn_mutex f04617f665a6a3c61e76fd4c9e9ebbfb720741bb
",copybara-service[bot],2025-01-13 11:10:23+00:00,[],2025-01-13 11:56:23+00:00,2025-01-13 11:56:22+00:00,https://github.com/tensorflow/tensorflow/pull/84753,[],[],
2783658481,pull_request,closed,,PR #20948: [GPU] Fix NCCL with CUDA 12.6.3.,"PR #20948: [GPU] Fix NCCL with CUDA 12.6.3.

Imported from GitHub PR https://github.com/openxla/xla/pull/20948


Copybara import of the project:

--
bc566f263b10c2fffe14e0f8007f60983509e02e by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Fix NCCL with CUDA 12.6.3.

Merging this change closes #20948

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20948 from openxla:fix_nccl_cuda_1263 bc566f263b10c2fffe14e0f8007f60983509e02e
",copybara-service[bot],2025-01-13 11:08:38+00:00,[],2025-01-13 11:48:52+00:00,2025-01-13 11:48:51+00:00,https://github.com/tensorflow/tensorflow/pull/84752,[],[],
2783626059,pull_request,open,,PR #20633: Improve the error message of the host out-of-memory,"PR #20633: Improve the error message of the host out-of-memory

Imported from GitHub PR https://github.com/openxla/xla/pull/20633

When working on weight offloading and activation offloading for MaxText Llama2-7B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit.
Copybara import of the project:

--
e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu <janeliu@nvidia.com>:

Improve the error message of the host out-of-memory.

Merging this change closes #20633

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenying-liu:host-OOM e38fac1d00c13185cbb96972814ddc45b0508cd8
",copybara-service[bot],2025-01-13 10:53:24+00:00,[],2025-01-13 11:47:03+00:00,,https://github.com/tensorflow/tensorflow/pull/84751,[],[],
2783617305,pull_request,closed,,[XLA:GPU] Fix broken build,"[XLA:GPU] Fix broken build
",copybara-service[bot],2025-01-13 10:49:19+00:00,[],2025-01-13 11:19:30+00:00,2025-01-13 11:19:30+00:00,https://github.com/tensorflow/tensorflow/pull/84749,[],[],
2783578350,pull_request,open,,PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,"PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute

Imported from GitHub PR https://github.com/openxla/xla/pull/19699

This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId.

This PR is another part of breaking up PR #17982.
Copybara import of the project:

--
db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts <chaser@nvidia.com>:

Explicit streams are picked up in stream assignment.

Merging this change closes #19699

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8
",copybara-service[bot],2025-01-13 10:31:02+00:00,[],2025-01-13 13:23:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84746,[],[],
2783550867,pull_request,closed,,PR #19462: Add ExplicitStreamAnnotationAsyncWrapper pass,"PR #19462: Add ExplicitStreamAnnotationAsyncWrapper pass

Imported from GitHub PR https://github.com/openxla/xla/pull/19462

This PR introduces a new pass `ExplicitStreamAnnotationAsyncWrapper`. This pass takes `kCall` instructions that are annotated with the frontend attribute `xla_gpu_experimental_stream_annotation`, and wraps the call with an async start-done pair.


Initially, I tried to integrate this with the existing `StreamAttributeAnnotator` and `StreamAttributeAsyncWrapper` passes, but much of that code is specifically just for the `windowedeinsum` logic, and users wont necessarily want both enabled at the same time. Thus, I found it cleaner to instead just make a new pass.
Copybara import of the project:

--
4873de9e28551ace7be0ab7e15f5d6e31e17a6cb by chaserileyroberts <chaser@nvidia.com>:

Added new async wrap pass

Merging this change closes #19462

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19462 from chaserileyroberts:chase/stream_async_wrap 4873de9e28551ace7be0ab7e15f5d6e31e17a6cb
",copybara-service[bot],2025-01-13 10:18:29+00:00,[],2025-01-13 15:04:48+00:00,2025-01-13 15:04:48+00:00,https://github.com/tensorflow/tensorflow/pull/84745,[],[],
2783508534,pull_request,closed,,Rename MlirFusionEmitterBase to EmitterBase and move the code (NFC),"Rename MlirFusionEmitterBase to EmitterBase and move the code (NFC)

The code is moved to xla/backends/gpu/codegen/emitters directory.
",copybara-service[bot],2025-01-13 10:00:13+00:00,['akuegel'],2025-01-13 12:38:00+00:00,2025-01-13 12:37:59+00:00,https://github.com/tensorflow/tensorflow/pull/84744,[],[],
2783481639,pull_request,closed,,PR #21301: Document the xla_gpu_sharded_autotuning flag,"PR #21301: Document the xla_gpu_sharded_autotuning flag

Imported from GitHub PR https://github.com/openxla/xla/pull/21301


Copybara import of the project:

--
54d932d3204174c087be84ed636ede6c9020aae3 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Document the xla_gpu_sharded_autotuning flag

Merging this change closes #21301

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21301 from dimvar:document-shard-autotuning 54d932d3204174c087be84ed636ede6c9020aae3
",copybara-service[bot],2025-01-13 09:48:31+00:00,[],2025-01-13 10:22:38+00:00,2025-01-13 10:22:37+00:00,https://github.com/tensorflow/tensorflow/pull/84742,[],[],
2783455673,pull_request,closed,,Develop upstream sync 20250113,"Weekly sync 20250113
Unmerged paths:
  (use ""git add <file>..."" to mark resolution)
        both modified:   .bazelrc
        both modified:   tensorflow/core/common_runtime/gpu/gpu_device_test.cc
        both modified:   tensorflow/core/kernels/matmul_op_fused.cc
        both modified:   tensorflow/core/kernels/matmul_op_impl.h
        both modified:   tensorflow/core/kernels/matmul_util.cc
        both modified:   tensorflow/core/kernels/matmul_util.h
        both modified:   third_party/gpus/rocm_configure.bzl
        both modified:   third_party/xla/third_party/tsl/third_party/gpus/rocm_configure.bzl
        both modified:   third_party/xla/xla/service/gpu/fusions/triton/dot_algorithms_test.cc
        both modified:   third_party/xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
        both modified:   third_party/xla/xla/tests/BUILD",alekstheod,2025-01-13 09:36:26+00:00,['gbaned'],2025-01-13 09:37:03+00:00,2025-01-13 09:36:58+00:00,https://github.com/tensorflow/tensorflow/pull/84741,"[('size:XL', 'CL Change Size:Extra Large')]",[],
2783445081,pull_request,open,,PR #21301: Document the xla_gpu_sharded_autotuning flag,"PR #21301: Document the xla_gpu_sharded_autotuning flag

Imported from GitHub PR https://github.com/openxla/xla/pull/21301


Copybara import of the project:

--
54d932d3204174c087be84ed636ede6c9020aae3 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Document the xla_gpu_sharded_autotuning flag

Merging this change closes #21301

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21301 from dimvar:document-shard-autotuning 54d932d3204174c087be84ed636ede6c9020aae3
",copybara-service[bot],2025-01-13 09:31:26+00:00,[],2025-01-13 09:31:26+00:00,,https://github.com/tensorflow/tensorflow/pull/84740,[],[],
2783405943,pull_request,closed,,PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,"PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default

Imported from GitHub PR https://github.com/openxla/xla/pull/19451

We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability.
Copybara import of the project:

--
808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu <tjx@nvidia.com>:

Turn xla_gpu_multi_streamed_windowed_einsum on by default

--
8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu <tjx@nvidia.com>:

Add an option to StreamAttributeAnnotator to skip annotating copy-start
and async DUS
Don't annotate copy-start and async DUS when the pass is run before
remat

--
352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu <tjx@nvidia.com>:

Remove the option to skip annotating copy start and inpect if the module
has schedule

--
257ff6768b59fc7c47c04fa5faa524399f74c80e by TJ Xu <tjx@nvidia.com>:

Address roll-back by disabling a2a rewrite by default

--
d3bafebdc0961d61384a49616c29cb9bb6c59db9 by TJ Xu <tjx@nvidia.com>:

reverted new flag changes

Merging this change closes #19451

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag d3bafebdc0961d61384a49616c29cb9bb6c59db9
",copybara-service[bot],2025-01-13 09:13:48+00:00,[],2025-01-13 10:38:18+00:00,2025-01-13 10:38:17+00:00,https://github.com/tensorflow/tensorflow/pull/84738,[],[],
2783288238,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 08:11:09+00:00,[],2025-01-13 08:11:09+00:00,,https://github.com/tensorflow/tensorflow/pull/84730,[],[],
2783216214,pull_request,open,,PR #73149: Example usage is created for tf.sets.size in sets_impl.py,"PR #73149: Example usage is created for tf.sets.size in sets_impl.py

Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/73149


Example is provided for tf.sets.size in sets_impl.py

Thank You
Copybara import of the project:

--
bf84bda7a94609110b6752ef5d6d84801a167575 by LakshmiKalaKadali <149650845+LakshmiKalaKadali@users.noreply.github.com>:

Example usage is created for tf.sets.size in sets_impl.py

Example is provided for tf.sets.size in sets_impl.py

Merging this change closes #73149

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/73149 from tensorflow:LakshmiKalaKadali-patch-4 bf84bda7a94609110b6752ef5d6d84801a167575
",copybara-service[bot],2025-01-13 07:28:09+00:00,[],2025-01-13 07:28:09+00:00,,https://github.com/tensorflow/tensorflow/pull/84722,[],[],
2783209372,pull_request,open,,PR #63959: Typos are fixed in quantization_debugger.ipynb,"PR #63959: Typos are fixed in quantization_debugger.ipynb

Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/63959

Typos and Grammatical errors are fixed
Copybara import of the project:

--
bc8a6549529194af0ed5d5e86d93fe8d0652c10e by LakshmiKalaKadali <149650845+LakshmiKalaKadali@users.noreply.github.com>:

Typos are fixed in quantization_debugger.ipynb

Typos and Grammatical errors are fixed

Merging this change closes #63959

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63959 from tensorflow:LakshmiKalaKadali-patch-3 bc8a6549529194af0ed5d5e86d93fe8d0652c10e
",copybara-service[bot],2025-01-13 07:23:41+00:00,[],2025-01-13 07:23:47+00:00,,https://github.com/tensorflow/tensorflow/pull/84721,[],"[{'comment_id': 2586367654, 'issue_id': 2783209372, 'author': 'review-notebook-app[bot]', 'body': 'Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/tensorflow/tensorflow/pull/84721""><img align=""absmiddle""  alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href=\'https://www.reviewnb.com/?utm_source=gh\'>ReviewNB</a></i>', 'created_at': datetime.datetime(2025, 1, 13, 7, 23, 46, tzinfo=datetime.timezone.utc)}]","review-notebook-app[bot] on (2025-01-13 07:23:46 UTC): Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/tensorflow/tensorflow/pull/84721""><img align=""absmiddle""  alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> 

 See visual diffs & provide feedback on Jupyter Notebooks. 

---

 <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>

"
2783207174,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 07:22:16+00:00,[],2025-01-14 08:29:34+00:00,2025-01-14 08:29:33+00:00,https://github.com/tensorflow/tensorflow/pull/84719,[],[],
2783169413,pull_request,closed,,PR #20739: Arch specific FP8 SDPA test,"PR #20739: Arch specific FP8 SDPA test

Imported from GitHub PR https://github.com/openxla/xla/pull/20739

The workspace size required by CuDNN are different on Hopper and Blackwell. To make the HLO string arch agnostic, pin the workspace size to 0.
Copybara import of the project:

--
f9eafadfdb69d3e68ba6c186979f65dd40e45000 by shuw <shuw@nvidia.com>:

Fix fp8 SDPA test workspace to 0

Merging this change closes #20739

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20739 from wenscarl:arch_spec_fp8_sdpa_test f9eafadfdb69d3e68ba6c186979f65dd40e45000
",copybara-service[bot],2025-01-13 06:56:35+00:00,[],2025-01-13 09:48:14+00:00,2025-01-13 09:48:13+00:00,https://github.com/tensorflow/tensorflow/pull/84714,[],[],
2783058208,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 05:37:36+00:00,[],2025-01-17 13:23:52+00:00,2025-01-17 13:23:51+00:00,https://github.com/tensorflow/tensorflow/pull/84704,[],[],
2783046233,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 05:25:45+00:00,[],2025-01-14 05:46:06+00:00,2025-01-14 05:46:05+00:00,https://github.com/tensorflow/tensorflow/pull/84703,[],[],
2782976281,pull_request,open,,Fix --cpu=k8 mapping,"Fix --cpu=k8 mapping
",copybara-service[bot],2025-01-13 04:17:17+00:00,[],2025-01-13 04:17:17+00:00,,https://github.com/tensorflow/tensorflow/pull/84702,[],[],
2782946395,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 03:45:24+00:00,[],2025-01-13 03:45:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84700,[],[],
2782938533,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 03:39:03+00:00,[],2025-01-13 03:39:03+00:00,,https://github.com/tensorflow/tensorflow/pull/84699,[],[],
2782902176,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-13 03:00:24+00:00,[],2025-01-21 20:59:57+00:00,2025-01-21 20:59:55+00:00,https://github.com/tensorflow/tensorflow/pull/84698,[],[],
2782760808,pull_request,closed,,Reverts d42f44f9c6912bd26a0810f004a4cd8527b9d9fd,"Reverts d42f44f9c6912bd26a0810f004a4cd8527b9d9fd
",copybara-service[bot],2025-01-12 23:50:22+00:00,[],2025-01-15 06:42:18+00:00,2025-01-15 06:42:17+00:00,https://github.com/tensorflow/tensorflow/pull/84697,[],[],
2782734939,pull_request,closed,,enhancement: add sonargit pr metrics,"This PR introduces a GitHub workflow that leverages the [SonarGit Action](https://github.com/sonargit-actions/pr-metrics) to collect pull request metrics such as open times, merge rates, and change failure rates. These metrics provide actionable insights to help improve the repository's development workflow.

Currently, the workflow logs PR information to the console. Optionally, [SonarGit](https://sonargit.com/) offers a dashboard to visualize the data for deeper analysis—and it’s completely free for life!

I’m doing this to help the developer community and to get my name out there. If you’d like more information or assistance in setting this up, feel free to reach out.",axlrommel,2025-01-12 22:41:52+00:00,['gbaned'],2025-01-13 14:06:10+00:00,2025-01-13 14:06:08+00:00,https://github.com/tensorflow/tensorflow/pull/84696,"[('size:S', 'CL Change Size: Small')]",[],
2782615839,pull_request,closed,,[XLA:Python] Add locking around the pytree registry in free threading mode.,"[XLA:Python] Add locking around the pytree registry in free threading mode.

Fixes tsan races from JAX test suite under free threading.
",copybara-service[bot],2025-01-12 18:11:08+00:00,[],2025-01-13 03:26:15+00:00,2025-01-13 03:26:14+00:00,https://github.com/tensorflow/tensorflow/pull/84695,[],[],
2782592904,pull_request,closed,,PR #21265: Attempt to add pmap free-threading support,"PR #21265: Attempt to add pmap free-threading support

Imported from GitHub PR https://github.com/openxla/xla/pull/21265

Description:
- A tentative to add free-threading to pmap_lib
Copybara import of the project:

--
d2f5df9c0decdb7e55a2013f5506dee6fc358298 by vfdev-5 <vfdev.5@gmail.com>:

WIP

Merging this change closes #21265

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21265 from vfdev-5:pmap-free-threading-support d2f5df9c0decdb7e55a2013f5506dee6fc358298
",copybara-service[bot],2025-01-12 17:15:41+00:00,[],2025-01-12 17:53:21+00:00,2025-01-12 17:53:16+00:00,https://github.com/tensorflow/tensorflow/pull/84694,[],[],
2782561202,pull_request,closed,,Update nanobind to a commit that includes,"Update nanobind to a commit that includes
* https://github.com/wjakob/nanobind/commit/20a367a056d39970ef3cd3bcbd86ccc839828f0a
* https://github.com/wjakob/nanobind/commit/27ba245d82babdd7f504977a1dff25adff00eabf

which are two commits that fix race conditions under Python 3.13 free-threading.
",copybara-service[bot],2025-01-12 16:06:49+00:00,[],2025-01-13 16:10:17+00:00,2025-01-13 16:10:16+00:00,https://github.com/tensorflow/tensorflow/pull/84693,[],[],
2782308581,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-12 06:15:59+00:00,[],2025-01-12 06:15:59+00:00,,https://github.com/tensorflow/tensorflow/pull/84691,[],[],
2782287839,pull_request,closed,,Add a utility pass for writing atom programs and main IFRT func to files.,"Add a utility pass for writing atom programs and main IFRT func to files.
",copybara-service[bot],2025-01-12 04:57:49+00:00,[],2025-01-13 22:08:29+00:00,2025-01-13 22:08:28+00:00,https://github.com/tensorflow/tensorflow/pull/84690,[],[],
2782262110,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-12 03:34:00+00:00,[],2025-01-12 03:34:00+00:00,,https://github.com/tensorflow/tensorflow/pull/84689,[],[],
2782260989,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-12 03:32:05+00:00,[],2025-01-12 03:32:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84688,[],[],
2782259283,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-12 03:25:55+00:00,[],2025-01-12 03:25:55+00:00,,https://github.com/tensorflow/tensorflow/pull/84687,[],[],
2782201080,pull_request,closed,,"optimizing layers utilities by improving runtime, memory and readability","This PR introduces several optimizations and enhancements to the layer utilities module, improving runtime performance, memory efficiency, and overall code readability. These changes aim to streamline operations, reduce redundancy, and ensure better maintainability. The code has not been updated in the past 2 years.

List of the changes:
- introduced VALID_DATA_FORMATS and VALID_PADDING constants to replace repetitive string comparisons. Used set membership checks for faster validation.
- combined tuple validation and conversion into a single step.
- improved error handling and error messages for clarity.
- added lazy evaluation for `true_fn` and `false_fn` using lambdas, ensuring functions are only executed when needed.
- simplified checks for boolean and integer inputs to avoid unnecessary conditions.",advaitpatel,2025-01-12 00:10:46+00:00,['gbaned'],2025-01-19 22:43:31+00:00,2025-01-19 22:43:26+00:00,https://github.com/tensorflow/tensorflow/pull/84686,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2585488610, 'issue_id': 2782201080, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84686/checks?check_run_id=35479625343) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 12, 0, 10, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585489208, 'issue_id': 2782201080, 'author': 'advaitpatel', 'body': 'I have signed the CLA', 'created_at': datetime.datetime(2025, 1, 12, 0, 12, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586020122, 'issue_id': 2782201080, 'author': 'advaitpatel', 'body': 'refactored code based on the above PR review feedback\r\n- refactored convert_data_format to use match-case for better readability and maintainability.\r\n- avoided unnecessary dictionary creation for a single get operation.\r\n- enhanced error messages to provide more clarity to users.', 'created_at': datetime.datetime(2025, 1, 13, 1, 20, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601051414, 'issue_id': 2782201080, 'author': 'advaitpatel', 'body': 'closing this for now. and will raise a new one', 'created_at': datetime.datetime(2025, 1, 19, 22, 43, 26, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-12 00:10:50 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84686/checks?check_run_id=35479625343) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

advaitpatel (Issue Creator) on (2025-01-12 00:12:39 UTC): I have signed the CLA

advaitpatel (Issue Creator) on (2025-01-13 01:20:29 UTC): refactored code based on the above PR review feedback
- refactored convert_data_format to use match-case for better readability and maintainability.
- avoided unnecessary dictionary creation for a single get operation.
- enhanced error messages to provide more clarity to users.

advaitpatel (Issue Creator) on (2025-01-19 22:43:26 UTC): closing this for now. and will raise a new one

"
2782139627,pull_request,closed,,"Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).","Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).
",copybara-service[bot],2025-01-11 20:44:49+00:00,['wangpengmit'],2025-01-14 19:56:28+00:00,2025-01-14 19:56:28+00:00,https://github.com/tensorflow/tensorflow/pull/84685,[],[],
2782090442,pull_request,closed,,[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations,"[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations

Sprinkle std::move when appropriate and add ABSL_PREDICT_FALSE on unlikely branches.

```
name                old cpu/op   new cpu/op   delta
BM_IfRtAddScalars   372ns ± 2%   355ns ± 3%  -4.75%  (p=0.000 n=38+35)
```
",copybara-service[bot],2025-01-11 19:09:00+00:00,['ezhulenev'],2025-01-13 19:27:48+00:00,2025-01-13 19:27:48+00:00,https://github.com/tensorflow/tensorflow/pull/84684,[],[],
2782043358,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 17:55:25+00:00,[],2025-01-11 17:55:25+00:00,,https://github.com/tensorflow/tensorflow/pull/84683,[],[],
2782036336,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 17:37:25+00:00,[],2025-01-14 08:49:36+00:00,2025-01-14 08:49:35+00:00,https://github.com/tensorflow/tensorflow/pull/84682,[],[],
2782028605,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 17:18:22+00:00,[],2025-01-15 06:30:51+00:00,2025-01-15 06:30:50+00:00,https://github.com/tensorflow/tensorflow/pull/84681,[],[],
2782006562,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 16:42:01+00:00,[],2025-01-11 16:42:01+00:00,,https://github.com/tensorflow/tensorflow/pull/84680,[],[],
2782001494,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 16:34:13+00:00,[],2025-01-15 05:57:19+00:00,2025-01-15 05:57:18+00:00,https://github.com/tensorflow/tensorflow/pull/84679,[],[],
2782001140,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 16:33:52+00:00,[],2025-01-11 16:33:52+00:00,,https://github.com/tensorflow/tensorflow/pull/84678,[],[],
2781999755,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 16:30:42+00:00,[],2025-01-11 16:30:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84677,[],[],
2781980240,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 16:00:57+00:00,[],2025-01-11 16:00:57+00:00,,https://github.com/tensorflow/tensorflow/pull/84676,[],[],
2781731257,pull_request,closed,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-01-11 09:39:07+00:00,['ddunl'],2025-01-16 03:58:00+00:00,2025-01-16 03:57:59+00:00,https://github.com/tensorflow/tensorflow/pull/84675,[],[],
2781722719,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:20:55+00:00,[],2025-01-11 09:20:55+00:00,,https://github.com/tensorflow/tensorflow/pull/84674,[],[],
2781722515,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:20:19+00:00,[],2025-01-11 09:20:19+00:00,,https://github.com/tensorflow/tensorflow/pull/84673,[],[],
2781722388,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:19:58+00:00,[],2025-01-11 09:19:58+00:00,,https://github.com/tensorflow/tensorflow/pull/84672,[],[],
2781722379,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:19:56+00:00,[],2025-01-11 09:19:56+00:00,,https://github.com/tensorflow/tensorflow/pull/84671,[],[],
2781722365,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:19:52+00:00,[],2025-01-11 09:19:52+00:00,,https://github.com/tensorflow/tensorflow/pull/84670,[],[],
2781722281,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:19:39+00:00,[],2025-01-11 09:19:39+00:00,,https://github.com/tensorflow/tensorflow/pull/84669,[],[],
2781722064,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:19:00+00:00,[],2025-01-11 09:19:00+00:00,,https://github.com/tensorflow/tensorflow/pull/84668,[],[],
2781721942,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:18:35+00:00,[],2025-01-16 06:30:55+00:00,2025-01-16 06:30:55+00:00,https://github.com/tensorflow/tensorflow/pull/84667,[],[],
2781721897,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:18:26+00:00,[],2025-01-11 09:18:26+00:00,,https://github.com/tensorflow/tensorflow/pull/84666,[],[],
2781721879,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:18:22+00:00,[],2025-01-11 09:18:22+00:00,,https://github.com/tensorflow/tensorflow/pull/84665,[],[],
2781721845,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:18:16+00:00,[],2025-01-11 09:18:16+00:00,,https://github.com/tensorflow/tensorflow/pull/84664,[],[],
2781721757,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:17:58+00:00,[],2025-01-11 09:17:58+00:00,,https://github.com/tensorflow/tensorflow/pull/84663,[],[],
2781721383,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:16:57+00:00,[],2025-01-11 09:16:57+00:00,,https://github.com/tensorflow/tensorflow/pull/84662,[],[],
2781721363,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:16:54+00:00,[],2025-01-11 09:16:54+00:00,,https://github.com/tensorflow/tensorflow/pull/84661,[],[],
2781721049,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:16:01+00:00,[],2025-01-11 09:16:01+00:00,,https://github.com/tensorflow/tensorflow/pull/84660,[],[],
2781720841,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:15:27+00:00,[],2025-01-11 09:15:27+00:00,,https://github.com/tensorflow/tensorflow/pull/84659,[],[],
2781720818,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:15:24+00:00,[],2025-01-11 09:15:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84658,[],[],
2781720687,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:14:59+00:00,[],2025-01-11 09:14:59+00:00,,https://github.com/tensorflow/tensorflow/pull/84657,[],[],
2781720530,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:14:25+00:00,[],2025-01-11 09:14:25+00:00,,https://github.com/tensorflow/tensorflow/pull/84656,[],[],
2781720418,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:14:07+00:00,[],2025-01-11 09:14:07+00:00,,https://github.com/tensorflow/tensorflow/pull/84655,[],[],
2781720284,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 09:13:45+00:00,[],2025-01-11 09:13:45+00:00,,https://github.com/tensorflow/tensorflow/pull/84654,[],[],
2781688140,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-11 07:53:36+00:00,[],2025-01-11 07:53:36+00:00,,https://github.com/tensorflow/tensorflow/pull/84653,[],[],
2781652416,pull_request,open,,Update users of TSL headers and targets to new location in XLA,"Update users of TSL headers and targets to new location in XLA

Updating:
 - `env.h`
 - `env_time.h`
 - `errors.h`
 - `file_statistics.h`
 - `file_system.h`
 - `file_system_helper.h`
 - `logging.h`
 - `macros.h`
 - `status.h`
 - `status_matchers.h`
 - `status_to_from_proto.h`
 - `statusor.h`
 - `test.h`
 - `test_benchmark.h`
 - `threadpool.h`
 - `threadpool_async_executor.h`
 - `threadpool_interface.h`
 - `threadpool_options.h`
 - `types.h`

and associated targets.
",copybara-service[bot],2025-01-11 06:13:45+00:00,['ddunl'],2025-02-07 00:04:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84652,[],[],
2781636303,pull_request,open,,fix a bug when partitioning scatter instruction with same operands,"fix a bug when partitioning scatter instruction with same operands
",copybara-service[bot],2025-01-11 05:46:05+00:00,[],2025-01-11 22:48:50+00:00,,https://github.com/tensorflow/tensorflow/pull/84651,[],[],
2781622186,pull_request,closed,,[xla:pjrt] Add support for forwarding FFI context to C API client,"[xla:pjrt] Add support for forwarding FFI context to C API client

+ Correctly (zero/value-)initialize PJRT_ExecuteOptions in tests and pjrt_c_api_client

```
If the number of initializer clauses is less than the number of members or
initializer list is completely empty, the remaining members are value-initialized
```

Context: https://github.com/openxla/xla/pull/20429
",copybara-service[bot],2025-01-11 05:13:30+00:00,['ezhulenev'],2025-01-15 21:29:12+00:00,2025-01-15 21:29:11+00:00,https://github.com/tensorflow/tensorflow/pull/84650,[],[],
2781613008,pull_request,closed,,Internal relative changes only,"Internal relative changes only
",copybara-service[bot],2025-01-11 04:54:31+00:00,['zzzaries'],2025-01-11 05:08:08+00:00,2025-01-11 05:08:07+00:00,https://github.com/tensorflow/tensorflow/pull/84649,[],[],
2781611969,pull_request,closed,,Adjust the build config to an existing value defined in .bazelrc,"In .bazelrc, there is no build configuration called opt. Update to avoid build failure. ",codinglover222,2025-01-11 04:51:38+00:00,['gbaned'],2025-01-13 05:45:30+00:00,2025-01-13 05:45:30+00:00,https://github.com/tensorflow/tensorflow/pull/84648,"[('ready to pull', 'PR ready for merge process'), ('size:XS', 'CL Change Size: Extra Small')]",[],
2781596107,pull_request,closed,,PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR #18838: [NVIDIA GPU] Support multi-operand collective-permute

Imported from GitHub PR https://github.com/openxla/xla/pull/18838

For collective-permutes with small message sizes, it is beneficial to combine them into a single collective because
1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion;
2. fewer collectives make it easier for LHS to make better decision.

In order to support combining collective-permutes, we need to support multi-operand collective-permute first, a.k.a. the combined collective-permute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands.
Copybara import of the project:

--
5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun <tesun@nvidia.com>:

support multi-operand cp

--
170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun <tesun@nvidia.com>:

minor refactoring

--
0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun <tesun@nvidia.com>:

update python interface

--
9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun <tesun@nvidia.com>:

polish python interface

--
3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun <tesun@nvidia.com>:

formatting

--
d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun <tesun@nvidia.com>:

formatting

--
9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun <tesun@nvidia.com>:

refactor overloading

--
0aff5e0a372af9e4a859b54681acf0501adca096 by Terry Sun <tesun@nvidia.com>:

minor refactor

--
20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a by Terry Sun <tesun@nvidia.com>:

add parser test

--
e5ad9d9b601ada1c3984f34967c98574061bd043 by Terry Sun <tesun@nvidia.com>:

fix merge issue

Merging this change closes #18838

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp e5ad9d9b601ada1c3984f34967c98574061bd043
",copybara-service[bot],2025-01-11 04:15:20+00:00,[],2025-01-16 20:41:05+00:00,2025-01-16 20:41:05+00:00,https://github.com/tensorflow/tensorflow/pull/84646,[],[],
2781596103,pull_request,open,,[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions,"[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions

For consistency with C++ API pass execute context in ExecuteOptions.
",copybara-service[bot],2025-01-11 04:15:19+00:00,['ezhulenev'],2025-01-11 04:15:20+00:00,,https://github.com/tensorflow/tensorflow/pull/84645,[],[],
2781596067,pull_request,closed,,"OpenCL wrappers for device, command queue and buffer management","OpenCL wrappers for device, command queue and buffer management
OpenCL loaders for various platforms.
",copybara-service[bot],2025-01-11 04:15:14+00:00,[],2025-01-11 05:41:04+00:00,2025-01-11 05:41:03+00:00,https://github.com/tensorflow/tensorflow/pull/84644,[],[],
2781591744,pull_request,open,,[xla:pjrt] Implement default PJRT_ExecuteContext_Create in pjrt_c_api_wrapper_impl,"[xla:pjrt] Implement default PJRT_ExecuteContext_Create in pjrt_c_api_wrapper_impl
",copybara-service[bot],2025-01-11 04:02:52+00:00,['ezhulenev'],2025-01-11 04:02:53+00:00,,https://github.com/tensorflow/tensorflow/pull/84643,[],[],
2781590742,pull_request,closed,,PR #20924: Fix typo in the definition of XLA_PredicatedExtractOp,"PR #20924: Fix typo in the definition of XLA_PredicatedExtractOp

Imported from GitHub PR https://github.com/openxla/xla/pull/20924


Copybara import of the project:

--
b5fc4cb865855be6c653b269592931fe7a2c8fd1 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Fix typo in the definition of XLA_PredicatedExtractOp

Merging this change closes #20924

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20924 from dimvar:dv-misc b5fc4cb865855be6c653b269592931fe7a2c8fd1
",copybara-service[bot],2025-01-11 03:59:37+00:00,[],2025-01-11 04:36:39+00:00,2025-01-11 04:36:37+00:00,https://github.com/tensorflow/tensorflow/pull/84642,[],[],
2781540036,pull_request,closed,,PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction

Imported from GitHub PR https://github.com/openxla/xla/pull/21022

For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality.
Copybara import of the project:

--
5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel <akhil.goel@intel.com>:

Modify conv bias shape

Merging this change closes #21022

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Intel-tensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1
",copybara-service[bot],2025-01-11 02:27:04+00:00,[],2025-01-13 09:32:47+00:00,2025-01-13 09:32:46+00:00,https://github.com/tensorflow/tensorflow/pull/84641,[],"[{'comment_id': 2585013157, 'issue_id': 2781540036, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84641/checks?check_run_id=35461870333) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 11, 2, 27, 9, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-11 02:27:09 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84641/checks?check_run_id=35461870333) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2781530635,pull_request,closed,,PR #20965: [GPU][NFC] Add new cuDNN status handling macro.,"PR #20965: [GPU][NFC] Add new cuDNN status handling macro.

Imported from GitHub PR https://github.com/openxla/xla/pull/20965


Copybara import of the project:

--
951a7f4c1cd0191f5e7260370fbef942d0ab76ca by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Add new cuDNN status handling macro.

Merging this change closes #20965

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20965 from openxla:cudnn_status_macro 951a7f4c1cd0191f5e7260370fbef942d0ab76ca
",copybara-service[bot],2025-01-11 02:20:35+00:00,[],2025-01-13 09:42:14+00:00,2025-01-13 09:42:13+00:00,https://github.com/tensorflow/tensorflow/pull/84640,[],[],
2781523911,pull_request,open,,Integrate LLVM at llvm/llvm-project@b302633bc5b9,"Integrate LLVM at llvm/llvm-project@b302633bc5b9

Updates LLVM usage to match
[b302633bc5b9](https://github.com/llvm/llvm-project/commit/b302633bc5b9)
",copybara-service[bot],2025-01-11 02:07:42+00:00,[],2025-01-11 02:07:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84639,[],[],
2781522359,pull_request,closed,,PR #21134: [XLA:GPU] Add profiler annotation for sequential thunk.,"PR #21134: [XLA:GPU] Add profiler annotation for sequential thunk.

Imported from GitHub PR https://github.com/openxla/xla/pull/21134

This PR wraps sequential thunk with profiler annotations, which will make loop iterations, and conditional branch more easy to read in the profiler. 

The nsys profile looks like this: 
 
 
![image](https://github.com/user-attachments/assets/8a3dd0be-4e1a-4516-ae64-b376336799bd)

Copybara import of the project:

--
eea74b86f5e2b71c915553ec302e16645927e191 by Shawn Wang <shawnw@nvidia.com>:

add nvtx marker for sequential thunk

Merging this change closes #21134

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21134 from shawnwang18:shawnw/add_profiler_for_while_body_cond eea74b86f5e2b71c915553ec302e16645927e191
",copybara-service[bot],2025-01-11 02:03:59+00:00,[],2025-01-11 03:47:59+00:00,2025-01-11 03:47:58+00:00,https://github.com/tensorflow/tensorflow/pull/84638,[],[],
2781520739,pull_request,closed,,PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.

Imported from GitHub PR https://github.com/openxla/xla/pull/20808

This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group.

In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `all-gather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an element-wise op.

The unit test added provides a repro. Also, the JAX unit test in https://github.com/jax-ml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.py#L2167 gives a real-world JAX example.
Copybara import of the project:

--
8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu <yunlongl@x.ai>:

A hacky fix on partitioning collective permute.

--
e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu <yunlongl@x.ai>:

Local change.

--
84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu <yunlongl@x.ai>:

Simplifies sharding in tests.

Merging this change closes #20808

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7
",copybara-service[bot],2025-01-11 02:02:21+00:00,[],2025-01-13 08:26:05+00:00,2025-01-13 08:26:04+00:00,https://github.com/tensorflow/tensorflow/pull/84637,[],[],
2781513757,pull_request,closed,,PR #20340: Fix missing template value,"PR #20340: Fix missing template value

Imported from GitHub PR https://github.com/openxla/xla/pull/20340

Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944

The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `-fno-canonical-system-headers` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined.
Copybara import of the project:

--
75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer <Charles.Hofer@amd.com>:

Fix missing template value

--
e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer <Charles.Hofer@amd.com>:

Change flag to compiler_is_clang

--
373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer <Charles.Hofer@amd.com>:

Fix typo

--
2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Add cuda-only tags for nvidia profiler test

Merging this change closes #20340

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fix-missing-template-value 2be3c309d05f93a48dd9fdd06af8159108920516
",copybara-service[bot],2025-01-11 01:55:36+00:00,[],2025-01-11 03:33:16+00:00,2025-01-11 03:33:15+00:00,https://github.com/tensorflow/tensorflow/pull/84636,[],[],
2781498353,pull_request,closed,,PR #20494: Update slop_factor flag desc in debug_options_flags.cc,"PR #20494: Update slop_factor flag desc in debug_options_flags.cc

Imported from GitHub PR https://github.com/openxla/xla/pull/20494


Copybara import of the project:

--
04a8e94d73c04e7ffcf3674698a5ad3063918703 by Sevin Varoglu <svaroglu@nvidia.com>:

Update slop_factor flag desc in debug_options_flags.cc

--
4a5d4fe9c515e3b96e31628336fdbbb22c4251d4 by Sevin Varoglu <svaroglu@nvidia.com>:

Fix error

--
0347b54cdc337e6239f89baf69ba3f6d6c8f160c by Sevin Varoglu <svaroglu@nvidia.com>:

Add default value

Merging this change closes #20494

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20494 from sfvaroglu:sevin/update_comment 0347b54cdc337e6239f89baf69ba3f6d6c8f160c
",copybara-service[bot],2025-01-11 01:29:43+00:00,[],2025-01-11 02:26:51+00:00,2025-01-11 02:26:50+00:00,https://github.com/tensorflow/tensorflow/pull/84635,[],[],
2781467431,pull_request,closed,,Create a SourceTargetPairs class.,"Create a SourceTargetPairs class.
",copybara-service[bot],2025-01-11 00:33:50+00:00,[],2025-01-18 01:17:32+00:00,2025-01-18 01:17:31+00:00,https://github.com/tensorflow/tensorflow/pull/84634,[],[],
2781467106,pull_request,closed,,PR #21213: [GPU] Fix mutex locking of a cuDNN handle.,"PR #21213: [GPU] Fix mutex locking of a cuDNN handle.

Imported from GitHub PR https://github.com/openxla/xla/pull/21213

The CudnnHandle object containing a mutex has to stay alive while cudnnHandle_t it guards is in use. This brings the use in sync with the other uses in this file. There is no evidence that this caused failures so far, rather prefetching potential problems, therefore no test added.
Copybara import of the project:

--
04729723c06b5dd8e819d45290268bcde2c2ee00 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Fix mutex locking of a cuDNN handle.

The CudnnHandle object containing a mutex has to stay alive while
cudnnHandle_t it guards is in use. This brings the use in sync with the
other uses in this file. There is no evidence that this caused failures
so far, rather prefetching potential problems, therefore no test added.

Merging this change closes #21213

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21213 from openxla:fix_cudnn_locking 04729723c06b5dd8e819d45290268bcde2c2ee00
",copybara-service[bot],2025-01-11 00:33:16+00:00,[],2025-01-11 01:21:36+00:00,2025-01-11 01:21:36+00:00,https://github.com/tensorflow/tensorflow/pull/84633,[],[],
2781467013,pull_request,closed,,PR #21192: [xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes,"PR #21192: [xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes

Imported from GitHub PR https://github.com/openxla/xla/pull/21192

Enables logging when we set TF_CPP_MAX_VLOG_LEVEL and TF_CPP_MIN_LOG_LEVEL

Copybara import of the project:

--
5f0a1883fc9638be4a47d3a3578fdbdb3f2352e1 by Crefeda Rodrigues <crefeda.rodrigues@arm.com>:

[xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes

Signed-off-by: Crefeda Rodrigues <crefeda.rodrigues@arm.com>

Merging this change closes #21192

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21192 from cfRod:onednn-pass-logging 5f0a1883fc9638be4a47d3a3578fdbdb3f2352e1
",copybara-service[bot],2025-01-11 00:33:04+00:00,[],2025-01-11 01:33:38+00:00,2025-01-11 01:33:37+00:00,https://github.com/tensorflow/tensorflow/pull/84632,[],[],
2781457151,pull_request,closed,,Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.,"Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.
",copybara-service[bot],2025-01-11 00:16:51+00:00,[],2025-01-11 17:00:05+00:00,2025-01-11 16:59:51+00:00,https://github.com/tensorflow/tensorflow/pull/84631,[],[],
2781451452,pull_request,closed,,Add idle and busy time for TPUs to OpStats.,"Add idle and busy time for TPUs to OpStats.
Add DutyCycleCombiner for handling intra and inter chip duty cycle aggregation.
Fix DutyCycleTracker bugs with idleness and duplicate active times.
",copybara-service[bot],2025-01-11 00:08:55+00:00,[],2025-01-17 19:49:30+00:00,2025-01-17 19:49:29+00:00,https://github.com/tensorflow/tensorflow/pull/84630,[],[],
2781380118,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:09:26+00:00,['sdasgup3'],2025-01-11 21:48:19+00:00,2025-01-11 21:48:07+00:00,https://github.com/tensorflow/tensorflow/pull/84629,[],[],
2781378969,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:08:41+00:00,['sdasgup3'],2025-01-14 20:17:03+00:00,2025-01-14 20:17:01+00:00,https://github.com/tensorflow/tensorflow/pull/84628,[],[],
2781377757,pull_request,closed,,Apply proper version scripts to pywrap_library artifacts,"Apply proper version scripts to pywrap_library artifacts
",copybara-service[bot],2025-01-10 23:07:53+00:00,['vam-google'],2025-01-11 01:15:35+00:00,2025-01-11 01:15:34+00:00,https://github.com/tensorflow/tensorflow/pull/84627,[],[],
2781374548,pull_request,open,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:05:39+00:00,['sdasgup3'],2025-01-22 22:19:39+00:00,,https://github.com/tensorflow/tensorflow/pull/84626,[],[],
2781374473,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:05:33+00:00,['sdasgup3'],2025-01-11 22:23:50+00:00,2025-01-11 22:23:49+00:00,https://github.com/tensorflow/tensorflow/pull/84625,[],[],
2781374466,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:05:32+00:00,['sdasgup3'],2025-01-11 00:34:21+00:00,2025-01-11 00:34:20+00:00,https://github.com/tensorflow/tensorflow/pull/84624,[],[],
2781374439,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:05:31+00:00,['sdasgup3'],2025-01-14 20:44:59+00:00,2025-01-14 20:44:58+00:00,https://github.com/tensorflow/tensorflow/pull/84623,[],[],
2781374077,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag d3bafebdc0961d61384a49616c29cb9bb6c59db9
",copybara-service[bot],2025-01-10 23:05:26+00:00,['sdasgup3'],2025-01-13 11:28:30+00:00,2025-01-13 11:28:29+00:00,https://github.com/tensorflow/tensorflow/pull/84622,[],[],
2781373455,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:05:02+00:00,['sdasgup3'],2025-01-14 18:46:29+00:00,2025-01-14 18:46:28+00:00,https://github.com/tensorflow/tensorflow/pull/84621,[],[],
2781373370,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:04:56+00:00,['sdasgup3'],2025-01-11 21:58:51+00:00,2025-01-11 21:58:50+00:00,https://github.com/tensorflow/tensorflow/pull/84620,[],[],
2781372758,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:04:31+00:00,['sdasgup3'],2025-01-13 19:19:47+00:00,2025-01-13 19:19:45+00:00,https://github.com/tensorflow/tensorflow/pull/84619,[],[],
2781372711,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:04:27+00:00,['sdasgup3'],2025-01-22 22:51:32+00:00,2025-01-22 22:51:31+00:00,https://github.com/tensorflow/tensorflow/pull/84618,[],[],
2781372682,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:04:24+00:00,['sdasgup3'],2025-01-14 02:16:12+00:00,2025-01-14 02:16:11+00:00,https://github.com/tensorflow/tensorflow/pull/84617,[],[],
2781372309,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:04:19+00:00,['sdasgup3'],2025-01-11 04:57:19+00:00,2025-01-11 04:57:18+00:00,https://github.com/tensorflow/tensorflow/pull/84616,[],[],
2781371328,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:03:27+00:00,['sdasgup3'],2025-01-11 22:12:47+00:00,2025-01-11 22:12:47+00:00,https://github.com/tensorflow/tensorflow/pull/84615,[],[],
2781368829,pull_request,closed,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-10 23:01:50+00:00,['sdasgup3'],2025-01-14 19:31:33+00:00,2025-01-14 19:31:33+00:00,https://github.com/tensorflow/tensorflow/pull/84614,[],[],
2781343525,pull_request,closed,,internal change only to update dependency visibility,"internal change only to update dependency visibility

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Intel-tensorflow:mabuzain/handle-onednn-scalar 576e244530ce0698de0b7137d8e93965fef9d528
",copybara-service[bot],2025-01-10 22:45:39+00:00,[],2025-01-11 04:12:37+00:00,2025-01-11 04:12:37+00:00,https://github.com/tensorflow/tensorflow/pull/84613,[],[],
2781309557,pull_request,closed,,Clarify error messaging for struct size check,"Clarify error messaging for struct size check

This function is the struct size checker that's used only on the plugin side (and is only valid there since it's checking to see if the struct is greater than or equal). Make the error text it generates clear that the plugin version is later than the framework, and hence, is an unsupported combo.
",copybara-service[bot],2025-01-10 22:24:37+00:00,['jparkerh'],2025-01-13 21:52:23+00:00,2025-01-13 21:52:22+00:00,https://github.com/tensorflow/tensorflow/pull/84612,[],[],
2781292091,pull_request,closed,,[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.,"[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.

cache_'s object lock protects executables_ under free-threading mode, so we have to hold the lock.
",copybara-service[bot],2025-01-10 22:14:19+00:00,[],2025-01-11 02:01:04+00:00,2025-01-11 02:01:04+00:00,https://github.com/tensorflow/tensorflow/pull/84611,[],[],
2781248609,pull_request,open,,Reverts b164ad6269c98426b8ae8b1ce9b7926bf2691b11,"Reverts b164ad6269c98426b8ae8b1ce9b7926bf2691b11
",copybara-service[bot],2025-01-10 21:50:01+00:00,[],2025-01-10 21:50:01+00:00,,https://github.com/tensorflow/tensorflow/pull/84610,[],[],
2781219254,pull_request,closed,,Add default condition to `selects` inside `uv.BUILD`,"Add default condition to `selects` inside `uv.BUILD`
",copybara-service[bot],2025-01-10 21:35:33+00:00,['ddunl'],2025-01-13 15:41:22+00:00,2025-01-13 15:41:21+00:00,https://github.com/tensorflow/tensorflow/pull/84609,[],[],
2781198302,pull_request,closed,,[XLA:Python] Use PyEval_SetProfileAllThreads to install the python profiler in all threads under Python 3.12+.,"[XLA:Python] Use PyEval_SetProfileAllThreads to install the python profiler in all threads under Python 3.12+.

This API is thread-safe under Python 3.13 free-threading, not to mention simpler.
",copybara-service[bot],2025-01-10 21:25:17+00:00,[],2025-01-11 01:49:57+00:00,2025-01-11 01:49:56+00:00,https://github.com/tensorflow/tensorflow/pull/84608,[],[],
2781167046,pull_request,closed,,Create an IFRT wrapper around NanoRT.,"Create an IFRT wrapper around NanoRT.

This will allow NanoRT to be easily used from a caller that depends on IFRT, but we can add faster ""pass-through"" APIs as needed when we encounter performance defects.
",copybara-service[bot],2025-01-10 21:08:47+00:00,[],2025-01-11 00:47:23+00:00,2025-01-11 00:47:21+00:00,https://github.com/tensorflow/tensorflow/pull/84607,[],[],
2781150071,pull_request,open,,[XLA:Python] Migrate python profiler hooks to nanobind instead of pybind11.,"[XLA:Python] Migrate python profiler hooks to nanobind instead of pybind11.

This is one of the few remaining users of pybind11 left in JAX. Change in preparation for supporting Python 3.13 free-threading mode in JAX.

In passing, also remove a stale comment referencing pybind11 in xla/python.
",copybara-service[bot],2025-01-10 21:00:12+00:00,[],2025-01-10 21:00:12+00:00,,https://github.com/tensorflow/tensorflow/pull/84606,[],[],
2781109437,pull_request,closed,,[Coordination Service] Fix pjrt_c_api_gpu_test after introducing TryGet,"[Coordination Service] Fix pjrt_c_api_gpu_test after introducing TryGet

KV try_get functions should be linked in pjrt_client creation
",copybara-service[bot],2025-01-10 20:40:16+00:00,['ishark'],2025-01-10 21:12:39+00:00,2025-01-10 21:12:39+00:00,https://github.com/tensorflow/tensorflow/pull/84605,[],[],
2781024527,pull_request,closed,,Integrate LLVM at llvm/llvm-project@35e76b6a4fc7,"Integrate LLVM at llvm/llvm-project@35e76b6a4fc7

Updates LLVM usage to match
[35e76b6a4fc7](https://github.com/llvm/llvm-project/commit/35e76b6a4fc7)
",copybara-service[bot],2025-01-10 20:00:28+00:00,[],2025-01-10 23:42:24+00:00,2025-01-10 23:42:23+00:00,https://github.com/tensorflow/tensorflow/pull/84604,[],[],
2781021429,pull_request,open,,[xla:cpu] Micro-optimizations for ThunkExecutor,"[xla:cpu] Micro-optimizations for ThunkExecutor

Keep in-edges and out-edges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache.

name                                   old cpu/op   new cpu/op   delta
BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%  
BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  -1.63%  
BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  -1.10%  
BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  -0.95%  
BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  -2.21%  
BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  -2.03%  
BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  -3.73%  
BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  -5.46%  
BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  -6.81%  
BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  -7.93%
",copybara-service[bot],2025-01-10 19:59:04+00:00,['ezhulenev'],2025-01-10 19:59:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84603,[],[],
2780826447,pull_request,closed,,[XLA:GPU][Emitters] Allow to vectorize 128 bits for scatter.,"[XLA:GPU][Emitters] Allow to vectorize 128 bits for scatter.
",copybara-service[bot],2025-01-10 18:29:43+00:00,['pifon2a'],2025-01-10 19:28:30+00:00,2025-01-10 19:28:30+00:00,https://github.com/tensorflow/tensorflow/pull/84602,[],[],
2780792964,pull_request,closed,,[TF:TPU] Enable cast tests for recently added FP8 types.,"[TF:TPU] Enable cast tests for recently added FP8 types.

This registers `float8_e4m3fnuz`, `float8_e4m3b11fnuz` and `float8_e5m2fnuz` as supported types for TF TPU devices.
",copybara-service[bot],2025-01-10 18:13:36+00:00,[],2025-01-10 23:34:30+00:00,2025-01-10 23:34:29+00:00,https://github.com/tensorflow/tensorflow/pull/84601,[],[],
2780782191,pull_request,closed,,Delete tfcompile documentation,"Delete tfcompile documentation

tfcompile is deprecated and will be eventually removed. Don't mention it in public documentation.
",copybara-service[bot],2025-01-10 18:08:43+00:00,['ezhulenev'],2025-01-10 18:31:08+00:00,2025-01-10 18:31:07+00:00,https://github.com/tensorflow/tensorflow/pull/84600,[],[],
2780712392,pull_request,closed,,[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.,"[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.

The change upstream has been integrated.
",copybara-service[bot],2025-01-10 17:32:27+00:00,['pifon2a'],2025-01-10 18:40:11+00:00,2025-01-10 18:40:11+00:00,https://github.com/tensorflow/tensorflow/pull/84599,[],[],
2780656898,pull_request,closed,,Cleanup: Remove PjRtMemoryDescription in favor of MemoryKind.,"Cleanup: Remove PjRtMemoryDescription in favor of MemoryKind.
",copybara-service[bot],2025-01-10 16:58:37+00:00,[],2025-01-10 19:37:02+00:00,2025-01-10 19:37:01+00:00,https://github.com/tensorflow/tensorflow/pull/84598,[],[],
2780629119,pull_request,closed,,Reverts a72d9bf92d333bff536f0b9d8eb05d7cff468023,"Reverts a72d9bf92d333bff536f0b9d8eb05d7cff468023
",copybara-service[bot],2025-01-10 16:44:14+00:00,['mihaimaruseac'],2025-01-10 17:17:09+00:00,2025-01-10 17:17:08+00:00,https://github.com/tensorflow/tensorflow/pull/84597,"[('ready to pull', 'PR ready for merge process')]",[],
2780559523,pull_request,closed,,[XLA:GPU] add fusion wrapper tool,"[XLA:GPU] add fusion wrapper tool

converts a file with a single pass to a module
",copybara-service[bot],2025-01-10 16:17:09+00:00,['metaflow'],2025-01-10 17:46:14+00:00,2025-01-10 17:46:13+00:00,https://github.com/tensorflow/tensorflow/pull/84596,[],[],
2780534910,pull_request,closed,,Migrate tf lite owned parsers from mlir_roundtrip_flags to their directory.,"Migrate tf lite owned parsers from mlir_roundtrip_flags to their directory.
",copybara-service[bot],2025-01-10 16:07:28+00:00,['rocketas'],2025-01-15 06:09:31+00:00,2025-01-15 06:09:30+00:00,https://github.com/tensorflow/tensorflow/pull/84595,[],[],
2780496857,pull_request,open,,Add JAX unit test for Shardy which causes the compiler to introduce the `mlir::tensor::TensorDialect`. This was causing the compiler to crash.,"Add JAX unit test for Shardy which causes the compiler to introduce the `mlir::tensor::TensorDialect`. This was causing the compiler to crash.
",copybara-service[bot],2025-01-10 15:53:09+00:00,[],2025-01-10 15:53:09+00:00,,https://github.com/tensorflow/tensorflow/pull/84594,[],[],
2780493361,pull_request,closed,,#sdy fix bug due to tensor dialect being introduced,"#sdy fix bug due to tensor dialect being introduced

When investigating a bug, I discovered this fails in JAX:
```py
NS = jax.sharding.NamedSharding
P = jax.sharding.PartitionSpec

mesh = jax.sharding.Mesh(
        np.reshape(np.array(jax.devices()), (4,2)), ('data', 'model'))

in_avals = (jax.ShapeDtypeStruct((4, 8), jnp.float32),)
shardings = (NS(mesh, P('data',)),)
@partial(jax.jit, out_shardings=shardings)
def gen_dummy_inputs():
  return tuple(
      jax.random.normal(
          jax.random.key(42), shape=in_aval.shape
      ).astype(in_aval.dtype)
      for in_aval in in_avals
  )
gen_dummy_inputs()
```

with the error

```
LLVM ERROR: Building op `tensor.cast` but it isn't known in this MLIRContext: the dialect may not be loaded or this operation hasn't been added by the dialect. See also https://mlir.llvm.org/getting_started/Faq/#registered-loaded-dependent-whats-up-with-dialects-management
```

This was because the sdy-round-trip-import introduces the tensor dialect. I'm unsure which pass adds it, but overall what I see is it is actually undone. The details shouldn't matter as long as the pass doesn't crash and the dialect doesn't show up during propagation.
",copybara-service[bot],2025-01-10 15:51:50+00:00,[],2025-01-10 17:05:34+00:00,2025-01-10 17:05:34+00:00,https://github.com/tensorflow/tensorflow/pull/84593,[],[],
2780427011,pull_request,closed,,Integrate Triton up to [632bfc34](https://github.com/openai/triton/commits/632bfc342d3a7d63ce8b21209355139ee070d392),"Integrate Triton up to [632bfc34](https://github.com/openai/triton/commits/632bfc342d3a7d63ce8b21209355139ee070d392)
",copybara-service[bot],2025-01-10 15:21:10+00:00,[],2025-01-14 06:58:52+00:00,2025-01-14 06:58:50+00:00,https://github.com/tensorflow/tensorflow/pull/84592,[],[],
2780344784,pull_request,closed,,Integrate LLVM at llvm/llvm-project@a531800344dc,"Integrate LLVM at llvm/llvm-project@a531800344dc

Updates LLVM usage to match
[a531800344dc](https://github.com/llvm/llvm-project/commit/a531800344dc)
",copybara-service[bot],2025-01-10 14:41:25+00:00,['d0k'],2025-01-10 15:54:31+00:00,2025-01-10 15:54:30+00:00,https://github.com/tensorflow/tensorflow/pull/84591,[],[],
2780343379,pull_request,closed,,tfcompile: mark tf_library as deprecated,"tfcompile: mark tf_library as deprecated
",copybara-service[bot],2025-01-10 14:40:43+00:00,['cota'],2025-01-13 17:27:17+00:00,2025-01-13 17:27:16+00:00,https://github.com/tensorflow/tensorflow/pull/84590,[],[],
2780333802,pull_request,closed,,"When calling AppendFeatureValues, reserve capacity for the new total size, not","When calling AppendFeatureValues, reserve capacity for the new total size, not
just the size of the newly added values.
",copybara-service[bot],2025-01-10 14:35:48+00:00,[],2025-01-18 07:09:16+00:00,2025-01-18 07:09:16+00:00,https://github.com/tensorflow/tensorflow/pull/84589,[],[],
2780308813,pull_request,closed,,tfcompile: internal visibility change,"tfcompile: internal visibility change
",copybara-service[bot],2025-01-10 14:22:57+00:00,['cota'],2025-01-13 16:16:07+00:00,2025-01-13 16:16:05+00:00,https://github.com/tensorflow/tensorflow/pull/84588,[],[],
2780293524,pull_request,closed,,[XLA:CPU] Add initial thunk serialization.,"[XLA:CPU] Add initial thunk serialization.
",copybara-service[bot],2025-01-10 14:15:13+00:00,[],2025-01-16 19:26:34+00:00,2025-01-16 19:26:33+00:00,https://github.com/tensorflow/tensorflow/pull/84587,[],[],
2780224680,pull_request,closed,,Roll back https://github.com/openxla/xla/pull/19067 because it broke tests.,"Roll back https://github.com/openxla/xla/pull/19067 because it broke tests.

Reverts 83fb63b0afac8c0efa34c9003bd46b4e916a7146
",copybara-service[bot],2025-01-10 13:43:56+00:00,['penpornk'],2025-01-10 14:22:41+00:00,2025-01-10 14:22:41+00:00,https://github.com/tensorflow/tensorflow/pull/84586,[],[],
2780080971,pull_request,closed,,Don't set the promotion state explicetly.,"Don't set the promotion state explicetly.

The method `_set_promotion_state` was removed in numpy 2.2 and the promotion state is set to weak by default: https://numpy.org/devdocs/release/2.2.0-notes.html#nep-50-promotion-state-option-removed
",copybara-service[bot],2025-01-10 12:30:57+00:00,[],2025-01-10 13:25:58+00:00,2025-01-10 13:25:57+00:00,https://github.com/tensorflow/tensorflow/pull/84584,[],[],
2780051180,pull_request,closed,,[XLA:CPU] Emit nested computation name rather than caller's,"[XLA:CPU] Emit nested computation name rather than caller's
",copybara-service[bot],2025-01-10 12:15:07+00:00,[],2025-01-10 12:56:18+00:00,2025-01-10 12:56:18+00:00,https://github.com/tensorflow/tensorflow/pull/84583,[],[],
2780048428,pull_request,closed,,[XLA] Support nested fusions in HloFusionAdaptor,"[XLA] Support nested fusions in HloFusionAdaptor

So far, we assumed that `HloComputationFusion` itself contains no fusion instructions. Adding support for that is one step towards a generic Triton emitter that uses nested fusions for the operands of some specific ops (`dot`, `reduce` and potentially `concat`).
",copybara-service[bot],2025-01-10 12:13:56+00:00,['chsigg'],2025-01-13 11:37:30+00:00,2025-01-13 11:37:29+00:00,https://github.com/tensorflow/tensorflow/pull/84582,[],[],
2780028865,pull_request,closed,,[xla:cpu] Add operator[] to SortIterator,"[xla:cpu] Add operator[] to SortIterator

So it satisfies the requirements for random access iterators.

Upcoming libc++ change requires this
https://github.com/llvm/llvm-project/commit/69b54c1a05c0c63ee28de1279b3a689b7f026e94
",copybara-service[bot],2025-01-10 12:04:58+00:00,['d0k'],2025-01-10 12:35:58+00:00,2025-01-10 12:35:56+00:00,https://github.com/tensorflow/tensorflow/pull/84581,[],[],
2779971503,pull_request,closed,,Fix typo regarding `ImportConstantsPass` comment.,"Fix typo regarding `ImportConstantsPass` comment.

The pass is still operating on `mhlo` constants not `stablehlo` constants. This is because we need to call `mhlo::createFlattenTuplePass` after which is a greedy pattern with folding.
",copybara-service[bot],2025-01-10 11:34:51+00:00,[],2025-01-10 12:45:00+00:00,2025-01-10 12:45:00+00:00,https://github.com/tensorflow/tensorflow/pull/84580,[],[],
2779826461,pull_request,closed,,[XLA:CPU] Integrating ObjectLoader into JITCompiler,"[XLA:CPU] Integrating ObjectLoader into JITCompiler
",copybara-service[bot],2025-01-10 10:25:51+00:00,[],2025-01-29 13:20:41+00:00,2025-01-29 13:20:41+00:00,https://github.com/tensorflow/tensorflow/pull/84579,[],[],
2779682989,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:21:04+00:00,[],2025-01-10 09:21:04+00:00,,https://github.com/tensorflow/tensorflow/pull/84576,[],[],
2779681928,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:20:35+00:00,[],2025-01-10 09:20:35+00:00,,https://github.com/tensorflow/tensorflow/pull/84575,[],[],
2779681551,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:20:24+00:00,[],2025-01-10 09:20:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84574,[],[],
2779679030,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:19:04+00:00,[],2025-01-11 08:47:27+00:00,,https://github.com/tensorflow/tensorflow/pull/84573,[],[],
2779677968,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:18:42+00:00,[],2025-01-10 09:18:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84572,[],[],
2779676217,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:17:48+00:00,[],2025-01-10 09:17:48+00:00,,https://github.com/tensorflow/tensorflow/pull/84571,[],[],
2779675443,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:17:24+00:00,[],2025-01-10 09:17:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84570,[],[],
2779674892,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:17:06+00:00,[],2025-01-10 09:17:06+00:00,,https://github.com/tensorflow/tensorflow/pull/84569,[],[],
2779673445,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:16:21+00:00,[],2025-01-11 08:15:58+00:00,2025-01-11 08:15:58+00:00,https://github.com/tensorflow/tensorflow/pull/84568,[],[],
2779673240,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:16:15+00:00,[],2025-01-11 08:39:16+00:00,2025-01-11 08:39:16+00:00,https://github.com/tensorflow/tensorflow/pull/84567,[],[],
2779671297,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 09:15:16+00:00,[],2025-01-10 09:15:16+00:00,,https://github.com/tensorflow/tensorflow/pull/84566,[],[],
2779647100,pull_request,open,,[NFC] Minor cleanup.,"[NFC] Minor cleanup.
",copybara-service[bot],2025-01-10 09:03:54+00:00,[],2025-01-10 14:08:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84565,[],[],
2779613223,pull_request,closed,,PR #21191: [xla:cpu] Fix missing header in oneDNN ACL build,"PR #21191: [xla:cpu] Fix missing header in oneDNN ACL build

Imported from GitHub PR https://github.com/openxla/xla/pull/21191

Fixes build error 
```
Compiling src/cpu/jit_utils/jit_utils.cpp failed: (Exit 1): clang failed: error executing command (from target @mkl_dnn_acl_compatible//:mkl_dnn_acl) /usr/lib/llvm-14/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 ... (remaining 126 arguments skipped)
 
Use --sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging
external/mkl_dnn_acl_compatible/src/cpu/jit_utils/jit_utils.cpp:34:10: fatal error: 'common/ittnotify/jitprofiling.h' file not found
#include ""common/ittnotify/jitprofiling.h""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
INFO: Elapsed time: 524.121s, Critical Path: 452.78s
INFO: 543 processes: 45 internal, 498 linux-sandbox.
FAILED: Build did NOT complete successfully
```
Build step:
bazel build --config=mkl_aarch64_threadpool --test_output=all --spawn_strategy=sandboxed //xla/...
Copybara import of the project:

--
23e8fadc3e88208219e685115435c40674efec43 by Crefeda Rodrigues <crefeda.rodrigues@arm.com>:

[xla:cpu] Fix missing headers in oneDNN ACL build

Signed-off-by: Crefeda Rodrigues <crefeda.rodrigues@arm.com>

Merging this change closes #21191

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21191 from cfRod:aarch64-xla-build 23e8fadc3e88208219e685115435c40674efec43
",copybara-service[bot],2025-01-10 08:47:08+00:00,[],2025-01-10 13:46:53+00:00,2025-01-10 13:46:53+00:00,https://github.com/tensorflow/tensorflow/pull/84564,[],[],
2779602465,pull_request,closed,,PR #21234: [ROCm] Fix failing dot tests,"PR #21234: [ROCm] Fix failing dot tests

Imported from GitHub PR https://github.com/openxla/xla/pull/21234

Issue introduced here https://github.com/openxla/xla/commit/d1f63e2f60ee4ccb73a5e06484f4783eae79420a

The following tests failed to build:
```
//xla/tests:dot_operation_single_threaded_runtime_test_gpu_amd_any FAILED TO BUILD
//xla/tests:dot_operation_test_autotune_disabled_gpu_amd_any    FAILED TO BUILD
//xla/tests:dot_operation_test_gpu_amd_any                      FAILED TO BUILD
```

...with:
```
[2025-01-09T01:19:37.573Z] xla/tests/dot_operation_test.cc:1014:24: error: there are no arguments to ‘CreateScalarMaxComputation’ that depend on a template parameter, so a declaration of ‘CreateScalarMaxComputation’ must be available [-fpermissive]
[2025-01-09T01:19:37.573Z]  1014 |   XlaComputation max = CreateScalarMaxComputation(F32, &builder);
[2025-01-09T01:19:37.573Z]       |                        ^~~~~~~~~~~~~~~~~~~~~~~~~~
[2025-01-09T01:19:37.573Z] xla/tests/dot_operation_test.cc:1014:24: note: (if you use ‘-fpermissive’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
[2025-01-09T01:19:37.573Z] xla/tests/dot_operation_test.cc: In instantiation of ‘void xla::{anonymous}::DotOperationTestWithCublasLt_F8_ScaledABScaledDWithDAmaxF8_Test<gtest_TypeParam_>::TestBody() [with gtest_TypeParam_ = ml_dtypes::float8_internal::float8_e4m3fnuz]’:
[2025-01-09T01:19:37.573Z] xla/tests/dot_operation_test.cc:986:1:   required from here
[2025-01-09T01:19:37.573Z] xla/tests/dot_operation_test.cc:1014:50: error: ‘CreateScalarMaxComputation’ was not declared in this scope
[2025-01-09T01:19:37.573Z]  1014 |   XlaComputation max = CreateScalarMaxComputation(F32, &builder);
```

Returning `""xla/hlo/builder/lib/arithmetic.h""` include fixed the problem.


Copybara import of the project:

--
79efd63f12e9b41da73a91c2ed1813559734712c by Milica Makevic <Milica.Makevic@amd.com>:

Add ""xla/hlo/builder/lib/arithmetic.h"" include to dot_operation_test

Merging this change closes #21234

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21234 from ROCm:ci_fix_dot_operation_test 79efd63f12e9b41da73a91c2ed1813559734712c
",copybara-service[bot],2025-01-10 08:41:07+00:00,[],2025-01-10 11:03:00+00:00,2025-01-10 11:03:00+00:00,https://github.com/tensorflow/tensorflow/pull/84563,[],[],
2779557763,pull_request,open,,Fix after cl/713720788: Add back the necessary build target.,"Fix after cl/713720788: Add back the necessary build target.
",copybara-service[bot],2025-01-10 08:15:04+00:00,['chsigg'],2025-01-10 08:15:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84562,[],[],
2779552436,pull_request,closed,,[XLA:GPU] Fix broken build.,"[XLA:GPU] Fix broken build.
",copybara-service[bot],2025-01-10 08:12:18+00:00,[],2025-01-10 08:50:46+00:00,2025-01-10 08:50:46+00:00,https://github.com/tensorflow/tensorflow/pull/84561,[],[],
2779517268,pull_request,closed,,[lite/kernels] cpu_backend_gemm: Update TFLITE_WITH_RUY comments,"[lite/kernels] cpu_backend_gemm: Update TFLITE_WITH_RUY comments

Document default on ARM and x86.

Remove mention on non-existent TFLITE_X86_RUY_ENABLED
",copybara-service[bot],2025-01-10 07:50:23+00:00,[],2025-01-10 18:55:25+00:00,2025-01-10 18:55:24+00:00,https://github.com/tensorflow/tensorflow/pull/84560,[],[],
2779475889,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 07:23:32+00:00,[],2025-01-10 07:23:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84559,[],[],
2779431563,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 06:54:17+00:00,[],2025-01-10 06:54:17+00:00,,https://github.com/tensorflow/tensorflow/pull/84557,[],[],
2779414846,pull_request,closed,,PR #21245: Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu,"PR #21245: Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu

Imported from GitHub PR https://github.com/openxla/xla/pull/21245

This test fails because the hold is not checked before use. Added the check.
Copybara import of the project:

--
c4c71fecbdd28080fd9b50c2adc7d05c65dc6921 by Shraiysh Vaishay <svaishay@nvidia.com>:

Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu

This test fails because the hold is not checked before use. Added the
check.

Merging this change closes #21245

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21245 from shraiysh:pjrt_client_test_se_gpu c4c71fecbdd28080fd9b50c2adc7d05c65dc6921
",copybara-service[bot],2025-01-10 06:42:15+00:00,[],2025-01-11 01:41:42+00:00,2025-01-11 01:41:42+00:00,https://github.com/tensorflow/tensorflow/pull/84556,[],[],
2779383413,pull_request,closed,,[xla:cpu][oneDNN] Add missing deps for onednn.,"[xla:cpu][oneDNN] Add missing deps for onednn.
",copybara-service[bot],2025-01-10 06:19:45+00:00,[],2025-01-10 15:41:58+00:00,2025-01-10 15:41:56+00:00,https://github.com/tensorflow/tensorflow/pull/84555,[],[],
2779376861,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 06:14:27+00:00,[],2025-01-10 06:14:27+00:00,,https://github.com/tensorflow/tensorflow/pull/84554,[],[],
2779364803,pull_request,open,,Internal test changes for int1/2 types.,"Internal test changes for int1/2 types.
",copybara-service[bot],2025-01-10 06:04:21+00:00,[],2025-01-10 06:04:21+00:00,,https://github.com/tensorflow/tensorflow/pull/84553,[],[],
2779359746,pull_request,closed,,Handle missing dtype cases in `xla::ifrt::DType::DebugString()`,"Handle missing dtype cases in `xla::ifrt::DType::DebugString()`
",copybara-service[bot],2025-01-10 06:00:43+00:00,[],2025-01-10 16:55:05+00:00,2025-01-10 16:55:04+00:00,https://github.com/tensorflow/tensorflow/pull/84552,[],[],
2779332657,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 05:37:50+00:00,[],2025-01-10 08:29:12+00:00,2025-01-10 08:29:12+00:00,https://github.com/tensorflow/tensorflow/pull/84551,[],[],
2779324105,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 05:29:49+00:00,[],2025-01-10 05:29:49+00:00,,https://github.com/tensorflow/tensorflow/pull/84550,[],[],
2779317274,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 05:23:21+00:00,[],2025-01-10 05:23:21+00:00,,https://github.com/tensorflow/tensorflow/pull/84549,[],[],
2779306479,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 05:13:01+00:00,[],2025-01-15 12:28:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84548,[],[],
2779270913,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:44:11+00:00,[],2025-01-10 04:44:11+00:00,,https://github.com/tensorflow/tensorflow/pull/84547,[],[],
2779269312,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:42:36+00:00,[],2025-01-10 09:29:55+00:00,2025-01-10 09:29:54+00:00,https://github.com/tensorflow/tensorflow/pull/84546,[],[],
2779262510,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:35:28+00:00,[],2025-01-11 08:52:45+00:00,2025-01-11 08:52:44+00:00,https://github.com/tensorflow/tensorflow/pull/84545,[],[],
2779260854,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:33:34+00:00,[],2025-01-10 04:33:34+00:00,,https://github.com/tensorflow/tensorflow/pull/84544,[],[],
2779259717,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:32:26+00:00,[],2025-01-15 07:05:32+00:00,2025-01-15 07:05:31+00:00,https://github.com/tensorflow/tensorflow/pull/84543,[],[],
2779259193,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:31:50+00:00,[],2025-01-10 08:11:54+00:00,2025-01-10 08:11:52+00:00,https://github.com/tensorflow/tensorflow/pull/84542,[],[],
2779259179,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:31:49+00:00,[],2025-01-16 11:53:59+00:00,,https://github.com/tensorflow/tensorflow/pull/84541,[],[],
2779258085,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:30:35+00:00,[],2025-01-10 04:30:35+00:00,,https://github.com/tensorflow/tensorflow/pull/84540,[],[],
2779257569,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:30:01+00:00,[],2025-01-11 07:33:21+00:00,2025-01-11 07:33:20+00:00,https://github.com/tensorflow/tensorflow/pull/84539,[],[],
2779252545,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:26:23+00:00,[],2025-01-10 04:26:23+00:00,,https://github.com/tensorflow/tensorflow/pull/84538,[],[],
2779245138,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:22:11+00:00,[],2025-01-13 08:45:55+00:00,2025-01-13 08:45:55+00:00,https://github.com/tensorflow/tensorflow/pull/84537,[],[],
2779243806,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-10 04:20:40+00:00,[],2025-01-10 04:20:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84536,[],[],
2779242558,pull_request,open,,"[lib/jpeg] Remove unused ""tensorflow/core/platform/types.h"" include from header files.","[lib/jpeg] Remove unused ""tensorflow/core/platform/types.h"" include from header files.

This reduces the number of transitive includes, which complicate the build graph, and lead to layering check failures.
",copybara-service[bot],2025-01-10 04:19:22+00:00,[],2025-01-10 05:03:10+00:00,,https://github.com/tensorflow/tensorflow/pull/84535,[],[],
2779185988,pull_request,closed,,Add support for int1 types in literal.cc,"Add support for int1 types in literal.cc
",copybara-service[bot],2025-01-10 03:20:17+00:00,[],2025-01-10 16:04:05+00:00,2025-01-10 16:04:05+00:00,https://github.com/tensorflow/tensorflow/pull/84534,[],[],
2779182745,pull_request,open,,In progress experimention. Add StringDType to JAX's supported types.,"In progress experimention. Add StringDType to JAX's supported types.
",copybara-service[bot],2025-01-10 03:16:47+00:00,[],2025-01-13 19:28:47+00:00,,https://github.com/tensorflow/tensorflow/pull/84533,[],[],
2779171851,pull_request,open,,In progress experimention. Add StringDType to JAX's supported types.,"In progress experimention. Add StringDType to JAX's supported types.
",copybara-service[bot],2025-01-10 03:05:10+00:00,[],2025-01-10 03:05:10+00:00,,https://github.com/tensorflow/tensorflow/pull/84532,[],[],
2779121516,pull_request,closed,,[xla:cpu] Micro-optimizations for ThunkExecutor,"[xla:cpu] Micro-optimizations for ThunkExecutor

Keep in-edges and out-edges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache.

name                                   old cpu/op   new cpu/op   delta
BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%  
BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  -1.63%  
BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  -1.10%  
BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  -0.95%  
BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  -2.21%  
BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  -2.03%  
BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  -3.73%  
BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  -5.46%  
BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  -6.81%  
BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  -7.93%
",copybara-service[bot],2025-01-10 02:10:00+00:00,['ezhulenev'],2025-01-10 19:53:20+00:00,2025-01-10 19:53:20+00:00,https://github.com/tensorflow/tensorflow/pull/84531,[],[],
2779120620,pull_request,closed,,Update ml_dtypes version to 0fa5313b65efe848c5968a15dd37dd220cc29567.,"Update ml_dtypes version to 0fa5313b65efe848c5968a15dd37dd220cc29567.

Also add mxfloat as a dependency to TensorFlow and TSL. This is needed to merge https://github.com/openxla/xla/pull/19096. Previously this was done in the merge commit for that PR, but the PR was rolled back since the new types caused an internal TF Android build to fail. Now it's being done in this separate, smaller change so its easier to rollback if issues occur.
",copybara-service[bot],2025-01-10 02:08:58+00:00,['reedwm'],2025-01-14 23:54:52+00:00,2025-01-11 02:20:12+00:00,https://github.com/tensorflow/tensorflow/pull/84530,[],"[{'comment_id': 2590899703, 'issue_id': 2779120620, 'author': 'matangover', 'body': '@reedwm I believe you should also update `GIT_TAG` in `ml_dtypes.cmake`. CMake build is currently failing due to missing mxfloat.', 'created_at': datetime.datetime(2025, 1, 14, 19, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590906909, 'issue_id': 2779120620, 'author': 'reedwm', 'body': 'Thank you for letting me know! I will update this immediately.', 'created_at': datetime.datetime(2025, 1, 14, 19, 11, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590967418, 'issue_id': 2779120620, 'author': 'matangover', 'body': 'Thank you!', 'created_at': datetime.datetime(2025, 1, 14, 19, 45, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591338751, 'issue_id': 2779120620, 'author': 'reedwm', 'body': 'Updated the GIT_TAG in https://github.com/tensorflow/tensorflow/commit/30e98fcdd9deccb894eb85cc384d06fa32b8d6f2.', 'created_at': datetime.datetime(2025, 1, 14, 23, 54, 51, tzinfo=datetime.timezone.utc)}]","matangover on (2025-01-14 19:07:17 UTC): @reedwm I believe you should also update `GIT_TAG` in `ml_dtypes.cmake`. CMake build is currently failing due to missing mxfloat.

reedwm (Assginee) on (2025-01-14 19:11:23 UTC): Thank you for letting me know! I will update this immediately.

matangover on (2025-01-14 19:45:08 UTC): Thank you!

reedwm (Assginee) on (2025-01-14 23:54:51 UTC): Updated the GIT_TAG in https://github.com/tensorflow/tensorflow/commit/30e98fcdd9deccb894eb85cc384d06fa32b8d6f2.

"
2779087769,pull_request,open,,"[PJRT:C] Fix PJRT_Api_STRUCT_SIZE, with the change to Create API to support TryGet API.","[PJRT:C] Fix PJRT_Api_STRUCT_SIZE, with the change to Create API to support TryGet API.
",copybara-service[bot],2025-01-10 01:44:28+00:00,['ishark'],2025-01-10 01:44:29+00:00,,https://github.com/tensorflow/tensorflow/pull/84528,[],[],
2779076167,pull_request,closed,,Fix bad merge that skipped exporting tags.,"Fix bad merge that skipped exporting tags.
",copybara-service[bot],2025-01-10 01:32:21+00:00,['jpienaar'],2025-01-10 13:37:29+00:00,2025-01-10 13:37:29+00:00,https://github.com/tensorflow/tensorflow/pull/84527,[],[],
2779049655,pull_request,closed,,Plug the `allow_id_dropping` from the user configuration.,"Plug the `allow_id_dropping` from the user configuration.
",copybara-service[bot],2025-01-10 00:59:54+00:00,['pineapplejuice233'],2025-01-10 23:22:35+00:00,2025-01-10 23:22:35+00:00,https://github.com/tensorflow/tensorflow/pull/84526,[],[],
2779049489,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2025-01-10 00:59:41+00:00,[],2025-01-10 03:19:43+00:00,2025-01-10 03:19:43+00:00,https://github.com/tensorflow/tensorflow/pull/84525,[],[],
2779043568,pull_request,closed,,Extract a common helper function `HandleElementwiseWithDimsToReplicate` in `SpmdPartitioningVisitor`.,"Extract a common helper function `HandleElementwiseWithDimsToReplicate` in `SpmdPartitioningVisitor`.

Based on that, add `HandleCholesky` and `HandleTriangularSolve`. Before this change, we replicate all dimensions in these ops. With this cl, we only replicate the last two dimensions for these two operations.
",copybara-service[bot],2025-01-10 00:52:09+00:00,[],2025-01-11 01:09:23+00:00,2025-01-11 01:09:23+00:00,https://github.com/tensorflow/tensorflow/pull/84524,[],[],
2779026503,pull_request,closed,,Add original cp name prefix to the names of the decomposed instructions when breaking circular cp for better traceability.,"Add original cp name prefix to the names of the decomposed instructions when breaking circular cp for better traceability.
",copybara-service[bot],2025-01-10 00:30:47+00:00,[],2025-01-13 22:26:30+00:00,2025-01-13 22:26:29+00:00,https://github.com/tensorflow/tensorflow/pull/84522,[],[],
2779023233,pull_request,closed,,[xla] Rename RendezvousSingle to Rendezvous,"[xla] Rename RendezvousSingle to Rendezvous
",copybara-service[bot],2025-01-10 00:26:55+00:00,['ezhulenev'],2025-01-10 10:06:59+00:00,2025-01-10 10:06:58+00:00,https://github.com/tensorflow/tensorflow/pull/84521,[],[],
2779022676,pull_request,closed,,Remove host memory space as input to HostOffloader constructor.,"Remove host memory space as input to HostOffloader constructor.
",copybara-service[bot],2025-01-10 00:26:18+00:00,['SandSnip3r'],2025-01-11 00:16:36+00:00,2025-01-11 00:16:35+00:00,https://github.com/tensorflow/tensorflow/pull/84520,[],[],
2779021707,pull_request,closed,,[xla:cpu] Migrate ReduceScatter to RendezvousSingle API,"[xla:cpu] Migrate ReduceScatter to RendezvousSingle API
",copybara-service[bot],2025-01-10 00:25:15+00:00,['ezhulenev'],2025-01-10 01:15:26+00:00,2025-01-10 01:15:25+00:00,https://github.com/tensorflow/tensorflow/pull/84519,[],[],
2779021447,pull_request,closed,,[xla] Delete unused refcounting hashmap,"[xla] Delete unused refcounting hashmap
",copybara-service[bot],2025-01-10 00:24:56+00:00,['ezhulenev'],2025-01-10 01:34:56+00:00,2025-01-10 01:34:55+00:00,https://github.com/tensorflow/tensorflow/pull/84518,[],[],
2779019538,pull_request,open,,DO NOT SUBMIT: test for presubmit.,"DO NOT SUBMIT: test for presubmit.
",copybara-service[bot],2025-01-10 00:22:46+00:00,['terryheo'],2025-01-10 01:29:54+00:00,,https://github.com/tensorflow/tensorflow/pull/84517,[],[],
2778956377,pull_request,closed,,Add `SpmdPartitioningVisitor::HandleBitcastConvert`.,"Add `SpmdPartitioningVisitor::HandleBitcastConvert`.

Before this change, we use the default action for BitcastConvert operations. If the input and output has the same rank, it is recognized as an element-wise operations and is handled by `HandleElementwise`. However, if the input and output has different rank, we will always replicate the input, which is inefficient.

With this cl, we can handle cases with different rank smartly. We keep the sharding in batch dims and only replicate the extra dims.

Given the following input
```
ENTRY entry {
  p0 = s64[4] parameter(0), sharding={devices=[2,2]<=[2,2]T(1,0) last_tile_dim_replicate}
  ROOT result = f32[4,2] bitcast-convert(p0), sharding={devices=[2,2]<=[4]}
})"";
```

Previous result replicate the input
```
ENTRY %entry_spmd (param: s64[2]) -> f32[2,1] {
  %param = s64[2]{0} parameter(0), sharding={devices=[2,2]<=[2,2]T(1,0) last_tile_dim_replicate}
  %all-gather = s64[4]{0} all-gather(s64[2]{0} %param), channel_id=1, replica_groups=[2,2]<=[4], dimensions={0}, use_global_device_ids=true
  %result.1 = f32[4,2]{1,0} bitcast-convert(s64[4]{0} %all-gather)
  %constant = s32[4]{0} constant({0, 0, 2, 2})
  %partition-id = u32[] partition-id()
  %dynamic-slice = s32[1]{0} dynamic-slice(s32[4]{0} %constant, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape = s32[] reshape(s32[1]{0} %dynamic-slice)
  %constant.1 = s32[4]{0} constant({0, 1, 0, 1})
  %dynamic-slice.1 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.1, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.1 = s32[] reshape(s32[1]{0} %dynamic-slice.1)
  ROOT %dynamic-slice.2 = f32[2,1]{1,0} dynamic-slice(f32[4,2]{1,0} %result.1, s32[] %reshape, s32[] %reshape.1), dynamic_slice_sizes={2,1}
}
```

New result avoid replication in the batch dimensions
```
ENTRY %entry_spmd (param: s64[2]) -> f32[2,1] {
  %param = s64[2]{0} parameter(0), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}
  %collective-permute = s64[2]{0} collective-permute(s64[2]{0} %param), channel_id=1, source_target_pairs={{0,0},{2,1},{1,2},{3,3}}
  %result.1 = f32[2,2]{1,0} bitcast-convert(s64[2]{0} %collective-permute)
  %constant.3 = s32[4]{0} constant({0, 0, 2, 2})
  %partition-id = u32[] partition-id()
  %dynamic-slice.1 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.3, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.1 = s32[] reshape(s32[1]{0} %dynamic-slice.1)
  %subtract = s32[] subtract(s32[] %reshape.1, s32[] %reshape.1)
  %constant.4 = s32[4]{0} constant({0, 1, 0, 1})
  %dynamic-slice.2 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.4, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.2 = s32[] reshape(s32[1]{0} %dynamic-slice.2)
  %constant.6 = s32[] constant(0)
  %subtract.1 = s32[] subtract(s32[] %reshape.2, s32[] %constant.6)
  ROOT %dynamic-slice.4 = f32[2,1]{1,0} dynamic-slice(f32[2,2]{1,0} %result.1, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={2,1}
}
```
",copybara-service[bot],2025-01-09 23:24:30+00:00,[],2025-01-10 07:15:23+00:00,2025-01-10 07:15:22+00:00,https://github.com/tensorflow/tensorflow/pull/84516,[],[],
2778955943,pull_request,closed,,[XLA] Simplify the scheduler test HLO.,"[XLA] Simplify the scheduler test HLO.
",copybara-service[bot],2025-01-09 23:24:24+00:00,['seherellis'],2025-01-10 00:36:36+00:00,2025-01-10 00:36:35+00:00,https://github.com/tensorflow/tensorflow/pull/84515,[],[],
2778943222,pull_request,closed,,Make ML Drift's fingerprinting logic into a helper function,"Make ML Drift's fingerprinting logic into a helper function
",copybara-service[bot],2025-01-09 23:16:46+00:00,['tf-marissaw'],2025-01-15 22:45:02+00:00,2025-01-15 22:45:02+00:00,https://github.com/tensorflow/tensorflow/pull/84514,[],[],
2778935964,pull_request,closed,,Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding,"Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding
to be available in jaxlib. These will be useful for implementing
c++ device_put.
",copybara-service[bot],2025-01-09 23:11:41+00:00,['pschuh'],2025-01-10 00:16:57+00:00,2025-01-10 00:16:56+00:00,https://github.com/tensorflow/tensorflow/pull/84513,[],[],
2778933927,pull_request,closed,,[XLA:SchedulingAnnotations] Handle instructions with control dependencies.,"[XLA:SchedulingAnnotations] Handle instructions with control dependencies.
",copybara-service[bot],2025-01-09 23:10:25+00:00,['seherellis'],2025-01-13 07:53:49+00:00,2025-01-13 07:53:49+00:00,https://github.com/tensorflow/tensorflow/pull/84512,[],[],
2778914715,pull_request,closed,,Add remaining FP8 (B11)FNUZ types to Tensorflow. This exposes,"Add remaining FP8 (B11)FNUZ types to Tensorflow. This exposes
  * `tf.experimental.float8_e4m3fnuz`
  * `tf.experimental.float8_e4m3b11fnuz`
  * `tf.experimental.float8_e5m2fnuz` as public tensorflow dtypes. With this change we can create and save tensors with these types.
",copybara-service[bot],2025-01-09 22:58:49+00:00,[],2025-01-10 18:00:09+00:00,2025-01-10 18:00:08+00:00,https://github.com/tensorflow/tensorflow/pull/84511,[],[],
2778894936,pull_request,closed,,Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`,"Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`

Also sets `test --test_env=""GTEST_INSTALL_FAILURE_SIGNAL_HANDLER=1""` in the bazelrc to retain stacktrace behavior when tests are killed
",copybara-service[bot],2025-01-09 22:47:03+00:00,['ddunl'],2025-01-13 20:55:32+00:00,2025-01-13 20:55:31+00:00,https://github.com/tensorflow/tensorflow/pull/84510,[],[],
2778894004,pull_request,closed,,Fix oss buld error of dispatch_api,"Fix oss buld error of dispatch_api
",copybara-service[bot],2025-01-09 22:46:22+00:00,['terryheo'],2025-01-10 00:53:26+00:00,2025-01-10 00:53:26+00:00,https://github.com/tensorflow/tensorflow/pull/84509,[],[],
2778886111,pull_request,closed,,This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate),"This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate)

Future CLs will make use of these new classes to eliminate the need for adding bespoke Allocate/Deallocate pairs as new MemoryTypes are added.
",copybara-service[bot],2025-01-09 22:42:17+00:00,[],2025-01-15 00:50:12+00:00,2025-01-15 00:50:11+00:00,https://github.com/tensorflow/tensorflow/pull/84508,[],[],
2778875899,pull_request,open,,In progress experimention. Add StringDType to JAX's supported types.,"In progress experimention. Add StringDType to JAX's supported types.
",copybara-service[bot],2025-01-09 22:35:25+00:00,[],2025-01-10 02:44:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84507,[],[],
2778850384,pull_request,open,,Reverts f25e674a719c0f96a71111ad69bdb1e820cd4118,"Reverts f25e674a719c0f96a71111ad69bdb1e820cd4118
",copybara-service[bot],2025-01-09 22:21:00+00:00,[],2025-01-09 22:21:00+00:00,,https://github.com/tensorflow/tensorflow/pull/84506,[],[],
2778835732,pull_request,closed,,[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion.,"[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion.

The computations are not being sorted in a semantically meaningful order; they are sorted by instruction count with ties being broken consistently but arbitrarily (based on a hash of the string representation of the computation). There is therefore no reason why these passes need to traverse the computations in this specific order.
",copybara-service[bot],2025-01-09 22:09:56+00:00,[],2025-01-13 23:18:21+00:00,2025-01-13 23:18:21+00:00,https://github.com/tensorflow/tensorflow/pull/84505,[],[],
